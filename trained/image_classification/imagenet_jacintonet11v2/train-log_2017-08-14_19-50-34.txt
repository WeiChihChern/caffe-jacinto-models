Logging output to training/imagenet_jacintonet11v2_2017-08-14_19-50-34/train-log_2017-08-14_19-50-34.txt
training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse
training/imagenet_jacintonet11v2_2017-08-14_19-50-34/test
training/imagenet_jacintonet11v2_2017-08-14_19-50-34/test_quantize
I0814 19:50:37.064523 11027 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 19:50:36 2017
I0814 19:50:37.064656 11027 caffe.cpp:611] CuDNN version: 6021
I0814 19:50:37.064661 11027 caffe.cpp:612] CuBLAS version: 8000
I0814 19:50:37.064662 11027 caffe.cpp:613] CUDA version: 8000
I0814 19:50:37.064664 11027 caffe.cpp:614] CUDA driver version: 8000
I0814 19:50:37.325132 11027 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 19:50:37.325716 11027 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 19:50:37.326243 11027 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0814 19:50:37.326757 11027 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0814 19:50:37.326766 11027 caffe.cpp:208] Using GPUs 0, 1, 2
I0814 19:50:37.327088 11027 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0814 19:50:37.327410 11027 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0814 19:50:37.327733 11027 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0814 19:50:37.327766 11027 solver.cpp:42] Solver data type: FLOAT
I0814 19:50:37.327795 11027 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/train.prototxt"
test_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/test.prototxt"
test_iter: 1000
test_interval: 2000
base_lr: 0.1
display: 100
max_iter: 320000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 2
type: "SGD"
I0814 19:50:37.333406 11027 solver.cpp:77] Creating training net from train_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/train.prototxt
I0814 19:50:37.333806 11027 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0814 19:50:37.333812 11027 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0814 19:50:37.333834 11027 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0814 19:50:37.334012 11027 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_train_lmdb"
    batch_size: 43
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0814 19:50:37.334108 11027 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:37.334112 11027 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:37.334115 11027 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:50:37.334122 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.334166 11027 net.cpp:184] Created Layer data (0)
I0814 19:50:37.334169 11027 net.cpp:530] data -> data
I0814 19:50:37.334179 11027 net.cpp:530] data -> label
I0814 19:50:37.334199 11027 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0814 19:50:37.334220 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:37.334939 11065 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0814 19:50:37.336685 11027 data_layer.cpp:185] [0] ReshapePrefetch 43, 3, 224, 224
I0814 19:50:37.336750 11027 data_layer.cpp:209] [0] Output data size: 43, 3, 224, 224
I0814 19:50:37.336755 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:37.336774 11027 net.cpp:245] Setting up data
I0814 19:50:37.336784 11027 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 3 224 224 (6472704)
I0814 19:50:37.336791 11027 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 (43)
I0814 19:50:37.336807 11027 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:50:37.336812 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.336828 11027 net.cpp:184] Created Layer data/bias (1)
I0814 19:50:37.336833 11027 net.cpp:561] data/bias <- data
I0814 19:50:37.336845 11027 net.cpp:530] data/bias -> data/bias
I0814 19:50:37.338804 11027 net.cpp:245] Setting up data/bias
I0814 19:50:37.338815 11027 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 43 3 224 224 (6472704)
I0814 19:50:37.338825 11027 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:50:37.338831 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.338847 11027 net.cpp:184] Created Layer conv1a (2)
I0814 19:50:37.338851 11027 net.cpp:561] conv1a <- data/bias
I0814 19:50:37.338855 11027 net.cpp:530] conv1a -> conv1a
I0814 19:50:37.650455 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.97G, req 0G)
I0814 19:50:37.650482 11027 net.cpp:245] Setting up conv1a
I0814 19:50:37.650488 11027 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 43 32 112 112 (17260544)
I0814 19:50:37.650497 11027 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:50:37.650501 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.650512 11027 net.cpp:184] Created Layer conv1a/bn (3)
I0814 19:50:37.650516 11027 net.cpp:561] conv1a/bn <- conv1a
I0814 19:50:37.650519 11027 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:50:37.651221 11027 net.cpp:245] Setting up conv1a/bn
I0814 19:50:37.651229 11027 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 43 32 112 112 (17260544)
I0814 19:50:37.651237 11027 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:50:37.651239 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.651245 11027 net.cpp:184] Created Layer conv1a/relu (4)
I0814 19:50:37.651247 11027 net.cpp:561] conv1a/relu <- conv1a
I0814 19:50:37.651249 11027 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:50:37.651263 11027 net.cpp:245] Setting up conv1a/relu
I0814 19:50:37.651266 11027 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 43 32 112 112 (17260544)
I0814 19:50:37.651268 11027 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:50:37.651271 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.651278 11027 net.cpp:184] Created Layer conv1b (5)
I0814 19:50:37.651280 11027 net.cpp:561] conv1b <- conv1a
I0814 19:50:37.651283 11027 net.cpp:530] conv1b -> conv1b
I0814 19:50:37.676460 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0814 19:50:37.676472 11027 net.cpp:245] Setting up conv1b
I0814 19:50:37.676477 11027 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 43 32 112 112 (17260544)
I0814 19:50:37.676484 11027 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:50:37.676487 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.676492 11027 net.cpp:184] Created Layer conv1b/bn (6)
I0814 19:50:37.676503 11027 net.cpp:561] conv1b/bn <- conv1b
I0814 19:50:37.676506 11027 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:50:37.677119 11027 net.cpp:245] Setting up conv1b/bn
I0814 19:50:37.677125 11027 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 43 32 112 112 (17260544)
I0814 19:50:37.677131 11027 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:50:37.677134 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.677137 11027 net.cpp:184] Created Layer conv1b/relu (7)
I0814 19:50:37.677139 11027 net.cpp:561] conv1b/relu <- conv1b
I0814 19:50:37.677142 11027 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:50:37.677145 11027 net.cpp:245] Setting up conv1b/relu
I0814 19:50:37.677147 11027 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 43 32 112 112 (17260544)
I0814 19:50:37.677150 11027 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:50:37.677152 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.677157 11027 net.cpp:184] Created Layer pool1 (8)
I0814 19:50:37.677160 11027 net.cpp:561] pool1 <- conv1b
I0814 19:50:37.677163 11027 net.cpp:530] pool1 -> pool1
I0814 19:50:37.677233 11027 net.cpp:245] Setting up pool1
I0814 19:50:37.677238 11027 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 43 32 56 56 (4315136)
I0814 19:50:37.677240 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:50:37.677243 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.677250 11027 net.cpp:184] Created Layer res2a_branch2a (9)
I0814 19:50:37.677253 11027 net.cpp:561] res2a_branch2a <- pool1
I0814 19:50:37.677254 11027 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:50:37.701200 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.71G, req 0G)
I0814 19:50:37.701218 11027 net.cpp:245] Setting up res2a_branch2a
I0814 19:50:37.701223 11027 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 43 64 56 56 (8630272)
I0814 19:50:37.701231 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.701236 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.701242 11027 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0814 19:50:37.701246 11027 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:50:37.701249 11027 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:50:37.701892 11027 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:50:37.701900 11027 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 43 64 56 56 (8630272)
I0814 19:50:37.701906 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.701908 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.701911 11027 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0814 19:50:37.701913 11027 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:50:37.701916 11027 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:50:37.701920 11027 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:50:37.701922 11027 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 43 64 56 56 (8630272)
I0814 19:50:37.701925 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:50:37.701926 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.701933 11027 net.cpp:184] Created Layer res2a_branch2b (12)
I0814 19:50:37.701936 11027 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:50:37.701938 11027 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:50:37.715044 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.64G, req 0G)
I0814 19:50:37.715097 11027 net.cpp:245] Setting up res2a_branch2b
I0814 19:50:37.715109 11027 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 43 64 56 56 (8630272)
I0814 19:50:37.715123 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.715131 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.715148 11027 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0814 19:50:37.715155 11027 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:50:37.715163 11027 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:50:37.716583 11027 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:50:37.716593 11027 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 43 64 56 56 (8630272)
I0814 19:50:37.716599 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.716601 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.716605 11027 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0814 19:50:37.716608 11027 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:50:37.716610 11027 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:50:37.716614 11027 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:50:37.716616 11027 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 43 64 56 56 (8630272)
I0814 19:50:37.716619 11027 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:50:37.716620 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.716626 11027 net.cpp:184] Created Layer pool2 (15)
I0814 19:50:37.716629 11027 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:50:37.716631 11027 net.cpp:530] pool2 -> pool2
I0814 19:50:37.716691 11027 net.cpp:245] Setting up pool2
I0814 19:50:37.716696 11027 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 43 64 28 28 (2157568)
I0814 19:50:37.716699 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:50:37.716701 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.716709 11027 net.cpp:184] Created Layer res3a_branch2a (16)
I0814 19:50:37.716711 11027 net.cpp:561] res3a_branch2a <- pool2
I0814 19:50:37.716714 11027 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:50:37.736474 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.58G, req 0G)
I0814 19:50:37.736486 11027 net.cpp:245] Setting up res3a_branch2a
I0814 19:50:37.736490 11027 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 43 128 28 28 (4315136)
I0814 19:50:37.736495 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.736497 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.736501 11027 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0814 19:50:37.736505 11027 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:50:37.736506 11027 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:50:37.737113 11027 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:50:37.737120 11027 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 43 128 28 28 (4315136)
I0814 19:50:37.737128 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.737130 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.737134 11027 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0814 19:50:37.737136 11027 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:50:37.737138 11027 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:50:37.737143 11027 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:50:37.737144 11027 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 43 128 28 28 (4315136)
I0814 19:50:37.737155 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:50:37.737159 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.737166 11027 net.cpp:184] Created Layer res3a_branch2b (19)
I0814 19:50:37.737169 11027 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:50:37.737171 11027 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:50:37.744837 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.54G, req 0G)
I0814 19:50:37.744858 11027 net.cpp:245] Setting up res3a_branch2b
I0814 19:50:37.744864 11027 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 43 128 28 28 (4315136)
I0814 19:50:37.744873 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.744879 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.744889 11027 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0814 19:50:37.744894 11027 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:50:37.744897 11027 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:50:37.745744 11027 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:50:37.745754 11027 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 43 128 28 28 (4315136)
I0814 19:50:37.745762 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.745766 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.745772 11027 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0814 19:50:37.745775 11027 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:50:37.745779 11027 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:50:37.745785 11027 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:50:37.745790 11027 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 43 128 28 28 (4315136)
I0814 19:50:37.745793 11027 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:50:37.745797 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.745803 11027 net.cpp:184] Created Layer pool3 (22)
I0814 19:50:37.745807 11027 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:50:37.745810 11027 net.cpp:530] pool3 -> pool3
I0814 19:50:37.745892 11027 net.cpp:245] Setting up pool3
I0814 19:50:37.745898 11027 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 43 128 14 14 (1078784)
I0814 19:50:37.745901 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:50:37.745905 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.745914 11027 net.cpp:184] Created Layer res4a_branch2a (23)
I0814 19:50:37.745918 11027 net.cpp:561] res4a_branch2a <- pool3
I0814 19:50:37.745920 11027 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:50:37.769359 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.51G, req 0G)
I0814 19:50:37.769383 11027 net.cpp:245] Setting up res4a_branch2a
I0814 19:50:37.769392 11027 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 43 256 14 14 (2157568)
I0814 19:50:37.769402 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.769409 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.769420 11027 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0814 19:50:37.769428 11027 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:50:37.769433 11027 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:50:37.770390 11027 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:50:37.770400 11027 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 43 256 14 14 (2157568)
I0814 19:50:37.770408 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.770422 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.770427 11027 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0814 19:50:37.770431 11027 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:50:37.770436 11027 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:50:37.770442 11027 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:50:37.770447 11027 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 43 256 14 14 (2157568)
I0814 19:50:37.770450 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:50:37.770455 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.770463 11027 net.cpp:184] Created Layer res4a_branch2b (26)
I0814 19:50:37.770467 11027 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:50:37.770472 11027 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:50:37.781527 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0G)
I0814 19:50:37.781555 11027 net.cpp:245] Setting up res4a_branch2b
I0814 19:50:37.781563 11027 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 43 256 14 14 (2157568)
I0814 19:50:37.781574 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.781579 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.781591 11027 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0814 19:50:37.781597 11027 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:50:37.781602 11027 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:50:37.782634 11027 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:50:37.782647 11027 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 43 256 14 14 (2157568)
I0814 19:50:37.782656 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.782661 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.782672 11027 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0814 19:50:37.782677 11027 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:50:37.782681 11027 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:50:37.782688 11027 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:50:37.782699 11027 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 43 256 14 14 (2157568)
I0814 19:50:37.782703 11027 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:50:37.782706 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.782716 11027 net.cpp:184] Created Layer pool4 (29)
I0814 19:50:37.782719 11027 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:50:37.782723 11027 net.cpp:530] pool4 -> pool4
I0814 19:50:37.782809 11027 net.cpp:245] Setting up pool4
I0814 19:50:37.782815 11027 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 43 256 7 7 (539392)
I0814 19:50:37.782820 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:50:37.782825 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.782835 11027 net.cpp:184] Created Layer res5a_branch2a (30)
I0814 19:50:37.782840 11027 net.cpp:561] res5a_branch2a <- pool4
I0814 19:50:37.782845 11027 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:50:37.837329 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.45G, req 0G)
I0814 19:50:37.837347 11027 net.cpp:245] Setting up res5a_branch2a
I0814 19:50:37.837352 11027 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 43 512 7 7 (1078784)
I0814 19:50:37.837358 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.837374 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.837388 11027 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0814 19:50:37.837391 11027 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:50:37.837395 11027 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:50:37.838028 11027 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:50:37.838035 11027 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 43 512 7 7 (1078784)
I0814 19:50:37.838042 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.838044 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.838049 11027 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0814 19:50:37.838052 11027 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:50:37.838053 11027 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:50:37.838057 11027 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:50:37.838060 11027 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 43 512 7 7 (1078784)
I0814 19:50:37.838063 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:50:37.838064 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.838074 11027 net.cpp:184] Created Layer res5a_branch2b (33)
I0814 19:50:37.838078 11027 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:50:37.838079 11027 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:50:37.856647 11027 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.44G, req 0G)
I0814 19:50:37.856660 11027 net.cpp:245] Setting up res5a_branch2b
I0814 19:50:37.856667 11027 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 43 512 7 7 (1078784)
I0814 19:50:37.856675 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.856679 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.856685 11027 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0814 19:50:37.856688 11027 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:50:37.856691 11027 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:50:37.857342 11027 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:50:37.857348 11027 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 43 512 7 7 (1078784)
I0814 19:50:37.857354 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.857358 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.857362 11027 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0814 19:50:37.857364 11027 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:50:37.857367 11027 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:50:37.857372 11027 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:50:37.857375 11027 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 43 512 7 7 (1078784)
I0814 19:50:37.857378 11027 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:50:37.857380 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.857384 11027 net.cpp:184] Created Layer pool5 (36)
I0814 19:50:37.857388 11027 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:50:37.857389 11027 net.cpp:530] pool5 -> pool5
I0814 19:50:37.857415 11027 net.cpp:245] Setting up pool5
I0814 19:50:37.857419 11027 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 43 512 1 1 (22016)
I0814 19:50:37.857422 11027 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0814 19:50:37.857425 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.857431 11027 net.cpp:184] Created Layer fc1000 (37)
I0814 19:50:37.857442 11027 net.cpp:561] fc1000 <- pool5
I0814 19:50:37.857446 11027 net.cpp:530] fc1000 -> fc1000
I0814 19:50:37.868490 11027 net.cpp:245] Setting up fc1000
I0814 19:50:37.868500 11027 net.cpp:252] TRAIN Top shape for layer 37 'fc1000' 43 1000 (43000)
I0814 19:50:37.868505 11027 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:50:37.868508 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.868521 11027 net.cpp:184] Created Layer loss (38)
I0814 19:50:37.868523 11027 net.cpp:561] loss <- fc1000
I0814 19:50:37.868526 11027 net.cpp:561] loss <- label
I0814 19:50:37.868530 11027 net.cpp:530] loss -> loss
I0814 19:50:37.868705 11027 net.cpp:245] Setting up loss
I0814 19:50:37.868710 11027 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0814 19:50:37.868713 11027 net.cpp:256]     with loss weight 1
I0814 19:50:37.868716 11027 net.cpp:323] loss needs backward computation.
I0814 19:50:37.868719 11027 net.cpp:323] fc1000 needs backward computation.
I0814 19:50:37.868721 11027 net.cpp:323] pool5 needs backward computation.
I0814 19:50:37.868722 11027 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:50:37.868731 11027 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:50:37.868732 11027 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:50:37.868734 11027 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:50:37.868736 11027 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:50:37.868737 11027 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:50:37.868741 11027 net.cpp:323] pool4 needs backward computation.
I0814 19:50:37.868742 11027 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:50:37.868744 11027 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:50:37.868746 11027 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:50:37.868752 11027 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:50:37.868753 11027 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:50:37.868755 11027 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:50:37.868757 11027 net.cpp:323] pool3 needs backward computation.
I0814 19:50:37.868763 11027 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:50:37.868765 11027 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:50:37.868767 11027 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:50:37.868769 11027 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:50:37.868772 11027 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:50:37.868773 11027 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:50:37.868775 11027 net.cpp:323] pool2 needs backward computation.
I0814 19:50:37.868778 11027 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:50:37.868780 11027 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:50:37.868782 11027 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:50:37.868783 11027 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:50:37.868787 11027 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:50:37.868788 11027 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:50:37.868789 11027 net.cpp:323] pool1 needs backward computation.
I0814 19:50:37.868793 11027 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:50:37.868794 11027 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:50:37.868796 11027 net.cpp:323] conv1b needs backward computation.
I0814 19:50:37.868798 11027 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:50:37.868800 11027 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:50:37.868803 11027 net.cpp:323] conv1a needs backward computation.
I0814 19:50:37.868804 11027 net.cpp:325] data/bias does not need backward computation.
I0814 19:50:37.868808 11027 net.cpp:325] data does not need backward computation.
I0814 19:50:37.868816 11027 net.cpp:367] This network produces output loss
I0814 19:50:37.868849 11027 net.cpp:389] Top memory (TRAIN) required for data: 802615296 diff: 802615304
I0814 19:50:37.868852 11027 net.cpp:392] Bottom memory (TRAIN) required for data: 802615296 diff: 802615296
I0814 19:50:37.868854 11027 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 535076864 diff: 535076864
I0814 19:50:37.868856 11027 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0814 19:50:37.868858 11027 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0814 19:50:37.868860 11027 net.cpp:407] Network initialization done.
I0814 19:50:37.869246 11027 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/test.prototxt
W0814 19:50:37.869303 11027 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:50:37.869437 11027 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 19:50:37.869549 11027 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:37.869554 11027 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:37.869557 11027 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:50:37.869561 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.869578 11027 net.cpp:184] Created Layer data (0)
I0814 19:50:37.869582 11027 net.cpp:530] data -> data
I0814 19:50:37.869586 11027 net.cpp:530] data -> label
I0814 19:50:37.869598 11027 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:50:37.869611 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:37.872068 11099 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0814 19:50:37.874652 11027 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 224, 224
I0814 19:50:37.874740 11027 data_layer.cpp:209] (0) Output data size: 17, 3, 224, 224
I0814 19:50:37.874745 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:37.874764 11027 net.cpp:245] Setting up data
I0814 19:50:37.874771 11027 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 224 224 (2558976)
I0814 19:50:37.874778 11027 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0814 19:50:37.874783 11027 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 19:50:37.874788 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.874797 11027 net.cpp:184] Created Layer label_data_1_split (1)
I0814 19:50:37.874812 11027 net.cpp:561] label_data_1_split <- label
I0814 19:50:37.874817 11027 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 19:50:37.874824 11027 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 19:50:37.874827 11027 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 19:50:37.874918 11027 net.cpp:245] Setting up label_data_1_split
I0814 19:50:37.874927 11027 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:50:37.874933 11027 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:50:37.874938 11027 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:50:37.874943 11027 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:50:37.874946 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.874953 11027 net.cpp:184] Created Layer data/bias (2)
I0814 19:50:37.874958 11027 net.cpp:561] data/bias <- data
I0814 19:50:37.874961 11027 net.cpp:530] data/bias -> data/bias
I0814 19:50:37.875183 11027 net.cpp:245] Setting up data/bias
I0814 19:50:37.875192 11027 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 224 224 (2558976)
I0814 19:50:37.875208 11027 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:50:37.875219 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.875236 11027 net.cpp:184] Created Layer conv1a (3)
I0814 19:50:37.875246 11027 net.cpp:561] conv1a <- data/bias
I0814 19:50:37.875254 11027 net.cpp:530] conv1a -> conv1a
I0814 19:50:37.875679 11100 data_layer.cpp:97] (0) Parser threads: 1
I0814 19:50:37.875687 11100 data_layer.cpp:99] (0) Transformer threads: 1
I0814 19:50:37.881799 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.4G, req 0G)
I0814 19:50:37.881819 11027 net.cpp:245] Setting up conv1a
I0814 19:50:37.881827 11027 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 112 112 (6823936)
I0814 19:50:37.881839 11027 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:50:37.881844 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.881855 11027 net.cpp:184] Created Layer conv1a/bn (4)
I0814 19:50:37.881860 11027 net.cpp:561] conv1a/bn <- conv1a
I0814 19:50:37.881865 11027 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:50:37.882766 11027 net.cpp:245] Setting up conv1a/bn
I0814 19:50:37.882776 11027 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 112 112 (6823936)
I0814 19:50:37.882787 11027 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:50:37.882791 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.882797 11027 net.cpp:184] Created Layer conv1a/relu (5)
I0814 19:50:37.882800 11027 net.cpp:561] conv1a/relu <- conv1a
I0814 19:50:37.882804 11027 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:50:37.882809 11027 net.cpp:245] Setting up conv1a/relu
I0814 19:50:37.882814 11027 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 112 112 (6823936)
I0814 19:50:37.882818 11027 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:50:37.882822 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.882834 11027 net.cpp:184] Created Layer conv1b (6)
I0814 19:50:37.882838 11027 net.cpp:561] conv1b <- conv1a
I0814 19:50:37.882841 11027 net.cpp:530] conv1b -> conv1b
I0814 19:50:37.888367 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0814 19:50:37.888381 11027 net.cpp:245] Setting up conv1b
I0814 19:50:37.888386 11027 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 112 112 (6823936)
I0814 19:50:37.888392 11027 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:50:37.888396 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.888413 11027 net.cpp:184] Created Layer conv1b/bn (7)
I0814 19:50:37.888419 11027 net.cpp:561] conv1b/bn <- conv1b
I0814 19:50:37.888423 11027 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:50:37.889291 11027 net.cpp:245] Setting up conv1b/bn
I0814 19:50:37.889299 11027 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 112 112 (6823936)
I0814 19:50:37.889308 11027 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:50:37.889313 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.889318 11027 net.cpp:184] Created Layer conv1b/relu (8)
I0814 19:50:37.889322 11027 net.cpp:561] conv1b/relu <- conv1b
I0814 19:50:37.889325 11027 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:50:37.889331 11027 net.cpp:245] Setting up conv1b/relu
I0814 19:50:37.889336 11027 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 112 112 (6823936)
I0814 19:50:37.889340 11027 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:50:37.889344 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.889350 11027 net.cpp:184] Created Layer pool1 (9)
I0814 19:50:37.889355 11027 net.cpp:561] pool1 <- conv1b
I0814 19:50:37.889360 11027 net.cpp:530] pool1 -> pool1
I0814 19:50:37.889444 11027 net.cpp:245] Setting up pool1
I0814 19:50:37.889451 11027 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 56 56 (1705984)
I0814 19:50:37.889456 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:50:37.889461 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.889469 11027 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 19:50:37.889473 11027 net.cpp:561] res2a_branch2a <- pool1
I0814 19:50:37.889478 11027 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:50:37.895040 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.34G, req 0G)
I0814 19:50:37.895052 11027 net.cpp:245] Setting up res2a_branch2a
I0814 19:50:37.895057 11027 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 56 56 (3411968)
I0814 19:50:37.895064 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.895068 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.895074 11027 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 19:50:37.895078 11027 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:50:37.895083 11027 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:50:37.895948 11027 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:50:37.895958 11027 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 56 56 (3411968)
I0814 19:50:37.895967 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.895972 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.895977 11027 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 19:50:37.895982 11027 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:50:37.895985 11027 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:50:37.895995 11027 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:50:37.895999 11027 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 56 56 (3411968)
I0814 19:50:37.896004 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:50:37.896008 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.896015 11027 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 19:50:37.896147 11027 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:50:37.896159 11027 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:50:37.901507 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0814 19:50:37.901527 11027 net.cpp:245] Setting up res2a_branch2b
I0814 19:50:37.901531 11027 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 56 56 (3411968)
I0814 19:50:37.901536 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.901540 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.901543 11027 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 19:50:37.901546 11027 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:50:37.901548 11027 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:50:37.902215 11027 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:50:37.902222 11027 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 56 56 (3411968)
I0814 19:50:37.902227 11027 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.902230 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.902233 11027 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 19:50:37.902235 11027 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:50:37.902237 11027 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:50:37.902241 11027 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:50:37.902245 11027 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 56 56 (3411968)
I0814 19:50:37.902246 11027 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:50:37.902248 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.902252 11027 net.cpp:184] Created Layer pool2 (16)
I0814 19:50:37.902254 11027 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:50:37.902257 11027 net.cpp:530] pool2 -> pool2
I0814 19:50:37.902318 11027 net.cpp:245] Setting up pool2
I0814 19:50:37.902323 11027 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 28 28 (852992)
I0814 19:50:37.902324 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:50:37.902326 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.902333 11027 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 19:50:37.902334 11027 net.cpp:561] res3a_branch2a <- pool2
I0814 19:50:37.902336 11027 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:50:37.909060 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.31G, req 0G)
I0814 19:50:37.909070 11027 net.cpp:245] Setting up res3a_branch2a
I0814 19:50:37.909075 11027 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 28 28 (1705984)
I0814 19:50:37.909080 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.909082 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.909090 11027 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 19:50:37.909091 11027 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:50:37.909095 11027 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:50:37.909765 11027 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:50:37.909772 11027 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 28 28 (1705984)
I0814 19:50:37.909780 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.909783 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.909786 11027 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 19:50:37.909788 11027 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:50:37.909790 11027 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:50:37.909795 11027 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:50:37.909796 11027 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 28 28 (1705984)
I0814 19:50:37.909809 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:50:37.909812 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.909819 11027 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 19:50:37.909822 11027 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:50:37.909824 11027 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:50:37.913005 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.3G, req 0G)
I0814 19:50:37.913017 11027 net.cpp:245] Setting up res3a_branch2b
I0814 19:50:37.913020 11027 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 28 28 (1705984)
I0814 19:50:37.913024 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.913028 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.913031 11027 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 19:50:37.913033 11027 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:50:37.913036 11027 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:50:37.913707 11027 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:50:37.913714 11027 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 28 28 (1705984)
I0814 19:50:37.913720 11027 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.913722 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.913725 11027 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 19:50:37.913728 11027 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:50:37.913730 11027 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:50:37.913733 11027 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:50:37.913736 11027 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 28 28 (1705984)
I0814 19:50:37.913738 11027 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:50:37.913740 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.913744 11027 net.cpp:184] Created Layer pool3 (23)
I0814 19:50:37.913746 11027 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:50:37.913753 11027 net.cpp:530] pool3 -> pool3
I0814 19:50:37.913815 11027 net.cpp:245] Setting up pool3
I0814 19:50:37.913820 11027 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 14 14 (426496)
I0814 19:50:37.913821 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:50:37.913825 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.913830 11027 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 19:50:37.913831 11027 net.cpp:561] res4a_branch2a <- pool3
I0814 19:50:37.913833 11027 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:50:37.925045 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.29G, req 0G)
I0814 19:50:37.925060 11027 net.cpp:245] Setting up res4a_branch2a
I0814 19:50:37.925065 11027 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 14 14 (852992)
I0814 19:50:37.925071 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.925074 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.925081 11027 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 19:50:37.925086 11027 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:50:37.925088 11027 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:50:37.925773 11027 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:50:37.925781 11027 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 14 14 (852992)
I0814 19:50:37.925787 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.925798 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.925802 11027 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 19:50:37.925804 11027 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:50:37.925806 11027 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:50:37.925812 11027 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:50:37.925813 11027 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 14 14 (852992)
I0814 19:50:37.925815 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:50:37.925819 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.925880 11027 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 19:50:37.925884 11027 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:50:37.925887 11027 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:50:37.931658 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.28G, req 0G)
I0814 19:50:37.931668 11027 net.cpp:245] Setting up res4a_branch2b
I0814 19:50:37.931673 11027 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 14 14 (852992)
I0814 19:50:37.931677 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.931679 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.931689 11027 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 19:50:37.931692 11027 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:50:37.931694 11027 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:50:37.932389 11027 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:50:37.932397 11027 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 14 14 (852992)
I0814 19:50:37.932402 11027 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.932405 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.932409 11027 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 19:50:37.932410 11027 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:50:37.932412 11027 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:50:37.932416 11027 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:50:37.932418 11027 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 14 14 (852992)
I0814 19:50:37.932420 11027 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:50:37.932423 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.932426 11027 net.cpp:184] Created Layer pool4 (30)
I0814 19:50:37.932428 11027 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:50:37.932432 11027 net.cpp:530] pool4 -> pool4
I0814 19:50:37.932497 11027 net.cpp:245] Setting up pool4
I0814 19:50:37.932500 11027 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 7 7 (213248)
I0814 19:50:37.932503 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:50:37.932505 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.932510 11027 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 19:50:37.932513 11027 net.cpp:561] res5a_branch2a <- pool4
I0814 19:50:37.932515 11027 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:50:37.962658 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.28G, req 0G)
I0814 19:50:37.962677 11027 net.cpp:245] Setting up res5a_branch2a
I0814 19:50:37.962682 11027 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 7 7 (426496)
I0814 19:50:37.962687 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:37.962690 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.962713 11027 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 19:50:37.962718 11027 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:50:37.962721 11027 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:50:37.963445 11027 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:50:37.963454 11027 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 7 7 (426496)
I0814 19:50:37.963460 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:50:37.963464 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.963471 11027 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 19:50:37.963474 11027 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:50:37.963476 11027 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:50:37.963481 11027 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:50:37.963485 11027 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 7 7 (426496)
I0814 19:50:37.963486 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:50:37.963488 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.963500 11027 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 19:50:37.963505 11027 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:50:37.963508 11027 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:50:37.980080 11027 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.27G, req 0G)
I0814 19:50:37.980098 11027 net.cpp:245] Setting up res5a_branch2b
I0814 19:50:37.980105 11027 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 7 7 (426496)
I0814 19:50:37.980118 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:37.980123 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.980144 11027 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 19:50:37.980149 11027 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:50:37.980154 11027 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:50:37.980895 11027 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:50:37.980903 11027 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 7 7 (426496)
I0814 19:50:37.980912 11027 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:50:37.980916 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.980922 11027 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 19:50:37.980927 11027 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:50:37.980931 11027 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:50:37.980938 11027 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:50:37.980943 11027 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 7 7 (426496)
I0814 19:50:37.980947 11027 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:50:37.980952 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.980958 11027 net.cpp:184] Created Layer pool5 (37)
I0814 19:50:37.980962 11027 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:50:37.980967 11027 net.cpp:530] pool5 -> pool5
I0814 19:50:37.980998 11027 net.cpp:245] Setting up pool5
I0814 19:50:37.981003 11027 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0814 19:50:37.981007 11027 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0814 19:50:37.981011 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.981019 11027 net.cpp:184] Created Layer fc1000 (38)
I0814 19:50:37.981024 11027 net.cpp:561] fc1000 <- pool5
I0814 19:50:37.981035 11027 net.cpp:530] fc1000 -> fc1000
I0814 19:50:37.992133 11027 net.cpp:245] Setting up fc1000
I0814 19:50:37.992149 11027 net.cpp:252] TEST Top shape for layer 38 'fc1000' 17 1000 (17000)
I0814 19:50:37.992157 11027 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0814 19:50:37.992162 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.992168 11027 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0814 19:50:37.992173 11027 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0814 19:50:37.992178 11027 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0814 19:50:37.992183 11027 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0814 19:50:37.992189 11027 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0814 19:50:37.992265 11027 net.cpp:245] Setting up fc1000_fc1000_0_split
I0814 19:50:37.992272 11027 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0814 19:50:37.992277 11027 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0814 19:50:37.992281 11027 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0814 19:50:37.992285 11027 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:50:37.992290 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.992300 11027 net.cpp:184] Created Layer loss (40)
I0814 19:50:37.992303 11027 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0814 19:50:37.992308 11027 net.cpp:561] loss <- label_data_1_split_0
I0814 19:50:37.992313 11027 net.cpp:530] loss -> loss
I0814 19:50:37.992491 11027 net.cpp:245] Setting up loss
I0814 19:50:37.992498 11027 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 19:50:37.992501 11027 net.cpp:256]     with loss weight 1
I0814 19:50:37.992508 11027 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 19:50:37.992513 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.992522 11027 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 19:50:37.992525 11027 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0814 19:50:37.992530 11027 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 19:50:37.992535 11027 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 19:50:37.992542 11027 net.cpp:245] Setting up accuracy/top1
I0814 19:50:37.992547 11027 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 19:50:37.992552 11027 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 19:50:37.992555 11027 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:37.992561 11027 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 19:50:37.992565 11027 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0814 19:50:37.992569 11027 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 19:50:37.992573 11027 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 19:50:37.992579 11027 net.cpp:245] Setting up accuracy/top5
I0814 19:50:37.992584 11027 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 19:50:37.992588 11027 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 19:50:37.992593 11027 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 19:50:37.992597 11027 net.cpp:323] loss needs backward computation.
I0814 19:50:37.992601 11027 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0814 19:50:37.992605 11027 net.cpp:323] fc1000 needs backward computation.
I0814 19:50:37.992609 11027 net.cpp:323] pool5 needs backward computation.
I0814 19:50:37.992614 11027 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:50:37.992617 11027 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:50:37.992621 11027 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:50:37.992624 11027 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:50:37.992636 11027 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:50:37.992640 11027 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:50:37.992645 11027 net.cpp:323] pool4 needs backward computation.
I0814 19:50:37.992650 11027 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:50:37.992652 11027 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:50:37.992656 11027 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:50:37.992660 11027 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:50:37.992664 11027 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:50:37.992668 11027 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:50:37.992673 11027 net.cpp:323] pool3 needs backward computation.
I0814 19:50:37.992677 11027 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:50:37.992681 11027 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:50:37.992684 11027 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:50:37.992688 11027 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:50:37.992691 11027 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:50:37.992696 11027 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:50:37.992700 11027 net.cpp:323] pool2 needs backward computation.
I0814 19:50:37.992703 11027 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:50:37.992707 11027 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:50:37.992712 11027 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:50:37.992715 11027 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:50:37.992718 11027 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:50:37.992722 11027 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:50:37.992727 11027 net.cpp:323] pool1 needs backward computation.
I0814 19:50:37.992730 11027 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:50:37.992734 11027 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:50:37.992738 11027 net.cpp:323] conv1b needs backward computation.
I0814 19:50:37.992743 11027 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:50:37.992746 11027 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:50:37.992749 11027 net.cpp:323] conv1a needs backward computation.
I0814 19:50:37.992754 11027 net.cpp:325] data/bias does not need backward computation.
I0814 19:50:37.992758 11027 net.cpp:325] label_data_1_split does not need backward computation.
I0814 19:50:37.992763 11027 net.cpp:325] data does not need backward computation.
I0814 19:50:37.992766 11027 net.cpp:367] This network produces output accuracy/top1
I0814 19:50:37.992770 11027 net.cpp:367] This network produces output accuracy/top5
I0814 19:50:37.992774 11027 net.cpp:367] This network produces output loss
I0814 19:50:37.992805 11027 net.cpp:389] Top memory (TEST) required for data: 317313024 diff: 8
I0814 19:50:37.992807 11027 net.cpp:392] Bottom memory (TEST) required for data: 317313024 diff: 317313024
I0814 19:50:37.992811 11027 net.cpp:395] Shared (in-place) memory (TEST) by data: 211542016 diff: 211542016
I0814 19:50:37.992815 11027 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 19:50:37.992818 11027 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 19:50:37.992822 11027 net.cpp:407] Network initialization done.
I0814 19:50:37.992873 11027 solver.cpp:56] Solver scaffolding done.
I0814 19:50:37.997059 11027 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0814 19:50:37.997068 11027 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0814 19:50:37.997072 11027 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0814 19:50:37.997076 11027 parallel.cpp:59] Starting Optimization
I0814 19:50:37.997079 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:37.997122 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:50:37.997148 11027 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:50:37.997815 11101 device_alternate.hpp:116] NVML initialized on thread 140594450110208
I0814 19:50:38.042485 11101 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:50:38.042549 11102 device_alternate.hpp:116] NVML initialized on thread 140594441717504
I0814 19:50:38.043645 11102 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:50:38.043661 11103 device_alternate.hpp:116] NVML initialized on thread 140594433324800
I0814 19:50:38.044329 11103 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:50:38.048924 11102 solver.cpp:42] Solver data type: FLOAT
W0814 19:50:38.049602 11102 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0814 19:50:38.049716 11102 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:38.049722 11102 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:38.049753 11102 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0814 19:50:38.049764 11102 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:50:38.053733 11103 solver.cpp:42] Solver data type: FLOAT
W0814 19:50:38.054111 11103 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0814 19:50:38.054220 11103 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:38.054225 11103 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:38.054250 11103 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0814 19:50:38.054261 11103 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:50:38.054433 11104 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0814 19:50:38.055178 11105 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0814 19:50:38.056732 11102 data_layer.cpp:185] [1] ReshapePrefetch 43, 3, 224, 224
I0814 19:50:38.057041 11103 data_layer.cpp:185] [2] ReshapePrefetch 43, 3, 224, 224
I0814 19:50:38.057170 11102 data_layer.cpp:209] [1] Output data size: 43, 3, 224, 224
I0814 19:50:38.057178 11102 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:50:38.057186 11103 data_layer.cpp:209] [2] Output data size: 43, 3, 224, 224
I0814 19:50:38.057191 11103 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:50:38.508805 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0814 19:50:38.527277 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0814 19:50:38.536005 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0814 19:50:38.554559 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0814 19:50:38.561796 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0814 19:50:38.575036 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0814 19:50:38.580971 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0814 19:50:38.597124 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0814 19:50:38.602109 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.68G, req 0G)
I0814 19:50:38.612742 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0814 19:50:38.619879 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.68G, req 0G)
I0814 19:50:38.629292 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0814 19:50:38.641294 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0814 19:50:38.651650 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0814 19:50:38.655917 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0814 19:50:38.665793 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0814 19:50:38.707104 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0814 19:50:38.716475 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0814 19:50:38.728688 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 1  (limit 7.53G, req 0G)
I0814 19:50:38.737545 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.53G, req 0G)
I0814 19:50:38.742234 11102 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/test.prototxt
W0814 19:50:38.742287 11102 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:50:38.742372 11102 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:38.742377 11102 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:38.742393 11102 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:50:38.742401 11102 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:50:38.743173 11111 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0814 19:50:38.743841 11102 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 224, 224
I0814 19:50:38.743947 11102 data_layer.cpp:209] (1) Output data size: 17, 3, 224, 224
I0814 19:50:38.743953 11102 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:50:38.744657 11112 data_layer.cpp:97] (1) Parser threads: 1
I0814 19:50:38.744665 11112 data_layer.cpp:99] (1) Transformer threads: 1
I0814 19:50:38.750275 11103 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/test.prototxt
W0814 19:50:38.750346 11103 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:50:38.750450 11103 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:38.750457 11103 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:38.750478 11103 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:50:38.750486 11103 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:50:38.750680 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0814 19:50:38.751487 11113 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0814 19:50:38.752460 11103 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 224, 224
I0814 19:50:38.752605 11103 data_layer.cpp:209] (2) Output data size: 17, 3, 224, 224
I0814 19:50:38.752624 11103 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:50:38.757547 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0814 19:50:38.761247 11114 data_layer.cpp:97] (2) Parser threads: 1
I0814 19:50:38.761286 11114 data_layer.cpp:99] (2) Transformer threads: 1
I0814 19:50:38.761812 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0814 19:50:38.766093 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.43G, req 0G)
I0814 19:50:38.771869 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0814 19:50:38.772713 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0814 19:50:38.780632 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.44G, req 0G)
I0814 19:50:38.784689 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0814 19:50:38.791383 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0814 19:50:38.795729 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0814 19:50:38.805323 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0814 19:50:38.805552 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0814 19:50:38.810672 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0814 19:50:38.813679 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0814 19:50:38.824389 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0814 19:50:38.831388 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0814 19:50:38.849736 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0814 19:50:38.863145 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0814 19:50:38.869110 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0814 19:50:38.880878 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0814 19:50:38.884723 11102 solver.cpp:56] Solver scaffolding done.
I0814 19:50:38.893801 11103 solver.cpp:56] Solver scaffolding done.
I0814 19:50:38.937397 11101 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0814 19:50:38.937423 11103 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0814 19:50:38.937424 11102 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0814 19:50:39.115531 11102 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:50:39.115531 11101 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:50:39.115553 11102 solver.cpp:439] Learning Rate Policy: poly
I0814 19:50:39.115557 11101 solver.cpp:439] Learning Rate Policy: poly
I0814 19:50:39.115558 11103 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:50:39.115566 11103 solver.cpp:439] Learning Rate Policy: poly
I0814 19:50:39.124017 11102 solver.cpp:227] Starting Optimization on GPU 1
I0814 19:50:39.124017 11103 solver.cpp:227] Starting Optimization on GPU 2
I0814 19:50:39.124018 11101 solver.cpp:227] Starting Optimization on GPU 0
I0814 19:50:39.124253 11101 solver.cpp:509] Iteration 0, Testing net (#0)
I0814 19:50:39.124284 11130 device_alternate.hpp:116] NVML initialized on thread 140452615071488
I0814 19:50:39.124305 11130 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:50:39.124315 11131 device_alternate.hpp:116] NVML initialized on thread 140452598286080
I0814 19:50:39.124325 11131 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:50:39.124992 11129 device_alternate.hpp:116] NVML initialized on thread 140452606678784
I0814 19:50:39.125005 11129 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:50:39.138424 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.31G, req 0G)
I0814 19:50:39.138926 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.31G, req 0G)
I0814 19:50:39.140844 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.24G, req 0G)
I0814 19:50:39.150158 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.26G, req 0G)
I0814 19:50:39.151382 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.26G, req 0G)
I0814 19:50:39.157848 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.17G, req 0G)
I0814 19:50:39.163622 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.19G, req 0G)
I0814 19:50:39.164345 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.19G, req 0G)
I0814 19:50:39.169348 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0814 19:50:39.171171 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.16G, req 0G)
I0814 19:50:39.171875 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.16G, req 0G)
I0814 19:50:39.175974 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0814 19:50:39.178501 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0814 19:50:39.179105 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0814 19:50:39.183646 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.04G, req 0G)
I0814 19:50:39.185415 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0814 19:50:39.185762 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0814 19:50:39.190065 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0814 19:50:39.194197 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0814 19:50:39.194661 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0814 19:50:39.197001 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0814 19:50:39.198767 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0814 19:50:39.199867 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0814 19:50:39.201598 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0814 19:50:39.207191 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.08G, req 0G)
I0814 19:50:39.207533 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.08G, req 0G)
I0814 19:50:39.208909 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 6.99G, req 0G)
I0814 19:50:39.214061 11103 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0814 19:50:39.214509 11102 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0814 19:50:39.215160 11101 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 6.99G, req 0G)
I0814 19:50:39.221050 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0
I0814 19:50:39.221062 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0
I0814 19:50:39.221067 11101 solver.cpp:594]     Test net output #2: loss = 87.3365 (* 1 = 87.3365 loss)
I0814 19:50:39.221079 11101 solver.cpp:254] [MultiGPU] Initial Test completed
I0814 19:50:39.221091 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 19:50:39.221091 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 19:50:39.319200 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.01G, req 0G)
I0814 19:50:39.319777 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.93G, req 0G)
I0814 19:50:39.320436 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 1  (limit 7.01G, req 0G)
I0814 19:50:39.349571 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.79G, req 0G)
I0814 19:50:39.350736 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.87G, req 0G)
I0814 19:50:39.351260 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.87G, req 0G)
I0814 19:50:39.379676 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 6.63G, req 0G)
I0814 19:50:39.387614 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 6.71G, req 0G)
I0814 19:50:39.387766 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 6.71G, req 0G)
I0814 19:50:39.396261 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.56G, req 0G)
I0814 19:50:39.402341 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.64G, req 0G)
I0814 19:50:39.402889 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.64G, req 0G)
I0814 19:50:39.417068 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.48G, req 0G)
I0814 19:50:39.425251 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.56G, req 0G)
I0814 19:50:39.425518 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.56G, req 0G)
I0814 19:50:39.427695 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.44G, req 0G)
I0814 19:50:39.435868 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.52G, req 0G)
I0814 19:50:39.436159 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.52G, req 0G)
I0814 19:50:39.446990 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.39G, req 0G)
I0814 19:50:39.453845 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.37G, req 0G)
I0814 19:50:39.456609 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.48G, req 0G)
I0814 19:50:39.457306 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 6.48G, req 0G)
I0814 19:50:39.464982 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.46G, req 0G)
I0814 19:50:39.466064 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.46G, req 0G)
I0814 19:50:39.478816 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 6.34G, req 0G)
I0814 19:50:39.485303 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.33G, req 0G)
I0814 19:50:39.488263 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 6.43G, req 0G)
I0814 19:50:39.494189 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 6.43G, req 0G)
I0814 19:50:39.496856 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.42G, req 0G)
I0814 19:50:39.502799 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.42G, req 0G)
I0814 19:50:39.538310 11066 data_layer.cpp:97] [0] Parser threads: 1
I0814 19:50:39.538329 11066 data_layer.cpp:99] [0] Transformer threads: 1
I0814 19:50:39.550602 11107 data_layer.cpp:97] [2] Parser threads: 1
I0814 19:50:39.550616 11107 data_layer.cpp:99] [2] Transformer threads: 1
I0814 19:50:39.560394 11106 data_layer.cpp:97] [1] Parser threads: 1
I0814 19:50:39.560405 11106 data_layer.cpp:99] [1] Transformer threads: 1
I0814 19:50:39.705776 11101 solver.cpp:317] Iteration 0 (0.484654 s), loss = 7.15223
I0814 19:50:39.705808 11101 solver.cpp:334]     Train net output #0: loss = 7.15778 (* 1 = 7.15778 loss)
I0814 19:50:39.705816 11101 sgd_solver.cpp:136] Iteration 0, lr = 0.1, m = 0.9
I0814 19:50:39.735481 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.32G, req 0G)
I0814 19:50:39.753293 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 1  (limit 5.41G, req 0G)
I0814 19:50:39.758275 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.41G, req 0G)
I0814 19:50:39.780160 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.61G, req 0G)
I0814 19:50:39.808262 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0G)
I0814 19:50:39.811112 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0G)
I0814 19:50:39.828337 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.61G, req 0G)
I0814 19:50:39.846657 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.61G, req 0G)
I0814 19:50:39.855748 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.7G, req 0G)
I0814 19:50:39.857900 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 1  (limit 4.7G, req 0G)
I0814 19:50:39.873360 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 1  (limit 4.61G, req 0G)
I0814 19:50:39.874346 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.7G, req 0G)
I0814 19:50:39.876926 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.7G, req 0G)
I0814 19:50:39.882869 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.61G, req 0G)
I0814 19:50:39.901983 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.7G, req 0.06G)
I0814 19:50:39.906272 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.7G, req 0.06G)
I0814 19:50:39.909636 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.61G, req 0.04G)
I0814 19:50:39.913086 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0.06G)
I0814 19:50:39.916792 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0.06G)
I0814 19:50:39.917623 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.61G, req 0.04G)
I0814 19:50:39.942224 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.7G, req 0.06G)
I0814 19:50:39.946383 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.7G, req 0.06G)
I0814 19:50:39.950382 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0.06G)
I0814 19:50:39.954774 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.7G, req 0.06G)
I0814 19:50:39.969154 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.61G, req 0.04G)
I0814 19:50:39.979724 11101 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 6 1 5  (limit 4.61G, req 0.04G)
I0814 19:50:39.999114 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.7G, req 0.06G)
I0814 19:50:40.003717 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.7G, req 0.06G)
I0814 19:50:40.009934 11103 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 6 1 5  (limit 4.7G, req 0.06G)
I0814 19:50:40.015465 11102 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 7 5 5  (limit 4.7G, req 0.06G)
I0814 19:50:40.020555 11101 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.43G -> 0.09G
I0814 19:50:40.052439 11103 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0814 19:50:40.061019 11102 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0814 19:50:40.142096 11101 solver.cpp:317] Iteration 1 (0.436301 s), loss = 7.11891
I0814 19:50:40.142125 11101 solver.cpp:334]     Train net output #0: loss = 7.1578 (* 1 = 7.1578 loss)
I0814 19:50:40.295296 11101 solver.cpp:317] Iteration 2 (0.153193 s), loss = 7.0921
I0814 19:50:40.295318 11101 solver.cpp:334]     Train net output #0: loss = 7.06254 (* 1 = 7.06254 loss)
I0814 19:50:54.419786 11101 solver.cpp:312] Iteration 100 (6.9385 iter/s, 14.1241s/98 iter), loss = 6.54335
I0814 19:50:54.419814 11101 solver.cpp:334]     Train net output #0: loss = 6.68785 (* 1 = 6.68785 loss)
I0814 19:50:54.419821 11101 sgd_solver.cpp:136] Iteration 100, lr = 0.0999688, m = 0.9
I0814 19:51:08.699071 11101 solver.cpp:312] Iteration 200 (7.00335 iter/s, 14.2789s/100 iter), loss = 6.1991
I0814 19:51:08.699190 11101 solver.cpp:334]     Train net output #0: loss = 6.31256 (* 1 = 6.31256 loss)
I0814 19:51:08.699203 11101 sgd_solver.cpp:136] Iteration 200, lr = 0.0999375, m = 0.9
I0814 19:51:23.034523 11101 solver.cpp:312] Iteration 300 (6.97591 iter/s, 14.335s/100 iter), loss = 6.15036
I0814 19:51:23.034548 11101 solver.cpp:334]     Train net output #0: loss = 5.9604 (* 1 = 5.9604 loss)
I0814 19:51:23.034554 11101 sgd_solver.cpp:136] Iteration 300, lr = 0.0999063, m = 0.9
I0814 19:51:37.567703 11101 solver.cpp:312] Iteration 400 (6.881 iter/s, 14.5328s/100 iter), loss = 5.84841
I0814 19:51:37.567726 11101 solver.cpp:334]     Train net output #0: loss = 6.0392 (* 1 = 6.0392 loss)
I0814 19:51:37.567733 11101 sgd_solver.cpp:136] Iteration 400, lr = 0.099875, m = 0.9
I0814 19:51:52.181656 11101 solver.cpp:312] Iteration 500 (6.84297 iter/s, 14.6135s/100 iter), loss = 5.63098
I0814 19:51:52.188186 11101 solver.cpp:334]     Train net output #0: loss = 5.78988 (* 1 = 5.78988 loss)
I0814 19:51:52.188206 11101 sgd_solver.cpp:136] Iteration 500, lr = 0.0998438, m = 0.9
I0814 19:52:06.757910 11101 solver.cpp:312] Iteration 600 (6.86066 iter/s, 14.5759s/100 iter), loss = 5.57442
I0814 19:52:06.757980 11101 solver.cpp:334]     Train net output #0: loss = 5.48549 (* 1 = 5.48549 loss)
I0814 19:52:06.757998 11101 sgd_solver.cpp:136] Iteration 600, lr = 0.0998125, m = 0.9
I0814 19:52:21.110262 11101 solver.cpp:312] Iteration 700 (6.96769 iter/s, 14.352s/100 iter), loss = 5.436
I0814 19:52:21.110342 11101 solver.cpp:334]     Train net output #0: loss = 5.64767 (* 1 = 5.64767 loss)
I0814 19:52:21.110361 11101 sgd_solver.cpp:136] Iteration 700, lr = 0.0997813, m = 0.9
I0814 19:52:35.453290 11101 solver.cpp:312] Iteration 800 (6.97222 iter/s, 14.3426s/100 iter), loss = 5.47082
I0814 19:52:35.453361 11101 solver.cpp:334]     Train net output #0: loss = 5.50939 (* 1 = 5.50939 loss)
I0814 19:52:35.453368 11101 sgd_solver.cpp:136] Iteration 800, lr = 0.09975, m = 0.9
I0814 19:52:49.719832 11101 solver.cpp:312] Iteration 900 (7.0096 iter/s, 14.2661s/100 iter), loss = 5.10596
I0814 19:52:49.719859 11101 solver.cpp:334]     Train net output #0: loss = 5.2533 (* 1 = 5.2533 loss)
I0814 19:52:49.719866 11101 sgd_solver.cpp:136] Iteration 900, lr = 0.0997187, m = 0.9
I0814 19:53:04.140532 11101 solver.cpp:312] Iteration 1000 (6.93467 iter/s, 14.4203s/100 iter), loss = 4.80438
I0814 19:53:04.140561 11101 solver.cpp:334]     Train net output #0: loss = 4.87827 (* 1 = 4.87827 loss)
I0814 19:53:04.140568 11101 sgd_solver.cpp:136] Iteration 1000, lr = 0.0996875, m = 0.9
I0814 19:53:18.611155 11101 solver.cpp:312] Iteration 1100 (6.91075 iter/s, 14.4702s/100 iter), loss = 4.86442
I0814 19:53:18.611407 11101 solver.cpp:334]     Train net output #0: loss = 4.68505 (* 1 = 4.68505 loss)
I0814 19:53:18.611518 11101 sgd_solver.cpp:136] Iteration 1100, lr = 0.0996562, m = 0.9
I0814 19:53:32.948977 11101 solver.cpp:312] Iteration 1200 (6.97476 iter/s, 14.3374s/100 iter), loss = 5.04499
I0814 19:53:32.949193 11101 solver.cpp:334]     Train net output #0: loss = 4.90713 (* 1 = 4.90713 loss)
I0814 19:53:32.949302 11101 sgd_solver.cpp:136] Iteration 1200, lr = 0.099625, m = 0.9
I0814 19:53:47.370499 11101 solver.cpp:312] Iteration 1300 (6.93428 iter/s, 14.4211s/100 iter), loss = 4.52957
I0814 19:53:47.370525 11101 solver.cpp:334]     Train net output #0: loss = 4.51571 (* 1 = 4.51571 loss)
I0814 19:53:47.370532 11101 sgd_solver.cpp:136] Iteration 1300, lr = 0.0995938, m = 0.9
I0814 19:54:01.790846 11101 solver.cpp:312] Iteration 1400 (6.93484 iter/s, 14.4199s/100 iter), loss = 4.57344
I0814 19:54:01.790951 11101 solver.cpp:334]     Train net output #0: loss = 4.65904 (* 1 = 4.65904 loss)
I0814 19:54:01.790971 11101 sgd_solver.cpp:136] Iteration 1400, lr = 0.0995625, m = 0.9
I0814 19:54:16.206753 11101 solver.cpp:312] Iteration 1500 (6.93698 iter/s, 14.4155s/100 iter), loss = 4.52216
I0814 19:54:16.206781 11101 solver.cpp:334]     Train net output #0: loss = 4.80774 (* 1 = 4.80774 loss)
I0814 19:54:16.206787 11101 sgd_solver.cpp:136] Iteration 1500, lr = 0.0995313, m = 0.9
I0814 19:54:30.745362 11101 solver.cpp:312] Iteration 1600 (6.87843 iter/s, 14.5382s/100 iter), loss = 4.59312
I0814 19:54:30.745390 11101 solver.cpp:334]     Train net output #0: loss = 4.49153 (* 1 = 4.49153 loss)
I0814 19:54:30.745396 11101 sgd_solver.cpp:136] Iteration 1600, lr = 0.0995, m = 0.9
I0814 19:54:45.443904 11101 solver.cpp:312] Iteration 1700 (6.80359 iter/s, 14.6981s/100 iter), loss = 4.43769
I0814 19:54:45.444005 11101 solver.cpp:334]     Train net output #0: loss = 4.31585 (* 1 = 4.31585 loss)
I0814 19:54:45.444023 11101 sgd_solver.cpp:136] Iteration 1700, lr = 0.0994688, m = 0.9
I0814 19:54:59.765728 11101 solver.cpp:312] Iteration 1800 (6.98255 iter/s, 14.3214s/100 iter), loss = 4.53216
I0814 19:54:59.765799 11101 solver.cpp:334]     Train net output #0: loss = 4.67962 (* 1 = 4.67962 loss)
I0814 19:54:59.765820 11101 sgd_solver.cpp:136] Iteration 1800, lr = 0.0994375, m = 0.9
I0814 19:55:14.138955 11101 solver.cpp:312] Iteration 1900 (6.95758 iter/s, 14.3728s/100 iter), loss = 4.1321
I0814 19:55:14.138981 11101 solver.cpp:334]     Train net output #0: loss = 4.4036 (* 1 = 4.4036 loss)
I0814 19:55:14.138988 11101 sgd_solver.cpp:136] Iteration 1900, lr = 0.0994062, m = 0.9
I0814 19:55:28.634054 11101 solver.cpp:509] Iteration 2000, Testing net (#0)
I0814 19:55:48.566614 11099 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:55:49.613382 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.10894
I0814 19:55:49.613409 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.266881
I0814 19:55:49.613415 11101 solver.cpp:594]     Test net output #2: loss = 4.78247 (* 1 = 4.78247 loss)
I0814 19:55:49.613440 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9788s
I0814 19:55:49.785842 11101 solver.cpp:312] Iteration 2000 (2.80537 iter/s, 35.6459s/100 iter), loss = 4.57387
I0814 19:55:49.785868 11101 solver.cpp:334]     Train net output #0: loss = 4.91321 (* 1 = 4.91321 loss)
I0814 19:55:49.785873 11101 sgd_solver.cpp:136] Iteration 2000, lr = 0.099375, m = 0.9
I0814 19:56:04.243508 11101 solver.cpp:312] Iteration 2100 (6.91694 iter/s, 14.4573s/100 iter), loss = 4.02393
I0814 19:56:04.243562 11101 solver.cpp:334]     Train net output #0: loss = 4.19025 (* 1 = 4.19025 loss)
I0814 19:56:04.243569 11101 sgd_solver.cpp:136] Iteration 2100, lr = 0.0993438, m = 0.9
I0814 19:56:18.540897 11101 solver.cpp:312] Iteration 2200 (6.99448 iter/s, 14.297s/100 iter), loss = 4.13376
I0814 19:56:18.540925 11101 solver.cpp:334]     Train net output #0: loss = 4.16116 (* 1 = 4.16116 loss)
I0814 19:56:18.540928 11101 sgd_solver.cpp:136] Iteration 2200, lr = 0.0993125, m = 0.9
I0814 19:56:32.756124 11101 solver.cpp:312] Iteration 2300 (7.03491 iter/s, 14.2148s/100 iter), loss = 3.99575
I0814 19:56:32.756158 11101 solver.cpp:334]     Train net output #0: loss = 3.87672 (* 1 = 3.87672 loss)
I0814 19:56:32.756163 11101 sgd_solver.cpp:136] Iteration 2300, lr = 0.0992813, m = 0.9
I0814 19:56:47.092372 11101 solver.cpp:312] Iteration 2400 (6.97552 iter/s, 14.3358s/100 iter), loss = 4.55622
I0814 19:56:47.092612 11101 solver.cpp:334]     Train net output #0: loss = 4.50174 (* 1 = 4.50174 loss)
I0814 19:56:47.092722 11101 sgd_solver.cpp:136] Iteration 2400, lr = 0.09925, m = 0.9
I0814 19:57:01.360221 11101 solver.cpp:312] Iteration 2500 (7.00896 iter/s, 14.2674s/100 iter), loss = 3.88908
I0814 19:57:01.360288 11101 solver.cpp:334]     Train net output #0: loss = 3.90655 (* 1 = 3.90655 loss)
I0814 19:57:01.360306 11101 sgd_solver.cpp:136] Iteration 2500, lr = 0.0992187, m = 0.9
I0814 19:57:15.707649 11101 solver.cpp:312] Iteration 2600 (6.97009 iter/s, 14.347s/100 iter), loss = 4.25698
I0814 19:57:15.707677 11101 solver.cpp:334]     Train net output #0: loss = 4.11926 (* 1 = 4.11926 loss)
I0814 19:57:15.707684 11101 sgd_solver.cpp:136] Iteration 2600, lr = 0.0991875, m = 0.9
I0814 19:57:30.260491 11101 solver.cpp:312] Iteration 2700 (6.8717 iter/s, 14.5524s/100 iter), loss = 3.89511
I0814 19:57:30.260567 11101 solver.cpp:334]     Train net output #0: loss = 4.13145 (* 1 = 4.13145 loss)
I0814 19:57:30.260576 11101 sgd_solver.cpp:136] Iteration 2700, lr = 0.0991563, m = 0.9
I0814 19:57:45.133996 11101 solver.cpp:312] Iteration 2800 (6.72355 iter/s, 14.8731s/100 iter), loss = 3.71563
I0814 19:57:45.134060 11101 solver.cpp:334]     Train net output #0: loss = 3.97598 (* 1 = 3.97598 loss)
I0814 19:57:45.134078 11101 sgd_solver.cpp:136] Iteration 2800, lr = 0.099125, m = 0.9
I0814 19:57:59.488119 11101 solver.cpp:312] Iteration 2900 (6.96684 iter/s, 14.3537s/100 iter), loss = 3.65126
I0814 19:57:59.488157 11101 solver.cpp:334]     Train net output #0: loss = 3.66938 (* 1 = 3.66938 loss)
I0814 19:57:59.488164 11101 sgd_solver.cpp:136] Iteration 2900, lr = 0.0990938, m = 0.9
I0814 19:58:13.914058 11101 solver.cpp:312] Iteration 3000 (6.93215 iter/s, 14.4255s/100 iter), loss = 4.09179
I0814 19:58:13.914119 11101 solver.cpp:334]     Train net output #0: loss = 4.32663 (* 1 = 4.32663 loss)
I0814 19:58:13.914126 11101 sgd_solver.cpp:136] Iteration 3000, lr = 0.0990625, m = 0.9
I0814 19:58:28.403452 11101 solver.cpp:312] Iteration 3100 (6.90179 iter/s, 14.489s/100 iter), loss = 3.84519
I0814 19:58:28.403517 11101 solver.cpp:334]     Train net output #0: loss = 3.39747 (* 1 = 3.39747 loss)
I0814 19:58:28.403540 11101 sgd_solver.cpp:136] Iteration 3100, lr = 0.0990313, m = 0.9
I0814 19:58:42.765802 11101 solver.cpp:312] Iteration 3200 (6.96285 iter/s, 14.3619s/100 iter), loss = 4.02578
I0814 19:58:42.765877 11101 solver.cpp:334]     Train net output #0: loss = 3.89528 (* 1 = 3.89528 loss)
I0814 19:58:42.765894 11101 sgd_solver.cpp:136] Iteration 3200, lr = 0.099, m = 0.9
I0814 19:58:57.261637 11101 solver.cpp:312] Iteration 3300 (6.89873 iter/s, 14.4954s/100 iter), loss = 3.62104
I0814 19:58:57.261716 11101 solver.cpp:334]     Train net output #0: loss = 3.72332 (* 1 = 3.72332 loss)
I0814 19:58:57.261723 11101 sgd_solver.cpp:136] Iteration 3300, lr = 0.0989688, m = 0.9
I0814 19:59:11.876121 11101 solver.cpp:312] Iteration 3400 (6.84272 iter/s, 14.6141s/100 iter), loss = 3.6189
I0814 19:59:11.876191 11101 solver.cpp:334]     Train net output #0: loss = 3.74804 (* 1 = 3.74804 loss)
I0814 19:59:11.876209 11101 sgd_solver.cpp:136] Iteration 3400, lr = 0.0989375, m = 0.9
I0814 19:59:26.337026 11101 solver.cpp:312] Iteration 3500 (6.91539 iter/s, 14.4605s/100 iter), loss = 4.18924
I0814 19:59:26.337051 11101 solver.cpp:334]     Train net output #0: loss = 4.00932 (* 1 = 4.00932 loss)
I0814 19:59:26.337055 11101 sgd_solver.cpp:136] Iteration 3500, lr = 0.0989062, m = 0.9
I0814 19:59:40.735352 11101 solver.cpp:312] Iteration 3600 (6.94545 iter/s, 14.3979s/100 iter), loss = 3.70146
I0814 19:59:40.736860 11101 solver.cpp:334]     Train net output #0: loss = 3.84811 (* 1 = 3.84811 loss)
I0814 19:59:40.736871 11101 sgd_solver.cpp:136] Iteration 3600, lr = 0.098875, m = 0.9
I0814 19:59:55.187444 11101 solver.cpp:312] Iteration 3700 (6.91961 iter/s, 14.4517s/100 iter), loss = 3.63623
I0814 19:59:55.187516 11101 solver.cpp:334]     Train net output #0: loss = 4.21824 (* 1 = 4.21824 loss)
I0814 19:59:55.187523 11101 sgd_solver.cpp:136] Iteration 3700, lr = 0.0988437, m = 0.9
I0814 20:00:09.663643 11101 solver.cpp:312] Iteration 3800 (6.90808 iter/s, 14.4758s/100 iter), loss = 3.70913
I0814 20:00:09.663697 11101 solver.cpp:334]     Train net output #0: loss = 3.79752 (* 1 = 3.79752 loss)
I0814 20:00:09.663712 11101 sgd_solver.cpp:136] Iteration 3800, lr = 0.0988125, m = 0.9
I0814 20:00:24.044116 11101 solver.cpp:312] Iteration 3900 (6.95407 iter/s, 14.3801s/100 iter), loss = 3.66412
I0814 20:00:24.044214 11101 solver.cpp:334]     Train net output #0: loss = 3.93238 (* 1 = 3.93238 loss)
I0814 20:00:24.044227 11101 sgd_solver.cpp:136] Iteration 3900, lr = 0.0987813, m = 0.9
I0814 20:00:38.143069 11101 solver.cpp:509] Iteration 4000, Testing net (#0)
I0814 20:00:59.006120 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.142764
I0814 20:00:59.006176 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.328764
I0814 20:00:59.006182 11101 solver.cpp:594]     Test net output #2: loss = 4.61048 (* 1 = 4.61048 loss)
I0814 20:00:59.006201 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8626s
I0814 20:00:59.171066 11101 solver.cpp:312] Iteration 4000 (2.8469 iter/s, 35.126s/100 iter), loss = 3.28749
I0814 20:00:59.171093 11101 solver.cpp:334]     Train net output #0: loss = 3.60689 (* 1 = 3.60689 loss)
I0814 20:00:59.171097 11101 sgd_solver.cpp:136] Iteration 4000, lr = 0.09875, m = 0.9
I0814 20:01:13.961943 11101 solver.cpp:312] Iteration 4100 (6.76111 iter/s, 14.7905s/100 iter), loss = 3.48087
I0814 20:01:13.961975 11101 solver.cpp:334]     Train net output #0: loss = 3.44812 (* 1 = 3.44812 loss)
I0814 20:01:13.961982 11101 sgd_solver.cpp:136] Iteration 4100, lr = 0.0987188, m = 0.9
I0814 20:01:28.955579 11101 solver.cpp:312] Iteration 4200 (6.66968 iter/s, 14.9932s/100 iter), loss = 3.36651
I0814 20:01:28.955638 11101 solver.cpp:334]     Train net output #0: loss = 3.49122 (* 1 = 3.49122 loss)
I0814 20:01:28.955655 11101 sgd_solver.cpp:136] Iteration 4200, lr = 0.0986875, m = 0.9
I0814 20:01:43.269176 11101 solver.cpp:312] Iteration 4300 (6.98656 iter/s, 14.3132s/100 iter), loss = 3.70387
I0814 20:01:43.269251 11101 solver.cpp:334]     Train net output #0: loss = 3.8746 (* 1 = 3.8746 loss)
I0814 20:01:43.269259 11101 sgd_solver.cpp:136] Iteration 4300, lr = 0.0986563, m = 0.9
I0814 20:01:57.830240 11101 solver.cpp:312] Iteration 4400 (6.86782 iter/s, 14.5607s/100 iter), loss = 3.51958
I0814 20:01:57.830400 11101 solver.cpp:334]     Train net output #0: loss = 3.39853 (* 1 = 3.39853 loss)
I0814 20:01:57.830420 11101 sgd_solver.cpp:136] Iteration 4400, lr = 0.098625, m = 0.9
I0814 20:02:12.166414 11101 solver.cpp:312] Iteration 4500 (6.97556 iter/s, 14.3358s/100 iter), loss = 3.83263
I0814 20:02:12.166486 11101 solver.cpp:334]     Train net output #0: loss = 4.04037 (* 1 = 4.04037 loss)
I0814 20:02:12.166505 11101 sgd_solver.cpp:136] Iteration 4500, lr = 0.0985937, m = 0.9
I0814 20:02:26.324439 11101 solver.cpp:312] Iteration 4600 (7.06333 iter/s, 14.1576s/100 iter), loss = 3.35121
I0814 20:02:26.324511 11101 solver.cpp:334]     Train net output #0: loss = 3.5224 (* 1 = 3.5224 loss)
I0814 20:02:26.324517 11101 sgd_solver.cpp:136] Iteration 4600, lr = 0.0985625, m = 0.9
I0814 20:02:40.811555 11101 solver.cpp:312] Iteration 4700 (6.90288 iter/s, 14.4867s/100 iter), loss = 4.11119
I0814 20:02:40.811584 11101 solver.cpp:334]     Train net output #0: loss = 3.98628 (* 1 = 3.98628 loss)
I0814 20:02:40.811589 11101 sgd_solver.cpp:136] Iteration 4700, lr = 0.0985313, m = 0.9
I0814 20:02:55.022858 11101 solver.cpp:312] Iteration 4800 (7.03685 iter/s, 14.2109s/100 iter), loss = 3.35979
I0814 20:02:55.024149 11101 solver.cpp:334]     Train net output #0: loss = 3.8745 (* 1 = 3.8745 loss)
I0814 20:02:55.024286 11101 sgd_solver.cpp:136] Iteration 4800, lr = 0.0985, m = 0.9
I0814 20:03:09.121628 11101 solver.cpp:312] Iteration 4900 (7.09302 iter/s, 14.0984s/100 iter), loss = 3.58348
I0814 20:03:09.121682 11101 solver.cpp:334]     Train net output #0: loss = 3.5709 (* 1 = 3.5709 loss)
I0814 20:03:09.121690 11101 sgd_solver.cpp:136] Iteration 4900, lr = 0.0984688, m = 0.9
I0814 20:03:23.471585 11101 solver.cpp:312] Iteration 5000 (6.96886 iter/s, 14.3496s/100 iter), loss = 3.34446
I0814 20:03:23.471614 11101 solver.cpp:334]     Train net output #0: loss = 3.8243 (* 1 = 3.8243 loss)
I0814 20:03:23.471621 11101 sgd_solver.cpp:136] Iteration 5000, lr = 0.0984375, m = 0.9
I0814 20:03:37.793817 11101 solver.cpp:312] Iteration 5100 (6.98235 iter/s, 14.3218s/100 iter), loss = 3.26537
I0814 20:03:37.793846 11101 solver.cpp:334]     Train net output #0: loss = 3.2305 (* 1 = 3.2305 loss)
I0814 20:03:37.793854 11101 sgd_solver.cpp:136] Iteration 5100, lr = 0.0984062, m = 0.9
I0814 20:03:52.112758 11101 solver.cpp:312] Iteration 5200 (6.98395 iter/s, 14.3185s/100 iter), loss = 3.41762
I0814 20:03:52.112861 11101 solver.cpp:334]     Train net output #0: loss = 3.14619 (* 1 = 3.14619 loss)
I0814 20:03:52.112870 11101 sgd_solver.cpp:136] Iteration 5200, lr = 0.098375, m = 0.9
I0814 20:04:06.469588 11101 solver.cpp:312] Iteration 5300 (6.96552 iter/s, 14.3564s/100 iter), loss = 3.44822
I0814 20:04:06.469661 11101 solver.cpp:334]     Train net output #0: loss = 3.52712 (* 1 = 3.52712 loss)
I0814 20:04:06.469681 11101 sgd_solver.cpp:136] Iteration 5300, lr = 0.0983438, m = 0.9
I0814 20:04:20.925762 11101 solver.cpp:312] Iteration 5400 (6.91765 iter/s, 14.4558s/100 iter), loss = 3.43875
I0814 20:04:20.925827 11101 solver.cpp:334]     Train net output #0: loss = 3.72278 (* 1 = 3.72278 loss)
I0814 20:04:20.925844 11101 sgd_solver.cpp:136] Iteration 5400, lr = 0.0983125, m = 0.9
I0814 20:04:35.430040 11101 solver.cpp:312] Iteration 5500 (6.89471 iter/s, 14.5039s/100 iter), loss = 3.13784
I0814 20:04:35.430155 11101 solver.cpp:334]     Train net output #0: loss = 3.13304 (* 1 = 3.13304 loss)
I0814 20:04:35.430176 11101 sgd_solver.cpp:136] Iteration 5500, lr = 0.0982813, m = 0.9
I0814 20:04:49.783005 11101 solver.cpp:312] Iteration 5600 (6.9674 iter/s, 14.3526s/100 iter), loss = 3.08506
I0814 20:04:49.783082 11101 solver.cpp:334]     Train net output #0: loss = 3.02922 (* 1 = 3.02922 loss)
I0814 20:04:49.783102 11101 sgd_solver.cpp:136] Iteration 5600, lr = 0.09825, m = 0.9
I0814 20:05:04.113471 11101 solver.cpp:312] Iteration 5700 (6.97834 iter/s, 14.3301s/100 iter), loss = 3.53979
I0814 20:05:04.113499 11101 solver.cpp:334]     Train net output #0: loss = 3.66633 (* 1 = 3.66633 loss)
I0814 20:05:04.113505 11101 sgd_solver.cpp:136] Iteration 5700, lr = 0.0982188, m = 0.9
I0814 20:05:18.354449 11101 solver.cpp:312] Iteration 5800 (7.02219 iter/s, 14.2406s/100 iter), loss = 2.83947
I0814 20:05:18.354538 11101 solver.cpp:334]     Train net output #0: loss = 2.81359 (* 1 = 2.81359 loss)
I0814 20:05:18.354552 11101 sgd_solver.cpp:136] Iteration 5800, lr = 0.0981875, m = 0.9
I0814 20:05:32.717361 11101 solver.cpp:312] Iteration 5900 (6.96257 iter/s, 14.3625s/100 iter), loss = 2.93668
I0814 20:05:32.717430 11101 solver.cpp:334]     Train net output #0: loss = 2.91802 (* 1 = 2.91802 loss)
I0814 20:05:32.717447 11101 sgd_solver.cpp:136] Iteration 5900, lr = 0.0981563, m = 0.9
I0814 20:05:46.922786 11101 solver.cpp:509] Iteration 6000, Testing net (#0)
I0814 20:06:06.213282 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 20:06:07.815920 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.214469
I0814 20:06:07.815945 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.444529
I0814 20:06:07.815953 11101 solver.cpp:594]     Test net output #2: loss = 3.94922 (* 1 = 3.94922 loss)
I0814 20:06:07.816004 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8926s
I0814 20:06:07.980123 11101 solver.cpp:312] Iteration 6000 (2.83593 iter/s, 35.2618s/100 iter), loss = 3.28718
I0814 20:06:07.980159 11101 solver.cpp:334]     Train net output #0: loss = 3.34845 (* 1 = 3.34845 loss)
I0814 20:06:07.980165 11101 sgd_solver.cpp:136] Iteration 6000, lr = 0.098125, m = 0.9
I0814 20:06:22.044206 11101 solver.cpp:312] Iteration 6100 (7.11051 iter/s, 14.0637s/100 iter), loss = 3.4379
I0814 20:06:22.044232 11101 solver.cpp:334]     Train net output #0: loss = 3.35892 (* 1 = 3.35892 loss)
I0814 20:06:22.044239 11101 sgd_solver.cpp:136] Iteration 6100, lr = 0.0980937, m = 0.9
I0814 20:06:36.262253 11101 solver.cpp:312] Iteration 6200 (7.03351 iter/s, 14.2176s/100 iter), loss = 3.74656
I0814 20:06:36.262315 11101 solver.cpp:334]     Train net output #0: loss = 3.36941 (* 1 = 3.36941 loss)
I0814 20:06:36.262322 11101 sgd_solver.cpp:136] Iteration 6200, lr = 0.0980625, m = 0.9
I0814 20:06:50.729405 11101 solver.cpp:312] Iteration 6300 (6.9124 iter/s, 14.4667s/100 iter), loss = 2.91392
I0814 20:06:50.729457 11101 solver.cpp:334]     Train net output #0: loss = 2.74882 (* 1 = 2.74882 loss)
I0814 20:06:50.729471 11101 sgd_solver.cpp:136] Iteration 6300, lr = 0.0980313, m = 0.9
I0814 20:07:05.083365 11101 solver.cpp:312] Iteration 6400 (6.96691 iter/s, 14.3536s/100 iter), loss = 2.77997
I0814 20:07:05.083389 11101 solver.cpp:334]     Train net output #0: loss = 3.16557 (* 1 = 3.16557 loss)
I0814 20:07:05.083395 11101 sgd_solver.cpp:136] Iteration 6400, lr = 0.098, m = 0.9
I0814 20:07:19.425372 11101 solver.cpp:312] Iteration 6500 (6.97272 iter/s, 14.3416s/100 iter), loss = 3.18575
I0814 20:07:19.425436 11101 solver.cpp:334]     Train net output #0: loss = 3.22315 (* 1 = 3.22315 loss)
I0814 20:07:19.425442 11101 sgd_solver.cpp:136] Iteration 6500, lr = 0.0979687, m = 0.9
I0814 20:07:33.796313 11101 solver.cpp:312] Iteration 6600 (6.95868 iter/s, 14.3705s/100 iter), loss = 2.58819
I0814 20:07:33.796336 11101 solver.cpp:334]     Train net output #0: loss = 2.59907 (* 1 = 2.59907 loss)
I0814 20:07:33.796340 11101 sgd_solver.cpp:136] Iteration 6600, lr = 0.0979375, m = 0.9
I0814 20:07:48.225952 11101 solver.cpp:312] Iteration 6700 (6.93038 iter/s, 14.4292s/100 iter), loss = 3.44251
I0814 20:07:48.225985 11101 solver.cpp:334]     Train net output #0: loss = 3.38115 (* 1 = 3.38115 loss)
I0814 20:07:48.225993 11101 sgd_solver.cpp:136] Iteration 6700, lr = 0.0979063, m = 0.9
I0814 20:08:02.749533 11101 solver.cpp:312] Iteration 6800 (6.88555 iter/s, 14.5232s/100 iter), loss = 2.86303
I0814 20:08:02.749594 11101 solver.cpp:334]     Train net output #0: loss = 3.14224 (* 1 = 3.14224 loss)
I0814 20:08:02.749600 11101 sgd_solver.cpp:136] Iteration 6800, lr = 0.097875, m = 0.9
I0814 20:08:17.522527 11101 solver.cpp:312] Iteration 6900 (6.7693 iter/s, 14.7726s/100 iter), loss = 3.07739
I0814 20:08:17.522581 11101 solver.cpp:334]     Train net output #0: loss = 3.07038 (* 1 = 3.07038 loss)
I0814 20:08:17.522600 11101 sgd_solver.cpp:136] Iteration 6900, lr = 0.0978438, m = 0.9
I0814 20:08:32.008388 11101 solver.cpp:312] Iteration 7000 (6.90348 iter/s, 14.4855s/100 iter), loss = 2.71806
I0814 20:08:32.008421 11101 solver.cpp:334]     Train net output #0: loss = 2.60467 (* 1 = 2.60467 loss)
I0814 20:08:32.008430 11101 sgd_solver.cpp:136] Iteration 7000, lr = 0.0978125, m = 0.9
I0814 20:08:46.620518 11101 solver.cpp:312] Iteration 7100 (6.84382 iter/s, 14.6117s/100 iter), loss = 3.25137
I0814 20:08:46.620640 11101 solver.cpp:334]     Train net output #0: loss = 3.43255 (* 1 = 3.43255 loss)
I0814 20:08:46.620659 11101 sgd_solver.cpp:136] Iteration 7100, lr = 0.0977813, m = 0.9
I0814 20:09:00.978099 11101 solver.cpp:312] Iteration 7200 (6.96516 iter/s, 14.3572s/100 iter), loss = 3.34462
I0814 20:09:00.978127 11101 solver.cpp:334]     Train net output #0: loss = 3.41831 (* 1 = 3.41831 loss)
I0814 20:09:00.978134 11101 sgd_solver.cpp:136] Iteration 7200, lr = 0.09775, m = 0.9
I0814 20:09:15.513984 11101 solver.cpp:312] Iteration 7300 (6.87972 iter/s, 14.5355s/100 iter), loss = 3.06876
I0814 20:09:15.514161 11101 solver.cpp:334]     Train net output #0: loss = 3.2347 (* 1 = 3.2347 loss)
I0814 20:09:15.514178 11101 sgd_solver.cpp:136] Iteration 7300, lr = 0.0977188, m = 0.9
I0814 20:09:30.050639 11101 solver.cpp:312] Iteration 7400 (6.87935 iter/s, 14.5363s/100 iter), loss = 3.30489
I0814 20:09:30.050688 11101 solver.cpp:334]     Train net output #0: loss = 3.26675 (* 1 = 3.26675 loss)
I0814 20:09:30.050693 11101 sgd_solver.cpp:136] Iteration 7400, lr = 0.0976875, m = 0.9
I0814 20:09:44.191857 11101 solver.cpp:312] Iteration 7500 (7.07173 iter/s, 14.1408s/100 iter), loss = 3.48871
I0814 20:09:44.192068 11101 solver.cpp:334]     Train net output #0: loss = 3.8682 (* 1 = 3.8682 loss)
I0814 20:09:44.192185 11101 sgd_solver.cpp:136] Iteration 7500, lr = 0.0976562, m = 0.9
I0814 20:09:58.865281 11101 solver.cpp:312] Iteration 7600 (6.81523 iter/s, 14.673s/100 iter), loss = 2.40562
I0814 20:09:58.865332 11101 solver.cpp:334]     Train net output #0: loss = 2.13656 (* 1 = 2.13656 loss)
I0814 20:09:58.865345 11101 sgd_solver.cpp:136] Iteration 7600, lr = 0.097625, m = 0.9
I0814 20:10:13.770730 11101 solver.cpp:312] Iteration 7700 (6.70914 iter/s, 14.905s/100 iter), loss = 2.86444
I0814 20:10:13.770789 11101 solver.cpp:334]     Train net output #0: loss = 2.85215 (* 1 = 2.85215 loss)
I0814 20:10:13.770797 11101 sgd_solver.cpp:136] Iteration 7700, lr = 0.0975937, m = 0.9
I0814 20:10:28.212890 11101 solver.cpp:312] Iteration 7800 (6.92436 iter/s, 14.4418s/100 iter), loss = 2.90918
I0814 20:10:28.212955 11101 solver.cpp:334]     Train net output #0: loss = 3.0859 (* 1 = 3.0859 loss)
I0814 20:10:28.212973 11101 sgd_solver.cpp:136] Iteration 7800, lr = 0.0975625, m = 0.9
I0814 20:10:43.232110 11101 solver.cpp:312] Iteration 7900 (6.65832 iter/s, 15.0188s/100 iter), loss = 2.33298
I0814 20:10:43.232158 11101 solver.cpp:334]     Train net output #0: loss = 2.62511 (* 1 = 2.62511 loss)
I0814 20:10:43.232180 11101 sgd_solver.cpp:136] Iteration 7900, lr = 0.0975313, m = 0.9
I0814 20:10:57.670254 11101 solver.cpp:509] Iteration 8000, Testing net (#0)
I0814 20:11:18.596827 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.259292
I0814 20:11:18.596846 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.506118
I0814 20:11:18.596853 11101 solver.cpp:594]     Test net output #2: loss = 3.64902 (* 1 = 3.64902 loss)
I0814 20:11:18.596884 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9261s
I0814 20:11:18.771391 11101 solver.cpp:312] Iteration 8000 (2.81387 iter/s, 35.5383s/100 iter), loss = 2.92151
I0814 20:11:18.771420 11101 solver.cpp:334]     Train net output #0: loss = 2.73549 (* 1 = 2.73549 loss)
I0814 20:11:18.771425 11101 sgd_solver.cpp:136] Iteration 8000, lr = 0.0975, m = 0.9
I0814 20:11:33.032775 11101 solver.cpp:312] Iteration 8100 (7.01214 iter/s, 14.261s/100 iter), loss = 3.22825
I0814 20:11:33.032853 11101 solver.cpp:334]     Train net output #0: loss = 3.20199 (* 1 = 3.20199 loss)
I0814 20:11:33.032874 11101 sgd_solver.cpp:136] Iteration 8100, lr = 0.0974688, m = 0.9
I0814 20:11:47.773720 11101 solver.cpp:312] Iteration 8200 (6.78402 iter/s, 14.7405s/100 iter), loss = 2.91538
I0814 20:11:47.773751 11101 solver.cpp:334]     Train net output #0: loss = 2.87252 (* 1 = 2.87252 loss)
I0814 20:11:47.773756 11101 sgd_solver.cpp:136] Iteration 8200, lr = 0.0974375, m = 0.9
I0814 20:12:02.677253 11101 solver.cpp:312] Iteration 8300 (6.71 iter/s, 14.9031s/100 iter), loss = 2.48461
I0814 20:12:02.677310 11101 solver.cpp:334]     Train net output #0: loss = 2.68731 (* 1 = 2.68731 loss)
I0814 20:12:02.677325 11101 sgd_solver.cpp:136] Iteration 8300, lr = 0.0974063, m = 0.9
I0814 20:12:17.051565 11101 solver.cpp:312] Iteration 8400 (6.95705 iter/s, 14.3739s/100 iter), loss = 2.84083
I0814 20:12:17.051615 11101 solver.cpp:334]     Train net output #0: loss = 2.99717 (* 1 = 2.99717 loss)
I0814 20:12:17.051621 11101 sgd_solver.cpp:136] Iteration 8400, lr = 0.097375, m = 0.9
I0814 20:12:31.557746 11101 solver.cpp:312] Iteration 8500 (6.8938 iter/s, 14.5058s/100 iter), loss = 2.90327
I0814 20:12:31.557775 11101 solver.cpp:334]     Train net output #0: loss = 3.2111 (* 1 = 3.2111 loss)
I0814 20:12:31.557780 11101 sgd_solver.cpp:136] Iteration 8500, lr = 0.0973438, m = 0.9
I0814 20:12:46.051158 11101 solver.cpp:312] Iteration 8600 (6.89988 iter/s, 14.493s/100 iter), loss = 2.97767
I0814 20:12:46.051182 11101 solver.cpp:334]     Train net output #0: loss = 2.98417 (* 1 = 2.98417 loss)
I0814 20:12:46.051189 11101 sgd_solver.cpp:136] Iteration 8600, lr = 0.0973125, m = 0.9
I0814 20:13:00.647864 11101 solver.cpp:312] Iteration 8700 (6.85105 iter/s, 14.5963s/100 iter), loss = 2.56779
I0814 20:13:00.648123 11101 solver.cpp:334]     Train net output #0: loss = 2.44875 (* 1 = 2.44875 loss)
I0814 20:13:00.648150 11101 sgd_solver.cpp:136] Iteration 8700, lr = 0.0972812, m = 0.9
I0814 20:13:15.082083 11101 solver.cpp:312] Iteration 8800 (6.92817 iter/s, 14.4338s/100 iter), loss = 2.64692
I0814 20:13:15.082108 11101 solver.cpp:334]     Train net output #0: loss = 3.00862 (* 1 = 3.00862 loss)
I0814 20:13:15.082114 11101 sgd_solver.cpp:136] Iteration 8800, lr = 0.09725, m = 0.9
I0814 20:13:29.583634 11101 solver.cpp:312] Iteration 8900 (6.89601 iter/s, 14.5011s/100 iter), loss = 2.97393
I0814 20:13:29.583662 11101 solver.cpp:334]     Train net output #0: loss = 2.86702 (* 1 = 2.86702 loss)
I0814 20:13:29.583667 11101 sgd_solver.cpp:136] Iteration 8900, lr = 0.0972188, m = 0.9
I0814 20:13:44.156332 11101 solver.cpp:312] Iteration 9000 (6.86234 iter/s, 14.5723s/100 iter), loss = 2.84935
I0814 20:13:44.156428 11101 solver.cpp:334]     Train net output #0: loss = 2.79775 (* 1 = 2.79775 loss)
I0814 20:13:44.156448 11101 sgd_solver.cpp:136] Iteration 9000, lr = 0.0971875, m = 0.9
I0814 20:13:58.636716 11101 solver.cpp:312] Iteration 9100 (6.90609 iter/s, 14.48s/100 iter), loss = 2.61124
I0814 20:13:58.636750 11101 solver.cpp:334]     Train net output #0: loss = 2.28281 (* 1 = 2.28281 loss)
I0814 20:13:58.636756 11101 sgd_solver.cpp:136] Iteration 9100, lr = 0.0971562, m = 0.9
I0814 20:14:13.258815 11101 solver.cpp:312] Iteration 9200 (6.83915 iter/s, 14.6217s/100 iter), loss = 2.67034
I0814 20:14:13.258842 11101 solver.cpp:334]     Train net output #0: loss = 3.14074 (* 1 = 3.14074 loss)
I0814 20:14:13.258846 11101 sgd_solver.cpp:136] Iteration 9200, lr = 0.097125, m = 0.9
I0814 20:14:27.950783 11101 solver.cpp:312] Iteration 9300 (6.80663 iter/s, 14.6916s/100 iter), loss = 3.41587
I0814 20:14:27.950850 11101 solver.cpp:334]     Train net output #0: loss = 3.54736 (* 1 = 3.54736 loss)
I0814 20:14:27.950857 11101 sgd_solver.cpp:136] Iteration 9300, lr = 0.0970938, m = 0.9
I0814 20:14:42.517529 11101 solver.cpp:312] Iteration 9400 (6.86514 iter/s, 14.5663s/100 iter), loss = 2.85015
I0814 20:14:42.517575 11101 solver.cpp:334]     Train net output #0: loss = 2.52426 (* 1 = 2.52426 loss)
I0814 20:14:42.517588 11101 sgd_solver.cpp:136] Iteration 9400, lr = 0.0970625, m = 0.9
I0814 20:14:56.844861 11101 solver.cpp:312] Iteration 9500 (6.97986 iter/s, 14.3269s/100 iter), loss = 2.95367
I0814 20:14:56.844921 11101 solver.cpp:334]     Train net output #0: loss = 2.98778 (* 1 = 2.98778 loss)
I0814 20:14:56.844944 11101 sgd_solver.cpp:136] Iteration 9500, lr = 0.0970313, m = 0.9
I0814 20:15:11.219781 11101 solver.cpp:312] Iteration 9600 (6.95675 iter/s, 14.3745s/100 iter), loss = 3.08284
I0814 20:15:11.220383 11101 solver.cpp:334]     Train net output #0: loss = 3.11859 (* 1 = 3.11859 loss)
I0814 20:15:11.220415 11101 sgd_solver.cpp:136] Iteration 9600, lr = 0.097, m = 0.9
I0814 20:15:25.721948 11101 solver.cpp:312] Iteration 9700 (6.89571 iter/s, 14.5018s/100 iter), loss = 2.99675
I0814 20:15:25.722010 11101 solver.cpp:334]     Train net output #0: loss = 3.28172 (* 1 = 3.28172 loss)
I0814 20:15:25.722026 11101 sgd_solver.cpp:136] Iteration 9700, lr = 0.0969688, m = 0.9
I0814 20:15:40.154709 11101 solver.cpp:312] Iteration 9800 (6.92887 iter/s, 14.4324s/100 iter), loss = 2.82806
I0814 20:15:40.154742 11101 solver.cpp:334]     Train net output #0: loss = 2.7896 (* 1 = 2.7896 loss)
I0814 20:15:40.154748 11101 sgd_solver.cpp:136] Iteration 9800, lr = 0.0969375, m = 0.9
I0814 20:15:54.345674 11101 solver.cpp:312] Iteration 9900 (7.04693 iter/s, 14.1906s/100 iter), loss = 2.5733
I0814 20:15:54.345762 11101 solver.cpp:334]     Train net output #0: loss = 2.39685 (* 1 = 2.39685 loss)
I0814 20:15:54.345774 11101 sgd_solver.cpp:136] Iteration 9900, lr = 0.0969063, m = 0.9
I0814 20:16:08.633189 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_10000.caffemodel
I0814 20:16:08.657944 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_10000.solverstate
I0814 20:16:08.664366 11101 solver.cpp:509] Iteration 10000, Testing net (#0)
I0814 20:16:29.229581 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.307705
I0814 20:16:29.229655 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.551648
I0814 20:16:29.229665 11101 solver.cpp:594]     Test net output #2: loss = 3.36732 (* 1 = 3.36732 loss)
I0814 20:16:29.229686 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.5648s
I0814 20:16:29.372922 11101 solver.cpp:312] Iteration 10000 (2.855 iter/s, 35.0263s/100 iter), loss = 3.04758
I0814 20:16:29.372952 11101 solver.cpp:334]     Train net output #0: loss = 2.85054 (* 1 = 2.85054 loss)
I0814 20:16:29.372958 11101 sgd_solver.cpp:136] Iteration 10000, lr = 0.096875, m = 0.9
I0814 20:16:44.249001 11101 solver.cpp:312] Iteration 10100 (6.72239 iter/s, 14.8757s/100 iter), loss = 2.50417
I0814 20:16:44.249024 11101 solver.cpp:334]     Train net output #0: loss = 2.71483 (* 1 = 2.71483 loss)
I0814 20:16:44.249028 11101 sgd_solver.cpp:136] Iteration 10100, lr = 0.0968437, m = 0.9
I0814 20:16:58.747984 11101 solver.cpp:312] Iteration 10200 (6.89723 iter/s, 14.4986s/100 iter), loss = 2.55973
I0814 20:16:58.748054 11101 solver.cpp:334]     Train net output #0: loss = 2.38345 (* 1 = 2.38345 loss)
I0814 20:16:58.748075 11101 sgd_solver.cpp:136] Iteration 10200, lr = 0.0968125, m = 0.9
I0814 20:17:13.141160 11101 solver.cpp:312] Iteration 10300 (6.94793 iter/s, 14.3928s/100 iter), loss = 2.66691
I0814 20:17:13.141216 11101 solver.cpp:334]     Train net output #0: loss = 3.06412 (* 1 = 3.06412 loss)
I0814 20:17:13.141222 11101 sgd_solver.cpp:136] Iteration 10300, lr = 0.0967812, m = 0.9
I0814 20:17:27.468575 11101 solver.cpp:312] Iteration 10400 (6.97982 iter/s, 14.327s/100 iter), loss = 2.55674
I0814 20:17:27.468600 11101 solver.cpp:334]     Train net output #0: loss = 2.11456 (* 1 = 2.11456 loss)
I0814 20:17:27.468605 11101 sgd_solver.cpp:136] Iteration 10400, lr = 0.09675, m = 0.9
I0814 20:17:41.785290 11101 solver.cpp:312] Iteration 10500 (6.98504 iter/s, 14.3163s/100 iter), loss = 2.42258
I0814 20:17:41.785356 11101 solver.cpp:334]     Train net output #0: loss = 2.14091 (* 1 = 2.14091 loss)
I0814 20:17:41.785373 11101 sgd_solver.cpp:136] Iteration 10500, lr = 0.0967188, m = 0.9
I0814 20:17:56.457305 11101 solver.cpp:312] Iteration 10600 (6.81589 iter/s, 14.6716s/100 iter), loss = 3.30439
I0814 20:17:56.457470 11101 solver.cpp:334]     Train net output #0: loss = 3.48936 (* 1 = 3.48936 loss)
I0814 20:17:56.457478 11101 sgd_solver.cpp:136] Iteration 10600, lr = 0.0966875, m = 0.9
I0814 20:18:11.269722 11101 solver.cpp:312] Iteration 10700 (6.75128 iter/s, 14.812s/100 iter), loss = 2.84647
I0814 20:18:11.269754 11101 solver.cpp:334]     Train net output #0: loss = 2.3997 (* 1 = 2.3997 loss)
I0814 20:18:11.269762 11101 sgd_solver.cpp:136] Iteration 10700, lr = 0.0966563, m = 0.9
I0814 20:18:25.626579 11101 solver.cpp:312] Iteration 10800 (6.9655 iter/s, 14.3565s/100 iter), loss = 2.65957
I0814 20:18:25.626755 11101 solver.cpp:334]     Train net output #0: loss = 2.48859 (* 1 = 2.48859 loss)
I0814 20:18:25.626837 11101 sgd_solver.cpp:136] Iteration 10800, lr = 0.096625, m = 0.9
I0814 20:18:40.090991 11101 solver.cpp:312] Iteration 10900 (6.91371 iter/s, 14.464s/100 iter), loss = 2.57573
I0814 20:18:40.091068 11101 solver.cpp:334]     Train net output #0: loss = 2.78843 (* 1 = 2.78843 loss)
I0814 20:18:40.091079 11101 sgd_solver.cpp:136] Iteration 10900, lr = 0.0965938, m = 0.9
I0814 20:18:54.278322 11101 solver.cpp:312] Iteration 11000 (7.04874 iter/s, 14.1869s/100 iter), loss = 2.54948
I0814 20:18:54.278352 11101 solver.cpp:334]     Train net output #0: loss = 2.47835 (* 1 = 2.47835 loss)
I0814 20:18:54.278357 11101 sgd_solver.cpp:136] Iteration 11000, lr = 0.0965625, m = 0.9
I0814 20:19:08.844415 11101 solver.cpp:312] Iteration 11100 (6.86545 iter/s, 14.5657s/100 iter), loss = 2.77768
I0814 20:19:08.844437 11101 solver.cpp:334]     Train net output #0: loss = 2.76999 (* 1 = 2.76999 loss)
I0814 20:19:08.844442 11101 sgd_solver.cpp:136] Iteration 11100, lr = 0.0965312, m = 0.9
I0814 20:19:23.150856 11101 solver.cpp:312] Iteration 11200 (6.99005 iter/s, 14.306s/100 iter), loss = 2.52758
I0814 20:19:23.150955 11101 solver.cpp:334]     Train net output #0: loss = 2.36151 (* 1 = 2.36151 loss)
I0814 20:19:23.150972 11101 sgd_solver.cpp:136] Iteration 11200, lr = 0.0965, m = 0.9
I0814 20:19:37.623456 11101 solver.cpp:312] Iteration 11300 (6.9098 iter/s, 14.4722s/100 iter), loss = 2.68284
I0814 20:19:37.623632 11101 solver.cpp:334]     Train net output #0: loss = 2.52857 (* 1 = 2.52857 loss)
I0814 20:19:37.623720 11101 sgd_solver.cpp:136] Iteration 11300, lr = 0.0964688, m = 0.9
I0814 20:19:52.105253 11101 solver.cpp:312] Iteration 11400 (6.90541 iter/s, 14.4814s/100 iter), loss = 2.88451
I0814 20:19:52.105285 11101 solver.cpp:334]     Train net output #0: loss = 2.90858 (* 1 = 2.90858 loss)
I0814 20:19:52.105291 11101 sgd_solver.cpp:136] Iteration 11400, lr = 0.0964375, m = 0.9
I0814 20:20:06.517138 11101 solver.cpp:312] Iteration 11500 (6.93891 iter/s, 14.4115s/100 iter), loss = 3.08571
I0814 20:20:06.517393 11101 solver.cpp:334]     Train net output #0: loss = 2.96141 (* 1 = 2.96141 loss)
I0814 20:20:06.517413 11101 sgd_solver.cpp:136] Iteration 11500, lr = 0.0964063, m = 0.9
I0814 20:20:21.056398 11101 solver.cpp:312] Iteration 11600 (6.87812 iter/s, 14.5389s/100 iter), loss = 2.39687
I0814 20:20:21.056424 11101 solver.cpp:334]     Train net output #0: loss = 2.37082 (* 1 = 2.37082 loss)
I0814 20:20:21.056430 11101 sgd_solver.cpp:136] Iteration 11600, lr = 0.096375, m = 0.9
I0814 20:20:35.360209 11101 solver.cpp:312] Iteration 11700 (6.99134 iter/s, 14.3034s/100 iter), loss = 2.80384
I0814 20:20:35.360239 11101 solver.cpp:334]     Train net output #0: loss = 3.03676 (* 1 = 3.03676 loss)
I0814 20:20:35.360245 11101 sgd_solver.cpp:136] Iteration 11700, lr = 0.0963437, m = 0.9
I0814 20:20:49.979427 11101 solver.cpp:312] Iteration 11800 (6.8405 iter/s, 14.6188s/100 iter), loss = 2.63692
I0814 20:20:49.979499 11101 solver.cpp:334]     Train net output #0: loss = 2.18941 (* 1 = 2.18941 loss)
I0814 20:20:49.979506 11101 sgd_solver.cpp:136] Iteration 11800, lr = 0.0963125, m = 0.9
I0814 20:21:04.370811 11101 solver.cpp:312] Iteration 11900 (6.94879 iter/s, 14.391s/100 iter), loss = 3.04524
I0814 20:21:04.370841 11101 solver.cpp:334]     Train net output #0: loss = 3.36125 (* 1 = 3.36125 loss)
I0814 20:21:04.370846 11101 sgd_solver.cpp:136] Iteration 11900, lr = 0.0962813, m = 0.9
I0814 20:21:18.729374 11101 solver.cpp:509] Iteration 12000, Testing net (#0)
I0814 20:21:35.063459 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 20:21:39.494304 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.270704
I0814 20:21:39.494328 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.509883
I0814 20:21:39.494333 11101 solver.cpp:594]     Test net output #2: loss = 3.61821 (* 1 = 3.61821 loss)
I0814 20:21:39.494355 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7644s
I0814 20:21:39.656226 11101 solver.cpp:312] Iteration 12000 (2.83411 iter/s, 35.2845s/100 iter), loss = 2.41053
I0814 20:21:39.656275 11101 solver.cpp:334]     Train net output #0: loss = 2.34137 (* 1 = 2.34137 loss)
I0814 20:21:39.656288 11101 sgd_solver.cpp:136] Iteration 12000, lr = 0.09625, m = 0.9
I0814 20:21:54.138496 11101 solver.cpp:312] Iteration 12100 (6.90519 iter/s, 14.4819s/100 iter), loss = 2.97897
I0814 20:21:54.138551 11101 solver.cpp:334]     Train net output #0: loss = 2.79554 (* 1 = 2.79554 loss)
I0814 20:21:54.138564 11101 sgd_solver.cpp:136] Iteration 12100, lr = 0.0962188, m = 0.9
I0814 20:22:08.590294 11101 solver.cpp:312] Iteration 12200 (6.91975 iter/s, 14.4514s/100 iter), loss = 2.66028
I0814 20:22:08.590394 11101 solver.cpp:334]     Train net output #0: loss = 2.3311 (* 1 = 2.3311 loss)
I0814 20:22:08.590411 11101 sgd_solver.cpp:136] Iteration 12200, lr = 0.0961875, m = 0.9
I0814 20:22:22.837352 11101 solver.cpp:312] Iteration 12300 (7.01919 iter/s, 14.2467s/100 iter), loss = 2.38104
I0814 20:22:22.837416 11101 solver.cpp:334]     Train net output #0: loss = 2.42414 (* 1 = 2.42414 loss)
I0814 20:22:22.837433 11101 sgd_solver.cpp:136] Iteration 12300, lr = 0.0961563, m = 0.9
I0814 20:22:37.240556 11101 solver.cpp:312] Iteration 12400 (6.94309 iter/s, 14.4028s/100 iter), loss = 2.56347
I0814 20:22:37.240737 11101 solver.cpp:334]     Train net output #0: loss = 2.43688 (* 1 = 2.43688 loss)
I0814 20:22:37.240823 11101 sgd_solver.cpp:136] Iteration 12400, lr = 0.096125, m = 0.9
I0814 20:22:51.495420 11101 solver.cpp:312] Iteration 12500 (7.01534 iter/s, 14.2545s/100 iter), loss = 2.67334
I0814 20:22:51.495477 11101 solver.cpp:334]     Train net output #0: loss = 2.70689 (* 1 = 2.70689 loss)
I0814 20:22:51.495484 11101 sgd_solver.cpp:136] Iteration 12500, lr = 0.0960938, m = 0.9
I0814 20:23:05.953096 11101 solver.cpp:312] Iteration 12600 (6.91693 iter/s, 14.4573s/100 iter), loss = 3.43194
I0814 20:23:05.953158 11101 solver.cpp:334]     Train net output #0: loss = 3.08251 (* 1 = 3.08251 loss)
I0814 20:23:05.953166 11101 sgd_solver.cpp:136] Iteration 12600, lr = 0.0960625, m = 0.9
I0814 20:23:20.408821 11101 solver.cpp:312] Iteration 12700 (6.91786 iter/s, 14.4553s/100 iter), loss = 2.48226
I0814 20:23:20.408870 11101 solver.cpp:334]     Train net output #0: loss = 2.30474 (* 1 = 2.30474 loss)
I0814 20:23:20.408885 11101 sgd_solver.cpp:136] Iteration 12700, lr = 0.0960312, m = 0.9
I0814 20:23:35.300680 11101 solver.cpp:312] Iteration 12800 (6.71526 iter/s, 14.8914s/100 iter), loss = 2.94889
I0814 20:23:35.300734 11101 solver.cpp:334]     Train net output #0: loss = 3.18095 (* 1 = 3.18095 loss)
I0814 20:23:35.300740 11101 sgd_solver.cpp:136] Iteration 12800, lr = 0.096, m = 0.9
I0814 20:23:49.882968 11101 solver.cpp:312] Iteration 12900 (6.85782 iter/s, 14.5819s/100 iter), loss = 2.11695
I0814 20:23:49.882997 11101 solver.cpp:334]     Train net output #0: loss = 2.3373 (* 1 = 2.3373 loss)
I0814 20:23:49.883036 11101 sgd_solver.cpp:136] Iteration 12900, lr = 0.0959687, m = 0.9
I0814 20:24:04.766296 11101 solver.cpp:312] Iteration 13000 (6.71911 iter/s, 14.8829s/100 iter), loss = 2.86005
I0814 20:24:04.766378 11101 solver.cpp:334]     Train net output #0: loss = 2.97985 (* 1 = 2.97985 loss)
I0814 20:24:04.766399 11101 sgd_solver.cpp:136] Iteration 13000, lr = 0.0959375, m = 0.9
I0814 20:24:19.048583 11101 solver.cpp:312] Iteration 13100 (7.00187 iter/s, 14.2819s/100 iter), loss = 3.05884
I0814 20:24:19.048660 11101 solver.cpp:334]     Train net output #0: loss = 2.95322 (* 1 = 2.95322 loss)
I0814 20:24:19.048666 11101 sgd_solver.cpp:136] Iteration 13100, lr = 0.0959063, m = 0.9
I0814 20:24:33.425796 11101 solver.cpp:312] Iteration 13200 (6.95564 iter/s, 14.3768s/100 iter), loss = 3.09338
I0814 20:24:33.425825 11101 solver.cpp:334]     Train net output #0: loss = 3.11061 (* 1 = 3.11061 loss)
I0814 20:24:33.425832 11101 sgd_solver.cpp:136] Iteration 13200, lr = 0.095875, m = 0.9
I0814 20:24:48.195221 11101 solver.cpp:312] Iteration 13300 (6.77093 iter/s, 14.769s/100 iter), loss = 2.53804
I0814 20:24:48.195250 11101 solver.cpp:334]     Train net output #0: loss = 2.28895 (* 1 = 2.28895 loss)
I0814 20:24:48.195255 11101 sgd_solver.cpp:136] Iteration 13300, lr = 0.0958438, m = 0.9
I0814 20:25:02.533880 11101 solver.cpp:312] Iteration 13400 (6.97434 iter/s, 14.3383s/100 iter), loss = 2.65755
I0814 20:25:02.533958 11101 solver.cpp:334]     Train net output #0: loss = 2.60439 (* 1 = 2.60439 loss)
I0814 20:25:02.533965 11101 sgd_solver.cpp:136] Iteration 13400, lr = 0.0958125, m = 0.9
I0814 20:25:16.863914 11101 solver.cpp:312] Iteration 13500 (6.97854 iter/s, 14.3296s/100 iter), loss = 2.58508
I0814 20:25:16.863978 11101 solver.cpp:334]     Train net output #0: loss = 2.84429 (* 1 = 2.84429 loss)
I0814 20:25:16.863996 11101 sgd_solver.cpp:136] Iteration 13500, lr = 0.0957813, m = 0.9
I0814 20:25:31.296049 11101 solver.cpp:312] Iteration 13600 (6.92917 iter/s, 14.4317s/100 iter), loss = 2.60731
I0814 20:25:31.296075 11101 solver.cpp:334]     Train net output #0: loss = 2.62106 (* 1 = 2.62106 loss)
I0814 20:25:31.296115 11101 sgd_solver.cpp:136] Iteration 13600, lr = 0.09575, m = 0.9
I0814 20:25:45.739078 11101 solver.cpp:312] Iteration 13700 (6.92395 iter/s, 14.4426s/100 iter), loss = 2.35321
I0814 20:25:45.739171 11101 solver.cpp:334]     Train net output #0: loss = 2.48107 (* 1 = 2.48107 loss)
I0814 20:25:45.739183 11101 sgd_solver.cpp:136] Iteration 13700, lr = 0.0957187, m = 0.9
I0814 20:26:00.306171 11101 solver.cpp:312] Iteration 13800 (6.86498 iter/s, 14.5667s/100 iter), loss = 2.25222
I0814 20:26:00.306207 11101 solver.cpp:334]     Train net output #0: loss = 2.09701 (* 1 = 2.09701 loss)
I0814 20:26:00.306213 11101 sgd_solver.cpp:136] Iteration 13800, lr = 0.0956875, m = 0.9
I0814 20:26:14.778131 11101 solver.cpp:312] Iteration 13900 (6.9101 iter/s, 14.4716s/100 iter), loss = 2.46026
I0814 20:26:14.778195 11101 solver.cpp:334]     Train net output #0: loss = 2.69088 (* 1 = 2.69088 loss)
I0814 20:26:14.778213 11101 sgd_solver.cpp:136] Iteration 13900, lr = 0.0956563, m = 0.9
I0814 20:26:28.986754 11101 solver.cpp:509] Iteration 14000, Testing net (#0)
I0814 20:26:49.818038 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.315411
I0814 20:26:49.818063 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.575588
I0814 20:26:49.818070 11101 solver.cpp:594]     Test net output #2: loss = 3.30198 (* 1 = 3.30198 loss)
I0814 20:26:49.818092 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8308s
I0814 20:26:49.993108 11101 solver.cpp:312] Iteration 14000 (2.83978 iter/s, 35.214s/100 iter), loss = 2.63051
I0814 20:26:49.993181 11101 solver.cpp:334]     Train net output #0: loss = 2.80976 (* 1 = 2.80976 loss)
I0814 20:26:49.993201 11101 sgd_solver.cpp:136] Iteration 14000, lr = 0.095625, m = 0.9
I0814 20:27:04.599799 11101 solver.cpp:312] Iteration 14100 (6.84637 iter/s, 14.6063s/100 iter), loss = 2.94471
I0814 20:27:04.600044 11101 solver.cpp:334]     Train net output #0: loss = 3.31381 (* 1 = 3.31381 loss)
I0814 20:27:04.600159 11101 sgd_solver.cpp:136] Iteration 14100, lr = 0.0955938, m = 0.9
I0814 20:27:18.869611 11101 solver.cpp:312] Iteration 14200 (7.00799 iter/s, 14.2694s/100 iter), loss = 2.80655
I0814 20:27:18.869637 11101 solver.cpp:334]     Train net output #0: loss = 3.04456 (* 1 = 3.04456 loss)
I0814 20:27:18.869642 11101 sgd_solver.cpp:136] Iteration 14200, lr = 0.0955625, m = 0.9
I0814 20:27:33.163257 11101 solver.cpp:312] Iteration 14300 (6.99631 iter/s, 14.2933s/100 iter), loss = 2.9452
I0814 20:27:33.163285 11101 solver.cpp:334]     Train net output #0: loss = 2.76504 (* 1 = 2.76504 loss)
I0814 20:27:33.163291 11101 sgd_solver.cpp:136] Iteration 14300, lr = 0.0955312, m = 0.9
I0814 20:27:47.474799 11101 solver.cpp:312] Iteration 14400 (6.98756 iter/s, 14.3111s/100 iter), loss = 2.53262
I0814 20:27:47.474905 11101 solver.cpp:334]     Train net output #0: loss = 2.51254 (* 1 = 2.51254 loss)
I0814 20:27:47.474923 11101 sgd_solver.cpp:136] Iteration 14400, lr = 0.0955, m = 0.9
I0814 20:28:02.114202 11101 solver.cpp:312] Iteration 14500 (6.83107 iter/s, 14.639s/100 iter), loss = 2.68381
I0814 20:28:02.114230 11101 solver.cpp:334]     Train net output #0: loss = 2.70019 (* 1 = 2.70019 loss)
I0814 20:28:02.114235 11101 sgd_solver.cpp:136] Iteration 14500, lr = 0.0954688, m = 0.9
I0814 20:28:16.631356 11101 solver.cpp:312] Iteration 14600 (6.8886 iter/s, 14.5167s/100 iter), loss = 2.48962
I0814 20:28:16.631386 11101 solver.cpp:334]     Train net output #0: loss = 2.23263 (* 1 = 2.23263 loss)
I0814 20:28:16.631392 11101 sgd_solver.cpp:136] Iteration 14600, lr = 0.0954375, m = 0.9
I0814 20:28:31.194228 11101 solver.cpp:312] Iteration 14700 (6.86698 iter/s, 14.5624s/100 iter), loss = 2.83363
I0814 20:28:31.196215 11101 solver.cpp:334]     Train net output #0: loss = 2.7932 (* 1 = 2.7932 loss)
I0814 20:28:31.196230 11101 sgd_solver.cpp:136] Iteration 14700, lr = 0.0954063, m = 0.9
I0814 20:28:45.518493 11101 solver.cpp:312] Iteration 14800 (6.98136 iter/s, 14.3238s/100 iter), loss = 2.46612
I0814 20:28:45.518518 11101 solver.cpp:334]     Train net output #0: loss = 2.69185 (* 1 = 2.69185 loss)
I0814 20:28:45.518522 11101 sgd_solver.cpp:136] Iteration 14800, lr = 0.095375, m = 0.9
I0814 20:28:59.793857 11101 solver.cpp:312] Iteration 14900 (7.00528 iter/s, 14.275s/100 iter), loss = 2.45587
I0814 20:28:59.793886 11101 solver.cpp:334]     Train net output #0: loss = 2.60878 (* 1 = 2.60878 loss)
I0814 20:28:59.793892 11101 sgd_solver.cpp:136] Iteration 14900, lr = 0.0953438, m = 0.9
I0814 20:29:14.314009 11101 solver.cpp:312] Iteration 15000 (6.88718 iter/s, 14.5197s/100 iter), loss = 2.69434
I0814 20:29:14.314106 11101 solver.cpp:334]     Train net output #0: loss = 2.69363 (* 1 = 2.69363 loss)
I0814 20:29:14.314126 11101 sgd_solver.cpp:136] Iteration 15000, lr = 0.0953125, m = 0.9
I0814 20:29:28.553308 11101 solver.cpp:312] Iteration 15100 (7.02302 iter/s, 14.2389s/100 iter), loss = 2.59775
I0814 20:29:28.553333 11101 solver.cpp:334]     Train net output #0: loss = 2.40856 (* 1 = 2.40856 loss)
I0814 20:29:28.553339 11101 sgd_solver.cpp:136] Iteration 15100, lr = 0.0952813, m = 0.9
I0814 20:29:42.957062 11101 solver.cpp:312] Iteration 15200 (6.94284 iter/s, 14.4033s/100 iter), loss = 3.04225
I0814 20:29:42.957087 11101 solver.cpp:334]     Train net output #0: loss = 3.30208 (* 1 = 3.30208 loss)
I0814 20:29:42.957090 11101 sgd_solver.cpp:136] Iteration 15200, lr = 0.09525, m = 0.9
I0814 20:29:57.381232 11101 solver.cpp:312] Iteration 15300 (6.93301 iter/s, 14.4238s/100 iter), loss = 2.80398
I0814 20:29:57.381434 11101 solver.cpp:334]     Train net output #0: loss = 2.97375 (* 1 = 2.97375 loss)
I0814 20:29:57.381522 11101 sgd_solver.cpp:136] Iteration 15300, lr = 0.0952187, m = 0.9
I0814 20:30:12.370824 11101 solver.cpp:312] Iteration 15400 (6.67149 iter/s, 14.9892s/100 iter), loss = 2.55471
I0814 20:30:12.370899 11101 solver.cpp:334]     Train net output #0: loss = 2.72879 (* 1 = 2.72879 loss)
I0814 20:30:12.370918 11101 sgd_solver.cpp:136] Iteration 15400, lr = 0.0951875, m = 0.9
I0814 20:30:26.700042 11101 solver.cpp:312] Iteration 15500 (6.97895 iter/s, 14.3288s/100 iter), loss = 3.11053
I0814 20:30:26.700069 11101 solver.cpp:334]     Train net output #0: loss = 3.13541 (* 1 = 3.13541 loss)
I0814 20:30:26.700074 11101 sgd_solver.cpp:136] Iteration 15500, lr = 0.0951563, m = 0.9
I0814 20:30:40.936280 11101 solver.cpp:312] Iteration 15600 (7.02453 iter/s, 14.2358s/100 iter), loss = 2.83215
I0814 20:30:40.936468 11101 solver.cpp:334]     Train net output #0: loss = 2.66061 (* 1 = 2.66061 loss)
I0814 20:30:40.936488 11101 sgd_solver.cpp:136] Iteration 15600, lr = 0.095125, m = 0.9
I0814 20:30:55.345068 11101 solver.cpp:312] Iteration 15700 (6.94041 iter/s, 14.4084s/100 iter), loss = 2.81573
I0814 20:30:55.345093 11101 solver.cpp:334]     Train net output #0: loss = 2.50153 (* 1 = 2.50153 loss)
I0814 20:30:55.345096 11101 sgd_solver.cpp:136] Iteration 15700, lr = 0.0950937, m = 0.9
I0814 20:31:10.032479 11101 solver.cpp:312] Iteration 15800 (6.80875 iter/s, 14.687s/100 iter), loss = 2.50272
I0814 20:31:10.032510 11101 solver.cpp:334]     Train net output #0: loss = 2.40526 (* 1 = 2.40526 loss)
I0814 20:31:10.032516 11101 sgd_solver.cpp:136] Iteration 15800, lr = 0.0950625, m = 0.9
I0814 20:31:24.523972 11101 solver.cpp:312] Iteration 15900 (6.9008 iter/s, 14.4911s/100 iter), loss = 2.76321
I0814 20:31:24.524035 11101 solver.cpp:334]     Train net output #0: loss = 2.52937 (* 1 = 2.52937 loss)
I0814 20:31:24.524042 11101 sgd_solver.cpp:136] Iteration 15900, lr = 0.0950313, m = 0.9
I0814 20:31:38.964529 11101 solver.cpp:509] Iteration 16000, Testing net (#0)
I0814 20:31:55.625157 11099 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 20:31:59.882942 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.299705
I0814 20:31:59.882966 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.543764
I0814 20:31:59.882973 11101 solver.cpp:594]     Test net output #2: loss = 3.45162 (* 1 = 3.45162 loss)
I0814 20:31:59.883185 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9179s
I0814 20:32:00.035852 11101 solver.cpp:312] Iteration 16000 (2.81604 iter/s, 35.5109s/100 iter), loss = 2.34931
I0814 20:32:00.035970 11101 solver.cpp:334]     Train net output #0: loss = 2.11634 (* 1 = 2.11634 loss)
I0814 20:32:00.035984 11101 sgd_solver.cpp:136] Iteration 16000, lr = 0.095, m = 0.9
I0814 20:32:14.636651 11101 solver.cpp:312] Iteration 16100 (6.84914 iter/s, 14.6004s/100 iter), loss = 2.51901
I0814 20:32:14.636684 11101 solver.cpp:334]     Train net output #0: loss = 2.4302 (* 1 = 2.4302 loss)
I0814 20:32:14.636692 11101 sgd_solver.cpp:136] Iteration 16100, lr = 0.0949688, m = 0.9
I0814 20:32:29.478494 11101 solver.cpp:312] Iteration 16200 (6.7379 iter/s, 14.8414s/100 iter), loss = 2.48441
I0814 20:32:29.478557 11101 solver.cpp:334]     Train net output #0: loss = 2.39241 (* 1 = 2.39241 loss)
I0814 20:32:29.478564 11101 sgd_solver.cpp:136] Iteration 16200, lr = 0.0949375, m = 0.9
I0814 20:32:44.387634 11101 solver.cpp:312] Iteration 16300 (6.70749 iter/s, 14.9087s/100 iter), loss = 2.19372
I0814 20:32:44.387662 11101 solver.cpp:334]     Train net output #0: loss = 1.97504 (* 1 = 1.97504 loss)
I0814 20:32:44.387668 11101 sgd_solver.cpp:136] Iteration 16300, lr = 0.0949063, m = 0.9
I0814 20:32:59.317072 11101 solver.cpp:312] Iteration 16400 (6.69837 iter/s, 14.929s/100 iter), loss = 2.67535
I0814 20:32:59.317292 11101 solver.cpp:334]     Train net output #0: loss = 2.36934 (* 1 = 2.36934 loss)
I0814 20:32:59.317401 11101 sgd_solver.cpp:136] Iteration 16400, lr = 0.094875, m = 0.9
I0814 20:33:13.777945 11101 solver.cpp:312] Iteration 16500 (6.91541 iter/s, 14.4605s/100 iter), loss = 2.76297
I0814 20:33:13.778009 11101 solver.cpp:334]     Train net output #0: loss = 2.92914 (* 1 = 2.92914 loss)
I0814 20:33:13.778017 11101 sgd_solver.cpp:136] Iteration 16500, lr = 0.0948438, m = 0.9
I0814 20:33:28.313527 11101 solver.cpp:312] Iteration 16600 (6.87987 iter/s, 14.5352s/100 iter), loss = 2.723
I0814 20:33:28.313555 11101 solver.cpp:334]     Train net output #0: loss = 2.66904 (* 1 = 2.66904 loss)
I0814 20:33:28.313561 11101 sgd_solver.cpp:136] Iteration 16600, lr = 0.0948125, m = 0.9
I0814 20:33:42.680455 11101 solver.cpp:312] Iteration 16700 (6.96063 iter/s, 14.3665s/100 iter), loss = 2.71576
I0814 20:33:42.680483 11101 solver.cpp:334]     Train net output #0: loss = 2.89382 (* 1 = 2.89382 loss)
I0814 20:33:42.680487 11101 sgd_solver.cpp:136] Iteration 16700, lr = 0.0947812, m = 0.9
I0814 20:33:57.162792 11101 solver.cpp:312] Iteration 16800 (6.90516 iter/s, 14.4819s/100 iter), loss = 2.11002
I0814 20:33:57.162856 11101 solver.cpp:334]     Train net output #0: loss = 2.17257 (* 1 = 2.17257 loss)
I0814 20:33:57.162863 11101 sgd_solver.cpp:136] Iteration 16800, lr = 0.09475, m = 0.9
I0814 20:34:11.738329 11101 solver.cpp:312] Iteration 16900 (6.86101 iter/s, 14.5751s/100 iter), loss = 2.33286
I0814 20:34:11.738355 11101 solver.cpp:334]     Train net output #0: loss = 2.10196 (* 1 = 2.10196 loss)
I0814 20:34:11.738361 11101 sgd_solver.cpp:136] Iteration 16900, lr = 0.0947187, m = 0.9
I0814 20:34:26.100628 11101 solver.cpp:312] Iteration 17000 (6.96288 iter/s, 14.3619s/100 iter), loss = 2.51257
I0814 20:34:26.100657 11101 solver.cpp:334]     Train net output #0: loss = 2.43774 (* 1 = 2.43774 loss)
I0814 20:34:26.100663 11101 sgd_solver.cpp:136] Iteration 17000, lr = 0.0946875, m = 0.9
I0814 20:34:40.839467 11101 solver.cpp:312] Iteration 17100 (6.78499 iter/s, 14.7384s/100 iter), loss = 2.61378
I0814 20:34:40.839527 11101 solver.cpp:334]     Train net output #0: loss = 2.79812 (* 1 = 2.79812 loss)
I0814 20:34:40.839534 11101 sgd_solver.cpp:136] Iteration 17100, lr = 0.0946563, m = 0.9
I0814 20:34:55.691722 11101 solver.cpp:312] Iteration 17200 (6.73318 iter/s, 14.8518s/100 iter), loss = 2.96277
I0814 20:34:55.691776 11101 solver.cpp:334]     Train net output #0: loss = 3.04624 (* 1 = 3.04624 loss)
I0814 20:34:55.691789 11101 sgd_solver.cpp:136] Iteration 17200, lr = 0.094625, m = 0.9
I0814 20:35:10.370961 11101 solver.cpp:312] Iteration 17300 (6.81254 iter/s, 14.6788s/100 iter), loss = 2.40813
I0814 20:35:10.370990 11101 solver.cpp:334]     Train net output #0: loss = 1.93044 (* 1 = 1.93044 loss)
I0814 20:35:10.370997 11101 sgd_solver.cpp:136] Iteration 17300, lr = 0.0945938, m = 0.9
I0814 20:35:24.978341 11101 solver.cpp:312] Iteration 17400 (6.84605 iter/s, 14.607s/100 iter), loss = 2.44333
I0814 20:35:24.978392 11101 solver.cpp:334]     Train net output #0: loss = 2.88866 (* 1 = 2.88866 loss)
I0814 20:35:24.978399 11101 sgd_solver.cpp:136] Iteration 17400, lr = 0.0945625, m = 0.9
I0814 20:35:39.267285 11101 solver.cpp:312] Iteration 17500 (6.99862 iter/s, 14.2885s/100 iter), loss = 2.54311
I0814 20:35:39.267510 11101 solver.cpp:334]     Train net output #0: loss = 2.38292 (* 1 = 2.38292 loss)
I0814 20:35:39.267623 11101 sgd_solver.cpp:136] Iteration 17500, lr = 0.0945313, m = 0.9
I0814 20:35:53.815898 11101 solver.cpp:312] Iteration 17600 (6.87371 iter/s, 14.5482s/100 iter), loss = 2.35766
I0814 20:35:53.815925 11101 solver.cpp:334]     Train net output #0: loss = 2.5578 (* 1 = 2.5578 loss)
I0814 20:35:53.815932 11101 sgd_solver.cpp:136] Iteration 17600, lr = 0.0945, m = 0.9
I0814 20:36:08.326767 11101 solver.cpp:312] Iteration 17700 (6.89158 iter/s, 14.5105s/100 iter), loss = 2.86763
I0814 20:36:08.326822 11101 solver.cpp:334]     Train net output #0: loss = 3.21427 (* 1 = 3.21427 loss)
I0814 20:36:08.326827 11101 sgd_solver.cpp:136] Iteration 17700, lr = 0.0944688, m = 0.9
I0814 20:36:22.805913 11101 solver.cpp:312] Iteration 17800 (6.90668 iter/s, 14.4787s/100 iter), loss = 2.75476
I0814 20:36:22.806126 11101 solver.cpp:334]     Train net output #0: loss = 2.5323 (* 1 = 2.5323 loss)
I0814 20:36:22.806234 11101 sgd_solver.cpp:136] Iteration 17800, lr = 0.0944375, m = 0.9
I0814 20:36:37.290139 11101 solver.cpp:312] Iteration 17900 (6.90426 iter/s, 14.4838s/100 iter), loss = 2.27866
I0814 20:36:37.290166 11101 solver.cpp:334]     Train net output #0: loss = 1.93691 (* 1 = 1.93691 loss)
I0814 20:36:37.290171 11101 sgd_solver.cpp:136] Iteration 17900, lr = 0.0944062, m = 0.9
I0814 20:36:51.524153 11101 solver.cpp:509] Iteration 18000, Testing net (#0)
I0814 20:37:05.578160 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 20:37:12.359146 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.302999
I0814 20:37:12.359175 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.555117
I0814 20:37:12.359182 11101 solver.cpp:594]     Test net output #2: loss = 3.35653 (* 1 = 3.35653 loss)
I0814 20:37:12.359305 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8359s
I0814 20:37:12.507329 11101 solver.cpp:312] Iteration 18000 (2.8396 iter/s, 35.2162s/100 iter), loss = 2.72726
I0814 20:37:12.507356 11101 solver.cpp:334]     Train net output #0: loss = 2.21751 (* 1 = 2.21751 loss)
I0814 20:37:12.507362 11101 sgd_solver.cpp:136] Iteration 18000, lr = 0.094375, m = 0.9
I0814 20:37:26.805768 11101 solver.cpp:312] Iteration 18100 (6.99397 iter/s, 14.298s/100 iter), loss = 2.64096
I0814 20:37:26.805847 11101 solver.cpp:334]     Train net output #0: loss = 2.58072 (* 1 = 2.58072 loss)
I0814 20:37:26.805855 11101 sgd_solver.cpp:136] Iteration 18100, lr = 0.0943438, m = 0.9
I0814 20:37:41.397615 11101 solver.cpp:312] Iteration 18200 (6.85334 iter/s, 14.5914s/100 iter), loss = 2.53738
I0814 20:37:41.397640 11101 solver.cpp:334]     Train net output #0: loss = 2.15826 (* 1 = 2.15826 loss)
I0814 20:37:41.397644 11101 sgd_solver.cpp:136] Iteration 18200, lr = 0.0943125, m = 0.9
I0814 20:37:55.697923 11101 solver.cpp:312] Iteration 18300 (6.99306 iter/s, 14.2999s/100 iter), loss = 2.72378
I0814 20:37:55.697952 11101 solver.cpp:334]     Train net output #0: loss = 2.73242 (* 1 = 2.73242 loss)
I0814 20:37:55.697958 11101 sgd_solver.cpp:136] Iteration 18300, lr = 0.0942812, m = 0.9
I0814 20:38:10.561477 11101 solver.cpp:312] Iteration 18400 (6.72806 iter/s, 14.8631s/100 iter), loss = 2.26075
I0814 20:38:10.561569 11101 solver.cpp:334]     Train net output #0: loss = 2.48612 (* 1 = 2.48612 loss)
I0814 20:38:10.561588 11101 sgd_solver.cpp:136] Iteration 18400, lr = 0.09425, m = 0.9
I0814 20:38:25.062728 11101 solver.cpp:312] Iteration 18500 (6.89616 iter/s, 14.5008s/100 iter), loss = 2.31201
I0814 20:38:25.062754 11101 solver.cpp:334]     Train net output #0: loss = 2.61883 (* 1 = 2.61883 loss)
I0814 20:38:25.062760 11101 sgd_solver.cpp:136] Iteration 18500, lr = 0.0942188, m = 0.9
I0814 20:38:39.655515 11101 solver.cpp:312] Iteration 18600 (6.8529 iter/s, 14.5924s/100 iter), loss = 2.50529
I0814 20:38:39.655544 11101 solver.cpp:334]     Train net output #0: loss = 2.62419 (* 1 = 2.62419 loss)
I0814 20:38:39.655551 11101 sgd_solver.cpp:136] Iteration 18600, lr = 0.0941875, m = 0.9
I0814 20:38:54.263535 11101 solver.cpp:312] Iteration 18700 (6.84575 iter/s, 14.6076s/100 iter), loss = 2.55538
I0814 20:38:54.263633 11101 solver.cpp:334]     Train net output #0: loss = 2.51662 (* 1 = 2.51662 loss)
I0814 20:38:54.263653 11101 sgd_solver.cpp:136] Iteration 18700, lr = 0.0941563, m = 0.9
I0814 20:39:09.333724 11101 solver.cpp:312] Iteration 18800 (6.63581 iter/s, 15.0698s/100 iter), loss = 2.3121
I0814 20:39:09.333751 11101 solver.cpp:334]     Train net output #0: loss = 2.26812 (* 1 = 2.26812 loss)
I0814 20:39:09.333757 11101 sgd_solver.cpp:136] Iteration 18800, lr = 0.094125, m = 0.9
I0814 20:39:23.704025 11101 solver.cpp:312] Iteration 18900 (6.959 iter/s, 14.3699s/100 iter), loss = 2.92005
I0814 20:39:23.704089 11101 solver.cpp:334]     Train net output #0: loss = 3.02053 (* 1 = 3.02053 loss)
I0814 20:39:23.704107 11101 sgd_solver.cpp:136] Iteration 18900, lr = 0.0940938, m = 0.9
I0814 20:39:38.111737 11101 solver.cpp:312] Iteration 19000 (6.94093 iter/s, 14.4073s/100 iter), loss = 2.86191
I0814 20:39:38.111830 11101 solver.cpp:334]     Train net output #0: loss = 2.67743 (* 1 = 2.67743 loss)
I0814 20:39:38.111847 11101 sgd_solver.cpp:136] Iteration 19000, lr = 0.0940625, m = 0.9
I0814 20:39:52.521771 11101 solver.cpp:312] Iteration 19100 (6.93981 iter/s, 14.4096s/100 iter), loss = 2.2207
I0814 20:39:52.521800 11101 solver.cpp:334]     Train net output #0: loss = 1.91316 (* 1 = 1.91316 loss)
I0814 20:39:52.521806 11101 sgd_solver.cpp:136] Iteration 19100, lr = 0.0940313, m = 0.9
I0814 20:40:06.859300 11101 solver.cpp:312] Iteration 19200 (6.9749 iter/s, 14.3371s/100 iter), loss = 2.44242
I0814 20:40:06.859326 11101 solver.cpp:334]     Train net output #0: loss = 2.76927 (* 1 = 2.76927 loss)
I0814 20:40:06.859333 11101 sgd_solver.cpp:136] Iteration 19200, lr = 0.094, m = 0.9
I0814 20:40:21.224679 11101 solver.cpp:312] Iteration 19300 (6.96138 iter/s, 14.365s/100 iter), loss = 2.82211
I0814 20:40:21.224793 11101 solver.cpp:334]     Train net output #0: loss = 2.94761 (* 1 = 2.94761 loss)
I0814 20:40:21.224812 11101 sgd_solver.cpp:136] Iteration 19300, lr = 0.0939687, m = 0.9
I0814 20:40:35.859962 11101 solver.cpp:312] Iteration 19400 (6.833 iter/s, 14.6349s/100 iter), loss = 2.42248
I0814 20:40:35.859989 11101 solver.cpp:334]     Train net output #0: loss = 2.27749 (* 1 = 2.27749 loss)
I0814 20:40:35.859995 11101 sgd_solver.cpp:136] Iteration 19400, lr = 0.0939375, m = 0.9
I0814 20:40:50.432288 11101 solver.cpp:312] Iteration 19500 (6.86252 iter/s, 14.5719s/100 iter), loss = 2.58718
I0814 20:40:50.432317 11101 solver.cpp:334]     Train net output #0: loss = 2.60614 (* 1 = 2.60614 loss)
I0814 20:40:50.432324 11101 sgd_solver.cpp:136] Iteration 19500, lr = 0.0939062, m = 0.9
I0814 20:41:04.846601 11101 solver.cpp:312] Iteration 19600 (6.93775 iter/s, 14.4139s/100 iter), loss = 2.36132
I0814 20:41:04.846657 11101 solver.cpp:334]     Train net output #0: loss = 2.26813 (* 1 = 2.26813 loss)
I0814 20:41:04.846662 11101 sgd_solver.cpp:136] Iteration 19600, lr = 0.093875, m = 0.9
I0814 20:41:19.256243 11101 solver.cpp:312] Iteration 19700 (6.94 iter/s, 14.4092s/100 iter), loss = 2.22307
I0814 20:41:19.256271 11101 solver.cpp:334]     Train net output #0: loss = 2.07783 (* 1 = 2.07783 loss)
I0814 20:41:19.256278 11101 sgd_solver.cpp:136] Iteration 19700, lr = 0.0938438, m = 0.9
I0814 20:41:33.712836 11101 solver.cpp:312] Iteration 19800 (6.91746 iter/s, 14.4562s/100 iter), loss = 3.20391
I0814 20:41:33.712862 11101 solver.cpp:334]     Train net output #0: loss = 2.63997 (* 1 = 2.63997 loss)
I0814 20:41:33.712867 11101 sgd_solver.cpp:136] Iteration 19800, lr = 0.0938125, m = 0.9
I0814 20:41:48.452623 11101 solver.cpp:312] Iteration 19900 (6.78455 iter/s, 14.7394s/100 iter), loss = 2.61821
I0814 20:41:48.452718 11101 solver.cpp:334]     Train net output #0: loss = 2.10724 (* 1 = 2.10724 loss)
I0814 20:41:48.452740 11101 sgd_solver.cpp:136] Iteration 19900, lr = 0.0937813, m = 0.9
I0814 20:42:02.904716 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_20000.caffemodel
I0814 20:42:02.915009 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_20000.solverstate
I0814 20:42:02.919824 11101 solver.cpp:509] Iteration 20000, Testing net (#0)
I0814 20:42:23.443543 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.335882
I0814 20:42:23.443608 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.597058
I0814 20:42:23.443617 11101 solver.cpp:594]     Test net output #2: loss = 3.14475 (* 1 = 3.14475 loss)
I0814 20:42:23.443639 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.5232s
I0814 20:42:23.615283 11101 solver.cpp:312] Iteration 20000 (2.84401 iter/s, 35.1617s/100 iter), loss = 2.53693
I0814 20:42:23.615345 11101 solver.cpp:334]     Train net output #0: loss = 2.35189 (* 1 = 2.35189 loss)
I0814 20:42:23.615361 11101 sgd_solver.cpp:136] Iteration 20000, lr = 0.09375, m = 0.9
I0814 20:42:37.959026 11101 solver.cpp:312] Iteration 20100 (6.97188 iter/s, 14.3433s/100 iter), loss = 2.55751
I0814 20:42:37.959054 11101 solver.cpp:334]     Train net output #0: loss = 2.9878 (* 1 = 2.9878 loss)
I0814 20:42:37.959061 11101 sgd_solver.cpp:136] Iteration 20100, lr = 0.0937188, m = 0.9
I0814 20:42:52.366492 11101 solver.cpp:312] Iteration 20200 (6.94105 iter/s, 14.4071s/100 iter), loss = 2.65627
I0814 20:42:52.366521 11101 solver.cpp:334]     Train net output #0: loss = 2.43517 (* 1 = 2.43517 loss)
I0814 20:42:52.366528 11101 sgd_solver.cpp:136] Iteration 20200, lr = 0.0936875, m = 0.9
I0814 20:43:06.860241 11101 solver.cpp:312] Iteration 20300 (6.89972 iter/s, 14.4933s/100 iter), loss = 2.63873
I0814 20:43:06.864157 11101 solver.cpp:334]     Train net output #0: loss = 2.88864 (* 1 = 2.88864 loss)
I0814 20:43:06.864167 11101 sgd_solver.cpp:136] Iteration 20300, lr = 0.0936562, m = 0.9
I0814 20:43:21.254359 11101 solver.cpp:312] Iteration 20400 (6.94748 iter/s, 14.3937s/100 iter), loss = 2.34922
I0814 20:43:21.254387 11101 solver.cpp:334]     Train net output #0: loss = 2.33246 (* 1 = 2.33246 loss)
I0814 20:43:21.254393 11101 sgd_solver.cpp:136] Iteration 20400, lr = 0.093625, m = 0.9
I0814 20:43:35.860342 11101 solver.cpp:312] Iteration 20500 (6.84671 iter/s, 14.6056s/100 iter), loss = 2.6339
I0814 20:43:35.860373 11101 solver.cpp:334]     Train net output #0: loss = 2.33448 (* 1 = 2.33448 loss)
I0814 20:43:35.860379 11101 sgd_solver.cpp:136] Iteration 20500, lr = 0.0935938, m = 0.9
I0814 20:43:50.454529 11101 solver.cpp:312] Iteration 20600 (6.85224 iter/s, 14.5938s/100 iter), loss = 2.75788
I0814 20:43:50.454792 11101 solver.cpp:334]     Train net output #0: loss = 2.439 (* 1 = 2.439 loss)
I0814 20:43:50.454901 11101 sgd_solver.cpp:136] Iteration 20600, lr = 0.0935625, m = 0.9
I0814 20:44:04.943177 11101 solver.cpp:312] Iteration 20700 (6.90215 iter/s, 14.4882s/100 iter), loss = 2.55491
I0814 20:44:04.943207 11101 solver.cpp:334]     Train net output #0: loss = 2.3348 (* 1 = 2.3348 loss)
I0814 20:44:04.943213 11101 sgd_solver.cpp:136] Iteration 20700, lr = 0.0935313, m = 0.9
I0814 20:44:19.520972 11101 solver.cpp:312] Iteration 20800 (6.85994 iter/s, 14.5774s/100 iter), loss = 2.23694
I0814 20:44:19.520999 11101 solver.cpp:334]     Train net output #0: loss = 2.56546 (* 1 = 2.56546 loss)
I0814 20:44:19.521005 11101 sgd_solver.cpp:136] Iteration 20800, lr = 0.0935, m = 0.9
I0814 20:44:34.146677 11101 solver.cpp:312] Iteration 20900 (6.83747 iter/s, 14.6253s/100 iter), loss = 2.53125
I0814 20:44:34.146922 11101 solver.cpp:334]     Train net output #0: loss = 2.59514 (* 1 = 2.59514 loss)
I0814 20:44:34.147007 11101 sgd_solver.cpp:136] Iteration 20900, lr = 0.0934687, m = 0.9
I0814 20:44:48.765640 11101 solver.cpp:312] Iteration 21000 (6.84063 iter/s, 14.6185s/100 iter), loss = 2.64126
I0814 20:44:48.765674 11101 solver.cpp:334]     Train net output #0: loss = 2.17658 (* 1 = 2.17658 loss)
I0814 20:44:48.765681 11101 sgd_solver.cpp:136] Iteration 21000, lr = 0.0934375, m = 0.9
I0814 20:45:03.562222 11101 solver.cpp:312] Iteration 21100 (6.75851 iter/s, 14.7962s/100 iter), loss = 2.64715
I0814 20:45:03.562250 11101 solver.cpp:334]     Train net output #0: loss = 2.25573 (* 1 = 2.25573 loss)
I0814 20:45:03.562255 11101 sgd_solver.cpp:136] Iteration 21100, lr = 0.0934063, m = 0.9
I0814 20:45:18.580855 11101 solver.cpp:312] Iteration 21200 (6.65859 iter/s, 15.0182s/100 iter), loss = 2.70016
I0814 20:45:18.580946 11101 solver.cpp:334]     Train net output #0: loss = 3.06336 (* 1 = 3.06336 loss)
I0814 20:45:18.580966 11101 sgd_solver.cpp:136] Iteration 21200, lr = 0.093375, m = 0.9
I0814 20:45:32.889073 11101 solver.cpp:312] Iteration 21300 (6.98919 iter/s, 14.3078s/100 iter), loss = 2.52444
I0814 20:45:32.889102 11101 solver.cpp:334]     Train net output #0: loss = 3.02566 (* 1 = 3.02566 loss)
I0814 20:45:32.889109 11101 sgd_solver.cpp:136] Iteration 21300, lr = 0.0933437, m = 0.9
I0814 20:45:47.287302 11101 solver.cpp:312] Iteration 21400 (6.9455 iter/s, 14.3978s/100 iter), loss = 2.16656
I0814 20:45:47.287329 11101 solver.cpp:334]     Train net output #0: loss = 1.94987 (* 1 = 1.94987 loss)
I0814 20:45:47.287335 11101 sgd_solver.cpp:136] Iteration 21400, lr = 0.0933125, m = 0.9
I0814 20:46:01.689067 11101 solver.cpp:312] Iteration 21500 (6.94379 iter/s, 14.4014s/100 iter), loss = 2.31519
I0814 20:46:01.689180 11101 solver.cpp:334]     Train net output #0: loss = 2.51033 (* 1 = 2.51033 loss)
I0814 20:46:01.689200 11101 sgd_solver.cpp:136] Iteration 21500, lr = 0.0932813, m = 0.9
I0814 20:46:16.212826 11101 solver.cpp:312] Iteration 21600 (6.88547 iter/s, 14.5233s/100 iter), loss = 3.0343
I0814 20:46:16.212853 11101 solver.cpp:334]     Train net output #0: loss = 2.8768 (* 1 = 2.8768 loss)
I0814 20:46:16.212858 11101 sgd_solver.cpp:136] Iteration 21600, lr = 0.09325, m = 0.9
I0814 20:46:30.820160 11101 solver.cpp:312] Iteration 21700 (6.84607 iter/s, 14.6069s/100 iter), loss = 2.51079
I0814 20:46:30.820189 11101 solver.cpp:334]     Train net output #0: loss = 2.64743 (* 1 = 2.64743 loss)
I0814 20:46:30.820195 11101 sgd_solver.cpp:136] Iteration 21700, lr = 0.0932188, m = 0.9
I0814 20:46:45.255985 11101 solver.cpp:312] Iteration 21800 (6.92741 iter/s, 14.4354s/100 iter), loss = 2.3177
I0814 20:46:45.260200 11101 solver.cpp:334]     Train net output #0: loss = 2.23041 (* 1 = 2.23041 loss)
I0814 20:46:45.260222 11101 sgd_solver.cpp:136] Iteration 21800, lr = 0.0931875, m = 0.9
I0814 20:46:59.606052 11101 solver.cpp:312] Iteration 21900 (6.96881 iter/s, 14.3497s/100 iter), loss = 2.49717
I0814 20:46:59.606079 11101 solver.cpp:334]     Train net output #0: loss = 3.09824 (* 1 = 3.09824 loss)
I0814 20:46:59.606123 11101 sgd_solver.cpp:136] Iteration 21900, lr = 0.0931562, m = 0.9
I0814 20:47:13.824440 11101 solver.cpp:509] Iteration 22000, Testing net (#0)
I0814 20:47:34.649380 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.343764
I0814 20:47:34.649441 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.60247
I0814 20:47:34.649448 11101 solver.cpp:594]     Test net output #2: loss = 3.09176 (* 1 = 3.09176 loss)
I0814 20:47:34.649466 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8244s
I0814 20:47:34.818987 11101 solver.cpp:312] Iteration 22000 (2.83995 iter/s, 35.2119s/100 iter), loss = 2.7982
I0814 20:47:34.819037 11101 solver.cpp:334]     Train net output #0: loss = 2.35142 (* 1 = 2.35142 loss)
I0814 20:47:34.819051 11101 sgd_solver.cpp:136] Iteration 22000, lr = 0.093125, m = 0.9
I0814 20:47:49.304543 11101 solver.cpp:312] Iteration 22100 (6.90363 iter/s, 14.4851s/100 iter), loss = 2.57627
I0814 20:47:49.304572 11101 solver.cpp:334]     Train net output #0: loss = 2.7268 (* 1 = 2.7268 loss)
I0814 20:47:49.304579 11101 sgd_solver.cpp:136] Iteration 22100, lr = 0.0930938, m = 0.9
I0814 20:48:03.832173 11101 solver.cpp:312] Iteration 22200 (6.88364 iter/s, 14.5272s/100 iter), loss = 2.24126
I0814 20:48:03.832227 11101 solver.cpp:334]     Train net output #0: loss = 2.04727 (* 1 = 2.04727 loss)
I0814 20:48:03.832240 11101 sgd_solver.cpp:136] Iteration 22200, lr = 0.0930625, m = 0.9
I0814 20:48:18.253239 11101 solver.cpp:312] Iteration 22300 (6.9345 iter/s, 14.4207s/100 iter), loss = 1.95167
I0814 20:48:18.253309 11101 solver.cpp:334]     Train net output #0: loss = 1.82251 (* 1 = 1.82251 loss)
I0814 20:48:18.253324 11101 sgd_solver.cpp:136] Iteration 22300, lr = 0.0930312, m = 0.9
I0814 20:48:32.581548 11101 solver.cpp:312] Iteration 22400 (6.97939 iter/s, 14.3279s/100 iter), loss = 2.52871
I0814 20:48:32.581571 11101 solver.cpp:334]     Train net output #0: loss = 2.42071 (* 1 = 2.42071 loss)
I0814 20:48:32.581575 11101 sgd_solver.cpp:136] Iteration 22400, lr = 0.093, m = 0.9
I0814 20:48:47.065035 11101 solver.cpp:312] Iteration 22500 (6.90461 iter/s, 14.4831s/100 iter), loss = 2.18772
I0814 20:48:47.065062 11101 solver.cpp:334]     Train net output #0: loss = 2.34368 (* 1 = 2.34368 loss)
I0814 20:48:47.065068 11101 sgd_solver.cpp:136] Iteration 22500, lr = 0.0929688, m = 0.9
I0814 20:49:01.573150 11101 solver.cpp:312] Iteration 22600 (6.89289 iter/s, 14.5077s/100 iter), loss = 2.33813
I0814 20:49:01.573230 11101 solver.cpp:334]     Train net output #0: loss = 1.68857 (* 1 = 1.68857 loss)
I0814 20:49:01.573245 11101 sgd_solver.cpp:136] Iteration 22600, lr = 0.0929375, m = 0.9
I0814 20:49:16.401101 11101 solver.cpp:312] Iteration 22700 (6.74421 iter/s, 14.8275s/100 iter), loss = 2.28053
I0814 20:49:16.401155 11101 solver.cpp:334]     Train net output #0: loss = 2.51707 (* 1 = 2.51707 loss)
I0814 20:49:16.401168 11101 sgd_solver.cpp:136] Iteration 22700, lr = 0.0929063, m = 0.9
I0814 20:49:30.949286 11101 solver.cpp:312] Iteration 22800 (6.87391 iter/s, 14.5478s/100 iter), loss = 2.49268
I0814 20:49:30.949316 11101 solver.cpp:334]     Train net output #0: loss = 2.58469 (* 1 = 2.58469 loss)
I0814 20:49:30.949321 11101 sgd_solver.cpp:136] Iteration 22800, lr = 0.092875, m = 0.9
I0814 20:49:45.481451 11101 solver.cpp:312] Iteration 22900 (6.88148 iter/s, 14.5317s/100 iter), loss = 2.44082
I0814 20:49:45.481655 11101 solver.cpp:334]     Train net output #0: loss = 2.41718 (* 1 = 2.41718 loss)
I0814 20:49:45.481664 11101 sgd_solver.cpp:136] Iteration 22900, lr = 0.0928437, m = 0.9
I0814 20:49:59.919101 11101 solver.cpp:312] Iteration 23000 (6.92653 iter/s, 14.4372s/100 iter), loss = 2.68163
I0814 20:49:59.919170 11101 solver.cpp:334]     Train net output #0: loss = 2.64328 (* 1 = 2.64328 loss)
I0814 20:49:59.919189 11101 sgd_solver.cpp:136] Iteration 23000, lr = 0.0928125, m = 0.9
I0814 20:50:14.600920 11101 solver.cpp:312] Iteration 23100 (6.81134 iter/s, 14.6814s/100 iter), loss = 2.43229
I0814 20:50:14.600950 11101 solver.cpp:334]     Train net output #0: loss = 2.34003 (* 1 = 2.34003 loss)
I0814 20:50:14.600956 11101 sgd_solver.cpp:136] Iteration 23100, lr = 0.0927813, m = 0.9
I0814 20:50:28.960995 11101 solver.cpp:312] Iteration 23200 (6.96395 iter/s, 14.3597s/100 iter), loss = 2.50507
I0814 20:50:28.961231 11101 solver.cpp:334]     Train net output #0: loss = 2.1936 (* 1 = 2.1936 loss)
I0814 20:50:28.961340 11101 sgd_solver.cpp:136] Iteration 23200, lr = 0.09275, m = 0.9
I0814 20:50:43.491720 11101 solver.cpp:312] Iteration 23300 (6.88216 iter/s, 14.5303s/100 iter), loss = 2.2452
I0814 20:50:43.491749 11101 solver.cpp:334]     Train net output #0: loss = 2.2107 (* 1 = 2.2107 loss)
I0814 20:50:43.491755 11101 sgd_solver.cpp:136] Iteration 23300, lr = 0.0927188, m = 0.9
I0814 20:50:57.881500 11101 solver.cpp:312] Iteration 23400 (6.94958 iter/s, 14.3894s/100 iter), loss = 2.4551
I0814 20:50:57.881521 11101 solver.cpp:334]     Train net output #0: loss = 2.64009 (* 1 = 2.64009 loss)
I0814 20:50:57.881526 11101 sgd_solver.cpp:136] Iteration 23400, lr = 0.0926875, m = 0.9
I0814 20:51:12.464835 11101 solver.cpp:312] Iteration 23500 (6.85734 iter/s, 14.5829s/100 iter), loss = 2.51175
I0814 20:51:12.464917 11101 solver.cpp:334]     Train net output #0: loss = 2.58564 (* 1 = 2.58564 loss)
I0814 20:51:12.464936 11101 sgd_solver.cpp:136] Iteration 23500, lr = 0.0926562, m = 0.9
I0814 20:51:26.872849 11101 solver.cpp:312] Iteration 23600 (6.94078 iter/s, 14.4076s/100 iter), loss = 2.63294
I0814 20:51:26.872875 11101 solver.cpp:334]     Train net output #0: loss = 2.78453 (* 1 = 2.78453 loss)
I0814 20:51:26.872879 11101 sgd_solver.cpp:136] Iteration 23600, lr = 0.092625, m = 0.9
I0814 20:51:41.462786 11101 solver.cpp:312] Iteration 23700 (6.85424 iter/s, 14.5895s/100 iter), loss = 2.52205
I0814 20:51:41.462815 11101 solver.cpp:334]     Train net output #0: loss = 2.35003 (* 1 = 2.35003 loss)
I0814 20:51:41.462822 11101 sgd_solver.cpp:136] Iteration 23700, lr = 0.0925938, m = 0.9
I0814 20:51:56.198426 11101 solver.cpp:312] Iteration 23800 (6.78646 iter/s, 14.7352s/100 iter), loss = 2.39093
I0814 20:51:56.198501 11101 solver.cpp:334]     Train net output #0: loss = 2.51607 (* 1 = 2.51607 loss)
I0814 20:51:56.198508 11101 sgd_solver.cpp:136] Iteration 23800, lr = 0.0925625, m = 0.9
I0814 20:52:10.685655 11101 solver.cpp:312] Iteration 23900 (6.90283 iter/s, 14.4868s/100 iter), loss = 2.18525
I0814 20:52:10.685684 11101 solver.cpp:334]     Train net output #0: loss = 2.1803 (* 1 = 2.1803 loss)
I0814 20:52:10.685690 11101 sgd_solver.cpp:136] Iteration 23900, lr = 0.0925313, m = 0.9
I0814 20:52:25.345628 11101 solver.cpp:509] Iteration 24000, Testing net (#0)
I0814 20:52:38.387471 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 20:52:46.281445 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.298116
I0814 20:52:46.281466 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.551588
I0814 20:52:46.281471 11101 solver.cpp:594]     Test net output #2: loss = 3.49331 (* 1 = 3.49331 loss)
I0814 20:52:46.281491 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9353s
I0814 20:52:46.434254 11101 solver.cpp:312] Iteration 24000 (2.79739 iter/s, 35.7476s/100 iter), loss = 2.4882
I0814 20:52:46.434332 11101 solver.cpp:334]     Train net output #0: loss = 2.09676 (* 1 = 2.09676 loss)
I0814 20:52:46.434351 11101 sgd_solver.cpp:136] Iteration 24000, lr = 0.0925, m = 0.9
I0814 20:53:00.966441 11101 solver.cpp:312] Iteration 24100 (6.88147 iter/s, 14.5318s/100 iter), loss = 2.44982
I0814 20:53:00.966514 11101 solver.cpp:334]     Train net output #0: loss = 2.41418 (* 1 = 2.41418 loss)
I0814 20:53:00.966531 11101 sgd_solver.cpp:136] Iteration 24100, lr = 0.0924688, m = 0.9
I0814 20:53:15.479202 11101 solver.cpp:312] Iteration 24200 (6.89069 iter/s, 14.5123s/100 iter), loss = 2.26515
I0814 20:53:15.479284 11101 solver.cpp:334]     Train net output #0: loss = 1.9558 (* 1 = 1.9558 loss)
I0814 20:53:15.479292 11101 sgd_solver.cpp:136] Iteration 24200, lr = 0.0924375, m = 0.9
I0814 20:53:30.115269 11101 solver.cpp:312] Iteration 24300 (6.83263 iter/s, 14.6357s/100 iter), loss = 2.15273
I0814 20:53:30.115332 11101 solver.cpp:334]     Train net output #0: loss = 2.17994 (* 1 = 2.17994 loss)
I0814 20:53:30.115350 11101 sgd_solver.cpp:136] Iteration 24300, lr = 0.0924063, m = 0.9
I0814 20:53:44.513437 11101 solver.cpp:312] Iteration 24400 (6.94553 iter/s, 14.3978s/100 iter), loss = 2.45037
I0814 20:53:44.513466 11101 solver.cpp:334]     Train net output #0: loss = 2.52202 (* 1 = 2.52202 loss)
I0814 20:53:44.513473 11101 sgd_solver.cpp:136] Iteration 24400, lr = 0.092375, m = 0.9
I0814 20:53:58.929229 11101 solver.cpp:312] Iteration 24500 (6.93704 iter/s, 14.4154s/100 iter), loss = 2.52177
I0814 20:53:58.929294 11101 solver.cpp:334]     Train net output #0: loss = 2.76427 (* 1 = 2.76427 loss)
I0814 20:53:58.929301 11101 sgd_solver.cpp:136] Iteration 24500, lr = 0.0923437, m = 0.9
I0814 20:54:13.556175 11101 solver.cpp:312] Iteration 24600 (6.83689 iter/s, 14.6265s/100 iter), loss = 2.38678
I0814 20:54:13.556203 11101 solver.cpp:334]     Train net output #0: loss = 2.03432 (* 1 = 2.03432 loss)
I0814 20:54:13.556208 11101 sgd_solver.cpp:136] Iteration 24600, lr = 0.0923125, m = 0.9
I0814 20:54:28.162356 11101 solver.cpp:312] Iteration 24700 (6.84661 iter/s, 14.6058s/100 iter), loss = 2.24995
I0814 20:54:28.162384 11101 solver.cpp:334]     Train net output #0: loss = 2.52909 (* 1 = 2.52909 loss)
I0814 20:54:28.162389 11101 sgd_solver.cpp:136] Iteration 24700, lr = 0.0922813, m = 0.9
I0814 20:54:42.699703 11101 solver.cpp:312] Iteration 24800 (6.87903 iter/s, 14.5369s/100 iter), loss = 2.05159
I0814 20:54:42.699757 11101 solver.cpp:334]     Train net output #0: loss = 1.94587 (* 1 = 1.94587 loss)
I0814 20:54:42.699764 11101 sgd_solver.cpp:136] Iteration 24800, lr = 0.09225, m = 0.9
I0814 20:54:57.134557 11101 solver.cpp:312] Iteration 24900 (6.92787 iter/s, 14.4344s/100 iter), loss = 2.24811
I0814 20:54:57.134621 11101 solver.cpp:334]     Train net output #0: loss = 2.12273 (* 1 = 2.12273 loss)
I0814 20:54:57.134639 11101 sgd_solver.cpp:136] Iteration 24900, lr = 0.0922187, m = 0.9
I0814 20:55:11.673017 11101 solver.cpp:312] Iteration 25000 (6.8785 iter/s, 14.538s/100 iter), loss = 2.33813
I0814 20:55:11.673044 11101 solver.cpp:334]     Train net output #0: loss = 2.15707 (* 1 = 2.15707 loss)
I0814 20:55:11.673050 11101 sgd_solver.cpp:136] Iteration 25000, lr = 0.0921875, m = 0.9
I0814 20:55:26.499225 11101 solver.cpp:312] Iteration 25100 (6.745 iter/s, 14.8258s/100 iter), loss = 2.79334
I0814 20:55:26.499277 11101 solver.cpp:334]     Train net output #0: loss = 2.80312 (* 1 = 2.80312 loss)
I0814 20:55:26.499284 11101 sgd_solver.cpp:136] Iteration 25100, lr = 0.0921563, m = 0.9
I0814 20:55:40.868444 11101 solver.cpp:312] Iteration 25200 (6.95952 iter/s, 14.3688s/100 iter), loss = 2.17271
I0814 20:55:40.868515 11101 solver.cpp:334]     Train net output #0: loss = 2.11923 (* 1 = 2.11923 loss)
I0814 20:55:40.868533 11101 sgd_solver.cpp:136] Iteration 25200, lr = 0.092125, m = 0.9
I0814 20:55:55.798029 11101 solver.cpp:312] Iteration 25300 (6.6983 iter/s, 14.9292s/100 iter), loss = 2.78903
I0814 20:55:55.798100 11101 solver.cpp:334]     Train net output #0: loss = 2.74265 (* 1 = 2.74265 loss)
I0814 20:55:55.798126 11101 sgd_solver.cpp:136] Iteration 25300, lr = 0.0920938, m = 0.9
I0814 20:56:10.232650 11101 solver.cpp:312] Iteration 25400 (6.92799 iter/s, 14.4342s/100 iter), loss = 2.49328
I0814 20:56:10.232870 11101 solver.cpp:334]     Train net output #0: loss = 2.42377 (* 1 = 2.42377 loss)
I0814 20:56:10.232878 11101 sgd_solver.cpp:136] Iteration 25400, lr = 0.0920625, m = 0.9
I0814 20:56:24.832159 11101 solver.cpp:312] Iteration 25500 (6.84974 iter/s, 14.5991s/100 iter), loss = 2.66978
I0814 20:56:24.832232 11101 solver.cpp:334]     Train net output #0: loss = 2.85835 (* 1 = 2.85835 loss)
I0814 20:56:24.832257 11101 sgd_solver.cpp:136] Iteration 25500, lr = 0.0920313, m = 0.9
I0814 20:56:39.271191 11101 solver.cpp:312] Iteration 25600 (6.92587 iter/s, 14.4386s/100 iter), loss = 2.46898
I0814 20:56:39.271220 11101 solver.cpp:334]     Train net output #0: loss = 2.29599 (* 1 = 2.29599 loss)
I0814 20:56:39.271226 11101 sgd_solver.cpp:136] Iteration 25600, lr = 0.092, m = 0.9
I0814 20:56:53.804442 11101 solver.cpp:312] Iteration 25700 (6.88097 iter/s, 14.5328s/100 iter), loss = 2.48188
I0814 20:56:53.804540 11101 solver.cpp:334]     Train net output #0: loss = 2.12339 (* 1 = 2.12339 loss)
I0814 20:56:53.804560 11101 sgd_solver.cpp:136] Iteration 25700, lr = 0.0919688, m = 0.9
I0814 20:57:08.326967 11101 solver.cpp:312] Iteration 25800 (6.88605 iter/s, 14.5221s/100 iter), loss = 2.53157
I0814 20:57:08.326997 11101 solver.cpp:334]     Train net output #0: loss = 2.32139 (* 1 = 2.32139 loss)
I0814 20:57:08.327003 11101 sgd_solver.cpp:136] Iteration 25800, lr = 0.0919375, m = 0.9
I0814 20:57:22.848994 11101 solver.cpp:312] Iteration 25900 (6.88629 iter/s, 14.5216s/100 iter), loss = 2.56584
I0814 20:57:22.849021 11101 solver.cpp:334]     Train net output #0: loss = 2.85614 (* 1 = 2.85614 loss)
I0814 20:57:22.849026 11101 sgd_solver.cpp:136] Iteration 25900, lr = 0.0919062, m = 0.9
I0814 20:57:37.509153 11101 solver.cpp:509] Iteration 26000, Testing net (#0)
I0814 20:57:58.416643 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.374176
I0814 20:57:58.416667 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.639587
I0814 20:57:58.416676 11101 solver.cpp:594]     Test net output #2: loss = 2.88589 (* 1 = 2.88589 loss)
I0814 20:57:58.416700 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.907s
I0814 20:57:58.561381 11101 solver.cpp:312] Iteration 26000 (2.80023 iter/s, 35.7114s/100 iter), loss = 1.96388
I0814 20:57:58.561439 11101 solver.cpp:334]     Train net output #0: loss = 2.03608 (* 1 = 2.03608 loss)
I0814 20:57:58.561458 11101 sgd_solver.cpp:136] Iteration 26000, lr = 0.091875, m = 0.9
I0814 20:58:13.189142 11101 solver.cpp:312] Iteration 26100 (6.83651 iter/s, 14.6273s/100 iter), loss = 2.50685
I0814 20:58:13.189323 11101 solver.cpp:334]     Train net output #0: loss = 2.59886 (* 1 = 2.59886 loss)
I0814 20:58:13.189342 11101 sgd_solver.cpp:136] Iteration 26100, lr = 0.0918437, m = 0.9
I0814 20:58:27.632308 11101 solver.cpp:312] Iteration 26200 (6.92389 iter/s, 14.4428s/100 iter), loss = 2.43638
I0814 20:58:27.632335 11101 solver.cpp:334]     Train net output #0: loss = 2.51228 (* 1 = 2.51228 loss)
I0814 20:58:27.632341 11101 sgd_solver.cpp:136] Iteration 26200, lr = 0.0918125, m = 0.9
I0814 20:58:42.182931 11101 solver.cpp:312] Iteration 26300 (6.87276 iter/s, 14.5502s/100 iter), loss = 2.82291
I0814 20:58:42.182996 11101 solver.cpp:334]     Train net output #0: loss = 3.37799 (* 1 = 3.37799 loss)
I0814 20:58:42.183013 11101 sgd_solver.cpp:136] Iteration 26300, lr = 0.0917813, m = 0.9
I0814 20:58:56.476781 11101 solver.cpp:312] Iteration 26400 (6.99622 iter/s, 14.2934s/100 iter), loss = 2.09318
I0814 20:58:56.476877 11101 solver.cpp:334]     Train net output #0: loss = 1.72533 (* 1 = 1.72533 loss)
I0814 20:58:56.476896 11101 sgd_solver.cpp:136] Iteration 26400, lr = 0.09175, m = 0.9
I0814 20:59:11.183771 11101 solver.cpp:312] Iteration 26500 (6.79968 iter/s, 14.7066s/100 iter), loss = 2.65441
I0814 20:59:11.183835 11101 solver.cpp:334]     Train net output #0: loss = 2.66312 (* 1 = 2.66312 loss)
I0814 20:59:11.183858 11101 sgd_solver.cpp:136] Iteration 26500, lr = 0.0917188, m = 0.9
I0814 20:59:25.749348 11101 solver.cpp:312] Iteration 26600 (6.8657 iter/s, 14.5652s/100 iter), loss = 2.3242
I0814 20:59:25.749377 11101 solver.cpp:334]     Train net output #0: loss = 2.27055 (* 1 = 2.27055 loss)
I0814 20:59:25.749383 11101 sgd_solver.cpp:136] Iteration 26600, lr = 0.0916875, m = 0.9
I0814 20:59:40.283870 11101 solver.cpp:312] Iteration 26700 (6.88037 iter/s, 14.5341s/100 iter), loss = 2.403
I0814 20:59:40.283949 11101 solver.cpp:334]     Train net output #0: loss = 2.36056 (* 1 = 2.36056 loss)
I0814 20:59:40.283957 11101 sgd_solver.cpp:136] Iteration 26700, lr = 0.0916563, m = 0.9
I0814 20:59:54.764966 11101 solver.cpp:312] Iteration 26800 (6.90575 iter/s, 14.4807s/100 iter), loss = 2.32899
I0814 20:59:54.765033 11101 solver.cpp:334]     Train net output #0: loss = 2.11622 (* 1 = 2.11622 loss)
I0814 20:59:54.765051 11101 sgd_solver.cpp:136] Iteration 26800, lr = 0.091625, m = 0.9
I0814 21:00:09.099805 11101 solver.cpp:312] Iteration 26900 (6.97621 iter/s, 14.3344s/100 iter), loss = 2.43981
I0814 21:00:09.099834 11101 solver.cpp:334]     Train net output #0: loss = 2.55852 (* 1 = 2.55852 loss)
I0814 21:00:09.099840 11101 sgd_solver.cpp:136] Iteration 26900, lr = 0.0915937, m = 0.9
I0814 21:00:23.669260 11101 solver.cpp:312] Iteration 27000 (6.86387 iter/s, 14.569s/100 iter), loss = 2.33897
I0814 21:00:23.669317 11101 solver.cpp:334]     Train net output #0: loss = 2.34121 (* 1 = 2.34121 loss)
I0814 21:00:23.669322 11101 sgd_solver.cpp:136] Iteration 27000, lr = 0.0915625, m = 0.9
I0814 21:00:38.061352 11101 solver.cpp:312] Iteration 27100 (6.94846 iter/s, 14.3917s/100 iter), loss = 2.30202
I0814 21:00:38.061377 11101 solver.cpp:334]     Train net output #0: loss = 2.59921 (* 1 = 2.59921 loss)
I0814 21:00:38.061380 11101 sgd_solver.cpp:136] Iteration 27100, lr = 0.0915313, m = 0.9
I0814 21:00:52.536485 11101 solver.cpp:312] Iteration 27200 (6.9086 iter/s, 14.4747s/100 iter), loss = 2.60137
I0814 21:00:52.536509 11101 solver.cpp:334]     Train net output #0: loss = 2.45363 (* 1 = 2.45363 loss)
I0814 21:00:52.536512 11101 sgd_solver.cpp:136] Iteration 27200, lr = 0.0915, m = 0.9
I0814 21:01:07.314802 11101 solver.cpp:312] Iteration 27300 (6.76687 iter/s, 14.7779s/100 iter), loss = 2.72982
I0814 21:01:07.314872 11101 solver.cpp:334]     Train net output #0: loss = 2.83431 (* 1 = 2.83431 loss)
I0814 21:01:07.314878 11101 sgd_solver.cpp:136] Iteration 27300, lr = 0.0914688, m = 0.9
I0814 21:01:22.139550 11101 solver.cpp:312] Iteration 27400 (6.74567 iter/s, 14.8243s/100 iter), loss = 2.05731
I0814 21:01:22.139580 11101 solver.cpp:334]     Train net output #0: loss = 1.70913 (* 1 = 1.70913 loss)
I0814 21:01:22.139586 11101 sgd_solver.cpp:136] Iteration 27400, lr = 0.0914375, m = 0.9
I0814 21:01:36.798840 11101 solver.cpp:312] Iteration 27500 (6.82181 iter/s, 14.6589s/100 iter), loss = 2.37288
I0814 21:01:36.798869 11101 solver.cpp:334]     Train net output #0: loss = 2.60692 (* 1 = 2.60692 loss)
I0814 21:01:36.798876 11101 sgd_solver.cpp:136] Iteration 27500, lr = 0.0914062, m = 0.9
I0814 21:01:51.053180 11101 solver.cpp:312] Iteration 27600 (7.01561 iter/s, 14.2539s/100 iter), loss = 2.59906
I0814 21:01:51.053288 11101 solver.cpp:334]     Train net output #0: loss = 2.79095 (* 1 = 2.79095 loss)
I0814 21:01:51.053308 11101 sgd_solver.cpp:136] Iteration 27600, lr = 0.091375, m = 0.9
I0814 21:02:05.791398 11101 solver.cpp:312] Iteration 27700 (6.78528 iter/s, 14.7378s/100 iter), loss = 2.21182
I0814 21:02:05.791419 11101 solver.cpp:334]     Train net output #0: loss = 2.15147 (* 1 = 2.15147 loss)
I0814 21:02:05.791425 11101 sgd_solver.cpp:136] Iteration 27700, lr = 0.0913438, m = 0.9
I0814 21:02:20.619467 11101 solver.cpp:312] Iteration 27800 (6.74416 iter/s, 14.8276s/100 iter), loss = 2.19602
I0814 21:02:20.619494 11101 solver.cpp:334]     Train net output #0: loss = 2.34763 (* 1 = 2.34763 loss)
I0814 21:02:20.619500 11101 sgd_solver.cpp:136] Iteration 27800, lr = 0.0913125, m = 0.9
I0814 21:02:35.054890 11101 solver.cpp:312] Iteration 27900 (6.9276 iter/s, 14.435s/100 iter), loss = 2.19371
I0814 21:02:35.055003 11101 solver.cpp:334]     Train net output #0: loss = 2.01814 (* 1 = 2.01814 loss)
I0814 21:02:35.055022 11101 sgd_solver.cpp:136] Iteration 27900, lr = 0.0912813, m = 0.9
I0814 21:02:49.562721 11101 solver.cpp:509] Iteration 28000, Testing net (#0)
I0814 21:03:10.273697 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.338117
I0814 21:03:10.273774 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.601176
I0814 21:03:10.273783 11101 solver.cpp:594]     Test net output #2: loss = 3.13014 (* 1 = 3.13014 loss)
I0814 21:03:10.273807 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7105s
I0814 21:03:10.448923 11101 solver.cpp:312] Iteration 28000 (2.82542 iter/s, 35.393s/100 iter), loss = 2.47515
I0814 21:03:10.448947 11101 solver.cpp:334]     Train net output #0: loss = 2.41998 (* 1 = 2.41998 loss)
I0814 21:03:10.448951 11101 sgd_solver.cpp:136] Iteration 28000, lr = 0.09125, m = 0.9
I0814 21:03:25.014729 11101 solver.cpp:312] Iteration 28100 (6.86559 iter/s, 14.5654s/100 iter), loss = 2.42271
I0814 21:03:25.014752 11101 solver.cpp:334]     Train net output #0: loss = 1.96629 (* 1 = 1.96629 loss)
I0814 21:03:25.014782 11101 sgd_solver.cpp:136] Iteration 28100, lr = 0.0912188, m = 0.9
I0814 21:03:39.827719 11101 solver.cpp:312] Iteration 28200 (6.75103 iter/s, 14.8126s/100 iter), loss = 2.35722
I0814 21:03:39.827770 11101 solver.cpp:334]     Train net output #0: loss = 2.92349 (* 1 = 2.92349 loss)
I0814 21:03:39.827783 11101 sgd_solver.cpp:136] Iteration 28200, lr = 0.0911875, m = 0.9
I0814 21:03:54.458984 11101 solver.cpp:312] Iteration 28300 (6.83488 iter/s, 14.6308s/100 iter), loss = 2.0939
I0814 21:03:54.459049 11101 solver.cpp:334]     Train net output #0: loss = 2.10664 (* 1 = 2.10664 loss)
I0814 21:03:54.459055 11101 sgd_solver.cpp:136] Iteration 28300, lr = 0.0911563, m = 0.9
I0814 21:04:09.495769 11101 solver.cpp:312] Iteration 28400 (6.65055 iter/s, 15.0363s/100 iter), loss = 2.36517
I0814 21:04:09.495805 11101 solver.cpp:334]     Train net output #0: loss = 2.03484 (* 1 = 2.03484 loss)
I0814 21:04:09.495813 11101 sgd_solver.cpp:136] Iteration 28400, lr = 0.091125, m = 0.9
I0814 21:04:24.387370 11101 solver.cpp:312] Iteration 28500 (6.71539 iter/s, 14.8912s/100 iter), loss = 2.53316
I0814 21:04:24.387403 11101 solver.cpp:334]     Train net output #0: loss = 2.16007 (* 1 = 2.16007 loss)
I0814 21:04:24.387408 11101 sgd_solver.cpp:136] Iteration 28500, lr = 0.0910937, m = 0.9
I0814 21:04:39.137718 11101 solver.cpp:312] Iteration 28600 (6.7797 iter/s, 14.7499s/100 iter), loss = 2.12402
I0814 21:04:39.137780 11101 solver.cpp:334]     Train net output #0: loss = 2.11964 (* 1 = 2.11964 loss)
I0814 21:04:39.137787 11101 sgd_solver.cpp:136] Iteration 28600, lr = 0.0910625, m = 0.9
I0814 21:04:53.741601 11101 solver.cpp:312] Iteration 28700 (6.84769 iter/s, 14.6035s/100 iter), loss = 2.52412
I0814 21:04:53.741629 11101 solver.cpp:334]     Train net output #0: loss = 2.77275 (* 1 = 2.77275 loss)
I0814 21:04:53.741636 11101 sgd_solver.cpp:136] Iteration 28700, lr = 0.0910313, m = 0.9
I0814 21:05:08.212157 11101 solver.cpp:312] Iteration 28800 (6.91078 iter/s, 14.4701s/100 iter), loss = 2.58981
I0814 21:05:08.212183 11101 solver.cpp:334]     Train net output #0: loss = 2.63737 (* 1 = 2.63737 loss)
I0814 21:05:08.212188 11101 sgd_solver.cpp:136] Iteration 28800, lr = 0.091, m = 0.9
I0814 21:05:22.546075 11101 solver.cpp:312] Iteration 28900 (6.97666 iter/s, 14.3335s/100 iter), loss = 1.97533
I0814 21:05:22.546154 11101 solver.cpp:334]     Train net output #0: loss = 1.92957 (* 1 = 1.92957 loss)
I0814 21:05:22.546161 11101 sgd_solver.cpp:136] Iteration 28900, lr = 0.0909688, m = 0.9
I0814 21:05:36.959619 11101 solver.cpp:312] Iteration 29000 (6.93812 iter/s, 14.4131s/100 iter), loss = 1.93605
I0814 21:05:36.959645 11101 solver.cpp:334]     Train net output #0: loss = 1.99491 (* 1 = 1.99491 loss)
I0814 21:05:36.959650 11101 sgd_solver.cpp:136] Iteration 29000, lr = 0.0909375, m = 0.9
I0814 21:05:51.449537 11101 solver.cpp:312] Iteration 29100 (6.90155 iter/s, 14.4895s/100 iter), loss = 2.24921
I0814 21:05:51.449570 11101 solver.cpp:334]     Train net output #0: loss = 2.02007 (* 1 = 2.02007 loss)
I0814 21:05:51.449576 11101 sgd_solver.cpp:136] Iteration 29100, lr = 0.0909063, m = 0.9
I0814 21:06:05.960381 11101 solver.cpp:312] Iteration 29200 (6.8916 iter/s, 14.5104s/100 iter), loss = 2.51847
I0814 21:06:05.960450 11101 solver.cpp:334]     Train net output #0: loss = 2.0933 (* 1 = 2.0933 loss)
I0814 21:06:05.960456 11101 sgd_solver.cpp:136] Iteration 29200, lr = 0.090875, m = 0.9
I0814 21:06:20.503545 11101 solver.cpp:312] Iteration 29300 (6.87628 iter/s, 14.5428s/100 iter), loss = 2.77727
I0814 21:06:20.503572 11101 solver.cpp:334]     Train net output #0: loss = 2.42031 (* 1 = 2.42031 loss)
I0814 21:06:20.503579 11101 sgd_solver.cpp:136] Iteration 29300, lr = 0.0908438, m = 0.9
I0814 21:06:35.000244 11101 solver.cpp:312] Iteration 29400 (6.89832 iter/s, 14.4963s/100 iter), loss = 2.18325
I0814 21:06:35.000268 11101 solver.cpp:334]     Train net output #0: loss = 1.84058 (* 1 = 1.84058 loss)
I0814 21:06:35.000274 11101 sgd_solver.cpp:136] Iteration 29400, lr = 0.0908125, m = 0.9
I0814 21:06:49.715687 11101 solver.cpp:312] Iteration 29500 (6.79578 iter/s, 14.715s/100 iter), loss = 1.98041
I0814 21:06:49.715749 11101 solver.cpp:334]     Train net output #0: loss = 1.7869 (* 1 = 1.7869 loss)
I0814 21:06:49.715754 11101 sgd_solver.cpp:136] Iteration 29500, lr = 0.0907812, m = 0.9
I0814 21:07:04.520951 11101 solver.cpp:312] Iteration 29600 (6.75455 iter/s, 14.8048s/100 iter), loss = 2.09866
I0814 21:07:04.521016 11101 solver.cpp:334]     Train net output #0: loss = 2.67925 (* 1 = 2.67925 loss)
I0814 21:07:04.521034 11101 sgd_solver.cpp:136] Iteration 29600, lr = 0.09075, m = 0.9
I0814 21:07:19.488512 11101 solver.cpp:312] Iteration 29700 (6.68131 iter/s, 14.9671s/100 iter), loss = 2.2964
I0814 21:07:19.488543 11101 solver.cpp:334]     Train net output #0: loss = 1.78537 (* 1 = 1.78537 loss)
I0814 21:07:19.488549 11101 sgd_solver.cpp:136] Iteration 29700, lr = 0.0907188, m = 0.9
I0814 21:07:34.288633 11101 solver.cpp:312] Iteration 29800 (6.75689 iter/s, 14.7997s/100 iter), loss = 2.42031
I0814 21:07:34.288686 11101 solver.cpp:334]     Train net output #0: loss = 2.40903 (* 1 = 2.40903 loss)
I0814 21:07:34.288691 11101 sgd_solver.cpp:136] Iteration 29800, lr = 0.0906875, m = 0.9
I0814 21:07:48.810235 11101 solver.cpp:312] Iteration 29900 (6.88649 iter/s, 14.5212s/100 iter), loss = 2.27395
I0814 21:07:48.810286 11101 solver.cpp:334]     Train net output #0: loss = 2.39523 (* 1 = 2.39523 loss)
I0814 21:07:48.810297 11101 sgd_solver.cpp:136] Iteration 29900, lr = 0.0906563, m = 0.9
I0814 21:08:03.354305 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_30000.caffemodel
I0814 21:08:03.368290 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_30000.solverstate
I0814 21:08:03.372548 11101 solver.cpp:509] Iteration 30000, Testing net (#0)
I0814 21:08:14.513072 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 21:08:17.666182 11099 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 21:08:24.174619 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.32394
I0814 21:08:24.174639 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.574294
I0814 21:08:24.174645 11101 solver.cpp:594]     Test net output #2: loss = 3.2981 (* 1 = 3.2981 loss)
I0814 21:08:24.174664 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8015s
I0814 21:08:24.330905 11101 solver.cpp:312] Iteration 30000 (2.81534 iter/s, 35.5197s/100 iter), loss = 2.42341
I0814 21:08:24.330932 11101 solver.cpp:334]     Train net output #0: loss = 2.594 (* 1 = 2.594 loss)
I0814 21:08:24.330938 11101 sgd_solver.cpp:136] Iteration 30000, lr = 0.090625, m = 0.9
I0814 21:08:38.724140 11101 solver.cpp:312] Iteration 30100 (6.94791 iter/s, 14.3928s/100 iter), loss = 2.39998
I0814 21:08:38.724189 11101 solver.cpp:334]     Train net output #0: loss = 2.91896 (* 1 = 2.91896 loss)
I0814 21:08:38.724202 11101 sgd_solver.cpp:136] Iteration 30100, lr = 0.0905937, m = 0.9
I0814 21:08:53.081408 11101 solver.cpp:312] Iteration 30200 (6.96531 iter/s, 14.3569s/100 iter), loss = 2.42566
I0814 21:08:53.081600 11101 solver.cpp:334]     Train net output #0: loss = 2.24561 (* 1 = 2.24561 loss)
I0814 21:08:53.081619 11101 sgd_solver.cpp:136] Iteration 30200, lr = 0.0905625, m = 0.9
I0814 21:09:07.729738 11101 solver.cpp:312] Iteration 30300 (6.82691 iter/s, 14.6479s/100 iter), loss = 2.09662
I0814 21:09:07.729765 11101 solver.cpp:334]     Train net output #0: loss = 2.32546 (* 1 = 2.32546 loss)
I0814 21:09:07.729771 11101 sgd_solver.cpp:136] Iteration 30300, lr = 0.0905313, m = 0.9
I0814 21:09:22.185580 11101 solver.cpp:312] Iteration 30400 (6.91782 iter/s, 14.4554s/100 iter), loss = 2.49864
I0814 21:09:22.185652 11101 solver.cpp:334]     Train net output #0: loss = 2.45046 (* 1 = 2.45046 loss)
I0814 21:09:22.185672 11101 sgd_solver.cpp:136] Iteration 30400, lr = 0.0905, m = 0.9
I0814 21:09:36.909384 11101 solver.cpp:312] Iteration 30500 (6.79192 iter/s, 14.7234s/100 iter), loss = 2.11093
I0814 21:09:36.909559 11101 solver.cpp:334]     Train net output #0: loss = 1.96361 (* 1 = 1.96361 loss)
I0814 21:09:36.909644 11101 sgd_solver.cpp:136] Iteration 30500, lr = 0.0904688, m = 0.9
I0814 21:09:51.331090 11101 solver.cpp:312] Iteration 30600 (6.93419 iter/s, 14.4213s/100 iter), loss = 2.53436
I0814 21:09:51.331158 11101 solver.cpp:334]     Train net output #0: loss = 2.51473 (* 1 = 2.51473 loss)
I0814 21:09:51.331178 11101 sgd_solver.cpp:136] Iteration 30600, lr = 0.0904375, m = 0.9
I0814 21:10:06.032716 11101 solver.cpp:312] Iteration 30700 (6.80216 iter/s, 14.7012s/100 iter), loss = 2.71512
I0814 21:10:06.032737 11101 solver.cpp:334]     Train net output #0: loss = 2.76387 (* 1 = 2.76387 loss)
I0814 21:10:06.032742 11101 sgd_solver.cpp:136] Iteration 30700, lr = 0.0904063, m = 0.9
I0814 21:10:20.603463 11101 solver.cpp:312] Iteration 30800 (6.86327 iter/s, 14.5703s/100 iter), loss = 2.71818
I0814 21:10:20.603564 11101 solver.cpp:334]     Train net output #0: loss = 2.92304 (* 1 = 2.92304 loss)
I0814 21:10:20.603582 11101 sgd_solver.cpp:136] Iteration 30800, lr = 0.090375, m = 0.9
I0814 21:10:35.586729 11101 solver.cpp:312] Iteration 30900 (6.6743 iter/s, 14.9828s/100 iter), loss = 2.51752
I0814 21:10:35.586756 11101 solver.cpp:334]     Train net output #0: loss = 2.46794 (* 1 = 2.46794 loss)
I0814 21:10:35.586761 11101 sgd_solver.cpp:136] Iteration 30900, lr = 0.0903438, m = 0.9
I0814 21:10:50.716979 11101 solver.cpp:312] Iteration 31000 (6.60947 iter/s, 15.1298s/100 iter), loss = 2.3919
I0814 21:10:50.717036 11101 solver.cpp:334]     Train net output #0: loss = 2.40814 (* 1 = 2.40814 loss)
I0814 21:10:50.717042 11101 sgd_solver.cpp:136] Iteration 31000, lr = 0.0903125, m = 0.9
I0814 21:11:06.010375 11101 solver.cpp:312] Iteration 31100 (6.53896 iter/s, 15.293s/100 iter), loss = 2.18345
I0814 21:11:06.010403 11101 solver.cpp:334]     Train net output #0: loss = 1.9936 (* 1 = 1.9936 loss)
I0814 21:11:06.010411 11101 sgd_solver.cpp:136] Iteration 31100, lr = 0.0902812, m = 0.9
I0814 21:11:20.993329 11101 solver.cpp:312] Iteration 31200 (6.67444 iter/s, 14.9825s/100 iter), loss = 2.37284
I0814 21:11:20.993387 11101 solver.cpp:334]     Train net output #0: loss = 2.06988 (* 1 = 2.06988 loss)
I0814 21:11:20.993392 11101 sgd_solver.cpp:136] Iteration 31200, lr = 0.09025, m = 0.9
I0814 21:11:35.857736 11101 solver.cpp:312] Iteration 31300 (6.72767 iter/s, 14.864s/100 iter), loss = 2.4657
I0814 21:11:35.857803 11101 solver.cpp:334]     Train net output #0: loss = 2.33428 (* 1 = 2.33428 loss)
I0814 21:11:35.857821 11101 sgd_solver.cpp:136] Iteration 31300, lr = 0.0902187, m = 0.9
I0814 21:11:50.594094 11101 solver.cpp:312] Iteration 31400 (6.78613 iter/s, 14.7359s/100 iter), loss = 2.17086
I0814 21:11:50.594122 11101 solver.cpp:334]     Train net output #0: loss = 2.06304 (* 1 = 2.06304 loss)
I0814 21:11:50.594126 11101 sgd_solver.cpp:136] Iteration 31400, lr = 0.0901875, m = 0.9
I0814 21:12:05.276073 11101 solver.cpp:312] Iteration 31500 (6.81126 iter/s, 14.6816s/100 iter), loss = 2.62513
I0814 21:12:05.276195 11101 solver.cpp:334]     Train net output #0: loss = 2.6515 (* 1 = 2.6515 loss)
I0814 21:12:05.276216 11101 sgd_solver.cpp:136] Iteration 31500, lr = 0.0901562, m = 0.9
I0814 21:12:19.836156 11101 solver.cpp:312] Iteration 31600 (6.86829 iter/s, 14.5597s/100 iter), loss = 2.0291
I0814 21:12:19.836187 11101 solver.cpp:334]     Train net output #0: loss = 2.0786 (* 1 = 2.0786 loss)
I0814 21:12:19.836194 11101 sgd_solver.cpp:136] Iteration 31600, lr = 0.090125, m = 0.9
I0814 21:12:34.767570 11101 solver.cpp:312] Iteration 31700 (6.69748 iter/s, 14.931s/100 iter), loss = 2.64355
I0814 21:12:34.767599 11101 solver.cpp:334]     Train net output #0: loss = 2.56892 (* 1 = 2.56892 loss)
I0814 21:12:34.767606 11101 sgd_solver.cpp:136] Iteration 31700, lr = 0.0900938, m = 0.9
I0814 21:12:49.463538 11101 solver.cpp:312] Iteration 31800 (6.80478 iter/s, 14.6955s/100 iter), loss = 2.3198
I0814 21:12:49.474907 11101 solver.cpp:334]     Train net output #0: loss = 2.46317 (* 1 = 2.46317 loss)
I0814 21:12:49.474936 11101 sgd_solver.cpp:136] Iteration 31800, lr = 0.0900625, m = 0.9
I0814 21:13:03.983140 11101 solver.cpp:312] Iteration 31900 (6.88744 iter/s, 14.5192s/100 iter), loss = 2.52758
I0814 21:13:03.983170 11101 solver.cpp:334]     Train net output #0: loss = 2.38719 (* 1 = 2.38719 loss)
I0814 21:13:03.983176 11101 sgd_solver.cpp:136] Iteration 31900, lr = 0.0900313, m = 0.9
I0814 21:13:18.584161 11101 solver.cpp:509] Iteration 32000, Testing net (#0)
I0814 21:13:39.505761 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.346411
I0814 21:13:39.505827 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.597058
I0814 21:13:39.505836 11101 solver.cpp:594]     Test net output #2: loss = 3.16124 (* 1 = 3.16124 loss)
I0814 21:13:39.505856 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9211s
I0814 21:13:39.657120 11101 solver.cpp:312] Iteration 32000 (2.80324 iter/s, 35.673s/100 iter), loss = 2.37765
I0814 21:13:39.657146 11101 solver.cpp:334]     Train net output #0: loss = 1.83065 (* 1 = 1.83065 loss)
I0814 21:13:39.657151 11101 sgd_solver.cpp:136] Iteration 32000, lr = 0.09, m = 0.9
I0814 21:13:54.212427 11101 solver.cpp:312] Iteration 32100 (6.87055 iter/s, 14.5549s/100 iter), loss = 2.10572
I0814 21:13:54.212455 11101 solver.cpp:334]     Train net output #0: loss = 2.31924 (* 1 = 2.31924 loss)
I0814 21:13:54.212460 11101 sgd_solver.cpp:136] Iteration 32100, lr = 0.0899688, m = 0.9
I0814 21:14:08.799546 11101 solver.cpp:312] Iteration 32200 (6.85556 iter/s, 14.5867s/100 iter), loss = 2.4851
I0814 21:14:08.799568 11101 solver.cpp:334]     Train net output #0: loss = 2.80611 (* 1 = 2.80611 loss)
I0814 21:14:08.799573 11101 sgd_solver.cpp:136] Iteration 32200, lr = 0.0899375, m = 0.9
I0814 21:14:23.818658 11101 solver.cpp:312] Iteration 32300 (6.65838 iter/s, 15.0187s/100 iter), loss = 2.25879
I0814 21:14:23.818727 11101 solver.cpp:334]     Train net output #0: loss = 2.10354 (* 1 = 2.10354 loss)
I0814 21:14:23.818734 11101 sgd_solver.cpp:136] Iteration 32300, lr = 0.0899063, m = 0.9
I0814 21:14:38.707072 11101 solver.cpp:312] Iteration 32400 (6.71682 iter/s, 14.888s/100 iter), loss = 2.32771
I0814 21:14:38.707095 11101 solver.cpp:334]     Train net output #0: loss = 2.35828 (* 1 = 2.35828 loss)
I0814 21:14:38.707101 11101 sgd_solver.cpp:136] Iteration 32400, lr = 0.089875, m = 0.9
I0814 21:14:53.449911 11101 solver.cpp:312] Iteration 32500 (6.78315 iter/s, 14.7424s/100 iter), loss = 2.31599
I0814 21:14:53.449942 11101 solver.cpp:334]     Train net output #0: loss = 2.66248 (* 1 = 2.66248 loss)
I0814 21:14:53.449949 11101 sgd_solver.cpp:136] Iteration 32500, lr = 0.0898438, m = 0.9
I0814 21:15:08.139340 11101 solver.cpp:312] Iteration 32600 (6.80781 iter/s, 14.689s/100 iter), loss = 2.55935
I0814 21:15:08.139407 11101 solver.cpp:334]     Train net output #0: loss = 3.09617 (* 1 = 3.09617 loss)
I0814 21:15:08.139415 11101 sgd_solver.cpp:136] Iteration 32600, lr = 0.0898125, m = 0.9
I0814 21:15:22.712287 11101 solver.cpp:312] Iteration 32700 (6.86223 iter/s, 14.5725s/100 iter), loss = 2.1454
I0814 21:15:22.712311 11101 solver.cpp:334]     Train net output #0: loss = 2.42623 (* 1 = 2.42623 loss)
I0814 21:15:22.712316 11101 sgd_solver.cpp:136] Iteration 32700, lr = 0.0897812, m = 0.9
I0814 21:15:37.484518 11101 solver.cpp:312] Iteration 32800 (6.76965 iter/s, 14.7718s/100 iter), loss = 2.0443
I0814 21:15:37.484741 11101 solver.cpp:334]     Train net output #0: loss = 2.0898 (* 1 = 2.0898 loss)
I0814 21:15:37.484850 11101 sgd_solver.cpp:136] Iteration 32800, lr = 0.08975, m = 0.9
I0814 21:15:52.480233 11101 solver.cpp:312] Iteration 32900 (6.66876 iter/s, 14.9953s/100 iter), loss = 2.87778
I0814 21:15:52.480324 11101 solver.cpp:334]     Train net output #0: loss = 3.11527 (* 1 = 3.11527 loss)
I0814 21:15:52.480337 11101 sgd_solver.cpp:136] Iteration 32900, lr = 0.0897188, m = 0.9
I0814 21:16:07.021672 11101 solver.cpp:312] Iteration 33000 (6.8771 iter/s, 14.541s/100 iter), loss = 2.66979
I0814 21:16:07.021726 11101 solver.cpp:334]     Train net output #0: loss = 2.3399 (* 1 = 2.3399 loss)
I0814 21:16:07.021740 11101 sgd_solver.cpp:136] Iteration 33000, lr = 0.0896875, m = 0.9
I0814 21:16:21.696370 11101 solver.cpp:312] Iteration 33100 (6.81465 iter/s, 14.6743s/100 iter), loss = 2.43047
I0814 21:16:21.696399 11101 solver.cpp:334]     Train net output #0: loss = 2.68199 (* 1 = 2.68199 loss)
I0814 21:16:21.696406 11101 sgd_solver.cpp:136] Iteration 33100, lr = 0.0896563, m = 0.9
I0814 21:16:36.652885 11101 solver.cpp:312] Iteration 33200 (6.68624 iter/s, 14.9561s/100 iter), loss = 2.25144
I0814 21:16:36.652988 11101 solver.cpp:334]     Train net output #0: loss = 1.94315 (* 1 = 1.94315 loss)
I0814 21:16:36.653007 11101 sgd_solver.cpp:136] Iteration 33200, lr = 0.089625, m = 0.9
I0814 21:16:51.614797 11101 solver.cpp:312] Iteration 33300 (6.68383 iter/s, 14.9615s/100 iter), loss = 2.52516
I0814 21:16:51.614866 11101 solver.cpp:334]     Train net output #0: loss = 2.35494 (* 1 = 2.35494 loss)
I0814 21:16:51.614883 11101 sgd_solver.cpp:136] Iteration 33300, lr = 0.0895938, m = 0.9
I0814 21:17:06.279997 11101 solver.cpp:312] Iteration 33400 (6.81906 iter/s, 14.6648s/100 iter), loss = 2.54336
I0814 21:17:06.280053 11101 solver.cpp:334]     Train net output #0: loss = 2.14456 (* 1 = 2.14456 loss)
I0814 21:17:06.280066 11101 sgd_solver.cpp:136] Iteration 33400, lr = 0.0895625, m = 0.9
I0814 21:17:20.898509 11101 solver.cpp:312] Iteration 33500 (6.84084 iter/s, 14.6181s/100 iter), loss = 2.57146
I0814 21:17:20.898569 11101 solver.cpp:334]     Train net output #0: loss = 2.30659 (* 1 = 2.30659 loss)
I0814 21:17:20.898578 11101 sgd_solver.cpp:136] Iteration 33500, lr = 0.0895313, m = 0.9
I0814 21:17:35.529675 11101 solver.cpp:312] Iteration 33600 (6.83492 iter/s, 14.6307s/100 iter), loss = 2.49005
I0814 21:17:35.529703 11101 solver.cpp:334]     Train net output #0: loss = 2.65112 (* 1 = 2.65112 loss)
I0814 21:17:35.529709 11101 sgd_solver.cpp:136] Iteration 33600, lr = 0.0895, m = 0.9
I0814 21:17:50.345134 11101 solver.cpp:312] Iteration 33700 (6.7499 iter/s, 14.815s/100 iter), loss = 2.28967
I0814 21:17:50.345165 11101 solver.cpp:334]     Train net output #0: loss = 2.66754 (* 1 = 2.66754 loss)
I0814 21:17:50.345170 11101 sgd_solver.cpp:136] Iteration 33700, lr = 0.0894688, m = 0.9
I0814 21:18:05.178375 11101 solver.cpp:312] Iteration 33800 (6.74181 iter/s, 14.8328s/100 iter), loss = 2.67862
I0814 21:18:05.178565 11101 solver.cpp:334]     Train net output #0: loss = 2.99314 (* 1 = 2.99314 loss)
I0814 21:18:05.178639 11101 sgd_solver.cpp:136] Iteration 33800, lr = 0.0894375, m = 0.9
I0814 21:18:20.165807 11101 solver.cpp:312] Iteration 33900 (6.67245 iter/s, 14.987s/100 iter), loss = 2.24799
I0814 21:18:20.165829 11101 solver.cpp:334]     Train net output #0: loss = 2.18324 (* 1 = 2.18324 loss)
I0814 21:18:20.165834 11101 sgd_solver.cpp:136] Iteration 33900, lr = 0.0894063, m = 0.9
I0814 21:18:35.124151 11101 solver.cpp:509] Iteration 34000, Testing net (#0)
I0814 21:18:56.295331 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.336058
I0814 21:18:56.295456 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.589235
I0814 21:18:56.295465 11101 solver.cpp:594]     Test net output #2: loss = 3.19395 (* 1 = 3.19395 loss)
I0814 21:18:56.295483 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1707s
I0814 21:18:56.453483 11101 solver.cpp:312] Iteration 34000 (2.75583 iter/s, 36.2866s/100 iter), loss = 2.73471
I0814 21:18:56.453546 11101 solver.cpp:334]     Train net output #0: loss = 2.53955 (* 1 = 2.53955 loss)
I0814 21:18:56.453563 11101 sgd_solver.cpp:136] Iteration 34000, lr = 0.089375, m = 0.9
I0814 21:19:11.081861 11101 solver.cpp:312] Iteration 34100 (6.83623 iter/s, 14.628s/100 iter), loss = 2.31292
I0814 21:19:11.081928 11101 solver.cpp:334]     Train net output #0: loss = 2.75292 (* 1 = 2.75292 loss)
I0814 21:19:11.081949 11101 sgd_solver.cpp:136] Iteration 34100, lr = 0.0893437, m = 0.9
I0814 21:19:25.460006 11101 solver.cpp:312] Iteration 34200 (6.9552 iter/s, 14.3777s/100 iter), loss = 2.33041
I0814 21:19:25.460036 11101 solver.cpp:334]     Train net output #0: loss = 1.8569 (* 1 = 1.8569 loss)
I0814 21:19:25.460042 11101 sgd_solver.cpp:136] Iteration 34200, lr = 0.0893125, m = 0.9
I0814 21:19:39.893959 11101 solver.cpp:312] Iteration 34300 (6.92831 iter/s, 14.4335s/100 iter), loss = 2.35272
I0814 21:19:39.894074 11101 solver.cpp:334]     Train net output #0: loss = 2.30548 (* 1 = 2.30548 loss)
I0814 21:19:39.894094 11101 sgd_solver.cpp:136] Iteration 34300, lr = 0.0892813, m = 0.9
I0814 21:19:54.254690 11101 solver.cpp:312] Iteration 34400 (6.96363 iter/s, 14.3603s/100 iter), loss = 2.33008
I0814 21:19:54.254717 11101 solver.cpp:334]     Train net output #0: loss = 2.20968 (* 1 = 2.20968 loss)
I0814 21:19:54.254721 11101 sgd_solver.cpp:136] Iteration 34400, lr = 0.08925, m = 0.9
I0814 21:20:08.759745 11101 solver.cpp:312] Iteration 34500 (6.89435 iter/s, 14.5046s/100 iter), loss = 2.22166
I0814 21:20:08.759773 11101 solver.cpp:334]     Train net output #0: loss = 2.39198 (* 1 = 2.39198 loss)
I0814 21:20:08.759778 11101 sgd_solver.cpp:136] Iteration 34500, lr = 0.0892188, m = 0.9
I0814 21:20:23.131078 11101 solver.cpp:312] Iteration 34600 (6.9585 iter/s, 14.3709s/100 iter), loss = 2.14029
I0814 21:20:23.131155 11101 solver.cpp:334]     Train net output #0: loss = 1.95329 (* 1 = 1.95329 loss)
I0814 21:20:23.131168 11101 sgd_solver.cpp:136] Iteration 34600, lr = 0.0891875, m = 0.9
I0814 21:20:37.445502 11101 solver.cpp:312] Iteration 34700 (6.98616 iter/s, 14.314s/100 iter), loss = 2.10882
I0814 21:20:37.445554 11101 solver.cpp:334]     Train net output #0: loss = 2.27241 (* 1 = 2.27241 loss)
I0814 21:20:37.445569 11101 sgd_solver.cpp:136] Iteration 34700, lr = 0.0891563, m = 0.9
I0814 21:20:51.754345 11101 solver.cpp:312] Iteration 34800 (6.98889 iter/s, 14.3084s/100 iter), loss = 2.32838
I0814 21:20:51.754372 11101 solver.cpp:334]     Train net output #0: loss = 2.63561 (* 1 = 2.63561 loss)
I0814 21:20:51.754379 11101 sgd_solver.cpp:136] Iteration 34800, lr = 0.089125, m = 0.9
I0814 21:21:06.353675 11101 solver.cpp:312] Iteration 34900 (6.84982 iter/s, 14.5989s/100 iter), loss = 2.16788
I0814 21:21:06.353763 11101 solver.cpp:334]     Train net output #0: loss = 2.3692 (* 1 = 2.3692 loss)
I0814 21:21:06.353780 11101 sgd_solver.cpp:136] Iteration 34900, lr = 0.0890938, m = 0.9
I0814 21:21:20.823840 11101 solver.cpp:312] Iteration 35000 (6.91097 iter/s, 14.4698s/100 iter), loss = 2.36021
I0814 21:21:20.823868 11101 solver.cpp:334]     Train net output #0: loss = 2.01999 (* 1 = 2.01999 loss)
I0814 21:21:20.823873 11101 sgd_solver.cpp:136] Iteration 35000, lr = 0.0890625, m = 0.9
I0814 21:21:35.216039 11101 solver.cpp:312] Iteration 35100 (6.94841 iter/s, 14.3918s/100 iter), loss = 2.03836
I0814 21:21:35.216104 11101 solver.cpp:334]     Train net output #0: loss = 2.59366 (* 1 = 2.59366 loss)
I0814 21:21:35.216120 11101 sgd_solver.cpp:136] Iteration 35100, lr = 0.0890312, m = 0.9
I0814 21:21:49.783087 11101 solver.cpp:312] Iteration 35200 (6.86501 iter/s, 14.5666s/100 iter), loss = 2.32548
I0814 21:21:49.783167 11101 solver.cpp:334]     Train net output #0: loss = 2.44502 (* 1 = 2.44502 loss)
I0814 21:21:49.783174 11101 sgd_solver.cpp:136] Iteration 35200, lr = 0.089, m = 0.9
I0814 21:22:04.206799 11101 solver.cpp:312] Iteration 35300 (6.93323 iter/s, 14.4233s/100 iter), loss = 2.68992
I0814 21:22:04.206822 11101 solver.cpp:334]     Train net output #0: loss = 1.96956 (* 1 = 1.96956 loss)
I0814 21:22:04.206826 11101 sgd_solver.cpp:136] Iteration 35300, lr = 0.0889687, m = 0.9
I0814 21:22:18.992169 11101 solver.cpp:312] Iteration 35400 (6.76364 iter/s, 14.7849s/100 iter), loss = 2.15681
I0814 21:22:18.992245 11101 solver.cpp:334]     Train net output #0: loss = 2.03962 (* 1 = 2.03962 loss)
I0814 21:22:18.992265 11101 sgd_solver.cpp:136] Iteration 35400, lr = 0.0889375, m = 0.9
I0814 21:22:33.636528 11101 solver.cpp:312] Iteration 35500 (6.82876 iter/s, 14.6439s/100 iter), loss = 2.4636
I0814 21:22:33.636623 11101 solver.cpp:334]     Train net output #0: loss = 2.26241 (* 1 = 2.26241 loss)
I0814 21:22:33.636643 11101 sgd_solver.cpp:136] Iteration 35500, lr = 0.0889063, m = 0.9
I0814 21:22:48.048164 11101 solver.cpp:312] Iteration 35600 (6.93904 iter/s, 14.4112s/100 iter), loss = 2.74343
I0814 21:22:48.048189 11101 solver.cpp:334]     Train net output #0: loss = 2.95978 (* 1 = 2.95978 loss)
I0814 21:22:48.048197 11101 sgd_solver.cpp:136] Iteration 35600, lr = 0.088875, m = 0.9
I0814 21:23:02.621120 11101 solver.cpp:312] Iteration 35700 (6.86222 iter/s, 14.5725s/100 iter), loss = 2.19616
I0814 21:23:02.621146 11101 solver.cpp:334]     Train net output #0: loss = 1.59269 (* 1 = 1.59269 loss)
I0814 21:23:02.621153 11101 sgd_solver.cpp:136] Iteration 35700, lr = 0.0888438, m = 0.9
I0814 21:23:17.285291 11101 solver.cpp:312] Iteration 35800 (6.81954 iter/s, 14.6638s/100 iter), loss = 2.27168
I0814 21:23:17.285339 11101 solver.cpp:334]     Train net output #0: loss = 2.43443 (* 1 = 2.43443 loss)
I0814 21:23:17.285344 11101 sgd_solver.cpp:136] Iteration 35800, lr = 0.0888125, m = 0.9
I0814 21:23:31.713603 11101 solver.cpp:312] Iteration 35900 (6.93101 iter/s, 14.4279s/100 iter), loss = 2.3211
I0814 21:23:31.713665 11101 solver.cpp:334]     Train net output #0: loss = 2.26628 (* 1 = 2.26628 loss)
I0814 21:23:31.713681 11101 sgd_solver.cpp:136] Iteration 35900, lr = 0.0887813, m = 0.9
I0814 21:23:46.229375 11101 solver.cpp:509] Iteration 36000, Testing net (#0)
I0814 21:23:54.237313 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 21:24:07.365701 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.364352
I0814 21:24:07.365722 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.616059
I0814 21:24:07.365731 11101 solver.cpp:594]     Test net output #2: loss = 3.02421 (* 1 = 3.02421 loss)
I0814 21:24:07.365759 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1358s
I0814 21:24:07.519100 11101 solver.cpp:312] Iteration 36000 (2.79295 iter/s, 35.8045s/100 iter), loss = 2.52967
I0814 21:24:07.519124 11101 solver.cpp:334]     Train net output #0: loss = 2.33384 (* 1 = 2.33384 loss)
I0814 21:24:07.519129 11101 sgd_solver.cpp:136] Iteration 36000, lr = 0.08875, m = 0.9
I0814 21:24:22.048676 11101 solver.cpp:312] Iteration 36100 (6.88271 iter/s, 14.5292s/100 iter), loss = 2.2052
I0814 21:24:22.048703 11101 solver.cpp:334]     Train net output #0: loss = 2.2035 (* 1 = 2.2035 loss)
I0814 21:24:22.048709 11101 sgd_solver.cpp:136] Iteration 36100, lr = 0.0887187, m = 0.9
I0814 21:24:36.644446 11101 solver.cpp:312] Iteration 36200 (6.8515 iter/s, 14.5954s/100 iter), loss = 2.81204
I0814 21:24:36.644937 11101 solver.cpp:334]     Train net output #0: loss = 2.6927 (* 1 = 2.6927 loss)
I0814 21:24:36.644958 11101 sgd_solver.cpp:136] Iteration 36200, lr = 0.0886875, m = 0.9
I0814 21:24:51.076689 11101 solver.cpp:312] Iteration 36300 (6.92913 iter/s, 14.4318s/100 iter), loss = 2.31039
I0814 21:24:51.076721 11101 solver.cpp:334]     Train net output #0: loss = 2.49592 (* 1 = 2.49592 loss)
I0814 21:24:51.076727 11101 sgd_solver.cpp:136] Iteration 36300, lr = 0.0886562, m = 0.9
I0814 21:25:05.684119 11101 solver.cpp:312] Iteration 36400 (6.84603 iter/s, 14.607s/100 iter), loss = 2.17341
I0814 21:25:05.684150 11101 solver.cpp:334]     Train net output #0: loss = 2.51915 (* 1 = 2.51915 loss)
I0814 21:25:05.684156 11101 sgd_solver.cpp:136] Iteration 36400, lr = 0.088625, m = 0.9
I0814 21:25:19.988819 11101 solver.cpp:312] Iteration 36500 (6.99091 iter/s, 14.3043s/100 iter), loss = 2.55665
I0814 21:25:19.988896 11101 solver.cpp:334]     Train net output #0: loss = 2.58524 (* 1 = 2.58524 loss)
I0814 21:25:19.988903 11101 sgd_solver.cpp:136] Iteration 36500, lr = 0.0885938, m = 0.9
I0814 21:25:34.643877 11101 solver.cpp:312] Iteration 36600 (6.82378 iter/s, 14.6546s/100 iter), loss = 2.21986
I0814 21:25:34.643906 11101 solver.cpp:334]     Train net output #0: loss = 2.05531 (* 1 = 2.05531 loss)
I0814 21:25:34.643911 11101 sgd_solver.cpp:136] Iteration 36600, lr = 0.0885625, m = 0.9
I0814 21:25:49.302278 11101 solver.cpp:312] Iteration 36700 (6.82222 iter/s, 14.658s/100 iter), loss = 2.1888
I0814 21:25:49.302348 11101 solver.cpp:334]     Train net output #0: loss = 1.8589 (* 1 = 1.8589 loss)
I0814 21:25:49.302368 11101 sgd_solver.cpp:136] Iteration 36700, lr = 0.0885312, m = 0.9
I0814 21:26:03.762110 11101 solver.cpp:312] Iteration 36800 (6.91591 iter/s, 14.4594s/100 iter), loss = 2.0753
I0814 21:26:03.762204 11101 solver.cpp:334]     Train net output #0: loss = 2.4362 (* 1 = 2.4362 loss)
I0814 21:26:03.762221 11101 sgd_solver.cpp:136] Iteration 36800, lr = 0.0885, m = 0.9
I0814 21:26:18.231703 11101 solver.cpp:312] Iteration 36900 (6.91124 iter/s, 14.4692s/100 iter), loss = 2.47138
I0814 21:26:18.231760 11101 solver.cpp:334]     Train net output #0: loss = 2.79135 (* 1 = 2.79135 loss)
I0814 21:26:18.231775 11101 sgd_solver.cpp:136] Iteration 36900, lr = 0.0884688, m = 0.9
I0814 21:26:32.865130 11101 solver.cpp:312] Iteration 37000 (6.83387 iter/s, 14.633s/100 iter), loss = 2.28333
I0814 21:26:32.865185 11101 solver.cpp:334]     Train net output #0: loss = 1.82413 (* 1 = 1.82413 loss)
I0814 21:26:32.865200 11101 sgd_solver.cpp:136] Iteration 37000, lr = 0.0884375, m = 0.9
I0814 21:26:47.484403 11101 solver.cpp:312] Iteration 37100 (6.84048 iter/s, 14.6189s/100 iter), loss = 1.94291
I0814 21:26:47.484486 11101 solver.cpp:334]     Train net output #0: loss = 1.50973 (* 1 = 1.50973 loss)
I0814 21:26:47.484498 11101 sgd_solver.cpp:136] Iteration 37100, lr = 0.0884063, m = 0.9
I0814 21:27:02.283001 11101 solver.cpp:312] Iteration 37200 (6.75759 iter/s, 14.7982s/100 iter), loss = 2.55935
I0814 21:27:02.283051 11101 solver.cpp:334]     Train net output #0: loss = 2.38961 (* 1 = 2.38961 loss)
I0814 21:27:02.283064 11101 sgd_solver.cpp:136] Iteration 37200, lr = 0.088375, m = 0.9
I0814 21:27:16.912413 11101 solver.cpp:312] Iteration 37300 (6.83574 iter/s, 14.629s/100 iter), loss = 2.60474
I0814 21:27:16.912437 11101 solver.cpp:334]     Train net output #0: loss = 2.61232 (* 1 = 2.61232 loss)
I0814 21:27:16.912441 11101 sgd_solver.cpp:136] Iteration 37300, lr = 0.0883438, m = 0.9
I0814 21:27:31.600589 11101 solver.cpp:312] Iteration 37400 (6.80839 iter/s, 14.6878s/100 iter), loss = 2.65519
I0814 21:27:31.600684 11101 solver.cpp:334]     Train net output #0: loss = 2.77343 (* 1 = 2.77343 loss)
I0814 21:27:31.600704 11101 sgd_solver.cpp:136] Iteration 37400, lr = 0.0883125, m = 0.9
I0814 21:27:46.331926 11101 solver.cpp:312] Iteration 37500 (6.78844 iter/s, 14.7309s/100 iter), loss = 2.48486
I0814 21:27:46.331955 11101 solver.cpp:334]     Train net output #0: loss = 2.89334 (* 1 = 2.89334 loss)
I0814 21:27:46.331960 11101 sgd_solver.cpp:136] Iteration 37500, lr = 0.0882813, m = 0.9
I0814 21:28:00.834007 11101 solver.cpp:312] Iteration 37600 (6.89576 iter/s, 14.5017s/100 iter), loss = 2.35861
I0814 21:28:00.834071 11101 solver.cpp:334]     Train net output #0: loss = 2.47941 (* 1 = 2.47941 loss)
I0814 21:28:00.834089 11101 sgd_solver.cpp:136] Iteration 37600, lr = 0.08825, m = 0.9
I0814 21:28:15.224226 11101 solver.cpp:312] Iteration 37700 (6.94936 iter/s, 14.3898s/100 iter), loss = 2.59062
I0814 21:28:15.224293 11101 solver.cpp:334]     Train net output #0: loss = 2.33693 (* 1 = 2.33693 loss)
I0814 21:28:15.224299 11101 sgd_solver.cpp:136] Iteration 37700, lr = 0.0882187, m = 0.9
I0814 21:28:29.771281 11101 solver.cpp:312] Iteration 37800 (6.87444 iter/s, 14.5466s/100 iter), loss = 2.47217
I0814 21:28:29.771344 11101 solver.cpp:334]     Train net output #0: loss = 2.05298 (* 1 = 2.05298 loss)
I0814 21:28:29.771363 11101 sgd_solver.cpp:136] Iteration 37800, lr = 0.0881875, m = 0.9
I0814 21:28:44.292522 11101 solver.cpp:312] Iteration 37900 (6.88666 iter/s, 14.5208s/100 iter), loss = 2.39918
I0814 21:28:44.292548 11101 solver.cpp:334]     Train net output #0: loss = 2.90852 (* 1 = 2.90852 loss)
I0814 21:28:44.292552 11101 sgd_solver.cpp:136] Iteration 37900, lr = 0.0881562, m = 0.9
I0814 21:28:58.695489 11101 solver.cpp:509] Iteration 38000, Testing net (#0)
I0814 21:29:19.617190 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.313058
I0814 21:29:19.617213 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.565706
I0814 21:29:19.617219 11101 solver.cpp:594]     Test net output #2: loss = 3.36581 (* 1 = 3.36581 loss)
I0814 21:29:19.617241 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9212s
I0814 21:29:19.787843 11101 solver.cpp:312] Iteration 38000 (2.81735 iter/s, 35.4943s/100 iter), loss = 2.29224
I0814 21:29:19.787988 11101 solver.cpp:334]     Train net output #0: loss = 2.4458 (* 1 = 2.4458 loss)
I0814 21:29:19.788007 11101 sgd_solver.cpp:136] Iteration 38000, lr = 0.088125, m = 0.9
I0814 21:29:34.640782 11101 solver.cpp:312] Iteration 38100 (6.73287 iter/s, 14.8525s/100 iter), loss = 2.44377
I0814 21:29:34.640995 11101 solver.cpp:334]     Train net output #0: loss = 2.83633 (* 1 = 2.83633 loss)
I0814 21:29:34.641084 11101 sgd_solver.cpp:136] Iteration 38100, lr = 0.0880938, m = 0.9
I0814 21:29:49.082113 11101 solver.cpp:312] Iteration 38200 (6.92477 iter/s, 14.4409s/100 iter), loss = 2.19498
I0814 21:29:49.082168 11101 solver.cpp:334]     Train net output #0: loss = 2.41017 (* 1 = 2.41017 loss)
I0814 21:29:49.082191 11101 sgd_solver.cpp:136] Iteration 38200, lr = 0.0880625, m = 0.9
I0814 21:30:03.385231 11101 solver.cpp:312] Iteration 38300 (6.99168 iter/s, 14.3027s/100 iter), loss = 2.45866
I0814 21:30:03.385257 11101 solver.cpp:334]     Train net output #0: loss = 2.42457 (* 1 = 2.42457 loss)
I0814 21:30:03.385263 11101 sgd_solver.cpp:136] Iteration 38300, lr = 0.0880313, m = 0.9
I0814 21:30:17.751189 11101 solver.cpp:312] Iteration 38400 (6.9611 iter/s, 14.3656s/100 iter), loss = 2.06545
I0814 21:30:17.751246 11101 solver.cpp:334]     Train net output #0: loss = 1.98095 (* 1 = 1.98095 loss)
I0814 21:30:17.751253 11101 sgd_solver.cpp:136] Iteration 38400, lr = 0.088, m = 0.9
I0814 21:30:32.090555 11101 solver.cpp:312] Iteration 38500 (6.97401 iter/s, 14.339s/100 iter), loss = 2.4268
I0814 21:30:32.090626 11101 solver.cpp:334]     Train net output #0: loss = 2.33912 (* 1 = 2.33912 loss)
I0814 21:30:32.090644 11101 sgd_solver.cpp:136] Iteration 38500, lr = 0.0879688, m = 0.9
I0814 21:30:46.661715 11101 solver.cpp:312] Iteration 38600 (6.86307 iter/s, 14.5707s/100 iter), loss = 2.28648
I0814 21:30:46.661743 11101 solver.cpp:334]     Train net output #0: loss = 2.20032 (* 1 = 2.20032 loss)
I0814 21:30:46.661749 11101 sgd_solver.cpp:136] Iteration 38600, lr = 0.0879375, m = 0.9
I0814 21:31:01.161108 11101 solver.cpp:312] Iteration 38700 (6.89704 iter/s, 14.499s/100 iter), loss = 1.77984
I0814 21:31:01.161207 11101 solver.cpp:334]     Train net output #0: loss = 1.62847 (* 1 = 1.62847 loss)
I0814 21:31:01.161227 11101 sgd_solver.cpp:136] Iteration 38700, lr = 0.0879063, m = 0.9
I0814 21:31:15.674998 11101 solver.cpp:312] Iteration 38800 (6.89015 iter/s, 14.5135s/100 iter), loss = 2.09394
I0814 21:31:15.675067 11101 solver.cpp:334]     Train net output #0: loss = 2.23746 (* 1 = 2.23746 loss)
I0814 21:31:15.675087 11101 sgd_solver.cpp:136] Iteration 38800, lr = 0.087875, m = 0.9
I0814 21:31:30.232915 11101 solver.cpp:312] Iteration 38900 (6.86931 iter/s, 14.5575s/100 iter), loss = 2.61398
I0814 21:31:30.232985 11101 solver.cpp:334]     Train net output #0: loss = 2.5694 (* 1 = 2.5694 loss)
I0814 21:31:30.233003 11101 sgd_solver.cpp:136] Iteration 38900, lr = 0.0878438, m = 0.9
I0814 21:31:44.672049 11101 solver.cpp:312] Iteration 39000 (6.92582 iter/s, 14.4387s/100 iter), loss = 2.1254
I0814 21:31:44.672143 11101 solver.cpp:334]     Train net output #0: loss = 2.22576 (* 1 = 2.22576 loss)
I0814 21:31:44.672248 11101 sgd_solver.cpp:136] Iteration 39000, lr = 0.0878125, m = 0.9
I0814 21:31:59.413075 11101 solver.cpp:312] Iteration 39100 (6.78398 iter/s, 14.7406s/100 iter), loss = 2.41962
I0814 21:31:59.413103 11101 solver.cpp:334]     Train net output #0: loss = 2.30634 (* 1 = 2.30634 loss)
I0814 21:31:59.413108 11101 sgd_solver.cpp:136] Iteration 39100, lr = 0.0877813, m = 0.9
I0814 21:32:14.446421 11101 solver.cpp:312] Iteration 39200 (6.65207 iter/s, 15.0329s/100 iter), loss = 2.16105
I0814 21:32:14.446447 11101 solver.cpp:334]     Train net output #0: loss = 2.11387 (* 1 = 2.11387 loss)
I0814 21:32:14.446454 11101 sgd_solver.cpp:136] Iteration 39200, lr = 0.08775, m = 0.9
I0814 21:32:29.229552 11101 solver.cpp:312] Iteration 39300 (6.76466 iter/s, 14.7827s/100 iter), loss = 2.57619
I0814 21:32:29.230536 11101 solver.cpp:334]     Train net output #0: loss = 2.54425 (* 1 = 2.54425 loss)
I0814 21:32:29.230558 11101 sgd_solver.cpp:136] Iteration 39300, lr = 0.0877187, m = 0.9
I0814 21:32:43.633934 11101 solver.cpp:312] Iteration 39400 (6.94253 iter/s, 14.404s/100 iter), loss = 2.17629
I0814 21:32:43.633996 11101 solver.cpp:334]     Train net output #0: loss = 1.92006 (* 1 = 1.92006 loss)
I0814 21:32:43.634014 11101 sgd_solver.cpp:136] Iteration 39400, lr = 0.0876875, m = 0.9
I0814 21:32:58.148504 11101 solver.cpp:312] Iteration 39500 (6.88983 iter/s, 14.5142s/100 iter), loss = 2.36371
I0814 21:32:58.148576 11101 solver.cpp:334]     Train net output #0: loss = 2.20381 (* 1 = 2.20381 loss)
I0814 21:32:58.148597 11101 sgd_solver.cpp:136] Iteration 39500, lr = 0.0876563, m = 0.9
I0814 21:33:12.815538 11101 solver.cpp:312] Iteration 39600 (6.81821 iter/s, 14.6666s/100 iter), loss = 2.0415
I0814 21:33:12.815631 11101 solver.cpp:334]     Train net output #0: loss = 2.05527 (* 1 = 2.05527 loss)
I0814 21:33:12.815649 11101 sgd_solver.cpp:136] Iteration 39600, lr = 0.087625, m = 0.9
I0814 21:33:27.323297 11101 solver.cpp:312] Iteration 39700 (6.89306 iter/s, 14.5073s/100 iter), loss = 2.38869
I0814 21:33:27.323323 11101 solver.cpp:334]     Train net output #0: loss = 2.2454 (* 1 = 2.2454 loss)
I0814 21:33:27.323329 11101 sgd_solver.cpp:136] Iteration 39700, lr = 0.0875938, m = 0.9
I0814 21:33:41.788671 11101 solver.cpp:312] Iteration 39800 (6.91326 iter/s, 14.465s/100 iter), loss = 2.27983
I0814 21:33:41.788694 11101 solver.cpp:334]     Train net output #0: loss = 2.10194 (* 1 = 2.10194 loss)
I0814 21:33:41.788700 11101 sgd_solver.cpp:136] Iteration 39800, lr = 0.0875625, m = 0.9
I0814 21:33:56.131783 11101 solver.cpp:312] Iteration 39900 (6.97219 iter/s, 14.3427s/100 iter), loss = 2.32929
I0814 21:33:56.131837 11101 solver.cpp:334]     Train net output #0: loss = 2.5004 (* 1 = 2.5004 loss)
I0814 21:33:56.131844 11101 sgd_solver.cpp:136] Iteration 39900, lr = 0.0875313, m = 0.9
I0814 21:34:10.434324 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_40000.caffemodel
I0814 21:34:10.481889 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_40000.solverstate
I0814 21:34:10.486377 11101 solver.cpp:509] Iteration 40000, Testing net (#0)
I0814 21:34:31.232930 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.350646
I0814 21:34:31.233019 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.612
I0814 21:34:31.233028 11101 solver.cpp:594]     Test net output #2: loss = 3.08553 (* 1 = 3.08553 loss)
I0814 21:34:31.233047 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7461s
I0814 21:34:31.381924 11101 solver.cpp:312] Iteration 40000 (2.83695 iter/s, 35.2491s/100 iter), loss = 2.60849
I0814 21:34:31.381953 11101 solver.cpp:334]     Train net output #0: loss = 2.37918 (* 1 = 2.37918 loss)
I0814 21:34:31.381959 11101 sgd_solver.cpp:136] Iteration 40000, lr = 0.0875, m = 0.9
I0814 21:34:45.829552 11101 solver.cpp:312] Iteration 40100 (6.92175 iter/s, 14.4472s/100 iter), loss = 2.57891
I0814 21:34:45.829622 11101 solver.cpp:334]     Train net output #0: loss = 2.03965 (* 1 = 2.03965 loss)
I0814 21:34:45.829640 11101 sgd_solver.cpp:136] Iteration 40100, lr = 0.0874688, m = 0.9
I0814 21:35:00.276165 11101 solver.cpp:312] Iteration 40200 (6.92224 iter/s, 14.4462s/100 iter), loss = 2.26454
I0814 21:35:00.276191 11101 solver.cpp:334]     Train net output #0: loss = 2.22047 (* 1 = 2.22047 loss)
I0814 21:35:00.276196 11101 sgd_solver.cpp:136] Iteration 40200, lr = 0.0874375, m = 0.9
I0814 21:35:15.065156 11101 solver.cpp:312] Iteration 40300 (6.76198 iter/s, 14.7886s/100 iter), loss = 2.31456
I0814 21:35:15.065245 11101 solver.cpp:334]     Train net output #0: loss = 2.38446 (* 1 = 2.38446 loss)
I0814 21:35:15.065263 11101 sgd_solver.cpp:136] Iteration 40300, lr = 0.0874062, m = 0.9
I0814 21:35:29.940311 11101 solver.cpp:312] Iteration 40400 (6.72281 iter/s, 14.8747s/100 iter), loss = 1.8697
I0814 21:35:29.940385 11101 solver.cpp:334]     Train net output #0: loss = 1.60113 (* 1 = 1.60113 loss)
I0814 21:35:29.940404 11101 sgd_solver.cpp:136] Iteration 40400, lr = 0.087375, m = 0.9
I0814 21:35:44.700809 11101 solver.cpp:312] Iteration 40500 (6.77503 iter/s, 14.7601s/100 iter), loss = 2.58109
I0814 21:35:44.700873 11101 solver.cpp:334]     Train net output #0: loss = 2.56276 (* 1 = 2.56276 loss)
I0814 21:35:44.700891 11101 sgd_solver.cpp:136] Iteration 40500, lr = 0.0873438, m = 0.9
I0814 21:35:59.474612 11101 solver.cpp:312] Iteration 40600 (6.76893 iter/s, 14.7734s/100 iter), loss = 2.34102
I0814 21:35:59.474671 11101 solver.cpp:334]     Train net output #0: loss = 2.31487 (* 1 = 2.31487 loss)
I0814 21:35:59.474678 11101 sgd_solver.cpp:136] Iteration 40600, lr = 0.0873125, m = 0.9
I0814 21:36:13.947242 11101 solver.cpp:312] Iteration 40700 (6.90979 iter/s, 14.4722s/100 iter), loss = 1.73565
I0814 21:36:13.947296 11101 solver.cpp:334]     Train net output #0: loss = 1.66034 (* 1 = 1.66034 loss)
I0814 21:36:13.947309 11101 sgd_solver.cpp:136] Iteration 40700, lr = 0.0872812, m = 0.9
I0814 21:36:28.702536 11101 solver.cpp:312] Iteration 40800 (6.77742 iter/s, 14.7549s/100 iter), loss = 2.34178
I0814 21:36:28.702571 11101 solver.cpp:334]     Train net output #0: loss = 2.45542 (* 1 = 2.45542 loss)
I0814 21:36:28.702577 11101 sgd_solver.cpp:136] Iteration 40800, lr = 0.08725, m = 0.9
I0814 21:36:43.472523 11101 solver.cpp:312] Iteration 40900 (6.77068 iter/s, 14.7696s/100 iter), loss = 2.11068
I0814 21:36:43.472771 11101 solver.cpp:334]     Train net output #0: loss = 1.91082 (* 1 = 1.91082 loss)
I0814 21:36:43.472883 11101 sgd_solver.cpp:136] Iteration 40900, lr = 0.0872188, m = 0.9
I0814 21:36:58.326889 11101 solver.cpp:312] Iteration 41000 (6.73222 iter/s, 14.8539s/100 iter), loss = 2.02119
I0814 21:36:58.326918 11101 solver.cpp:334]     Train net output #0: loss = 2.26532 (* 1 = 2.26532 loss)
I0814 21:36:58.326925 11101 sgd_solver.cpp:136] Iteration 41000, lr = 0.0871875, m = 0.9
I0814 21:37:12.894564 11101 solver.cpp:312] Iteration 41100 (6.86471 iter/s, 14.5673s/100 iter), loss = 2.67092
I0814 21:37:12.894595 11101 solver.cpp:334]     Train net output #0: loss = 2.47506 (* 1 = 2.47506 loss)
I0814 21:37:12.894601 11101 sgd_solver.cpp:136] Iteration 41100, lr = 0.0871563, m = 0.9
I0814 21:37:27.194563 11101 solver.cpp:312] Iteration 41200 (6.99321 iter/s, 14.2996s/100 iter), loss = 2.48206
I0814 21:37:27.196509 11101 solver.cpp:334]     Train net output #0: loss = 2.42302 (* 1 = 2.42302 loss)
I0814 21:37:27.196522 11101 sgd_solver.cpp:136] Iteration 41200, lr = 0.087125, m = 0.9
I0814 21:37:41.827756 11101 solver.cpp:312] Iteration 41300 (6.83397 iter/s, 14.6328s/100 iter), loss = 1.89585
I0814 21:37:41.827783 11101 solver.cpp:334]     Train net output #0: loss = 2.12852 (* 1 = 2.12852 loss)
I0814 21:37:41.827790 11101 sgd_solver.cpp:136] Iteration 41300, lr = 0.0870937, m = 0.9
I0814 21:37:56.454746 11101 solver.cpp:312] Iteration 41400 (6.83687 iter/s, 14.6266s/100 iter), loss = 2.31037
I0814 21:37:56.454776 11101 solver.cpp:334]     Train net output #0: loss = 1.95472 (* 1 = 1.95472 loss)
I0814 21:37:56.454782 11101 sgd_solver.cpp:136] Iteration 41400, lr = 0.0870625, m = 0.9
I0814 21:38:10.875995 11101 solver.cpp:312] Iteration 41500 (6.93441 iter/s, 14.4208s/100 iter), loss = 2.32402
I0814 21:38:10.876092 11101 solver.cpp:334]     Train net output #0: loss = 2.32073 (* 1 = 2.32073 loss)
I0814 21:38:10.876106 11101 sgd_solver.cpp:136] Iteration 41500, lr = 0.0870313, m = 0.9
I0814 21:38:25.522635 11101 solver.cpp:312] Iteration 41600 (6.8277 iter/s, 14.6462s/100 iter), loss = 2.39858
I0814 21:38:25.522663 11101 solver.cpp:334]     Train net output #0: loss = 1.76391 (* 1 = 1.76391 loss)
I0814 21:38:25.522670 11101 sgd_solver.cpp:136] Iteration 41600, lr = 0.087, m = 0.9
I0814 21:38:40.167001 11101 solver.cpp:312] Iteration 41700 (6.82876 iter/s, 14.644s/100 iter), loss = 2.12448
I0814 21:38:40.167049 11101 solver.cpp:334]     Train net output #0: loss = 2.01142 (* 1 = 2.01142 loss)
I0814 21:38:40.167062 11101 sgd_solver.cpp:136] Iteration 41700, lr = 0.0869687, m = 0.9
I0814 21:38:54.621459 11101 solver.cpp:312] Iteration 41800 (6.91848 iter/s, 14.454s/100 iter), loss = 2.30944
I0814 21:38:54.621551 11101 solver.cpp:334]     Train net output #0: loss = 2.04589 (* 1 = 2.04589 loss)
I0814 21:38:54.621567 11101 sgd_solver.cpp:136] Iteration 41800, lr = 0.0869375, m = 0.9
I0814 21:39:09.229310 11101 solver.cpp:312] Iteration 41900 (6.84583 iter/s, 14.6074s/100 iter), loss = 2.9404
I0814 21:39:09.229374 11101 solver.cpp:334]     Train net output #0: loss = 3.38482 (* 1 = 3.38482 loss)
I0814 21:39:09.229393 11101 sgd_solver.cpp:136] Iteration 41900, lr = 0.0869062, m = 0.9
I0814 21:39:23.607360 11101 solver.cpp:509] Iteration 42000, Testing net (#0)
I0814 21:39:30.066196 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 21:39:44.520982 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.355058
I0814 21:39:44.521003 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.608823
I0814 21:39:44.521008 11101 solver.cpp:594]     Test net output #2: loss = 3.12557 (* 1 = 3.12557 loss)
I0814 21:39:44.521028 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9131s
I0814 21:39:44.691772 11101 solver.cpp:312] Iteration 42000 (2.81996 iter/s, 35.4615s/100 iter), loss = 2.21583
I0814 21:39:44.691841 11101 solver.cpp:334]     Train net output #0: loss = 2.08313 (* 1 = 2.08313 loss)
I0814 21:39:44.691861 11101 sgd_solver.cpp:136] Iteration 42000, lr = 0.086875, m = 0.9
I0814 21:39:59.366662 11101 solver.cpp:312] Iteration 42100 (6.81456 iter/s, 14.6745s/100 iter), loss = 1.79446
I0814 21:39:59.366704 11101 solver.cpp:334]     Train net output #0: loss = 2.14608 (* 1 = 2.14608 loss)
I0814 21:39:59.366713 11101 sgd_solver.cpp:136] Iteration 42100, lr = 0.0868438, m = 0.9
I0814 21:40:13.831797 11101 solver.cpp:312] Iteration 42200 (6.91337 iter/s, 14.4647s/100 iter), loss = 2.57056
I0814 21:40:13.831861 11101 solver.cpp:334]     Train net output #0: loss = 2.62141 (* 1 = 2.62141 loss)
I0814 21:40:13.831867 11101 sgd_solver.cpp:136] Iteration 42200, lr = 0.0868125, m = 0.9
I0814 21:40:28.484112 11101 solver.cpp:312] Iteration 42300 (6.82506 iter/s, 14.6519s/100 iter), loss = 1.96283
I0814 21:40:28.484259 11101 solver.cpp:334]     Train net output #0: loss = 1.98744 (* 1 = 1.98744 loss)
I0814 21:40:28.484274 11101 sgd_solver.cpp:136] Iteration 42300, lr = 0.0867813, m = 0.9
I0814 21:40:43.023953 11101 solver.cpp:312] Iteration 42400 (6.87785 iter/s, 14.5394s/100 iter), loss = 2.6359
I0814 21:40:43.024001 11101 solver.cpp:334]     Train net output #0: loss = 2.4704 (* 1 = 2.4704 loss)
I0814 21:40:43.024014 11101 sgd_solver.cpp:136] Iteration 42400, lr = 0.08675, m = 0.9
I0814 21:40:57.914409 11101 solver.cpp:312] Iteration 42500 (6.7159 iter/s, 14.89s/100 iter), loss = 2.01865
I0814 21:40:57.914515 11101 solver.cpp:334]     Train net output #0: loss = 1.75576 (* 1 = 1.75576 loss)
I0814 21:40:57.914536 11101 sgd_solver.cpp:136] Iteration 42500, lr = 0.0867188, m = 0.9
I0814 21:41:12.465386 11101 solver.cpp:312] Iteration 42600 (6.87259 iter/s, 14.5506s/100 iter), loss = 2.02648
I0814 21:41:12.468155 11101 solver.cpp:334]     Train net output #0: loss = 1.90489 (* 1 = 1.90489 loss)
I0814 21:41:12.468194 11101 sgd_solver.cpp:136] Iteration 42600, lr = 0.0866875, m = 0.9
I0814 21:41:26.918083 11101 solver.cpp:312] Iteration 42700 (6.91932 iter/s, 14.4523s/100 iter), loss = 2.35799
I0814 21:41:26.918262 11101 solver.cpp:334]     Train net output #0: loss = 2.47061 (* 1 = 2.47061 loss)
I0814 21:41:26.918349 11101 sgd_solver.cpp:136] Iteration 42700, lr = 0.0866563, m = 0.9
I0814 21:41:41.614975 11101 solver.cpp:312] Iteration 42800 (6.80435 iter/s, 14.6965s/100 iter), loss = 1.79711
I0814 21:41:41.615026 11101 solver.cpp:334]     Train net output #0: loss = 1.62312 (* 1 = 1.62312 loss)
I0814 21:41:41.615033 11101 sgd_solver.cpp:136] Iteration 42800, lr = 0.086625, m = 0.9
I0814 21:41:56.142722 11101 solver.cpp:312] Iteration 42900 (6.88358 iter/s, 14.5273s/100 iter), loss = 2.19919
I0814 21:41:56.142776 11101 solver.cpp:334]     Train net output #0: loss = 2.63653 (* 1 = 2.63653 loss)
I0814 21:41:56.142789 11101 sgd_solver.cpp:136] Iteration 42900, lr = 0.0865937, m = 0.9
I0814 21:42:11.056789 11101 solver.cpp:312] Iteration 43000 (6.70527 iter/s, 14.9136s/100 iter), loss = 2.11099
I0814 21:42:11.056815 11101 solver.cpp:334]     Train net output #0: loss = 2.05114 (* 1 = 2.05114 loss)
I0814 21:42:11.056820 11101 sgd_solver.cpp:136] Iteration 43000, lr = 0.0865625, m = 0.9
I0814 21:42:25.659996 11101 solver.cpp:312] Iteration 43100 (6.848 iter/s, 14.6028s/100 iter), loss = 1.9466
I0814 21:42:25.660063 11101 solver.cpp:334]     Train net output #0: loss = 2.23442 (* 1 = 2.23442 loss)
I0814 21:42:25.660076 11101 sgd_solver.cpp:136] Iteration 43100, lr = 0.0865313, m = 0.9
I0814 21:42:40.288647 11101 solver.cpp:312] Iteration 43200 (6.83609 iter/s, 14.6282s/100 iter), loss = 2.31254
I0814 21:42:40.288704 11101 solver.cpp:334]     Train net output #0: loss = 2.65524 (* 1 = 2.65524 loss)
I0814 21:42:40.288718 11101 sgd_solver.cpp:136] Iteration 43200, lr = 0.0865, m = 0.9
I0814 21:42:55.298746 11101 solver.cpp:312] Iteration 43300 (6.66237 iter/s, 15.0097s/100 iter), loss = 1.86679
I0814 21:42:55.298771 11101 solver.cpp:334]     Train net output #0: loss = 2.11766 (* 1 = 2.11766 loss)
I0814 21:42:55.298777 11101 sgd_solver.cpp:136] Iteration 43300, lr = 0.0864687, m = 0.9
I0814 21:43:10.590405 11101 solver.cpp:312] Iteration 43400 (6.5397 iter/s, 15.2912s/100 iter), loss = 2.27973
I0814 21:43:10.592161 11101 solver.cpp:334]     Train net output #0: loss = 2.6085 (* 1 = 2.6085 loss)
I0814 21:43:10.592171 11101 sgd_solver.cpp:136] Iteration 43400, lr = 0.0864375, m = 0.9
I0814 21:43:25.373973 11101 solver.cpp:312] Iteration 43500 (6.76446 iter/s, 14.7831s/100 iter), loss = 2.25152
I0814 21:43:25.374037 11101 solver.cpp:334]     Train net output #0: loss = 2.376 (* 1 = 2.376 loss)
I0814 21:43:25.374055 11101 sgd_solver.cpp:136] Iteration 43500, lr = 0.0864063, m = 0.9
I0814 21:43:40.387212 11101 solver.cpp:312] Iteration 43600 (6.66098 iter/s, 15.0128s/100 iter), loss = 2.26726
I0814 21:43:40.387238 11101 solver.cpp:334]     Train net output #0: loss = 1.94773 (* 1 = 1.94773 loss)
I0814 21:43:40.387244 11101 sgd_solver.cpp:136] Iteration 43600, lr = 0.086375, m = 0.9
I0814 21:43:55.238756 11101 solver.cpp:312] Iteration 43700 (6.7335 iter/s, 14.8511s/100 iter), loss = 2.71221
I0814 21:43:55.238819 11101 solver.cpp:334]     Train net output #0: loss = 2.18623 (* 1 = 2.18623 loss)
I0814 21:43:55.238826 11101 sgd_solver.cpp:136] Iteration 43700, lr = 0.0863438, m = 0.9
I0814 21:44:09.996429 11101 solver.cpp:312] Iteration 43800 (6.77633 iter/s, 14.7573s/100 iter), loss = 2.25823
I0814 21:44:09.996456 11101 solver.cpp:334]     Train net output #0: loss = 2.88078 (* 1 = 2.88078 loss)
I0814 21:44:09.996464 11101 sgd_solver.cpp:136] Iteration 43800, lr = 0.0863125, m = 0.9
I0814 21:44:24.595872 11101 solver.cpp:312] Iteration 43900 (6.84977 iter/s, 14.599s/100 iter), loss = 2.64187
I0814 21:44:24.595944 11101 solver.cpp:334]     Train net output #0: loss = 3.13226 (* 1 = 3.13226 loss)
I0814 21:44:24.595963 11101 sgd_solver.cpp:136] Iteration 43900, lr = 0.0862813, m = 0.9
I0814 21:44:38.965977 11101 solver.cpp:509] Iteration 44000, Testing net (#0)
I0814 21:44:59.619071 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.35694
I0814 21:44:59.619091 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.614706
I0814 21:44:59.619098 11101 solver.cpp:594]     Test net output #2: loss = 3.01993 (* 1 = 3.01993 loss)
I0814 21:44:59.619132 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6526s
I0814 21:44:59.766991 11101 solver.cpp:312] Iteration 44000 (2.84332 iter/s, 35.1701s/100 iter), loss = 2.30882
I0814 21:44:59.767017 11101 solver.cpp:334]     Train net output #0: loss = 2.6148 (* 1 = 2.6148 loss)
I0814 21:44:59.767022 11101 sgd_solver.cpp:136] Iteration 44000, lr = 0.08625, m = 0.9
I0814 21:45:14.052156 11101 solver.cpp:312] Iteration 44100 (7.00047 iter/s, 14.2848s/100 iter), loss = 2.24116
I0814 21:45:14.052211 11101 solver.cpp:334]     Train net output #0: loss = 2.27148 (* 1 = 2.27148 loss)
I0814 21:45:14.052217 11101 sgd_solver.cpp:136] Iteration 44100, lr = 0.0862188, m = 0.9
I0814 21:45:28.680721 11101 solver.cpp:312] Iteration 44200 (6.83614 iter/s, 14.6281s/100 iter), loss = 2.02302
I0814 21:45:28.680749 11101 solver.cpp:334]     Train net output #0: loss = 2.26798 (* 1 = 2.26798 loss)
I0814 21:45:28.680755 11101 sgd_solver.cpp:136] Iteration 44200, lr = 0.0861875, m = 0.9
I0814 21:45:43.086045 11101 solver.cpp:312] Iteration 44300 (6.94208 iter/s, 14.4049s/100 iter), loss = 2.37653
I0814 21:45:43.086112 11101 solver.cpp:334]     Train net output #0: loss = 2.04558 (* 1 = 2.04558 loss)
I0814 21:45:43.086130 11101 sgd_solver.cpp:136] Iteration 44300, lr = 0.0861562, m = 0.9
I0814 21:45:57.434367 11101 solver.cpp:312] Iteration 44400 (6.96965 iter/s, 14.3479s/100 iter), loss = 2.3311
I0814 21:45:57.434468 11101 solver.cpp:334]     Train net output #0: loss = 2.55597 (* 1 = 2.55597 loss)
I0814 21:45:57.434487 11101 sgd_solver.cpp:136] Iteration 44400, lr = 0.086125, m = 0.9
I0814 21:46:11.931531 11101 solver.cpp:312] Iteration 44500 (6.8981 iter/s, 14.4968s/100 iter), loss = 2.52902
I0814 21:46:11.931602 11101 solver.cpp:334]     Train net output #0: loss = 2.29629 (* 1 = 2.29629 loss)
I0814 21:46:11.931622 11101 sgd_solver.cpp:136] Iteration 44500, lr = 0.0860937, m = 0.9
I0814 21:46:26.341728 11101 solver.cpp:312] Iteration 44600 (6.93973 iter/s, 14.4098s/100 iter), loss = 2.6502
I0814 21:46:26.341756 11101 solver.cpp:334]     Train net output #0: loss = 2.4773 (* 1 = 2.4773 loss)
I0814 21:46:26.341761 11101 sgd_solver.cpp:136] Iteration 44600, lr = 0.0860625, m = 0.9
I0814 21:46:39.575029 11065 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 21:46:41.060191 11101 solver.cpp:312] Iteration 44700 (6.79438 iter/s, 14.718s/100 iter), loss = 2.1663
I0814 21:46:41.060219 11101 solver.cpp:334]     Train net output #0: loss = 2.26411 (* 1 = 2.26411 loss)
I0814 21:46:41.060225 11101 sgd_solver.cpp:136] Iteration 44700, lr = 0.0860313, m = 0.9
I0814 21:46:55.676376 11101 solver.cpp:312] Iteration 44800 (6.84193 iter/s, 14.6158s/100 iter), loss = 2.53344
I0814 21:46:55.676450 11101 solver.cpp:334]     Train net output #0: loss = 2.58231 (* 1 = 2.58231 loss)
I0814 21:46:55.676470 11101 sgd_solver.cpp:136] Iteration 44800, lr = 0.086, m = 0.9
I0814 21:47:10.299811 11101 solver.cpp:312] Iteration 44900 (6.83853 iter/s, 14.623s/100 iter), loss = 2.14145
I0814 21:47:10.299870 11101 solver.cpp:334]     Train net output #0: loss = 2.40064 (* 1 = 2.40064 loss)
I0814 21:47:10.299875 11101 sgd_solver.cpp:136] Iteration 44900, lr = 0.0859688, m = 0.9
I0814 21:47:24.913223 11101 solver.cpp:312] Iteration 45000 (6.84322 iter/s, 14.613s/100 iter), loss = 2.18074
I0814 21:47:24.913251 11101 solver.cpp:334]     Train net output #0: loss = 2.32962 (* 1 = 2.32962 loss)
I0814 21:47:24.913256 11101 sgd_solver.cpp:136] Iteration 45000, lr = 0.0859375, m = 0.9
I0814 21:47:39.657743 11101 solver.cpp:312] Iteration 45100 (6.78237 iter/s, 14.7441s/100 iter), loss = 2.49725
I0814 21:47:39.657768 11101 solver.cpp:334]     Train net output #0: loss = 2.72997 (* 1 = 2.72997 loss)
I0814 21:47:39.657774 11101 sgd_solver.cpp:136] Iteration 45100, lr = 0.0859063, m = 0.9
I0814 21:47:54.584635 11101 solver.cpp:312] Iteration 45200 (6.69951 iter/s, 14.9265s/100 iter), loss = 2.54211
I0814 21:47:54.584708 11101 solver.cpp:334]     Train net output #0: loss = 2.26964 (* 1 = 2.26964 loss)
I0814 21:47:54.584715 11101 sgd_solver.cpp:136] Iteration 45200, lr = 0.085875, m = 0.9
I0814 21:48:09.186926 11101 solver.cpp:312] Iteration 45300 (6.84843 iter/s, 14.6019s/100 iter), loss = 2.52978
I0814 21:48:09.186990 11101 solver.cpp:334]     Train net output #0: loss = 2.50079 (* 1 = 2.50079 loss)
I0814 21:48:09.187007 11101 sgd_solver.cpp:136] Iteration 45300, lr = 0.0858437, m = 0.9
I0814 21:48:24.143443 11101 solver.cpp:312] Iteration 45400 (6.68624 iter/s, 14.9561s/100 iter), loss = 2.17276
I0814 21:48:24.143471 11101 solver.cpp:334]     Train net output #0: loss = 2.1758 (* 1 = 2.1758 loss)
I0814 21:48:24.143477 11101 sgd_solver.cpp:136] Iteration 45400, lr = 0.0858125, m = 0.9
I0814 21:48:38.864174 11101 solver.cpp:312] Iteration 45500 (6.79333 iter/s, 14.7203s/100 iter), loss = 2.53833
I0814 21:48:38.864271 11101 solver.cpp:334]     Train net output #0: loss = 2.81469 (* 1 = 2.81469 loss)
I0814 21:48:38.864295 11101 sgd_solver.cpp:136] Iteration 45500, lr = 0.0857813, m = 0.9
I0814 21:48:53.374056 11101 solver.cpp:312] Iteration 45600 (6.89205 iter/s, 14.5095s/100 iter), loss = 1.91765
I0814 21:48:53.374084 11101 solver.cpp:334]     Train net output #0: loss = 1.98126 (* 1 = 1.98126 loss)
I0814 21:48:53.374090 11101 sgd_solver.cpp:136] Iteration 45600, lr = 0.08575, m = 0.9
I0814 21:49:07.873854 11101 solver.cpp:312] Iteration 45700 (6.89684 iter/s, 14.4994s/100 iter), loss = 2.16936
I0814 21:49:07.873878 11101 solver.cpp:334]     Train net output #0: loss = 2.4163 (* 1 = 2.4163 loss)
I0814 21:49:07.873884 11101 sgd_solver.cpp:136] Iteration 45700, lr = 0.0857188, m = 0.9
I0814 21:49:22.575074 11101 solver.cpp:312] Iteration 45800 (6.80235 iter/s, 14.7008s/100 iter), loss = 2.49567
I0814 21:49:22.575328 11101 solver.cpp:334]     Train net output #0: loss = 2.59615 (* 1 = 2.59615 loss)
I0814 21:49:22.575438 11101 sgd_solver.cpp:136] Iteration 45800, lr = 0.0856875, m = 0.9
I0814 21:49:37.137742 11101 solver.cpp:312] Iteration 45900 (6.86707 iter/s, 14.5623s/100 iter), loss = 2.20903
I0814 21:49:37.137768 11101 solver.cpp:334]     Train net output #0: loss = 2.3003 (* 1 = 2.3003 loss)
I0814 21:49:37.137774 11101 sgd_solver.cpp:136] Iteration 45900, lr = 0.0856562, m = 0.9
I0814 21:49:51.431538 11101 solver.cpp:509] Iteration 46000, Testing net (#0)
I0814 21:50:12.406324 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.398117
I0814 21:50:12.406381 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.65194
I0814 21:50:12.406389 11101 solver.cpp:594]     Test net output #2: loss = 2.85317 (* 1 = 2.85317 loss)
I0814 21:50:12.406410 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9743s
I0814 21:50:12.563803 11101 solver.cpp:312] Iteration 46000 (2.82286 iter/s, 35.4251s/100 iter), loss = 2.52967
I0814 21:50:12.563979 11101 solver.cpp:334]     Train net output #0: loss = 2.5568 (* 1 = 2.5568 loss)
I0814 21:50:12.564066 11101 sgd_solver.cpp:136] Iteration 46000, lr = 0.085625, m = 0.9
I0814 21:50:27.186584 11101 solver.cpp:312] Iteration 46100 (6.83884 iter/s, 14.6224s/100 iter), loss = 2.16077
I0814 21:50:27.186771 11101 solver.cpp:334]     Train net output #0: loss = 2.03009 (* 1 = 2.03009 loss)
I0814 21:50:27.186883 11101 sgd_solver.cpp:136] Iteration 46100, lr = 0.0855938, m = 0.9
I0814 21:50:41.724606 11101 solver.cpp:312] Iteration 46200 (6.87871 iter/s, 14.5376s/100 iter), loss = 1.86154
I0814 21:50:41.724665 11101 solver.cpp:334]     Train net output #0: loss = 1.32329 (* 1 = 1.32329 loss)
I0814 21:50:41.724671 11101 sgd_solver.cpp:136] Iteration 46200, lr = 0.0855625, m = 0.9
I0814 21:50:56.227223 11101 solver.cpp:312] Iteration 46300 (6.8955 iter/s, 14.5022s/100 iter), loss = 2.23688
I0814 21:50:56.227332 11101 solver.cpp:334]     Train net output #0: loss = 2.14438 (* 1 = 2.14438 loss)
I0814 21:50:56.227352 11101 sgd_solver.cpp:136] Iteration 46300, lr = 0.0855312, m = 0.9
I0814 21:51:10.803324 11101 solver.cpp:312] Iteration 46400 (6.86074 iter/s, 14.5757s/100 iter), loss = 2.24365
I0814 21:51:10.803350 11101 solver.cpp:334]     Train net output #0: loss = 1.86185 (* 1 = 1.86185 loss)
I0814 21:51:10.803354 11101 sgd_solver.cpp:136] Iteration 46400, lr = 0.0855, m = 0.9
I0814 21:51:25.904698 11101 solver.cpp:312] Iteration 46500 (6.6221 iter/s, 15.1009s/100 iter), loss = 2.36503
I0814 21:51:25.904772 11101 solver.cpp:334]     Train net output #0: loss = 2.24702 (* 1 = 2.24702 loss)
I0814 21:51:25.904791 11101 sgd_solver.cpp:136] Iteration 46500, lr = 0.0854688, m = 0.9
I0814 21:51:40.765928 11101 solver.cpp:312] Iteration 46600 (6.72911 iter/s, 14.8608s/100 iter), loss = 1.96295
I0814 21:51:40.766494 11101 solver.cpp:334]     Train net output #0: loss = 1.63468 (* 1 = 1.63468 loss)
I0814 21:51:40.766511 11101 sgd_solver.cpp:136] Iteration 46600, lr = 0.0854375, m = 0.9
I0814 21:51:55.515167 11101 solver.cpp:312] Iteration 46700 (6.7802 iter/s, 14.7488s/100 iter), loss = 2.73528
I0814 21:51:55.515197 11101 solver.cpp:334]     Train net output #0: loss = 2.39654 (* 1 = 2.39654 loss)
I0814 21:51:55.515234 11101 sgd_solver.cpp:136] Iteration 46700, lr = 0.0854063, m = 0.9
I0814 21:52:10.299307 11101 solver.cpp:312] Iteration 46800 (6.76419 iter/s, 14.7837s/100 iter), loss = 2.48131
I0814 21:52:10.299331 11101 solver.cpp:334]     Train net output #0: loss = 2.76419 (* 1 = 2.76419 loss)
I0814 21:52:10.299335 11101 sgd_solver.cpp:136] Iteration 46800, lr = 0.085375, m = 0.9
I0814 21:52:25.120749 11101 solver.cpp:312] Iteration 46900 (6.74717 iter/s, 14.821s/100 iter), loss = 2.19634
I0814 21:52:25.124163 11101 solver.cpp:334]     Train net output #0: loss = 2.33288 (* 1 = 2.33288 loss)
I0814 21:52:25.124174 11101 sgd_solver.cpp:136] Iteration 46900, lr = 0.0853437, m = 0.9
I0814 21:52:39.812829 11101 solver.cpp:312] Iteration 47000 (6.80658 iter/s, 14.6917s/100 iter), loss = 2.32871
I0814 21:52:39.812855 11101 solver.cpp:334]     Train net output #0: loss = 2.02338 (* 1 = 2.02338 loss)
I0814 21:52:39.812860 11101 sgd_solver.cpp:136] Iteration 47000, lr = 0.0853125, m = 0.9
I0814 21:52:54.409755 11101 solver.cpp:312] Iteration 47100 (6.85095 iter/s, 14.5965s/100 iter), loss = 2.75803
I0814 21:52:54.409780 11101 solver.cpp:334]     Train net output #0: loss = 3.03745 (* 1 = 3.03745 loss)
I0814 21:52:54.409786 11101 sgd_solver.cpp:136] Iteration 47100, lr = 0.0852813, m = 0.9
I0814 21:53:09.081441 11101 solver.cpp:312] Iteration 47200 (6.81604 iter/s, 14.6713s/100 iter), loss = 2.37977
I0814 21:53:09.081688 11101 solver.cpp:334]     Train net output #0: loss = 1.84811 (* 1 = 1.84811 loss)
I0814 21:53:09.082080 11101 sgd_solver.cpp:136] Iteration 47200, lr = 0.08525, m = 0.9
I0814 21:53:23.657500 11101 solver.cpp:312] Iteration 47300 (6.86076 iter/s, 14.5756s/100 iter), loss = 2.03493
I0814 21:53:23.657569 11101 solver.cpp:334]     Train net output #0: loss = 2.11645 (* 1 = 2.11645 loss)
I0814 21:53:23.657588 11101 sgd_solver.cpp:136] Iteration 47300, lr = 0.0852187, m = 0.9
I0814 21:53:38.650168 11101 solver.cpp:312] Iteration 47400 (6.67012 iter/s, 14.9922s/100 iter), loss = 2.14285
I0814 21:53:38.650218 11101 solver.cpp:334]     Train net output #0: loss = 2.29012 (* 1 = 2.29012 loss)
I0814 21:53:38.650233 11101 sgd_solver.cpp:136] Iteration 47400, lr = 0.0851875, m = 0.9
I0814 21:53:53.390655 11101 solver.cpp:312] Iteration 47500 (6.78423 iter/s, 14.7401s/100 iter), loss = 2.26876
I0814 21:53:53.390715 11101 solver.cpp:334]     Train net output #0: loss = 2.35519 (* 1 = 2.35519 loss)
I0814 21:53:53.390722 11101 sgd_solver.cpp:136] Iteration 47500, lr = 0.0851563, m = 0.9
I0814 21:54:08.025045 11101 solver.cpp:312] Iteration 47600 (6.83341 iter/s, 14.634s/100 iter), loss = 2.75918
I0814 21:54:08.025100 11101 solver.cpp:334]     Train net output #0: loss = 2.71888 (* 1 = 2.71888 loss)
I0814 21:54:08.025115 11101 sgd_solver.cpp:136] Iteration 47600, lr = 0.085125, m = 0.9
I0814 21:54:22.626353 11101 solver.cpp:312] Iteration 47700 (6.8489 iter/s, 14.6009s/100 iter), loss = 2.4405
I0814 21:54:22.626381 11101 solver.cpp:334]     Train net output #0: loss = 2.04435 (* 1 = 2.04435 loss)
I0814 21:54:22.626389 11101 sgd_solver.cpp:136] Iteration 47700, lr = 0.0850938, m = 0.9
I0814 21:54:37.183058 11101 solver.cpp:312] Iteration 47800 (6.86988 iter/s, 14.5563s/100 iter), loss = 1.8927
I0814 21:54:37.183133 11101 solver.cpp:334]     Train net output #0: loss = 1.86952 (* 1 = 1.86952 loss)
I0814 21:54:37.183140 11101 sgd_solver.cpp:136] Iteration 47800, lr = 0.0850625, m = 0.9
I0814 21:54:52.344270 11101 solver.cpp:312] Iteration 47900 (6.59596 iter/s, 15.1608s/100 iter), loss = 2.0527
I0814 21:54:52.344303 11101 solver.cpp:334]     Train net output #0: loss = 2.4184 (* 1 = 2.4184 loss)
I0814 21:54:52.344310 11101 sgd_solver.cpp:136] Iteration 47900, lr = 0.0850312, m = 0.9
I0814 21:55:07.046808 11101 solver.cpp:509] Iteration 48000, Testing net (#0)
I0814 21:55:10.576266 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 21:55:28.360476 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.373529
I0814 21:55:28.360496 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.633999
I0814 21:55:28.360502 11101 solver.cpp:594]     Test net output #2: loss = 2.93901 (* 1 = 2.93901 loss)
I0814 21:55:28.360522 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.312s
I0814 21:55:28.527001 11101 solver.cpp:312] Iteration 48000 (2.76383 iter/s, 36.1817s/100 iter), loss = 2.50508
I0814 21:55:28.527070 11101 solver.cpp:334]     Train net output #0: loss = 2.70587 (* 1 = 2.70587 loss)
I0814 21:55:28.527089 11101 sgd_solver.cpp:136] Iteration 48000, lr = 0.085, m = 0.9
I0814 21:55:43.188524 11101 solver.cpp:312] Iteration 48100 (6.82077 iter/s, 14.6611s/100 iter), loss = 2.16087
I0814 21:55:43.188602 11101 solver.cpp:334]     Train net output #0: loss = 2.37103 (* 1 = 2.37103 loss)
I0814 21:55:43.188614 11101 sgd_solver.cpp:136] Iteration 48100, lr = 0.0849688, m = 0.9
I0814 21:55:57.823158 11101 solver.cpp:312] Iteration 48200 (6.8333 iter/s, 14.6342s/100 iter), loss = 2.65953
I0814 21:55:57.823220 11101 solver.cpp:334]     Train net output #0: loss = 2.85051 (* 1 = 2.85051 loss)
I0814 21:55:57.823235 11101 sgd_solver.cpp:136] Iteration 48200, lr = 0.0849375, m = 0.9
I0814 21:56:12.358147 11101 solver.cpp:312] Iteration 48300 (6.88015 iter/s, 14.5346s/100 iter), loss = 1.89091
I0814 21:56:12.358176 11101 solver.cpp:334]     Train net output #0: loss = 2.08658 (* 1 = 2.08658 loss)
I0814 21:56:12.358182 11101 sgd_solver.cpp:136] Iteration 48300, lr = 0.0849063, m = 0.9
I0814 21:56:27.198529 11101 solver.cpp:312] Iteration 48400 (6.73856 iter/s, 14.84s/100 iter), loss = 2.29125
I0814 21:56:27.198626 11101 solver.cpp:334]     Train net output #0: loss = 1.73116 (* 1 = 1.73116 loss)
I0814 21:56:27.198647 11101 sgd_solver.cpp:136] Iteration 48400, lr = 0.084875, m = 0.9
I0814 21:56:41.966723 11101 solver.cpp:312] Iteration 48500 (6.7715 iter/s, 14.7678s/100 iter), loss = 2.21548
I0814 21:56:41.966750 11101 solver.cpp:334]     Train net output #0: loss = 2.32722 (* 1 = 2.32722 loss)
I0814 21:56:41.966784 11101 sgd_solver.cpp:136] Iteration 48500, lr = 0.0848437, m = 0.9
I0814 21:56:56.715708 11101 solver.cpp:312] Iteration 48600 (6.78032 iter/s, 14.7486s/100 iter), loss = 2.23925
I0814 21:56:56.715735 11101 solver.cpp:334]     Train net output #0: loss = 2.40239 (* 1 = 2.40239 loss)
I0814 21:56:56.715741 11101 sgd_solver.cpp:136] Iteration 48600, lr = 0.0848125, m = 0.9
I0814 21:57:11.404606 11101 solver.cpp:312] Iteration 48700 (6.80805 iter/s, 14.6885s/100 iter), loss = 2.20117
I0814 21:57:11.404695 11101 solver.cpp:334]     Train net output #0: loss = 2.05267 (* 1 = 2.05267 loss)
I0814 21:57:11.404709 11101 sgd_solver.cpp:136] Iteration 48700, lr = 0.0847813, m = 0.9
I0814 21:57:26.109210 11101 solver.cpp:312] Iteration 48800 (6.80078 iter/s, 14.7042s/100 iter), loss = 2.21626
I0814 21:57:26.109236 11101 solver.cpp:334]     Train net output #0: loss = 2.53196 (* 1 = 2.53196 loss)
I0814 21:57:26.109241 11101 sgd_solver.cpp:136] Iteration 48800, lr = 0.08475, m = 0.9
I0814 21:57:40.685642 11101 solver.cpp:312] Iteration 48900 (6.86058 iter/s, 14.576s/100 iter), loss = 2.63459
I0814 21:57:40.685668 11101 solver.cpp:334]     Train net output #0: loss = 2.565 (* 1 = 2.565 loss)
I0814 21:57:40.685673 11101 sgd_solver.cpp:136] Iteration 48900, lr = 0.0847188, m = 0.9
I0814 21:57:55.246175 11101 solver.cpp:312] Iteration 49000 (6.86807 iter/s, 14.5601s/100 iter), loss = 2.52836
I0814 21:57:55.246265 11101 solver.cpp:334]     Train net output #0: loss = 2.59847 (* 1 = 2.59847 loss)
I0814 21:57:55.246284 11101 sgd_solver.cpp:136] Iteration 49000, lr = 0.0846875, m = 0.9
I0814 21:58:09.915853 11101 solver.cpp:312] Iteration 49100 (6.81697 iter/s, 14.6693s/100 iter), loss = 2.34544
I0814 21:58:09.915881 11101 solver.cpp:334]     Train net output #0: loss = 2.28487 (* 1 = 2.28487 loss)
I0814 21:58:09.915887 11101 sgd_solver.cpp:136] Iteration 49100, lr = 0.0846563, m = 0.9
I0814 21:58:24.361470 11101 solver.cpp:312] Iteration 49200 (6.92271 iter/s, 14.4452s/100 iter), loss = 2.5329
I0814 21:58:24.361536 11101 solver.cpp:334]     Train net output #0: loss = 2.76432 (* 1 = 2.76432 loss)
I0814 21:58:24.361553 11101 sgd_solver.cpp:136] Iteration 49200, lr = 0.084625, m = 0.9
I0814 21:58:38.762127 11101 solver.cpp:312] Iteration 49300 (6.94432 iter/s, 14.4003s/100 iter), loss = 2.23491
I0814 21:58:38.762223 11101 solver.cpp:334]     Train net output #0: loss = 2.32972 (* 1 = 2.32972 loss)
I0814 21:58:38.762239 11101 sgd_solver.cpp:136] Iteration 49300, lr = 0.0845938, m = 0.9
I0814 21:58:53.464530 11101 solver.cpp:312] Iteration 49400 (6.8018 iter/s, 14.702s/100 iter), loss = 1.81222
I0814 21:58:53.464561 11101 solver.cpp:334]     Train net output #0: loss = 1.75137 (* 1 = 1.75137 loss)
I0814 21:58:53.464568 11101 sgd_solver.cpp:136] Iteration 49400, lr = 0.0845625, m = 0.9
I0814 21:59:07.929872 11101 solver.cpp:312] Iteration 49500 (6.91327 iter/s, 14.4649s/100 iter), loss = 2.0762
I0814 21:59:07.929941 11101 solver.cpp:334]     Train net output #0: loss = 1.64595 (* 1 = 1.64595 loss)
I0814 21:59:07.929961 11101 sgd_solver.cpp:136] Iteration 49500, lr = 0.0845312, m = 0.9
I0814 21:59:22.660490 11101 solver.cpp:312] Iteration 49600 (6.78878 iter/s, 14.7302s/100 iter), loss = 2.24669
I0814 21:59:22.663075 11101 solver.cpp:334]     Train net output #0: loss = 2.32001 (* 1 = 2.32001 loss)
I0814 21:59:22.663100 11101 sgd_solver.cpp:136] Iteration 49600, lr = 0.0845, m = 0.9
I0814 21:59:37.522527 11101 solver.cpp:312] Iteration 49700 (6.72874 iter/s, 14.8616s/100 iter), loss = 2.42478
I0814 21:59:37.522552 11101 solver.cpp:334]     Train net output #0: loss = 2.2397 (* 1 = 2.2397 loss)
I0814 21:59:37.522557 11101 sgd_solver.cpp:136] Iteration 49700, lr = 0.0844688, m = 0.9
I0814 21:59:52.575603 11101 solver.cpp:312] Iteration 49800 (6.64335 iter/s, 15.0527s/100 iter), loss = 2.47117
I0814 21:59:52.575678 11101 solver.cpp:334]     Train net output #0: loss = 2.71329 (* 1 = 2.71329 loss)
I0814 21:59:52.575698 11101 sgd_solver.cpp:136] Iteration 49800, lr = 0.0844375, m = 0.9
I0814 22:00:07.193325 11101 solver.cpp:312] Iteration 49900 (6.84121 iter/s, 14.6173s/100 iter), loss = 1.96225
I0814 22:00:07.193382 11101 solver.cpp:334]     Train net output #0: loss = 2.08931 (* 1 = 2.08931 loss)
I0814 22:00:07.193390 11101 sgd_solver.cpp:136] Iteration 49900, lr = 0.0844062, m = 0.9
I0814 22:00:21.633059 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_50000.caffemodel
I0814 22:00:21.649960 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_50000.solverstate
I0814 22:00:21.655912 11101 solver.cpp:509] Iteration 50000, Testing net (#0)
I0814 22:00:42.342371 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.354294
I0814 22:00:42.342480 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.61694
I0814 22:00:42.342490 11101 solver.cpp:594]     Test net output #2: loss = 3.03641 (* 1 = 3.03641 loss)
I0814 22:00:42.342509 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.686s
I0814 22:00:42.497695 11101 solver.cpp:312] Iteration 50000 (2.83259 iter/s, 35.3034s/100 iter), loss = 2.16297
I0814 22:00:42.497720 11101 solver.cpp:334]     Train net output #0: loss = 2.12409 (* 1 = 2.12409 loss)
I0814 22:00:42.497726 11101 sgd_solver.cpp:136] Iteration 50000, lr = 0.084375, m = 0.9
I0814 22:00:57.348266 11101 solver.cpp:312] Iteration 50100 (6.73394 iter/s, 14.8501s/100 iter), loss = 2.18815
I0814 22:00:57.348338 11101 solver.cpp:334]     Train net output #0: loss = 2.18528 (* 1 = 2.18528 loss)
I0814 22:00:57.348359 11101 sgd_solver.cpp:136] Iteration 50100, lr = 0.0843438, m = 0.9
I0814 22:01:12.080047 11101 solver.cpp:312] Iteration 50200 (6.78824 iter/s, 14.7314s/100 iter), loss = 2.33392
I0814 22:01:12.080075 11101 solver.cpp:334]     Train net output #0: loss = 1.95555 (* 1 = 1.95555 loss)
I0814 22:01:12.080080 11101 sgd_solver.cpp:136] Iteration 50200, lr = 0.0843125, m = 0.9
I0814 22:01:26.708557 11101 solver.cpp:312] Iteration 50300 (6.83616 iter/s, 14.6281s/100 iter), loss = 2.452
I0814 22:01:26.708621 11101 solver.cpp:334]     Train net output #0: loss = 2.4792 (* 1 = 2.4792 loss)
I0814 22:01:26.708628 11101 sgd_solver.cpp:136] Iteration 50300, lr = 0.0842813, m = 0.9
I0814 22:01:41.167759 11101 solver.cpp:312] Iteration 50400 (6.91621 iter/s, 14.4588s/100 iter), loss = 2.52069
I0814 22:01:41.167811 11101 solver.cpp:334]     Train net output #0: loss = 2.6099 (* 1 = 2.6099 loss)
I0814 22:01:41.167825 11101 sgd_solver.cpp:136] Iteration 50400, lr = 0.08425, m = 0.9
I0814 22:01:55.798794 11101 solver.cpp:312] Iteration 50500 (6.83498 iter/s, 14.6306s/100 iter), loss = 2.35917
I0814 22:01:55.798818 11101 solver.cpp:334]     Train net output #0: loss = 2.27431 (* 1 = 2.27431 loss)
I0814 22:01:55.798822 11101 sgd_solver.cpp:136] Iteration 50500, lr = 0.0842188, m = 0.9
I0814 22:02:10.261034 11101 solver.cpp:312] Iteration 50600 (6.91475 iter/s, 14.4618s/100 iter), loss = 2.06505
I0814 22:02:10.261096 11101 solver.cpp:334]     Train net output #0: loss = 2.3966 (* 1 = 2.3966 loss)
I0814 22:02:10.261123 11101 sgd_solver.cpp:136] Iteration 50600, lr = 0.0841875, m = 0.9
I0814 22:02:24.722980 11101 solver.cpp:312] Iteration 50700 (6.91489 iter/s, 14.4615s/100 iter), loss = 2.16314
I0814 22:02:24.723045 11101 solver.cpp:334]     Train net output #0: loss = 2.19973 (* 1 = 2.19973 loss)
I0814 22:02:24.723063 11101 sgd_solver.cpp:136] Iteration 50700, lr = 0.0841563, m = 0.9
I0814 22:02:39.538404 11101 solver.cpp:312] Iteration 50800 (6.74991 iter/s, 14.815s/100 iter), loss = 2.38217
I0814 22:02:39.538434 11101 solver.cpp:334]     Train net output #0: loss = 2.45686 (* 1 = 2.45686 loss)
I0814 22:02:39.538440 11101 sgd_solver.cpp:136] Iteration 50800, lr = 0.084125, m = 0.9
I0814 22:02:54.442656 11101 solver.cpp:312] Iteration 50900 (6.71 iter/s, 14.9031s/100 iter), loss = 2.17181
I0814 22:02:54.442715 11101 solver.cpp:334]     Train net output #0: loss = 2.17053 (* 1 = 2.17053 loss)
I0814 22:02:54.442721 11101 sgd_solver.cpp:136] Iteration 50900, lr = 0.0840937, m = 0.9
I0814 22:03:09.126399 11101 solver.cpp:312] Iteration 51000 (6.81045 iter/s, 14.6833s/100 iter), loss = 2.20717
I0814 22:03:09.126456 11101 solver.cpp:334]     Train net output #0: loss = 2.48592 (* 1 = 2.48592 loss)
I0814 22:03:09.126468 11101 sgd_solver.cpp:136] Iteration 51000, lr = 0.0840625, m = 0.9
I0814 22:03:23.590917 11101 solver.cpp:312] Iteration 51100 (6.91366 iter/s, 14.4641s/100 iter), loss = 2.29151
I0814 22:03:23.590986 11101 solver.cpp:334]     Train net output #0: loss = 2.22216 (* 1 = 2.22216 loss)
I0814 22:03:23.591003 11101 sgd_solver.cpp:136] Iteration 51100, lr = 0.0840312, m = 0.9
I0814 22:03:38.164178 11101 solver.cpp:312] Iteration 51200 (6.86207 iter/s, 14.5729s/100 iter), loss = 2.28837
I0814 22:03:38.164283 11101 solver.cpp:334]     Train net output #0: loss = 2.12482 (* 1 = 2.12482 loss)
I0814 22:03:38.164301 11101 sgd_solver.cpp:136] Iteration 51200, lr = 0.084, m = 0.9
I0814 22:03:52.612167 11101 solver.cpp:312] Iteration 51300 (6.92157 iter/s, 14.4476s/100 iter), loss = 2.17652
I0814 22:03:52.612237 11101 solver.cpp:334]     Train net output #0: loss = 2.38713 (* 1 = 2.38713 loss)
I0814 22:03:52.612251 11101 sgd_solver.cpp:136] Iteration 51300, lr = 0.0839688, m = 0.9
I0814 22:04:07.162345 11101 solver.cpp:312] Iteration 51400 (6.87296 iter/s, 14.5498s/100 iter), loss = 2.52194
I0814 22:04:07.162396 11101 solver.cpp:334]     Train net output #0: loss = 2.34704 (* 1 = 2.34704 loss)
I0814 22:04:07.162408 11101 sgd_solver.cpp:136] Iteration 51400, lr = 0.0839375, m = 0.9
I0814 22:04:21.505067 11101 solver.cpp:312] Iteration 51500 (6.97237 iter/s, 14.3423s/100 iter), loss = 2.19798
I0814 22:04:21.505129 11101 solver.cpp:334]     Train net output #0: loss = 2.12438 (* 1 = 2.12438 loss)
I0814 22:04:21.505137 11101 sgd_solver.cpp:136] Iteration 51500, lr = 0.0839063, m = 0.9
I0814 22:04:35.678859 11101 solver.cpp:312] Iteration 51600 (7.05547 iter/s, 14.1734s/100 iter), loss = 1.97861
I0814 22:04:35.678886 11101 solver.cpp:334]     Train net output #0: loss = 1.9406 (* 1 = 1.9406 loss)
I0814 22:04:35.678894 11101 sgd_solver.cpp:136] Iteration 51600, lr = 0.083875, m = 0.9
I0814 22:04:50.136430 11101 solver.cpp:312] Iteration 51700 (6.91698 iter/s, 14.4572s/100 iter), loss = 2.09259
I0814 22:04:50.136451 11101 solver.cpp:334]     Train net output #0: loss = 2.22522 (* 1 = 2.22522 loss)
I0814 22:04:50.136456 11101 sgd_solver.cpp:136] Iteration 51700, lr = 0.0838438, m = 0.9
I0814 22:05:04.938560 11101 solver.cpp:312] Iteration 51800 (6.75597 iter/s, 14.8017s/100 iter), loss = 1.78798
I0814 22:05:04.938797 11101 solver.cpp:334]     Train net output #0: loss = 1.3357 (* 1 = 1.3357 loss)
I0814 22:05:04.938805 11101 sgd_solver.cpp:136] Iteration 51800, lr = 0.0838125, m = 0.9
I0814 22:05:19.965919 11101 solver.cpp:312] Iteration 51900 (6.65472 iter/s, 15.0269s/100 iter), loss = 2.5121
I0814 22:05:19.965946 11101 solver.cpp:334]     Train net output #0: loss = 2.04503 (* 1 = 2.04503 loss)
I0814 22:05:19.965951 11101 sgd_solver.cpp:136] Iteration 51900, lr = 0.0837812, m = 0.9
I0814 22:05:34.374723 11101 solver.cpp:509] Iteration 52000, Testing net (#0)
I0814 22:05:55.314033 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.364706
I0814 22:05:55.314085 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.622588
I0814 22:05:55.314092 11101 solver.cpp:594]     Test net output #2: loss = 3.00218 (* 1 = 3.00218 loss)
I0814 22:05:55.314113 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9388s
I0814 22:05:55.473970 11101 solver.cpp:312] Iteration 52000 (2.81634 iter/s, 35.5071s/100 iter), loss = 2.3744
I0814 22:05:55.473999 11101 solver.cpp:334]     Train net output #0: loss = 1.98926 (* 1 = 1.98926 loss)
I0814 22:05:55.474006 11101 sgd_solver.cpp:136] Iteration 52000, lr = 0.08375, m = 0.9
I0814 22:06:10.514964 11101 solver.cpp:312] Iteration 52100 (6.64868 iter/s, 15.0406s/100 iter), loss = 2.06892
I0814 22:06:10.514991 11101 solver.cpp:334]     Train net output #0: loss = 1.97603 (* 1 = 1.97603 loss)
I0814 22:06:10.514997 11101 sgd_solver.cpp:136] Iteration 52100, lr = 0.0837188, m = 0.9
I0814 22:06:25.503983 11101 solver.cpp:312] Iteration 52200 (6.67174 iter/s, 14.9886s/100 iter), loss = 2.01688
I0814 22:06:25.504057 11101 solver.cpp:334]     Train net output #0: loss = 1.96541 (* 1 = 1.96541 loss)
I0814 22:06:25.504066 11101 sgd_solver.cpp:136] Iteration 52200, lr = 0.0836875, m = 0.9
I0814 22:06:40.290645 11101 solver.cpp:312] Iteration 52300 (6.76304 iter/s, 14.7862s/100 iter), loss = 2.42099
I0814 22:06:40.290707 11101 solver.cpp:334]     Train net output #0: loss = 2.48294 (* 1 = 2.48294 loss)
I0814 22:06:40.290725 11101 sgd_solver.cpp:136] Iteration 52300, lr = 0.0836563, m = 0.9
I0814 22:06:54.760123 11101 solver.cpp:312] Iteration 52400 (6.9113 iter/s, 14.4691s/100 iter), loss = 1.95224
I0814 22:06:54.760154 11101 solver.cpp:334]     Train net output #0: loss = 2.0023 (* 1 = 2.0023 loss)
I0814 22:06:54.760160 11101 sgd_solver.cpp:136] Iteration 52400, lr = 0.083625, m = 0.9
I0814 22:07:09.635677 11101 solver.cpp:312] Iteration 52500 (6.72263 iter/s, 14.8751s/100 iter), loss = 2.38155
I0814 22:07:09.636395 11101 solver.cpp:334]     Train net output #0: loss = 1.75361 (* 1 = 1.75361 loss)
I0814 22:07:09.636404 11101 sgd_solver.cpp:136] Iteration 52500, lr = 0.0835937, m = 0.9
I0814 22:07:24.403991 11101 solver.cpp:312] Iteration 52600 (6.77144 iter/s, 14.7679s/100 iter), loss = 2.04426
I0814 22:07:24.404018 11101 solver.cpp:334]     Train net output #0: loss = 2.12613 (* 1 = 2.12613 loss)
I0814 22:07:24.404023 11101 sgd_solver.cpp:136] Iteration 52600, lr = 0.0835625, m = 0.9
I0814 22:07:38.917920 11101 solver.cpp:312] Iteration 52700 (6.89013 iter/s, 14.5135s/100 iter), loss = 1.98049
I0814 22:07:38.917986 11101 solver.cpp:334]     Train net output #0: loss = 1.62164 (* 1 = 1.62164 loss)
I0814 22:07:38.918004 11101 sgd_solver.cpp:136] Iteration 52700, lr = 0.0835313, m = 0.9
I0814 22:07:53.499892 11101 solver.cpp:312] Iteration 52800 (6.85798 iter/s, 14.5816s/100 iter), loss = 2.48633
I0814 22:07:53.500082 11101 solver.cpp:334]     Train net output #0: loss = 2.15963 (* 1 = 2.15963 loss)
I0814 22:07:53.500185 11101 sgd_solver.cpp:136] Iteration 52800, lr = 0.0835, m = 0.9
I0814 22:08:08.152492 11101 solver.cpp:312] Iteration 52900 (6.82492 iter/s, 14.6522s/100 iter), loss = 2.43768
I0814 22:08:08.152519 11101 solver.cpp:334]     Train net output #0: loss = 2.37692 (* 1 = 2.37692 loss)
I0814 22:08:08.152523 11101 sgd_solver.cpp:136] Iteration 52900, lr = 0.0834688, m = 0.9
I0814 22:08:22.890225 11101 solver.cpp:312] Iteration 53000 (6.78549 iter/s, 14.7373s/100 iter), loss = 2.28131
I0814 22:08:22.890254 11101 solver.cpp:334]     Train net output #0: loss = 2.18354 (* 1 = 2.18354 loss)
I0814 22:08:22.890260 11101 sgd_solver.cpp:136] Iteration 53000, lr = 0.0834375, m = 0.9
I0814 22:08:37.673332 11101 solver.cpp:312] Iteration 53100 (6.76467 iter/s, 14.7827s/100 iter), loss = 2.08525
I0814 22:08:37.673409 11101 solver.cpp:334]     Train net output #0: loss = 2.04434 (* 1 = 2.04434 loss)
I0814 22:08:37.673422 11101 sgd_solver.cpp:136] Iteration 53100, lr = 0.0834063, m = 0.9
I0814 22:08:52.598587 11101 solver.cpp:312] Iteration 53200 (6.70024 iter/s, 14.9248s/100 iter), loss = 1.96055
I0814 22:08:52.598611 11101 solver.cpp:334]     Train net output #0: loss = 1.85736 (* 1 = 1.85736 loss)
I0814 22:08:52.598618 11101 sgd_solver.cpp:136] Iteration 53200, lr = 0.083375, m = 0.9
I0814 22:09:07.360195 11101 solver.cpp:312] Iteration 53300 (6.77452 iter/s, 14.7612s/100 iter), loss = 2.18414
I0814 22:09:07.360278 11101 solver.cpp:334]     Train net output #0: loss = 2.29773 (* 1 = 2.29773 loss)
I0814 22:09:07.360301 11101 sgd_solver.cpp:136] Iteration 53300, lr = 0.0833438, m = 0.9
I0814 22:09:21.955996 11101 solver.cpp:312] Iteration 53400 (6.85148 iter/s, 14.5954s/100 iter), loss = 2.45305
I0814 22:09:21.956079 11101 solver.cpp:334]     Train net output #0: loss = 2.39756 (* 1 = 2.39756 loss)
I0814 22:09:21.956091 11101 sgd_solver.cpp:136] Iteration 53400, lr = 0.0833125, m = 0.9
I0814 22:09:36.358681 11101 solver.cpp:312] Iteration 53500 (6.94335 iter/s, 14.4023s/100 iter), loss = 2.08963
I0814 22:09:36.358707 11101 solver.cpp:334]     Train net output #0: loss = 2.35672 (* 1 = 2.35672 loss)
I0814 22:09:36.358713 11101 sgd_solver.cpp:136] Iteration 53500, lr = 0.0832812, m = 0.9
I0814 22:09:50.873314 11101 solver.cpp:312] Iteration 53600 (6.88979 iter/s, 14.5142s/100 iter), loss = 2.28962
I0814 22:09:50.873386 11101 solver.cpp:334]     Train net output #0: loss = 2.34343 (* 1 = 2.34343 loss)
I0814 22:09:50.873406 11101 sgd_solver.cpp:136] Iteration 53600, lr = 0.08325, m = 0.9
I0814 22:10:05.562919 11101 solver.cpp:312] Iteration 53700 (6.80773 iter/s, 14.6892s/100 iter), loss = 2.07513
I0814 22:10:05.562988 11101 solver.cpp:334]     Train net output #0: loss = 2.06098 (* 1 = 2.06098 loss)
I0814 22:10:05.563012 11101 sgd_solver.cpp:136] Iteration 53700, lr = 0.0832188, m = 0.9
I0814 22:10:19.997119 11101 solver.cpp:312] Iteration 53800 (6.92818 iter/s, 14.4338s/100 iter), loss = 2.34229
I0814 22:10:19.997151 11101 solver.cpp:334]     Train net output #0: loss = 2.94811 (* 1 = 2.94811 loss)
I0814 22:10:19.997158 11101 sgd_solver.cpp:136] Iteration 53800, lr = 0.0831875, m = 0.9
I0814 22:10:34.398690 11101 solver.cpp:312] Iteration 53900 (6.94388 iter/s, 14.4012s/100 iter), loss = 2.37311
I0814 22:10:34.398717 11101 solver.cpp:334]     Train net output #0: loss = 2.12734 (* 1 = 2.12734 loss)
I0814 22:10:34.398725 11101 sgd_solver.cpp:136] Iteration 53900, lr = 0.0831563, m = 0.9
I0814 22:10:48.709157 11101 solver.cpp:509] Iteration 54000, Testing net (#0)
I0814 22:10:49.849040 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 22:11:09.467161 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.382588
I0814 22:11:09.467180 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.643058
I0814 22:11:09.467188 11101 solver.cpp:594]     Test net output #2: loss = 2.87094 (* 1 = 2.87094 loss)
I0814 22:11:09.467211 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7575s
I0814 22:11:09.604874 11101 solver.cpp:312] Iteration 54000 (2.84049 iter/s, 35.2052s/100 iter), loss = 1.9569
I0814 22:11:09.604899 11101 solver.cpp:334]     Train net output #0: loss = 2.12716 (* 1 = 2.12716 loss)
I0814 22:11:09.604903 11101 sgd_solver.cpp:136] Iteration 54000, lr = 0.083125, m = 0.9
I0814 22:11:24.428321 11101 solver.cpp:312] Iteration 54100 (6.74626 iter/s, 14.823s/100 iter), loss = 1.96415
I0814 22:11:24.428385 11101 solver.cpp:334]     Train net output #0: loss = 2.08736 (* 1 = 2.08736 loss)
I0814 22:11:24.428391 11101 sgd_solver.cpp:136] Iteration 54100, lr = 0.0830938, m = 0.9
I0814 22:11:38.980031 11101 solver.cpp:312] Iteration 54200 (6.87224 iter/s, 14.5513s/100 iter), loss = 2.56283
I0814 22:11:38.980057 11101 solver.cpp:334]     Train net output #0: loss = 2.18197 (* 1 = 2.18197 loss)
I0814 22:11:38.980062 11101 sgd_solver.cpp:136] Iteration 54200, lr = 0.0830625, m = 0.9
I0814 22:11:53.680090 11101 solver.cpp:312] Iteration 54300 (6.80289 iter/s, 14.6996s/100 iter), loss = 2.11899
I0814 22:11:53.680115 11101 solver.cpp:334]     Train net output #0: loss = 1.92229 (* 1 = 1.92229 loss)
I0814 22:11:53.680121 11101 sgd_solver.cpp:136] Iteration 54300, lr = 0.0830313, m = 0.9
I0814 22:12:08.201179 11101 solver.cpp:312] Iteration 54400 (6.88673 iter/s, 14.5207s/100 iter), loss = 2.30932
I0814 22:12:08.201233 11101 solver.cpp:334]     Train net output #0: loss = 2.35053 (* 1 = 2.35053 loss)
I0814 22:12:08.201241 11101 sgd_solver.cpp:136] Iteration 54400, lr = 0.083, m = 0.9
I0814 22:12:22.873785 11101 solver.cpp:312] Iteration 54500 (6.81561 iter/s, 14.6722s/100 iter), loss = 2.39231
I0814 22:12:22.873811 11101 solver.cpp:334]     Train net output #0: loss = 2.78362 (* 1 = 2.78362 loss)
I0814 22:12:22.873817 11101 sgd_solver.cpp:136] Iteration 54500, lr = 0.0829687, m = 0.9
I0814 22:12:37.407353 11101 solver.cpp:312] Iteration 54600 (6.88082 iter/s, 14.5332s/100 iter), loss = 2.06596
I0814 22:12:37.407416 11101 solver.cpp:334]     Train net output #0: loss = 2.0167 (* 1 = 2.0167 loss)
I0814 22:12:37.407434 11101 sgd_solver.cpp:136] Iteration 54600, lr = 0.0829375, m = 0.9
I0814 22:12:52.090998 11101 solver.cpp:312] Iteration 54700 (6.81049 iter/s, 14.6832s/100 iter), loss = 2.31106
I0814 22:12:52.091065 11101 solver.cpp:334]     Train net output #0: loss = 2.43249 (* 1 = 2.43249 loss)
I0814 22:12:52.091071 11101 sgd_solver.cpp:136] Iteration 54700, lr = 0.0829063, m = 0.9
I0814 22:13:06.617344 11101 solver.cpp:312] Iteration 54800 (6.88424 iter/s, 14.5259s/100 iter), loss = 2.11799
I0814 22:13:06.617391 11101 solver.cpp:334]     Train net output #0: loss = 2.28892 (* 1 = 2.28892 loss)
I0814 22:13:06.617405 11101 sgd_solver.cpp:136] Iteration 54800, lr = 0.082875, m = 0.9
I0814 22:13:21.301511 11101 solver.cpp:312] Iteration 54900 (6.81025 iter/s, 14.6837s/100 iter), loss = 2.55921
I0814 22:13:21.301537 11101 solver.cpp:334]     Train net output #0: loss = 2.60276 (* 1 = 2.60276 loss)
I0814 22:13:21.301540 11101 sgd_solver.cpp:136] Iteration 54900, lr = 0.0828438, m = 0.9
I0814 22:13:35.925112 11101 solver.cpp:312] Iteration 55000 (6.83846 iter/s, 14.6232s/100 iter), loss = 2.03903
I0814 22:13:35.925189 11101 solver.cpp:334]     Train net output #0: loss = 2.1995 (* 1 = 2.1995 loss)
I0814 22:13:35.925195 11101 sgd_solver.cpp:136] Iteration 55000, lr = 0.0828125, m = 0.9
I0814 22:13:50.631729 11101 solver.cpp:312] Iteration 55100 (6.79986 iter/s, 14.7062s/100 iter), loss = 2.09196
I0814 22:13:50.631793 11101 solver.cpp:334]     Train net output #0: loss = 1.84083 (* 1 = 1.84083 loss)
I0814 22:13:50.631808 11101 sgd_solver.cpp:136] Iteration 55100, lr = 0.0827812, m = 0.9
I0814 22:14:05.296213 11101 solver.cpp:312] Iteration 55200 (6.81939 iter/s, 14.6641s/100 iter), loss = 2.25751
I0814 22:14:05.296241 11101 solver.cpp:334]     Train net output #0: loss = 2.60877 (* 1 = 2.60877 loss)
I0814 22:14:05.296247 11101 sgd_solver.cpp:136] Iteration 55200, lr = 0.08275, m = 0.9
I0814 22:14:20.045893 11101 solver.cpp:312] Iteration 55300 (6.78 iter/s, 14.7493s/100 iter), loss = 2.00948
I0814 22:14:20.045979 11101 solver.cpp:334]     Train net output #0: loss = 2.14323 (* 1 = 2.14323 loss)
I0814 22:14:20.045986 11101 sgd_solver.cpp:136] Iteration 55300, lr = 0.0827188, m = 0.9
I0814 22:14:34.770195 11101 solver.cpp:312] Iteration 55400 (6.79169 iter/s, 14.7239s/100 iter), loss = 2.5305
I0814 22:14:34.770218 11101 solver.cpp:334]     Train net output #0: loss = 2.43883 (* 1 = 2.43883 loss)
I0814 22:14:34.770225 11101 sgd_solver.cpp:136] Iteration 55400, lr = 0.0826875, m = 0.9
I0814 22:14:49.688966 11101 solver.cpp:312] Iteration 55500 (6.70316 iter/s, 14.9183s/100 iter), loss = 2.22286
I0814 22:14:49.689038 11101 solver.cpp:334]     Train net output #0: loss = 1.99874 (* 1 = 1.99874 loss)
I0814 22:14:49.689057 11101 sgd_solver.cpp:136] Iteration 55500, lr = 0.0826563, m = 0.9
I0814 22:15:05.048801 11101 solver.cpp:312] Iteration 55600 (6.51067 iter/s, 15.3594s/100 iter), loss = 2.54966
I0814 22:15:05.048853 11101 solver.cpp:334]     Train net output #0: loss = 2.6607 (* 1 = 2.6607 loss)
I0814 22:15:05.048859 11101 sgd_solver.cpp:136] Iteration 55600, lr = 0.082625, m = 0.9
I0814 22:15:19.690259 11101 solver.cpp:312] Iteration 55700 (6.83012 iter/s, 14.641s/100 iter), loss = 2.34027
I0814 22:15:19.690323 11101 solver.cpp:334]     Train net output #0: loss = 2.42721 (* 1 = 2.42721 loss)
I0814 22:15:19.690340 11101 sgd_solver.cpp:136] Iteration 55700, lr = 0.0825938, m = 0.9
I0814 22:15:34.208745 11101 solver.cpp:312] Iteration 55800 (6.88797 iter/s, 14.5181s/100 iter), loss = 2.49442
I0814 22:15:34.208772 11101 solver.cpp:334]     Train net output #0: loss = 2.76412 (* 1 = 2.76412 loss)
I0814 22:15:34.208820 11101 sgd_solver.cpp:136] Iteration 55800, lr = 0.0825625, m = 0.9
I0814 22:15:48.875438 11101 solver.cpp:312] Iteration 55900 (6.81836 iter/s, 14.6663s/100 iter), loss = 1.94831
I0814 22:15:48.875495 11101 solver.cpp:334]     Train net output #0: loss = 1.948 (* 1 = 1.948 loss)
I0814 22:15:48.875504 11101 sgd_solver.cpp:136] Iteration 55900, lr = 0.0825313, m = 0.9
I0814 22:16:03.402586 11101 solver.cpp:509] Iteration 56000, Testing net (#0)
I0814 22:16:24.306393 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.327999
I0814 22:16:24.306486 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.570176
I0814 22:16:24.306502 11101 solver.cpp:594]     Test net output #2: loss = 3.31318 (* 1 = 3.31318 loss)
I0814 22:16:24.306535 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9034s
I0814 22:16:24.457003 11101 solver.cpp:312] Iteration 56000 (2.81052 iter/s, 35.5806s/100 iter), loss = 1.89492
I0814 22:16:24.457064 11101 solver.cpp:334]     Train net output #0: loss = 2.09064 (* 1 = 2.09064 loss)
I0814 22:16:24.457082 11101 sgd_solver.cpp:136] Iteration 56000, lr = 0.0825, m = 0.9
I0814 22:16:38.846815 11101 solver.cpp:312] Iteration 56100 (6.94956 iter/s, 14.3894s/100 iter), loss = 2.17955
I0814 22:16:38.846895 11101 solver.cpp:334]     Train net output #0: loss = 2.43455 (* 1 = 2.43455 loss)
I0814 22:16:38.846913 11101 sgd_solver.cpp:136] Iteration 56100, lr = 0.0824687, m = 0.9
I0814 22:16:53.491691 11101 solver.cpp:312] Iteration 56200 (6.82852 iter/s, 14.6445s/100 iter), loss = 2.20574
I0814 22:16:53.491720 11101 solver.cpp:334]     Train net output #0: loss = 2.44203 (* 1 = 2.44203 loss)
I0814 22:16:53.491724 11101 sgd_solver.cpp:136] Iteration 56200, lr = 0.0824375, m = 0.9
I0814 22:17:08.307696 11101 solver.cpp:312] Iteration 56300 (6.74965 iter/s, 14.8156s/100 iter), loss = 2.49042
I0814 22:17:08.307960 11101 solver.cpp:334]     Train net output #0: loss = 2.82737 (* 1 = 2.82737 loss)
I0814 22:17:08.308068 11101 sgd_solver.cpp:136] Iteration 56300, lr = 0.0824062, m = 0.9
I0814 22:17:22.708518 11101 solver.cpp:312] Iteration 56400 (6.94425 iter/s, 14.4004s/100 iter), loss = 2.55792
I0814 22:17:22.708587 11101 solver.cpp:334]     Train net output #0: loss = 2.17047 (* 1 = 2.17047 loss)
I0814 22:17:22.708607 11101 sgd_solver.cpp:136] Iteration 56400, lr = 0.082375, m = 0.9
I0814 22:17:36.960016 11101 solver.cpp:312] Iteration 56500 (7.01701 iter/s, 14.2511s/100 iter), loss = 2.15495
I0814 22:17:36.960086 11101 solver.cpp:334]     Train net output #0: loss = 2.06229 (* 1 = 2.06229 loss)
I0814 22:17:36.960108 11101 sgd_solver.cpp:136] Iteration 56500, lr = 0.0823437, m = 0.9
I0814 22:17:51.696633 11101 solver.cpp:312] Iteration 56600 (6.78601 iter/s, 14.7362s/100 iter), loss = 2.04391
I0814 22:17:51.696735 11101 solver.cpp:334]     Train net output #0: loss = 1.83068 (* 1 = 1.83068 loss)
I0814 22:17:51.696756 11101 sgd_solver.cpp:136] Iteration 56600, lr = 0.0823125, m = 0.9
I0814 22:18:05.990869 11101 solver.cpp:312] Iteration 56700 (6.99603 iter/s, 14.2938s/100 iter), loss = 2.33876
I0814 22:18:05.990895 11101 solver.cpp:334]     Train net output #0: loss = 2.77147 (* 1 = 2.77147 loss)
I0814 22:18:05.990900 11101 sgd_solver.cpp:136] Iteration 56700, lr = 0.0822813, m = 0.9
I0814 22:18:20.388026 11101 solver.cpp:312] Iteration 56800 (6.94601 iter/s, 14.3967s/100 iter), loss = 2.23932
I0814 22:18:20.388054 11101 solver.cpp:334]     Train net output #0: loss = 2.42775 (* 1 = 2.42775 loss)
I0814 22:18:20.388062 11101 sgd_solver.cpp:136] Iteration 56800, lr = 0.08225, m = 0.9
I0814 22:18:35.241041 11101 solver.cpp:312] Iteration 56900 (6.73283 iter/s, 14.8526s/100 iter), loss = 2.11406
I0814 22:18:35.241292 11101 solver.cpp:334]     Train net output #0: loss = 2.25018 (* 1 = 2.25018 loss)
I0814 22:18:35.241403 11101 sgd_solver.cpp:136] Iteration 56900, lr = 0.0822188, m = 0.9
I0814 22:18:50.049473 11101 solver.cpp:312] Iteration 57000 (6.75311 iter/s, 14.808s/100 iter), loss = 2.13366
I0814 22:18:50.049516 11101 solver.cpp:334]     Train net output #0: loss = 2.15627 (* 1 = 2.15627 loss)
I0814 22:18:50.049526 11101 sgd_solver.cpp:136] Iteration 57000, lr = 0.0821875, m = 0.9
I0814 22:19:04.650120 11101 solver.cpp:312] Iteration 57100 (6.8492 iter/s, 14.6002s/100 iter), loss = 2.19602
I0814 22:19:04.650147 11101 solver.cpp:334]     Train net output #0: loss = 1.99385 (* 1 = 1.99385 loss)
I0814 22:19:04.650153 11101 sgd_solver.cpp:136] Iteration 57100, lr = 0.0821563, m = 0.9
I0814 22:19:19.257978 11101 solver.cpp:312] Iteration 57200 (6.84583 iter/s, 14.6074s/100 iter), loss = 2.12293
I0814 22:19:19.258033 11101 solver.cpp:334]     Train net output #0: loss = 1.63953 (* 1 = 1.63953 loss)
I0814 22:19:19.258041 11101 sgd_solver.cpp:136] Iteration 57200, lr = 0.082125, m = 0.9
I0814 22:19:34.363699 11101 solver.cpp:312] Iteration 57300 (6.6202 iter/s, 15.1053s/100 iter), loss = 2.12397
I0814 22:19:34.363773 11101 solver.cpp:334]     Train net output #0: loss = 2.23062 (* 1 = 2.23062 loss)
I0814 22:19:34.363801 11101 sgd_solver.cpp:136] Iteration 57300, lr = 0.0820938, m = 0.9
I0814 22:19:48.998252 11101 solver.cpp:312] Iteration 57400 (6.83334 iter/s, 14.6341s/100 iter), loss = 2.41916
I0814 22:19:48.998330 11101 solver.cpp:334]     Train net output #0: loss = 2.55031 (* 1 = 2.55031 loss)
I0814 22:19:48.998350 11101 sgd_solver.cpp:136] Iteration 57400, lr = 0.0820625, m = 0.9
I0814 22:20:03.763161 11101 solver.cpp:312] Iteration 57500 (6.77301 iter/s, 14.7645s/100 iter), loss = 1.71604
I0814 22:20:03.763228 11101 solver.cpp:334]     Train net output #0: loss = 1.44195 (* 1 = 1.44195 loss)
I0814 22:20:03.763236 11101 sgd_solver.cpp:136] Iteration 57500, lr = 0.0820312, m = 0.9
I0814 22:20:18.383692 11101 solver.cpp:312] Iteration 57600 (6.83989 iter/s, 14.6201s/100 iter), loss = 2.26528
I0814 22:20:18.383718 11101 solver.cpp:334]     Train net output #0: loss = 2.64242 (* 1 = 2.64242 loss)
I0814 22:20:18.383723 11101 sgd_solver.cpp:136] Iteration 57600, lr = 0.082, m = 0.9
I0814 22:20:33.038710 11101 solver.cpp:312] Iteration 57700 (6.82379 iter/s, 14.6546s/100 iter), loss = 2.18266
I0814 22:20:33.038733 11101 solver.cpp:334]     Train net output #0: loss = 2.28691 (* 1 = 2.28691 loss)
I0814 22:20:33.038736 11101 sgd_solver.cpp:136] Iteration 57700, lr = 0.0819687, m = 0.9
I0814 22:20:47.758421 11101 solver.cpp:312] Iteration 57800 (6.7938 iter/s, 14.7193s/100 iter), loss = 2.46347
I0814 22:20:47.758481 11101 solver.cpp:334]     Train net output #0: loss = 2.4233 (* 1 = 2.4233 loss)
I0814 22:20:47.758488 11101 sgd_solver.cpp:136] Iteration 57800, lr = 0.0819375, m = 0.9
I0814 22:21:02.277933 11101 solver.cpp:312] Iteration 57900 (6.88748 iter/s, 14.5191s/100 iter), loss = 2.42197
I0814 22:21:02.277961 11101 solver.cpp:334]     Train net output #0: loss = 2.59879 (* 1 = 2.59879 loss)
I0814 22:21:02.277967 11101 sgd_solver.cpp:136] Iteration 57900, lr = 0.0819063, m = 0.9
I0814 22:21:17.036597 11101 solver.cpp:509] Iteration 58000, Testing net (#0)
I0814 22:21:38.231374 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.381764
I0814 22:21:38.231421 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.638294
I0814 22:21:38.231427 11101 solver.cpp:594]     Test net output #2: loss = 2.87667 (* 1 = 2.87667 loss)
I0814 22:21:38.231446 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1943s
I0814 22:21:38.396509 11101 solver.cpp:312] Iteration 58000 (2.76874 iter/s, 36.1176s/100 iter), loss = 2.6359
I0814 22:21:38.396538 11101 solver.cpp:334]     Train net output #0: loss = 2.13959 (* 1 = 2.13959 loss)
I0814 22:21:38.396544 11101 sgd_solver.cpp:136] Iteration 58000, lr = 0.081875, m = 0.9
I0814 22:21:52.745525 11101 solver.cpp:312] Iteration 58100 (6.96932 iter/s, 14.3486s/100 iter), loss = 2.15392
I0814 22:21:52.745555 11101 solver.cpp:334]     Train net output #0: loss = 2.54085 (* 1 = 2.54085 loss)
I0814 22:21:52.745563 11101 sgd_solver.cpp:136] Iteration 58100, lr = 0.0818438, m = 0.9
I0814 22:22:07.210779 11101 solver.cpp:312] Iteration 58200 (6.91332 iter/s, 14.4648s/100 iter), loss = 2.30755
I0814 22:22:07.210844 11101 solver.cpp:334]     Train net output #0: loss = 2.27802 (* 1 = 2.27802 loss)
I0814 22:22:07.210860 11101 sgd_solver.cpp:136] Iteration 58200, lr = 0.0818125, m = 0.9
I0814 22:22:22.026099 11101 solver.cpp:312] Iteration 58300 (6.74996 iter/s, 14.8149s/100 iter), loss = 2.76267
I0814 22:22:22.026201 11101 solver.cpp:334]     Train net output #0: loss = 2.83111 (* 1 = 2.83111 loss)
I0814 22:22:22.026222 11101 sgd_solver.cpp:136] Iteration 58300, lr = 0.0817813, m = 0.9
I0814 22:22:36.792237 11101 solver.cpp:312] Iteration 58400 (6.77244 iter/s, 14.7657s/100 iter), loss = 2.47066
I0814 22:22:36.792296 11101 solver.cpp:334]     Train net output #0: loss = 2.9053 (* 1 = 2.9053 loss)
I0814 22:22:36.792315 11101 sgd_solver.cpp:136] Iteration 58400, lr = 0.08175, m = 0.9
I0814 22:22:51.358480 11101 solver.cpp:312] Iteration 58500 (6.86539 iter/s, 14.5658s/100 iter), loss = 2.45967
I0814 22:22:51.358503 11101 solver.cpp:334]     Train net output #0: loss = 2.22255 (* 1 = 2.22255 loss)
I0814 22:22:51.358508 11101 sgd_solver.cpp:136] Iteration 58500, lr = 0.0817188, m = 0.9
I0814 22:23:05.868491 11101 solver.cpp:312] Iteration 58600 (6.89199 iter/s, 14.5096s/100 iter), loss = 2.17876
I0814 22:23:05.868826 11101 solver.cpp:334]     Train net output #0: loss = 1.97038 (* 1 = 1.97038 loss)
I0814 22:23:05.868844 11101 sgd_solver.cpp:136] Iteration 58600, lr = 0.0816875, m = 0.9
I0814 22:23:20.361444 11101 solver.cpp:312] Iteration 58700 (6.9001 iter/s, 14.4925s/100 iter), loss = 2.06287
I0814 22:23:20.361469 11101 solver.cpp:334]     Train net output #0: loss = 1.90579 (* 1 = 1.90579 loss)
I0814 22:23:20.361474 11101 sgd_solver.cpp:136] Iteration 58700, lr = 0.0816563, m = 0.9
I0814 22:23:28.794111 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 22:23:35.033905 11101 solver.cpp:312] Iteration 58800 (6.81568 iter/s, 14.672s/100 iter), loss = 2.82861
I0814 22:23:35.033931 11101 solver.cpp:334]     Train net output #0: loss = 2.98712 (* 1 = 2.98712 loss)
I0814 22:23:35.033937 11101 sgd_solver.cpp:136] Iteration 58800, lr = 0.081625, m = 0.9
I0814 22:23:50.003412 11101 solver.cpp:312] Iteration 58900 (6.68044 iter/s, 14.9691s/100 iter), loss = 2.04567
I0814 22:23:50.003482 11101 solver.cpp:334]     Train net output #0: loss = 2.08339 (* 1 = 2.08339 loss)
I0814 22:23:50.003489 11101 sgd_solver.cpp:136] Iteration 58900, lr = 0.0815938, m = 0.9
I0814 22:24:04.635713 11101 solver.cpp:312] Iteration 59000 (6.83439 iter/s, 14.6319s/100 iter), loss = 2.25518
I0814 22:24:04.635783 11101 solver.cpp:334]     Train net output #0: loss = 2.56284 (* 1 = 2.56284 loss)
I0814 22:24:04.635803 11101 sgd_solver.cpp:136] Iteration 59000, lr = 0.0815625, m = 0.9
I0814 22:24:19.331686 11101 solver.cpp:312] Iteration 59100 (6.80478 iter/s, 14.6956s/100 iter), loss = 2.50629
I0814 22:24:19.331866 11101 solver.cpp:334]     Train net output #0: loss = 1.87224 (* 1 = 1.87224 loss)
I0814 22:24:19.331953 11101 sgd_solver.cpp:136] Iteration 59100, lr = 0.0815312, m = 0.9
I0814 22:24:33.748873 11101 solver.cpp:312] Iteration 59200 (6.93636 iter/s, 14.4168s/100 iter), loss = 2.34512
I0814 22:24:33.748942 11101 solver.cpp:334]     Train net output #0: loss = 2.33723 (* 1 = 2.33723 loss)
I0814 22:24:33.748951 11101 sgd_solver.cpp:136] Iteration 59200, lr = 0.0815, m = 0.9
I0814 22:24:48.387411 11101 solver.cpp:312] Iteration 59300 (6.83148 iter/s, 14.6381s/100 iter), loss = 2.49203
I0814 22:24:48.387439 11101 solver.cpp:334]     Train net output #0: loss = 2.18321 (* 1 = 2.18321 loss)
I0814 22:24:48.387445 11101 sgd_solver.cpp:136] Iteration 59300, lr = 0.0814688, m = 0.9
I0814 22:25:03.086694 11101 solver.cpp:312] Iteration 59400 (6.80325 iter/s, 14.6989s/100 iter), loss = 2.76398
I0814 22:25:03.086721 11101 solver.cpp:334]     Train net output #0: loss = 2.85856 (* 1 = 2.85856 loss)
I0814 22:25:03.086727 11101 sgd_solver.cpp:136] Iteration 59400, lr = 0.0814375, m = 0.9
I0814 22:25:17.825407 11101 solver.cpp:312] Iteration 59500 (6.78505 iter/s, 14.7383s/100 iter), loss = 2.0689
I0814 22:25:17.825481 11101 solver.cpp:334]     Train net output #0: loss = 2.217 (* 1 = 2.217 loss)
I0814 22:25:17.825487 11101 sgd_solver.cpp:136] Iteration 59500, lr = 0.0814063, m = 0.9
I0814 22:25:30.429478 11065 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 22:25:32.372740 11101 solver.cpp:312] Iteration 59600 (6.87431 iter/s, 14.5469s/100 iter), loss = 2.29719
I0814 22:25:32.372805 11101 solver.cpp:334]     Train net output #0: loss = 2.30262 (* 1 = 2.30262 loss)
I0814 22:25:32.372824 11101 sgd_solver.cpp:136] Iteration 59600, lr = 0.081375, m = 0.9
I0814 22:25:47.126225 11101 solver.cpp:312] Iteration 59700 (6.77825 iter/s, 14.7531s/100 iter), loss = 2.46502
I0814 22:25:47.126253 11101 solver.cpp:334]     Train net output #0: loss = 2.64262 (* 1 = 2.64262 loss)
I0814 22:25:47.126260 11101 sgd_solver.cpp:136] Iteration 59700, lr = 0.0813438, m = 0.9
I0814 22:26:01.553298 11101 solver.cpp:312] Iteration 59800 (6.93161 iter/s, 14.4267s/100 iter), loss = 2.34386
I0814 22:26:01.553503 11101 solver.cpp:334]     Train net output #0: loss = 2.25359 (* 1 = 2.25359 loss)
I0814 22:26:01.553509 11101 sgd_solver.cpp:136] Iteration 59800, lr = 0.0813125, m = 0.9
I0814 22:26:16.028170 11101 solver.cpp:312] Iteration 59900 (6.90872 iter/s, 14.4745s/100 iter), loss = 2.16009
I0814 22:26:16.028224 11101 solver.cpp:334]     Train net output #0: loss = 2.22117 (* 1 = 2.22117 loss)
I0814 22:26:16.028239 11101 sgd_solver.cpp:136] Iteration 59900, lr = 0.0812813, m = 0.9
I0814 22:26:30.516151 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_60000.caffemodel
I0814 22:26:30.549134 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_60000.solverstate
I0814 22:26:30.555141 11101 solver.cpp:509] Iteration 60000, Testing net (#0)
I0814 22:26:51.127388 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.409412
I0814 22:26:51.127462 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.660822
I0814 22:26:51.127475 11101 solver.cpp:594]     Test net output #2: loss = 2.77899 (* 1 = 2.77899 loss)
I0814 22:26:51.127501 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.5718s
I0814 22:26:51.280338 11101 solver.cpp:312] Iteration 60000 (2.83678 iter/s, 35.2512s/100 iter), loss = 2.6691
I0814 22:26:51.280397 11101 solver.cpp:334]     Train net output #0: loss = 3.24755 (* 1 = 3.24755 loss)
I0814 22:26:51.280416 11101 sgd_solver.cpp:136] Iteration 60000, lr = 0.08125, m = 0.9
I0814 22:27:06.179608 11101 solver.cpp:312] Iteration 60100 (6.71193 iter/s, 14.8988s/100 iter), loss = 2.56258
I0814 22:27:06.179666 11101 solver.cpp:334]     Train net output #0: loss = 2.49999 (* 1 = 2.49999 loss)
I0814 22:27:06.179683 11101 sgd_solver.cpp:136] Iteration 60100, lr = 0.0812187, m = 0.9
I0814 22:27:20.717725 11101 solver.cpp:312] Iteration 60200 (6.87867 iter/s, 14.5377s/100 iter), loss = 2.31279
I0814 22:27:20.717948 11101 solver.cpp:334]     Train net output #0: loss = 1.97151 (* 1 = 1.97151 loss)
I0814 22:27:20.718055 11101 sgd_solver.cpp:136] Iteration 60200, lr = 0.0811875, m = 0.9
I0814 22:27:35.226938 11101 solver.cpp:312] Iteration 60300 (6.89237 iter/s, 14.5088s/100 iter), loss = 2.16114
I0814 22:27:35.227038 11101 solver.cpp:334]     Train net output #0: loss = 2.26933 (* 1 = 2.26933 loss)
I0814 22:27:35.227056 11101 sgd_solver.cpp:136] Iteration 60300, lr = 0.0811562, m = 0.9
I0814 22:27:49.920413 11101 solver.cpp:312] Iteration 60400 (6.80594 iter/s, 14.6931s/100 iter), loss = 2.18056
I0814 22:27:49.920441 11101 solver.cpp:334]     Train net output #0: loss = 2.05082 (* 1 = 2.05082 loss)
I0814 22:27:49.920447 11101 sgd_solver.cpp:136] Iteration 60400, lr = 0.081125, m = 0.9
I0814 22:28:04.692441 11101 solver.cpp:312] Iteration 60500 (6.76974 iter/s, 14.7716s/100 iter), loss = 1.97022
I0814 22:28:04.692464 11101 solver.cpp:334]     Train net output #0: loss = 1.8008 (* 1 = 1.8008 loss)
I0814 22:28:04.692468 11101 sgd_solver.cpp:136] Iteration 60500, lr = 0.0810938, m = 0.9
I0814 22:28:19.752825 11101 solver.cpp:312] Iteration 60600 (6.64012 iter/s, 15.06s/100 iter), loss = 2.25956
I0814 22:28:19.752900 11101 solver.cpp:334]     Train net output #0: loss = 2.44561 (* 1 = 2.44561 loss)
I0814 22:28:19.752912 11101 sgd_solver.cpp:136] Iteration 60600, lr = 0.0810625, m = 0.9
I0814 22:28:34.172507 11101 solver.cpp:312] Iteration 60700 (6.93516 iter/s, 14.4193s/100 iter), loss = 2.48961
I0814 22:28:34.172535 11101 solver.cpp:334]     Train net output #0: loss = 2.37467 (* 1 = 2.37467 loss)
I0814 22:28:34.172541 11101 sgd_solver.cpp:136] Iteration 60700, lr = 0.0810313, m = 0.9
I0814 22:28:48.664196 11101 solver.cpp:312] Iteration 60800 (6.9007 iter/s, 14.4913s/100 iter), loss = 2.09307
I0814 22:28:48.664221 11101 solver.cpp:334]     Train net output #0: loss = 2.45429 (* 1 = 2.45429 loss)
I0814 22:28:48.664224 11101 sgd_solver.cpp:136] Iteration 60800, lr = 0.081, m = 0.9
I0814 22:29:03.343384 11101 solver.cpp:312] Iteration 60900 (6.81256 iter/s, 14.6788s/100 iter), loss = 2.54488
I0814 22:29:03.343479 11101 solver.cpp:334]     Train net output #0: loss = 2.36887 (* 1 = 2.36887 loss)
I0814 22:29:03.343498 11101 sgd_solver.cpp:136] Iteration 60900, lr = 0.0809688, m = 0.9
I0814 22:29:17.895462 11101 solver.cpp:312] Iteration 61000 (6.87207 iter/s, 14.5517s/100 iter), loss = 2.24099
I0814 22:29:17.895488 11101 solver.cpp:334]     Train net output #0: loss = 2.2411 (* 1 = 2.2411 loss)
I0814 22:29:17.895493 11101 sgd_solver.cpp:136] Iteration 61000, lr = 0.0809375, m = 0.9
I0814 22:29:32.392721 11101 solver.cpp:312] Iteration 61100 (6.89805 iter/s, 14.4968s/100 iter), loss = 1.99525
I0814 22:29:32.392748 11101 solver.cpp:334]     Train net output #0: loss = 1.80205 (* 1 = 1.80205 loss)
I0814 22:29:32.392755 11101 sgd_solver.cpp:136] Iteration 61100, lr = 0.0809062, m = 0.9
I0814 22:29:46.833705 11101 solver.cpp:312] Iteration 61200 (6.92493 iter/s, 14.4406s/100 iter), loss = 2.2773
I0814 22:29:46.833806 11101 solver.cpp:334]     Train net output #0: loss = 2.76692 (* 1 = 2.76692 loss)
I0814 22:29:46.833823 11101 sgd_solver.cpp:136] Iteration 61200, lr = 0.080875, m = 0.9
I0814 22:30:01.383371 11101 solver.cpp:312] Iteration 61300 (6.87321 iter/s, 14.5493s/100 iter), loss = 2.56772
I0814 22:30:01.383400 11101 solver.cpp:334]     Train net output #0: loss = 2.43436 (* 1 = 2.43436 loss)
I0814 22:30:01.383406 11101 sgd_solver.cpp:136] Iteration 61300, lr = 0.0808437, m = 0.9
I0814 22:30:16.491571 11101 solver.cpp:312] Iteration 61400 (6.61911 iter/s, 15.1078s/100 iter), loss = 2.30828
I0814 22:30:16.491626 11101 solver.cpp:334]     Train net output #0: loss = 2.44688 (* 1 = 2.44688 loss)
I0814 22:30:16.491637 11101 sgd_solver.cpp:136] Iteration 61400, lr = 0.0808125, m = 0.9
I0814 22:30:31.268736 11101 solver.cpp:312] Iteration 61500 (6.76739 iter/s, 14.7767s/100 iter), loss = 2.0515
I0814 22:30:31.268831 11101 solver.cpp:334]     Train net output #0: loss = 2.06488 (* 1 = 2.06488 loss)
I0814 22:30:31.268848 11101 sgd_solver.cpp:136] Iteration 61500, lr = 0.0807813, m = 0.9
I0814 22:30:45.572402 11101 solver.cpp:312] Iteration 61600 (6.99141 iter/s, 14.3033s/100 iter), loss = 1.96922
I0814 22:30:45.572432 11101 solver.cpp:334]     Train net output #0: loss = 1.80564 (* 1 = 1.80564 loss)
I0814 22:30:45.572438 11101 sgd_solver.cpp:136] Iteration 61600, lr = 0.08075, m = 0.9
I0814 22:31:00.222527 11101 solver.cpp:312] Iteration 61700 (6.82608 iter/s, 14.6497s/100 iter), loss = 2.38587
I0814 22:31:00.222579 11101 solver.cpp:334]     Train net output #0: loss = 2.45194 (* 1 = 2.45194 loss)
I0814 22:31:00.222596 11101 sgd_solver.cpp:136] Iteration 61700, lr = 0.0807187, m = 0.9
I0814 22:31:14.858721 11101 solver.cpp:312] Iteration 61800 (6.83257 iter/s, 14.6358s/100 iter), loss = 2.16776
I0814 22:31:14.858808 11101 solver.cpp:334]     Train net output #0: loss = 2.27534 (* 1 = 2.27534 loss)
I0814 22:31:14.858815 11101 sgd_solver.cpp:136] Iteration 61800, lr = 0.0806875, m = 0.9
I0814 22:31:29.329044 11101 solver.cpp:312] Iteration 61900 (6.91089 iter/s, 14.4699s/100 iter), loss = 2.43539
I0814 22:31:29.329119 11101 solver.cpp:334]     Train net output #0: loss = 2.2185 (* 1 = 2.2185 loss)
I0814 22:31:29.329141 11101 sgd_solver.cpp:136] Iteration 61900, lr = 0.0806563, m = 0.9
I0814 22:31:43.699990 11101 solver.cpp:509] Iteration 62000, Testing net (#0)
I0814 22:32:04.501114 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.33094
I0814 22:32:04.501176 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.595
I0814 22:32:04.501183 11101 solver.cpp:594]     Test net output #2: loss = 3.22426 (* 1 = 3.22426 loss)
I0814 22:32:04.501199 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8006s
I0814 22:32:04.661026 11101 solver.cpp:312] Iteration 62000 (2.83038 iter/s, 35.331s/100 iter), loss = 2.23432
I0814 22:32:04.661051 11101 solver.cpp:334]     Train net output #0: loss = 1.99757 (* 1 = 1.99757 loss)
I0814 22:32:04.661057 11101 sgd_solver.cpp:136] Iteration 62000, lr = 0.080625, m = 0.9
I0814 22:32:19.427167 11101 solver.cpp:312] Iteration 62100 (6.77244 iter/s, 14.7657s/100 iter), loss = 2.16452
I0814 22:32:19.427193 11101 solver.cpp:334]     Train net output #0: loss = 2.25766 (* 1 = 2.25766 loss)
I0814 22:32:19.427199 11101 sgd_solver.cpp:136] Iteration 62100, lr = 0.0805938, m = 0.9
I0814 22:32:33.876353 11101 solver.cpp:312] Iteration 62200 (6.921 iter/s, 14.4488s/100 iter), loss = 2.04565
I0814 22:32:33.876425 11101 solver.cpp:334]     Train net output #0: loss = 2.10738 (* 1 = 2.10738 loss)
I0814 22:32:33.876442 11101 sgd_solver.cpp:136] Iteration 62200, lr = 0.0805625, m = 0.9
I0814 22:32:48.262434 11101 solver.cpp:312] Iteration 62300 (6.95136 iter/s, 14.3857s/100 iter), loss = 2.19075
I0814 22:32:48.262495 11101 solver.cpp:334]     Train net output #0: loss = 2.71221 (* 1 = 2.71221 loss)
I0814 22:32:48.262501 11101 sgd_solver.cpp:136] Iteration 62300, lr = 0.0805313, m = 0.9
I0814 22:33:02.769152 11101 solver.cpp:312] Iteration 62400 (6.89355 iter/s, 14.5063s/100 iter), loss = 2.35225
I0814 22:33:02.769215 11101 solver.cpp:334]     Train net output #0: loss = 2.23042 (* 1 = 2.23042 loss)
I0814 22:33:02.769234 11101 sgd_solver.cpp:136] Iteration 62400, lr = 0.0805, m = 0.9
I0814 22:33:17.551566 11101 solver.cpp:312] Iteration 62500 (6.76499 iter/s, 14.782s/100 iter), loss = 2.27624
I0814 22:33:17.551595 11101 solver.cpp:334]     Train net output #0: loss = 2.28416 (* 1 = 2.28416 loss)
I0814 22:33:17.551601 11101 sgd_solver.cpp:136] Iteration 62500, lr = 0.0804688, m = 0.9
I0814 22:33:32.723996 11101 solver.cpp:312] Iteration 62600 (6.59109 iter/s, 15.172s/100 iter), loss = 2.01574
I0814 22:33:32.724057 11101 solver.cpp:334]     Train net output #0: loss = 1.91012 (* 1 = 1.91012 loss)
I0814 22:33:32.724066 11101 sgd_solver.cpp:136] Iteration 62600, lr = 0.0804375, m = 0.9
I0814 22:33:47.746788 11101 solver.cpp:312] Iteration 62700 (6.65674 iter/s, 15.0224s/100 iter), loss = 1.89501
I0814 22:33:47.746839 11101 solver.cpp:334]     Train net output #0: loss = 2.16669 (* 1 = 2.16669 loss)
I0814 22:33:47.746851 11101 sgd_solver.cpp:136] Iteration 62700, lr = 0.0804062, m = 0.9
I0814 22:34:02.624384 11101 solver.cpp:312] Iteration 62800 (6.72171 iter/s, 14.8772s/100 iter), loss = 2.34286
I0814 22:34:02.624415 11101 solver.cpp:334]     Train net output #0: loss = 2.29422 (* 1 = 2.29422 loss)
I0814 22:34:02.624420 11101 sgd_solver.cpp:136] Iteration 62800, lr = 0.080375, m = 0.9
I0814 22:34:17.106652 11101 solver.cpp:312] Iteration 62900 (6.90519 iter/s, 14.4819s/100 iter), loss = 2.26435
I0814 22:34:17.106741 11101 solver.cpp:334]     Train net output #0: loss = 2.00871 (* 1 = 2.00871 loss)
I0814 22:34:17.106755 11101 sgd_solver.cpp:136] Iteration 62900, lr = 0.0803437, m = 0.9
I0814 22:34:31.550745 11101 solver.cpp:312] Iteration 63000 (6.92344 iter/s, 14.4437s/100 iter), loss = 2.27031
I0814 22:34:31.550773 11101 solver.cpp:334]     Train net output #0: loss = 1.85109 (* 1 = 1.85109 loss)
I0814 22:34:31.550781 11101 sgd_solver.cpp:136] Iteration 63000, lr = 0.0803125, m = 0.9
I0814 22:34:46.175458 11101 solver.cpp:312] Iteration 63100 (6.83793 iter/s, 14.6243s/100 iter), loss = 2.3406
I0814 22:34:46.175482 11101 solver.cpp:334]     Train net output #0: loss = 2.08145 (* 1 = 2.08145 loss)
I0814 22:34:46.175487 11101 sgd_solver.cpp:136] Iteration 63100, lr = 0.0802813, m = 0.9
I0814 22:35:00.695626 11101 solver.cpp:312] Iteration 63200 (6.88717 iter/s, 14.5198s/100 iter), loss = 2.22847
I0814 22:35:00.695698 11101 solver.cpp:334]     Train net output #0: loss = 2.26773 (* 1 = 2.26773 loss)
I0814 22:35:00.695705 11101 sgd_solver.cpp:136] Iteration 63200, lr = 0.08025, m = 0.9
I0814 22:35:15.629999 11101 solver.cpp:312] Iteration 63300 (6.69615 iter/s, 14.934s/100 iter), loss = 2.24218
I0814 22:35:15.630026 11101 solver.cpp:334]     Train net output #0: loss = 2.10223 (* 1 = 2.10223 loss)
I0814 22:35:15.630031 11101 sgd_solver.cpp:136] Iteration 63300, lr = 0.0802188, m = 0.9
I0814 22:35:30.516993 11101 solver.cpp:312] Iteration 63400 (6.71746 iter/s, 14.8866s/100 iter), loss = 2.13102
I0814 22:35:30.517020 11101 solver.cpp:334]     Train net output #0: loss = 2.40586 (* 1 = 2.40586 loss)
I0814 22:35:30.517065 11101 sgd_solver.cpp:136] Iteration 63400, lr = 0.0801875, m = 0.9
I0814 22:35:45.328346 11101 solver.cpp:312] Iteration 63500 (6.75177 iter/s, 14.8109s/100 iter), loss = 2.03852
I0814 22:35:45.328562 11101 solver.cpp:334]     Train net output #0: loss = 1.87997 (* 1 = 1.87997 loss)
I0814 22:35:45.328570 11101 sgd_solver.cpp:136] Iteration 63500, lr = 0.0801563, m = 0.9
I0814 22:35:59.951508 11101 solver.cpp:312] Iteration 63600 (6.83866 iter/s, 14.6227s/100 iter), loss = 2.60025
I0814 22:35:59.951560 11101 solver.cpp:334]     Train net output #0: loss = 2.533 (* 1 = 2.533 loss)
I0814 22:35:59.951572 11101 sgd_solver.cpp:136] Iteration 63600, lr = 0.080125, m = 0.9
I0814 22:36:14.657371 11101 solver.cpp:312] Iteration 63700 (6.8002 iter/s, 14.7054s/100 iter), loss = 2.32435
I0814 22:36:14.657395 11101 solver.cpp:334]     Train net output #0: loss = 2.3641 (* 1 = 2.3641 loss)
I0814 22:36:14.657400 11101 sgd_solver.cpp:136] Iteration 63700, lr = 0.0800938, m = 0.9
I0814 22:36:29.236418 11101 solver.cpp:312] Iteration 63800 (6.85935 iter/s, 14.5786s/100 iter), loss = 2.21208
I0814 22:36:29.236474 11101 solver.cpp:334]     Train net output #0: loss = 2.24835 (* 1 = 2.24835 loss)
I0814 22:36:29.236481 11101 sgd_solver.cpp:136] Iteration 63800, lr = 0.0800625, m = 0.9
I0814 22:36:43.762789 11101 solver.cpp:312] Iteration 63900 (6.88423 iter/s, 14.526s/100 iter), loss = 2.46017
I0814 22:36:43.762858 11101 solver.cpp:334]     Train net output #0: loss = 2.54452 (* 1 = 2.54452 loss)
I0814 22:36:43.762876 11101 sgd_solver.cpp:136] Iteration 63900, lr = 0.0800313, m = 0.9
I0814 22:36:58.169872 11101 solver.cpp:509] Iteration 64000, Testing net (#0)
I0814 22:37:16.629640 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 22:37:18.942544 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.387647
I0814 22:37:18.942572 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.642705
I0814 22:37:18.942579 11101 solver.cpp:594]     Test net output #2: loss = 2.9118 (* 1 = 2.9118 loss)
I0814 22:37:18.942682 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7722s
I0814 22:37:19.103603 11101 solver.cpp:312] Iteration 64000 (2.82967 iter/s, 35.3398s/100 iter), loss = 2.32064
I0814 22:37:19.103626 11101 solver.cpp:334]     Train net output #0: loss = 2.73689 (* 1 = 2.73689 loss)
I0814 22:37:19.103631 11101 sgd_solver.cpp:136] Iteration 64000, lr = 0.08, m = 0.9
I0814 22:37:33.571370 11101 solver.cpp:312] Iteration 64100 (6.91211 iter/s, 14.4674s/100 iter), loss = 2.29359
I0814 22:37:33.571398 11101 solver.cpp:334]     Train net output #0: loss = 1.89824 (* 1 = 1.89824 loss)
I0814 22:37:33.571403 11101 sgd_solver.cpp:136] Iteration 64100, lr = 0.0799688, m = 0.9
I0814 22:37:47.923183 11101 solver.cpp:312] Iteration 64200 (6.96796 iter/s, 14.3514s/100 iter), loss = 2.02235
I0814 22:37:47.923256 11101 solver.cpp:334]     Train net output #0: loss = 1.84999 (* 1 = 1.84999 loss)
I0814 22:37:47.923264 11101 sgd_solver.cpp:136] Iteration 64200, lr = 0.0799375, m = 0.9
I0814 22:38:02.796895 11101 solver.cpp:312] Iteration 64300 (6.72346 iter/s, 14.8733s/100 iter), loss = 2.35941
I0814 22:38:02.796921 11101 solver.cpp:334]     Train net output #0: loss = 2.62744 (* 1 = 2.62744 loss)
I0814 22:38:02.796948 11101 sgd_solver.cpp:136] Iteration 64300, lr = 0.0799062, m = 0.9
I0814 22:38:18.037086 11101 solver.cpp:312] Iteration 64400 (6.56179 iter/s, 15.2398s/100 iter), loss = 2.39853
I0814 22:38:18.040315 11101 solver.cpp:334]     Train net output #0: loss = 2.34908 (* 1 = 2.34908 loss)
I0814 22:38:18.040340 11101 sgd_solver.cpp:136] Iteration 64400, lr = 0.079875, m = 0.9
I0814 22:38:32.971820 11101 solver.cpp:312] Iteration 64500 (6.69599 iter/s, 14.9343s/100 iter), loss = 2.01396
I0814 22:38:32.971894 11101 solver.cpp:334]     Train net output #0: loss = 2.32989 (* 1 = 2.32989 loss)
I0814 22:38:32.971913 11101 sgd_solver.cpp:136] Iteration 64500, lr = 0.0798438, m = 0.9
I0814 22:38:47.593000 11101 solver.cpp:312] Iteration 64600 (6.83959 iter/s, 14.6208s/100 iter), loss = 2.51628
I0814 22:38:47.593030 11101 solver.cpp:334]     Train net output #0: loss = 2.5475 (* 1 = 2.5475 loss)
I0814 22:38:47.593036 11101 sgd_solver.cpp:136] Iteration 64600, lr = 0.0798125, m = 0.9
I0814 22:39:02.363602 11101 solver.cpp:312] Iteration 64700 (6.7704 iter/s, 14.7702s/100 iter), loss = 2.73768
I0814 22:39:02.363683 11101 solver.cpp:334]     Train net output #0: loss = 2.70955 (* 1 = 2.70955 loss)
I0814 22:39:02.363690 11101 sgd_solver.cpp:136] Iteration 64700, lr = 0.0797813, m = 0.9
I0814 22:39:16.994536 11101 solver.cpp:312] Iteration 64800 (6.83503 iter/s, 14.6305s/100 iter), loss = 2.67342
I0814 22:39:16.994606 11101 solver.cpp:334]     Train net output #0: loss = 2.82443 (* 1 = 2.82443 loss)
I0814 22:39:16.994623 11101 sgd_solver.cpp:136] Iteration 64800, lr = 0.07975, m = 0.9
I0814 22:39:32.091661 11101 solver.cpp:312] Iteration 64900 (6.62397 iter/s, 15.0967s/100 iter), loss = 2.19545
I0814 22:39:32.091687 11101 solver.cpp:334]     Train net output #0: loss = 2.14219 (* 1 = 2.14219 loss)
I0814 22:39:32.091696 11101 sgd_solver.cpp:136] Iteration 64900, lr = 0.0797188, m = 0.9
I0814 22:39:46.962888 11101 solver.cpp:312] Iteration 65000 (6.72458 iter/s, 14.8708s/100 iter), loss = 1.75909
I0814 22:39:46.962952 11101 solver.cpp:334]     Train net output #0: loss = 2.01175 (* 1 = 2.01175 loss)
I0814 22:39:46.962960 11101 sgd_solver.cpp:136] Iteration 65000, lr = 0.0796875, m = 0.9
I0814 22:40:01.901087 11101 solver.cpp:312] Iteration 65100 (6.69444 iter/s, 14.9378s/100 iter), loss = 1.99776
I0814 22:40:01.901157 11101 solver.cpp:334]     Train net output #0: loss = 2.04179 (* 1 = 2.04179 loss)
I0814 22:40:01.901176 11101 sgd_solver.cpp:136] Iteration 65100, lr = 0.0796563, m = 0.9
I0814 22:40:16.576591 11101 solver.cpp:312] Iteration 65200 (6.81427 iter/s, 14.6751s/100 iter), loss = 2.38551
I0814 22:40:16.576655 11101 solver.cpp:334]     Train net output #0: loss = 2.28886 (* 1 = 2.28886 loss)
I0814 22:40:16.576674 11101 sgd_solver.cpp:136] Iteration 65200, lr = 0.079625, m = 0.9
I0814 22:40:31.237133 11101 solver.cpp:312] Iteration 65300 (6.82122 iter/s, 14.6601s/100 iter), loss = 2.36621
I0814 22:40:31.237241 11101 solver.cpp:334]     Train net output #0: loss = 2.29077 (* 1 = 2.29077 loss)
I0814 22:40:31.237262 11101 sgd_solver.cpp:136] Iteration 65300, lr = 0.0795937, m = 0.9
I0814 22:40:45.757885 11101 solver.cpp:312] Iteration 65400 (6.88689 iter/s, 14.5203s/100 iter), loss = 2.0188
I0814 22:40:45.757913 11101 solver.cpp:334]     Train net output #0: loss = 2.23278 (* 1 = 2.23278 loss)
I0814 22:40:45.757920 11101 sgd_solver.cpp:136] Iteration 65400, lr = 0.0795625, m = 0.9
I0814 22:41:00.233120 11101 solver.cpp:312] Iteration 65500 (6.90855 iter/s, 14.4748s/100 iter), loss = 1.82184
I0814 22:41:00.233150 11101 solver.cpp:334]     Train net output #0: loss = 1.44461 (* 1 = 1.44461 loss)
I0814 22:41:00.233155 11101 sgd_solver.cpp:136] Iteration 65500, lr = 0.0795313, m = 0.9
I0814 22:41:14.808001 11101 solver.cpp:312] Iteration 65600 (6.86131 iter/s, 14.5745s/100 iter), loss = 2.20315
I0814 22:41:14.808094 11101 solver.cpp:334]     Train net output #0: loss = 1.98215 (* 1 = 1.98215 loss)
I0814 22:41:14.808111 11101 sgd_solver.cpp:136] Iteration 65600, lr = 0.0795, m = 0.9
I0814 22:41:29.566908 11101 solver.cpp:312] Iteration 65700 (6.77576 iter/s, 14.7585s/100 iter), loss = 2.18765
I0814 22:41:29.566931 11101 solver.cpp:334]     Train net output #0: loss = 2.3038 (* 1 = 2.3038 loss)
I0814 22:41:29.566934 11101 sgd_solver.cpp:136] Iteration 65700, lr = 0.0794687, m = 0.9
I0814 22:41:44.168148 11101 solver.cpp:312] Iteration 65800 (6.84893 iter/s, 14.6008s/100 iter), loss = 2.31465
I0814 22:41:44.168215 11101 solver.cpp:334]     Train net output #0: loss = 2.00047 (* 1 = 2.00047 loss)
I0814 22:41:44.168231 11101 sgd_solver.cpp:136] Iteration 65800, lr = 0.0794375, m = 0.9
I0814 22:41:58.948153 11101 solver.cpp:312] Iteration 65900 (6.76609 iter/s, 14.7796s/100 iter), loss = 2.67034
I0814 22:41:58.948213 11101 solver.cpp:334]     Train net output #0: loss = 3.22619 (* 1 = 3.22619 loss)
I0814 22:41:58.948220 11101 sgd_solver.cpp:136] Iteration 65900, lr = 0.0794063, m = 0.9
I0814 22:42:13.488019 11101 solver.cpp:509] Iteration 66000, Testing net (#0)
I0814 22:42:34.133839 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.37747
I0814 22:42:34.133909 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.638529
I0814 22:42:34.133916 11101 solver.cpp:594]     Test net output #2: loss = 2.91978 (* 1 = 2.91978 loss)
I0814 22:42:34.133937 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6453s
I0814 22:42:34.277932 11101 solver.cpp:312] Iteration 66000 (2.83055 iter/s, 35.3288s/100 iter), loss = 2.3958
I0814 22:42:34.278107 11101 solver.cpp:334]     Train net output #0: loss = 2.50955 (* 1 = 2.50955 loss)
I0814 22:42:34.278197 11101 sgd_solver.cpp:136] Iteration 66000, lr = 0.079375, m = 0.9
I0814 22:42:48.718030 11101 solver.cpp:312] Iteration 66100 (6.92536 iter/s, 14.4397s/100 iter), loss = 2.10824
I0814 22:42:48.718052 11101 solver.cpp:334]     Train net output #0: loss = 1.92037 (* 1 = 1.92037 loss)
I0814 22:42:48.718058 11101 sgd_solver.cpp:136] Iteration 66100, lr = 0.0793438, m = 0.9
I0814 22:43:03.448617 11101 solver.cpp:312] Iteration 66200 (6.78879 iter/s, 14.7302s/100 iter), loss = 2.06409
I0814 22:43:03.448644 11101 solver.cpp:334]     Train net output #0: loss = 2.45249 (* 1 = 2.45249 loss)
I0814 22:43:03.448652 11101 sgd_solver.cpp:136] Iteration 66200, lr = 0.0793125, m = 0.9
I0814 22:43:17.781374 11101 solver.cpp:312] Iteration 66300 (6.97722 iter/s, 14.3324s/100 iter), loss = 2.59676
I0814 22:43:17.781457 11101 solver.cpp:334]     Train net output #0: loss = 2.63745 (* 1 = 2.63745 loss)
I0814 22:43:17.781466 11101 sgd_solver.cpp:136] Iteration 66300, lr = 0.0792812, m = 0.9
I0814 22:43:32.343660 11101 solver.cpp:312] Iteration 66400 (6.86725 iter/s, 14.5619s/100 iter), loss = 1.96461
I0814 22:43:32.343727 11101 solver.cpp:334]     Train net output #0: loss = 2.19003 (* 1 = 2.19003 loss)
I0814 22:43:32.343745 11101 sgd_solver.cpp:136] Iteration 66400, lr = 0.07925, m = 0.9
I0814 22:43:46.985602 11101 solver.cpp:312] Iteration 66500 (6.82989 iter/s, 14.6415s/100 iter), loss = 2.35421
I0814 22:43:46.985685 11101 solver.cpp:334]     Train net output #0: loss = 2.38074 (* 1 = 2.38074 loss)
I0814 22:43:46.985704 11101 sgd_solver.cpp:136] Iteration 66500, lr = 0.0792188, m = 0.9
I0814 22:44:01.595371 11101 solver.cpp:312] Iteration 66600 (6.84493 iter/s, 14.6094s/100 iter), loss = 1.98888
I0814 22:44:01.595589 11101 solver.cpp:334]     Train net output #0: loss = 2.02887 (* 1 = 2.02887 loss)
I0814 22:44:01.595597 11101 sgd_solver.cpp:136] Iteration 66600, lr = 0.0791875, m = 0.9
I0814 22:44:16.454861 11101 solver.cpp:312] Iteration 66700 (6.7299 iter/s, 14.8591s/100 iter), loss = 2.36448
I0814 22:44:16.454890 11101 solver.cpp:334]     Train net output #0: loss = 2.59644 (* 1 = 2.59644 loss)
I0814 22:44:16.454897 11101 sgd_solver.cpp:136] Iteration 66700, lr = 0.0791562, m = 0.9
I0814 22:44:30.908720 11101 solver.cpp:312] Iteration 66800 (6.91876 iter/s, 14.4534s/100 iter), loss = 2.01182
I0814 22:44:30.908794 11101 solver.cpp:334]     Train net output #0: loss = 1.99753 (* 1 = 1.99753 loss)
I0814 22:44:30.908815 11101 sgd_solver.cpp:136] Iteration 66800, lr = 0.079125, m = 0.9
I0814 22:44:45.472329 11101 solver.cpp:312] Iteration 66900 (6.86662 iter/s, 14.5632s/100 iter), loss = 2.20914
I0814 22:44:45.472527 11101 solver.cpp:334]     Train net output #0: loss = 2.14798 (* 1 = 2.14798 loss)
I0814 22:44:45.472535 11101 sgd_solver.cpp:136] Iteration 66900, lr = 0.0790937, m = 0.9
I0814 22:45:00.057762 11101 solver.cpp:312] Iteration 67000 (6.85635 iter/s, 14.585s/100 iter), loss = 2.31471
I0814 22:45:00.057785 11101 solver.cpp:334]     Train net output #0: loss = 2.51795 (* 1 = 2.51795 loss)
I0814 22:45:00.057790 11101 sgd_solver.cpp:136] Iteration 67000, lr = 0.0790625, m = 0.9
I0814 22:45:15.108873 11101 solver.cpp:312] Iteration 67100 (6.64421 iter/s, 15.0507s/100 iter), loss = 2.14331
I0814 22:45:15.108901 11101 solver.cpp:334]     Train net output #0: loss = 2.22799 (* 1 = 2.22799 loss)
I0814 22:45:15.108906 11101 sgd_solver.cpp:136] Iteration 67100, lr = 0.0790313, m = 0.9
I0814 22:45:30.276445 11101 solver.cpp:312] Iteration 67200 (6.5932 iter/s, 15.1671s/100 iter), loss = 2.14464
I0814 22:45:30.276515 11101 solver.cpp:334]     Train net output #0: loss = 2.47862 (* 1 = 2.47862 loss)
I0814 22:45:30.276521 11101 sgd_solver.cpp:136] Iteration 67200, lr = 0.079, m = 0.9
I0814 22:45:45.517453 11101 solver.cpp:312] Iteration 67300 (6.56143 iter/s, 15.2406s/100 iter), loss = 2.04749
I0814 22:45:45.517480 11101 solver.cpp:334]     Train net output #0: loss = 2.0222 (* 1 = 2.0222 loss)
I0814 22:45:45.517487 11101 sgd_solver.cpp:136] Iteration 67300, lr = 0.0789688, m = 0.9
I0814 22:46:00.372761 11101 solver.cpp:312] Iteration 67400 (6.73179 iter/s, 14.8549s/100 iter), loss = 2.16877
I0814 22:46:00.372834 11101 solver.cpp:334]     Train net output #0: loss = 2.13797 (* 1 = 2.13797 loss)
I0814 22:46:00.372841 11101 sgd_solver.cpp:136] Iteration 67400, lr = 0.0789375, m = 0.9
I0814 22:46:15.032138 11101 solver.cpp:312] Iteration 67500 (6.82177 iter/s, 14.659s/100 iter), loss = 2.12428
I0814 22:46:15.032163 11101 solver.cpp:334]     Train net output #0: loss = 1.98165 (* 1 = 1.98165 loss)
I0814 22:46:15.032169 11101 sgd_solver.cpp:136] Iteration 67500, lr = 0.0789063, m = 0.9
I0814 22:46:29.301952 11101 solver.cpp:312] Iteration 67600 (7.00799 iter/s, 14.2694s/100 iter), loss = 2.03162
I0814 22:46:29.302003 11101 solver.cpp:334]     Train net output #0: loss = 1.99461 (* 1 = 1.99461 loss)
I0814 22:46:29.302110 11101 sgd_solver.cpp:136] Iteration 67600, lr = 0.078875, m = 0.9
I0814 22:46:43.649729 11101 solver.cpp:312] Iteration 67700 (6.96992 iter/s, 14.3474s/100 iter), loss = 1.91379
I0814 22:46:43.652174 11101 solver.cpp:334]     Train net output #0: loss = 1.95188 (* 1 = 1.95188 loss)
I0814 22:46:43.652190 11101 sgd_solver.cpp:136] Iteration 67700, lr = 0.0788438, m = 0.9
I0814 22:46:58.203235 11101 solver.cpp:312] Iteration 67800 (6.87139 iter/s, 14.5531s/100 iter), loss = 2.04593
I0814 22:46:58.203301 11101 solver.cpp:334]     Train net output #0: loss = 2.29967 (* 1 = 2.29967 loss)
I0814 22:46:58.203320 11101 sgd_solver.cpp:136] Iteration 67800, lr = 0.0788125, m = 0.9
I0814 22:47:12.922991 11101 solver.cpp:312] Iteration 67900 (6.79378 iter/s, 14.7193s/100 iter), loss = 2.24892
I0814 22:47:12.923053 11101 solver.cpp:334]     Train net output #0: loss = 2.35613 (* 1 = 2.35613 loss)
I0814 22:47:12.923070 11101 sgd_solver.cpp:136] Iteration 67900, lr = 0.0787812, m = 0.9
I0814 22:47:27.351963 11101 solver.cpp:509] Iteration 68000, Testing net (#0)
I0814 22:47:48.028672 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.398411
I0814 22:47:48.028693 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.655058
I0814 22:47:48.028698 11101 solver.cpp:594]     Test net output #2: loss = 2.82699 (* 1 = 2.82699 loss)
I0814 22:47:48.028717 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6762s
I0814 22:47:48.204140 11101 solver.cpp:312] Iteration 68000 (2.83445 iter/s, 35.2802s/100 iter), loss = 2.40028
I0814 22:47:48.204169 11101 solver.cpp:334]     Train net output #0: loss = 2.15161 (* 1 = 2.15161 loss)
I0814 22:47:48.204182 11101 sgd_solver.cpp:136] Iteration 68000, lr = 0.07875, m = 0.9
I0814 22:48:02.476585 11101 solver.cpp:312] Iteration 68100 (7.0067 iter/s, 14.272s/100 iter), loss = 2.09627
I0814 22:48:02.476665 11101 solver.cpp:334]     Train net output #0: loss = 2.05599 (* 1 = 2.05599 loss)
I0814 22:48:02.476676 11101 sgd_solver.cpp:136] Iteration 68100, lr = 0.0787188, m = 0.9
I0814 22:48:16.781435 11101 solver.cpp:312] Iteration 68200 (6.99084 iter/s, 14.3044s/100 iter), loss = 2.19467
I0814 22:48:16.781464 11101 solver.cpp:334]     Train net output #0: loss = 2.27656 (* 1 = 2.27656 loss)
I0814 22:48:16.781468 11101 sgd_solver.cpp:136] Iteration 68200, lr = 0.0786875, m = 0.9
I0814 22:48:31.315587 11101 solver.cpp:312] Iteration 68300 (6.88054 iter/s, 14.5337s/100 iter), loss = 2.61512
I0814 22:48:31.315611 11101 solver.cpp:334]     Train net output #0: loss = 2.62423 (* 1 = 2.62423 loss)
I0814 22:48:31.315618 11101 sgd_solver.cpp:136] Iteration 68300, lr = 0.0786562, m = 0.9
I0814 22:48:45.974520 11101 solver.cpp:312] Iteration 68400 (6.82197 iter/s, 14.6585s/100 iter), loss = 2.24988
I0814 22:48:45.974603 11101 solver.cpp:334]     Train net output #0: loss = 2.29102 (* 1 = 2.29102 loss)
I0814 22:48:45.974611 11101 sgd_solver.cpp:136] Iteration 68400, lr = 0.078625, m = 0.9
I0814 22:49:00.597204 11101 solver.cpp:312] Iteration 68500 (6.83888 iter/s, 14.6223s/100 iter), loss = 2.13677
I0814 22:49:00.597232 11101 solver.cpp:334]     Train net output #0: loss = 2.40645 (* 1 = 2.40645 loss)
I0814 22:49:00.597239 11101 sgd_solver.cpp:136] Iteration 68500, lr = 0.0785938, m = 0.9
I0814 22:49:15.353793 11101 solver.cpp:312] Iteration 68600 (6.77683 iter/s, 14.7562s/100 iter), loss = 1.90121
I0814 22:49:15.354014 11101 solver.cpp:334]     Train net output #0: loss = 2.20042 (* 1 = 2.20042 loss)
I0814 22:49:15.354125 11101 sgd_solver.cpp:136] Iteration 68600, lr = 0.0785625, m = 0.9
I0814 22:49:30.004067 11101 solver.cpp:312] Iteration 68700 (6.826 iter/s, 14.6499s/100 iter), loss = 2.07908
I0814 22:49:30.004178 11101 solver.cpp:334]     Train net output #0: loss = 2.1072 (* 1 = 2.1072 loss)
I0814 22:49:30.004195 11101 sgd_solver.cpp:136] Iteration 68700, lr = 0.0785313, m = 0.9
I0814 22:49:44.670372 11101 solver.cpp:312] Iteration 68800 (6.81854 iter/s, 14.6659s/100 iter), loss = 2.01813
I0814 22:49:44.670399 11101 solver.cpp:334]     Train net output #0: loss = 2.3148 (* 1 = 2.3148 loss)
I0814 22:49:44.670403 11101 sgd_solver.cpp:136] Iteration 68800, lr = 0.0785, m = 0.9
I0814 22:49:59.398826 11101 solver.cpp:312] Iteration 68900 (6.78977 iter/s, 14.728s/100 iter), loss = 1.6947
I0814 22:49:59.398878 11101 solver.cpp:334]     Train net output #0: loss = 2.02139 (* 1 = 2.02139 loss)
I0814 22:49:59.398891 11101 sgd_solver.cpp:136] Iteration 68900, lr = 0.0784688, m = 0.9
I0814 22:50:13.966168 11101 solver.cpp:312] Iteration 69000 (6.86486 iter/s, 14.5669s/100 iter), loss = 2.45691
I0814 22:50:13.966226 11101 solver.cpp:334]     Train net output #0: loss = 2.33079 (* 1 = 2.33079 loss)
I0814 22:50:13.966234 11101 sgd_solver.cpp:136] Iteration 69000, lr = 0.0784375, m = 0.9
I0814 22:50:28.405313 11101 solver.cpp:312] Iteration 69100 (6.92581 iter/s, 14.4387s/100 iter), loss = 2.1428
I0814 22:50:28.405344 11101 solver.cpp:334]     Train net output #0: loss = 2.48665 (* 1 = 2.48665 loss)
I0814 22:50:28.405351 11101 sgd_solver.cpp:136] Iteration 69100, lr = 0.0784063, m = 0.9
I0814 22:50:42.832182 11101 solver.cpp:312] Iteration 69200 (6.93171 iter/s, 14.4265s/100 iter), loss = 2.34769
I0814 22:50:42.832211 11101 solver.cpp:334]     Train net output #0: loss = 1.95828 (* 1 = 1.95828 loss)
I0814 22:50:42.832218 11101 sgd_solver.cpp:136] Iteration 69200, lr = 0.078375, m = 0.9
I0814 22:50:57.640681 11101 solver.cpp:312] Iteration 69300 (6.75307 iter/s, 14.8081s/100 iter), loss = 2.25228
I0814 22:50:57.652212 11101 solver.cpp:334]     Train net output #0: loss = 1.81495 (* 1 = 1.81495 loss)
I0814 22:50:57.652248 11101 sgd_solver.cpp:136] Iteration 69300, lr = 0.0783437, m = 0.9
I0814 22:51:12.033192 11101 solver.cpp:312] Iteration 69400 (6.94825 iter/s, 14.3921s/100 iter), loss = 2.30159
I0814 22:51:12.033254 11101 solver.cpp:334]     Train net output #0: loss = 1.74778 (* 1 = 1.74778 loss)
I0814 22:51:12.033272 11101 sgd_solver.cpp:136] Iteration 69400, lr = 0.0783125, m = 0.9
I0814 22:51:26.697823 11101 solver.cpp:312] Iteration 69500 (6.81932 iter/s, 14.6642s/100 iter), loss = 2.16667
I0814 22:51:26.697849 11101 solver.cpp:334]     Train net output #0: loss = 2.12886 (* 1 = 2.12886 loss)
I0814 22:51:26.697852 11101 sgd_solver.cpp:136] Iteration 69500, lr = 0.0782812, m = 0.9
I0814 22:51:41.498159 11101 solver.cpp:312] Iteration 69600 (6.75679 iter/s, 14.7999s/100 iter), loss = 1.8364
I0814 22:51:41.498237 11101 solver.cpp:334]     Train net output #0: loss = 1.8783 (* 1 = 1.8783 loss)
I0814 22:51:41.498250 11101 sgd_solver.cpp:136] Iteration 69600, lr = 0.07825, m = 0.9
I0814 22:51:56.230800 11101 solver.cpp:312] Iteration 69700 (6.78784 iter/s, 14.7322s/100 iter), loss = 2.25899
I0814 22:51:56.230852 11101 solver.cpp:334]     Train net output #0: loss = 2.04552 (* 1 = 2.04552 loss)
I0814 22:51:56.230865 11101 sgd_solver.cpp:136] Iteration 69700, lr = 0.0782188, m = 0.9
I0814 22:52:10.750099 11101 solver.cpp:312] Iteration 69800 (6.88758 iter/s, 14.5189s/100 iter), loss = 2.5057
I0814 22:52:10.750150 11101 solver.cpp:334]     Train net output #0: loss = 2.49168 (* 1 = 2.49168 loss)
I0814 22:52:10.750171 11101 sgd_solver.cpp:136] Iteration 69800, lr = 0.0781875, m = 0.9
I0814 22:52:25.203341 11101 solver.cpp:312] Iteration 69900 (6.91906 iter/s, 14.4528s/100 iter), loss = 1.7892
I0814 22:52:25.203451 11101 solver.cpp:334]     Train net output #0: loss = 2.11732 (* 1 = 2.11732 loss)
I0814 22:52:25.203471 11101 sgd_solver.cpp:136] Iteration 69900, lr = 0.0781563, m = 0.9
I0814 22:52:39.832877 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_70000.caffemodel
I0814 22:52:39.866791 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_70000.solverstate
I0814 22:52:39.871191 11101 solver.cpp:509] Iteration 70000, Testing net (#0)
I0814 22:52:55.147374 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 22:53:00.816289 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.38847
I0814 22:53:00.816336 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.64494
I0814 22:53:00.816344 11101 solver.cpp:594]     Test net output #2: loss = 2.90838 (* 1 = 2.90838 loss)
I0814 22:53:00.816395 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9446s
I0814 22:53:00.990252 11101 solver.cpp:312] Iteration 70000 (2.7944 iter/s, 35.7859s/100 iter), loss = 2.23968
I0814 22:53:00.990279 11101 solver.cpp:334]     Train net output #0: loss = 2.47635 (* 1 = 2.47635 loss)
I0814 22:53:00.990284 11101 sgd_solver.cpp:136] Iteration 70000, lr = 0.078125, m = 0.9
I0814 22:53:15.675099 11101 solver.cpp:312] Iteration 70100 (6.80993 iter/s, 14.6844s/100 iter), loss = 2.16519
I0814 22:53:15.675169 11101 solver.cpp:334]     Train net output #0: loss = 1.98491 (* 1 = 1.98491 loss)
I0814 22:53:15.675190 11101 sgd_solver.cpp:136] Iteration 70100, lr = 0.0780938, m = 0.9
I0814 22:53:30.601774 11101 solver.cpp:312] Iteration 70200 (6.6996 iter/s, 14.9263s/100 iter), loss = 1.94536
I0814 22:53:30.601802 11101 solver.cpp:334]     Train net output #0: loss = 2.01876 (* 1 = 2.01876 loss)
I0814 22:53:30.601809 11101 sgd_solver.cpp:136] Iteration 70200, lr = 0.0780625, m = 0.9
I0814 22:53:44.977984 11101 solver.cpp:312] Iteration 70300 (6.95613 iter/s, 14.3758s/100 iter), loss = 2.47017
I0814 22:53:44.978068 11101 solver.cpp:334]     Train net output #0: loss = 2.05654 (* 1 = 2.05654 loss)
I0814 22:53:44.978082 11101 sgd_solver.cpp:136] Iteration 70300, lr = 0.0780312, m = 0.9
I0814 22:53:59.661437 11101 solver.cpp:312] Iteration 70400 (6.81058 iter/s, 14.683s/100 iter), loss = 1.89577
I0814 22:53:59.661490 11101 solver.cpp:334]     Train net output #0: loss = 1.68884 (* 1 = 1.68884 loss)
I0814 22:53:59.661504 11101 sgd_solver.cpp:136] Iteration 70400, lr = 0.078, m = 0.9
I0814 22:54:14.317418 11101 solver.cpp:312] Iteration 70500 (6.82335 iter/s, 14.6556s/100 iter), loss = 1.95949
I0814 22:54:14.317445 11101 solver.cpp:334]     Train net output #0: loss = 2.15002 (* 1 = 2.15002 loss)
I0814 22:54:14.317451 11101 sgd_solver.cpp:136] Iteration 70500, lr = 0.0779688, m = 0.9
I0814 22:54:29.213837 11101 solver.cpp:312] Iteration 70600 (6.71321 iter/s, 14.896s/100 iter), loss = 2.50164
I0814 22:54:29.213927 11101 solver.cpp:334]     Train net output #0: loss = 2.28394 (* 1 = 2.28394 loss)
I0814 22:54:29.213944 11101 sgd_solver.cpp:136] Iteration 70600, lr = 0.0779375, m = 0.9
I0814 22:54:43.658915 11101 solver.cpp:312] Iteration 70700 (6.92297 iter/s, 14.4447s/100 iter), loss = 2.2843
I0814 22:54:43.658946 11101 solver.cpp:334]     Train net output #0: loss = 2.07733 (* 1 = 2.07733 loss)
I0814 22:54:43.658951 11101 sgd_solver.cpp:136] Iteration 70700, lr = 0.0779063, m = 0.9
I0814 22:54:58.171573 11101 solver.cpp:312] Iteration 70800 (6.89073 iter/s, 14.5122s/100 iter), loss = 1.99577
I0814 22:54:58.171602 11101 solver.cpp:334]     Train net output #0: loss = 1.93771 (* 1 = 1.93771 loss)
I0814 22:54:58.171607 11101 sgd_solver.cpp:136] Iteration 70800, lr = 0.077875, m = 0.9
I0814 22:55:12.731240 11101 solver.cpp:312] Iteration 70900 (6.86848 iter/s, 14.5593s/100 iter), loss = 1.89214
I0814 22:55:12.731477 11101 solver.cpp:334]     Train net output #0: loss = 1.56764 (* 1 = 1.56764 loss)
I0814 22:55:12.731586 11101 sgd_solver.cpp:136] Iteration 70900, lr = 0.0778437, m = 0.9
I0814 22:55:27.351619 11101 solver.cpp:312] Iteration 71000 (6.83996 iter/s, 14.62s/100 iter), loss = 1.66981
I0814 22:55:27.351681 11101 solver.cpp:334]     Train net output #0: loss = 1.66406 (* 1 = 1.66406 loss)
I0814 22:55:27.351699 11101 sgd_solver.cpp:136] Iteration 71000, lr = 0.0778125, m = 0.9
I0814 22:55:41.807924 11101 solver.cpp:312] Iteration 71100 (6.91759 iter/s, 14.4559s/100 iter), loss = 1.81907
I0814 22:55:41.807988 11101 solver.cpp:334]     Train net output #0: loss = 2.07018 (* 1 = 2.07018 loss)
I0814 22:55:41.808009 11101 sgd_solver.cpp:136] Iteration 71100, lr = 0.0777813, m = 0.9
I0814 22:55:56.507421 11101 solver.cpp:312] Iteration 71200 (6.80315 iter/s, 14.6991s/100 iter), loss = 2.16746
I0814 22:55:56.507664 11101 solver.cpp:334]     Train net output #0: loss = 1.85905 (* 1 = 1.85905 loss)
I0814 22:55:56.507776 11101 sgd_solver.cpp:136] Iteration 71200, lr = 0.07775, m = 0.9
I0814 22:56:11.209329 11101 solver.cpp:312] Iteration 71300 (6.80203 iter/s, 14.7015s/100 iter), loss = 1.94466
I0814 22:56:11.209359 11101 solver.cpp:334]     Train net output #0: loss = 1.96967 (* 1 = 1.96967 loss)
I0814 22:56:11.209365 11101 sgd_solver.cpp:136] Iteration 71300, lr = 0.0777187, m = 0.9
I0814 22:56:25.763883 11101 solver.cpp:312] Iteration 71400 (6.8709 iter/s, 14.5541s/100 iter), loss = 2.31008
I0814 22:56:25.763960 11101 solver.cpp:334]     Train net output #0: loss = 2.70893 (* 1 = 2.70893 loss)
I0814 22:56:25.763980 11101 sgd_solver.cpp:136] Iteration 71400, lr = 0.0776875, m = 0.9
I0814 22:56:40.730038 11101 solver.cpp:312] Iteration 71500 (6.68193 iter/s, 14.9657s/100 iter), loss = 1.90725
I0814 22:56:40.730124 11101 solver.cpp:334]     Train net output #0: loss = 1.95678 (* 1 = 1.95678 loss)
I0814 22:56:40.730144 11101 sgd_solver.cpp:136] Iteration 71500, lr = 0.0776563, m = 0.9
I0814 22:56:55.368463 11101 solver.cpp:312] Iteration 71600 (6.83153 iter/s, 14.638s/100 iter), loss = 1.7741
I0814 22:56:55.368535 11101 solver.cpp:334]     Train net output #0: loss = 1.68009 (* 1 = 1.68009 loss)
I0814 22:56:55.368552 11101 sgd_solver.cpp:136] Iteration 71600, lr = 0.077625, m = 0.9
I0814 22:57:09.927321 11101 solver.cpp:312] Iteration 71700 (6.86886 iter/s, 14.5584s/100 iter), loss = 2.28093
I0814 22:57:09.927351 11101 solver.cpp:334]     Train net output #0: loss = 2.23405 (* 1 = 2.23405 loss)
I0814 22:57:09.927358 11101 sgd_solver.cpp:136] Iteration 71700, lr = 0.0775938, m = 0.9
I0814 22:57:24.478945 11101 solver.cpp:312] Iteration 71800 (6.87228 iter/s, 14.5512s/100 iter), loss = 2.41668
I0814 22:57:24.479051 11101 solver.cpp:334]     Train net output #0: loss = 2.27036 (* 1 = 2.27036 loss)
I0814 22:57:24.479070 11101 sgd_solver.cpp:136] Iteration 71800, lr = 0.0775625, m = 0.9
I0814 22:57:38.913843 11101 solver.cpp:312] Iteration 71900 (6.92785 iter/s, 14.4345s/100 iter), loss = 2.44286
I0814 22:57:38.913914 11101 solver.cpp:334]     Train net output #0: loss = 2.08525 (* 1 = 2.08525 loss)
I0814 22:57:38.913934 11101 sgd_solver.cpp:136] Iteration 71900, lr = 0.0775312, m = 0.9
I0814 22:57:53.670106 11101 solver.cpp:509] Iteration 72000, Testing net (#0)
I0814 22:58:14.868399 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.376117
I0814 22:58:14.868450 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.63447
I0814 22:58:14.868458 11101 solver.cpp:594]     Test net output #2: loss = 2.9278 (* 1 = 2.9278 loss)
I0814 22:58:14.868474 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1978s
I0814 22:58:15.008864 11101 solver.cpp:312] Iteration 72000 (2.77054 iter/s, 36.094s/100 iter), loss = 2.39293
I0814 22:58:15.008890 11101 solver.cpp:334]     Train net output #0: loss = 2.16025 (* 1 = 2.16025 loss)
I0814 22:58:15.008896 11101 sgd_solver.cpp:136] Iteration 72000, lr = 0.0775, m = 0.9
I0814 22:58:29.337515 11101 solver.cpp:312] Iteration 72100 (6.97922 iter/s, 14.3282s/100 iter), loss = 2.25236
I0814 22:58:29.337590 11101 solver.cpp:334]     Train net output #0: loss = 2.05681 (* 1 = 2.05681 loss)
I0814 22:58:29.337599 11101 sgd_solver.cpp:136] Iteration 72100, lr = 0.0774688, m = 0.9
I0814 22:58:43.993974 11101 solver.cpp:312] Iteration 72200 (6.82312 iter/s, 14.656s/100 iter), loss = 2.02519
I0814 22:58:43.994002 11101 solver.cpp:334]     Train net output #0: loss = 2.13528 (* 1 = 2.13528 loss)
I0814 22:58:43.994007 11101 sgd_solver.cpp:136] Iteration 72200, lr = 0.0774375, m = 0.9
I0814 22:58:58.295653 11101 solver.cpp:312] Iteration 72300 (6.99238 iter/s, 14.3013s/100 iter), loss = 2.07714
I0814 22:58:58.295909 11101 solver.cpp:334]     Train net output #0: loss = 1.69474 (* 1 = 1.69474 loss)
I0814 22:58:58.296020 11101 sgd_solver.cpp:136] Iteration 72300, lr = 0.0774062, m = 0.9
I0814 22:59:12.878070 11101 solver.cpp:312] Iteration 72400 (6.85777 iter/s, 14.582s/100 iter), loss = 2.38381
I0814 22:59:12.878098 11101 solver.cpp:334]     Train net output #0: loss = 2.6592 (* 1 = 2.6592 loss)
I0814 22:59:12.878103 11101 sgd_solver.cpp:136] Iteration 72400, lr = 0.077375, m = 0.9
I0814 22:59:27.249402 11101 solver.cpp:312] Iteration 72500 (6.95849 iter/s, 14.3709s/100 iter), loss = 1.81803
I0814 22:59:27.249428 11101 solver.cpp:334]     Train net output #0: loss = 1.99871 (* 1 = 1.99871 loss)
I0814 22:59:27.249433 11101 sgd_solver.cpp:136] Iteration 72500, lr = 0.0773438, m = 0.9
I0814 22:59:41.593765 11101 solver.cpp:312] Iteration 72600 (6.97158 iter/s, 14.344s/100 iter), loss = 1.98357
I0814 22:59:41.593834 11101 solver.cpp:334]     Train net output #0: loss = 2.06606 (* 1 = 2.06606 loss)
I0814 22:59:41.593842 11101 sgd_solver.cpp:136] Iteration 72600, lr = 0.0773125, m = 0.9
I0814 22:59:56.180254 11101 solver.cpp:312] Iteration 72700 (6.85585 iter/s, 14.5861s/100 iter), loss = 1.91313
I0814 22:59:56.180281 11101 solver.cpp:334]     Train net output #0: loss = 1.80319 (* 1 = 1.80319 loss)
I0814 22:59:56.180287 11101 sgd_solver.cpp:136] Iteration 72700, lr = 0.0772813, m = 0.9
I0814 23:00:10.525594 11101 solver.cpp:312] Iteration 72800 (6.9711 iter/s, 14.3449s/100 iter), loss = 2.08468
I0814 23:00:10.525624 11101 solver.cpp:334]     Train net output #0: loss = 2.26445 (* 1 = 2.26445 loss)
I0814 23:00:10.525630 11101 sgd_solver.cpp:136] Iteration 72800, lr = 0.07725, m = 0.9
I0814 23:00:25.286648 11101 solver.cpp:312] Iteration 72900 (6.77477 iter/s, 14.7606s/100 iter), loss = 2.26933
I0814 23:00:25.286751 11101 solver.cpp:334]     Train net output #0: loss = 2.31891 (* 1 = 2.31891 loss)
I0814 23:00:25.286770 11101 sgd_solver.cpp:136] Iteration 72900, lr = 0.0772187, m = 0.9
I0814 23:00:40.086774 11101 solver.cpp:312] Iteration 73000 (6.75689 iter/s, 14.7997s/100 iter), loss = 2.10369
I0814 23:00:40.086848 11101 solver.cpp:334]     Train net output #0: loss = 2.23136 (* 1 = 2.23136 loss)
I0814 23:00:40.086869 11101 sgd_solver.cpp:136] Iteration 73000, lr = 0.0771875, m = 0.9
I0814 23:00:54.597913 11101 solver.cpp:312] Iteration 73100 (6.89145 iter/s, 14.5107s/100 iter), loss = 2.45075
I0814 23:00:54.597940 11101 solver.cpp:334]     Train net output #0: loss = 2.36957 (* 1 = 2.36957 loss)
I0814 23:00:54.598044 11101 sgd_solver.cpp:136] Iteration 73100, lr = 0.0771563, m = 0.9
I0814 23:01:09.388628 11101 solver.cpp:312] Iteration 73200 (6.76119 iter/s, 14.7903s/100 iter), loss = 1.96354
I0814 23:01:09.388695 11101 solver.cpp:334]     Train net output #0: loss = 2.05063 (* 1 = 2.05063 loss)
I0814 23:01:09.388702 11101 sgd_solver.cpp:136] Iteration 73200, lr = 0.077125, m = 0.9
I0814 23:01:24.375062 11101 solver.cpp:312] Iteration 73300 (6.67289 iter/s, 14.986s/100 iter), loss = 1.87941
I0814 23:01:24.375090 11101 solver.cpp:334]     Train net output #0: loss = 2.11699 (* 1 = 2.11699 loss)
I0814 23:01:24.375097 11101 sgd_solver.cpp:136] Iteration 73300, lr = 0.0770938, m = 0.9
I0814 23:01:39.373376 11101 solver.cpp:312] Iteration 73400 (6.6676 iter/s, 14.9979s/100 iter), loss = 1.93398
I0814 23:01:39.373440 11101 solver.cpp:334]     Train net output #0: loss = 1.8247 (* 1 = 1.8247 loss)
I0814 23:01:39.373457 11101 sgd_solver.cpp:136] Iteration 73400, lr = 0.0770625, m = 0.9
I0814 23:01:54.194916 11101 solver.cpp:312] Iteration 73500 (6.74713 iter/s, 14.8211s/100 iter), loss = 2.2199
I0814 23:01:54.194984 11101 solver.cpp:334]     Train net output #0: loss = 1.9483 (* 1 = 1.9483 loss)
I0814 23:01:54.194993 11101 sgd_solver.cpp:136] Iteration 73500, lr = 0.0770312, m = 0.9
I0814 23:02:08.635504 11101 solver.cpp:312] Iteration 73600 (6.92512 iter/s, 14.4402s/100 iter), loss = 1.5352
I0814 23:02:08.635576 11101 solver.cpp:334]     Train net output #0: loss = 1.41665 (* 1 = 1.41665 loss)
I0814 23:02:08.635596 11101 sgd_solver.cpp:136] Iteration 73600, lr = 0.077, m = 0.9
I0814 23:02:23.228621 11101 solver.cpp:312] Iteration 73700 (6.85274 iter/s, 14.5927s/100 iter), loss = 2.31837
I0814 23:02:23.228646 11101 solver.cpp:334]     Train net output #0: loss = 2.39745 (* 1 = 2.39745 loss)
I0814 23:02:23.228651 11101 sgd_solver.cpp:136] Iteration 73700, lr = 0.0769688, m = 0.9
I0814 23:02:38.325278 11101 solver.cpp:312] Iteration 73800 (6.62417 iter/s, 15.0962s/100 iter), loss = 2.12225
I0814 23:02:38.325353 11101 solver.cpp:334]     Train net output #0: loss = 2.40722 (* 1 = 2.40722 loss)
I0814 23:02:38.325361 11101 sgd_solver.cpp:136] Iteration 73800, lr = 0.0769375, m = 0.9
I0814 23:02:52.932169 11101 solver.cpp:312] Iteration 73900 (6.84628 iter/s, 14.6065s/100 iter), loss = 2.07116
I0814 23:02:52.932198 11101 solver.cpp:334]     Train net output #0: loss = 1.8196 (* 1 = 1.8196 loss)
I0814 23:02:52.932204 11101 sgd_solver.cpp:136] Iteration 73900, lr = 0.0769063, m = 0.9
I0814 23:03:08.050355 11101 solver.cpp:509] Iteration 74000, Testing net (#0)
I0814 23:03:13.672940 11099 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 23:03:28.929059 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.417471
I0814 23:03:28.929083 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.680058
I0814 23:03:28.929090 11101 solver.cpp:594]     Test net output #2: loss = 2.66817 (* 1 = 2.66817 loss)
I0814 23:03:28.929141 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8782s
I0814 23:03:29.096159 11101 solver.cpp:312] Iteration 74000 (2.76526 iter/s, 36.163s/100 iter), loss = 2.13122
I0814 23:03:29.096181 11101 solver.cpp:334]     Train net output #0: loss = 1.99708 (* 1 = 1.99708 loss)
I0814 23:03:29.096187 11101 sgd_solver.cpp:136] Iteration 74000, lr = 0.076875, m = 0.9
I0814 23:03:43.575316 11101 solver.cpp:312] Iteration 74100 (6.90667 iter/s, 14.4787s/100 iter), loss = 2.34048
I0814 23:03:43.575346 11101 solver.cpp:334]     Train net output #0: loss = 2.22017 (* 1 = 2.22017 loss)
I0814 23:03:43.575352 11101 sgd_solver.cpp:136] Iteration 74100, lr = 0.0768438, m = 0.9
I0814 23:03:58.295537 11101 solver.cpp:312] Iteration 74200 (6.79357 iter/s, 14.7198s/100 iter), loss = 2.39283
I0814 23:03:58.295630 11101 solver.cpp:334]     Train net output #0: loss = 2.57778 (* 1 = 2.57778 loss)
I0814 23:03:58.295647 11101 sgd_solver.cpp:136] Iteration 74200, lr = 0.0768125, m = 0.9
I0814 23:04:13.008513 11101 solver.cpp:312] Iteration 74300 (6.79691 iter/s, 14.7126s/100 iter), loss = 2.12958
I0814 23:04:13.008579 11101 solver.cpp:334]     Train net output #0: loss = 2.37068 (* 1 = 2.37068 loss)
I0814 23:04:13.008596 11101 sgd_solver.cpp:136] Iteration 74300, lr = 0.0767813, m = 0.9
I0814 23:04:27.408112 11101 solver.cpp:312] Iteration 74400 (6.94483 iter/s, 14.3992s/100 iter), loss = 1.78478
I0814 23:04:27.408149 11101 solver.cpp:334]     Train net output #0: loss = 1.57626 (* 1 = 1.57626 loss)
I0814 23:04:27.408154 11101 sgd_solver.cpp:136] Iteration 74400, lr = 0.07675, m = 0.9
I0814 23:04:42.179625 11101 solver.cpp:312] Iteration 74500 (6.76998 iter/s, 14.7711s/100 iter), loss = 2.30739
I0814 23:04:42.179790 11101 solver.cpp:334]     Train net output #0: loss = 2.79483 (* 1 = 2.79483 loss)
I0814 23:04:42.179808 11101 sgd_solver.cpp:136] Iteration 74500, lr = 0.0767187, m = 0.9
I0814 23:04:56.736177 11101 solver.cpp:312] Iteration 74600 (6.86995 iter/s, 14.5561s/100 iter), loss = 2.00134
I0814 23:04:56.736202 11101 solver.cpp:334]     Train net output #0: loss = 1.83321 (* 1 = 1.83321 loss)
I0814 23:04:56.736208 11101 sgd_solver.cpp:136] Iteration 74600, lr = 0.0766875, m = 0.9
I0814 23:05:11.268671 11101 solver.cpp:312] Iteration 74700 (6.88133 iter/s, 14.5321s/100 iter), loss = 2.03395
I0814 23:05:11.268695 11101 solver.cpp:334]     Train net output #0: loss = 2.01592 (* 1 = 2.01592 loss)
I0814 23:05:11.268700 11101 sgd_solver.cpp:136] Iteration 74700, lr = 0.0766563, m = 0.9
I0814 23:05:25.993541 11101 solver.cpp:312] Iteration 74800 (6.79142 iter/s, 14.7245s/100 iter), loss = 2.51711
I0814 23:05:25.993593 11101 solver.cpp:334]     Train net output #0: loss = 2.67762 (* 1 = 2.67762 loss)
I0814 23:05:25.993600 11101 sgd_solver.cpp:136] Iteration 74800, lr = 0.076625, m = 0.9
I0814 23:05:40.298050 11101 solver.cpp:312] Iteration 74900 (6.991 iter/s, 14.3041s/100 iter), loss = 2.42851
I0814 23:05:40.298075 11101 solver.cpp:334]     Train net output #0: loss = 2.4752 (* 1 = 2.4752 loss)
I0814 23:05:40.298080 11101 sgd_solver.cpp:136] Iteration 74900, lr = 0.0765937, m = 0.9
I0814 23:05:54.687840 11101 solver.cpp:312] Iteration 75000 (6.94957 iter/s, 14.3894s/100 iter), loss = 1.93608
I0814 23:05:54.687867 11101 solver.cpp:334]     Train net output #0: loss = 1.73071 (* 1 = 1.73071 loss)
I0814 23:05:54.687875 11101 sgd_solver.cpp:136] Iteration 75000, lr = 0.0765625, m = 0.9
I0814 23:06:09.195209 11101 solver.cpp:312] Iteration 75100 (6.89324 iter/s, 14.507s/100 iter), loss = 1.4494
I0814 23:06:09.195274 11101 solver.cpp:334]     Train net output #0: loss = 1.52868 (* 1 = 1.52868 loss)
I0814 23:06:09.195283 11101 sgd_solver.cpp:136] Iteration 75100, lr = 0.0765313, m = 0.9
I0814 23:06:23.743131 11101 solver.cpp:312] Iteration 75200 (6.87403 iter/s, 14.5475s/100 iter), loss = 2.21375
I0814 23:06:23.743158 11101 solver.cpp:334]     Train net output #0: loss = 2.31433 (* 1 = 2.31433 loss)
I0814 23:06:23.743165 11101 sgd_solver.cpp:136] Iteration 75200, lr = 0.0765, m = 0.9
I0814 23:06:38.287802 11101 solver.cpp:312] Iteration 75300 (6.87557 iter/s, 14.5443s/100 iter), loss = 2.27412
I0814 23:06:38.287829 11101 solver.cpp:334]     Train net output #0: loss = 2.83133 (* 1 = 2.83133 loss)
I0814 23:06:38.287833 11101 sgd_solver.cpp:136] Iteration 75300, lr = 0.0764688, m = 0.9
I0814 23:06:52.924878 11101 solver.cpp:312] Iteration 75400 (6.83216 iter/s, 14.6367s/100 iter), loss = 2.34413
I0814 23:06:52.924945 11101 solver.cpp:334]     Train net output #0: loss = 2.53711 (* 1 = 2.53711 loss)
I0814 23:06:52.924952 11101 sgd_solver.cpp:136] Iteration 75400, lr = 0.0764375, m = 0.9
I0814 23:07:07.651636 11101 solver.cpp:312] Iteration 75500 (6.79055 iter/s, 14.7263s/100 iter), loss = 2.28603
I0814 23:07:07.651700 11101 solver.cpp:334]     Train net output #0: loss = 2.41726 (* 1 = 2.41726 loss)
I0814 23:07:07.651717 11101 sgd_solver.cpp:136] Iteration 75500, lr = 0.0764063, m = 0.9
I0814 23:07:22.605809 11101 solver.cpp:312] Iteration 75600 (6.68729 iter/s, 14.9538s/100 iter), loss = 1.81658
I0814 23:07:22.605835 11101 solver.cpp:334]     Train net output #0: loss = 1.78001 (* 1 = 1.78001 loss)
I0814 23:07:22.605839 11101 sgd_solver.cpp:136] Iteration 75600, lr = 0.076375, m = 0.9
I0814 23:07:37.102370 11101 solver.cpp:312] Iteration 75700 (6.89838 iter/s, 14.4962s/100 iter), loss = 2.08217
I0814 23:07:37.102460 11101 solver.cpp:334]     Train net output #0: loss = 2.23053 (* 1 = 2.23053 loss)
I0814 23:07:37.102478 11101 sgd_solver.cpp:136] Iteration 75700, lr = 0.0763438, m = 0.9
I0814 23:07:51.854312 11101 solver.cpp:312] Iteration 75800 (6.77896 iter/s, 14.7515s/100 iter), loss = 1.96506
I0814 23:07:51.854382 11101 solver.cpp:334]     Train net output #0: loss = 2.1591 (* 1 = 2.1591 loss)
I0814 23:07:51.854405 11101 sgd_solver.cpp:136] Iteration 75800, lr = 0.0763125, m = 0.9
I0814 23:08:06.675683 11101 solver.cpp:312] Iteration 75900 (6.7472 iter/s, 14.821s/100 iter), loss = 1.99125
I0814 23:08:06.675712 11101 solver.cpp:334]     Train net output #0: loss = 1.72732 (* 1 = 1.72732 loss)
I0814 23:08:06.675719 11101 sgd_solver.cpp:136] Iteration 75900, lr = 0.0762812, m = 0.9
I0814 23:08:20.990526 11101 solver.cpp:509] Iteration 76000, Testing net (#0)
I0814 23:08:33.191905 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 23:08:42.277895 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.39747
I0814 23:08:42.277917 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.652352
I0814 23:08:42.277925 11101 solver.cpp:594]     Test net output #2: loss = 2.88134 (* 1 = 2.88134 loss)
I0814 23:08:42.277946 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.2868s
I0814 23:08:42.462249 11101 solver.cpp:312] Iteration 76000 (2.79442 iter/s, 35.7855s/100 iter), loss = 2.3322
I0814 23:08:42.462324 11101 solver.cpp:334]     Train net output #0: loss = 2.4029 (* 1 = 2.4029 loss)
I0814 23:08:42.462350 11101 sgd_solver.cpp:136] Iteration 76000, lr = 0.07625, m = 0.9
I0814 23:08:57.159548 11101 solver.cpp:312] Iteration 76100 (6.80416 iter/s, 14.6969s/100 iter), loss = 1.94072
I0814 23:08:57.159611 11101 solver.cpp:334]     Train net output #0: loss = 1.73957 (* 1 = 1.73957 loss)
I0814 23:08:57.159618 11101 sgd_solver.cpp:136] Iteration 76100, lr = 0.0762187, m = 0.9
I0814 23:09:11.808899 11101 solver.cpp:312] Iteration 76200 (6.82644 iter/s, 14.6489s/100 iter), loss = 1.81889
I0814 23:09:11.808925 11101 solver.cpp:334]     Train net output #0: loss = 1.81403 (* 1 = 1.81403 loss)
I0814 23:09:11.808931 11101 sgd_solver.cpp:136] Iteration 76200, lr = 0.0761875, m = 0.9
I0814 23:09:26.579020 11101 solver.cpp:312] Iteration 76300 (6.77062 iter/s, 14.7697s/100 iter), loss = 2.07009
I0814 23:09:26.579046 11101 solver.cpp:334]     Train net output #0: loss = 2.169 (* 1 = 2.169 loss)
I0814 23:09:26.579051 11101 sgd_solver.cpp:136] Iteration 76300, lr = 0.0761563, m = 0.9
I0814 23:09:41.482511 11101 solver.cpp:312] Iteration 76400 (6.71003 iter/s, 14.9031s/100 iter), loss = 1.72084
I0814 23:09:41.482575 11101 solver.cpp:334]     Train net output #0: loss = 1.60884 (* 1 = 1.60884 loss)
I0814 23:09:41.482584 11101 sgd_solver.cpp:136] Iteration 76400, lr = 0.076125, m = 0.9
I0814 23:09:56.190834 11101 solver.cpp:312] Iteration 76500 (6.79907 iter/s, 14.7079s/100 iter), loss = 2.07175
I0814 23:09:56.190857 11101 solver.cpp:334]     Train net output #0: loss = 1.95991 (* 1 = 1.95991 loss)
I0814 23:09:56.190861 11101 sgd_solver.cpp:136] Iteration 76500, lr = 0.0760938, m = 0.9
I0814 23:10:10.610847 11101 solver.cpp:312] Iteration 76600 (6.935 iter/s, 14.4196s/100 iter), loss = 2.00141
I0814 23:10:10.610872 11101 solver.cpp:334]     Train net output #0: loss = 1.94754 (* 1 = 1.94754 loss)
I0814 23:10:10.610877 11101 sgd_solver.cpp:136] Iteration 76600, lr = 0.0760625, m = 0.9
I0814 23:10:24.993083 11101 solver.cpp:312] Iteration 76700 (6.95322 iter/s, 14.3818s/100 iter), loss = 1.9486
I0814 23:10:24.993173 11101 solver.cpp:334]     Train net output #0: loss = 2.08258 (* 1 = 2.08258 loss)
I0814 23:10:24.993191 11101 sgd_solver.cpp:136] Iteration 76700, lr = 0.0760313, m = 0.9
I0814 23:10:39.394651 11101 solver.cpp:312] Iteration 76800 (6.94389 iter/s, 14.4012s/100 iter), loss = 1.80821
I0814 23:10:39.394680 11101 solver.cpp:334]     Train net output #0: loss = 1.65465 (* 1 = 1.65465 loss)
I0814 23:10:39.394687 11101 sgd_solver.cpp:136] Iteration 76800, lr = 0.076, m = 0.9
I0814 23:10:53.940667 11101 solver.cpp:312] Iteration 76900 (6.87493 iter/s, 14.5456s/100 iter), loss = 2.0758
I0814 23:10:53.940695 11101 solver.cpp:334]     Train net output #0: loss = 2.11321 (* 1 = 2.11321 loss)
I0814 23:10:53.940701 11101 sgd_solver.cpp:136] Iteration 76900, lr = 0.0759687, m = 0.9
I0814 23:11:08.138586 11101 solver.cpp:312] Iteration 77000 (7.04349 iter/s, 14.1975s/100 iter), loss = 2.23209
I0814 23:11:08.138664 11101 solver.cpp:334]     Train net output #0: loss = 2.16044 (* 1 = 2.16044 loss)
I0814 23:11:08.138670 11101 sgd_solver.cpp:136] Iteration 77000, lr = 0.0759375, m = 0.9
I0814 23:11:22.665122 11101 solver.cpp:312] Iteration 77100 (6.88415 iter/s, 14.5261s/100 iter), loss = 1.94394
I0814 23:11:22.665150 11101 solver.cpp:334]     Train net output #0: loss = 1.95298 (* 1 = 1.95298 loss)
I0814 23:11:22.665154 11101 sgd_solver.cpp:136] Iteration 77100, lr = 0.0759063, m = 0.9
I0814 23:11:37.168251 11101 solver.cpp:312] Iteration 77200 (6.89526 iter/s, 14.5027s/100 iter), loss = 1.86547
I0814 23:11:37.168329 11101 solver.cpp:334]     Train net output #0: loss = 1.49988 (* 1 = 1.49988 loss)
I0814 23:11:37.168354 11101 sgd_solver.cpp:136] Iteration 77200, lr = 0.075875, m = 0.9
I0814 23:11:51.776654 11101 solver.cpp:312] Iteration 77300 (6.84557 iter/s, 14.608s/100 iter), loss = 2.00476
I0814 23:11:51.776701 11101 solver.cpp:334]     Train net output #0: loss = 2.0059 (* 1 = 2.0059 loss)
I0814 23:11:51.776708 11101 sgd_solver.cpp:136] Iteration 77300, lr = 0.0758438, m = 0.9
I0814 23:12:06.220458 11101 solver.cpp:312] Iteration 77400 (6.92358 iter/s, 14.4434s/100 iter), loss = 2.05219
I0814 23:12:06.220485 11101 solver.cpp:334]     Train net output #0: loss = 1.76721 (* 1 = 1.76721 loss)
I0814 23:12:06.220489 11101 sgd_solver.cpp:136] Iteration 77400, lr = 0.0758125, m = 0.9
I0814 23:12:20.896037 11101 solver.cpp:312] Iteration 77500 (6.81423 iter/s, 14.6752s/100 iter), loss = 2.46493
I0814 23:12:20.896085 11101 solver.cpp:334]     Train net output #0: loss = 2.40453 (* 1 = 2.40453 loss)
I0814 23:12:20.896097 11101 sgd_solver.cpp:136] Iteration 77500, lr = 0.0757812, m = 0.9
I0814 23:12:35.501804 11101 solver.cpp:312] Iteration 77600 (6.8468 iter/s, 14.6054s/100 iter), loss = 1.83148
I0814 23:12:35.501912 11101 solver.cpp:334]     Train net output #0: loss = 1.59964 (* 1 = 1.59964 loss)
I0814 23:12:35.501927 11101 sgd_solver.cpp:136] Iteration 77600, lr = 0.07575, m = 0.9
I0814 23:12:50.208782 11101 solver.cpp:312] Iteration 77700 (6.79968 iter/s, 14.7066s/100 iter), loss = 2.31913
I0814 23:12:50.208832 11101 solver.cpp:334]     Train net output #0: loss = 2.36228 (* 1 = 2.36228 loss)
I0814 23:12:50.208845 11101 sgd_solver.cpp:136] Iteration 77700, lr = 0.0757188, m = 0.9
I0814 23:13:04.703675 11101 solver.cpp:312] Iteration 77800 (6.89918 iter/s, 14.4945s/100 iter), loss = 2.06989
I0814 23:13:04.703727 11101 solver.cpp:334]     Train net output #0: loss = 2.5204 (* 1 = 2.5204 loss)
I0814 23:13:04.703742 11101 sgd_solver.cpp:136] Iteration 77800, lr = 0.0756875, m = 0.9
I0814 23:13:19.085427 11101 solver.cpp:312] Iteration 77900 (6.95345 iter/s, 14.3814s/100 iter), loss = 1.74243
I0814 23:13:19.085482 11101 solver.cpp:334]     Train net output #0: loss = 1.58616 (* 1 = 1.58616 loss)
I0814 23:13:19.085489 11101 sgd_solver.cpp:136] Iteration 77900, lr = 0.0756563, m = 0.9
I0814 23:13:33.431990 11101 solver.cpp:509] Iteration 78000, Testing net (#0)
I0814 23:13:54.752766 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.359588
I0814 23:13:54.756217 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.617706
I0814 23:13:54.756253 11101 solver.cpp:594]     Test net output #2: loss = 3.0298 (* 1 = 3.0298 loss)
I0814 23:13:54.756300 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3237s
I0814 23:13:54.921870 11101 solver.cpp:312] Iteration 78000 (2.79053 iter/s, 35.8354s/100 iter), loss = 1.71358
I0814 23:13:54.921895 11101 solver.cpp:334]     Train net output #0: loss = 1.77254 (* 1 = 1.77254 loss)
I0814 23:13:54.921900 11101 sgd_solver.cpp:136] Iteration 78000, lr = 0.075625, m = 0.9
I0814 23:14:09.511970 11101 solver.cpp:312] Iteration 78100 (6.85416 iter/s, 14.5897s/100 iter), loss = 2.03886
I0814 23:14:09.511998 11101 solver.cpp:334]     Train net output #0: loss = 2.2008 (* 1 = 2.2008 loss)
I0814 23:14:09.512002 11101 sgd_solver.cpp:136] Iteration 78100, lr = 0.0755938, m = 0.9
I0814 23:14:24.075558 11101 solver.cpp:312] Iteration 78200 (6.86663 iter/s, 14.5632s/100 iter), loss = 2.2745
I0814 23:14:24.075628 11101 solver.cpp:334]     Train net output #0: loss = 2.31993 (* 1 = 2.31993 loss)
I0814 23:14:24.075645 11101 sgd_solver.cpp:136] Iteration 78200, lr = 0.0755625, m = 0.9
I0814 23:14:38.603363 11101 solver.cpp:312] Iteration 78300 (6.88355 iter/s, 14.5274s/100 iter), loss = 2.29618
I0814 23:14:38.603440 11101 solver.cpp:334]     Train net output #0: loss = 2.07572 (* 1 = 2.07572 loss)
I0814 23:14:38.603447 11101 sgd_solver.cpp:136] Iteration 78300, lr = 0.0755313, m = 0.9
I0814 23:14:53.184692 11101 solver.cpp:312] Iteration 78400 (6.85828 iter/s, 14.5809s/100 iter), loss = 2.06466
I0814 23:14:53.184720 11101 solver.cpp:334]     Train net output #0: loss = 2.13861 (* 1 = 2.13861 loss)
I0814 23:14:53.184725 11101 sgd_solver.cpp:136] Iteration 78400, lr = 0.0755, m = 0.9
I0814 23:15:08.016758 11101 solver.cpp:312] Iteration 78500 (6.74234 iter/s, 14.8316s/100 iter), loss = 1.93532
I0814 23:15:08.016821 11101 solver.cpp:334]     Train net output #0: loss = 2.19427 (* 1 = 2.19427 loss)
I0814 23:15:08.016839 11101 sgd_solver.cpp:136] Iteration 78500, lr = 0.0754687, m = 0.9
I0814 23:15:22.556668 11101 solver.cpp:312] Iteration 78600 (6.87782 iter/s, 14.5395s/100 iter), loss = 2.07778
I0814 23:15:22.556727 11101 solver.cpp:334]     Train net output #0: loss = 2.41679 (* 1 = 2.41679 loss)
I0814 23:15:22.556735 11101 sgd_solver.cpp:136] Iteration 78600, lr = 0.0754375, m = 0.9
I0814 23:15:37.002022 11101 solver.cpp:312] Iteration 78700 (6.92284 iter/s, 14.4449s/100 iter), loss = 2.45704
I0814 23:15:37.002049 11101 solver.cpp:334]     Train net output #0: loss = 2.53131 (* 1 = 2.53131 loss)
I0814 23:15:37.002055 11101 sgd_solver.cpp:136] Iteration 78700, lr = 0.0754063, m = 0.9
I0814 23:15:51.621914 11101 solver.cpp:312] Iteration 78800 (6.84019 iter/s, 14.6195s/100 iter), loss = 2.30118
I0814 23:15:51.621943 11101 solver.cpp:334]     Train net output #0: loss = 2.14254 (* 1 = 2.14254 loss)
I0814 23:15:51.621984 11101 sgd_solver.cpp:136] Iteration 78800, lr = 0.075375, m = 0.9
I0814 23:16:06.212466 11101 solver.cpp:312] Iteration 78900 (6.85394 iter/s, 14.5901s/100 iter), loss = 1.96647
I0814 23:16:06.212527 11101 solver.cpp:334]     Train net output #0: loss = 1.95215 (* 1 = 1.95215 loss)
I0814 23:16:06.212533 11101 sgd_solver.cpp:136] Iteration 78900, lr = 0.0753438, m = 0.9
I0814 23:16:20.732224 11101 solver.cpp:312] Iteration 79000 (6.88736 iter/s, 14.5193s/100 iter), loss = 1.88141
I0814 23:16:20.732250 11101 solver.cpp:334]     Train net output #0: loss = 1.93035 (* 1 = 1.93035 loss)
I0814 23:16:20.732256 11101 sgd_solver.cpp:136] Iteration 79000, lr = 0.0753125, m = 0.9
I0814 23:16:35.097949 11101 solver.cpp:312] Iteration 79100 (6.96121 iter/s, 14.3653s/100 iter), loss = 2.2619
I0814 23:16:35.097980 11101 solver.cpp:334]     Train net output #0: loss = 1.90611 (* 1 = 1.90611 loss)
I0814 23:16:35.097985 11101 sgd_solver.cpp:136] Iteration 79100, lr = 0.0752813, m = 0.9
I0814 23:16:49.636620 11101 solver.cpp:312] Iteration 79200 (6.8784 iter/s, 14.5383s/100 iter), loss = 2.3059
I0814 23:16:49.637011 11101 solver.cpp:334]     Train net output #0: loss = 2.41449 (* 1 = 2.41449 loss)
I0814 23:16:49.637118 11101 sgd_solver.cpp:136] Iteration 79200, lr = 0.07525, m = 0.9
I0814 23:17:04.163633 11101 solver.cpp:312] Iteration 79300 (6.88392 iter/s, 14.5266s/100 iter), loss = 1.73667
I0814 23:17:04.163704 11101 solver.cpp:334]     Train net output #0: loss = 1.90989 (* 1 = 1.90989 loss)
I0814 23:17:04.163723 11101 sgd_solver.cpp:136] Iteration 79300, lr = 0.0752188, m = 0.9
I0814 23:17:18.635342 11101 solver.cpp:312] Iteration 79400 (6.91023 iter/s, 14.4713s/100 iter), loss = 2.49401
I0814 23:17:18.635396 11101 solver.cpp:334]     Train net output #0: loss = 2.77517 (* 1 = 2.77517 loss)
I0814 23:17:18.635408 11101 sgd_solver.cpp:136] Iteration 79400, lr = 0.0751875, m = 0.9
I0814 23:17:33.063675 11101 solver.cpp:312] Iteration 79500 (6.931 iter/s, 14.4279s/100 iter), loss = 2.34118
I0814 23:17:33.063740 11101 solver.cpp:334]     Train net output #0: loss = 2.41317 (* 1 = 2.41317 loss)
I0814 23:17:33.063747 11101 sgd_solver.cpp:136] Iteration 79500, lr = 0.0751562, m = 0.9
I0814 23:17:47.731468 11101 solver.cpp:312] Iteration 79600 (6.81785 iter/s, 14.6674s/100 iter), loss = 1.76741
I0814 23:17:47.731531 11101 solver.cpp:334]     Train net output #0: loss = 1.8479 (* 1 = 1.8479 loss)
I0814 23:17:47.731550 11101 sgd_solver.cpp:136] Iteration 79600, lr = 0.075125, m = 0.9
I0814 23:18:02.499416 11101 solver.cpp:312] Iteration 79700 (6.77161 iter/s, 14.7675s/100 iter), loss = 2.02037
I0814 23:18:02.499444 11101 solver.cpp:334]     Train net output #0: loss = 2.28747 (* 1 = 2.28747 loss)
I0814 23:18:02.499449 11101 sgd_solver.cpp:136] Iteration 79700, lr = 0.0750938, m = 0.9
I0814 23:18:17.096086 11101 solver.cpp:312] Iteration 79800 (6.85107 iter/s, 14.5963s/100 iter), loss = 1.97135
I0814 23:18:17.096151 11101 solver.cpp:334]     Train net output #0: loss = 1.69378 (* 1 = 1.69378 loss)
I0814 23:18:17.096158 11101 sgd_solver.cpp:136] Iteration 79800, lr = 0.0750625, m = 0.9
I0814 23:18:31.813819 11101 solver.cpp:312] Iteration 79900 (6.79472 iter/s, 14.7173s/100 iter), loss = 2.61941
I0814 23:18:31.813848 11101 solver.cpp:334]     Train net output #0: loss = 2.62268 (* 1 = 2.62268 loss)
I0814 23:18:31.813854 11101 sgd_solver.cpp:136] Iteration 79900, lr = 0.0750313, m = 0.9
I0814 23:18:46.686714 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_80000.caffemodel
I0814 23:18:46.723348 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_80000.solverstate
I0814 23:18:46.727887 11101 solver.cpp:509] Iteration 80000, Testing net (#0)
I0814 23:19:07.202014 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.407999
I0814 23:19:07.202071 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.670293
I0814 23:19:07.202077 11101 solver.cpp:594]     Test net output #2: loss = 2.73182 (* 1 = 2.73182 loss)
I0814 23:19:07.202096 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.4736s
I0814 23:19:07.348709 11101 solver.cpp:312] Iteration 80000 (2.81421 iter/s, 35.5339s/100 iter), loss = 1.75936
I0814 23:19:07.348737 11101 solver.cpp:334]     Train net output #0: loss = 1.92509 (* 1 = 1.92509 loss)
I0814 23:19:07.348742 11101 sgd_solver.cpp:136] Iteration 80000, lr = 0.075, m = 0.9
I0814 23:19:21.944517 11101 solver.cpp:312] Iteration 80100 (6.85148 iter/s, 14.5954s/100 iter), loss = 2.14858
I0814 23:19:21.944540 11101 solver.cpp:334]     Train net output #0: loss = 2.26864 (* 1 = 2.26864 loss)
I0814 23:19:21.944545 11101 sgd_solver.cpp:136] Iteration 80100, lr = 0.0749687, m = 0.9
I0814 23:19:36.604941 11101 solver.cpp:312] Iteration 80200 (6.82128 iter/s, 14.66s/100 iter), loss = 2.0029
I0814 23:19:36.604969 11101 solver.cpp:334]     Train net output #0: loss = 2.16939 (* 1 = 2.16939 loss)
I0814 23:19:36.604975 11101 sgd_solver.cpp:136] Iteration 80200, lr = 0.0749375, m = 0.9
I0814 23:19:51.269642 11101 solver.cpp:312] Iteration 80300 (6.81929 iter/s, 14.6643s/100 iter), loss = 1.65919
I0814 23:19:51.269707 11101 solver.cpp:334]     Train net output #0: loss = 1.57317 (* 1 = 1.57317 loss)
I0814 23:19:51.269714 11101 sgd_solver.cpp:136] Iteration 80300, lr = 0.0749063, m = 0.9
I0814 23:20:05.865932 11101 solver.cpp:312] Iteration 80400 (6.85125 iter/s, 14.5959s/100 iter), loss = 1.77679
I0814 23:20:05.865960 11101 solver.cpp:334]     Train net output #0: loss = 1.66341 (* 1 = 1.66341 loss)
I0814 23:20:05.865967 11101 sgd_solver.cpp:136] Iteration 80400, lr = 0.074875, m = 0.9
I0814 23:20:20.358657 11101 solver.cpp:312] Iteration 80500 (6.90021 iter/s, 14.4923s/100 iter), loss = 2.08055
I0814 23:20:20.358708 11101 solver.cpp:334]     Train net output #0: loss = 1.94377 (* 1 = 1.94377 loss)
I0814 23:20:20.358722 11101 sgd_solver.cpp:136] Iteration 80500, lr = 0.0748438, m = 0.9
I0814 23:20:34.931869 11101 solver.cpp:312] Iteration 80600 (6.8621 iter/s, 14.5728s/100 iter), loss = 2.74311
I0814 23:20:34.931934 11101 solver.cpp:334]     Train net output #0: loss = 2.98861 (* 1 = 2.98861 loss)
I0814 23:20:34.931941 11101 sgd_solver.cpp:136] Iteration 80600, lr = 0.0748125, m = 0.9
I0814 23:20:49.836141 11101 solver.cpp:312] Iteration 80700 (6.70968 iter/s, 14.9038s/100 iter), loss = 2.03652
I0814 23:20:49.836205 11101 solver.cpp:334]     Train net output #0: loss = 2.33285 (* 1 = 2.33285 loss)
I0814 23:20:49.836223 11101 sgd_solver.cpp:136] Iteration 80700, lr = 0.0747813, m = 0.9
I0814 23:21:04.501724 11101 solver.cpp:312] Iteration 80800 (6.81888 iter/s, 14.6652s/100 iter), loss = 1.92412
I0814 23:21:04.501750 11101 solver.cpp:334]     Train net output #0: loss = 2.19237 (* 1 = 2.19237 loss)
I0814 23:21:04.501754 11101 sgd_solver.cpp:136] Iteration 80800, lr = 0.07475, m = 0.9
I0814 23:21:18.956048 11101 solver.cpp:312] Iteration 80900 (6.91854 iter/s, 14.4539s/100 iter), loss = 2.00891
I0814 23:21:18.956149 11101 solver.cpp:334]     Train net output #0: loss = 2.14725 (* 1 = 2.14725 loss)
I0814 23:21:18.956171 11101 sgd_solver.cpp:136] Iteration 80900, lr = 0.0747188, m = 0.9
I0814 23:21:33.637159 11101 solver.cpp:312] Iteration 81000 (6.81167 iter/s, 14.6807s/100 iter), loss = 2.32837
I0814 23:21:33.637224 11101 solver.cpp:334]     Train net output #0: loss = 2.09818 (* 1 = 2.09818 loss)
I0814 23:21:33.637241 11101 sgd_solver.cpp:136] Iteration 81000, lr = 0.0746875, m = 0.9
I0814 23:21:48.119887 11101 solver.cpp:312] Iteration 81100 (6.90497 iter/s, 14.4823s/100 iter), loss = 1.98575
I0814 23:21:48.119940 11101 solver.cpp:334]     Train net output #0: loss = 1.82402 (* 1 = 1.82402 loss)
I0814 23:21:48.119953 11101 sgd_solver.cpp:136] Iteration 81100, lr = 0.0746562, m = 0.9
I0814 23:22:02.531113 11101 solver.cpp:312] Iteration 81200 (6.93923 iter/s, 14.4108s/100 iter), loss = 1.98553
I0814 23:22:02.531172 11101 solver.cpp:334]     Train net output #0: loss = 2.10875 (* 1 = 2.10875 loss)
I0814 23:22:02.531177 11101 sgd_solver.cpp:136] Iteration 81200, lr = 0.074625, m = 0.9
I0814 23:22:16.838152 11101 solver.cpp:312] Iteration 81300 (6.98976 iter/s, 14.3066s/100 iter), loss = 2.19376
I0814 23:22:16.838179 11101 solver.cpp:334]     Train net output #0: loss = 1.78725 (* 1 = 1.78725 loss)
I0814 23:22:16.838183 11101 sgd_solver.cpp:136] Iteration 81300, lr = 0.0745937, m = 0.9
I0814 23:22:31.391662 11101 solver.cpp:312] Iteration 81400 (6.87139 iter/s, 14.5531s/100 iter), loss = 1.94496
I0814 23:22:31.391690 11101 solver.cpp:334]     Train net output #0: loss = 1.99546 (* 1 = 1.99546 loss)
I0814 23:22:31.391695 11101 sgd_solver.cpp:136] Iteration 81400, lr = 0.0745625, m = 0.9
I0814 23:22:45.980332 11101 solver.cpp:312] Iteration 81500 (6.85483 iter/s, 14.5883s/100 iter), loss = 2.19957
I0814 23:22:45.980393 11101 solver.cpp:334]     Train net output #0: loss = 2.12013 (* 1 = 2.12013 loss)
I0814 23:22:45.980401 11101 sgd_solver.cpp:136] Iteration 81500, lr = 0.0745312, m = 0.9
I0814 23:23:00.603765 11101 solver.cpp:312] Iteration 81600 (6.83853 iter/s, 14.623s/100 iter), loss = 2.01355
I0814 23:23:00.603941 11101 solver.cpp:334]     Train net output #0: loss = 1.9732 (* 1 = 1.9732 loss)
I0814 23:23:00.604029 11101 sgd_solver.cpp:136] Iteration 81600, lr = 0.0745, m = 0.9
I0814 23:23:15.193013 11101 solver.cpp:312] Iteration 81700 (6.85456 iter/s, 14.5888s/100 iter), loss = 1.98188
I0814 23:23:15.193044 11101 solver.cpp:334]     Train net output #0: loss = 1.88617 (* 1 = 1.88617 loss)
I0814 23:23:15.193050 11101 sgd_solver.cpp:136] Iteration 81700, lr = 0.0744688, m = 0.9
I0814 23:23:29.833657 11101 solver.cpp:312] Iteration 81800 (6.83049 iter/s, 14.6402s/100 iter), loss = 2.50927
I0814 23:23:29.833858 11101 solver.cpp:334]     Train net output #0: loss = 2.18502 (* 1 = 2.18502 loss)
I0814 23:23:29.833943 11101 sgd_solver.cpp:136] Iteration 81800, lr = 0.0744375, m = 0.9
I0814 23:23:44.290048 11101 solver.cpp:312] Iteration 81900 (6.91755 iter/s, 14.456s/100 iter), loss = 2.01785
I0814 23:23:44.290122 11101 solver.cpp:334]     Train net output #0: loss = 2.29113 (* 1 = 2.29113 loss)
I0814 23:23:44.290140 11101 sgd_solver.cpp:136] Iteration 81900, lr = 0.0744063, m = 0.9
I0814 23:23:58.631889 11101 solver.cpp:509] Iteration 82000, Testing net (#0)
I0814 23:24:08.572532 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 23:24:19.519817 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.398705
I0814 23:24:19.519840 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.653235
I0814 23:24:19.519848 11101 solver.cpp:594]     Test net output #2: loss = 2.81265 (* 1 = 2.81265 loss)
I0814 23:24:19.519896 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8874s
I0814 23:24:19.666529 11101 solver.cpp:312] Iteration 82000 (2.82682 iter/s, 35.3755s/100 iter), loss = 2.06662
I0814 23:24:19.666558 11101 solver.cpp:334]     Train net output #0: loss = 1.83867 (* 1 = 1.83867 loss)
I0814 23:24:19.666563 11101 sgd_solver.cpp:136] Iteration 82000, lr = 0.074375, m = 0.9
I0814 23:24:34.180964 11101 solver.cpp:312] Iteration 82100 (6.88989 iter/s, 14.514s/100 iter), loss = 2.06244
I0814 23:24:34.181165 11101 solver.cpp:334]     Train net output #0: loss = 2.09181 (* 1 = 2.09181 loss)
I0814 23:24:34.181272 11101 sgd_solver.cpp:136] Iteration 82100, lr = 0.0743438, m = 0.9
I0814 23:24:48.921177 11101 solver.cpp:312] Iteration 82200 (6.78436 iter/s, 14.7398s/100 iter), loss = 2.17473
I0814 23:24:48.921241 11101 solver.cpp:334]     Train net output #0: loss = 1.76569 (* 1 = 1.76569 loss)
I0814 23:24:48.921248 11101 sgd_solver.cpp:136] Iteration 82200, lr = 0.0743125, m = 0.9
I0814 23:25:03.436336 11101 solver.cpp:312] Iteration 82300 (6.88955 iter/s, 14.5147s/100 iter), loss = 2.1155
I0814 23:25:03.436365 11101 solver.cpp:334]     Train net output #0: loss = 1.95159 (* 1 = 1.95159 loss)
I0814 23:25:03.436372 11101 sgd_solver.cpp:136] Iteration 82300, lr = 0.0742813, m = 0.9
I0814 23:25:17.955891 11101 solver.cpp:312] Iteration 82400 (6.88746 iter/s, 14.5191s/100 iter), loss = 1.81521
I0814 23:25:17.955919 11101 solver.cpp:334]     Train net output #0: loss = 2.02348 (* 1 = 2.02348 loss)
I0814 23:25:17.955925 11101 sgd_solver.cpp:136] Iteration 82400, lr = 0.07425, m = 0.9
I0814 23:25:32.329368 11101 solver.cpp:312] Iteration 82500 (6.95746 iter/s, 14.3731s/100 iter), loss = 2.0464
I0814 23:25:32.329424 11101 solver.cpp:334]     Train net output #0: loss = 1.74388 (* 1 = 1.74388 loss)
I0814 23:25:32.329429 11101 sgd_solver.cpp:136] Iteration 82500, lr = 0.0742188, m = 0.9
I0814 23:25:46.906901 11101 solver.cpp:312] Iteration 82600 (6.86007 iter/s, 14.5771s/100 iter), loss = 2.27732
I0814 23:25:46.906955 11101 solver.cpp:334]     Train net output #0: loss = 2.43625 (* 1 = 2.43625 loss)
I0814 23:25:46.906968 11101 sgd_solver.cpp:136] Iteration 82600, lr = 0.0741875, m = 0.9
I0814 23:26:01.407738 11101 solver.cpp:312] Iteration 82700 (6.89635 iter/s, 14.5004s/100 iter), loss = 2.37994
I0814 23:26:01.407768 11101 solver.cpp:334]     Train net output #0: loss = 2.65058 (* 1 = 2.65058 loss)
I0814 23:26:01.407773 11101 sgd_solver.cpp:136] Iteration 82700, lr = 0.0741562, m = 0.9
I0814 23:26:15.891708 11101 solver.cpp:312] Iteration 82800 (6.90438 iter/s, 14.4836s/100 iter), loss = 2.0546
I0814 23:26:15.891768 11101 solver.cpp:334]     Train net output #0: loss = 2.20822 (* 1 = 2.20822 loss)
I0814 23:26:15.891774 11101 sgd_solver.cpp:136] Iteration 82800, lr = 0.074125, m = 0.9
I0814 23:26:30.559723 11101 solver.cpp:312] Iteration 82900 (6.81775 iter/s, 14.6676s/100 iter), loss = 2.01509
I0814 23:26:30.559785 11101 solver.cpp:334]     Train net output #0: loss = 2.12192 (* 1 = 2.12192 loss)
I0814 23:26:30.559803 11101 sgd_solver.cpp:136] Iteration 82900, lr = 0.0740938, m = 0.9
I0814 23:26:45.314894 11101 solver.cpp:312] Iteration 83000 (6.77748 iter/s, 14.7548s/100 iter), loss = 1.93006
I0814 23:26:45.314926 11101 solver.cpp:334]     Train net output #0: loss = 1.89989 (* 1 = 1.89989 loss)
I0814 23:26:45.314932 11101 sgd_solver.cpp:136] Iteration 83000, lr = 0.0740625, m = 0.9
I0814 23:26:59.892755 11101 solver.cpp:312] Iteration 83100 (6.85991 iter/s, 14.5774s/100 iter), loss = 2.53522
I0814 23:26:59.893018 11101 solver.cpp:334]     Train net output #0: loss = 2.82256 (* 1 = 2.82256 loss)
I0814 23:26:59.893131 11101 sgd_solver.cpp:136] Iteration 83100, lr = 0.0740312, m = 0.9
I0814 23:27:14.584118 11101 solver.cpp:312] Iteration 83200 (6.80691 iter/s, 14.6909s/100 iter), loss = 1.96846
I0814 23:27:14.584300 11101 solver.cpp:334]     Train net output #0: loss = 2.58273 (* 1 = 2.58273 loss)
I0814 23:27:14.584386 11101 sgd_solver.cpp:136] Iteration 83200, lr = 0.074, m = 0.9
I0814 23:27:29.152508 11101 solver.cpp:312] Iteration 83300 (6.86437 iter/s, 14.568s/100 iter), loss = 1.92657
I0814 23:27:29.152534 11101 solver.cpp:334]     Train net output #0: loss = 2.00336 (* 1 = 2.00336 loss)
I0814 23:27:29.152540 11101 sgd_solver.cpp:136] Iteration 83300, lr = 0.0739688, m = 0.9
I0814 23:27:44.235397 11101 solver.cpp:312] Iteration 83400 (6.63022 iter/s, 15.0825s/100 iter), loss = 2.60624
I0814 23:27:44.235491 11101 solver.cpp:334]     Train net output #0: loss = 2.78285 (* 1 = 2.78285 loss)
I0814 23:27:44.235509 11101 sgd_solver.cpp:136] Iteration 83400, lr = 0.0739375, m = 0.9
I0814 23:27:58.988629 11101 solver.cpp:312] Iteration 83500 (6.77837 iter/s, 14.7528s/100 iter), loss = 2.12509
I0814 23:27:58.988658 11101 solver.cpp:334]     Train net output #0: loss = 2.44996 (* 1 = 2.44996 loss)
I0814 23:27:58.988662 11101 sgd_solver.cpp:136] Iteration 83500, lr = 0.0739063, m = 0.9
I0814 23:28:13.527896 11101 solver.cpp:312] Iteration 83600 (6.87812 iter/s, 14.5389s/100 iter), loss = 2.5817
I0814 23:28:13.527930 11101 solver.cpp:334]     Train net output #0: loss = 2.32873 (* 1 = 2.32873 loss)
I0814 23:28:13.527935 11101 sgd_solver.cpp:136] Iteration 83600, lr = 0.073875, m = 0.9
I0814 23:28:28.255920 11101 solver.cpp:312] Iteration 83700 (6.78997 iter/s, 14.7276s/100 iter), loss = 2.35606
I0814 23:28:28.256016 11101 solver.cpp:334]     Train net output #0: loss = 2.61112 (* 1 = 2.61112 loss)
I0814 23:28:28.256034 11101 sgd_solver.cpp:136] Iteration 83700, lr = 0.0738438, m = 0.9
I0814 23:28:42.712604 11101 solver.cpp:312] Iteration 83800 (6.91741 iter/s, 14.4563s/100 iter), loss = 2.19351
I0814 23:28:42.712827 11101 solver.cpp:334]     Train net output #0: loss = 2.50962 (* 1 = 2.50962 loss)
I0814 23:28:42.712939 11101 sgd_solver.cpp:136] Iteration 83800, lr = 0.0738125, m = 0.9
I0814 23:28:57.567225 11101 solver.cpp:312] Iteration 83900 (6.7321 iter/s, 14.8542s/100 iter), loss = 2.10205
I0814 23:28:57.567255 11101 solver.cpp:334]     Train net output #0: loss = 2.04709 (* 1 = 2.04709 loss)
I0814 23:28:57.567261 11101 sgd_solver.cpp:136] Iteration 83900, lr = 0.0737813, m = 0.9
I0814 23:29:12.217125 11101 solver.cpp:509] Iteration 84000, Testing net (#0)
I0814 23:29:33.007833 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.388176
I0814 23:29:33.007858 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.643235
I0814 23:29:33.007863 11101 solver.cpp:594]     Test net output #2: loss = 2.90868 (* 1 = 2.90868 loss)
I0814 23:29:33.007892 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7902s
I0814 23:29:33.172780 11101 solver.cpp:312] Iteration 84000 (2.80863 iter/s, 35.6046s/100 iter), loss = 2.1613
I0814 23:29:33.172998 11101 solver.cpp:334]     Train net output #0: loss = 2.32092 (* 1 = 2.32092 loss)
I0814 23:29:33.173106 11101 sgd_solver.cpp:136] Iteration 84000, lr = 0.07375, m = 0.9
I0814 23:29:47.948037 11101 solver.cpp:312] Iteration 84100 (6.76826 iter/s, 14.7748s/100 iter), loss = 2.49415
I0814 23:29:47.949179 11101 solver.cpp:334]     Train net output #0: loss = 2.47155 (* 1 = 2.47155 loss)
I0814 23:29:47.949188 11101 sgd_solver.cpp:136] Iteration 84100, lr = 0.0737187, m = 0.9
I0814 23:30:03.030902 11101 solver.cpp:312] Iteration 84200 (6.63023 iter/s, 15.0824s/100 iter), loss = 2.19465
I0814 23:30:03.030926 11101 solver.cpp:334]     Train net output #0: loss = 2.55879 (* 1 = 2.55879 loss)
I0814 23:30:03.030930 11101 sgd_solver.cpp:136] Iteration 84200, lr = 0.0736875, m = 0.9
I0814 23:30:17.481513 11101 solver.cpp:312] Iteration 84300 (6.92032 iter/s, 14.4502s/100 iter), loss = 2.32274
I0814 23:30:17.481539 11101 solver.cpp:334]     Train net output #0: loss = 2.27196 (* 1 = 2.27196 loss)
I0814 23:30:17.481545 11101 sgd_solver.cpp:136] Iteration 84300, lr = 0.0736563, m = 0.9
I0814 23:30:31.877146 11101 solver.cpp:312] Iteration 84400 (6.94675 iter/s, 14.3952s/100 iter), loss = 2.01154
I0814 23:30:31.877223 11101 solver.cpp:334]     Train net output #0: loss = 1.98769 (* 1 = 1.98769 loss)
I0814 23:30:31.877233 11101 sgd_solver.cpp:136] Iteration 84400, lr = 0.073625, m = 0.9
I0814 23:30:46.612439 11101 solver.cpp:312] Iteration 84500 (6.78662 iter/s, 14.7349s/100 iter), loss = 2.51437
I0814 23:30:46.612468 11101 solver.cpp:334]     Train net output #0: loss = 2.21196 (* 1 = 2.21196 loss)
I0814 23:30:46.612473 11101 sgd_solver.cpp:136] Iteration 84500, lr = 0.0735938, m = 0.9
I0814 23:31:01.244118 11101 solver.cpp:312] Iteration 84600 (6.83468 iter/s, 14.6313s/100 iter), loss = 1.97919
I0814 23:31:01.244153 11101 solver.cpp:334]     Train net output #0: loss = 1.6012 (* 1 = 1.6012 loss)
I0814 23:31:01.244158 11101 sgd_solver.cpp:136] Iteration 84600, lr = 0.0735625, m = 0.9
I0814 23:31:15.669159 11101 solver.cpp:312] Iteration 84700 (6.93258 iter/s, 14.4246s/100 iter), loss = 2.05974
I0814 23:31:15.669250 11101 solver.cpp:334]     Train net output #0: loss = 1.84405 (* 1 = 1.84405 loss)
I0814 23:31:15.669268 11101 sgd_solver.cpp:136] Iteration 84700, lr = 0.0735312, m = 0.9
I0814 23:31:30.121737 11101 solver.cpp:312] Iteration 84800 (6.91938 iter/s, 14.4522s/100 iter), loss = 2.15418
I0814 23:31:30.121764 11101 solver.cpp:334]     Train net output #0: loss = 1.54577 (* 1 = 1.54577 loss)
I0814 23:31:30.121770 11101 sgd_solver.cpp:136] Iteration 84800, lr = 0.0735, m = 0.9
I0814 23:31:44.524310 11101 solver.cpp:312] Iteration 84900 (6.9434 iter/s, 14.4022s/100 iter), loss = 2.31707
I0814 23:31:44.524338 11101 solver.cpp:334]     Train net output #0: loss = 2.52493 (* 1 = 2.52493 loss)
I0814 23:31:44.524343 11101 sgd_solver.cpp:136] Iteration 84900, lr = 0.0734688, m = 0.9
I0814 23:31:59.135694 11101 solver.cpp:312] Iteration 85000 (6.84417 iter/s, 14.611s/100 iter), loss = 2.07613
I0814 23:31:59.135793 11101 solver.cpp:334]     Train net output #0: loss = 1.87751 (* 1 = 1.87751 loss)
I0814 23:31:59.135808 11101 sgd_solver.cpp:136] Iteration 85000, lr = 0.0734375, m = 0.9
I0814 23:32:13.640283 11101 solver.cpp:312] Iteration 85100 (6.89456 iter/s, 14.5042s/100 iter), loss = 2.18915
I0814 23:32:13.640313 11101 solver.cpp:334]     Train net output #0: loss = 2.15182 (* 1 = 2.15182 loss)
I0814 23:32:13.640319 11101 sgd_solver.cpp:136] Iteration 85100, lr = 0.0734062, m = 0.9
I0814 23:32:28.595552 11101 solver.cpp:312] Iteration 85200 (6.68679 iter/s, 14.9549s/100 iter), loss = 1.75909
I0814 23:32:28.595580 11101 solver.cpp:334]     Train net output #0: loss = 1.83803 (* 1 = 1.83803 loss)
I0814 23:32:28.595584 11101 sgd_solver.cpp:136] Iteration 85200, lr = 0.073375, m = 0.9
I0814 23:32:43.105777 11101 solver.cpp:312] Iteration 85300 (6.89189 iter/s, 14.5098s/100 iter), loss = 2.27288
I0814 23:32:43.105865 11101 solver.cpp:334]     Train net output #0: loss = 1.96288 (* 1 = 1.96288 loss)
I0814 23:32:43.105890 11101 sgd_solver.cpp:136] Iteration 85300, lr = 0.0733438, m = 0.9
I0814 23:32:57.662111 11101 solver.cpp:312] Iteration 85400 (6.87005 iter/s, 14.5559s/100 iter), loss = 2.0214
I0814 23:32:57.662135 11101 solver.cpp:334]     Train net output #0: loss = 2.11245 (* 1 = 2.11245 loss)
I0814 23:32:57.662139 11101 sgd_solver.cpp:136] Iteration 85400, lr = 0.0733125, m = 0.9
I0814 23:33:12.447798 11101 solver.cpp:312] Iteration 85500 (6.76349 iter/s, 14.7853s/100 iter), loss = 2.18069
I0814 23:33:12.447850 11101 solver.cpp:334]     Train net output #0: loss = 2.3177 (* 1 = 2.3177 loss)
I0814 23:33:12.447865 11101 sgd_solver.cpp:136] Iteration 85500, lr = 0.0732813, m = 0.9
I0814 23:33:27.362632 11101 solver.cpp:312] Iteration 85600 (6.70492 iter/s, 14.9144s/100 iter), loss = 2.08722
I0814 23:33:27.362691 11101 solver.cpp:334]     Train net output #0: loss = 2.16784 (* 1 = 2.16784 loss)
I0814 23:33:27.362699 11101 sgd_solver.cpp:136] Iteration 85600, lr = 0.07325, m = 0.9
I0814 23:33:41.839736 11101 solver.cpp:312] Iteration 85700 (6.90765 iter/s, 14.4767s/100 iter), loss = 1.8622
I0814 23:33:41.839766 11101 solver.cpp:334]     Train net output #0: loss = 2.23975 (* 1 = 2.23975 loss)
I0814 23:33:41.839772 11101 sgd_solver.cpp:136] Iteration 85700, lr = 0.0732188, m = 0.9
I0814 23:33:56.517887 11101 solver.cpp:312] Iteration 85800 (6.81304 iter/s, 14.6777s/100 iter), loss = 1.9366
I0814 23:33:56.517916 11101 solver.cpp:334]     Train net output #0: loss = 2.0886 (* 1 = 2.0886 loss)
I0814 23:33:56.517922 11101 sgd_solver.cpp:136] Iteration 85800, lr = 0.0731875, m = 0.9
I0814 23:34:10.933358 11101 solver.cpp:312] Iteration 85900 (6.93719 iter/s, 14.4151s/100 iter), loss = 2.26327
I0814 23:34:10.933442 11101 solver.cpp:334]     Train net output #0: loss = 2.26998 (* 1 = 2.26998 loss)
I0814 23:34:10.933449 11101 sgd_solver.cpp:136] Iteration 85900, lr = 0.0731563, m = 0.9
I0814 23:34:25.321588 11101 solver.cpp:509] Iteration 86000, Testing net (#0)
I0814 23:34:46.076006 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.382059
I0814 23:34:46.076051 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.643647
I0814 23:34:46.076057 11101 solver.cpp:594]     Test net output #2: loss = 2.87802 (* 1 = 2.87802 loss)
I0814 23:34:46.076074 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7539s
I0814 23:34:46.227583 11101 solver.cpp:312] Iteration 86000 (2.8334 iter/s, 35.2932s/100 iter), loss = 1.92705
I0814 23:34:46.227610 11101 solver.cpp:334]     Train net output #0: loss = 2.2922 (* 1 = 2.2922 loss)
I0814 23:34:46.227617 11101 sgd_solver.cpp:136] Iteration 86000, lr = 0.073125, m = 0.9
I0814 23:35:00.974803 11101 solver.cpp:312] Iteration 86100 (6.78113 iter/s, 14.7468s/100 iter), loss = 2.33242
I0814 23:35:00.974828 11101 solver.cpp:334]     Train net output #0: loss = 2.23689 (* 1 = 2.23689 loss)
I0814 23:35:00.974833 11101 sgd_solver.cpp:136] Iteration 86100, lr = 0.0730937, m = 0.9
I0814 23:35:15.449925 11101 solver.cpp:312] Iteration 86200 (6.9086 iter/s, 14.4747s/100 iter), loss = 2.12083
I0814 23:35:15.449995 11101 solver.cpp:334]     Train net output #0: loss = 2.03122 (* 1 = 2.03122 loss)
I0814 23:35:15.450016 11101 sgd_solver.cpp:136] Iteration 86200, lr = 0.0730625, m = 0.9
I0814 23:35:30.216507 11101 solver.cpp:312] Iteration 86300 (6.77224 iter/s, 14.7662s/100 iter), loss = 2.13269
I0814 23:35:30.216614 11101 solver.cpp:334]     Train net output #0: loss = 2.26889 (* 1 = 2.26889 loss)
I0814 23:35:30.216632 11101 sgd_solver.cpp:136] Iteration 86300, lr = 0.0730312, m = 0.9
I0814 23:35:44.843205 11101 solver.cpp:312] Iteration 86400 (6.83701 iter/s, 14.6263s/100 iter), loss = 2.26424
I0814 23:35:44.843235 11101 solver.cpp:334]     Train net output #0: loss = 2.11306 (* 1 = 2.11306 loss)
I0814 23:35:44.843241 11101 sgd_solver.cpp:136] Iteration 86400, lr = 0.073, m = 0.9
I0814 23:35:59.722375 11101 solver.cpp:312] Iteration 86500 (6.72099 iter/s, 14.8788s/100 iter), loss = 2.0874
I0814 23:35:59.722425 11101 solver.cpp:334]     Train net output #0: loss = 1.89483 (* 1 = 1.89483 loss)
I0814 23:35:59.722439 11101 sgd_solver.cpp:136] Iteration 86500, lr = 0.0729688, m = 0.9
I0814 23:36:14.581346 11101 solver.cpp:312] Iteration 86600 (6.73013 iter/s, 14.8585s/100 iter), loss = 2.35438
I0814 23:36:14.581413 11101 solver.cpp:334]     Train net output #0: loss = 2.21067 (* 1 = 2.21067 loss)
I0814 23:36:14.581421 11101 sgd_solver.cpp:136] Iteration 86600, lr = 0.0729375, m = 0.9
I0814 23:36:29.007382 11101 solver.cpp:312] Iteration 86700 (6.93211 iter/s, 14.4256s/100 iter), loss = 1.90577
I0814 23:36:29.007411 11101 solver.cpp:334]     Train net output #0: loss = 1.7821 (* 1 = 1.7821 loss)
I0814 23:36:29.007418 11101 sgd_solver.cpp:136] Iteration 86700, lr = 0.0729062, m = 0.9
I0814 23:36:43.391679 11101 solver.cpp:312] Iteration 86800 (6.95222 iter/s, 14.3839s/100 iter), loss = 2.18518
I0814 23:36:43.391701 11101 solver.cpp:334]     Train net output #0: loss = 2.24965 (* 1 = 2.24965 loss)
I0814 23:36:43.391741 11101 sgd_solver.cpp:136] Iteration 86800, lr = 0.072875, m = 0.9
I0814 23:36:57.872907 11101 solver.cpp:312] Iteration 86900 (6.90568 iter/s, 14.4808s/100 iter), loss = 2.01518
I0814 23:36:57.873001 11101 solver.cpp:334]     Train net output #0: loss = 1.75672 (* 1 = 1.75672 loss)
I0814 23:36:57.873018 11101 sgd_solver.cpp:136] Iteration 86900, lr = 0.0728438, m = 0.9
I0814 23:37:12.181205 11101 solver.cpp:312] Iteration 87000 (6.98915 iter/s, 14.3079s/100 iter), loss = 1.83354
I0814 23:37:12.181257 11101 solver.cpp:334]     Train net output #0: loss = 1.94781 (* 1 = 1.94781 loss)
I0814 23:37:12.181269 11101 sgd_solver.cpp:136] Iteration 87000, lr = 0.0728125, m = 0.9
I0814 23:37:26.920991 11101 solver.cpp:312] Iteration 87100 (6.78455 iter/s, 14.7394s/100 iter), loss = 1.94618
I0814 23:37:26.921018 11101 solver.cpp:334]     Train net output #0: loss = 1.98453 (* 1 = 1.98453 loss)
I0814 23:37:26.921021 11101 sgd_solver.cpp:136] Iteration 87100, lr = 0.0727813, m = 0.9
I0814 23:37:41.774595 11101 solver.cpp:312] Iteration 87200 (6.73256 iter/s, 14.8532s/100 iter), loss = 2.10171
I0814 23:37:41.774660 11101 solver.cpp:334]     Train net output #0: loss = 2.01126 (* 1 = 2.01126 loss)
I0814 23:37:41.774667 11101 sgd_solver.cpp:136] Iteration 87200, lr = 0.07275, m = 0.9
I0814 23:37:56.225291 11101 solver.cpp:312] Iteration 87300 (6.92028 iter/s, 14.4503s/100 iter), loss = 2.56287
I0814 23:37:56.225319 11101 solver.cpp:334]     Train net output #0: loss = 2.80808 (* 1 = 2.80808 loss)
I0814 23:37:56.225325 11101 sgd_solver.cpp:136] Iteration 87300, lr = 0.0727188, m = 0.9
I0814 23:38:10.784782 11101 solver.cpp:312] Iteration 87400 (6.86857 iter/s, 14.5591s/100 iter), loss = 2.20323
I0814 23:38:10.784811 11101 solver.cpp:334]     Train net output #0: loss = 2.35872 (* 1 = 2.35872 loss)
I0814 23:38:10.784817 11101 sgd_solver.cpp:136] Iteration 87400, lr = 0.0726875, m = 0.9
I0814 23:38:25.621855 11101 solver.cpp:312] Iteration 87500 (6.74006 iter/s, 14.8367s/100 iter), loss = 2.2339
I0814 23:38:25.621947 11101 solver.cpp:334]     Train net output #0: loss = 2.50621 (* 1 = 2.50621 loss)
I0814 23:38:25.621964 11101 sgd_solver.cpp:136] Iteration 87500, lr = 0.0726563, m = 0.9
I0814 23:38:40.073474 11101 solver.cpp:312] Iteration 87600 (6.91984 iter/s, 14.4512s/100 iter), loss = 1.82572
I0814 23:38:40.073496 11101 solver.cpp:334]     Train net output #0: loss = 2.02241 (* 1 = 2.02241 loss)
I0814 23:38:40.073500 11101 sgd_solver.cpp:136] Iteration 87600, lr = 0.072625, m = 0.9
I0814 23:38:54.352447 11101 solver.cpp:312] Iteration 87700 (7.0035 iter/s, 14.2786s/100 iter), loss = 2.17741
I0814 23:38:54.352473 11101 solver.cpp:334]     Train net output #0: loss = 1.95429 (* 1 = 1.95429 loss)
I0814 23:38:54.352480 11101 sgd_solver.cpp:136] Iteration 87700, lr = 0.0725937, m = 0.9
I0814 23:39:08.967885 11101 solver.cpp:312] Iteration 87800 (6.84227 iter/s, 14.615s/100 iter), loss = 2.68846
I0814 23:39:08.967944 11101 solver.cpp:334]     Train net output #0: loss = 2.77249 (* 1 = 2.77249 loss)
I0814 23:39:08.967979 11101 sgd_solver.cpp:136] Iteration 87800, lr = 0.0725625, m = 0.9
I0814 23:39:23.592897 11101 solver.cpp:312] Iteration 87900 (6.83779 iter/s, 14.6246s/100 iter), loss = 2.26468
I0814 23:39:23.592926 11101 solver.cpp:334]     Train net output #0: loss = 2.10766 (* 1 = 2.10766 loss)
I0814 23:39:23.592931 11101 sgd_solver.cpp:136] Iteration 87900, lr = 0.0725312, m = 0.9
I0814 23:39:37.977320 11101 solver.cpp:509] Iteration 88000, Testing net (#0)
I0814 23:39:40.956311 11099 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 23:39:45.202877 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 23:39:59.062615 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.403823
I0814 23:39:59.062635 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.658587
I0814 23:39:59.062640 11101 solver.cpp:594]     Test net output #2: loss = 2.80824 (* 1 = 2.80824 loss)
I0814 23:39:59.062693 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0848s
I0814 23:39:59.202586 11101 solver.cpp:312] Iteration 88000 (2.8083 iter/s, 35.6087s/100 iter), loss = 1.86245
I0814 23:39:59.202808 11101 solver.cpp:334]     Train net output #0: loss = 1.84973 (* 1 = 1.84973 loss)
I0814 23:39:59.202919 11101 sgd_solver.cpp:136] Iteration 88000, lr = 0.0725, m = 0.9
I0814 23:40:13.984155 11101 solver.cpp:312] Iteration 88100 (6.76537 iter/s, 14.7811s/100 iter), loss = 1.76343
I0814 23:40:13.984227 11101 solver.cpp:334]     Train net output #0: loss = 1.53791 (* 1 = 1.53791 loss)
I0814 23:40:13.984233 11101 sgd_solver.cpp:136] Iteration 88100, lr = 0.0724688, m = 0.9
I0814 23:40:28.560863 11101 solver.cpp:312] Iteration 88200 (6.86045 iter/s, 14.5763s/100 iter), loss = 1.99707
I0814 23:40:28.560927 11101 solver.cpp:334]     Train net output #0: loss = 2.06058 (* 1 = 2.06058 loss)
I0814 23:40:28.560945 11101 sgd_solver.cpp:136] Iteration 88200, lr = 0.0724375, m = 0.9
I0814 23:40:43.034868 11101 solver.cpp:312] Iteration 88300 (6.90913 iter/s, 14.4736s/100 iter), loss = 2.32021
I0814 23:40:43.034890 11101 solver.cpp:334]     Train net output #0: loss = 2.47326 (* 1 = 2.47326 loss)
I0814 23:40:43.034895 11101 sgd_solver.cpp:136] Iteration 88300, lr = 0.0724063, m = 0.9
I0814 23:40:57.361377 11101 solver.cpp:312] Iteration 88400 (6.98027 iter/s, 14.3261s/100 iter), loss = 2.33129
I0814 23:40:57.361634 11101 solver.cpp:334]     Train net output #0: loss = 2.68458 (* 1 = 2.68458 loss)
I0814 23:40:57.361656 11101 sgd_solver.cpp:136] Iteration 88400, lr = 0.072375, m = 0.9
I0814 23:41:11.869060 11101 solver.cpp:312] Iteration 88500 (6.8931 iter/s, 14.5073s/100 iter), loss = 1.77858
I0814 23:41:11.869084 11101 solver.cpp:334]     Train net output #0: loss = 1.83189 (* 1 = 1.83189 loss)
I0814 23:41:11.869091 11101 sgd_solver.cpp:136] Iteration 88500, lr = 0.0723438, m = 0.9
I0814 23:41:26.187214 11101 solver.cpp:312] Iteration 88600 (6.98434 iter/s, 14.3178s/100 iter), loss = 2.23339
I0814 23:41:26.187242 11101 solver.cpp:334]     Train net output #0: loss = 2.30689 (* 1 = 2.30689 loss)
I0814 23:41:26.187247 11101 sgd_solver.cpp:136] Iteration 88600, lr = 0.0723125, m = 0.9
I0814 23:41:40.849040 11101 solver.cpp:312] Iteration 88700 (6.82063 iter/s, 14.6614s/100 iter), loss = 2.49045
I0814 23:41:40.849095 11101 solver.cpp:334]     Train net output #0: loss = 2.80085 (* 1 = 2.80085 loss)
I0814 23:41:40.849102 11101 sgd_solver.cpp:136] Iteration 88700, lr = 0.0722813, m = 0.9
I0814 23:41:55.253319 11101 solver.cpp:312] Iteration 88800 (6.94258 iter/s, 14.4039s/100 iter), loss = 2.02341
I0814 23:41:55.253373 11101 solver.cpp:334]     Train net output #0: loss = 1.92761 (* 1 = 1.92761 loss)
I0814 23:41:55.253387 11101 sgd_solver.cpp:136] Iteration 88800, lr = 0.07225, m = 0.9
I0814 23:42:09.894613 11101 solver.cpp:312] Iteration 88900 (6.83019 iter/s, 14.6409s/100 iter), loss = 2.38618
I0814 23:42:09.894639 11101 solver.cpp:334]     Train net output #0: loss = 1.82899 (* 1 = 1.82899 loss)
I0814 23:42:09.894642 11101 sgd_solver.cpp:136] Iteration 88900, lr = 0.0722188, m = 0.9
I0814 23:42:24.363476 11101 solver.cpp:312] Iteration 89000 (6.91159 iter/s, 14.4685s/100 iter), loss = 2.37639
I0814 23:42:24.363574 11101 solver.cpp:334]     Train net output #0: loss = 2.59545 (* 1 = 2.59545 loss)
I0814 23:42:24.363592 11101 sgd_solver.cpp:136] Iteration 89000, lr = 0.0721875, m = 0.9
I0814 23:42:38.906033 11101 solver.cpp:312] Iteration 89100 (6.87656 iter/s, 14.5421s/100 iter), loss = 1.81316
I0814 23:42:38.906064 11101 solver.cpp:334]     Train net output #0: loss = 2.25423 (* 1 = 2.25423 loss)
I0814 23:42:38.906069 11101 sgd_solver.cpp:136] Iteration 89100, lr = 0.0721563, m = 0.9
I0814 23:42:53.333381 11101 solver.cpp:312] Iteration 89200 (6.93148 iter/s, 14.4269s/100 iter), loss = 2.38536
I0814 23:42:53.333410 11101 solver.cpp:334]     Train net output #0: loss = 2.18377 (* 1 = 2.18377 loss)
I0814 23:42:53.333417 11101 sgd_solver.cpp:136] Iteration 89200, lr = 0.072125, m = 0.9
I0814 23:43:07.641458 11101 solver.cpp:312] Iteration 89300 (6.98926 iter/s, 14.3077s/100 iter), loss = 1.84876
I0814 23:43:07.641561 11101 solver.cpp:334]     Train net output #0: loss = 1.67611 (* 1 = 1.67611 loss)
I0814 23:43:07.641567 11101 sgd_solver.cpp:136] Iteration 89300, lr = 0.0720937, m = 0.9
I0814 23:43:22.235496 11101 solver.cpp:312] Iteration 89400 (6.85231 iter/s, 14.5936s/100 iter), loss = 2.19761
I0814 23:43:22.235525 11101 solver.cpp:334]     Train net output #0: loss = 2.24424 (* 1 = 2.24424 loss)
I0814 23:43:22.235529 11101 sgd_solver.cpp:136] Iteration 89400, lr = 0.0720625, m = 0.9
I0814 23:43:37.199359 11101 solver.cpp:312] Iteration 89500 (6.68295 iter/s, 14.9634s/100 iter), loss = 1.85055
I0814 23:43:37.199424 11101 solver.cpp:334]     Train net output #0: loss = 1.57651 (* 1 = 1.57651 loss)
I0814 23:43:37.199442 11101 sgd_solver.cpp:136] Iteration 89500, lr = 0.0720313, m = 0.9
I0814 23:43:51.671037 11101 solver.cpp:312] Iteration 89600 (6.91024 iter/s, 14.4713s/100 iter), loss = 2.12203
I0814 23:43:51.671097 11101 solver.cpp:334]     Train net output #0: loss = 2.06152 (* 1 = 2.06152 loss)
I0814 23:43:51.671104 11101 sgd_solver.cpp:136] Iteration 89600, lr = 0.072, m = 0.9
I0814 23:44:06.285856 11101 solver.cpp:312] Iteration 89700 (6.84256 iter/s, 14.6144s/100 iter), loss = 2.1839
I0814 23:44:06.285883 11101 solver.cpp:334]     Train net output #0: loss = 2.05932 (* 1 = 2.05932 loss)
I0814 23:44:06.285889 11101 sgd_solver.cpp:136] Iteration 89700, lr = 0.0719687, m = 0.9
I0814 23:44:21.041579 11101 solver.cpp:312] Iteration 89800 (6.77722 iter/s, 14.7553s/100 iter), loss = 2.29997
I0814 23:44:21.041604 11101 solver.cpp:334]     Train net output #0: loss = 2.45498 (* 1 = 2.45498 loss)
I0814 23:44:21.041610 11101 sgd_solver.cpp:136] Iteration 89800, lr = 0.0719375, m = 0.9
I0814 23:44:35.758546 11101 solver.cpp:312] Iteration 89900 (6.79507 iter/s, 14.7166s/100 iter), loss = 2.15971
I0814 23:44:35.758790 11101 solver.cpp:334]     Train net output #0: loss = 2.49933 (* 1 = 2.49933 loss)
I0814 23:44:35.758903 11101 sgd_solver.cpp:136] Iteration 89900, lr = 0.0719063, m = 0.9
I0814 23:44:50.440065 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_90000.caffemodel
I0814 23:44:50.525676 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_90000.solverstate
I0814 23:44:50.532299 11101 solver.cpp:509] Iteration 90000, Testing net (#0)
I0814 23:45:11.656363 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.401117
I0814 23:45:11.656450 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.657646
I0814 23:45:11.656460 11101 solver.cpp:594]     Test net output #2: loss = 2.78748 (* 1 = 2.78748 loss)
I0814 23:45:11.656479 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1236s
I0814 23:45:11.814220 11101 solver.cpp:312] Iteration 90000 (2.77357 iter/s, 36.0547s/100 iter), loss = 2.17819
I0814 23:45:11.814245 11101 solver.cpp:334]     Train net output #0: loss = 2.32058 (* 1 = 2.32058 loss)
I0814 23:45:11.814251 11101 sgd_solver.cpp:136] Iteration 90000, lr = 0.071875, m = 0.9
I0814 23:45:26.323089 11101 solver.cpp:312] Iteration 90100 (6.89253 iter/s, 14.5085s/100 iter), loss = 2.22919
I0814 23:45:26.323117 11101 solver.cpp:334]     Train net output #0: loss = 2.124 (* 1 = 2.124 loss)
I0814 23:45:26.323123 11101 sgd_solver.cpp:136] Iteration 90100, lr = 0.0718438, m = 0.9
I0814 23:45:41.388155 11101 solver.cpp:312] Iteration 90200 (6.63806 iter/s, 15.0646s/100 iter), loss = 2.19914
I0814 23:45:41.388178 11101 solver.cpp:334]     Train net output #0: loss = 2.31936 (* 1 = 2.31936 loss)
I0814 23:45:41.388185 11101 sgd_solver.cpp:136] Iteration 90200, lr = 0.0718125, m = 0.9
I0814 23:45:55.783700 11101 solver.cpp:312] Iteration 90300 (6.94679 iter/s, 14.3951s/100 iter), loss = 1.75865
I0814 23:45:55.783800 11101 solver.cpp:334]     Train net output #0: loss = 1.58803 (* 1 = 1.58803 loss)
I0814 23:45:55.783824 11101 sgd_solver.cpp:136] Iteration 90300, lr = 0.0717813, m = 0.9
I0814 23:46:10.170403 11101 solver.cpp:312] Iteration 90400 (6.95106 iter/s, 14.3863s/100 iter), loss = 2.14448
I0814 23:46:10.170436 11101 solver.cpp:334]     Train net output #0: loss = 2.17876 (* 1 = 2.17876 loss)
I0814 23:46:10.170444 11101 sgd_solver.cpp:136] Iteration 90400, lr = 0.07175, m = 0.9
I0814 23:46:24.574450 11101 solver.cpp:312] Iteration 90500 (6.94269 iter/s, 14.4036s/100 iter), loss = 1.73067
I0814 23:46:24.574477 11101 solver.cpp:334]     Train net output #0: loss = 1.91402 (* 1 = 1.91402 loss)
I0814 23:46:24.574483 11101 sgd_solver.cpp:136] Iteration 90500, lr = 0.0717188, m = 0.9
I0814 23:46:39.098192 11101 solver.cpp:312] Iteration 90600 (6.88547 iter/s, 14.5233s/100 iter), loss = 2.21232
I0814 23:46:39.098781 11101 solver.cpp:334]     Train net output #0: loss = 1.8233 (* 1 = 1.8233 loss)
I0814 23:46:39.098804 11101 sgd_solver.cpp:136] Iteration 90600, lr = 0.0716875, m = 0.9
I0814 23:46:53.683249 11101 solver.cpp:312] Iteration 90700 (6.85652 iter/s, 14.5846s/100 iter), loss = 1.98515
I0814 23:46:53.683471 11101 solver.cpp:334]     Train net output #0: loss = 2.06853 (* 1 = 2.06853 loss)
I0814 23:46:53.683580 11101 sgd_solver.cpp:136] Iteration 90700, lr = 0.0716562, m = 0.9
I0814 23:47:08.205391 11101 solver.cpp:312] Iteration 90800 (6.88623 iter/s, 14.5217s/100 iter), loss = 2.34391
I0814 23:47:08.205420 11101 solver.cpp:334]     Train net output #0: loss = 1.90613 (* 1 = 1.90613 loss)
I0814 23:47:08.205426 11101 sgd_solver.cpp:136] Iteration 90800, lr = 0.071625, m = 0.9
I0814 23:47:22.723098 11101 solver.cpp:312] Iteration 90900 (6.88833 iter/s, 14.5173s/100 iter), loss = 2.5968
I0814 23:47:22.723189 11101 solver.cpp:334]     Train net output #0: loss = 2.55433 (* 1 = 2.55433 loss)
I0814 23:47:22.723206 11101 sgd_solver.cpp:136] Iteration 90900, lr = 0.0715938, m = 0.9
I0814 23:47:37.196166 11101 solver.cpp:312] Iteration 91000 (6.90958 iter/s, 14.4727s/100 iter), loss = 2.39921
I0814 23:47:37.196190 11101 solver.cpp:334]     Train net output #0: loss = 2.62555 (* 1 = 2.62555 loss)
I0814 23:47:37.196197 11101 sgd_solver.cpp:136] Iteration 91000, lr = 0.0715625, m = 0.9
I0814 23:47:51.751189 11101 solver.cpp:312] Iteration 91100 (6.87067 iter/s, 14.5546s/100 iter), loss = 2.14096
I0814 23:47:51.751216 11101 solver.cpp:334]     Train net output #0: loss = 2.29782 (* 1 = 2.29782 loss)
I0814 23:47:51.751224 11101 sgd_solver.cpp:136] Iteration 91100, lr = 0.0715313, m = 0.9
I0814 23:48:06.251407 11101 solver.cpp:312] Iteration 91200 (6.89664 iter/s, 14.4998s/100 iter), loss = 1.9269
I0814 23:48:06.251494 11101 solver.cpp:334]     Train net output #0: loss = 1.66355 (* 1 = 1.66355 loss)
I0814 23:48:06.251513 11101 sgd_solver.cpp:136] Iteration 91200, lr = 0.0715, m = 0.9
I0814 23:48:20.984680 11101 solver.cpp:312] Iteration 91300 (6.78755 iter/s, 14.7329s/100 iter), loss = 1.77563
I0814 23:48:20.984705 11101 solver.cpp:334]     Train net output #0: loss = 1.92327 (* 1 = 1.92327 loss)
I0814 23:48:20.984709 11101 sgd_solver.cpp:136] Iteration 91300, lr = 0.0714687, m = 0.9
I0814 23:48:35.426195 11101 solver.cpp:312] Iteration 91400 (6.92467 iter/s, 14.4411s/100 iter), loss = 1.82954
I0814 23:48:35.426223 11101 solver.cpp:334]     Train net output #0: loss = 1.91296 (* 1 = 1.91296 loss)
I0814 23:48:35.426229 11101 sgd_solver.cpp:136] Iteration 91400, lr = 0.0714375, m = 0.9
I0814 23:48:50.082733 11101 solver.cpp:312] Iteration 91500 (6.82309 iter/s, 14.6561s/100 iter), loss = 1.90081
I0814 23:48:50.082829 11101 solver.cpp:334]     Train net output #0: loss = 1.79696 (* 1 = 1.79696 loss)
I0814 23:48:50.082846 11101 sgd_solver.cpp:136] Iteration 91500, lr = 0.0714063, m = 0.9
I0814 23:49:04.788730 11101 solver.cpp:312] Iteration 91600 (6.80014 iter/s, 14.7056s/100 iter), loss = 1.76372
I0814 23:49:04.788758 11101 solver.cpp:334]     Train net output #0: loss = 1.88049 (* 1 = 1.88049 loss)
I0814 23:49:04.788764 11101 sgd_solver.cpp:136] Iteration 91600, lr = 0.071375, m = 0.9
I0814 23:49:19.551126 11101 solver.cpp:312] Iteration 91700 (6.77416 iter/s, 14.762s/100 iter), loss = 1.90283
I0814 23:49:19.551192 11101 solver.cpp:334]     Train net output #0: loss = 1.78051 (* 1 = 1.78051 loss)
I0814 23:49:19.551213 11101 sgd_solver.cpp:136] Iteration 91700, lr = 0.0713437, m = 0.9
I0814 23:49:34.408740 11101 solver.cpp:312] Iteration 91800 (6.73075 iter/s, 14.8572s/100 iter), loss = 2.07563
I0814 23:49:34.408813 11101 solver.cpp:334]     Train net output #0: loss = 2.30797 (* 1 = 2.30797 loss)
I0814 23:49:34.408820 11101 sgd_solver.cpp:136] Iteration 91800, lr = 0.0713125, m = 0.9
I0814 23:49:48.871757 11101 solver.cpp:312] Iteration 91900 (6.91438 iter/s, 14.4626s/100 iter), loss = 1.93876
I0814 23:49:48.871784 11101 solver.cpp:334]     Train net output #0: loss = 2.10102 (* 1 = 2.10102 loss)
I0814 23:49:48.871788 11101 sgd_solver.cpp:136] Iteration 91900, lr = 0.0712813, m = 0.9
I0814 23:50:03.480213 11101 solver.cpp:509] Iteration 92000, Testing net (#0)
I0814 23:50:24.142539 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.38747
I0814 23:50:24.142585 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.650058
I0814 23:50:24.142593 11101 solver.cpp:594]     Test net output #2: loss = 2.84338 (* 1 = 2.84338 loss)
I0814 23:50:24.142616 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6618s
I0814 23:50:24.298053 11101 solver.cpp:312] Iteration 92000 (2.82284 iter/s, 35.4253s/100 iter), loss = 2.02918
I0814 23:50:24.298101 11101 solver.cpp:334]     Train net output #0: loss = 2.01565 (* 1 = 2.01565 loss)
I0814 23:50:24.298115 11101 sgd_solver.cpp:136] Iteration 92000, lr = 0.07125, m = 0.9
I0814 23:50:38.773468 11101 solver.cpp:312] Iteration 92100 (6.90846 iter/s, 14.475s/100 iter), loss = 2.37966
I0814 23:50:38.773495 11101 solver.cpp:334]     Train net output #0: loss = 1.83654 (* 1 = 1.83654 loss)
I0814 23:50:38.773501 11101 sgd_solver.cpp:136] Iteration 92100, lr = 0.0712188, m = 0.9
I0814 23:50:53.584339 11101 solver.cpp:312] Iteration 92200 (6.75199 iter/s, 14.8105s/100 iter), loss = 2.12318
I0814 23:50:53.584412 11101 solver.cpp:334]     Train net output #0: loss = 1.98866 (* 1 = 1.98866 loss)
I0814 23:50:53.584432 11101 sgd_solver.cpp:136] Iteration 92200, lr = 0.0711875, m = 0.9
I0814 23:51:08.134631 11101 solver.cpp:312] Iteration 92300 (6.87291 iter/s, 14.5499s/100 iter), loss = 2.46868
I0814 23:51:08.134814 11101 solver.cpp:334]     Train net output #0: loss = 2.16923 (* 1 = 2.16923 loss)
I0814 23:51:08.134832 11101 sgd_solver.cpp:136] Iteration 92300, lr = 0.0711563, m = 0.9
I0814 23:51:22.789958 11101 solver.cpp:312] Iteration 92400 (6.82365 iter/s, 14.6549s/100 iter), loss = 1.96663
I0814 23:51:22.789991 11101 solver.cpp:334]     Train net output #0: loss = 1.84463 (* 1 = 1.84463 loss)
I0814 23:51:22.789997 11101 sgd_solver.cpp:136] Iteration 92400, lr = 0.071125, m = 0.9
I0814 23:51:37.623129 11101 solver.cpp:312] Iteration 92500 (6.74184 iter/s, 14.8328s/100 iter), loss = 1.77144
I0814 23:51:37.623155 11101 solver.cpp:334]     Train net output #0: loss = 1.76881 (* 1 = 1.76881 loss)
I0814 23:51:37.623160 11101 sgd_solver.cpp:136] Iteration 92500, lr = 0.0710938, m = 0.9
I0814 23:51:52.523851 11101 solver.cpp:312] Iteration 92600 (6.71127 iter/s, 14.9003s/100 iter), loss = 2.1297
I0814 23:51:52.523955 11101 solver.cpp:334]     Train net output #0: loss = 2.18772 (* 1 = 2.18772 loss)
I0814 23:51:52.523977 11101 sgd_solver.cpp:136] Iteration 92600, lr = 0.0710625, m = 0.9
I0814 23:52:07.091965 11101 solver.cpp:312] Iteration 92700 (6.8645 iter/s, 14.5677s/100 iter), loss = 1.8919
I0814 23:52:07.091992 11101 solver.cpp:334]     Train net output #0: loss = 1.48234 (* 1 = 1.48234 loss)
I0814 23:52:07.092000 11101 sgd_solver.cpp:136] Iteration 92700, lr = 0.0710313, m = 0.9
I0814 23:52:21.607991 11101 solver.cpp:312] Iteration 92800 (6.88913 iter/s, 14.5156s/100 iter), loss = 2.50238
I0814 23:52:21.608017 11101 solver.cpp:334]     Train net output #0: loss = 2.886 (* 1 = 2.886 loss)
I0814 23:52:21.608053 11101 sgd_solver.cpp:136] Iteration 92800, lr = 0.071, m = 0.9
I0814 23:52:36.107403 11101 solver.cpp:312] Iteration 92900 (6.89702 iter/s, 14.499s/100 iter), loss = 2.18549
I0814 23:52:36.107612 11101 solver.cpp:334]     Train net output #0: loss = 2.25086 (* 1 = 2.25086 loss)
I0814 23:52:36.107699 11101 sgd_solver.cpp:136] Iteration 92900, lr = 0.0709687, m = 0.9
I0814 23:52:50.984896 11101 solver.cpp:312] Iteration 93000 (6.72175 iter/s, 14.8771s/100 iter), loss = 2.23057
I0814 23:52:50.984925 11101 solver.cpp:334]     Train net output #0: loss = 2.62337 (* 1 = 2.62337 loss)
I0814 23:52:50.984931 11101 sgd_solver.cpp:136] Iteration 93000, lr = 0.0709375, m = 0.9
I0814 23:53:05.609158 11101 solver.cpp:312] Iteration 93100 (6.83814 iter/s, 14.6239s/100 iter), loss = 2.2207
I0814 23:53:05.609186 11101 solver.cpp:334]     Train net output #0: loss = 2.38272 (* 1 = 2.38272 loss)
I0814 23:53:05.609192 11101 sgd_solver.cpp:136] Iteration 93100, lr = 0.0709062, m = 0.9
I0814 23:53:20.017719 11101 solver.cpp:312] Iteration 93200 (6.94051 iter/s, 14.4082s/100 iter), loss = 1.86449
I0814 23:53:20.017776 11101 solver.cpp:334]     Train net output #0: loss = 1.70759 (* 1 = 1.70759 loss)
I0814 23:53:20.017781 11101 sgd_solver.cpp:136] Iteration 93200, lr = 0.070875, m = 0.9
I0814 23:53:34.719415 11101 solver.cpp:312] Iteration 93300 (6.80213 iter/s, 14.7013s/100 iter), loss = 1.9746
I0814 23:53:34.719440 11101 solver.cpp:334]     Train net output #0: loss = 1.77755 (* 1 = 1.77755 loss)
I0814 23:53:34.719445 11101 sgd_solver.cpp:136] Iteration 93300, lr = 0.0708437, m = 0.9
I0814 23:53:49.253160 11101 solver.cpp:312] Iteration 93400 (6.88073 iter/s, 14.5333s/100 iter), loss = 2.52409
I0814 23:53:49.253187 11101 solver.cpp:334]     Train net output #0: loss = 2.65037 (* 1 = 2.65037 loss)
I0814 23:53:49.253193 11101 sgd_solver.cpp:136] Iteration 93400, lr = 0.0708125, m = 0.9
I0814 23:54:03.954736 11101 solver.cpp:312] Iteration 93500 (6.80218 iter/s, 14.7012s/100 iter), loss = 2.30887
I0814 23:54:03.954833 11101 solver.cpp:334]     Train net output #0: loss = 2.45498 (* 1 = 2.45498 loss)
I0814 23:54:03.954851 11101 sgd_solver.cpp:136] Iteration 93500, lr = 0.0707813, m = 0.9
I0814 23:54:18.645006 11101 solver.cpp:312] Iteration 93600 (6.80742 iter/s, 14.6899s/100 iter), loss = 2.19251
I0814 23:54:18.645031 11101 solver.cpp:334]     Train net output #0: loss = 2.19405 (* 1 = 2.19405 loss)
I0814 23:54:18.645037 11101 sgd_solver.cpp:136] Iteration 93600, lr = 0.07075, m = 0.9
I0814 23:54:33.092386 11101 solver.cpp:312] Iteration 93700 (6.92187 iter/s, 14.447s/100 iter), loss = 2.06706
I0814 23:54:33.092458 11101 solver.cpp:334]     Train net output #0: loss = 1.969 (* 1 = 1.969 loss)
I0814 23:54:33.092478 11101 sgd_solver.cpp:136] Iteration 93700, lr = 0.0707188, m = 0.9
I0814 23:54:47.768457 11101 solver.cpp:312] Iteration 93800 (6.814 iter/s, 14.6757s/100 iter), loss = 1.65894
I0814 23:54:47.768549 11101 solver.cpp:334]     Train net output #0: loss = 1.4865 (* 1 = 1.4865 loss)
I0814 23:54:47.768570 11101 sgd_solver.cpp:136] Iteration 93800, lr = 0.0706875, m = 0.9
I0814 23:55:02.354748 11101 solver.cpp:312] Iteration 93900 (6.85595 iter/s, 14.5859s/100 iter), loss = 1.96379
I0814 23:55:02.354811 11101 solver.cpp:334]     Train net output #0: loss = 2.05295 (* 1 = 2.05295 loss)
I0814 23:55:02.354828 11101 sgd_solver.cpp:136] Iteration 93900, lr = 0.0706563, m = 0.9
I0814 23:55:16.828364 11101 solver.cpp:509] Iteration 94000, Testing net (#0)
I0814 23:55:21.757405 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 23:55:38.057147 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.434706
I0814 23:55:38.057168 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.694058
I0814 23:55:38.057173 11101 solver.cpp:594]     Test net output #2: loss = 2.56347 (* 1 = 2.56347 loss)
I0814 23:55:38.060194 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.2312s
I0814 23:55:38.216496 11101 solver.cpp:312] Iteration 94000 (2.78856 iter/s, 35.8607s/100 iter), loss = 2.33916
I0814 23:55:38.216527 11101 solver.cpp:334]     Train net output #0: loss = 2.96252 (* 1 = 2.96252 loss)
I0814 23:55:38.216533 11101 sgd_solver.cpp:136] Iteration 94000, lr = 0.070625, m = 0.9
I0814 23:55:52.803369 11101 solver.cpp:312] Iteration 94100 (6.85567 iter/s, 14.5865s/100 iter), loss = 2.50242
I0814 23:55:52.803431 11101 solver.cpp:334]     Train net output #0: loss = 2.41478 (* 1 = 2.41478 loss)
I0814 23:55:52.803437 11101 sgd_solver.cpp:136] Iteration 94100, lr = 0.0705938, m = 0.9
I0814 23:56:07.646153 11101 solver.cpp:312] Iteration 94200 (6.73747 iter/s, 14.8424s/100 iter), loss = 2.35857
I0814 23:56:07.646183 11101 solver.cpp:334]     Train net output #0: loss = 2.24281 (* 1 = 2.24281 loss)
I0814 23:56:07.646188 11101 sgd_solver.cpp:136] Iteration 94200, lr = 0.0705625, m = 0.9
I0814 23:56:22.328874 11101 solver.cpp:312] Iteration 94300 (6.81092 iter/s, 14.6823s/100 iter), loss = 2.06902
I0814 23:56:22.328905 11101 solver.cpp:334]     Train net output #0: loss = 2.41374 (* 1 = 2.41374 loss)
I0814 23:56:22.328912 11101 sgd_solver.cpp:136] Iteration 94300, lr = 0.0705312, m = 0.9
I0814 23:56:37.166064 11101 solver.cpp:312] Iteration 94400 (6.74001 iter/s, 14.8368s/100 iter), loss = 2.39759
I0814 23:56:37.166152 11101 solver.cpp:334]     Train net output #0: loss = 2.63792 (* 1 = 2.63792 loss)
I0814 23:56:37.166169 11101 sgd_solver.cpp:136] Iteration 94400, lr = 0.0705, m = 0.9
I0814 23:56:51.833590 11101 solver.cpp:312] Iteration 94500 (6.81798 iter/s, 14.6671s/100 iter), loss = 2.65403
I0814 23:56:51.833616 11101 solver.cpp:334]     Train net output #0: loss = 2.44307 (* 1 = 2.44307 loss)
I0814 23:56:51.833621 11101 sgd_solver.cpp:136] Iteration 94500, lr = 0.0704687, m = 0.9
I0814 23:57:06.440776 11101 solver.cpp:312] Iteration 94600 (6.84614 iter/s, 14.6068s/100 iter), loss = 1.68409
I0814 23:57:06.440841 11101 solver.cpp:334]     Train net output #0: loss = 2.00043 (* 1 = 2.00043 loss)
I0814 23:57:06.440860 11101 sgd_solver.cpp:136] Iteration 94600, lr = 0.0704375, m = 0.9
I0814 23:57:21.374963 11101 solver.cpp:312] Iteration 94700 (6.69623 iter/s, 14.9338s/100 iter), loss = 2.2183
I0814 23:57:21.375031 11101 solver.cpp:334]     Train net output #0: loss = 2.40755 (* 1 = 2.40755 loss)
I0814 23:57:21.375037 11101 sgd_solver.cpp:136] Iteration 94700, lr = 0.0704063, m = 0.9
I0814 23:57:36.262615 11101 solver.cpp:312] Iteration 94800 (6.71716 iter/s, 14.8872s/100 iter), loss = 1.69637
I0814 23:57:36.262681 11101 solver.cpp:334]     Train net output #0: loss = 1.59534 (* 1 = 1.59534 loss)
I0814 23:57:36.262702 11101 sgd_solver.cpp:136] Iteration 94800, lr = 0.070375, m = 0.9
I0814 23:57:50.970455 11101 solver.cpp:312] Iteration 94900 (6.79929 iter/s, 14.7074s/100 iter), loss = 1.90179
I0814 23:57:50.970518 11101 solver.cpp:334]     Train net output #0: loss = 2.00944 (* 1 = 2.00944 loss)
I0814 23:57:50.970536 11101 sgd_solver.cpp:136] Iteration 94900, lr = 0.0703438, m = 0.9
I0814 23:58:05.730904 11101 solver.cpp:312] Iteration 95000 (6.77505 iter/s, 14.76s/100 iter), loss = 2.08847
I0814 23:58:05.730995 11101 solver.cpp:334]     Train net output #0: loss = 1.93804 (* 1 = 1.93804 loss)
I0814 23:58:05.731012 11101 sgd_solver.cpp:136] Iteration 95000, lr = 0.0703125, m = 0.9
I0814 23:58:20.278574 11101 solver.cpp:312] Iteration 95100 (6.87415 iter/s, 14.5473s/100 iter), loss = 1.93439
I0814 23:58:20.278640 11101 solver.cpp:334]     Train net output #0: loss = 1.82779 (* 1 = 1.82779 loss)
I0814 23:58:20.278656 11101 sgd_solver.cpp:136] Iteration 95100, lr = 0.0702813, m = 0.9
I0814 23:58:34.778631 11101 solver.cpp:312] Iteration 95200 (6.89672 iter/s, 14.4996s/100 iter), loss = 2.00363
I0814 23:58:34.778661 11101 solver.cpp:334]     Train net output #0: loss = 2.05663 (* 1 = 2.05663 loss)
I0814 23:58:34.778666 11101 sgd_solver.cpp:136] Iteration 95200, lr = 0.07025, m = 0.9
I0814 23:58:49.164583 11101 solver.cpp:312] Iteration 95300 (6.95142 iter/s, 14.3855s/100 iter), loss = 1.81533
I0814 23:58:49.164676 11101 solver.cpp:334]     Train net output #0: loss = 2.03883 (* 1 = 2.03883 loss)
I0814 23:58:49.164695 11101 sgd_solver.cpp:136] Iteration 95300, lr = 0.0702188, m = 0.9
I0814 23:59:03.632056 11101 solver.cpp:312] Iteration 95400 (6.91225 iter/s, 14.4671s/100 iter), loss = 2.34373
I0814 23:59:03.632107 11101 solver.cpp:334]     Train net output #0: loss = 2.01432 (* 1 = 2.01432 loss)
I0814 23:59:03.632120 11101 sgd_solver.cpp:136] Iteration 95400, lr = 0.0701875, m = 0.9
I0814 23:59:18.164789 11101 solver.cpp:312] Iteration 95500 (6.88121 iter/s, 14.5323s/100 iter), loss = 2.35699
I0814 23:59:18.164820 11101 solver.cpp:334]     Train net output #0: loss = 2.18334 (* 1 = 2.18334 loss)
I0814 23:59:18.164825 11101 sgd_solver.cpp:136] Iteration 95500, lr = 0.0701563, m = 0.9
I0814 23:59:32.603866 11101 solver.cpp:312] Iteration 95600 (6.92584 iter/s, 14.4387s/100 iter), loss = 1.91076
I0814 23:59:32.603942 11101 solver.cpp:334]     Train net output #0: loss = 1.78744 (* 1 = 1.78744 loss)
I0814 23:59:32.603950 11101 sgd_solver.cpp:136] Iteration 95600, lr = 0.070125, m = 0.9
I0814 23:59:47.081490 11101 solver.cpp:312] Iteration 95700 (6.9074 iter/s, 14.4772s/100 iter), loss = 2.39008
I0814 23:59:47.081554 11101 solver.cpp:334]     Train net output #0: loss = 2.07943 (* 1 = 2.07943 loss)
I0814 23:59:47.081573 11101 sgd_solver.cpp:136] Iteration 95700, lr = 0.0700938, m = 0.9
I0815 00:00:01.416656 11101 solver.cpp:312] Iteration 95800 (6.97605 iter/s, 14.3348s/100 iter), loss = 2.0688
I0815 00:00:01.416687 11101 solver.cpp:334]     Train net output #0: loss = 2.12303 (* 1 = 2.12303 loss)
I0815 00:00:01.416692 11101 sgd_solver.cpp:136] Iteration 95800, lr = 0.0700625, m = 0.9
I0815 00:00:15.898629 11101 solver.cpp:312] Iteration 95900 (6.90533 iter/s, 14.4816s/100 iter), loss = 2.07807
I0815 00:00:15.898715 11101 solver.cpp:334]     Train net output #0: loss = 2.04092 (* 1 = 2.04092 loss)
I0815 00:00:15.898727 11101 sgd_solver.cpp:136] Iteration 95900, lr = 0.0700312, m = 0.9
I0815 00:00:30.524742 11101 solver.cpp:509] Iteration 96000, Testing net (#0)
I0815 00:00:51.512697 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.426647
I0815 00:00:51.512774 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.679352
I0815 00:00:51.512783 11101 solver.cpp:594]     Test net output #2: loss = 2.7211 (* 1 = 2.7211 loss)
I0815 00:00:51.512800 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9875s
I0815 00:00:51.666616 11101 solver.cpp:312] Iteration 96000 (2.79587 iter/s, 35.767s/100 iter), loss = 2.44064
I0815 00:00:51.666645 11101 solver.cpp:334]     Train net output #0: loss = 2.5453 (* 1 = 2.5453 loss)
I0815 00:00:51.666649 11101 sgd_solver.cpp:136] Iteration 96000, lr = 0.07, m = 0.9
I0815 00:01:06.063053 11101 solver.cpp:312] Iteration 96100 (6.94636 iter/s, 14.396s/100 iter), loss = 2.09158
I0815 00:01:06.063104 11101 solver.cpp:334]     Train net output #0: loss = 2.01883 (* 1 = 2.01883 loss)
I0815 00:01:06.063117 11101 sgd_solver.cpp:136] Iteration 96100, lr = 0.0699688, m = 0.9
I0815 00:01:20.637018 11101 solver.cpp:312] Iteration 96200 (6.86174 iter/s, 14.5736s/100 iter), loss = 2.30415
I0815 00:01:20.637048 11101 solver.cpp:334]     Train net output #0: loss = 2.1379 (* 1 = 2.1379 loss)
I0815 00:01:20.637055 11101 sgd_solver.cpp:136] Iteration 96200, lr = 0.0699375, m = 0.9
I0815 00:01:35.237807 11101 solver.cpp:312] Iteration 96300 (6.84914 iter/s, 14.6004s/100 iter), loss = 2.03063
I0815 00:01:35.237897 11101 solver.cpp:334]     Train net output #0: loss = 2.05225 (* 1 = 2.05225 loss)
I0815 00:01:35.237915 11101 sgd_solver.cpp:136] Iteration 96300, lr = 0.0699062, m = 0.9
I0815 00:01:49.730756 11101 solver.cpp:312] Iteration 96400 (6.9001 iter/s, 14.4925s/100 iter), loss = 2.01651
I0815 00:01:49.730785 11101 solver.cpp:334]     Train net output #0: loss = 1.84223 (* 1 = 1.84223 loss)
I0815 00:01:49.730792 11101 sgd_solver.cpp:136] Iteration 96400, lr = 0.069875, m = 0.9
I0815 00:02:04.295116 11101 solver.cpp:312] Iteration 96500 (6.86627 iter/s, 14.5639s/100 iter), loss = 2.56167
I0815 00:02:04.295145 11101 solver.cpp:334]     Train net output #0: loss = 2.84948 (* 1 = 2.84948 loss)
I0815 00:02:04.295150 11101 sgd_solver.cpp:136] Iteration 96500, lr = 0.0698438, m = 0.9
I0815 00:02:19.198771 11101 solver.cpp:312] Iteration 96600 (6.70995 iter/s, 14.9032s/100 iter), loss = 1.83092
I0815 00:02:19.198850 11101 solver.cpp:334]     Train net output #0: loss = 1.85744 (* 1 = 1.85744 loss)
I0815 00:02:19.198864 11101 sgd_solver.cpp:136] Iteration 96600, lr = 0.0698125, m = 0.9
I0815 00:02:33.649935 11101 solver.cpp:312] Iteration 96700 (6.92005 iter/s, 14.4508s/100 iter), loss = 2.00316
I0815 00:02:33.649962 11101 solver.cpp:334]     Train net output #0: loss = 1.92735 (* 1 = 1.92735 loss)
I0815 00:02:33.649969 11101 sgd_solver.cpp:136] Iteration 96700, lr = 0.0697813, m = 0.9
I0815 00:02:48.232103 11101 solver.cpp:312] Iteration 96800 (6.85788 iter/s, 14.5818s/100 iter), loss = 1.9889
I0815 00:02:48.232127 11101 solver.cpp:334]     Train net output #0: loss = 1.58802 (* 1 = 1.58802 loss)
I0815 00:02:48.232136 11101 sgd_solver.cpp:136] Iteration 96800, lr = 0.06975, m = 0.9
I0815 00:03:02.796247 11101 solver.cpp:312] Iteration 96900 (6.86637 iter/s, 14.5637s/100 iter), loss = 1.71075
I0815 00:03:02.796362 11101 solver.cpp:334]     Train net output #0: loss = 1.59464 (* 1 = 1.59464 loss)
I0815 00:03:02.796383 11101 sgd_solver.cpp:136] Iteration 96900, lr = 0.0697188, m = 0.9
I0815 00:03:17.581704 11101 solver.cpp:312] Iteration 97000 (6.76359 iter/s, 14.785s/100 iter), loss = 2.47244
I0815 00:03:17.581923 11101 solver.cpp:334]     Train net output #0: loss = 2.28399 (* 1 = 2.28399 loss)
I0815 00:03:17.582031 11101 sgd_solver.cpp:136] Iteration 97000, lr = 0.0696875, m = 0.9
I0815 00:03:32.128171 11101 solver.cpp:312] Iteration 97100 (6.87472 iter/s, 14.5461s/100 iter), loss = 2.37046
I0815 00:03:32.128348 11101 solver.cpp:334]     Train net output #0: loss = 1.81275 (* 1 = 1.81275 loss)
I0815 00:03:32.128432 11101 sgd_solver.cpp:136] Iteration 97100, lr = 0.0696563, m = 0.9
I0815 00:03:46.594027 11101 solver.cpp:312] Iteration 97200 (6.91302 iter/s, 14.4655s/100 iter), loss = 2.0869
I0815 00:03:46.594105 11101 solver.cpp:334]     Train net output #0: loss = 2.10411 (* 1 = 2.10411 loss)
I0815 00:03:46.594113 11101 sgd_solver.cpp:136] Iteration 97200, lr = 0.069625, m = 0.9
I0815 00:04:01.058822 11101 solver.cpp:312] Iteration 97300 (6.91353 iter/s, 14.4644s/100 iter), loss = 2.26612
I0815 00:04:01.058847 11101 solver.cpp:334]     Train net output #0: loss = 2.0932 (* 1 = 2.0932 loss)
I0815 00:04:01.058854 11101 sgd_solver.cpp:136] Iteration 97300, lr = 0.0695937, m = 0.9
I0815 00:04:15.518972 11101 solver.cpp:312] Iteration 97400 (6.91575 iter/s, 14.4597s/100 iter), loss = 2.04747
I0815 00:04:15.519031 11101 solver.cpp:334]     Train net output #0: loss = 1.39751 (* 1 = 1.39751 loss)
I0815 00:04:15.519043 11101 sgd_solver.cpp:136] Iteration 97400, lr = 0.0695625, m = 0.9
I0815 00:04:30.156330 11101 solver.cpp:312] Iteration 97500 (6.83203 iter/s, 14.6369s/100 iter), loss = 1.99527
I0815 00:04:30.156545 11101 solver.cpp:334]     Train net output #0: loss = 2.0065 (* 1 = 2.0065 loss)
I0815 00:04:30.156553 11101 sgd_solver.cpp:136] Iteration 97500, lr = 0.0695313, m = 0.9
I0815 00:04:44.755926 11101 solver.cpp:312] Iteration 97600 (6.8497 iter/s, 14.5992s/100 iter), loss = 1.64467
I0815 00:04:44.755985 11101 solver.cpp:334]     Train net output #0: loss = 1.61588 (* 1 = 1.61588 loss)
I0815 00:04:44.756000 11101 sgd_solver.cpp:136] Iteration 97600, lr = 0.0695, m = 0.9
I0815 00:04:59.446245 11101 solver.cpp:312] Iteration 97700 (6.80739 iter/s, 14.6899s/100 iter), loss = 2.55627
I0815 00:04:59.446312 11101 solver.cpp:334]     Train net output #0: loss = 2.58382 (* 1 = 2.58382 loss)
I0815 00:04:59.446333 11101 sgd_solver.cpp:136] Iteration 97700, lr = 0.0694688, m = 0.9
I0815 00:05:14.044108 11101 solver.cpp:312] Iteration 97800 (6.85051 iter/s, 14.5975s/100 iter), loss = 2.18421
I0815 00:05:14.044327 11101 solver.cpp:334]     Train net output #0: loss = 1.56827 (* 1 = 1.56827 loss)
I0815 00:05:14.044337 11101 sgd_solver.cpp:136] Iteration 97800, lr = 0.0694375, m = 0.9
I0815 00:05:28.640971 11101 solver.cpp:312] Iteration 97900 (6.85098 iter/s, 14.5965s/100 iter), loss = 2.18802
I0815 00:05:28.641000 11101 solver.cpp:334]     Train net output #0: loss = 2.35791 (* 1 = 2.35791 loss)
I0815 00:05:28.641006 11101 sgd_solver.cpp:136] Iteration 97900, lr = 0.0694062, m = 0.9
I0815 00:05:43.021814 11101 solver.cpp:509] Iteration 98000, Testing net (#0)
I0815 00:05:46.706818 11112 blocking_queue.cpp:40] Waiting for datum
I0815 00:06:03.849874 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.399882
I0815 00:06:03.849900 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.657764
I0815 00:06:03.849905 11101 solver.cpp:594]     Test net output #2: loss = 2.80676 (* 1 = 2.80676 loss)
I0815 00:06:03.849925 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8275s
I0815 00:06:04.015799 11101 solver.cpp:312] Iteration 98000 (2.82695 iter/s, 35.3738s/100 iter), loss = 2.01569
I0815 00:06:04.015862 11101 solver.cpp:334]     Train net output #0: loss = 2.08732 (* 1 = 2.08732 loss)
I0815 00:06:04.015879 11101 sgd_solver.cpp:136] Iteration 98000, lr = 0.069375, m = 0.9
I0815 00:06:18.750739 11101 solver.cpp:312] Iteration 98100 (6.78678 iter/s, 14.7345s/100 iter), loss = 1.81505
I0815 00:06:18.751060 11101 solver.cpp:334]     Train net output #0: loss = 1.62376 (* 1 = 1.62376 loss)
I0815 00:06:18.751070 11101 sgd_solver.cpp:136] Iteration 98100, lr = 0.0693437, m = 0.9
I0815 00:06:33.593070 11101 solver.cpp:312] Iteration 98200 (6.73768 iter/s, 14.8419s/100 iter), loss = 2.03063
I0815 00:06:33.593221 11101 solver.cpp:334]     Train net output #0: loss = 2.09437 (* 1 = 2.09437 loss)
I0815 00:06:33.593243 11101 sgd_solver.cpp:136] Iteration 98200, lr = 0.0693125, m = 0.9
I0815 00:06:48.550730 11101 solver.cpp:312] Iteration 98300 (6.68573 iter/s, 14.9572s/100 iter), loss = 1.84368
I0815 00:06:48.550756 11101 solver.cpp:334]     Train net output #0: loss = 1.68616 (* 1 = 1.68616 loss)
I0815 00:06:48.550761 11101 sgd_solver.cpp:136] Iteration 98300, lr = 0.0692813, m = 0.9
I0815 00:07:03.144351 11101 solver.cpp:312] Iteration 98400 (6.8525 iter/s, 14.5932s/100 iter), loss = 2.23477
I0815 00:07:03.144419 11101 solver.cpp:334]     Train net output #0: loss = 2.03993 (* 1 = 2.03993 loss)
I0815 00:07:03.144426 11101 sgd_solver.cpp:136] Iteration 98400, lr = 0.06925, m = 0.9
I0815 00:07:17.820495 11101 solver.cpp:312] Iteration 98500 (6.81397 iter/s, 14.6757s/100 iter), loss = 2.35199
I0815 00:07:17.820559 11101 solver.cpp:334]     Train net output #0: loss = 1.78788 (* 1 = 1.78788 loss)
I0815 00:07:17.820580 11101 sgd_solver.cpp:136] Iteration 98500, lr = 0.0692187, m = 0.9
I0815 00:07:32.746807 11101 solver.cpp:312] Iteration 98600 (6.69977 iter/s, 14.9259s/100 iter), loss = 2.65538
I0815 00:07:32.746835 11101 solver.cpp:334]     Train net output #0: loss = 2.61212 (* 1 = 2.61212 loss)
I0815 00:07:32.746841 11101 sgd_solver.cpp:136] Iteration 98600, lr = 0.0691875, m = 0.9
I0815 00:07:47.985522 11101 solver.cpp:312] Iteration 98700 (6.56242 iter/s, 15.2383s/100 iter), loss = 2.07858
I0815 00:07:47.985579 11101 solver.cpp:334]     Train net output #0: loss = 2.00749 (* 1 = 2.00749 loss)
I0815 00:07:47.985585 11101 sgd_solver.cpp:136] Iteration 98700, lr = 0.0691563, m = 0.9
I0815 00:08:03.076470 11101 solver.cpp:312] Iteration 98800 (6.62667 iter/s, 15.0905s/100 iter), loss = 2.14066
I0815 00:08:03.076534 11101 solver.cpp:334]     Train net output #0: loss = 2.60364 (* 1 = 2.60364 loss)
I0815 00:08:03.076553 11101 sgd_solver.cpp:136] Iteration 98800, lr = 0.069125, m = 0.9
I0815 00:08:18.256852 11101 solver.cpp:312] Iteration 98900 (6.58764 iter/s, 15.18s/100 iter), loss = 2.09921
I0815 00:08:18.262984 11101 solver.cpp:334]     Train net output #0: loss = 1.81124 (* 1 = 1.81124 loss)
I0815 00:08:18.262992 11101 sgd_solver.cpp:136] Iteration 98900, lr = 0.0690938, m = 0.9
I0815 00:08:33.614212 11101 solver.cpp:312] Iteration 99000 (6.51172 iter/s, 15.3569s/100 iter), loss = 2.39753
I0815 00:08:33.614277 11101 solver.cpp:334]     Train net output #0: loss = 2.56695 (* 1 = 2.56695 loss)
I0815 00:08:33.614296 11101 sgd_solver.cpp:136] Iteration 99000, lr = 0.0690625, m = 0.9
I0815 00:08:48.054502 11101 solver.cpp:312] Iteration 99100 (6.92526 iter/s, 14.4399s/100 iter), loss = 2.41199
I0815 00:08:48.055299 11101 solver.cpp:334]     Train net output #0: loss = 2.59619 (* 1 = 2.59619 loss)
I0815 00:08:48.055317 11101 sgd_solver.cpp:136] Iteration 99100, lr = 0.0690313, m = 0.9
I0815 00:09:02.887881 11101 solver.cpp:312] Iteration 99200 (6.74174 iter/s, 14.833s/100 iter), loss = 2.221
I0815 00:09:02.887964 11101 solver.cpp:334]     Train net output #0: loss = 2.06628 (* 1 = 2.06628 loss)
I0815 00:09:02.887972 11101 sgd_solver.cpp:136] Iteration 99200, lr = 0.069, m = 0.9
I0815 00:09:17.410686 11101 solver.cpp:312] Iteration 99300 (6.88591 iter/s, 14.5224s/100 iter), loss = 2.20262
I0815 00:09:17.410719 11101 solver.cpp:334]     Train net output #0: loss = 2.12306 (* 1 = 2.12306 loss)
I0815 00:09:17.410725 11101 sgd_solver.cpp:136] Iteration 99300, lr = 0.0689688, m = 0.9
I0815 00:09:32.128448 11101 solver.cpp:312] Iteration 99400 (6.7947 iter/s, 14.7174s/100 iter), loss = 2.30425
I0815 00:09:32.128478 11101 solver.cpp:334]     Train net output #0: loss = 2.39496 (* 1 = 2.39496 loss)
I0815 00:09:32.128484 11101 sgd_solver.cpp:136] Iteration 99400, lr = 0.0689375, m = 0.9
I0815 00:09:46.753254 11101 solver.cpp:312] Iteration 99500 (6.83789 iter/s, 14.6244s/100 iter), loss = 2.29251
I0815 00:09:46.753329 11101 solver.cpp:334]     Train net output #0: loss = 2.15471 (* 1 = 2.15471 loss)
I0815 00:09:46.753343 11101 sgd_solver.cpp:136] Iteration 99500, lr = 0.0689062, m = 0.9
I0815 00:10:01.213538 11101 solver.cpp:312] Iteration 99600 (6.91569 iter/s, 14.4599s/100 iter), loss = 1.88289
I0815 00:10:01.213713 11101 solver.cpp:334]     Train net output #0: loss = 1.89353 (* 1 = 1.89353 loss)
I0815 00:10:01.213794 11101 sgd_solver.cpp:136] Iteration 99600, lr = 0.068875, m = 0.9
I0815 00:10:15.901449 11101 solver.cpp:312] Iteration 99700 (6.80851 iter/s, 14.6875s/100 iter), loss = 2.30213
I0815 00:10:15.901477 11101 solver.cpp:334]     Train net output #0: loss = 2.53822 (* 1 = 2.53822 loss)
I0815 00:10:15.901484 11101 sgd_solver.cpp:136] Iteration 99700, lr = 0.0688437, m = 0.9
I0815 00:10:30.423391 11101 solver.cpp:312] Iteration 99800 (6.88632 iter/s, 14.5215s/100 iter), loss = 2.09919
I0815 00:10:30.423463 11101 solver.cpp:334]     Train net output #0: loss = 1.87476 (* 1 = 1.87476 loss)
I0815 00:10:30.423470 11101 sgd_solver.cpp:136] Iteration 99800, lr = 0.0688125, m = 0.9
I0815 00:10:45.189565 11101 solver.cpp:312] Iteration 99900 (6.77242 iter/s, 14.7658s/100 iter), loss = 1.83072
I0815 00:10:45.189591 11101 solver.cpp:334]     Train net output #0: loss = 1.86876 (* 1 = 1.86876 loss)
I0815 00:10:45.189597 11101 sgd_solver.cpp:136] Iteration 99900, lr = 0.0687812, m = 0.9
I0815 00:10:59.301200 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_100000.caffemodel
I0815 00:10:59.314347 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_100000.solverstate
I0815 00:10:59.320124 11101 solver.cpp:509] Iteration 100000, Testing net (#0)
I0815 00:11:00.622182 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 00:11:19.969427 11099 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 00:11:20.448038 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.425823
I0815 00:11:20.448060 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.683705
I0815 00:11:20.448065 11101 solver.cpp:594]     Test net output #2: loss = 2.63527 (* 1 = 2.63527 loss)
I0815 00:11:20.448109 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1274s
I0815 00:11:20.598342 11101 solver.cpp:312] Iteration 100000 (2.82424 iter/s, 35.4078s/100 iter), loss = 1.89393
I0815 00:11:20.598412 11101 solver.cpp:334]     Train net output #0: loss = 2.08804 (* 1 = 2.08804 loss)
I0815 00:11:20.598433 11101 sgd_solver.cpp:136] Iteration 100000, lr = 0.06875, m = 0.9
I0815 00:11:35.266510 11101 solver.cpp:312] Iteration 100100 (6.81768 iter/s, 14.6678s/100 iter), loss = 2.58992
I0815 00:11:35.266563 11101 solver.cpp:334]     Train net output #0: loss = 2.39821 (* 1 = 2.39821 loss)
I0815 00:11:35.266569 11101 sgd_solver.cpp:136] Iteration 100100, lr = 0.0687188, m = 0.9
I0815 00:11:49.795450 11101 solver.cpp:312] Iteration 100200 (6.88301 iter/s, 14.5285s/100 iter), loss = 2.21892
I0815 00:11:49.795517 11101 solver.cpp:334]     Train net output #0: loss = 2.48645 (* 1 = 2.48645 loss)
I0815 00:11:49.795536 11101 sgd_solver.cpp:136] Iteration 100200, lr = 0.0686875, m = 0.9
I0815 00:12:04.489672 11101 solver.cpp:312] Iteration 100300 (6.80558 iter/s, 14.6938s/100 iter), loss = 1.7426
I0815 00:12:04.489701 11101 solver.cpp:334]     Train net output #0: loss = 1.58427 (* 1 = 1.58427 loss)
I0815 00:12:04.489706 11101 sgd_solver.cpp:136] Iteration 100300, lr = 0.0686563, m = 0.9
I0815 00:12:19.289630 11101 solver.cpp:312] Iteration 100400 (6.75697 iter/s, 14.7995s/100 iter), loss = 2.3137
I0815 00:12:19.289832 11101 solver.cpp:334]     Train net output #0: loss = 2.37025 (* 1 = 2.37025 loss)
I0815 00:12:19.289851 11101 sgd_solver.cpp:136] Iteration 100400, lr = 0.068625, m = 0.9
I0815 00:12:33.982990 11101 solver.cpp:312] Iteration 100500 (6.80598 iter/s, 14.693s/100 iter), loss = 2.27343
I0815 00:12:33.983018 11101 solver.cpp:334]     Train net output #0: loss = 2.81983 (* 1 = 2.81983 loss)
I0815 00:12:33.983024 11101 sgd_solver.cpp:136] Iteration 100500, lr = 0.0685938, m = 0.9
I0815 00:12:48.694682 11101 solver.cpp:312] Iteration 100600 (6.7975 iter/s, 14.7113s/100 iter), loss = 2.59455
I0815 00:12:48.694710 11101 solver.cpp:334]     Train net output #0: loss = 2.81761 (* 1 = 2.81761 loss)
I0815 00:12:48.694715 11101 sgd_solver.cpp:136] Iteration 100600, lr = 0.0685625, m = 0.9
I0815 00:13:03.043767 11101 solver.cpp:312] Iteration 100700 (6.96928 iter/s, 14.3487s/100 iter), loss = 2.06464
I0815 00:13:03.043862 11101 solver.cpp:334]     Train net output #0: loss = 2.79241 (* 1 = 2.79241 loss)
I0815 00:13:03.043881 11101 sgd_solver.cpp:136] Iteration 100700, lr = 0.0685313, m = 0.9
I0815 00:13:17.628360 11101 solver.cpp:312] Iteration 100800 (6.85674 iter/s, 14.5842s/100 iter), loss = 1.58483
I0815 00:13:17.628433 11101 solver.cpp:334]     Train net output #0: loss = 1.37749 (* 1 = 1.37749 loss)
I0815 00:13:17.628454 11101 sgd_solver.cpp:136] Iteration 100800, lr = 0.0685, m = 0.9
I0815 00:13:32.473906 11101 solver.cpp:312] Iteration 100900 (6.73621 iter/s, 14.8451s/100 iter), loss = 1.91108
I0815 00:13:32.473930 11101 solver.cpp:334]     Train net output #0: loss = 2.15105 (* 1 = 2.15105 loss)
I0815 00:13:32.473935 11101 sgd_solver.cpp:136] Iteration 100900, lr = 0.0684687, m = 0.9
I0815 00:13:47.079610 11101 solver.cpp:312] Iteration 101000 (6.84683 iter/s, 14.6053s/100 iter), loss = 2.69757
I0815 00:13:47.079689 11101 solver.cpp:334]     Train net output #0: loss = 2.78666 (* 1 = 2.78666 loss)
I0815 00:13:47.079701 11101 sgd_solver.cpp:136] Iteration 101000, lr = 0.0684375, m = 0.9
I0815 00:14:01.539394 11101 solver.cpp:312] Iteration 101100 (6.91593 iter/s, 14.4594s/100 iter), loss = 2.09893
I0815 00:14:01.539458 11101 solver.cpp:334]     Train net output #0: loss = 2.01369 (* 1 = 2.01369 loss)
I0815 00:14:01.539475 11101 sgd_solver.cpp:136] Iteration 101100, lr = 0.0684062, m = 0.9
I0815 00:14:16.163009 11101 solver.cpp:312] Iteration 101200 (6.83844 iter/s, 14.6232s/100 iter), loss = 1.76996
I0815 00:14:16.163070 11101 solver.cpp:334]     Train net output #0: loss = 1.81907 (* 1 = 1.81907 loss)
I0815 00:14:16.163089 11101 sgd_solver.cpp:136] Iteration 101200, lr = 0.068375, m = 0.9
I0815 00:14:31.110854 11101 solver.cpp:312] Iteration 101300 (6.69011 iter/s, 14.9474s/100 iter), loss = 2.19152
I0815 00:14:31.110914 11101 solver.cpp:334]     Train net output #0: loss = 2.15871 (* 1 = 2.15871 loss)
I0815 00:14:31.110921 11101 sgd_solver.cpp:136] Iteration 101300, lr = 0.0683438, m = 0.9
I0815 00:14:45.681438 11101 solver.cpp:312] Iteration 101400 (6.86333 iter/s, 14.5702s/100 iter), loss = 1.69922
I0815 00:14:45.681468 11101 solver.cpp:334]     Train net output #0: loss = 1.75792 (* 1 = 1.75792 loss)
I0815 00:14:45.681473 11101 sgd_solver.cpp:136] Iteration 101400, lr = 0.0683125, m = 0.9
I0815 00:15:00.204493 11101 solver.cpp:312] Iteration 101500 (6.8858 iter/s, 14.5227s/100 iter), loss = 1.87541
I0815 00:15:00.204524 11101 solver.cpp:334]     Train net output #0: loss = 1.84525 (* 1 = 1.84525 loss)
I0815 00:15:00.204530 11101 sgd_solver.cpp:136] Iteration 101500, lr = 0.0682813, m = 0.9
I0815 00:15:14.878926 11101 solver.cpp:312] Iteration 101600 (6.81476 iter/s, 14.674s/100 iter), loss = 2.04978
I0815 00:15:14.879009 11101 solver.cpp:334]     Train net output #0: loss = 2.35186 (* 1 = 2.35186 loss)
I0815 00:15:14.879015 11101 sgd_solver.cpp:136] Iteration 101600, lr = 0.06825, m = 0.9
I0815 00:15:29.164163 11101 solver.cpp:312] Iteration 101700 (7.00043 iter/s, 14.2848s/100 iter), loss = 1.69722
I0815 00:15:29.164192 11101 solver.cpp:334]     Train net output #0: loss = 1.0704 (* 1 = 1.0704 loss)
I0815 00:15:29.164197 11101 sgd_solver.cpp:136] Iteration 101700, lr = 0.0682188, m = 0.9
I0815 00:15:43.838296 11101 solver.cpp:312] Iteration 101800 (6.8149 iter/s, 14.6737s/100 iter), loss = 2.21459
I0815 00:15:43.838349 11101 solver.cpp:334]     Train net output #0: loss = 2.60948 (* 1 = 2.60948 loss)
I0815 00:15:43.838363 11101 sgd_solver.cpp:136] Iteration 101800, lr = 0.0681875, m = 0.9
I0815 00:15:58.616441 11101 solver.cpp:312] Iteration 101900 (6.76694 iter/s, 14.7777s/100 iter), loss = 2.15247
I0815 00:15:58.616495 11101 solver.cpp:334]     Train net output #0: loss = 2.30987 (* 1 = 2.30987 loss)
I0815 00:15:58.616503 11101 sgd_solver.cpp:136] Iteration 101900, lr = 0.0681563, m = 0.9
I0815 00:16:13.241680 11101 solver.cpp:509] Iteration 102000, Testing net (#0)
I0815 00:16:34.550732 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.420411
I0815 00:16:34.550787 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.686528
I0815 00:16:34.550793 11101 solver.cpp:594]     Test net output #2: loss = 2.64857 (* 1 = 2.64857 loss)
I0815 00:16:34.550810 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3085s
I0815 00:16:34.700991 11101 solver.cpp:312] Iteration 102000 (2.77135 iter/s, 36.0836s/100 iter), loss = 2.62481
I0815 00:16:34.701014 11101 solver.cpp:334]     Train net output #0: loss = 3.12536 (* 1 = 3.12536 loss)
I0815 00:16:34.701020 11101 sgd_solver.cpp:136] Iteration 102000, lr = 0.068125, m = 0.9
I0815 00:16:49.226758 11101 solver.cpp:312] Iteration 102100 (6.88451 iter/s, 14.5254s/100 iter), loss = 1.53449
I0815 00:16:49.226784 11101 solver.cpp:334]     Train net output #0: loss = 1.27175 (* 1 = 1.27175 loss)
I0815 00:16:49.226790 11101 sgd_solver.cpp:136] Iteration 102100, lr = 0.0680938, m = 0.9
I0815 00:17:04.144052 11101 solver.cpp:312] Iteration 102200 (6.70382 iter/s, 14.9169s/100 iter), loss = 1.8612
I0815 00:17:04.144078 11101 solver.cpp:334]     Train net output #0: loss = 1.83959 (* 1 = 1.83959 loss)
I0815 00:17:04.144084 11101 sgd_solver.cpp:136] Iteration 102200, lr = 0.0680625, m = 0.9
I0815 00:17:18.732396 11101 solver.cpp:312] Iteration 102300 (6.85498 iter/s, 14.5879s/100 iter), loss = 2.49207
I0815 00:17:18.732455 11101 solver.cpp:334]     Train net output #0: loss = 2.89473 (* 1 = 2.89473 loss)
I0815 00:17:18.732462 11101 sgd_solver.cpp:136] Iteration 102300, lr = 0.0680313, m = 0.9
I0815 00:17:33.388099 11101 solver.cpp:312] Iteration 102400 (6.82347 iter/s, 14.6553s/100 iter), loss = 1.63018
I0815 00:17:33.388134 11101 solver.cpp:334]     Train net output #0: loss = 1.8158 (* 1 = 1.8158 loss)
I0815 00:17:33.388140 11101 sgd_solver.cpp:136] Iteration 102400, lr = 0.068, m = 0.9
I0815 00:17:47.971043 11101 solver.cpp:312] Iteration 102500 (6.85752 iter/s, 14.5825s/100 iter), loss = 1.8968
I0815 00:17:47.971068 11101 solver.cpp:334]     Train net output #0: loss = 1.69853 (* 1 = 1.69853 loss)
I0815 00:17:47.971073 11101 sgd_solver.cpp:136] Iteration 102500, lr = 0.0679687, m = 0.9
I0815 00:18:02.539394 11101 solver.cpp:312] Iteration 102600 (6.86439 iter/s, 14.5679s/100 iter), loss = 1.86344
I0815 00:18:02.539485 11101 solver.cpp:334]     Train net output #0: loss = 1.45766 (* 1 = 1.45766 loss)
I0815 00:18:02.539499 11101 sgd_solver.cpp:136] Iteration 102600, lr = 0.0679375, m = 0.9
I0815 00:18:17.112718 11101 solver.cpp:312] Iteration 102700 (6.86204 iter/s, 14.5729s/100 iter), loss = 2.26272
I0815 00:18:17.112870 11101 solver.cpp:334]     Train net output #0: loss = 2.47229 (* 1 = 2.47229 loss)
I0815 00:18:17.112938 11101 sgd_solver.cpp:136] Iteration 102700, lr = 0.0679063, m = 0.9
I0815 00:18:31.495339 11101 solver.cpp:312] Iteration 102800 (6.95303 iter/s, 14.3822s/100 iter), loss = 2.16178
I0815 00:18:31.495367 11101 solver.cpp:334]     Train net output #0: loss = 1.74029 (* 1 = 1.74029 loss)
I0815 00:18:31.495371 11101 sgd_solver.cpp:136] Iteration 102800, lr = 0.067875, m = 0.9
I0815 00:18:45.761915 11101 solver.cpp:312] Iteration 102900 (7.00959 iter/s, 14.2662s/100 iter), loss = 2.06234
I0815 00:18:45.761986 11101 solver.cpp:334]     Train net output #0: loss = 1.86423 (* 1 = 1.86423 loss)
I0815 00:18:45.761992 11101 sgd_solver.cpp:136] Iteration 102900, lr = 0.0678438, m = 0.9
I0815 00:19:00.294939 11101 solver.cpp:312] Iteration 103000 (6.88107 iter/s, 14.5326s/100 iter), loss = 2.26172
I0815 00:19:00.294991 11101 solver.cpp:334]     Train net output #0: loss = 1.62569 (* 1 = 1.62569 loss)
I0815 00:19:00.295006 11101 sgd_solver.cpp:136] Iteration 103000, lr = 0.0678125, m = 0.9
I0815 00:19:14.993638 11101 solver.cpp:312] Iteration 103100 (6.80351 iter/s, 14.6983s/100 iter), loss = 1.80281
I0815 00:19:14.993664 11101 solver.cpp:334]     Train net output #0: loss = 1.68511 (* 1 = 1.68511 loss)
I0815 00:19:14.993669 11101 sgd_solver.cpp:136] Iteration 103100, lr = 0.0677812, m = 0.9
I0815 00:19:29.697958 11101 solver.cpp:312] Iteration 103200 (6.80091 iter/s, 14.7039s/100 iter), loss = 2.24339
I0815 00:19:29.698043 11101 solver.cpp:334]     Train net output #0: loss = 2.16705 (* 1 = 2.16705 loss)
I0815 00:19:29.698065 11101 sgd_solver.cpp:136] Iteration 103200, lr = 0.06775, m = 0.9
I0815 00:19:44.410341 11101 solver.cpp:312] Iteration 103300 (6.79718 iter/s, 14.712s/100 iter), loss = 2.15872
I0815 00:19:44.410368 11101 solver.cpp:334]     Train net output #0: loss = 2.43975 (* 1 = 2.43975 loss)
I0815 00:19:44.410374 11101 sgd_solver.cpp:136] Iteration 103300, lr = 0.0677188, m = 0.9
I0815 00:19:59.167778 11101 solver.cpp:312] Iteration 103400 (6.77643 iter/s, 14.757s/100 iter), loss = 1.8964
I0815 00:19:59.167804 11101 solver.cpp:334]     Train net output #0: loss = 2.11765 (* 1 = 2.11765 loss)
I0815 00:19:59.167809 11101 sgd_solver.cpp:136] Iteration 103400, lr = 0.0676875, m = 0.9
I0815 00:20:13.777478 11101 solver.cpp:312] Iteration 103500 (6.8453 iter/s, 14.6086s/100 iter), loss = 1.81381
I0815 00:20:13.777581 11101 solver.cpp:334]     Train net output #0: loss = 1.67395 (* 1 = 1.67395 loss)
I0815 00:20:13.777601 11101 sgd_solver.cpp:136] Iteration 103500, lr = 0.0676562, m = 0.9
I0815 00:20:28.255332 11101 solver.cpp:312] Iteration 103600 (6.90695 iter/s, 14.4782s/100 iter), loss = 1.8753
I0815 00:20:28.255398 11101 solver.cpp:334]     Train net output #0: loss = 1.79115 (* 1 = 1.79115 loss)
I0815 00:20:28.255414 11101 sgd_solver.cpp:136] Iteration 103600, lr = 0.067625, m = 0.9
I0815 00:20:42.894985 11101 solver.cpp:312] Iteration 103700 (6.83095 iter/s, 14.6392s/100 iter), loss = 1.68998
I0815 00:20:42.895014 11101 solver.cpp:334]     Train net output #0: loss = 1.86524 (* 1 = 1.86524 loss)
I0815 00:20:42.895018 11101 sgd_solver.cpp:136] Iteration 103700, lr = 0.0675938, m = 0.9
I0815 00:20:57.573314 11101 solver.cpp:312] Iteration 103800 (6.81295 iter/s, 14.6779s/100 iter), loss = 2.06192
I0815 00:20:57.573400 11101 solver.cpp:334]     Train net output #0: loss = 1.99735 (* 1 = 1.99735 loss)
I0815 00:20:57.573413 11101 sgd_solver.cpp:136] Iteration 103800, lr = 0.0675625, m = 0.9
I0815 00:21:12.257879 11101 solver.cpp:312] Iteration 103900 (6.81006 iter/s, 14.6842s/100 iter), loss = 2.17652
I0815 00:21:12.257907 11101 solver.cpp:334]     Train net output #0: loss = 1.98043 (* 1 = 1.98043 loss)
I0815 00:21:12.257915 11101 sgd_solver.cpp:136] Iteration 103900, lr = 0.0675313, m = 0.9
I0815 00:21:26.569823 11101 solver.cpp:509] Iteration 104000, Testing net (#0)
I0815 00:21:47.589432 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.388823
I0815 00:21:47.589563 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.652117
I0815 00:21:47.589574 11101 solver.cpp:594]     Test net output #2: loss = 2.82796 (* 1 = 2.82796 loss)
I0815 00:21:47.589598 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0192s
I0815 00:21:47.746806 11101 solver.cpp:312] Iteration 104000 (2.81786 iter/s, 35.488s/100 iter), loss = 1.9678
I0815 00:21:47.746865 11101 solver.cpp:334]     Train net output #0: loss = 1.88098 (* 1 = 1.88098 loss)
I0815 00:21:47.746882 11101 sgd_solver.cpp:136] Iteration 104000, lr = 0.0675, m = 0.9
I0815 00:22:02.330463 11101 solver.cpp:312] Iteration 104100 (6.85718 iter/s, 14.5832s/100 iter), loss = 1.8478
I0815 00:22:02.330493 11101 solver.cpp:334]     Train net output #0: loss = 1.77808 (* 1 = 1.77808 loss)
I0815 00:22:02.330499 11101 sgd_solver.cpp:136] Iteration 104100, lr = 0.0674688, m = 0.9
I0815 00:22:16.934927 11101 solver.cpp:312] Iteration 104200 (6.84741 iter/s, 14.6041s/100 iter), loss = 2.19671
I0815 00:22:16.934995 11101 solver.cpp:334]     Train net output #0: loss = 2.39114 (* 1 = 2.39114 loss)
I0815 00:22:16.935012 11101 sgd_solver.cpp:136] Iteration 104200, lr = 0.0674375, m = 0.9
I0815 00:22:31.738353 11101 solver.cpp:312] Iteration 104300 (6.75538 iter/s, 14.803s/100 iter), loss = 2.00128
I0815 00:22:31.738410 11101 solver.cpp:334]     Train net output #0: loss = 2.43895 (* 1 = 2.43895 loss)
I0815 00:22:31.738417 11101 sgd_solver.cpp:136] Iteration 104300, lr = 0.0674063, m = 0.9
I0815 00:22:46.336164 11101 solver.cpp:312] Iteration 104400 (6.85053 iter/s, 14.5974s/100 iter), loss = 2.60417
I0815 00:22:46.336192 11101 solver.cpp:334]     Train net output #0: loss = 2.57727 (* 1 = 2.57727 loss)
I0815 00:22:46.336199 11101 sgd_solver.cpp:136] Iteration 104400, lr = 0.067375, m = 0.9
I0815 00:23:01.063450 11101 solver.cpp:312] Iteration 104500 (6.79031 iter/s, 14.7269s/100 iter), loss = 2.04589
I0815 00:23:01.063510 11101 solver.cpp:334]     Train net output #0: loss = 1.95768 (* 1 = 1.95768 loss)
I0815 00:23:01.063529 11101 sgd_solver.cpp:136] Iteration 104500, lr = 0.0673437, m = 0.9
I0815 00:23:15.881081 11101 solver.cpp:312] Iteration 104600 (6.7489 iter/s, 14.8172s/100 iter), loss = 2.05524
I0815 00:23:15.881177 11101 solver.cpp:334]     Train net output #0: loss = 2.28239 (* 1 = 2.28239 loss)
I0815 00:23:15.881197 11101 sgd_solver.cpp:136] Iteration 104600, lr = 0.0673125, m = 0.9
I0815 00:23:30.686473 11101 solver.cpp:312] Iteration 104700 (6.75449 iter/s, 14.805s/100 iter), loss = 1.83042
I0815 00:23:30.686506 11101 solver.cpp:334]     Train net output #0: loss = 1.98486 (* 1 = 1.98486 loss)
I0815 00:23:30.686512 11101 sgd_solver.cpp:136] Iteration 104700, lr = 0.0672812, m = 0.9
I0815 00:23:45.314736 11101 solver.cpp:312] Iteration 104800 (6.83627 iter/s, 14.6279s/100 iter), loss = 2.10256
I0815 00:23:45.314769 11101 solver.cpp:334]     Train net output #0: loss = 1.89332 (* 1 = 1.89332 loss)
I0815 00:23:45.314774 11101 sgd_solver.cpp:136] Iteration 104800, lr = 0.06725, m = 0.9
I0815 00:23:59.588454 11101 solver.cpp:312] Iteration 104900 (7.00608 iter/s, 14.2733s/100 iter), loss = 2.22361
I0815 00:23:59.588698 11101 solver.cpp:334]     Train net output #0: loss = 2.05676 (* 1 = 2.05676 loss)
I0815 00:23:59.588706 11101 sgd_solver.cpp:136] Iteration 104900, lr = 0.0672188, m = 0.9
I0815 00:24:14.300577 11101 solver.cpp:312] Iteration 105000 (6.79731 iter/s, 14.7117s/100 iter), loss = 1.72436
I0815 00:24:14.300648 11101 solver.cpp:334]     Train net output #0: loss = 2.01325 (* 1 = 2.01325 loss)
I0815 00:24:14.300668 11101 sgd_solver.cpp:136] Iteration 105000, lr = 0.0671875, m = 0.9
I0815 00:24:14.364285 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 00:24:29.142956 11101 solver.cpp:312] Iteration 105100 (6.73765 iter/s, 14.842s/100 iter), loss = 1.97865
I0815 00:24:29.142984 11101 solver.cpp:334]     Train net output #0: loss = 1.99054 (* 1 = 1.99054 loss)
I0815 00:24:29.142990 11101 sgd_solver.cpp:136] Iteration 105100, lr = 0.0671562, m = 0.9
I0815 00:24:43.828410 11101 solver.cpp:312] Iteration 105200 (6.80965 iter/s, 14.685s/100 iter), loss = 2.47727
I0815 00:24:43.828523 11101 solver.cpp:334]     Train net output #0: loss = 2.38422 (* 1 = 2.38422 loss)
I0815 00:24:43.828546 11101 sgd_solver.cpp:136] Iteration 105200, lr = 0.067125, m = 0.9
I0815 00:24:58.615489 11101 solver.cpp:312] Iteration 105300 (6.76285 iter/s, 14.7867s/100 iter), loss = 2.23846
I0815 00:24:58.615509 11101 solver.cpp:334]     Train net output #0: loss = 1.99023 (* 1 = 1.99023 loss)
I0815 00:24:58.615514 11101 sgd_solver.cpp:136] Iteration 105300, lr = 0.0670938, m = 0.9
I0815 00:25:13.249996 11101 solver.cpp:312] Iteration 105400 (6.83335 iter/s, 14.6341s/100 iter), loss = 2.21066
I0815 00:25:13.250025 11101 solver.cpp:334]     Train net output #0: loss = 2.05631 (* 1 = 2.05631 loss)
I0815 00:25:13.250030 11101 sgd_solver.cpp:136] Iteration 105400, lr = 0.0670625, m = 0.9
I0815 00:25:28.178931 11101 solver.cpp:312] Iteration 105500 (6.69859 iter/s, 14.9285s/100 iter), loss = 1.81818
I0815 00:25:28.179028 11101 solver.cpp:334]     Train net output #0: loss = 1.60039 (* 1 = 1.60039 loss)
I0815 00:25:28.179045 11101 sgd_solver.cpp:136] Iteration 105500, lr = 0.0670313, m = 0.9
I0815 00:25:43.174890 11101 solver.cpp:312] Iteration 105600 (6.66865 iter/s, 14.9955s/100 iter), loss = 2.15905
I0815 00:25:43.174918 11101 solver.cpp:334]     Train net output #0: loss = 1.87849 (* 1 = 1.87849 loss)
I0815 00:25:43.174924 11101 sgd_solver.cpp:136] Iteration 105600, lr = 0.067, m = 0.9
I0815 00:25:57.846424 11101 solver.cpp:312] Iteration 105700 (6.81611 iter/s, 14.6711s/100 iter), loss = 2.2569
I0815 00:25:57.846453 11101 solver.cpp:334]     Train net output #0: loss = 2.16158 (* 1 = 2.16158 loss)
I0815 00:25:57.846459 11101 sgd_solver.cpp:136] Iteration 105700, lr = 0.0669688, m = 0.9
I0815 00:26:12.430652 11101 solver.cpp:312] Iteration 105800 (6.85691 iter/s, 14.5838s/100 iter), loss = 2.05494
I0815 00:26:12.430724 11101 solver.cpp:334]     Train net output #0: loss = 2.27687 (* 1 = 2.27687 loss)
I0815 00:26:12.430732 11101 sgd_solver.cpp:136] Iteration 105800, lr = 0.0669375, m = 0.9
I0815 00:26:27.098104 11101 solver.cpp:312] Iteration 105900 (6.81801 iter/s, 14.667s/100 iter), loss = 1.78502
I0815 00:26:27.098130 11101 solver.cpp:334]     Train net output #0: loss = 2.23837 (* 1 = 2.23837 loss)
I0815 00:26:27.098134 11101 sgd_solver.cpp:136] Iteration 105900, lr = 0.0669063, m = 0.9
I0815 00:26:41.563951 11101 solver.cpp:509] Iteration 106000, Testing net (#0)
I0815 00:27:02.451764 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.421294
I0815 00:27:02.451812 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.678469
I0815 00:27:02.451819 11101 solver.cpp:594]     Test net output #2: loss = 2.66693 (* 1 = 2.66693 loss)
I0815 00:27:02.451836 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8873s
I0815 00:27:02.590914 11101 solver.cpp:312] Iteration 106000 (2.81755 iter/s, 35.4918s/100 iter), loss = 2.06098
I0815 00:27:02.590960 11101 solver.cpp:334]     Train net output #0: loss = 2.01628 (* 1 = 2.01628 loss)
I0815 00:27:02.590975 11101 sgd_solver.cpp:136] Iteration 106000, lr = 0.066875, m = 0.9
I0815 00:27:16.897197 11101 solver.cpp:312] Iteration 106100 (6.99013 iter/s, 14.3059s/100 iter), loss = 2.21948
I0815 00:27:16.897224 11101 solver.cpp:334]     Train net output #0: loss = 2.39048 (* 1 = 2.39048 loss)
I0815 00:27:16.897228 11101 sgd_solver.cpp:136] Iteration 106100, lr = 0.0668437, m = 0.9
I0815 00:27:31.820173 11101 solver.cpp:312] Iteration 106200 (6.70127 iter/s, 14.9226s/100 iter), loss = 2.33838
I0815 00:27:31.820236 11101 solver.cpp:334]     Train net output #0: loss = 2.27332 (* 1 = 2.27332 loss)
I0815 00:27:31.820255 11101 sgd_solver.cpp:136] Iteration 106200, lr = 0.0668125, m = 0.9
I0815 00:27:46.523357 11101 solver.cpp:312] Iteration 106300 (6.80143 iter/s, 14.7028s/100 iter), loss = 1.66469
I0815 00:27:46.523409 11101 solver.cpp:334]     Train net output #0: loss = 1.58427 (* 1 = 1.58427 loss)
I0815 00:27:46.523416 11101 sgd_solver.cpp:136] Iteration 106300, lr = 0.0667812, m = 0.9
I0815 00:28:01.211855 11101 solver.cpp:312] Iteration 106400 (6.80824 iter/s, 14.6881s/100 iter), loss = 1.94892
I0815 00:28:01.211882 11101 solver.cpp:334]     Train net output #0: loss = 2.30859 (* 1 = 2.30859 loss)
I0815 00:28:01.211889 11101 sgd_solver.cpp:136] Iteration 106400, lr = 0.06675, m = 0.9
I0815 00:28:16.035909 11101 solver.cpp:312] Iteration 106500 (6.74598 iter/s, 14.8236s/100 iter), loss = 2.07648
I0815 00:28:16.035935 11101 solver.cpp:334]     Train net output #0: loss = 1.93831 (* 1 = 1.93831 loss)
I0815 00:28:16.035941 11101 sgd_solver.cpp:136] Iteration 106500, lr = 0.0667187, m = 0.9
I0815 00:28:30.581024 11101 solver.cpp:312] Iteration 106600 (6.87535 iter/s, 14.5447s/100 iter), loss = 2.25013
I0815 00:28:30.581290 11101 solver.cpp:334]     Train net output #0: loss = 2.35081 (* 1 = 2.35081 loss)
I0815 00:28:30.581398 11101 sgd_solver.cpp:136] Iteration 106600, lr = 0.0666875, m = 0.9
I0815 00:28:45.097002 11101 solver.cpp:312] Iteration 106700 (6.88915 iter/s, 14.5156s/100 iter), loss = 2.22021
I0815 00:28:45.097064 11101 solver.cpp:334]     Train net output #0: loss = 2.09507 (* 1 = 2.09507 loss)
I0815 00:28:45.097082 11101 sgd_solver.cpp:136] Iteration 106700, lr = 0.0666563, m = 0.9
I0815 00:28:59.359252 11101 solver.cpp:312] Iteration 106800 (7.01171 iter/s, 14.2618s/100 iter), loss = 2.33472
I0815 00:28:59.359278 11101 solver.cpp:334]     Train net output #0: loss = 1.9905 (* 1 = 1.9905 loss)
I0815 00:28:59.359284 11101 sgd_solver.cpp:136] Iteration 106800, lr = 0.066625, m = 0.9
I0815 00:29:13.859226 11101 solver.cpp:312] Iteration 106900 (6.89676 iter/s, 14.4996s/100 iter), loss = 2.65888
I0815 00:29:13.859292 11101 solver.cpp:334]     Train net output #0: loss = 2.54516 (* 1 = 2.54516 loss)
I0815 00:29:13.859299 11101 sgd_solver.cpp:136] Iteration 106900, lr = 0.0665938, m = 0.9
I0815 00:29:28.510607 11101 solver.cpp:312] Iteration 107000 (6.82549 iter/s, 14.651s/100 iter), loss = 2.45354
I0815 00:29:28.510637 11101 solver.cpp:334]     Train net output #0: loss = 2.56824 (* 1 = 2.56824 loss)
I0815 00:29:28.510643 11101 sgd_solver.cpp:136] Iteration 107000, lr = 0.0665625, m = 0.9
I0815 00:29:42.907762 11101 solver.cpp:312] Iteration 107100 (6.94601 iter/s, 14.3968s/100 iter), loss = 2.11284
I0815 00:29:42.907788 11101 solver.cpp:334]     Train net output #0: loss = 2.28605 (* 1 = 2.28605 loss)
I0815 00:29:42.907794 11101 sgd_solver.cpp:136] Iteration 107100, lr = 0.0665313, m = 0.9
I0815 00:29:57.325103 11101 solver.cpp:312] Iteration 107200 (6.93628 iter/s, 14.4169s/100 iter), loss = 2.28087
I0815 00:29:57.325155 11101 solver.cpp:334]     Train net output #0: loss = 2.07331 (* 1 = 2.07331 loss)
I0815 00:29:57.325162 11101 sgd_solver.cpp:136] Iteration 107200, lr = 0.0665, m = 0.9
I0815 00:30:11.875866 11101 solver.cpp:312] Iteration 107300 (6.87268 iter/s, 14.5504s/100 iter), loss = 1.92355
I0815 00:30:11.875895 11101 solver.cpp:334]     Train net output #0: loss = 1.9497 (* 1 = 1.9497 loss)
I0815 00:30:11.875903 11101 sgd_solver.cpp:136] Iteration 107300, lr = 0.0664688, m = 0.9
I0815 00:30:26.216897 11101 solver.cpp:312] Iteration 107400 (6.97319 iter/s, 14.3406s/100 iter), loss = 2.13611
I0815 00:30:26.217120 11101 solver.cpp:334]     Train net output #0: loss = 2.30245 (* 1 = 2.30245 loss)
I0815 00:30:26.217228 11101 sgd_solver.cpp:136] Iteration 107400, lr = 0.0664375, m = 0.9
I0815 00:30:40.690441 11101 solver.cpp:312] Iteration 107500 (6.90935 iter/s, 14.4731s/100 iter), loss = 2.3277
I0815 00:30:40.690533 11101 solver.cpp:334]     Train net output #0: loss = 2.75352 (* 1 = 2.75352 loss)
I0815 00:30:40.690551 11101 sgd_solver.cpp:136] Iteration 107500, lr = 0.0664062, m = 0.9
I0815 00:30:54.974699 11101 solver.cpp:312] Iteration 107600 (7.00091 iter/s, 14.2839s/100 iter), loss = 1.70491
I0815 00:30:54.974920 11101 solver.cpp:334]     Train net output #0: loss = 1.73866 (* 1 = 1.73866 loss)
I0815 00:30:54.975030 11101 sgd_solver.cpp:136] Iteration 107600, lr = 0.066375, m = 0.9
I0815 00:31:09.838347 11101 solver.cpp:312] Iteration 107700 (6.72801 iter/s, 14.8632s/100 iter), loss = 1.82598
I0815 00:31:09.838412 11101 solver.cpp:334]     Train net output #0: loss = 1.87042 (* 1 = 1.87042 loss)
I0815 00:31:09.838435 11101 sgd_solver.cpp:136] Iteration 107700, lr = 0.0663437, m = 0.9
I0815 00:31:24.505710 11101 solver.cpp:312] Iteration 107800 (6.81805 iter/s, 14.667s/100 iter), loss = 1.99101
I0815 00:31:24.505795 11101 solver.cpp:334]     Train net output #0: loss = 2.05368 (* 1 = 2.05368 loss)
I0815 00:31:24.505803 11101 sgd_solver.cpp:136] Iteration 107800, lr = 0.0663125, m = 0.9
I0815 00:31:39.345463 11101 solver.cpp:312] Iteration 107900 (6.73884 iter/s, 14.8393s/100 iter), loss = 2.22534
I0815 00:31:39.345491 11101 solver.cpp:334]     Train net output #0: loss = 2.37632 (* 1 = 2.37632 loss)
I0815 00:31:39.345497 11101 sgd_solver.cpp:136] Iteration 107900, lr = 0.0662813, m = 0.9
I0815 00:31:54.150624 11101 solver.cpp:509] Iteration 108000, Testing net (#0)
I0815 00:32:15.462460 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.408235
I0815 00:32:15.462512 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.673352
I0815 00:32:15.462517 11101 solver.cpp:594]     Test net output #2: loss = 2.73167 (* 1 = 2.73167 loss)
I0815 00:32:15.462539 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3113s
I0815 00:32:15.617161 11101 solver.cpp:312] Iteration 108000 (2.75705 iter/s, 36.2707s/100 iter), loss = 2.02289
I0815 00:32:15.617210 11101 solver.cpp:334]     Train net output #0: loss = 1.73332 (* 1 = 1.73332 loss)
I0815 00:32:15.617223 11101 sgd_solver.cpp:136] Iteration 108000, lr = 0.06625, m = 0.9
I0815 00:32:30.245391 11101 solver.cpp:312] Iteration 108100 (6.83629 iter/s, 14.6278s/100 iter), loss = 2.21329
I0815 00:32:30.245440 11101 solver.cpp:334]     Train net output #0: loss = 2.34772 (* 1 = 2.34772 loss)
I0815 00:32:30.245551 11101 sgd_solver.cpp:136] Iteration 108100, lr = 0.0662187, m = 0.9
I0815 00:32:44.864684 11101 solver.cpp:312] Iteration 108200 (6.84047 iter/s, 14.6189s/100 iter), loss = 1.96682
I0815 00:32:44.864713 11101 solver.cpp:334]     Train net output #0: loss = 2.0681 (* 1 = 2.0681 loss)
I0815 00:32:44.864720 11101 sgd_solver.cpp:136] Iteration 108200, lr = 0.0661875, m = 0.9
I0815 00:32:59.549904 11101 solver.cpp:312] Iteration 108300 (6.80976 iter/s, 14.6848s/100 iter), loss = 1.84704
I0815 00:32:59.549967 11101 solver.cpp:334]     Train net output #0: loss = 1.93085 (* 1 = 1.93085 loss)
I0815 00:32:59.549973 11101 sgd_solver.cpp:136] Iteration 108300, lr = 0.0661563, m = 0.9
I0815 00:33:14.258117 11101 solver.cpp:312] Iteration 108400 (6.79911 iter/s, 14.7078s/100 iter), loss = 2.42327
I0815 00:33:14.258143 11101 solver.cpp:334]     Train net output #0: loss = 2.42871 (* 1 = 2.42871 loss)
I0815 00:33:14.258148 11101 sgd_solver.cpp:136] Iteration 108400, lr = 0.066125, m = 0.9
I0815 00:33:28.789983 11101 solver.cpp:312] Iteration 108500 (6.88162 iter/s, 14.5315s/100 iter), loss = 2.28514
I0815 00:33:28.790127 11101 solver.cpp:334]     Train net output #0: loss = 2.6565 (* 1 = 2.6565 loss)
I0815 00:33:28.790196 11101 sgd_solver.cpp:136] Iteration 108500, lr = 0.0660938, m = 0.9
I0815 00:33:43.465333 11101 solver.cpp:312] Iteration 108600 (6.81434 iter/s, 14.6749s/100 iter), loss = 2.05559
I0815 00:33:43.465436 11101 solver.cpp:334]     Train net output #0: loss = 2.29041 (* 1 = 2.29041 loss)
I0815 00:33:43.465456 11101 sgd_solver.cpp:136] Iteration 108600, lr = 0.0660625, m = 0.9
I0815 00:33:57.986888 11101 solver.cpp:312] Iteration 108700 (6.88651 iter/s, 14.5211s/100 iter), loss = 2.11941
I0815 00:33:57.986918 11101 solver.cpp:334]     Train net output #0: loss = 2.01559 (* 1 = 2.01559 loss)
I0815 00:33:57.986923 11101 sgd_solver.cpp:136] Iteration 108700, lr = 0.0660313, m = 0.9
I0815 00:34:12.793135 11101 solver.cpp:312] Iteration 108800 (6.75409 iter/s, 14.8058s/100 iter), loss = 2.17726
I0815 00:34:12.793198 11101 solver.cpp:334]     Train net output #0: loss = 2.13605 (* 1 = 2.13605 loss)
I0815 00:34:12.793223 11101 sgd_solver.cpp:136] Iteration 108800, lr = 0.066, m = 0.9
I0815 00:34:27.389521 11101 solver.cpp:312] Iteration 108900 (6.8512 iter/s, 14.596s/100 iter), loss = 2.1983
I0815 00:34:27.389600 11101 solver.cpp:334]     Train net output #0: loss = 2.25188 (* 1 = 2.25188 loss)
I0815 00:34:27.389607 11101 sgd_solver.cpp:136] Iteration 108900, lr = 0.0659688, m = 0.9
I0815 00:34:42.240828 11101 solver.cpp:312] Iteration 109000 (6.7336 iter/s, 14.8509s/100 iter), loss = 1.91194
I0815 00:34:42.240854 11101 solver.cpp:334]     Train net output #0: loss = 1.92858 (* 1 = 1.92858 loss)
I0815 00:34:42.240859 11101 sgd_solver.cpp:136] Iteration 109000, lr = 0.0659375, m = 0.9
I0815 00:34:56.970185 11101 solver.cpp:312] Iteration 109100 (6.78935 iter/s, 14.7289s/100 iter), loss = 2.38228
I0815 00:34:56.970221 11101 solver.cpp:334]     Train net output #0: loss = 2.35067 (* 1 = 2.35067 loss)
I0815 00:34:56.970228 11101 sgd_solver.cpp:136] Iteration 109100, lr = 0.0659062, m = 0.9
I0815 00:35:11.390766 11101 solver.cpp:312] Iteration 109200 (6.93473 iter/s, 14.4202s/100 iter), loss = 2.06148
I0815 00:35:11.391037 11101 solver.cpp:334]     Train net output #0: loss = 2.08827 (* 1 = 2.08827 loss)
I0815 00:35:11.391047 11101 sgd_solver.cpp:136] Iteration 109200, lr = 0.065875, m = 0.9
I0815 00:35:26.020768 11101 solver.cpp:312] Iteration 109300 (6.83546 iter/s, 14.6296s/100 iter), loss = 1.97672
I0815 00:35:26.020823 11101 solver.cpp:334]     Train net output #0: loss = 2.3334 (* 1 = 2.3334 loss)
I0815 00:35:26.020835 11101 sgd_solver.cpp:136] Iteration 109300, lr = 0.0658438, m = 0.9
I0815 00:35:40.767544 11101 solver.cpp:312] Iteration 109400 (6.78133 iter/s, 14.7464s/100 iter), loss = 1.86871
I0815 00:35:40.767573 11101 solver.cpp:334]     Train net output #0: loss = 1.85765 (* 1 = 1.85765 loss)
I0815 00:35:40.767580 11101 sgd_solver.cpp:136] Iteration 109400, lr = 0.0658125, m = 0.9
I0815 00:35:55.528522 11101 solver.cpp:312] Iteration 109500 (6.77481 iter/s, 14.7606s/100 iter), loss = 1.93276
I0815 00:35:55.528621 11101 solver.cpp:334]     Train net output #0: loss = 1.72163 (* 1 = 1.72163 loss)
I0815 00:35:55.528642 11101 sgd_solver.cpp:136] Iteration 109500, lr = 0.0657813, m = 0.9
I0815 00:36:09.932936 11101 solver.cpp:312] Iteration 109600 (6.94251 iter/s, 14.404s/100 iter), loss = 1.73769
I0815 00:36:09.932960 11101 solver.cpp:334]     Train net output #0: loss = 1.58271 (* 1 = 1.58271 loss)
I0815 00:36:09.932965 11101 sgd_solver.cpp:136] Iteration 109600, lr = 0.06575, m = 0.9
I0815 00:36:24.569295 11101 solver.cpp:312] Iteration 109700 (6.83249 iter/s, 14.636s/100 iter), loss = 2.21999
I0815 00:36:24.569376 11101 solver.cpp:334]     Train net output #0: loss = 2.28315 (* 1 = 2.28315 loss)
I0815 00:36:24.569396 11101 sgd_solver.cpp:136] Iteration 109700, lr = 0.0657187, m = 0.9
I0815 00:36:39.177824 11101 solver.cpp:312] Iteration 109800 (6.8455 iter/s, 14.6081s/100 iter), loss = 2.34831
I0815 00:36:39.178028 11101 solver.cpp:334]     Train net output #0: loss = 2.41892 (* 1 = 2.41892 loss)
I0815 00:36:39.178115 11101 sgd_solver.cpp:136] Iteration 109800, lr = 0.0656875, m = 0.9
I0815 00:36:54.072079 11101 solver.cpp:312] Iteration 109900 (6.71418 iter/s, 14.8938s/100 iter), loss = 2.17638
I0815 00:36:54.072129 11101 solver.cpp:334]     Train net output #0: loss = 2.2064 (* 1 = 2.2064 loss)
I0815 00:36:54.072154 11101 sgd_solver.cpp:136] Iteration 109900, lr = 0.0656563, m = 0.9
I0815 00:37:09.019397 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_110000.caffemodel
I0815 00:37:09.060609 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_110000.solverstate
I0815 00:37:09.066278 11101 solver.cpp:509] Iteration 110000, Testing net (#0)
I0815 00:37:27.977604 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 00:37:30.243329 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.381764
I0815 00:37:30.243350 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.638176
I0815 00:37:30.243355 11101 solver.cpp:594]     Test net output #2: loss = 2.93893 (* 1 = 2.93893 loss)
I0815 00:37:30.243384 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1765s
I0815 00:37:30.411130 11101 solver.cpp:312] Iteration 110000 (2.75194 iter/s, 36.3381s/100 iter), loss = 2.212
I0815 00:37:30.411157 11101 solver.cpp:334]     Train net output #0: loss = 2.60031 (* 1 = 2.60031 loss)
I0815 00:37:30.411164 11101 sgd_solver.cpp:136] Iteration 110000, lr = 0.065625, m = 0.9
I0815 00:37:44.706181 11101 solver.cpp:312] Iteration 110100 (6.99563 iter/s, 14.2946s/100 iter), loss = 2.1199
I0815 00:37:44.706245 11101 solver.cpp:334]     Train net output #0: loss = 1.62332 (* 1 = 1.62332 loss)
I0815 00:37:44.706264 11101 sgd_solver.cpp:136] Iteration 110100, lr = 0.0655937, m = 0.9
I0815 00:37:59.089627 11101 solver.cpp:312] Iteration 110200 (6.95263 iter/s, 14.383s/100 iter), loss = 2.11686
I0815 00:37:59.089761 11101 solver.cpp:334]     Train net output #0: loss = 1.90003 (* 1 = 1.90003 loss)
I0815 00:37:59.089781 11101 sgd_solver.cpp:136] Iteration 110200, lr = 0.0655625, m = 0.9
I0815 00:38:13.577841 11101 solver.cpp:312] Iteration 110300 (6.90236 iter/s, 14.4878s/100 iter), loss = 1.94385
I0815 00:38:13.577900 11101 solver.cpp:334]     Train net output #0: loss = 1.93098 (* 1 = 1.93098 loss)
I0815 00:38:13.577919 11101 sgd_solver.cpp:136] Iteration 110300, lr = 0.0655313, m = 0.9
I0815 00:38:28.250264 11101 solver.cpp:312] Iteration 110400 (6.8157 iter/s, 14.672s/100 iter), loss = 2.21337
I0815 00:38:28.250291 11101 solver.cpp:334]     Train net output #0: loss = 2.17695 (* 1 = 2.17695 loss)
I0815 00:38:28.250295 11101 sgd_solver.cpp:136] Iteration 110400, lr = 0.0655, m = 0.9
I0815 00:38:42.734032 11101 solver.cpp:312] Iteration 110500 (6.90447 iter/s, 14.4834s/100 iter), loss = 1.40316
I0815 00:38:42.734120 11101 solver.cpp:334]     Train net output #0: loss = 1.17644 (* 1 = 1.17644 loss)
I0815 00:38:42.734138 11101 sgd_solver.cpp:136] Iteration 110500, lr = 0.0654688, m = 0.9
I0815 00:38:57.339849 11101 solver.cpp:312] Iteration 110600 (6.84678 iter/s, 14.6054s/100 iter), loss = 2.19294
I0815 00:38:57.339923 11101 solver.cpp:334]     Train net output #0: loss = 1.99452 (* 1 = 1.99452 loss)
I0815 00:38:57.339942 11101 sgd_solver.cpp:136] Iteration 110600, lr = 0.0654375, m = 0.9
I0815 00:39:12.055515 11101 solver.cpp:312] Iteration 110700 (6.79567 iter/s, 14.7153s/100 iter), loss = 2.34665
I0815 00:39:12.055541 11101 solver.cpp:334]     Train net output #0: loss = 2.28963 (* 1 = 2.28963 loss)
I0815 00:39:12.055548 11101 sgd_solver.cpp:136] Iteration 110700, lr = 0.0654063, m = 0.9
I0815 00:39:26.874732 11101 solver.cpp:312] Iteration 110800 (6.74818 iter/s, 14.8188s/100 iter), loss = 2.21358
I0815 00:39:26.874792 11101 solver.cpp:334]     Train net output #0: loss = 2.62977 (* 1 = 2.62977 loss)
I0815 00:39:26.874800 11101 sgd_solver.cpp:136] Iteration 110800, lr = 0.065375, m = 0.9
I0815 00:39:41.283499 11101 solver.cpp:312] Iteration 110900 (6.94041 iter/s, 14.4084s/100 iter), loss = 2.21425
I0815 00:39:41.283560 11101 solver.cpp:334]     Train net output #0: loss = 2.33772 (* 1 = 2.33772 loss)
I0815 00:39:41.283577 11101 sgd_solver.cpp:136] Iteration 110900, lr = 0.0653438, m = 0.9
I0815 00:39:55.863960 11101 solver.cpp:312] Iteration 111000 (6.85869 iter/s, 14.5801s/100 iter), loss = 1.61135
I0815 00:39:55.864019 11101 solver.cpp:334]     Train net output #0: loss = 1.66756 (* 1 = 1.66756 loss)
I0815 00:39:55.864037 11101 sgd_solver.cpp:136] Iteration 111000, lr = 0.0653125, m = 0.9
I0815 00:40:10.473476 11101 solver.cpp:312] Iteration 111100 (6.84504 iter/s, 14.6091s/100 iter), loss = 2.07621
I0815 00:40:10.473551 11101 solver.cpp:334]     Train net output #0: loss = 2.33232 (* 1 = 2.33232 loss)
I0815 00:40:10.473563 11101 sgd_solver.cpp:136] Iteration 111100, lr = 0.0652812, m = 0.9
I0815 00:40:25.260843 11101 solver.cpp:312] Iteration 111200 (6.76272 iter/s, 14.787s/100 iter), loss = 2.28639
I0815 00:40:25.260871 11101 solver.cpp:334]     Train net output #0: loss = 2.22623 (* 1 = 2.22623 loss)
I0815 00:40:25.260877 11101 sgd_solver.cpp:136] Iteration 111200, lr = 0.06525, m = 0.9
I0815 00:40:40.348027 11101 solver.cpp:312] Iteration 111300 (6.62833 iter/s, 15.0868s/100 iter), loss = 2.50603
I0815 00:40:40.348057 11101 solver.cpp:334]     Train net output #0: loss = 2.79665 (* 1 = 2.79665 loss)
I0815 00:40:40.348063 11101 sgd_solver.cpp:136] Iteration 111300, lr = 0.0652187, m = 0.9
I0815 00:40:54.977710 11101 solver.cpp:312] Iteration 111400 (6.83561 iter/s, 14.6293s/100 iter), loss = 2.33483
I0815 00:40:54.977782 11101 solver.cpp:334]     Train net output #0: loss = 2.21603 (* 1 = 2.21603 loss)
I0815 00:40:54.977788 11101 sgd_solver.cpp:136] Iteration 111400, lr = 0.0651875, m = 0.9
I0815 00:41:09.536492 11101 solver.cpp:312] Iteration 111500 (6.8689 iter/s, 14.5584s/100 iter), loss = 2.01554
I0815 00:41:09.536545 11101 solver.cpp:334]     Train net output #0: loss = 1.81004 (* 1 = 1.81004 loss)
I0815 00:41:09.536557 11101 sgd_solver.cpp:136] Iteration 111500, lr = 0.0651563, m = 0.9
I0815 00:41:23.941378 11101 solver.cpp:312] Iteration 111600 (6.94228 iter/s, 14.4045s/100 iter), loss = 1.59853
I0815 00:41:23.941408 11101 solver.cpp:334]     Train net output #0: loss = 1.87262 (* 1 = 1.87262 loss)
I0815 00:41:23.941413 11101 sgd_solver.cpp:136] Iteration 111600, lr = 0.065125, m = 0.9
I0815 00:41:38.406067 11101 solver.cpp:312] Iteration 111700 (6.91358 iter/s, 14.4643s/100 iter), loss = 2.09584
I0815 00:41:38.406132 11101 solver.cpp:334]     Train net output #0: loss = 2.41243 (* 1 = 2.41243 loss)
I0815 00:41:38.406139 11101 sgd_solver.cpp:136] Iteration 111700, lr = 0.0650937, m = 0.9
I0815 00:41:52.921627 11101 solver.cpp:312] Iteration 111800 (6.88935 iter/s, 14.5152s/100 iter), loss = 1.73926
I0815 00:41:52.921650 11101 solver.cpp:334]     Train net output #0: loss = 1.57829 (* 1 = 1.57829 loss)
I0815 00:41:52.921654 11101 sgd_solver.cpp:136] Iteration 111800, lr = 0.0650625, m = 0.9
I0815 00:42:07.507026 11101 solver.cpp:312] Iteration 111900 (6.85636 iter/s, 14.585s/100 iter), loss = 1.90168
I0815 00:42:07.507052 11101 solver.cpp:334]     Train net output #0: loss = 1.40866 (* 1 = 1.40866 loss)
I0815 00:42:07.507060 11101 sgd_solver.cpp:136] Iteration 111900, lr = 0.0650313, m = 0.9
I0815 00:42:21.827064 11101 solver.cpp:509] Iteration 112000, Testing net (#0)
I0815 00:42:42.499315 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.419647
I0815 00:42:42.499337 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.678058
I0815 00:42:42.499344 11101 solver.cpp:594]     Test net output #2: loss = 2.66617 (* 1 = 2.66617 loss)
I0815 00:42:42.499366 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6717s
I0815 00:42:42.679153 11101 solver.cpp:312] Iteration 112000 (2.84324 iter/s, 35.1712s/100 iter), loss = 2.26743
I0815 00:42:42.679177 11101 solver.cpp:334]     Train net output #0: loss = 2.3497 (* 1 = 2.3497 loss)
I0815 00:42:42.679183 11101 sgd_solver.cpp:136] Iteration 112000, lr = 0.065, m = 0.9
I0815 00:42:57.221366 11101 solver.cpp:312] Iteration 112100 (6.87672 iter/s, 14.5418s/100 iter), loss = 2.28094
I0815 00:42:57.221422 11101 solver.cpp:334]     Train net output #0: loss = 2.50492 (* 1 = 2.50492 loss)
I0815 00:42:57.221429 11101 sgd_solver.cpp:136] Iteration 112100, lr = 0.0649688, m = 0.9
I0815 00:43:11.707304 11101 solver.cpp:312] Iteration 112200 (6.90344 iter/s, 14.4855s/100 iter), loss = 1.91051
I0815 00:43:11.707326 11101 solver.cpp:334]     Train net output #0: loss = 1.83025 (* 1 = 1.83025 loss)
I0815 00:43:11.707330 11101 sgd_solver.cpp:136] Iteration 112200, lr = 0.0649375, m = 0.9
I0815 00:43:26.412770 11101 solver.cpp:312] Iteration 112300 (6.80038 iter/s, 14.7051s/100 iter), loss = 2.04952
I0815 00:43:26.412801 11101 solver.cpp:334]     Train net output #0: loss = 1.94956 (* 1 = 1.94956 loss)
I0815 00:43:26.412806 11101 sgd_solver.cpp:136] Iteration 112300, lr = 0.0649063, m = 0.9
I0815 00:43:40.732944 11101 solver.cpp:312] Iteration 112400 (6.98335 iter/s, 14.3198s/100 iter), loss = 1.94772
I0815 00:43:40.733047 11101 solver.cpp:334]     Train net output #0: loss = 1.87791 (* 1 = 1.87791 loss)
I0815 00:43:40.733067 11101 sgd_solver.cpp:136] Iteration 112400, lr = 0.064875, m = 0.9
I0815 00:43:55.293628 11101 solver.cpp:312] Iteration 112500 (6.868 iter/s, 14.5603s/100 iter), loss = 2.1896
I0815 00:43:55.293678 11101 solver.cpp:334]     Train net output #0: loss = 2.015 (* 1 = 2.015 loss)
I0815 00:43:55.293691 11101 sgd_solver.cpp:136] Iteration 112500, lr = 0.0648438, m = 0.9
I0815 00:44:09.733860 11101 solver.cpp:312] Iteration 112600 (6.92529 iter/s, 14.4398s/100 iter), loss = 2.11418
I0815 00:44:09.733914 11101 solver.cpp:334]     Train net output #0: loss = 2.22656 (* 1 = 2.22656 loss)
I0815 00:44:09.733928 11101 sgd_solver.cpp:136] Iteration 112600, lr = 0.0648125, m = 0.9
I0815 00:44:24.384649 11101 solver.cpp:312] Iteration 112700 (6.82576 iter/s, 14.6504s/100 iter), loss = 1.96061
I0815 00:44:24.384897 11101 solver.cpp:334]     Train net output #0: loss = 2.16074 (* 1 = 2.16074 loss)
I0815 00:44:24.384917 11101 sgd_solver.cpp:136] Iteration 112700, lr = 0.0647812, m = 0.9
I0815 00:44:38.932734 11101 solver.cpp:312] Iteration 112800 (6.87395 iter/s, 14.5477s/100 iter), loss = 1.98113
I0815 00:44:38.932874 11101 solver.cpp:334]     Train net output #0: loss = 2.1012 (* 1 = 2.1012 loss)
I0815 00:44:38.932890 11101 sgd_solver.cpp:136] Iteration 112800, lr = 0.06475, m = 0.9
I0815 00:44:53.366278 11101 solver.cpp:312] Iteration 112900 (6.9285 iter/s, 14.4331s/100 iter), loss = 2.53135
I0815 00:44:53.366305 11101 solver.cpp:334]     Train net output #0: loss = 2.3616 (* 1 = 2.3616 loss)
I0815 00:44:53.366312 11101 sgd_solver.cpp:136] Iteration 112900, lr = 0.0647187, m = 0.9
I0815 00:45:07.704423 11101 solver.cpp:312] Iteration 113000 (6.9746 iter/s, 14.3377s/100 iter), loss = 2.3135
I0815 00:45:07.704479 11101 solver.cpp:334]     Train net output #0: loss = 2.14234 (* 1 = 2.14234 loss)
I0815 00:45:07.704484 11101 sgd_solver.cpp:136] Iteration 113000, lr = 0.0646875, m = 0.9
I0815 00:45:22.018270 11101 solver.cpp:312] Iteration 113100 (6.98644 iter/s, 14.3134s/100 iter), loss = 2.10555
I0815 00:45:22.018301 11101 solver.cpp:334]     Train net output #0: loss = 2.24667 (* 1 = 2.24667 loss)
I0815 00:45:22.018306 11101 sgd_solver.cpp:136] Iteration 113100, lr = 0.0646563, m = 0.9
I0815 00:45:36.882840 11101 solver.cpp:312] Iteration 113200 (6.72759 iter/s, 14.8642s/100 iter), loss = 1.76262
I0815 00:45:36.883074 11101 solver.cpp:334]     Train net output #0: loss = 1.80239 (* 1 = 1.80239 loss)
I0815 00:45:36.883188 11101 sgd_solver.cpp:136] Iteration 113200, lr = 0.064625, m = 0.9
I0815 00:45:51.496511 11101 solver.cpp:312] Iteration 113300 (6.8431 iter/s, 14.6133s/100 iter), loss = 2.59241
I0815 00:45:51.496575 11101 solver.cpp:334]     Train net output #0: loss = 1.98547 (* 1 = 1.98547 loss)
I0815 00:45:51.496582 11101 sgd_solver.cpp:136] Iteration 113300, lr = 0.0645938, m = 0.9
I0815 00:46:06.118679 11101 solver.cpp:312] Iteration 113400 (6.83912 iter/s, 14.6218s/100 iter), loss = 2.07329
I0815 00:46:06.118739 11101 solver.cpp:334]     Train net output #0: loss = 2.28124 (* 1 = 2.28124 loss)
I0815 00:46:06.118757 11101 sgd_solver.cpp:136] Iteration 113400, lr = 0.0645625, m = 0.9
I0815 00:46:21.183781 11101 solver.cpp:312] Iteration 113500 (6.63804 iter/s, 15.0647s/100 iter), loss = 1.96153
I0815 00:46:21.183837 11101 solver.cpp:334]     Train net output #0: loss = 2.01382 (* 1 = 2.01382 loss)
I0815 00:46:21.183851 11101 sgd_solver.cpp:136] Iteration 113500, lr = 0.0645313, m = 0.9
I0815 00:46:35.606703 11101 solver.cpp:312] Iteration 113600 (6.9336 iter/s, 14.4225s/100 iter), loss = 1.7353
I0815 00:46:35.606819 11101 solver.cpp:334]     Train net output #0: loss = 1.8456 (* 1 = 1.8456 loss)
I0815 00:46:35.606837 11101 sgd_solver.cpp:136] Iteration 113600, lr = 0.0645, m = 0.9
I0815 00:46:50.532191 11101 solver.cpp:312] Iteration 113700 (6.70013 iter/s, 14.9251s/100 iter), loss = 2.17168
I0815 00:46:50.532261 11101 solver.cpp:334]     Train net output #0: loss = 2.31361 (* 1 = 2.31361 loss)
I0815 00:46:50.532277 11101 sgd_solver.cpp:136] Iteration 113700, lr = 0.0644688, m = 0.9
I0815 00:47:05.114857 11101 solver.cpp:312] Iteration 113800 (6.85765 iter/s, 14.5823s/100 iter), loss = 2.22146
I0815 00:47:05.114887 11101 solver.cpp:334]     Train net output #0: loss = 2.44892 (* 1 = 2.44892 loss)
I0815 00:47:05.114893 11101 sgd_solver.cpp:136] Iteration 113800, lr = 0.0644375, m = 0.9
I0815 00:47:19.645337 11101 solver.cpp:312] Iteration 113900 (6.88227 iter/s, 14.5301s/100 iter), loss = 2.17075
I0815 00:47:19.645416 11101 solver.cpp:334]     Train net output #0: loss = 2.15384 (* 1 = 2.15384 loss)
I0815 00:47:19.645423 11101 sgd_solver.cpp:136] Iteration 113900, lr = 0.0644063, m = 0.9
I0815 00:47:33.861621 11101 solver.cpp:509] Iteration 114000, Testing net (#0)
I0815 00:47:54.865591 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.435941
I0815 00:47:54.865641 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.691999
I0815 00:47:54.865650 11101 solver.cpp:594]     Test net output #2: loss = 2.58941 (* 1 = 2.58941 loss)
I0815 00:47:54.865669 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0035s
I0815 00:47:55.029079 11101 solver.cpp:312] Iteration 114000 (2.82623 iter/s, 35.3828s/100 iter), loss = 1.518
I0815 00:47:55.029104 11101 solver.cpp:334]     Train net output #0: loss = 1.40127 (* 1 = 1.40127 loss)
I0815 00:47:55.029109 11101 sgd_solver.cpp:136] Iteration 114000, lr = 0.064375, m = 0.9
I0815 00:48:09.646898 11101 solver.cpp:312] Iteration 114100 (6.84116 iter/s, 14.6174s/100 iter), loss = 2.08762
I0815 00:48:09.646926 11101 solver.cpp:334]     Train net output #0: loss = 2.16168 (* 1 = 2.16168 loss)
I0815 00:48:09.646934 11101 sgd_solver.cpp:136] Iteration 114100, lr = 0.0643438, m = 0.9
I0815 00:48:24.464015 11101 solver.cpp:312] Iteration 114200 (6.74914 iter/s, 14.8167s/100 iter), loss = 2.03394
I0815 00:48:24.464078 11101 solver.cpp:334]     Train net output #0: loss = 1.92258 (* 1 = 1.92258 loss)
I0815 00:48:24.464095 11101 sgd_solver.cpp:136] Iteration 114200, lr = 0.0643125, m = 0.9
I0815 00:48:25.991415 11065 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 00:48:39.337196 11101 solver.cpp:312] Iteration 114300 (6.7237 iter/s, 14.8728s/100 iter), loss = 2.08146
I0815 00:48:39.337220 11101 solver.cpp:334]     Train net output #0: loss = 2.42022 (* 1 = 2.42022 loss)
I0815 00:48:39.337226 11101 sgd_solver.cpp:136] Iteration 114300, lr = 0.0642812, m = 0.9
I0815 00:48:54.420814 11101 solver.cpp:312] Iteration 114400 (6.6299 iter/s, 15.0832s/100 iter), loss = 2.27397
I0815 00:48:54.420840 11101 solver.cpp:334]     Train net output #0: loss = 2.38716 (* 1 = 2.38716 loss)
I0815 00:48:54.420845 11101 sgd_solver.cpp:136] Iteration 114400, lr = 0.06425, m = 0.9
I0815 00:49:09.957010 11101 solver.cpp:312] Iteration 114500 (6.43676 iter/s, 15.5358s/100 iter), loss = 2.4506
I0815 00:49:09.957115 11101 solver.cpp:334]     Train net output #0: loss = 2.8361 (* 1 = 2.8361 loss)
I0815 00:49:09.957135 11101 sgd_solver.cpp:136] Iteration 114500, lr = 0.0642188, m = 0.9
I0815 00:49:25.192477 11101 solver.cpp:312] Iteration 114600 (6.56382 iter/s, 15.235s/100 iter), loss = 2.1012
I0815 00:49:25.192499 11101 solver.cpp:334]     Train net output #0: loss = 2.0912 (* 1 = 2.0912 loss)
I0815 00:49:25.192503 11101 sgd_solver.cpp:136] Iteration 114600, lr = 0.0641875, m = 0.9
I0815 00:49:40.325392 11101 solver.cpp:312] Iteration 114700 (6.6083 iter/s, 15.1325s/100 iter), loss = 1.67247
I0815 00:49:40.325438 11101 solver.cpp:334]     Train net output #0: loss = 1.641 (* 1 = 1.641 loss)
I0815 00:49:40.325443 11101 sgd_solver.cpp:136] Iteration 114700, lr = 0.0641562, m = 0.9
I0815 00:49:55.132894 11101 solver.cpp:312] Iteration 114800 (6.75352 iter/s, 14.8071s/100 iter), loss = 2.32255
I0815 00:49:55.132966 11101 solver.cpp:334]     Train net output #0: loss = 2.22204 (* 1 = 2.22204 loss)
I0815 00:49:55.132985 11101 sgd_solver.cpp:136] Iteration 114800, lr = 0.064125, m = 0.9
I0815 00:50:09.737291 11101 solver.cpp:312] Iteration 114900 (6.84744 iter/s, 14.604s/100 iter), loss = 1.91605
I0815 00:50:09.737352 11101 solver.cpp:334]     Train net output #0: loss = 2.38727 (* 1 = 2.38727 loss)
I0815 00:50:09.737370 11101 sgd_solver.cpp:136] Iteration 114900, lr = 0.0640938, m = 0.9
I0815 00:50:24.584923 11101 solver.cpp:312] Iteration 115000 (6.73527 iter/s, 14.8472s/100 iter), loss = 2.36253
I0815 00:50:24.585067 11101 solver.cpp:334]     Train net output #0: loss = 1.94696 (* 1 = 1.94696 loss)
I0815 00:50:24.585088 11101 sgd_solver.cpp:136] Iteration 115000, lr = 0.0640625, m = 0.9
I0815 00:50:39.080106 11101 solver.cpp:312] Iteration 115100 (6.89903 iter/s, 14.4948s/100 iter), loss = 2.25624
I0815 00:50:39.080140 11101 solver.cpp:334]     Train net output #0: loss = 1.55016 (* 1 = 1.55016 loss)
I0815 00:50:39.080147 11101 sgd_solver.cpp:136] Iteration 115100, lr = 0.0640313, m = 0.9
I0815 00:50:53.745362 11101 solver.cpp:312] Iteration 115200 (6.81903 iter/s, 14.6648s/100 iter), loss = 2.0996
I0815 00:50:53.745390 11101 solver.cpp:334]     Train net output #0: loss = 2.19207 (* 1 = 2.19207 loss)
I0815 00:50:53.745394 11101 sgd_solver.cpp:136] Iteration 115200, lr = 0.064, m = 0.9
I0815 00:51:08.648947 11101 solver.cpp:312] Iteration 115300 (6.70998 iter/s, 14.9032s/100 iter), loss = 2.44448
I0815 00:51:08.649001 11101 solver.cpp:334]     Train net output #0: loss = 2.87029 (* 1 = 2.87029 loss)
I0815 00:51:08.649008 11101 sgd_solver.cpp:136] Iteration 115300, lr = 0.0639688, m = 0.9
I0815 00:51:23.544510 11101 solver.cpp:312] Iteration 115400 (6.71359 iter/s, 14.8952s/100 iter), loss = 1.47521
I0815 00:51:23.544582 11101 solver.cpp:334]     Train net output #0: loss = 1.63368 (* 1 = 1.63368 loss)
I0815 00:51:23.544602 11101 sgd_solver.cpp:136] Iteration 115400, lr = 0.0639375, m = 0.9
I0815 00:51:38.029312 11101 solver.cpp:312] Iteration 115500 (6.90398 iter/s, 14.4844s/100 iter), loss = 2.42383
I0815 00:51:38.029338 11101 solver.cpp:334]     Train net output #0: loss = 2.30297 (* 1 = 2.30297 loss)
I0815 00:51:38.029345 11101 sgd_solver.cpp:136] Iteration 115500, lr = 0.0639063, m = 0.9
I0815 00:51:52.494375 11101 solver.cpp:312] Iteration 115600 (6.9134 iter/s, 14.4647s/100 iter), loss = 2.39705
I0815 00:51:52.500185 11101 solver.cpp:334]     Train net output #0: loss = 2.58141 (* 1 = 2.58141 loss)
I0815 00:51:52.500208 11101 sgd_solver.cpp:136] Iteration 115600, lr = 0.063875, m = 0.9
I0815 00:52:07.062935 11101 solver.cpp:312] Iteration 115700 (6.86429 iter/s, 14.5682s/100 iter), loss = 1.61443
I0815 00:52:07.062958 11101 solver.cpp:334]     Train net output #0: loss = 1.47239 (* 1 = 1.47239 loss)
I0815 00:52:07.062963 11101 sgd_solver.cpp:136] Iteration 115700, lr = 0.0638437, m = 0.9
I0815 00:52:21.833820 11101 solver.cpp:312] Iteration 115800 (6.77026 iter/s, 14.7705s/100 iter), loss = 2.37803
I0815 00:52:21.833844 11101 solver.cpp:334]     Train net output #0: loss = 2.63659 (* 1 = 2.63659 loss)
I0815 00:52:21.833848 11101 sgd_solver.cpp:136] Iteration 115800, lr = 0.0638125, m = 0.9
I0815 00:52:36.722812 11101 solver.cpp:312] Iteration 115900 (6.71656 iter/s, 14.8886s/100 iter), loss = 2.06206
I0815 00:52:36.722892 11101 solver.cpp:334]     Train net output #0: loss = 2.17927 (* 1 = 2.17927 loss)
I0815 00:52:36.722903 11101 sgd_solver.cpp:136] Iteration 115900, lr = 0.0637813, m = 0.9
I0815 00:52:51.133522 11101 solver.cpp:509] Iteration 116000, Testing net (#0)
I0815 00:53:07.060724 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 00:53:12.342172 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.397882
I0815 00:53:12.342196 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.654823
I0815 00:53:12.342203 11101 solver.cpp:594]     Test net output #2: loss = 2.83063 (* 1 = 2.83063 loss)
I0815 00:53:12.342231 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.2097s
I0815 00:53:12.510592 11101 solver.cpp:312] Iteration 116000 (2.79433 iter/s, 35.7868s/100 iter), loss = 1.74719
I0815 00:53:12.510617 11101 solver.cpp:334]     Train net output #0: loss = 1.82488 (* 1 = 1.82488 loss)
I0815 00:53:12.510622 11101 sgd_solver.cpp:136] Iteration 116000, lr = 0.06375, m = 0.9
I0815 00:53:26.819389 11101 solver.cpp:312] Iteration 116100 (6.9889 iter/s, 14.3084s/100 iter), loss = 2.11065
I0815 00:53:26.819422 11101 solver.cpp:334]     Train net output #0: loss = 1.93053 (* 1 = 1.93053 loss)
I0815 00:53:26.819427 11101 sgd_solver.cpp:136] Iteration 116100, lr = 0.0637188, m = 0.9
I0815 00:53:41.178198 11101 solver.cpp:312] Iteration 116200 (6.96456 iter/s, 14.3584s/100 iter), loss = 2.03601
I0815 00:53:41.178272 11101 solver.cpp:334]     Train net output #0: loss = 1.89422 (* 1 = 1.89422 loss)
I0815 00:53:41.178279 11101 sgd_solver.cpp:136] Iteration 116200, lr = 0.0636875, m = 0.9
I0815 00:53:55.694907 11101 solver.cpp:312] Iteration 116300 (6.88881 iter/s, 14.5163s/100 iter), loss = 2.467
I0815 00:53:55.694931 11101 solver.cpp:334]     Train net output #0: loss = 2.58606 (* 1 = 2.58606 loss)
I0815 00:53:55.694938 11101 sgd_solver.cpp:136] Iteration 116300, lr = 0.0636562, m = 0.9
I0815 00:54:10.260275 11101 solver.cpp:312] Iteration 116400 (6.86579 iter/s, 14.565s/100 iter), loss = 1.98041
I0815 00:54:10.260426 11101 solver.cpp:334]     Train net output #0: loss = 1.70778 (* 1 = 1.70778 loss)
I0815 00:54:10.260447 11101 sgd_solver.cpp:136] Iteration 116400, lr = 0.063625, m = 0.9
I0815 00:54:24.841321 11101 solver.cpp:312] Iteration 116500 (6.85841 iter/s, 14.5806s/100 iter), loss = 2.07247
I0815 00:54:24.841415 11101 solver.cpp:334]     Train net output #0: loss = 2.13513 (* 1 = 2.13513 loss)
I0815 00:54:24.841434 11101 sgd_solver.cpp:136] Iteration 116500, lr = 0.0635938, m = 0.9
I0815 00:54:39.592057 11101 solver.cpp:312] Iteration 116600 (6.77951 iter/s, 14.7503s/100 iter), loss = 1.77666
I0815 00:54:39.592124 11101 solver.cpp:334]     Train net output #0: loss = 1.78879 (* 1 = 1.78879 loss)
I0815 00:54:39.592155 11101 sgd_solver.cpp:136] Iteration 116600, lr = 0.0635625, m = 0.9
I0815 00:54:54.282306 11101 solver.cpp:312] Iteration 116700 (6.80743 iter/s, 14.6898s/100 iter), loss = 1.78767
I0815 00:54:54.282369 11101 solver.cpp:334]     Train net output #0: loss = 1.72645 (* 1 = 1.72645 loss)
I0815 00:54:54.282387 11101 sgd_solver.cpp:136] Iteration 116700, lr = 0.0635312, m = 0.9
I0815 00:55:08.889895 11101 solver.cpp:312] Iteration 116800 (6.84595 iter/s, 14.6072s/100 iter), loss = 2.24353
I0815 00:55:08.889963 11101 solver.cpp:334]     Train net output #0: loss = 2.12097 (* 1 = 2.12097 loss)
I0815 00:55:08.889971 11101 sgd_solver.cpp:136] Iteration 116800, lr = 0.0635, m = 0.9
I0815 00:55:23.381963 11101 solver.cpp:312] Iteration 116900 (6.90052 iter/s, 14.4917s/100 iter), loss = 2.15335
I0815 00:55:23.382002 11101 solver.cpp:334]     Train net output #0: loss = 2.07733 (* 1 = 2.07733 loss)
I0815 00:55:23.382009 11101 sgd_solver.cpp:136] Iteration 116900, lr = 0.0634688, m = 0.9
I0815 00:55:38.025931 11101 solver.cpp:312] Iteration 117000 (6.82894 iter/s, 14.6436s/100 iter), loss = 2.08604
I0815 00:55:38.025993 11101 solver.cpp:334]     Train net output #0: loss = 2.14862 (* 1 = 2.14862 loss)
I0815 00:55:38.026010 11101 sgd_solver.cpp:136] Iteration 117000, lr = 0.0634375, m = 0.9
I0815 00:55:52.939003 11101 solver.cpp:312] Iteration 117100 (6.70571 iter/s, 14.9127s/100 iter), loss = 1.56672
I0815 00:55:52.939069 11101 solver.cpp:334]     Train net output #0: loss = 1.51877 (* 1 = 1.51877 loss)
I0815 00:55:52.939079 11101 sgd_solver.cpp:136] Iteration 117100, lr = 0.0634063, m = 0.9
I0815 00:56:07.639588 11101 solver.cpp:312] Iteration 117200 (6.80264 iter/s, 14.7002s/100 iter), loss = 2.20583
I0815 00:56:07.639657 11101 solver.cpp:334]     Train net output #0: loss = 2.00296 (* 1 = 2.00296 loss)
I0815 00:56:07.639675 11101 sgd_solver.cpp:136] Iteration 117200, lr = 0.063375, m = 0.9
I0815 00:56:22.195137 11101 solver.cpp:312] Iteration 117300 (6.87042 iter/s, 14.5551s/100 iter), loss = 1.97724
I0815 00:56:22.195188 11101 solver.cpp:334]     Train net output #0: loss = 2.12637 (* 1 = 2.12637 loss)
I0815 00:56:22.195201 11101 sgd_solver.cpp:136] Iteration 117300, lr = 0.0633438, m = 0.9
I0815 00:56:37.110596 11101 solver.cpp:312] Iteration 117400 (6.70464 iter/s, 14.915s/100 iter), loss = 2.08546
I0815 00:56:37.110661 11101 solver.cpp:334]     Train net output #0: loss = 1.89853 (* 1 = 1.89853 loss)
I0815 00:56:37.110668 11101 sgd_solver.cpp:136] Iteration 117400, lr = 0.0633125, m = 0.9
I0815 00:56:52.238064 11101 solver.cpp:312] Iteration 117500 (6.61068 iter/s, 15.127s/100 iter), loss = 2.11659
I0815 00:56:52.238095 11101 solver.cpp:334]     Train net output #0: loss = 1.56562 (* 1 = 1.56562 loss)
I0815 00:56:52.238101 11101 sgd_solver.cpp:136] Iteration 117500, lr = 0.0632813, m = 0.9
I0815 00:57:06.639473 11101 solver.cpp:312] Iteration 117600 (6.94396 iter/s, 14.401s/100 iter), loss = 1.83716
I0815 00:57:06.639544 11101 solver.cpp:334]     Train net output #0: loss = 1.69042 (* 1 = 1.69042 loss)
I0815 00:57:06.639564 11101 sgd_solver.cpp:136] Iteration 117600, lr = 0.06325, m = 0.9
I0815 00:57:21.196429 11101 solver.cpp:312] Iteration 117700 (6.86976 iter/s, 14.5566s/100 iter), loss = 1.70561
I0815 00:57:21.196550 11101 solver.cpp:334]     Train net output #0: loss = 1.58566 (* 1 = 1.58566 loss)
I0815 00:57:21.196570 11101 sgd_solver.cpp:136] Iteration 117700, lr = 0.0632188, m = 0.9
I0815 00:57:35.624861 11101 solver.cpp:312] Iteration 117800 (6.93096 iter/s, 14.428s/100 iter), loss = 1.89354
I0815 00:57:35.624939 11101 solver.cpp:334]     Train net output #0: loss = 2.02794 (* 1 = 2.02794 loss)
I0815 00:57:35.624958 11101 sgd_solver.cpp:136] Iteration 117800, lr = 0.0631875, m = 0.9
I0815 00:57:50.174732 11101 solver.cpp:312] Iteration 117900 (6.8731 iter/s, 14.5495s/100 iter), loss = 1.88794
I0815 00:57:50.174953 11101 solver.cpp:334]     Train net output #0: loss = 1.91812 (* 1 = 1.91812 loss)
I0815 00:57:50.175061 11101 sgd_solver.cpp:136] Iteration 117900, lr = 0.0631562, m = 0.9
I0815 00:58:04.869099 11101 solver.cpp:509] Iteration 118000, Testing net (#0)
I0815 00:58:25.883659 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.423294
I0815 00:58:25.883683 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.675941
I0815 00:58:25.883689 11101 solver.cpp:594]     Test net output #2: loss = 2.67357 (* 1 = 2.67357 loss)
I0815 00:58:25.883718 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.014s
I0815 00:58:26.047371 11101 solver.cpp:312] Iteration 118000 (2.78772 iter/s, 35.8717s/100 iter), loss = 2.29375
I0815 00:58:26.047397 11101 solver.cpp:334]     Train net output #0: loss = 2.18917 (* 1 = 2.18917 loss)
I0815 00:58:26.047402 11101 sgd_solver.cpp:136] Iteration 118000, lr = 0.063125, m = 0.9
I0815 00:58:40.392709 11101 solver.cpp:312] Iteration 118100 (6.9711 iter/s, 14.3449s/100 iter), loss = 2.12914
I0815 00:58:40.392768 11101 solver.cpp:334]     Train net output #0: loss = 2.15251 (* 1 = 2.15251 loss)
I0815 00:58:40.392774 11101 sgd_solver.cpp:136] Iteration 118100, lr = 0.0630937, m = 0.9
I0815 00:58:55.021334 11101 solver.cpp:312] Iteration 118200 (6.8361 iter/s, 14.6282s/100 iter), loss = 2.20799
I0815 00:58:55.021381 11101 solver.cpp:334]     Train net output #0: loss = 1.86409 (* 1 = 1.86409 loss)
I0815 00:58:55.021394 11101 sgd_solver.cpp:136] Iteration 118200, lr = 0.0630625, m = 0.9
I0815 00:59:09.462771 11101 solver.cpp:312] Iteration 118300 (6.92471 iter/s, 14.441s/100 iter), loss = 2.02294
I0815 00:59:09.462800 11101 solver.cpp:334]     Train net output #0: loss = 2.04196 (* 1 = 2.04196 loss)
I0815 00:59:09.462806 11101 sgd_solver.cpp:136] Iteration 118300, lr = 0.0630312, m = 0.9
I0815 00:59:24.284731 11101 solver.cpp:312] Iteration 118400 (6.74693 iter/s, 14.8215s/100 iter), loss = 2.09804
I0815 00:59:24.284943 11101 solver.cpp:334]     Train net output #0: loss = 1.80824 (* 1 = 1.80824 loss)
I0815 00:59:24.284950 11101 sgd_solver.cpp:136] Iteration 118400, lr = 0.063, m = 0.9
I0815 00:59:39.147322 11101 solver.cpp:312] Iteration 118500 (6.72849 iter/s, 14.8622s/100 iter), loss = 2.23655
I0815 00:59:39.147353 11101 solver.cpp:334]     Train net output #0: loss = 2.4618 (* 1 = 2.4618 loss)
I0815 00:59:39.147359 11101 sgd_solver.cpp:136] Iteration 118500, lr = 0.0629688, m = 0.9
I0815 00:59:53.717849 11101 solver.cpp:312] Iteration 118600 (6.86336 iter/s, 14.5701s/100 iter), loss = 1.72145
I0815 00:59:53.718051 11101 solver.cpp:334]     Train net output #0: loss = 1.80351 (* 1 = 1.80351 loss)
I0815 00:59:53.718153 11101 sgd_solver.cpp:136] Iteration 118600, lr = 0.0629375, m = 0.9
I0815 01:00:08.154314 11101 solver.cpp:312] Iteration 118700 (6.9271 iter/s, 14.4361s/100 iter), loss = 2.3122
I0815 01:00:08.154413 11101 solver.cpp:334]     Train net output #0: loss = 2.38458 (* 1 = 2.38458 loss)
I0815 01:00:08.154431 11101 sgd_solver.cpp:136] Iteration 118700, lr = 0.0629063, m = 0.9
I0815 01:00:22.723273 11101 solver.cpp:312] Iteration 118800 (6.8641 iter/s, 14.5686s/100 iter), loss = 1.89992
I0815 01:00:22.723304 11101 solver.cpp:334]     Train net output #0: loss = 2.0477 (* 1 = 2.0477 loss)
I0815 01:00:22.723312 11101 sgd_solver.cpp:136] Iteration 118800, lr = 0.062875, m = 0.9
I0815 01:00:37.683920 11101 solver.cpp:312] Iteration 118900 (6.68439 iter/s, 14.9602s/100 iter), loss = 1.70502
I0815 01:00:37.683948 11101 solver.cpp:334]     Train net output #0: loss = 1.6422 (* 1 = 1.6422 loss)
I0815 01:00:37.683953 11101 sgd_solver.cpp:136] Iteration 118900, lr = 0.0628438, m = 0.9
I0815 01:00:52.562758 11101 solver.cpp:312] Iteration 119000 (6.72114 iter/s, 14.8784s/100 iter), loss = 2.00818
I0815 01:00:52.563006 11101 solver.cpp:334]     Train net output #0: loss = 2.06995 (* 1 = 2.06995 loss)
I0815 01:00:52.563026 11101 sgd_solver.cpp:136] Iteration 119000, lr = 0.0628125, m = 0.9
I0815 01:01:07.280977 11101 solver.cpp:312] Iteration 119100 (6.79449 iter/s, 14.7178s/100 iter), loss = 2.16653
I0815 01:01:07.281004 11101 solver.cpp:334]     Train net output #0: loss = 2.57748 (* 1 = 2.57748 loss)
I0815 01:01:07.281008 11101 sgd_solver.cpp:136] Iteration 119100, lr = 0.0627813, m = 0.9
I0815 01:01:21.943624 11101 solver.cpp:312] Iteration 119200 (6.82024 iter/s, 14.6622s/100 iter), loss = 1.52969
I0815 01:01:21.943677 11101 solver.cpp:334]     Train net output #0: loss = 1.31318 (* 1 = 1.31318 loss)
I0815 01:01:21.943691 11101 sgd_solver.cpp:136] Iteration 119200, lr = 0.06275, m = 0.9
I0815 01:01:36.566592 11101 solver.cpp:312] Iteration 119300 (6.83875 iter/s, 14.6226s/100 iter), loss = 1.70645
I0815 01:01:36.566686 11101 solver.cpp:334]     Train net output #0: loss = 1.95112 (* 1 = 1.95112 loss)
I0815 01:01:36.566699 11101 sgd_solver.cpp:136] Iteration 119300, lr = 0.0627187, m = 0.9
I0815 01:01:51.476706 11101 solver.cpp:312] Iteration 119400 (6.70704 iter/s, 14.9097s/100 iter), loss = 1.7625
I0815 01:01:51.476930 11101 solver.cpp:334]     Train net output #0: loss = 1.58022 (* 1 = 1.58022 loss)
I0815 01:01:51.477041 11101 sgd_solver.cpp:136] Iteration 119400, lr = 0.0626875, m = 0.9
I0815 01:02:06.366595 11101 solver.cpp:312] Iteration 119500 (6.71615 iter/s, 14.8895s/100 iter), loss = 1.98199
I0815 01:02:06.366618 11101 solver.cpp:334]     Train net output #0: loss = 1.57688 (* 1 = 1.57688 loss)
I0815 01:02:06.366622 11101 sgd_solver.cpp:136] Iteration 119500, lr = 0.0626562, m = 0.9
I0815 01:02:21.164899 11101 solver.cpp:312] Iteration 119600 (6.75772 iter/s, 14.7979s/100 iter), loss = 1.87405
I0815 01:02:21.164963 11101 solver.cpp:334]     Train net output #0: loss = 2.27971 (* 1 = 2.27971 loss)
I0815 01:02:21.164970 11101 sgd_solver.cpp:136] Iteration 119600, lr = 0.062625, m = 0.9
I0815 01:02:35.743216 11101 solver.cpp:312] Iteration 119700 (6.85969 iter/s, 14.5779s/100 iter), loss = 1.78619
I0815 01:02:35.743242 11101 solver.cpp:334]     Train net output #0: loss = 1.67998 (* 1 = 1.67998 loss)
I0815 01:02:35.743247 11101 sgd_solver.cpp:136] Iteration 119700, lr = 0.0625938, m = 0.9
I0815 01:02:50.444684 11101 solver.cpp:312] Iteration 119800 (6.80223 iter/s, 14.7011s/100 iter), loss = 2.17635
I0815 01:02:50.444710 11101 solver.cpp:334]     Train net output #0: loss = 1.75885 (* 1 = 1.75885 loss)
I0815 01:02:50.444715 11101 sgd_solver.cpp:136] Iteration 119800, lr = 0.0625625, m = 0.9
I0815 01:03:04.956575 11101 solver.cpp:312] Iteration 119900 (6.89109 iter/s, 14.5115s/100 iter), loss = 1.87135
I0815 01:03:04.956672 11101 solver.cpp:334]     Train net output #0: loss = 1.91165 (* 1 = 1.91165 loss)
I0815 01:03:04.956691 11101 sgd_solver.cpp:136] Iteration 119900, lr = 0.0625313, m = 0.9
I0815 01:03:19.381081 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_120000.caffemodel
I0815 01:03:19.421120 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_120000.solverstate
I0815 01:03:19.427330 11101 solver.cpp:509] Iteration 120000, Testing net (#0)
I0815 01:03:40.313146 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.40747
I0815 01:03:40.313218 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.669175
I0815 01:03:40.313230 11101 solver.cpp:594]     Test net output #2: loss = 2.75017 (* 1 = 2.75017 loss)
I0815 01:03:40.313249 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8853s
I0815 01:03:40.466181 11101 solver.cpp:312] Iteration 120000 (2.81622 iter/s, 35.5086s/100 iter), loss = 1.88596
I0815 01:03:40.466248 11101 solver.cpp:334]     Train net output #0: loss = 2.00266 (* 1 = 2.00266 loss)
I0815 01:03:40.466269 11101 sgd_solver.cpp:136] Iteration 120000, lr = 0.0625, m = 0.9
I0815 01:03:55.342917 11101 solver.cpp:312] Iteration 120100 (6.72209 iter/s, 14.8763s/100 iter), loss = 2.27171
I0815 01:03:55.342941 11101 solver.cpp:334]     Train net output #0: loss = 2.67332 (* 1 = 2.67332 loss)
I0815 01:03:55.342948 11101 sgd_solver.cpp:136] Iteration 120100, lr = 0.0624687, m = 0.9
I0815 01:04:10.417476 11101 solver.cpp:312] Iteration 120200 (6.63388 iter/s, 15.0741s/100 iter), loss = 2.49163
I0815 01:04:10.417603 11101 solver.cpp:334]     Train net output #0: loss = 2.28218 (* 1 = 2.28218 loss)
I0815 01:04:10.417629 11101 sgd_solver.cpp:136] Iteration 120200, lr = 0.0624375, m = 0.9
I0815 01:04:25.374315 11101 solver.cpp:312] Iteration 120300 (6.68609 iter/s, 14.9564s/100 iter), loss = 2.38036
I0815 01:04:25.374464 11101 solver.cpp:334]     Train net output #0: loss = 1.60251 (* 1 = 1.60251 loss)
I0815 01:04:25.374550 11101 sgd_solver.cpp:136] Iteration 120300, lr = 0.0624063, m = 0.9
I0815 01:04:39.888685 11101 solver.cpp:312] Iteration 120400 (6.88992 iter/s, 14.514s/100 iter), loss = 1.88722
I0815 01:04:39.888712 11101 solver.cpp:334]     Train net output #0: loss = 1.56611 (* 1 = 1.56611 loss)
I0815 01:04:39.888717 11101 sgd_solver.cpp:136] Iteration 120400, lr = 0.062375, m = 0.9
I0815 01:04:54.652151 11101 solver.cpp:312] Iteration 120500 (6.77367 iter/s, 14.7631s/100 iter), loss = 2.11151
I0815 01:04:54.652212 11101 solver.cpp:334]     Train net output #0: loss = 1.8915 (* 1 = 1.8915 loss)
I0815 01:04:54.652220 11101 sgd_solver.cpp:136] Iteration 120500, lr = 0.0623438, m = 0.9
I0815 01:05:09.272903 11101 solver.cpp:312] Iteration 120600 (6.83978 iter/s, 14.6203s/100 iter), loss = 1.92708
I0815 01:05:09.272955 11101 solver.cpp:334]     Train net output #0: loss = 1.80145 (* 1 = 1.80145 loss)
I0815 01:05:09.272969 11101 sgd_solver.cpp:136] Iteration 120600, lr = 0.0623125, m = 0.9
I0815 01:05:24.146468 11101 solver.cpp:312] Iteration 120700 (6.72353 iter/s, 14.8732s/100 iter), loss = 1.70124
I0815 01:05:24.146540 11101 solver.cpp:334]     Train net output #0: loss = 1.41568 (* 1 = 1.41568 loss)
I0815 01:05:24.146561 11101 sgd_solver.cpp:136] Iteration 120700, lr = 0.0622813, m = 0.9
I0815 01:05:39.483090 11101 solver.cpp:312] Iteration 120800 (6.52052 iter/s, 15.3362s/100 iter), loss = 2.35719
I0815 01:05:39.483168 11101 solver.cpp:334]     Train net output #0: loss = 2.2793 (* 1 = 2.2793 loss)
I0815 01:05:39.483175 11101 sgd_solver.cpp:136] Iteration 120800, lr = 0.06225, m = 0.9
I0815 01:05:55.080655 11101 solver.cpp:312] Iteration 120900 (6.41143 iter/s, 15.5971s/100 iter), loss = 1.57834
I0815 01:05:55.080677 11101 solver.cpp:334]     Train net output #0: loss = 1.67656 (* 1 = 1.67656 loss)
I0815 01:05:55.080682 11101 sgd_solver.cpp:136] Iteration 120900, lr = 0.0622188, m = 0.9
I0815 01:06:10.267720 11101 solver.cpp:312] Iteration 121000 (6.58473 iter/s, 15.1866s/100 iter), loss = 2.04511
I0815 01:06:10.267827 11101 solver.cpp:334]     Train net output #0: loss = 1.78801 (* 1 = 1.78801 loss)
I0815 01:06:10.267850 11101 sgd_solver.cpp:136] Iteration 121000, lr = 0.0621875, m = 0.9
I0815 01:06:25.330672 11101 solver.cpp:312] Iteration 121100 (6.63899 iter/s, 15.0625s/100 iter), loss = 2.07114
I0815 01:06:25.330699 11101 solver.cpp:334]     Train net output #0: loss = 2.15445 (* 1 = 2.15445 loss)
I0815 01:06:25.330706 11101 sgd_solver.cpp:136] Iteration 121100, lr = 0.0621562, m = 0.9
I0815 01:06:40.351480 11101 solver.cpp:312] Iteration 121200 (6.65762 iter/s, 15.0204s/100 iter), loss = 2.51756
I0815 01:06:40.351575 11101 solver.cpp:334]     Train net output #0: loss = 2.18692 (* 1 = 2.18692 loss)
I0815 01:06:40.351595 11101 sgd_solver.cpp:136] Iteration 121200, lr = 0.062125, m = 0.9
I0815 01:06:55.842428 11101 solver.cpp:312] Iteration 121300 (6.45556 iter/s, 15.4905s/100 iter), loss = 2.05707
I0815 01:06:55.842494 11101 solver.cpp:334]     Train net output #0: loss = 1.62058 (* 1 = 1.62058 loss)
I0815 01:06:55.842514 11101 sgd_solver.cpp:136] Iteration 121300, lr = 0.0620937, m = 0.9
I0815 01:07:10.599079 11101 solver.cpp:312] Iteration 121400 (6.77679 iter/s, 14.7562s/100 iter), loss = 1.82591
I0815 01:07:10.599155 11101 solver.cpp:334]     Train net output #0: loss = 1.49801 (* 1 = 1.49801 loss)
I0815 01:07:10.599171 11101 sgd_solver.cpp:136] Iteration 121400, lr = 0.0620625, m = 0.9
I0815 01:07:25.199903 11101 solver.cpp:312] Iteration 121500 (6.84912 iter/s, 14.6004s/100 iter), loss = 2.15661
I0815 01:07:25.199928 11101 solver.cpp:334]     Train net output #0: loss = 2.04781 (* 1 = 2.04781 loss)
I0815 01:07:25.199934 11101 sgd_solver.cpp:136] Iteration 121500, lr = 0.0620313, m = 0.9
I0815 01:07:39.923653 11101 solver.cpp:312] Iteration 121600 (6.79194 iter/s, 14.7233s/100 iter), loss = 2.07478
I0815 01:07:39.923681 11101 solver.cpp:334]     Train net output #0: loss = 2.0449 (* 1 = 2.0449 loss)
I0815 01:07:39.923687 11101 sgd_solver.cpp:136] Iteration 121600, lr = 0.062, m = 0.9
I0815 01:07:54.845226 11101 solver.cpp:312] Iteration 121700 (6.70189 iter/s, 14.9212s/100 iter), loss = 2.02874
I0815 01:07:54.845285 11101 solver.cpp:334]     Train net output #0: loss = 1.91372 (* 1 = 1.91372 loss)
I0815 01:07:54.845293 11101 sgd_solver.cpp:136] Iteration 121700, lr = 0.0619688, m = 0.9
I0815 01:08:09.635624 11101 solver.cpp:312] Iteration 121800 (6.76133 iter/s, 14.79s/100 iter), loss = 1.98067
I0815 01:08:09.635649 11101 solver.cpp:334]     Train net output #0: loss = 2.01605 (* 1 = 2.01605 loss)
I0815 01:08:09.635656 11101 sgd_solver.cpp:136] Iteration 121800, lr = 0.0619375, m = 0.9
I0815 01:08:24.740531 11101 solver.cpp:312] Iteration 121900 (6.62055 iter/s, 15.1045s/100 iter), loss = 2.18813
I0815 01:08:24.740556 11101 solver.cpp:334]     Train net output #0: loss = 2.1791 (* 1 = 2.1791 loss)
I0815 01:08:24.740561 11101 sgd_solver.cpp:136] Iteration 121900, lr = 0.0619063, m = 0.9
I0815 01:08:39.100633 11101 solver.cpp:509] Iteration 122000, Testing net (#0)
I0815 01:08:50.514701 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 01:09:00.287820 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.407823
I0815 01:09:00.287847 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.661823
I0815 01:09:00.287853 11101 solver.cpp:594]     Test net output #2: loss = 2.79766 (* 1 = 2.79766 loss)
I0815 01:09:00.287911 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1867s
I0815 01:09:00.438711 11101 solver.cpp:312] Iteration 122000 (2.80134 iter/s, 35.6972s/100 iter), loss = 1.69625
I0815 01:09:00.438773 11101 solver.cpp:334]     Train net output #0: loss = 1.71538 (* 1 = 1.71538 loss)
I0815 01:09:00.438791 11101 sgd_solver.cpp:136] Iteration 122000, lr = 0.061875, m = 0.9
I0815 01:09:14.925781 11101 solver.cpp:312] Iteration 122100 (6.9029 iter/s, 14.4867s/100 iter), loss = 2.28063
I0815 01:09:14.925837 11101 solver.cpp:334]     Train net output #0: loss = 2.04056 (* 1 = 2.04056 loss)
I0815 01:09:14.925843 11101 sgd_solver.cpp:136] Iteration 122100, lr = 0.0618438, m = 0.9
I0815 01:09:29.293412 11101 solver.cpp:312] Iteration 122200 (6.96028 iter/s, 14.3672s/100 iter), loss = 1.91498
I0815 01:09:29.293437 11101 solver.cpp:334]     Train net output #0: loss = 1.80694 (* 1 = 1.80694 loss)
I0815 01:09:29.293442 11101 sgd_solver.cpp:136] Iteration 122200, lr = 0.0618125, m = 0.9
I0815 01:09:43.739150 11101 solver.cpp:312] Iteration 122300 (6.92265 iter/s, 14.4453s/100 iter), loss = 1.80402
I0815 01:09:43.739176 11101 solver.cpp:334]     Train net output #0: loss = 1.79774 (* 1 = 1.79774 loss)
I0815 01:09:43.739181 11101 sgd_solver.cpp:136] Iteration 122300, lr = 0.0617813, m = 0.9
I0815 01:09:58.318828 11101 solver.cpp:312] Iteration 122400 (6.85905 iter/s, 14.5793s/100 iter), loss = 2.00184
I0815 01:09:58.318909 11101 solver.cpp:334]     Train net output #0: loss = 1.94341 (* 1 = 1.94341 loss)
I0815 01:09:58.318917 11101 sgd_solver.cpp:136] Iteration 122400, lr = 0.06175, m = 0.9
I0815 01:10:13.053679 11101 solver.cpp:312] Iteration 122500 (6.78682 iter/s, 14.7344s/100 iter), loss = 2.09368
I0815 01:10:13.053706 11101 solver.cpp:334]     Train net output #0: loss = 2.00948 (* 1 = 2.00948 loss)
I0815 01:10:13.053710 11101 sgd_solver.cpp:136] Iteration 122500, lr = 0.0617188, m = 0.9
I0815 01:10:27.488585 11101 solver.cpp:312] Iteration 122600 (6.92784 iter/s, 14.4345s/100 iter), loss = 2.21968
I0815 01:10:27.488648 11101 solver.cpp:334]     Train net output #0: loss = 2.28166 (* 1 = 2.28166 loss)
I0815 01:10:27.488667 11101 sgd_solver.cpp:136] Iteration 122600, lr = 0.0616875, m = 0.9
I0815 01:10:41.943702 11101 solver.cpp:312] Iteration 122700 (6.91816 iter/s, 14.4547s/100 iter), loss = 1.99706
I0815 01:10:41.943802 11101 solver.cpp:334]     Train net output #0: loss = 1.97585 (* 1 = 1.97585 loss)
I0815 01:10:41.943825 11101 sgd_solver.cpp:136] Iteration 122700, lr = 0.0616562, m = 0.9
I0815 01:10:56.400974 11101 solver.cpp:312] Iteration 122800 (6.91712 iter/s, 14.4569s/100 iter), loss = 2.53882
I0815 01:10:56.401026 11101 solver.cpp:334]     Train net output #0: loss = 2.67088 (* 1 = 2.67088 loss)
I0815 01:10:56.401039 11101 sgd_solver.cpp:136] Iteration 122800, lr = 0.061625, m = 0.9
I0815 01:11:10.921084 11101 solver.cpp:312] Iteration 122900 (6.88719 iter/s, 14.5197s/100 iter), loss = 2.01474
I0815 01:11:10.921108 11101 solver.cpp:334]     Train net output #0: loss = 2.21313 (* 1 = 2.21313 loss)
I0815 01:11:10.921114 11101 sgd_solver.cpp:136] Iteration 122900, lr = 0.0615937, m = 0.9
I0815 01:11:25.493813 11101 solver.cpp:312] Iteration 123000 (6.86232 iter/s, 14.5723s/100 iter), loss = 1.79583
I0815 01:11:25.493891 11101 solver.cpp:334]     Train net output #0: loss = 1.66381 (* 1 = 1.66381 loss)
I0815 01:11:25.493898 11101 sgd_solver.cpp:136] Iteration 123000, lr = 0.0615625, m = 0.9
I0815 01:11:39.952510 11101 solver.cpp:312] Iteration 123100 (6.91644 iter/s, 14.4583s/100 iter), loss = 1.90947
I0815 01:11:39.952538 11101 solver.cpp:334]     Train net output #0: loss = 2.30662 (* 1 = 2.30662 loss)
I0815 01:11:39.952544 11101 sgd_solver.cpp:136] Iteration 123100, lr = 0.0615312, m = 0.9
I0815 01:11:54.528702 11101 solver.cpp:312] Iteration 123200 (6.86069 iter/s, 14.5758s/100 iter), loss = 2.05931
I0815 01:11:54.528729 11101 solver.cpp:334]     Train net output #0: loss = 2.00632 (* 1 = 2.00632 loss)
I0815 01:11:54.528733 11101 sgd_solver.cpp:136] Iteration 123200, lr = 0.0615, m = 0.9
I0815 01:12:08.968189 11101 solver.cpp:312] Iteration 123300 (6.92564 iter/s, 14.4391s/100 iter), loss = 2.17572
I0815 01:12:08.968283 11101 solver.cpp:334]     Train net output #0: loss = 1.93743 (* 1 = 1.93743 loss)
I0815 01:12:08.968303 11101 sgd_solver.cpp:136] Iteration 123300, lr = 0.0614688, m = 0.9
I0815 01:12:23.662942 11101 solver.cpp:312] Iteration 123400 (6.80534 iter/s, 14.6944s/100 iter), loss = 2.18666
I0815 01:12:23.663014 11101 solver.cpp:334]     Train net output #0: loss = 2.00527 (* 1 = 2.00527 loss)
I0815 01:12:23.663039 11101 sgd_solver.cpp:136] Iteration 123400, lr = 0.0614375, m = 0.9
I0815 01:12:38.296483 11101 solver.cpp:312] Iteration 123500 (6.8338 iter/s, 14.6331s/100 iter), loss = 2.00771
I0815 01:12:38.296509 11101 solver.cpp:334]     Train net output #0: loss = 1.58158 (* 1 = 1.58158 loss)
I0815 01:12:38.296514 11101 sgd_solver.cpp:136] Iteration 123500, lr = 0.0614063, m = 0.9
I0815 01:12:53.100317 11101 solver.cpp:312] Iteration 123600 (6.75519 iter/s, 14.8034s/100 iter), loss = 2.0686
I0815 01:12:53.100388 11101 solver.cpp:334]     Train net output #0: loss = 1.97837 (* 1 = 1.97837 loss)
I0815 01:12:53.100394 11101 sgd_solver.cpp:136] Iteration 123600, lr = 0.061375, m = 0.9
I0815 01:13:07.569716 11101 solver.cpp:312] Iteration 123700 (6.91133 iter/s, 14.469s/100 iter), loss = 2.34095
I0815 01:13:07.569744 11101 solver.cpp:334]     Train net output #0: loss = 2.64262 (* 1 = 2.64262 loss)
I0815 01:13:07.569751 11101 sgd_solver.cpp:136] Iteration 123700, lr = 0.0613438, m = 0.9
I0815 01:13:22.392575 11101 solver.cpp:312] Iteration 123800 (6.74652 iter/s, 14.8225s/100 iter), loss = 1.906
I0815 01:13:22.392645 11101 solver.cpp:334]     Train net output #0: loss = 2.0876 (* 1 = 2.0876 loss)
I0815 01:13:22.392665 11101 sgd_solver.cpp:136] Iteration 123800, lr = 0.0613125, m = 0.9
I0815 01:13:37.572895 11101 solver.cpp:312] Iteration 123900 (6.58766 iter/s, 15.1799s/100 iter), loss = 2.13266
I0815 01:13:37.573350 11101 solver.cpp:334]     Train net output #0: loss = 2.33246 (* 1 = 2.33246 loss)
I0815 01:13:37.573360 11101 sgd_solver.cpp:136] Iteration 123900, lr = 0.0612813, m = 0.9
I0815 01:13:51.914487 11101 solver.cpp:509] Iteration 124000, Testing net (#0)
I0815 01:14:13.003324 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.41347
I0815 01:14:13.003381 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.672529
I0815 01:14:13.003389 11101 solver.cpp:594]     Test net output #2: loss = 2.71061 (* 1 = 2.71061 loss)
I0815 01:14:13.003409 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0884s
I0815 01:14:13.144461 11101 solver.cpp:312] Iteration 124000 (2.81131 iter/s, 35.5706s/100 iter), loss = 2.1142
I0815 01:14:13.144490 11101 solver.cpp:334]     Train net output #0: loss = 2.03898 (* 1 = 2.03898 loss)
I0815 01:14:13.144495 11101 sgd_solver.cpp:136] Iteration 124000, lr = 0.06125, m = 0.9
I0815 01:14:27.608733 11101 solver.cpp:312] Iteration 124100 (6.91378 iter/s, 14.4639s/100 iter), loss = 2.13961
I0815 01:14:27.608762 11101 solver.cpp:334]     Train net output #0: loss = 1.97466 (* 1 = 1.97466 loss)
I0815 01:14:27.608768 11101 sgd_solver.cpp:136] Iteration 124100, lr = 0.0612187, m = 0.9
I0815 01:14:42.151449 11101 solver.cpp:312] Iteration 124200 (6.87649 iter/s, 14.5423s/100 iter), loss = 2.04817
I0815 01:14:42.151474 11101 solver.cpp:334]     Train net output #0: loss = 1.68152 (* 1 = 1.68152 loss)
I0815 01:14:42.151479 11101 sgd_solver.cpp:136] Iteration 124200, lr = 0.0611875, m = 0.9
I0815 01:14:56.538980 11101 solver.cpp:312] Iteration 124300 (6.95065 iter/s, 14.3871s/100 iter), loss = 2.13767
I0815 01:14:56.539067 11101 solver.cpp:334]     Train net output #0: loss = 2.26915 (* 1 = 2.26915 loss)
I0815 01:14:56.539084 11101 sgd_solver.cpp:136] Iteration 124300, lr = 0.0611563, m = 0.9
I0815 01:15:10.962273 11101 solver.cpp:312] Iteration 124400 (6.93342 iter/s, 14.4229s/100 iter), loss = 2.05301
I0815 01:15:10.962301 11101 solver.cpp:334]     Train net output #0: loss = 2.10408 (* 1 = 2.10408 loss)
I0815 01:15:10.962307 11101 sgd_solver.cpp:136] Iteration 124400, lr = 0.061125, m = 0.9
I0815 01:15:25.404559 11101 solver.cpp:312] Iteration 124500 (6.9243 iter/s, 14.4419s/100 iter), loss = 2.23586
I0815 01:15:25.404608 11101 solver.cpp:334]     Train net output #0: loss = 1.84951 (* 1 = 1.84951 loss)
I0815 01:15:25.404630 11101 sgd_solver.cpp:136] Iteration 124500, lr = 0.0610937, m = 0.9
I0815 01:15:39.983376 11101 solver.cpp:312] Iteration 124600 (6.85946 iter/s, 14.5784s/100 iter), loss = 2.21854
I0815 01:15:39.983466 11101 solver.cpp:334]     Train net output #0: loss = 2.01108 (* 1 = 2.01108 loss)
I0815 01:15:39.983490 11101 sgd_solver.cpp:136] Iteration 124600, lr = 0.0610625, m = 0.9
I0815 01:15:54.442910 11101 solver.cpp:312] Iteration 124700 (6.91604 iter/s, 14.4591s/100 iter), loss = 2.08159
I0815 01:15:54.442962 11101 solver.cpp:334]     Train net output #0: loss = 1.91997 (* 1 = 1.91997 loss)
I0815 01:15:54.442982 11101 sgd_solver.cpp:136] Iteration 124700, lr = 0.0610312, m = 0.9
I0815 01:16:09.339125 11101 solver.cpp:312] Iteration 124800 (6.7133 iter/s, 14.8958s/100 iter), loss = 1.8905
I0815 01:16:09.339152 11101 solver.cpp:334]     Train net output #0: loss = 1.7051 (* 1 = 1.7051 loss)
I0815 01:16:09.339156 11101 sgd_solver.cpp:136] Iteration 124800, lr = 0.061, m = 0.9
I0815 01:16:23.903321 11101 solver.cpp:312] Iteration 124900 (6.86634 iter/s, 14.5638s/100 iter), loss = 2.02797
I0815 01:16:23.903390 11101 solver.cpp:334]     Train net output #0: loss = 2.02678 (* 1 = 2.02678 loss)
I0815 01:16:23.903398 11101 sgd_solver.cpp:136] Iteration 124900, lr = 0.0609688, m = 0.9
I0815 01:16:38.652562 11101 solver.cpp:312] Iteration 125000 (6.7802 iter/s, 14.7488s/100 iter), loss = 2.25688
I0815 01:16:38.652588 11101 solver.cpp:334]     Train net output #0: loss = 1.96812 (* 1 = 1.96812 loss)
I0815 01:16:38.652595 11101 sgd_solver.cpp:136] Iteration 125000, lr = 0.0609375, m = 0.9
I0815 01:16:53.219188 11101 solver.cpp:312] Iteration 125100 (6.8652 iter/s, 14.5662s/100 iter), loss = 2.05506
I0815 01:16:53.219211 11101 solver.cpp:334]     Train net output #0: loss = 1.9385 (* 1 = 1.9385 loss)
I0815 01:16:53.219218 11101 sgd_solver.cpp:136] Iteration 125100, lr = 0.0609063, m = 0.9
I0815 01:17:07.465684 11101 solver.cpp:312] Iteration 125200 (7.01946 iter/s, 14.2461s/100 iter), loss = 1.63668
I0815 01:17:07.465740 11101 solver.cpp:334]     Train net output #0: loss = 1.70969 (* 1 = 1.70969 loss)
I0815 01:17:07.465746 11101 sgd_solver.cpp:136] Iteration 125200, lr = 0.060875, m = 0.9
I0815 01:17:21.835631 11101 solver.cpp:312] Iteration 125300 (6.95916 iter/s, 14.3696s/100 iter), loss = 2.29308
I0815 01:17:21.835808 11101 solver.cpp:334]     Train net output #0: loss = 2.207 (* 1 = 2.207 loss)
I0815 01:17:21.835893 11101 sgd_solver.cpp:136] Iteration 125300, lr = 0.0608438, m = 0.9
I0815 01:17:36.700678 11101 solver.cpp:312] Iteration 125400 (6.72738 iter/s, 14.8646s/100 iter), loss = 2.08724
I0815 01:17:36.700709 11101 solver.cpp:334]     Train net output #0: loss = 1.96378 (* 1 = 1.96378 loss)
I0815 01:17:36.700716 11101 sgd_solver.cpp:136] Iteration 125400, lr = 0.0608125, m = 0.9
I0815 01:17:51.552681 11101 solver.cpp:312] Iteration 125500 (6.73328 iter/s, 14.8516s/100 iter), loss = 2.09956
I0815 01:17:51.553418 11101 solver.cpp:334]     Train net output #0: loss = 2.07844 (* 1 = 2.07844 loss)
I0815 01:17:51.553439 11101 sgd_solver.cpp:136] Iteration 125500, lr = 0.0607813, m = 0.9
I0815 01:18:06.075026 11101 solver.cpp:312] Iteration 125600 (6.88613 iter/s, 14.5219s/100 iter), loss = 2.40414
I0815 01:18:06.075054 11101 solver.cpp:334]     Train net output #0: loss = 2.65036 (* 1 = 2.65036 loss)
I0815 01:18:06.075060 11101 sgd_solver.cpp:136] Iteration 125600, lr = 0.06075, m = 0.9
I0815 01:18:20.708364 11101 solver.cpp:312] Iteration 125700 (6.8339 iter/s, 14.6329s/100 iter), loss = 1.96296
I0815 01:18:20.708390 11101 solver.cpp:334]     Train net output #0: loss = 1.99401 (* 1 = 1.99401 loss)
I0815 01:18:20.708396 11101 sgd_solver.cpp:136] Iteration 125700, lr = 0.0607188, m = 0.9
I0815 01:18:36.060093 11101 solver.cpp:312] Iteration 125800 (6.5141 iter/s, 15.3513s/100 iter), loss = 1.86736
I0815 01:18:36.060154 11101 solver.cpp:334]     Train net output #0: loss = 2.21591 (* 1 = 2.21591 loss)
I0815 01:18:36.060161 11101 sgd_solver.cpp:136] Iteration 125800, lr = 0.0606875, m = 0.9
I0815 01:18:50.992915 11101 solver.cpp:312] Iteration 125900 (6.69684 iter/s, 14.9324s/100 iter), loss = 2.16368
I0815 01:18:50.992944 11101 solver.cpp:334]     Train net output #0: loss = 2.07513 (* 1 = 2.07513 loss)
I0815 01:18:50.992950 11101 sgd_solver.cpp:136] Iteration 125900, lr = 0.0606562, m = 0.9
I0815 01:19:06.082835 11101 solver.cpp:509] Iteration 126000, Testing net (#0)
I0815 01:19:27.295382 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.421529
I0815 01:19:27.295405 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.680587
I0815 01:19:27.295410 11101 solver.cpp:594]     Test net output #2: loss = 2.65721 (* 1 = 2.65721 loss)
I0815 01:19:27.295433 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.212s
I0815 01:19:27.455255 11101 solver.cpp:312] Iteration 126000 (2.74263 iter/s, 36.4613s/100 iter), loss = 2.50126
I0815 01:19:27.455279 11101 solver.cpp:334]     Train net output #0: loss = 2.25985 (* 1 = 2.25985 loss)
I0815 01:19:27.455284 11101 sgd_solver.cpp:136] Iteration 126000, lr = 0.060625, m = 0.9
I0815 01:19:42.307188 11101 solver.cpp:312] Iteration 126100 (6.73332 iter/s, 14.8515s/100 iter), loss = 1.78589
I0815 01:19:42.307278 11101 solver.cpp:334]     Train net output #0: loss = 1.81058 (* 1 = 1.81058 loss)
I0815 01:19:42.307291 11101 sgd_solver.cpp:136] Iteration 126100, lr = 0.0605938, m = 0.9
I0815 01:19:57.060602 11101 solver.cpp:312] Iteration 126200 (6.77828 iter/s, 14.753s/100 iter), loss = 2.14298
I0815 01:19:57.060629 11101 solver.cpp:334]     Train net output #0: loss = 2.06032 (* 1 = 2.06032 loss)
I0815 01:19:57.060636 11101 sgd_solver.cpp:136] Iteration 126200, lr = 0.0605625, m = 0.9
I0815 01:20:11.659479 11101 solver.cpp:312] Iteration 126300 (6.85003 iter/s, 14.5985s/100 iter), loss = 2.23076
I0815 01:20:11.659507 11101 solver.cpp:334]     Train net output #0: loss = 2.60542 (* 1 = 2.60542 loss)
I0815 01:20:11.659513 11101 sgd_solver.cpp:136] Iteration 126300, lr = 0.0605312, m = 0.9
I0815 01:20:26.057188 11101 solver.cpp:312] Iteration 126400 (6.94574 iter/s, 14.3973s/100 iter), loss = 2.10768
I0815 01:20:26.057247 11101 solver.cpp:334]     Train net output #0: loss = 1.98551 (* 1 = 1.98551 loss)
I0815 01:20:26.057255 11101 sgd_solver.cpp:136] Iteration 126400, lr = 0.0605, m = 0.9
I0815 01:20:40.308475 11101 solver.cpp:312] Iteration 126500 (7.0171 iter/s, 14.2509s/100 iter), loss = 2.33282
I0815 01:20:40.308540 11101 solver.cpp:334]     Train net output #0: loss = 2.19625 (* 1 = 2.19625 loss)
I0815 01:20:40.308558 11101 sgd_solver.cpp:136] Iteration 126500, lr = 0.0604688, m = 0.9
I0815 01:20:55.146875 11101 solver.cpp:312] Iteration 126600 (6.73946 iter/s, 14.838s/100 iter), loss = 2.40255
I0815 01:20:55.146903 11101 solver.cpp:334]     Train net output #0: loss = 2.07335 (* 1 = 2.07335 loss)
I0815 01:20:55.146908 11101 sgd_solver.cpp:136] Iteration 126600, lr = 0.0604375, m = 0.9
I0815 01:21:09.748241 11101 solver.cpp:312] Iteration 126700 (6.84886 iter/s, 14.601s/100 iter), loss = 1.87244
I0815 01:21:09.748304 11101 solver.cpp:334]     Train net output #0: loss = 1.99323 (* 1 = 1.99323 loss)
I0815 01:21:09.748311 11101 sgd_solver.cpp:136] Iteration 126700, lr = 0.0604062, m = 0.9
I0815 01:21:24.447903 11101 solver.cpp:312] Iteration 126800 (6.80306 iter/s, 14.6993s/100 iter), loss = 1.85593
I0815 01:21:24.447932 11101 solver.cpp:334]     Train net output #0: loss = 2.12156 (* 1 = 2.12156 loss)
I0815 01:21:24.447938 11101 sgd_solver.cpp:136] Iteration 126800, lr = 0.060375, m = 0.9
I0815 01:21:39.187243 11101 solver.cpp:312] Iteration 126900 (6.78475 iter/s, 14.7389s/100 iter), loss = 1.83012
I0815 01:21:39.187467 11101 solver.cpp:334]     Train net output #0: loss = 1.83888 (* 1 = 1.83888 loss)
I0815 01:21:39.187577 11101 sgd_solver.cpp:136] Iteration 126900, lr = 0.0603438, m = 0.9
I0815 01:21:53.631381 11101 solver.cpp:312] Iteration 127000 (6.92342 iter/s, 14.4437s/100 iter), loss = 2.01051
I0815 01:21:53.631438 11101 solver.cpp:334]     Train net output #0: loss = 2.1968 (* 1 = 2.1968 loss)
I0815 01:21:53.631444 11101 sgd_solver.cpp:136] Iteration 127000, lr = 0.0603125, m = 0.9
I0815 01:22:08.077718 11101 solver.cpp:312] Iteration 127100 (6.92236 iter/s, 14.4459s/100 iter), loss = 2.02763
I0815 01:22:08.077744 11101 solver.cpp:334]     Train net output #0: loss = 2.32181 (* 1 = 2.32181 loss)
I0815 01:22:08.077749 11101 sgd_solver.cpp:136] Iteration 127100, lr = 0.0602813, m = 0.9
I0815 01:22:22.685770 11101 solver.cpp:312] Iteration 127200 (6.84573 iter/s, 14.6076s/100 iter), loss = 1.81172
I0815 01:22:22.685793 11101 solver.cpp:334]     Train net output #0: loss = 1.83551 (* 1 = 1.83551 loss)
I0815 01:22:22.685797 11101 sgd_solver.cpp:136] Iteration 127200, lr = 0.06025, m = 0.9
I0815 01:22:37.714789 11101 solver.cpp:312] Iteration 127300 (6.65398 iter/s, 15.0286s/100 iter), loss = 1.89043
I0815 01:22:37.714884 11101 solver.cpp:334]     Train net output #0: loss = 2.10497 (* 1 = 2.10497 loss)
I0815 01:22:37.714897 11101 sgd_solver.cpp:136] Iteration 127300, lr = 0.0602188, m = 0.9
I0815 01:22:52.216272 11101 solver.cpp:312] Iteration 127400 (6.89603 iter/s, 14.5011s/100 iter), loss = 1.82033
I0815 01:22:52.216298 11101 solver.cpp:334]     Train net output #0: loss = 2.10467 (* 1 = 2.10467 loss)
I0815 01:22:52.216305 11101 sgd_solver.cpp:136] Iteration 127400, lr = 0.0601875, m = 0.9
I0815 01:23:06.712586 11101 solver.cpp:312] Iteration 127500 (6.89849 iter/s, 14.4959s/100 iter), loss = 2.21867
I0815 01:23:06.712615 11101 solver.cpp:334]     Train net output #0: loss = 2.08314 (* 1 = 2.08314 loss)
I0815 01:23:06.712620 11101 sgd_solver.cpp:136] Iteration 127500, lr = 0.0601563, m = 0.9
I0815 01:23:20.939998 11101 solver.cpp:312] Iteration 127600 (7.02888 iter/s, 14.227s/100 iter), loss = 2.19133
I0815 01:23:20.940054 11101 solver.cpp:334]     Train net output #0: loss = 2.54035 (* 1 = 2.54035 loss)
I0815 01:23:20.940062 11101 sgd_solver.cpp:136] Iteration 127600, lr = 0.060125, m = 0.9
I0815 01:23:35.497601 11101 solver.cpp:312] Iteration 127700 (6.86945 iter/s, 14.5572s/100 iter), loss = 2.01495
I0815 01:23:35.497629 11101 solver.cpp:334]     Train net output #0: loss = 2.36575 (* 1 = 2.36575 loss)
I0815 01:23:35.497635 11101 sgd_solver.cpp:136] Iteration 127700, lr = 0.0600937, m = 0.9
I0815 01:23:50.022035 11101 solver.cpp:312] Iteration 127800 (6.88514 iter/s, 14.524s/100 iter), loss = 2.21151
I0815 01:23:50.022059 11101 solver.cpp:334]     Train net output #0: loss = 2.22626 (* 1 = 2.22626 loss)
I0815 01:23:50.022099 11101 sgd_solver.cpp:136] Iteration 127800, lr = 0.0600625, m = 0.9
I0815 01:24:04.664769 11101 solver.cpp:312] Iteration 127900 (6.82951 iter/s, 14.6423s/100 iter), loss = 1.93622
I0815 01:24:04.666220 11101 solver.cpp:334]     Train net output #0: loss = 1.60865 (* 1 = 1.60865 loss)
I0815 01:24:04.666239 11101 sgd_solver.cpp:136] Iteration 127900, lr = 0.0600312, m = 0.9
I0815 01:24:19.003237 11101 solver.cpp:509] Iteration 128000, Testing net (#0)
I0815 01:24:27.702991 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 01:24:39.989249 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.428823
I0815 01:24:39.989303 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.687822
I0815 01:24:39.989310 11101 solver.cpp:594]     Test net output #2: loss = 2.61156 (* 1 = 2.61156 loss)
I0815 01:24:39.989333 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9855s
I0815 01:24:40.154422 11101 solver.cpp:312] Iteration 128000 (2.8178 iter/s, 35.4887s/100 iter), loss = 2.01854
I0815 01:24:40.154448 11101 solver.cpp:334]     Train net output #0: loss = 2.24985 (* 1 = 2.24985 loss)
I0815 01:24:40.154451 11101 sgd_solver.cpp:136] Iteration 128000, lr = 0.06, m = 0.9
I0815 01:24:54.683086 11101 solver.cpp:312] Iteration 128100 (6.88313 iter/s, 14.5283s/100 iter), loss = 1.84152
I0815 01:24:54.683148 11101 solver.cpp:334]     Train net output #0: loss = 1.98208 (* 1 = 1.98208 loss)
I0815 01:24:54.683166 11101 sgd_solver.cpp:136] Iteration 128100, lr = 0.0599687, m = 0.9
I0815 01:25:09.243610 11101 solver.cpp:312] Iteration 128200 (6.86808 iter/s, 14.5601s/100 iter), loss = 1.58013
I0815 01:25:09.243639 11101 solver.cpp:334]     Train net output #0: loss = 1.27117 (* 1 = 1.27117 loss)
I0815 01:25:09.243646 11101 sgd_solver.cpp:136] Iteration 128200, lr = 0.0599375, m = 0.9
I0815 01:25:24.077433 11101 solver.cpp:312] Iteration 128300 (6.74154 iter/s, 14.8334s/100 iter), loss = 1.85274
I0815 01:25:24.082649 11101 solver.cpp:334]     Train net output #0: loss = 1.7799 (* 1 = 1.7799 loss)
I0815 01:25:24.082748 11101 sgd_solver.cpp:136] Iteration 128300, lr = 0.0599063, m = 0.9
I0815 01:25:38.823248 11101 solver.cpp:312] Iteration 128400 (6.78177 iter/s, 14.7454s/100 iter), loss = 2.10753
I0815 01:25:38.823320 11101 solver.cpp:334]     Train net output #0: loss = 2.15775 (* 1 = 2.15775 loss)
I0815 01:25:38.823338 11101 sgd_solver.cpp:136] Iteration 128400, lr = 0.059875, m = 0.9
I0815 01:25:53.658186 11101 solver.cpp:312] Iteration 128500 (6.74103 iter/s, 14.8345s/100 iter), loss = 2.34457
I0815 01:25:53.658216 11101 solver.cpp:334]     Train net output #0: loss = 2.41013 (* 1 = 2.41013 loss)
I0815 01:25:53.658222 11101 sgd_solver.cpp:136] Iteration 128500, lr = 0.0598437, m = 0.9
I0815 01:26:08.182998 11101 solver.cpp:312] Iteration 128600 (6.88496 iter/s, 14.5244s/100 iter), loss = 1.90443
I0815 01:26:08.183054 11101 solver.cpp:334]     Train net output #0: loss = 1.48001 (* 1 = 1.48001 loss)
I0815 01:26:08.183060 11101 sgd_solver.cpp:136] Iteration 128600, lr = 0.0598125, m = 0.9
I0815 01:26:22.995262 11101 solver.cpp:312] Iteration 128700 (6.75135 iter/s, 14.8119s/100 iter), loss = 1.78427
I0815 01:26:22.995333 11101 solver.cpp:334]     Train net output #0: loss = 1.49364 (* 1 = 1.49364 loss)
I0815 01:26:22.995353 11101 sgd_solver.cpp:136] Iteration 128700, lr = 0.0597813, m = 0.9
I0815 01:26:37.633580 11101 solver.cpp:312] Iteration 128800 (6.83157 iter/s, 14.6379s/100 iter), loss = 1.9366
I0815 01:26:37.633633 11101 solver.cpp:334]     Train net output #0: loss = 2.08781 (* 1 = 2.08781 loss)
I0815 01:26:37.633646 11101 sgd_solver.cpp:136] Iteration 128800, lr = 0.05975, m = 0.9
I0815 01:26:52.156014 11101 solver.cpp:312] Iteration 128900 (6.88609 iter/s, 14.522s/100 iter), loss = 2.3001
I0815 01:26:52.156093 11101 solver.cpp:334]     Train net output #0: loss = 2.29857 (* 1 = 2.29857 loss)
I0815 01:26:52.156107 11101 sgd_solver.cpp:136] Iteration 128900, lr = 0.0597188, m = 0.9
I0815 01:27:06.675855 11101 solver.cpp:312] Iteration 129000 (6.88732 iter/s, 14.5194s/100 iter), loss = 1.98519
I0815 01:27:06.676010 11101 solver.cpp:334]     Train net output #0: loss = 2.13395 (* 1 = 2.13395 loss)
I0815 01:27:06.676033 11101 sgd_solver.cpp:136] Iteration 129000, lr = 0.0596875, m = 0.9
I0815 01:27:21.565567 11101 solver.cpp:312] Iteration 129100 (6.71623 iter/s, 14.8893s/100 iter), loss = 2.21381
I0815 01:27:21.565593 11101 solver.cpp:334]     Train net output #0: loss = 2.14166 (* 1 = 2.14166 loss)
I0815 01:27:21.565600 11101 sgd_solver.cpp:136] Iteration 129100, lr = 0.0596563, m = 0.9
I0815 01:27:22.751248 11065 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 01:27:36.337668 11101 solver.cpp:312] Iteration 129200 (6.7697 iter/s, 14.7717s/100 iter), loss = 2.25776
I0815 01:27:36.337739 11101 solver.cpp:334]     Train net output #0: loss = 1.82966 (* 1 = 1.82966 loss)
I0815 01:27:36.337757 11101 sgd_solver.cpp:136] Iteration 129200, lr = 0.059625, m = 0.9
I0815 01:27:51.193445 11101 solver.cpp:312] Iteration 129300 (6.73157 iter/s, 14.8554s/100 iter), loss = 1.68502
I0815 01:27:51.193476 11101 solver.cpp:334]     Train net output #0: loss = 1.98497 (* 1 = 1.98497 loss)
I0815 01:27:51.193482 11101 sgd_solver.cpp:136] Iteration 129300, lr = 0.0595937, m = 0.9
I0815 01:28:06.114733 11101 solver.cpp:312] Iteration 129400 (6.70202 iter/s, 14.9209s/100 iter), loss = 1.97623
I0815 01:28:06.114979 11101 solver.cpp:334]     Train net output #0: loss = 1.86514 (* 1 = 1.86514 loss)
I0815 01:28:06.115085 11101 sgd_solver.cpp:136] Iteration 129400, lr = 0.0595625, m = 0.9
I0815 01:28:20.985358 11101 solver.cpp:312] Iteration 129500 (6.72485 iter/s, 14.8702s/100 iter), loss = 1.68121
I0815 01:28:20.985386 11101 solver.cpp:334]     Train net output #0: loss = 1.99886 (* 1 = 1.99886 loss)
I0815 01:28:20.985393 11101 sgd_solver.cpp:136] Iteration 129500, lr = 0.0595312, m = 0.9
I0815 01:28:35.747845 11101 solver.cpp:312] Iteration 129600 (6.77411 iter/s, 14.7621s/100 iter), loss = 2.60047
I0815 01:28:35.747897 11101 solver.cpp:334]     Train net output #0: loss = 2.9941 (* 1 = 2.9941 loss)
I0815 01:28:35.747911 11101 sgd_solver.cpp:136] Iteration 129600, lr = 0.0595, m = 0.9
I0815 01:28:50.088644 11101 solver.cpp:312] Iteration 129700 (6.9733 iter/s, 14.3404s/100 iter), loss = 1.7283
I0815 01:28:50.088719 11101 solver.cpp:334]     Train net output #0: loss = 1.86987 (* 1 = 1.86987 loss)
I0815 01:28:50.088727 11101 sgd_solver.cpp:136] Iteration 129700, lr = 0.0594687, m = 0.9
I0815 01:29:04.571866 11101 solver.cpp:312] Iteration 129800 (6.90473 iter/s, 14.4828s/100 iter), loss = 2.40535
I0815 01:29:04.571893 11101 solver.cpp:334]     Train net output #0: loss = 2.37874 (* 1 = 2.37874 loss)
I0815 01:29:04.571899 11101 sgd_solver.cpp:136] Iteration 129800, lr = 0.0594375, m = 0.9
I0815 01:29:19.413872 11101 solver.cpp:312] Iteration 129900 (6.73782 iter/s, 14.8416s/100 iter), loss = 2.10689
I0815 01:29:19.413907 11101 solver.cpp:334]     Train net output #0: loss = 1.96425 (* 1 = 1.96425 loss)
I0815 01:29:19.413913 11101 sgd_solver.cpp:136] Iteration 129900, lr = 0.0594063, m = 0.9
I0815 01:29:33.712652 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_130000.caffemodel
I0815 01:29:33.736657 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_130000.solverstate
I0815 01:29:33.741505 11101 solver.cpp:509] Iteration 130000, Testing net (#0)
I0815 01:29:54.664527 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.425882
I0815 01:29:54.664551 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.68
I0815 01:29:54.664559 11101 solver.cpp:594]     Test net output #2: loss = 2.68607 (* 1 = 2.68607 loss)
I0815 01:29:54.664582 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9225s
I0815 01:29:54.814134 11101 solver.cpp:312] Iteration 130000 (2.82491 iter/s, 35.3993s/100 iter), loss = 1.88386
I0815 01:29:54.814183 11101 solver.cpp:334]     Train net output #0: loss = 2.45509 (* 1 = 2.45509 loss)
I0815 01:29:54.814195 11101 sgd_solver.cpp:136] Iteration 130000, lr = 0.059375, m = 0.9
I0815 01:30:09.365294 11101 solver.cpp:312] Iteration 130100 (6.8725 iter/s, 14.5508s/100 iter), loss = 2.0849
I0815 01:30:09.365424 11101 solver.cpp:334]     Train net output #0: loss = 2.23511 (* 1 = 2.23511 loss)
I0815 01:30:09.365432 11101 sgd_solver.cpp:136] Iteration 130100, lr = 0.0593438, m = 0.9
I0815 01:30:24.230392 11101 solver.cpp:312] Iteration 130200 (6.72735 iter/s, 14.8647s/100 iter), loss = 2.30121
I0815 01:30:24.230417 11101 solver.cpp:334]     Train net output #0: loss = 2.33346 (* 1 = 2.33346 loss)
I0815 01:30:24.230420 11101 sgd_solver.cpp:136] Iteration 130200, lr = 0.0593125, m = 0.9
I0815 01:30:38.802052 11101 solver.cpp:312] Iteration 130300 (6.86283 iter/s, 14.5713s/100 iter), loss = 1.89772
I0815 01:30:38.802125 11101 solver.cpp:334]     Train net output #0: loss = 1.55001 (* 1 = 1.55001 loss)
I0815 01:30:38.802145 11101 sgd_solver.cpp:136] Iteration 130300, lr = 0.0592813, m = 0.9
I0815 01:30:53.580866 11101 solver.cpp:312] Iteration 130400 (6.76663 iter/s, 14.7784s/100 iter), loss = 1.57663
I0815 01:30:53.580925 11101 solver.cpp:334]     Train net output #0: loss = 1.61696 (* 1 = 1.61696 loss)
I0815 01:30:53.580932 11101 sgd_solver.cpp:136] Iteration 130400, lr = 0.05925, m = 0.9
I0815 01:31:08.784582 11101 solver.cpp:312] Iteration 130500 (6.57752 iter/s, 15.2033s/100 iter), loss = 2.07939
I0815 01:31:08.784616 11101 solver.cpp:334]     Train net output #0: loss = 2.25454 (* 1 = 2.25454 loss)
I0815 01:31:08.784622 11101 sgd_solver.cpp:136] Iteration 130500, lr = 0.0592188, m = 0.9
I0815 01:31:23.808212 11101 solver.cpp:312] Iteration 130600 (6.65636 iter/s, 15.0232s/100 iter), loss = 2.46387
I0815 01:31:23.808264 11101 solver.cpp:334]     Train net output #0: loss = 2.87567 (* 1 = 2.87567 loss)
I0815 01:31:23.808270 11101 sgd_solver.cpp:136] Iteration 130600, lr = 0.0591875, m = 0.9
I0815 01:31:38.804821 11101 solver.cpp:312] Iteration 130700 (6.66836 iter/s, 14.9962s/100 iter), loss = 1.83188
I0815 01:31:38.804847 11101 solver.cpp:334]     Train net output #0: loss = 2.06994 (* 1 = 2.06994 loss)
I0815 01:31:38.804852 11101 sgd_solver.cpp:136] Iteration 130700, lr = 0.0591563, m = 0.9
I0815 01:31:53.653080 11101 solver.cpp:312] Iteration 130800 (6.73498 iter/s, 14.8478s/100 iter), loss = 1.97116
I0815 01:31:53.653110 11101 solver.cpp:334]     Train net output #0: loss = 2.13877 (* 1 = 2.13877 loss)
I0815 01:31:53.653116 11101 sgd_solver.cpp:136] Iteration 130800, lr = 0.059125, m = 0.9
I0815 01:32:08.264564 11101 solver.cpp:312] Iteration 130900 (6.84412 iter/s, 14.6111s/100 iter), loss = 2.03892
I0815 01:32:08.264652 11101 solver.cpp:334]     Train net output #0: loss = 2.26505 (* 1 = 2.26505 loss)
I0815 01:32:08.264662 11101 sgd_solver.cpp:136] Iteration 130900, lr = 0.0590938, m = 0.9
I0815 01:32:22.960958 11101 solver.cpp:312] Iteration 131000 (6.80458 iter/s, 14.696s/100 iter), loss = 2.29882
I0815 01:32:22.960983 11101 solver.cpp:334]     Train net output #0: loss = 2.66037 (* 1 = 2.66037 loss)
I0815 01:32:22.960986 11101 sgd_solver.cpp:136] Iteration 131000, lr = 0.0590625, m = 0.9
I0815 01:32:37.629178 11101 solver.cpp:312] Iteration 131100 (6.81765 iter/s, 14.6678s/100 iter), loss = 2.13396
I0815 01:32:37.629204 11101 solver.cpp:334]     Train net output #0: loss = 2.43185 (* 1 = 2.43185 loss)
I0815 01:32:37.629209 11101 sgd_solver.cpp:136] Iteration 131100, lr = 0.0590312, m = 0.9
I0815 01:32:52.210047 11101 solver.cpp:312] Iteration 131200 (6.85849 iter/s, 14.5805s/100 iter), loss = 2.10268
I0815 01:32:52.210115 11101 solver.cpp:334]     Train net output #0: loss = 2.65069 (* 1 = 2.65069 loss)
I0815 01:32:52.210124 11101 sgd_solver.cpp:136] Iteration 131200, lr = 0.059, m = 0.9
I0815 01:33:07.055734 11101 solver.cpp:312] Iteration 131300 (6.73615 iter/s, 14.8453s/100 iter), loss = 2.24896
I0815 01:33:07.055759 11101 solver.cpp:334]     Train net output #0: loss = 2.19393 (* 1 = 2.19393 loss)
I0815 01:33:07.055765 11101 sgd_solver.cpp:136] Iteration 131300, lr = 0.0589687, m = 0.9
I0815 01:33:21.696820 11101 solver.cpp:312] Iteration 131400 (6.83028 iter/s, 14.6407s/100 iter), loss = 2.0402
I0815 01:33:21.696847 11101 solver.cpp:334]     Train net output #0: loss = 2.34565 (* 1 = 2.34565 loss)
I0815 01:33:21.696853 11101 sgd_solver.cpp:136] Iteration 131400, lr = 0.0589375, m = 0.9
I0815 01:33:36.473485 11101 solver.cpp:312] Iteration 131500 (6.76761 iter/s, 14.7763s/100 iter), loss = 2.43043
I0815 01:33:36.473570 11101 solver.cpp:334]     Train net output #0: loss = 2.13814 (* 1 = 2.13814 loss)
I0815 01:33:36.473588 11101 sgd_solver.cpp:136] Iteration 131500, lr = 0.0589063, m = 0.9
I0815 01:33:51.149351 11101 solver.cpp:312] Iteration 131600 (6.81409 iter/s, 14.6755s/100 iter), loss = 1.47091
I0815 01:33:51.149376 11101 solver.cpp:334]     Train net output #0: loss = 1.2148 (* 1 = 1.2148 loss)
I0815 01:33:51.149382 11101 sgd_solver.cpp:136] Iteration 131600, lr = 0.058875, m = 0.9
I0815 01:34:06.038713 11101 solver.cpp:312] Iteration 131700 (6.71639 iter/s, 14.889s/100 iter), loss = 1.98124
I0815 01:34:06.038738 11101 solver.cpp:334]     Train net output #0: loss = 2.12456 (* 1 = 2.12456 loss)
I0815 01:34:06.038744 11101 sgd_solver.cpp:136] Iteration 131700, lr = 0.0588438, m = 0.9
I0815 01:34:21.020525 11101 solver.cpp:312] Iteration 131800 (6.67494 iter/s, 14.9814s/100 iter), loss = 2.48267
I0815 01:34:21.020613 11101 solver.cpp:334]     Train net output #0: loss = 2.78301 (* 1 = 2.78301 loss)
I0815 01:34:21.020630 11101 sgd_solver.cpp:136] Iteration 131800, lr = 0.0588125, m = 0.9
I0815 01:34:35.909664 11101 solver.cpp:312] Iteration 131900 (6.71649 iter/s, 14.8887s/100 iter), loss = 2.37588
I0815 01:34:35.909696 11101 solver.cpp:334]     Train net output #0: loss = 2.43325 (* 1 = 2.43325 loss)
I0815 01:34:35.909701 11101 sgd_solver.cpp:136] Iteration 131900, lr = 0.0587813, m = 0.9
I0815 01:34:50.652992 11101 solver.cpp:509] Iteration 132000, Testing net (#0)
I0815 01:35:11.665643 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.451059
I0815 01:35:11.665684 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.711175
I0815 01:35:11.665690 11101 solver.cpp:594]     Test net output #2: loss = 2.51975 (* 1 = 2.51975 loss)
I0815 01:35:11.665707 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0121s
I0815 01:35:11.820152 11101 solver.cpp:312] Iteration 132000 (2.78478 iter/s, 35.9095s/100 iter), loss = 2.01268
I0815 01:35:11.820200 11101 solver.cpp:334]     Train net output #0: loss = 2.02154 (* 1 = 2.02154 loss)
I0815 01:35:11.820212 11101 sgd_solver.cpp:136] Iteration 132000, lr = 0.05875, m = 0.9
I0815 01:35:26.239913 11101 solver.cpp:312] Iteration 132100 (6.93512 iter/s, 14.4194s/100 iter), loss = 2.39293
I0815 01:35:26.239943 11101 solver.cpp:334]     Train net output #0: loss = 2.03738 (* 1 = 2.03738 loss)
I0815 01:35:26.239948 11101 sgd_solver.cpp:136] Iteration 132100, lr = 0.0587188, m = 0.9
I0815 01:35:40.878473 11101 solver.cpp:312] Iteration 132200 (6.83146 iter/s, 14.6382s/100 iter), loss = 1.89555
I0815 01:35:40.878523 11101 solver.cpp:334]     Train net output #0: loss = 1.61092 (* 1 = 1.61092 loss)
I0815 01:35:40.878536 11101 sgd_solver.cpp:136] Iteration 132200, lr = 0.0586875, m = 0.9
I0815 01:35:55.341121 11101 solver.cpp:312] Iteration 132300 (6.91456 iter/s, 14.4622s/100 iter), loss = 2.00411
I0815 01:35:55.341245 11101 solver.cpp:334]     Train net output #0: loss = 2.35316 (* 1 = 2.35316 loss)
I0815 01:35:55.341260 11101 sgd_solver.cpp:136] Iteration 132300, lr = 0.0586563, m = 0.9
I0815 01:36:09.944386 11101 solver.cpp:312] Iteration 132400 (6.84797 iter/s, 14.6029s/100 iter), loss = 2.12716
I0815 01:36:09.944411 11101 solver.cpp:334]     Train net output #0: loss = 2.10133 (* 1 = 2.10133 loss)
I0815 01:36:09.944445 11101 sgd_solver.cpp:136] Iteration 132400, lr = 0.058625, m = 0.9
I0815 01:36:24.388681 11101 solver.cpp:312] Iteration 132500 (6.92334 iter/s, 14.4439s/100 iter), loss = 2.06001
I0815 01:36:24.388711 11101 solver.cpp:334]     Train net output #0: loss = 1.88889 (* 1 = 1.88889 loss)
I0815 01:36:24.388718 11101 sgd_solver.cpp:136] Iteration 132500, lr = 0.0585938, m = 0.9
I0815 01:36:39.144418 11101 solver.cpp:312] Iteration 132600 (6.77721 iter/s, 14.7553s/100 iter), loss = 1.90879
I0815 01:36:39.144520 11101 solver.cpp:334]     Train net output #0: loss = 1.94269 (* 1 = 1.94269 loss)
I0815 01:36:39.144539 11101 sgd_solver.cpp:136] Iteration 132600, lr = 0.0585625, m = 0.9
I0815 01:36:53.900061 11101 solver.cpp:312] Iteration 132700 (6.77725 iter/s, 14.7552s/100 iter), loss = 2.00462
I0815 01:36:53.900089 11101 solver.cpp:334]     Train net output #0: loss = 2.11246 (* 1 = 2.11246 loss)
I0815 01:36:53.900095 11101 sgd_solver.cpp:136] Iteration 132700, lr = 0.0585313, m = 0.9
I0815 01:37:08.539463 11101 solver.cpp:312] Iteration 132800 (6.83107 iter/s, 14.639s/100 iter), loss = 1.85087
I0815 01:37:08.539489 11101 solver.cpp:334]     Train net output #0: loss = 1.6783 (* 1 = 1.6783 loss)
I0815 01:37:08.539494 11101 sgd_solver.cpp:136] Iteration 132800, lr = 0.0585, m = 0.9
I0815 01:37:23.120882 11101 solver.cpp:312] Iteration 132900 (6.85823 iter/s, 14.581s/100 iter), loss = 1.91917
I0815 01:37:23.121093 11101 solver.cpp:334]     Train net output #0: loss = 1.82083 (* 1 = 1.82083 loss)
I0815 01:37:23.121181 11101 sgd_solver.cpp:136] Iteration 132900, lr = 0.0584687, m = 0.9
I0815 01:37:37.817665 11101 solver.cpp:312] Iteration 133000 (6.8044 iter/s, 14.6964s/100 iter), loss = 1.78706
I0815 01:37:37.817692 11101 solver.cpp:334]     Train net output #0: loss = 1.84105 (* 1 = 1.84105 loss)
I0815 01:37:37.817698 11101 sgd_solver.cpp:136] Iteration 133000, lr = 0.0584375, m = 0.9
I0815 01:37:52.873371 11101 solver.cpp:312] Iteration 133100 (6.64218 iter/s, 15.0553s/100 iter), loss = 2.03446
I0815 01:37:52.873395 11101 solver.cpp:334]     Train net output #0: loss = 2.40154 (* 1 = 2.40154 loss)
I0815 01:37:52.873399 11101 sgd_solver.cpp:136] Iteration 133100, lr = 0.0584062, m = 0.9
I0815 01:38:07.728346 11101 solver.cpp:312] Iteration 133200 (6.73194 iter/s, 14.8546s/100 iter), loss = 1.96207
I0815 01:38:07.728449 11101 solver.cpp:334]     Train net output #0: loss = 1.89278 (* 1 = 1.89278 loss)
I0815 01:38:07.728471 11101 sgd_solver.cpp:136] Iteration 133200, lr = 0.058375, m = 0.9
I0815 01:38:22.368989 11101 solver.cpp:312] Iteration 133300 (6.83049 iter/s, 14.6402s/100 iter), loss = 2.25601
I0815 01:38:22.369014 11101 solver.cpp:334]     Train net output #0: loss = 2.51372 (* 1 = 2.51372 loss)
I0815 01:38:22.369019 11101 sgd_solver.cpp:136] Iteration 133300, lr = 0.0583437, m = 0.9
I0815 01:38:37.113142 11101 solver.cpp:312] Iteration 133400 (6.78254 iter/s, 14.7437s/100 iter), loss = 1.88012
I0815 01:38:37.113214 11101 solver.cpp:334]     Train net output #0: loss = 1.89098 (* 1 = 1.89098 loss)
I0815 01:38:37.113232 11101 sgd_solver.cpp:136] Iteration 133400, lr = 0.0583125, m = 0.9
I0815 01:38:51.797011 11101 solver.cpp:312] Iteration 133500 (6.81038 iter/s, 14.6835s/100 iter), loss = 1.77265
I0815 01:38:51.811228 11101 solver.cpp:334]     Train net output #0: loss = 1.67588 (* 1 = 1.67588 loss)
I0815 01:38:51.811244 11101 sgd_solver.cpp:136] Iteration 133500, lr = 0.0582813, m = 0.9
I0815 01:39:06.417044 11101 solver.cpp:312] Iteration 133600 (6.84012 iter/s, 14.6196s/100 iter), loss = 2.22328
I0815 01:39:06.417096 11101 solver.cpp:334]     Train net output #0: loss = 2.33477 (* 1 = 2.33477 loss)
I0815 01:39:06.417109 11101 sgd_solver.cpp:136] Iteration 133600, lr = 0.05825, m = 0.9
I0815 01:39:21.004236 11101 solver.cpp:312] Iteration 133700 (6.85552 iter/s, 14.5868s/100 iter), loss = 2.02366
I0815 01:39:21.004261 11101 solver.cpp:334]     Train net output #0: loss = 1.70271 (* 1 = 1.70271 loss)
I0815 01:39:21.004266 11101 sgd_solver.cpp:136] Iteration 133700, lr = 0.0582188, m = 0.9
I0815 01:39:35.458710 11101 solver.cpp:312] Iteration 133800 (6.91846 iter/s, 14.4541s/100 iter), loss = 1.66624
I0815 01:39:35.458811 11101 solver.cpp:334]     Train net output #0: loss = 1.56304 (* 1 = 1.56304 loss)
I0815 01:39:35.458833 11101 sgd_solver.cpp:136] Iteration 133800, lr = 0.0581875, m = 0.9
I0815 01:39:50.150918 11101 solver.cpp:312] Iteration 133900 (6.80652 iter/s, 14.6918s/100 iter), loss = 2.01123
I0815 01:39:50.150946 11101 solver.cpp:334]     Train net output #0: loss = 2.61972 (* 1 = 2.61972 loss)
I0815 01:39:50.150954 11101 sgd_solver.cpp:136] Iteration 133900, lr = 0.0581563, m = 0.9
I0815 01:40:04.652773 11101 solver.cpp:509] Iteration 134000, Testing net (#0)
I0815 01:40:09.406991 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 01:40:25.442608 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.409941
I0815 01:40:25.442637 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.669352
I0815 01:40:25.442644 11101 solver.cpp:594]     Test net output #2: loss = 2.79417 (* 1 = 2.79417 loss)
I0815 01:40:25.442715 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7894s
I0815 01:40:25.580222 11101 solver.cpp:312] Iteration 134000 (2.8226 iter/s, 35.4283s/100 iter), loss = 1.82643
I0815 01:40:25.580283 11101 solver.cpp:334]     Train net output #0: loss = 1.72301 (* 1 = 1.72301 loss)
I0815 01:40:25.580302 11101 sgd_solver.cpp:136] Iteration 134000, lr = 0.058125, m = 0.9
I0815 01:40:39.943210 11101 solver.cpp:312] Iteration 134100 (6.96253 iter/s, 14.3626s/100 iter), loss = 2.21435
I0815 01:40:39.943272 11101 solver.cpp:334]     Train net output #0: loss = 2.45125 (* 1 = 2.45125 loss)
I0815 01:40:39.943279 11101 sgd_solver.cpp:136] Iteration 134100, lr = 0.0580938, m = 0.9
I0815 01:40:54.712862 11101 solver.cpp:312] Iteration 134200 (6.77083 iter/s, 14.7692s/100 iter), loss = 1.6528
I0815 01:40:54.712918 11101 solver.cpp:334]     Train net output #0: loss = 2.05779 (* 1 = 2.05779 loss)
I0815 01:40:54.712934 11101 sgd_solver.cpp:136] Iteration 134200, lr = 0.0580625, m = 0.9
I0815 01:41:09.052551 11101 solver.cpp:312] Iteration 134300 (6.97384 iter/s, 14.3393s/100 iter), loss = 1.94147
I0815 01:41:09.052577 11101 solver.cpp:334]     Train net output #0: loss = 1.86933 (* 1 = 1.86933 loss)
I0815 01:41:09.052582 11101 sgd_solver.cpp:136] Iteration 134300, lr = 0.0580312, m = 0.9
I0815 01:41:23.870496 11101 solver.cpp:312] Iteration 134400 (6.74876 iter/s, 14.8175s/100 iter), loss = 2.10526
I0815 01:41:23.870589 11101 solver.cpp:334]     Train net output #0: loss = 2.28624 (* 1 = 2.28624 loss)
I0815 01:41:23.870597 11101 sgd_solver.cpp:136] Iteration 134400, lr = 0.058, m = 0.9
I0815 01:41:38.523846 11101 solver.cpp:312] Iteration 134500 (6.82457 iter/s, 14.6529s/100 iter), loss = 1.975
I0815 01:41:38.523871 11101 solver.cpp:334]     Train net output #0: loss = 2.09714 (* 1 = 2.09714 loss)
I0815 01:41:38.523877 11101 sgd_solver.cpp:136] Iteration 134500, lr = 0.0579687, m = 0.9
I0815 01:41:53.265661 11101 solver.cpp:312] Iteration 134600 (6.78361 iter/s, 14.7414s/100 iter), loss = 2.20256
I0815 01:41:53.265733 11101 solver.cpp:334]     Train net output #0: loss = 1.8208 (* 1 = 1.8208 loss)
I0815 01:41:53.265753 11101 sgd_solver.cpp:136] Iteration 134600, lr = 0.0579375, m = 0.9
I0815 01:42:07.784632 11101 solver.cpp:312] Iteration 134700 (6.88773 iter/s, 14.5186s/100 iter), loss = 2.0246
I0815 01:42:07.784718 11101 solver.cpp:334]     Train net output #0: loss = 2.08516 (* 1 = 2.08516 loss)
I0815 01:42:07.784732 11101 sgd_solver.cpp:136] Iteration 134700, lr = 0.0579062, m = 0.9
I0815 01:42:22.322538 11101 solver.cpp:312] Iteration 134800 (6.87876 iter/s, 14.5375s/100 iter), loss = 1.66852
I0815 01:42:22.322603 11101 solver.cpp:334]     Train net output #0: loss = 1.9166 (* 1 = 1.9166 loss)
I0815 01:42:22.322621 11101 sgd_solver.cpp:136] Iteration 134800, lr = 0.057875, m = 0.9
I0815 01:42:36.947059 11101 solver.cpp:312] Iteration 134900 (6.83802 iter/s, 14.6241s/100 iter), loss = 1.6918
I0815 01:42:36.947124 11101 solver.cpp:334]     Train net output #0: loss = 1.67127 (* 1 = 1.67127 loss)
I0815 01:42:36.947141 11101 sgd_solver.cpp:136] Iteration 134900, lr = 0.0578438, m = 0.9
I0815 01:42:51.619473 11101 solver.cpp:312] Iteration 135000 (6.8157 iter/s, 14.672s/100 iter), loss = 2.38439
I0815 01:42:51.619554 11101 solver.cpp:334]     Train net output #0: loss = 3.003 (* 1 = 3.003 loss)
I0815 01:42:51.619572 11101 sgd_solver.cpp:136] Iteration 135000, lr = 0.0578125, m = 0.9
I0815 01:43:06.548516 11101 solver.cpp:312] Iteration 135100 (6.69854 iter/s, 14.9286s/100 iter), loss = 2.5737
I0815 01:43:06.548548 11101 solver.cpp:334]     Train net output #0: loss = 2.51895 (* 1 = 2.51895 loss)
I0815 01:43:06.548555 11101 sgd_solver.cpp:136] Iteration 135100, lr = 0.0577812, m = 0.9
I0815 01:43:21.423326 11101 solver.cpp:312] Iteration 135200 (6.72296 iter/s, 14.8744s/100 iter), loss = 2.57142
I0815 01:43:21.423390 11101 solver.cpp:334]     Train net output #0: loss = 2.75558 (* 1 = 2.75558 loss)
I0815 01:43:21.423408 11101 sgd_solver.cpp:136] Iteration 135200, lr = 0.05775, m = 0.9
I0815 01:43:35.910815 11101 solver.cpp:312] Iteration 135300 (6.9027 iter/s, 14.4871s/100 iter), loss = 2.0168
I0815 01:43:35.910878 11101 solver.cpp:334]     Train net output #0: loss = 1.93005 (* 1 = 1.93005 loss)
I0815 01:43:35.910886 11101 sgd_solver.cpp:136] Iteration 135300, lr = 0.0577188, m = 0.9
I0815 01:43:50.559772 11101 solver.cpp:312] Iteration 135400 (6.82661 iter/s, 14.6486s/100 iter), loss = 2.00898
I0815 01:43:50.559797 11101 solver.cpp:334]     Train net output #0: loss = 2.33121 (* 1 = 2.33121 loss)
I0815 01:43:50.559801 11101 sgd_solver.cpp:136] Iteration 135400, lr = 0.0576875, m = 0.9
I0815 01:44:05.229852 11101 solver.cpp:312] Iteration 135500 (6.81678 iter/s, 14.6697s/100 iter), loss = 1.70142
I0815 01:44:05.229933 11101 solver.cpp:334]     Train net output #0: loss = 1.36983 (* 1 = 1.36983 loss)
I0815 01:44:05.229954 11101 sgd_solver.cpp:136] Iteration 135500, lr = 0.0576563, m = 0.9
I0815 01:44:20.164471 11101 solver.cpp:312] Iteration 135600 (6.69603 iter/s, 14.9342s/100 iter), loss = 2.31714
I0815 01:44:20.164566 11101 solver.cpp:334]     Train net output #0: loss = 2.18182 (* 1 = 2.18182 loss)
I0815 01:44:20.164585 11101 sgd_solver.cpp:136] Iteration 135600, lr = 0.057625, m = 0.9
I0815 01:44:34.608006 11101 solver.cpp:312] Iteration 135700 (6.92371 iter/s, 14.4431s/100 iter), loss = 2.16239
I0815 01:44:34.608036 11101 solver.cpp:334]     Train net output #0: loss = 2.40966 (* 1 = 2.40966 loss)
I0815 01:44:34.608042 11101 sgd_solver.cpp:136] Iteration 135700, lr = 0.0575938, m = 0.9
I0815 01:44:49.402894 11101 solver.cpp:312] Iteration 135800 (6.75928 iter/s, 14.7945s/100 iter), loss = 2.19859
I0815 01:44:49.402969 11101 solver.cpp:334]     Train net output #0: loss = 2.57054 (* 1 = 2.57054 loss)
I0815 01:44:49.402989 11101 sgd_solver.cpp:136] Iteration 135800, lr = 0.0575625, m = 0.9
I0815 01:45:03.774112 11101 solver.cpp:312] Iteration 135900 (6.95854 iter/s, 14.3708s/100 iter), loss = 1.37992
I0815 01:45:03.774215 11101 solver.cpp:334]     Train net output #0: loss = 1.66987 (* 1 = 1.66987 loss)
I0815 01:45:03.774233 11101 sgd_solver.cpp:136] Iteration 135900, lr = 0.0575312, m = 0.9
I0815 01:45:18.199815 11101 solver.cpp:509] Iteration 136000, Testing net (#0)
I0815 01:45:38.935050 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.442412
I0815 01:45:38.935096 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.697411
I0815 01:45:38.935104 11101 solver.cpp:594]     Test net output #2: loss = 2.58864 (* 1 = 2.58864 loss)
I0815 01:45:38.935124 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7347s
I0815 01:45:39.079046 11101 solver.cpp:312] Iteration 136000 (2.83254 iter/s, 35.304s/100 iter), loss = 1.77431
I0815 01:45:39.079071 11101 solver.cpp:334]     Train net output #0: loss = 1.79657 (* 1 = 1.79657 loss)
I0815 01:45:39.079077 11101 sgd_solver.cpp:136] Iteration 136000, lr = 0.0575, m = 0.9
I0815 01:45:53.920076 11101 solver.cpp:312] Iteration 136100 (6.73826 iter/s, 14.8406s/100 iter), loss = 2.12679
I0815 01:45:53.920157 11101 solver.cpp:334]     Train net output #0: loss = 1.90794 (* 1 = 1.90794 loss)
I0815 01:45:53.920179 11101 sgd_solver.cpp:136] Iteration 136100, lr = 0.0574687, m = 0.9
I0815 01:46:08.843426 11101 solver.cpp:312] Iteration 136200 (6.70109 iter/s, 14.9229s/100 iter), loss = 1.64198
I0815 01:46:08.843454 11101 solver.cpp:334]     Train net output #0: loss = 1.4854 (* 1 = 1.4854 loss)
I0815 01:46:08.843458 11101 sgd_solver.cpp:136] Iteration 136200, lr = 0.0574375, m = 0.9
I0815 01:46:23.819646 11101 solver.cpp:312] Iteration 136300 (6.67744 iter/s, 14.9758s/100 iter), loss = 1.82195
I0815 01:46:23.819730 11101 solver.cpp:334]     Train net output #0: loss = 1.68392 (* 1 = 1.68392 loss)
I0815 01:46:23.819742 11101 sgd_solver.cpp:136] Iteration 136300, lr = 0.0574062, m = 0.9
I0815 01:46:38.554577 11101 solver.cpp:312] Iteration 136400 (6.78678 iter/s, 14.7345s/100 iter), loss = 1.99249
I0815 01:46:38.554631 11101 solver.cpp:334]     Train net output #0: loss = 1.80019 (* 1 = 1.80019 loss)
I0815 01:46:38.554643 11101 sgd_solver.cpp:136] Iteration 136400, lr = 0.057375, m = 0.9
I0815 01:46:53.128664 11101 solver.cpp:312] Iteration 136500 (6.86168 iter/s, 14.5737s/100 iter), loss = 2.06798
I0815 01:46:53.128690 11101 solver.cpp:334]     Train net output #0: loss = 1.93248 (* 1 = 1.93248 loss)
I0815 01:46:53.128695 11101 sgd_solver.cpp:136] Iteration 136500, lr = 0.0573438, m = 0.9
I0815 01:47:07.650336 11101 solver.cpp:312] Iteration 136600 (6.88645 iter/s, 14.5213s/100 iter), loss = 1.60892
I0815 01:47:07.650393 11101 solver.cpp:334]     Train net output #0: loss = 1.62191 (* 1 = 1.62191 loss)
I0815 01:47:07.650400 11101 sgd_solver.cpp:136] Iteration 136600, lr = 0.0573125, m = 0.9
I0815 01:47:22.115315 11101 solver.cpp:312] Iteration 136700 (6.91344 iter/s, 14.4646s/100 iter), loss = 1.97235
I0815 01:47:22.115453 11101 solver.cpp:334]     Train net output #0: loss = 2.10202 (* 1 = 2.10202 loss)
I0815 01:47:22.115470 11101 sgd_solver.cpp:136] Iteration 136700, lr = 0.0572813, m = 0.9
I0815 01:47:36.920385 11101 solver.cpp:312] Iteration 136800 (6.75463 iter/s, 14.8047s/100 iter), loss = 2.22146
I0815 01:47:36.920418 11101 solver.cpp:334]     Train net output #0: loss = 2.45755 (* 1 = 2.45755 loss)
I0815 01:47:36.920423 11101 sgd_solver.cpp:136] Iteration 136800, lr = 0.05725, m = 0.9
I0815 01:47:51.539289 11101 solver.cpp:312] Iteration 136900 (6.84065 iter/s, 14.6185s/100 iter), loss = 1.85572
I0815 01:47:51.539367 11101 solver.cpp:334]     Train net output #0: loss = 1.57797 (* 1 = 1.57797 loss)
I0815 01:47:51.539381 11101 sgd_solver.cpp:136] Iteration 136900, lr = 0.0572188, m = 0.9
I0815 01:48:06.261950 11101 solver.cpp:312] Iteration 137000 (6.79244 iter/s, 14.7223s/100 iter), loss = 1.93906
I0815 01:48:06.261975 11101 solver.cpp:334]     Train net output #0: loss = 2.16564 (* 1 = 2.16564 loss)
I0815 01:48:06.261979 11101 sgd_solver.cpp:136] Iteration 137000, lr = 0.0571875, m = 0.9
I0815 01:48:21.060230 11101 solver.cpp:312] Iteration 137100 (6.75773 iter/s, 14.7979s/100 iter), loss = 1.81702
I0815 01:48:21.060256 11101 solver.cpp:334]     Train net output #0: loss = 2.02862 (* 1 = 2.02862 loss)
I0815 01:48:21.060262 11101 sgd_solver.cpp:136] Iteration 137100, lr = 0.0571563, m = 0.9
I0815 01:48:35.986573 11101 solver.cpp:312] Iteration 137200 (6.69975 iter/s, 14.9259s/100 iter), loss = 2.00884
I0815 01:48:35.986656 11101 solver.cpp:334]     Train net output #0: loss = 2.04186 (* 1 = 2.04186 loss)
I0815 01:48:35.986665 11101 sgd_solver.cpp:136] Iteration 137200, lr = 0.057125, m = 0.9
I0815 01:48:50.681226 11101 solver.cpp:312] Iteration 137300 (6.80539 iter/s, 14.6942s/100 iter), loss = 1.647
I0815 01:48:50.681259 11101 solver.cpp:334]     Train net output #0: loss = 1.72883 (* 1 = 1.72883 loss)
I0815 01:48:50.681267 11101 sgd_solver.cpp:136] Iteration 137300, lr = 0.0570938, m = 0.9
I0815 01:49:05.274554 11101 solver.cpp:312] Iteration 137400 (6.85263 iter/s, 14.5929s/100 iter), loss = 2.35426
I0815 01:49:05.274582 11101 solver.cpp:334]     Train net output #0: loss = 2.36382 (* 1 = 2.36382 loss)
I0815 01:49:05.274590 11101 sgd_solver.cpp:136] Iteration 137400, lr = 0.0570625, m = 0.9
I0815 01:49:19.801862 11101 solver.cpp:312] Iteration 137500 (6.88378 iter/s, 14.5269s/100 iter), loss = 1.65109
I0815 01:49:19.801945 11101 solver.cpp:334]     Train net output #0: loss = 1.70075 (* 1 = 1.70075 loss)
I0815 01:49:19.801952 11101 sgd_solver.cpp:136] Iteration 137500, lr = 0.0570313, m = 0.9
I0815 01:49:34.413758 11101 solver.cpp:312] Iteration 137600 (6.84393 iter/s, 14.6115s/100 iter), loss = 1.99926
I0815 01:49:34.413789 11101 solver.cpp:334]     Train net output #0: loss = 1.83433 (* 1 = 1.83433 loss)
I0815 01:49:34.413795 11101 sgd_solver.cpp:136] Iteration 137600, lr = 0.057, m = 0.9
I0815 01:49:49.010548 11101 solver.cpp:312] Iteration 137700 (6.85101 iter/s, 14.5964s/100 iter), loss = 1.87953
I0815 01:49:49.010576 11101 solver.cpp:334]     Train net output #0: loss = 1.72809 (* 1 = 1.72809 loss)
I0815 01:49:49.010583 11101 sgd_solver.cpp:136] Iteration 137700, lr = 0.0569687, m = 0.9
I0815 01:50:03.520596 11101 solver.cpp:312] Iteration 137800 (6.89197 iter/s, 14.5096s/100 iter), loss = 2.10558
I0815 01:50:03.520689 11101 solver.cpp:334]     Train net output #0: loss = 2.15564 (* 1 = 2.15564 loss)
I0815 01:50:03.520704 11101 sgd_solver.cpp:136] Iteration 137800, lr = 0.0569375, m = 0.9
I0815 01:50:17.920552 11101 solver.cpp:312] Iteration 137900 (6.94466 iter/s, 14.3996s/100 iter), loss = 1.53593
I0815 01:50:17.920627 11101 solver.cpp:334]     Train net output #0: loss = 1.38445 (* 1 = 1.38445 loss)
I0815 01:50:17.920647 11101 sgd_solver.cpp:136] Iteration 137900, lr = 0.0569062, m = 0.9
I0815 01:50:32.298877 11101 solver.cpp:509] Iteration 138000, Testing net (#0)
I0815 01:50:53.217339 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.455176
I0815 01:50:53.217396 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.701293
I0815 01:50:53.217402 11101 solver.cpp:594]     Test net output #2: loss = 2.5082 (* 1 = 2.5082 loss)
I0815 01:50:53.217419 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.918s
I0815 01:50:53.374238 11101 solver.cpp:312] Iteration 138000 (2.82066 iter/s, 35.4527s/100 iter), loss = 1.95053
I0815 01:50:53.374260 11101 solver.cpp:334]     Train net output #0: loss = 2.10433 (* 1 = 2.10433 loss)
I0815 01:50:53.374265 11101 sgd_solver.cpp:136] Iteration 138000, lr = 0.056875, m = 0.9
I0815 01:51:07.817448 11101 solver.cpp:312] Iteration 138100 (6.92386 iter/s, 14.4428s/100 iter), loss = 1.40014
I0815 01:51:07.817476 11101 solver.cpp:334]     Train net output #0: loss = 1.14348 (* 1 = 1.14348 loss)
I0815 01:51:07.817481 11101 sgd_solver.cpp:136] Iteration 138100, lr = 0.0568437, m = 0.9
I0815 01:51:22.714010 11101 solver.cpp:312] Iteration 138200 (6.71314 iter/s, 14.8962s/100 iter), loss = 2.0731
I0815 01:51:22.714035 11101 solver.cpp:334]     Train net output #0: loss = 1.70787 (* 1 = 1.70787 loss)
I0815 01:51:22.714040 11101 sgd_solver.cpp:136] Iteration 138200, lr = 0.0568125, m = 0.9
I0815 01:51:36.996469 11101 solver.cpp:312] Iteration 138300 (7.00179 iter/s, 14.2821s/100 iter), loss = 2.03321
I0815 01:51:36.996572 11101 solver.cpp:334]     Train net output #0: loss = 2.07276 (* 1 = 2.07276 loss)
I0815 01:51:36.996585 11101 sgd_solver.cpp:136] Iteration 138300, lr = 0.0567813, m = 0.9
I0815 01:51:51.421331 11101 solver.cpp:312] Iteration 138400 (6.93267 iter/s, 14.4245s/100 iter), loss = 1.91473
I0815 01:51:51.421360 11101 solver.cpp:334]     Train net output #0: loss = 2.04999 (* 1 = 2.04999 loss)
I0815 01:51:51.421399 11101 sgd_solver.cpp:136] Iteration 138400, lr = 0.05675, m = 0.9
I0815 01:52:06.105756 11101 solver.cpp:312] Iteration 138500 (6.81012 iter/s, 14.684s/100 iter), loss = 2.0308
I0815 01:52:06.105784 11101 solver.cpp:334]     Train net output #0: loss = 1.93439 (* 1 = 1.93439 loss)
I0815 01:52:06.105790 11101 sgd_solver.cpp:136] Iteration 138500, lr = 0.0567187, m = 0.9
I0815 01:52:20.467104 11101 solver.cpp:312] Iteration 138600 (6.96333 iter/s, 14.361s/100 iter), loss = 2.11893
I0815 01:52:20.467192 11101 solver.cpp:334]     Train net output #0: loss = 2.07903 (* 1 = 2.07903 loss)
I0815 01:52:20.467206 11101 sgd_solver.cpp:136] Iteration 138600, lr = 0.0566875, m = 0.9
I0815 01:52:34.988164 11101 solver.cpp:312] Iteration 138700 (6.88674 iter/s, 14.5207s/100 iter), loss = 2.43208
I0815 01:52:34.988193 11101 solver.cpp:334]     Train net output #0: loss = 1.6848 (* 1 = 1.6848 loss)
I0815 01:52:34.988198 11101 sgd_solver.cpp:136] Iteration 138700, lr = 0.0566563, m = 0.9
I0815 01:52:49.560154 11101 solver.cpp:312] Iteration 138800 (6.86267 iter/s, 14.5716s/100 iter), loss = 1.96774
I0815 01:52:49.560180 11101 solver.cpp:334]     Train net output #0: loss = 1.86417 (* 1 = 1.86417 loss)
I0815 01:52:49.560186 11101 sgd_solver.cpp:136] Iteration 138800, lr = 0.056625, m = 0.9
I0815 01:53:04.047439 11101 solver.cpp:312] Iteration 138900 (6.90279 iter/s, 14.4869s/100 iter), loss = 2.10306
I0815 01:53:04.047528 11101 solver.cpp:334]     Train net output #0: loss = 1.9421 (* 1 = 1.9421 loss)
I0815 01:53:04.047541 11101 sgd_solver.cpp:136] Iteration 138900, lr = 0.0565938, m = 0.9
I0815 01:53:18.751727 11101 solver.cpp:312] Iteration 139000 (6.80092 iter/s, 14.7039s/100 iter), loss = 2.29126
I0815 01:53:18.751750 11101 solver.cpp:334]     Train net output #0: loss = 1.87351 (* 1 = 1.87351 loss)
I0815 01:53:18.751754 11101 sgd_solver.cpp:136] Iteration 139000, lr = 0.0565625, m = 0.9
I0815 01:53:33.260087 11101 solver.cpp:312] Iteration 139100 (6.89277 iter/s, 14.508s/100 iter), loss = 2.14546
I0815 01:53:33.260113 11101 solver.cpp:334]     Train net output #0: loss = 2.30501 (* 1 = 2.30501 loss)
I0815 01:53:33.260118 11101 sgd_solver.cpp:136] Iteration 139100, lr = 0.0565313, m = 0.9
I0815 01:53:48.195705 11101 solver.cpp:312] Iteration 139200 (6.69559 iter/s, 14.9352s/100 iter), loss = 1.8359
I0815 01:53:48.195768 11101 solver.cpp:334]     Train net output #0: loss = 1.8901 (* 1 = 1.8901 loss)
I0815 01:53:48.195775 11101 sgd_solver.cpp:136] Iteration 139200, lr = 0.0565, m = 0.9
I0815 01:54:02.766510 11101 solver.cpp:312] Iteration 139300 (6.86323 iter/s, 14.5704s/100 iter), loss = 1.77278
I0815 01:54:02.766571 11101 solver.cpp:334]     Train net output #0: loss = 1.72483 (* 1 = 1.72483 loss)
I0815 01:54:02.766587 11101 sgd_solver.cpp:136] Iteration 139300, lr = 0.0564688, m = 0.9
I0815 01:54:17.376191 11101 solver.cpp:312] Iteration 139400 (6.84497 iter/s, 14.6093s/100 iter), loss = 2.15948
I0815 01:54:17.376263 11101 solver.cpp:334]     Train net output #0: loss = 2.00918 (* 1 = 2.00918 loss)
I0815 01:54:17.376281 11101 sgd_solver.cpp:136] Iteration 139400, lr = 0.0564375, m = 0.9
I0815 01:54:32.085736 11101 solver.cpp:312] Iteration 139500 (6.79849 iter/s, 14.7091s/100 iter), loss = 1.6488
I0815 01:54:32.085852 11101 solver.cpp:334]     Train net output #0: loss = 2.02433 (* 1 = 2.02433 loss)
I0815 01:54:32.085871 11101 sgd_solver.cpp:136] Iteration 139500, lr = 0.0564062, m = 0.9
I0815 01:54:46.811616 11101 solver.cpp:312] Iteration 139600 (6.79095 iter/s, 14.7255s/100 iter), loss = 2.08203
I0815 01:54:46.811642 11101 solver.cpp:334]     Train net output #0: loss = 2.17986 (* 1 = 2.17986 loss)
I0815 01:54:46.811648 11101 sgd_solver.cpp:136] Iteration 139600, lr = 0.056375, m = 0.9
I0815 01:55:01.705498 11101 solver.cpp:312] Iteration 139700 (6.71435 iter/s, 14.8935s/100 iter), loss = 1.79316
I0815 01:55:01.705524 11101 solver.cpp:334]     Train net output #0: loss = 1.3977 (* 1 = 1.3977 loss)
I0815 01:55:01.705528 11101 sgd_solver.cpp:136] Iteration 139700, lr = 0.0563437, m = 0.9
I0815 01:55:16.454731 11101 solver.cpp:312] Iteration 139800 (6.7802 iter/s, 14.7488s/100 iter), loss = 2.09617
I0815 01:55:16.454792 11101 solver.cpp:334]     Train net output #0: loss = 2.04182 (* 1 = 2.04182 loss)
I0815 01:55:16.454799 11101 sgd_solver.cpp:136] Iteration 139800, lr = 0.0563125, m = 0.9
I0815 01:55:31.395011 11101 solver.cpp:312] Iteration 139900 (6.6935 iter/s, 14.9399s/100 iter), loss = 1.88123
I0815 01:55:31.395040 11101 solver.cpp:334]     Train net output #0: loss = 1.79314 (* 1 = 1.79314 loss)
I0815 01:55:31.395046 11101 sgd_solver.cpp:136] Iteration 139900, lr = 0.0562812, m = 0.9
I0815 01:55:46.152344 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_140000.caffemodel
I0815 01:55:46.189633 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_140000.solverstate
I0815 01:55:46.195829 11101 solver.cpp:509] Iteration 140000, Testing net (#0)
I0815 01:55:47.944639 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 01:56:06.478974 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.43247
I0815 01:56:06.479001 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.690528
I0815 01:56:06.479008 11101 solver.cpp:594]     Test net output #2: loss = 2.60148 (* 1 = 2.60148 loss)
I0815 01:56:06.479027 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.2826s
I0815 01:56:06.647106 11101 solver.cpp:312] Iteration 140000 (2.83679 iter/s, 35.2511s/100 iter), loss = 2.33089
I0815 01:56:06.647132 11101 solver.cpp:334]     Train net output #0: loss = 2.41663 (* 1 = 2.41663 loss)
I0815 01:56:06.647136 11101 sgd_solver.cpp:136] Iteration 140000, lr = 0.05625, m = 0.9
I0815 01:56:21.197932 11101 solver.cpp:312] Iteration 140100 (6.87265 iter/s, 14.5504s/100 iter), loss = 2.16408
I0815 01:56:21.198009 11101 solver.cpp:334]     Train net output #0: loss = 2.42053 (* 1 = 2.42053 loss)
I0815 01:56:21.198024 11101 sgd_solver.cpp:136] Iteration 140100, lr = 0.0562188, m = 0.9
I0815 01:56:35.762459 11101 solver.cpp:312] Iteration 140200 (6.86619 iter/s, 14.5641s/100 iter), loss = 2.42663
I0815 01:56:35.762689 11101 solver.cpp:334]     Train net output #0: loss = 2.65891 (* 1 = 2.65891 loss)
I0815 01:56:35.762800 11101 sgd_solver.cpp:136] Iteration 140200, lr = 0.0561875, m = 0.9
I0815 01:56:50.511562 11101 solver.cpp:312] Iteration 140300 (6.78026 iter/s, 14.7487s/100 iter), loss = 1.92395
I0815 01:56:50.511592 11101 solver.cpp:334]     Train net output #0: loss = 2.1965 (* 1 = 2.1965 loss)
I0815 01:56:50.511598 11101 sgd_solver.cpp:136] Iteration 140300, lr = 0.0561563, m = 0.9
I0815 01:57:04.977619 11101 solver.cpp:312] Iteration 140400 (6.91293 iter/s, 14.4657s/100 iter), loss = 2.34221
I0815 01:57:04.977685 11101 solver.cpp:334]     Train net output #0: loss = 2.39468 (* 1 = 2.39468 loss)
I0815 01:57:04.977700 11101 sgd_solver.cpp:136] Iteration 140400, lr = 0.056125, m = 0.9
I0815 01:57:19.405519 11101 solver.cpp:312] Iteration 140500 (6.93121 iter/s, 14.4275s/100 iter), loss = 2.10302
I0815 01:57:19.405592 11101 solver.cpp:334]     Train net output #0: loss = 2.11823 (* 1 = 2.11823 loss)
I0815 01:57:19.405617 11101 sgd_solver.cpp:136] Iteration 140500, lr = 0.0560938, m = 0.9
I0815 01:57:33.808269 11101 solver.cpp:312] Iteration 140600 (6.94331 iter/s, 14.4023s/100 iter), loss = 2.02459
I0815 01:57:33.808300 11101 solver.cpp:334]     Train net output #0: loss = 2.12508 (* 1 = 2.12508 loss)
I0815 01:57:33.808306 11101 sgd_solver.cpp:136] Iteration 140600, lr = 0.0560625, m = 0.9
I0815 01:57:48.404280 11101 solver.cpp:312] Iteration 140700 (6.85138 iter/s, 14.5956s/100 iter), loss = 1.87735
I0815 01:57:48.404366 11101 solver.cpp:334]     Train net output #0: loss = 1.75942 (* 1 = 1.75942 loss)
I0815 01:57:48.404373 11101 sgd_solver.cpp:136] Iteration 140700, lr = 0.0560313, m = 0.9
I0815 01:58:03.137672 11101 solver.cpp:312] Iteration 140800 (6.78749 iter/s, 14.733s/100 iter), loss = 2.0435
I0815 01:58:03.137699 11101 solver.cpp:334]     Train net output #0: loss = 2.2003 (* 1 = 2.2003 loss)
I0815 01:58:03.137704 11101 sgd_solver.cpp:136] Iteration 140800, lr = 0.056, m = 0.9
I0815 01:58:17.784771 11101 solver.cpp:312] Iteration 140900 (6.82748 iter/s, 14.6467s/100 iter), loss = 2.11
I0815 01:58:17.784796 11101 solver.cpp:334]     Train net output #0: loss = 2.27024 (* 1 = 2.27024 loss)
I0815 01:58:17.784802 11101 sgd_solver.cpp:136] Iteration 140900, lr = 0.0559688, m = 0.9
I0815 01:58:32.213063 11101 solver.cpp:312] Iteration 141000 (6.93102 iter/s, 14.4279s/100 iter), loss = 2.04243
I0815 01:58:32.213122 11101 solver.cpp:334]     Train net output #0: loss = 1.60216 (* 1 = 1.60216 loss)
I0815 01:58:32.213129 11101 sgd_solver.cpp:136] Iteration 141000, lr = 0.0559375, m = 0.9
I0815 01:58:46.700620 11101 solver.cpp:312] Iteration 141100 (6.90267 iter/s, 14.4872s/100 iter), loss = 2.35598
I0815 01:58:46.700685 11101 solver.cpp:334]     Train net output #0: loss = 2.83152 (* 1 = 2.83152 loss)
I0815 01:58:46.700703 11101 sgd_solver.cpp:136] Iteration 141100, lr = 0.0559062, m = 0.9
I0815 01:59:01.096967 11101 solver.cpp:312] Iteration 141200 (6.9464 iter/s, 14.3959s/100 iter), loss = 2.27742
I0815 01:59:01.096997 11101 solver.cpp:334]     Train net output #0: loss = 2.13142 (* 1 = 2.13142 loss)
I0815 01:59:01.097004 11101 sgd_solver.cpp:136] Iteration 141200, lr = 0.055875, m = 0.9
I0815 01:59:15.666395 11101 solver.cpp:312] Iteration 141300 (6.86387 iter/s, 14.569s/100 iter), loss = 1.92435
I0815 01:59:15.666455 11101 solver.cpp:334]     Train net output #0: loss = 1.81881 (* 1 = 1.81881 loss)
I0815 01:59:15.666486 11101 sgd_solver.cpp:136] Iteration 141300, lr = 0.0558437, m = 0.9
I0815 01:59:30.583894 11101 solver.cpp:312] Iteration 141400 (6.70372 iter/s, 14.9171s/100 iter), loss = 2.02123
I0815 01:59:30.583959 11101 solver.cpp:334]     Train net output #0: loss = 2.06213 (* 1 = 2.06213 loss)
I0815 01:59:30.583977 11101 sgd_solver.cpp:136] Iteration 141400, lr = 0.0558125, m = 0.9
I0815 01:59:44.990671 11101 solver.cpp:312] Iteration 141500 (6.94137 iter/s, 14.4064s/100 iter), loss = 2.10844
I0815 01:59:44.990705 11101 solver.cpp:334]     Train net output #0: loss = 1.98135 (* 1 = 1.98135 loss)
I0815 01:59:44.990713 11101 sgd_solver.cpp:136] Iteration 141500, lr = 0.0557813, m = 0.9
I0815 01:59:59.282506 11101 solver.cpp:312] Iteration 141600 (6.99719 iter/s, 14.2914s/100 iter), loss = 2.25135
I0815 01:59:59.282735 11101 solver.cpp:334]     Train net output #0: loss = 2.46234 (* 1 = 2.46234 loss)
I0815 01:59:59.282845 11101 sgd_solver.cpp:136] Iteration 141600, lr = 0.05575, m = 0.9
I0815 02:00:13.772977 11101 solver.cpp:312] Iteration 141700 (6.90128 iter/s, 14.4901s/100 iter), loss = 2.01126
I0815 02:00:13.773030 11101 solver.cpp:334]     Train net output #0: loss = 1.79763 (* 1 = 1.79763 loss)
I0815 02:00:13.773043 11101 sgd_solver.cpp:136] Iteration 141700, lr = 0.0557187, m = 0.9
I0815 02:00:28.237712 11101 solver.cpp:312] Iteration 141800 (6.91358 iter/s, 14.4643s/100 iter), loss = 1.50723
I0815 02:00:28.237828 11101 solver.cpp:334]     Train net output #0: loss = 1.6618 (* 1 = 1.6618 loss)
I0815 02:00:28.237853 11101 sgd_solver.cpp:136] Iteration 141800, lr = 0.0556875, m = 0.9
I0815 02:00:42.899377 11101 solver.cpp:312] Iteration 141900 (6.82069 iter/s, 14.6613s/100 iter), loss = 2.29928
I0815 02:00:42.899638 11101 solver.cpp:334]     Train net output #0: loss = 2.0614 (* 1 = 2.0614 loss)
I0815 02:00:42.899658 11101 sgd_solver.cpp:136] Iteration 141900, lr = 0.0556563, m = 0.9
I0815 02:00:57.314517 11101 solver.cpp:509] Iteration 142000, Testing net (#0)
I0815 02:01:17.766412 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.429294
I0815 02:01:17.766458 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.681764
I0815 02:01:17.766466 11101 solver.cpp:594]     Test net output #2: loss = 2.65266 (* 1 = 2.65266 loss)
I0815 02:01:17.766520 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.4514s
I0815 02:01:17.925135 11101 solver.cpp:312] Iteration 142000 (2.85512 iter/s, 35.0248s/100 iter), loss = 2.03368
I0815 02:01:17.925161 11101 solver.cpp:334]     Train net output #0: loss = 2.19033 (* 1 = 2.19033 loss)
I0815 02:01:17.925165 11101 sgd_solver.cpp:136] Iteration 142000, lr = 0.055625, m = 0.9
I0815 02:01:32.963994 11101 solver.cpp:312] Iteration 142100 (6.64962 iter/s, 15.0384s/100 iter), loss = 1.66676
I0815 02:01:32.964046 11101 solver.cpp:334]     Train net output #0: loss = 1.59258 (* 1 = 1.59258 loss)
I0815 02:01:32.964058 11101 sgd_solver.cpp:136] Iteration 142100, lr = 0.0555938, m = 0.9
I0815 02:01:47.951711 11101 solver.cpp:312] Iteration 142200 (6.67232 iter/s, 14.9873s/100 iter), loss = 1.73619
I0815 02:01:47.951763 11101 solver.cpp:334]     Train net output #0: loss = 1.7229 (* 1 = 1.7229 loss)
I0815 02:01:47.951769 11101 sgd_solver.cpp:136] Iteration 142200, lr = 0.0555625, m = 0.9
I0815 02:02:02.653393 11101 solver.cpp:312] Iteration 142300 (6.80213 iter/s, 14.7013s/100 iter), loss = 1.84214
I0815 02:02:02.653420 11101 solver.cpp:334]     Train net output #0: loss = 1.75263 (* 1 = 1.75263 loss)
I0815 02:02:02.653425 11101 sgd_solver.cpp:136] Iteration 142300, lr = 0.0555313, m = 0.9
I0815 02:02:17.667104 11101 solver.cpp:312] Iteration 142400 (6.66076 iter/s, 15.0133s/100 iter), loss = 2.25038
I0815 02:02:17.667155 11101 solver.cpp:334]     Train net output #0: loss = 2.53439 (* 1 = 2.53439 loss)
I0815 02:02:17.667167 11101 sgd_solver.cpp:136] Iteration 142400, lr = 0.0555, m = 0.9
I0815 02:02:32.262840 11101 solver.cpp:312] Iteration 142500 (6.85151 iter/s, 14.5953s/100 iter), loss = 2.38829
I0815 02:02:32.262902 11101 solver.cpp:334]     Train net output #0: loss = 2.60415 (* 1 = 2.60415 loss)
I0815 02:02:32.262909 11101 sgd_solver.cpp:136] Iteration 142500, lr = 0.0554687, m = 0.9
I0815 02:02:47.142859 11101 solver.cpp:312] Iteration 142600 (6.72061 iter/s, 14.8796s/100 iter), loss = 2.11396
I0815 02:02:47.142887 11101 solver.cpp:334]     Train net output #0: loss = 1.58869 (* 1 = 1.58869 loss)
I0815 02:02:47.142894 11101 sgd_solver.cpp:136] Iteration 142600, lr = 0.0554375, m = 0.9
I0815 02:03:01.941833 11101 solver.cpp:312] Iteration 142700 (6.75741 iter/s, 14.7986s/100 iter), loss = 1.65397
I0815 02:03:01.941864 11101 solver.cpp:334]     Train net output #0: loss = 1.5342 (* 1 = 1.5342 loss)
I0815 02:03:01.941869 11101 sgd_solver.cpp:136] Iteration 142700, lr = 0.0554063, m = 0.9
I0815 02:03:16.581218 11101 solver.cpp:312] Iteration 142800 (6.83108 iter/s, 14.639s/100 iter), loss = 1.82762
I0815 02:03:16.581281 11101 solver.cpp:334]     Train net output #0: loss = 1.75115 (* 1 = 1.75115 loss)
I0815 02:03:16.581287 11101 sgd_solver.cpp:136] Iteration 142800, lr = 0.055375, m = 0.9
I0815 02:03:31.315122 11101 solver.cpp:312] Iteration 142900 (6.78726 iter/s, 14.7335s/100 iter), loss = 2.08522
I0815 02:03:31.315148 11101 solver.cpp:334]     Train net output #0: loss = 2.53974 (* 1 = 2.53974 loss)
I0815 02:03:31.315155 11101 sgd_solver.cpp:136] Iteration 142900, lr = 0.0553437, m = 0.9
I0815 02:03:46.004106 11101 solver.cpp:312] Iteration 143000 (6.80801 iter/s, 14.6886s/100 iter), loss = 2.03899
I0815 02:03:46.004289 11101 solver.cpp:334]     Train net output #0: loss = 1.87134 (* 1 = 1.87134 loss)
I0815 02:03:46.004374 11101 sgd_solver.cpp:136] Iteration 143000, lr = 0.0553125, m = 0.9
I0815 02:04:00.959455 11101 solver.cpp:312] Iteration 143100 (6.68675 iter/s, 14.9549s/100 iter), loss = 1.91413
I0815 02:04:00.959553 11101 solver.cpp:334]     Train net output #0: loss = 1.67255 (* 1 = 1.67255 loss)
I0815 02:04:00.959565 11101 sgd_solver.cpp:136] Iteration 143100, lr = 0.0552812, m = 0.9
I0815 02:04:15.798053 11101 solver.cpp:312] Iteration 143200 (6.73937 iter/s, 14.8382s/100 iter), loss = 2.00567
I0815 02:04:15.798084 11101 solver.cpp:334]     Train net output #0: loss = 1.93203 (* 1 = 1.93203 loss)
I0815 02:04:15.798089 11101 sgd_solver.cpp:136] Iteration 143200, lr = 0.05525, m = 0.9
I0815 02:04:30.356850 11101 solver.cpp:312] Iteration 143300 (6.86889 iter/s, 14.5584s/100 iter), loss = 2.00753
I0815 02:04:30.356873 11101 solver.cpp:334]     Train net output #0: loss = 1.59455 (* 1 = 1.59455 loss)
I0815 02:04:30.356878 11101 sgd_solver.cpp:136] Iteration 143300, lr = 0.0552188, m = 0.9
I0815 02:04:45.340970 11101 solver.cpp:312] Iteration 143400 (6.67392 iter/s, 14.9837s/100 iter), loss = 2.1306
I0815 02:04:45.341035 11101 solver.cpp:334]     Train net output #0: loss = 2.26458 (* 1 = 2.26458 loss)
I0815 02:04:45.341042 11101 sgd_solver.cpp:136] Iteration 143400, lr = 0.0551875, m = 0.9
I0815 02:05:00.303468 11101 solver.cpp:312] Iteration 143500 (6.68356 iter/s, 14.9621s/100 iter), loss = 1.70093
I0815 02:05:00.303495 11101 solver.cpp:334]     Train net output #0: loss = 1.96262 (* 1 = 1.96262 loss)
I0815 02:05:00.303501 11101 sgd_solver.cpp:136] Iteration 143500, lr = 0.0551562, m = 0.9
I0815 02:05:14.836076 11101 solver.cpp:312] Iteration 143600 (6.88127 iter/s, 14.5322s/100 iter), loss = 2.33864
I0815 02:05:14.836153 11101 solver.cpp:334]     Train net output #0: loss = 2.10065 (* 1 = 2.10065 loss)
I0815 02:05:14.836174 11101 sgd_solver.cpp:136] Iteration 143600, lr = 0.055125, m = 0.9
I0815 02:05:29.280724 11101 solver.cpp:312] Iteration 143700 (6.92317 iter/s, 14.4442s/100 iter), loss = 2.04268
I0815 02:05:29.280905 11101 solver.cpp:334]     Train net output #0: loss = 2.27701 (* 1 = 2.27701 loss)
I0815 02:05:29.280988 11101 sgd_solver.cpp:136] Iteration 143700, lr = 0.0550938, m = 0.9
I0815 02:05:44.028005 11101 solver.cpp:312] Iteration 143800 (6.7811 iter/s, 14.7469s/100 iter), loss = 1.80866
I0815 02:05:44.028072 11101 solver.cpp:334]     Train net output #0: loss = 1.72099 (* 1 = 1.72099 loss)
I0815 02:05:44.028092 11101 sgd_solver.cpp:136] Iteration 143800, lr = 0.0550625, m = 0.9
I0815 02:05:58.773561 11101 solver.cpp:312] Iteration 143900 (6.78189 iter/s, 14.7452s/100 iter), loss = 2.14005
I0815 02:05:58.773627 11101 solver.cpp:334]     Train net output #0: loss = 1.98383 (* 1 = 1.98383 loss)
I0815 02:05:58.773648 11101 sgd_solver.cpp:136] Iteration 143900, lr = 0.0550313, m = 0.9
I0815 02:06:13.674209 11101 solver.cpp:509] Iteration 144000, Testing net (#0)
I0815 02:06:25.318683 11099 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 02:06:34.607349 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.438764
I0815 02:06:34.607381 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.696646
I0815 02:06:34.607389 11101 solver.cpp:594]     Test net output #2: loss = 2.55906 (* 1 = 2.55906 loss)
I0815 02:06:34.607416 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9326s
I0815 02:06:34.794217 11101 solver.cpp:312] Iteration 144000 (2.77626 iter/s, 36.0197s/100 iter), loss = 1.79677
I0815 02:06:34.794241 11101 solver.cpp:334]     Train net output #0: loss = 1.87515 (* 1 = 1.87515 loss)
I0815 02:06:34.794247 11101 sgd_solver.cpp:136] Iteration 144000, lr = 0.055, m = 0.9
I0815 02:06:49.309532 11101 solver.cpp:312] Iteration 144100 (6.88947 iter/s, 14.5149s/100 iter), loss = 1.74724
I0815 02:06:49.309614 11101 solver.cpp:334]     Train net output #0: loss = 1.5616 (* 1 = 1.5616 loss)
I0815 02:06:49.309629 11101 sgd_solver.cpp:136] Iteration 144100, lr = 0.0549688, m = 0.9
I0815 02:07:03.841184 11101 solver.cpp:312] Iteration 144200 (6.88172 iter/s, 14.5312s/100 iter), loss = 2.35616
I0815 02:07:03.841215 11101 solver.cpp:334]     Train net output #0: loss = 2.02809 (* 1 = 2.02809 loss)
I0815 02:07:03.841220 11101 sgd_solver.cpp:136] Iteration 144200, lr = 0.0549375, m = 0.9
I0815 02:07:18.320318 11101 solver.cpp:312] Iteration 144300 (6.90668 iter/s, 14.4787s/100 iter), loss = 2.08489
I0815 02:07:18.320348 11101 solver.cpp:334]     Train net output #0: loss = 2.81145 (* 1 = 2.81145 loss)
I0815 02:07:18.320353 11101 sgd_solver.cpp:136] Iteration 144300, lr = 0.0549062, m = 0.9
I0815 02:07:33.123019 11101 solver.cpp:312] Iteration 144400 (6.75571 iter/s, 14.8023s/100 iter), loss = 1.79792
I0815 02:07:33.123095 11101 solver.cpp:334]     Train net output #0: loss = 1.80909 (* 1 = 1.80909 loss)
I0815 02:07:33.123102 11101 sgd_solver.cpp:136] Iteration 144400, lr = 0.054875, m = 0.9
I0815 02:07:47.627830 11101 solver.cpp:312] Iteration 144500 (6.89446 iter/s, 14.5044s/100 iter), loss = 1.51293
I0815 02:07:47.627856 11101 solver.cpp:334]     Train net output #0: loss = 1.57266 (* 1 = 1.57266 loss)
I0815 02:07:47.627861 11101 sgd_solver.cpp:136] Iteration 144500, lr = 0.0548437, m = 0.9
I0815 02:08:02.373775 11101 solver.cpp:312] Iteration 144600 (6.78171 iter/s, 14.7455s/100 iter), loss = 1.85905
I0815 02:08:02.373800 11101 solver.cpp:334]     Train net output #0: loss = 2.041 (* 1 = 2.041 loss)
I0815 02:08:02.373806 11101 sgd_solver.cpp:136] Iteration 144600, lr = 0.0548125, m = 0.9
I0815 02:08:17.085484 11101 solver.cpp:312] Iteration 144700 (6.79749 iter/s, 14.7113s/100 iter), loss = 1.52954
I0815 02:08:17.085539 11101 solver.cpp:334]     Train net output #0: loss = 1.85999 (* 1 = 1.85999 loss)
I0815 02:08:17.085546 11101 sgd_solver.cpp:136] Iteration 144700, lr = 0.0547812, m = 0.9
I0815 02:08:31.951634 11101 solver.cpp:312] Iteration 144800 (6.72688 iter/s, 14.8657s/100 iter), loss = 1.93739
I0815 02:08:31.951661 11101 solver.cpp:334]     Train net output #0: loss = 1.96776 (* 1 = 1.96776 loss)
I0815 02:08:31.951666 11101 sgd_solver.cpp:136] Iteration 144800, lr = 0.05475, m = 0.9
I0815 02:08:46.894095 11101 solver.cpp:312] Iteration 144900 (6.69252 iter/s, 14.942s/100 iter), loss = 2.13246
I0815 02:08:46.894170 11101 solver.cpp:334]     Train net output #0: loss = 2.15273 (* 1 = 2.15273 loss)
I0815 02:08:46.894191 11101 sgd_solver.cpp:136] Iteration 144900, lr = 0.0547188, m = 0.9
I0815 02:08:49.296692 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 02:09:01.948292 11101 solver.cpp:312] Iteration 145000 (6.64285 iter/s, 15.0538s/100 iter), loss = 1.74893
I0815 02:09:01.948323 11101 solver.cpp:334]     Train net output #0: loss = 1.72697 (* 1 = 1.72697 loss)
I0815 02:09:01.948328 11101 sgd_solver.cpp:136] Iteration 145000, lr = 0.0546875, m = 0.9
I0815 02:09:16.681696 11101 solver.cpp:312] Iteration 145100 (6.78748 iter/s, 14.733s/100 iter), loss = 2.36647
I0815 02:09:16.681723 11101 solver.cpp:334]     Train net output #0: loss = 2.34067 (* 1 = 2.34067 loss)
I0815 02:09:16.681730 11101 sgd_solver.cpp:136] Iteration 145100, lr = 0.0546562, m = 0.9
I0815 02:09:31.155426 11101 solver.cpp:312] Iteration 145200 (6.90926 iter/s, 14.4733s/100 iter), loss = 2.0603
I0815 02:09:31.155629 11101 solver.cpp:334]     Train net output #0: loss = 2.48232 (* 1 = 2.48232 loss)
I0815 02:09:31.155637 11101 sgd_solver.cpp:136] Iteration 145200, lr = 0.054625, m = 0.9
I0815 02:09:45.878434 11101 solver.cpp:312] Iteration 145300 (6.79228 iter/s, 14.7226s/100 iter), loss = 2.34551
I0815 02:09:45.878659 11101 solver.cpp:334]     Train net output #0: loss = 2.27097 (* 1 = 2.27097 loss)
I0815 02:09:45.878768 11101 sgd_solver.cpp:136] Iteration 145300, lr = 0.0545938, m = 0.9
I0815 02:10:00.679314 11101 solver.cpp:312] Iteration 145400 (6.75654 iter/s, 14.8005s/100 iter), loss = 1.99456
I0815 02:10:00.679343 11101 solver.cpp:334]     Train net output #0: loss = 1.80576 (* 1 = 1.80576 loss)
I0815 02:10:00.679349 11101 sgd_solver.cpp:136] Iteration 145400, lr = 0.0545625, m = 0.9
I0815 02:10:15.671816 11101 solver.cpp:312] Iteration 145500 (6.67018 iter/s, 14.9921s/100 iter), loss = 1.93311
I0815 02:10:15.671921 11101 solver.cpp:334]     Train net output #0: loss = 1.82369 (* 1 = 1.82369 loss)
I0815 02:10:15.671943 11101 sgd_solver.cpp:136] Iteration 145500, lr = 0.0545313, m = 0.9
I0815 02:10:30.480057 11101 solver.cpp:312] Iteration 145600 (6.75318 iter/s, 14.8078s/100 iter), loss = 1.88041
I0815 02:10:30.480145 11101 solver.cpp:334]     Train net output #0: loss = 2.15593 (* 1 = 2.15593 loss)
I0815 02:10:30.480167 11101 sgd_solver.cpp:136] Iteration 145600, lr = 0.0545, m = 0.9
I0815 02:10:45.437234 11101 solver.cpp:312] Iteration 145700 (6.68594 iter/s, 14.9568s/100 iter), loss = 1.99347
I0815 02:10:45.437260 11101 solver.cpp:334]     Train net output #0: loss = 1.9188 (* 1 = 1.9188 loss)
I0815 02:10:45.437264 11101 sgd_solver.cpp:136] Iteration 145700, lr = 0.0544688, m = 0.9
I0815 02:11:00.047250 11101 solver.cpp:312] Iteration 145800 (6.84481 iter/s, 14.6096s/100 iter), loss = 2.12233
I0815 02:11:00.047341 11101 solver.cpp:334]     Train net output #0: loss = 2.28318 (* 1 = 2.28318 loss)
I0815 02:11:00.047359 11101 sgd_solver.cpp:136] Iteration 145800, lr = 0.0544375, m = 0.9
I0815 02:11:14.603441 11101 solver.cpp:312] Iteration 145900 (6.87012 iter/s, 14.5558s/100 iter), loss = 2.22909
I0815 02:11:14.603471 11101 solver.cpp:334]     Train net output #0: loss = 2.37994 (* 1 = 2.37994 loss)
I0815 02:11:14.603476 11101 sgd_solver.cpp:136] Iteration 145900, lr = 0.0544063, m = 0.9
I0815 02:11:29.112087 11101 solver.cpp:509] Iteration 146000, Testing net (#0)
I0815 02:11:50.533910 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.427882
I0815 02:11:50.533968 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.678705
I0815 02:11:50.533974 11101 solver.cpp:594]     Test net output #2: loss = 2.70419 (* 1 = 2.70419 loss)
I0815 02:11:50.533995 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.4213s
I0815 02:11:50.701233 11101 solver.cpp:312] Iteration 146000 (2.77033 iter/s, 36.0968s/100 iter), loss = 1.77755
I0815 02:11:50.701262 11101 solver.cpp:334]     Train net output #0: loss = 1.81412 (* 1 = 1.81412 loss)
I0815 02:11:50.701268 11101 sgd_solver.cpp:136] Iteration 146000, lr = 0.054375, m = 0.9
I0815 02:12:05.407229 11101 solver.cpp:312] Iteration 146100 (6.80014 iter/s, 14.7056s/100 iter), loss = 1.82998
I0815 02:12:05.407281 11101 solver.cpp:334]     Train net output #0: loss = 1.49519 (* 1 = 1.49519 loss)
I0815 02:12:05.407295 11101 sgd_solver.cpp:136] Iteration 146100, lr = 0.0543437, m = 0.9
I0815 02:12:19.755383 11101 solver.cpp:312] Iteration 146200 (6.96973 iter/s, 14.3478s/100 iter), loss = 1.76192
I0815 02:12:19.755606 11101 solver.cpp:334]     Train net output #0: loss = 1.93144 (* 1 = 1.93144 loss)
I0815 02:12:19.755715 11101 sgd_solver.cpp:136] Iteration 146200, lr = 0.0543125, m = 0.9
I0815 02:12:34.332576 11101 solver.cpp:312] Iteration 146300 (6.86022 iter/s, 14.5768s/100 iter), loss = 1.98339
I0815 02:12:34.332667 11101 solver.cpp:334]     Train net output #0: loss = 1.90735 (* 1 = 1.90735 loss)
I0815 02:12:34.332684 11101 sgd_solver.cpp:136] Iteration 146300, lr = 0.0542812, m = 0.9
I0815 02:12:48.969113 11101 solver.cpp:312] Iteration 146400 (6.83241 iter/s, 14.6361s/100 iter), loss = 2.24404
I0815 02:12:48.969141 11101 solver.cpp:334]     Train net output #0: loss = 2.59847 (* 1 = 2.59847 loss)
I0815 02:12:48.969147 11101 sgd_solver.cpp:136] Iteration 146400, lr = 0.05425, m = 0.9
I0815 02:13:03.500986 11101 solver.cpp:312] Iteration 146500 (6.88161 iter/s, 14.5315s/100 iter), loss = 1.78874
I0815 02:13:03.501013 11101 solver.cpp:334]     Train net output #0: loss = 1.73596 (* 1 = 1.73596 loss)
I0815 02:13:03.501018 11101 sgd_solver.cpp:136] Iteration 146500, lr = 0.0542188, m = 0.9
I0815 02:13:18.403116 11101 solver.cpp:312] Iteration 146600 (6.71064 iter/s, 14.9017s/100 iter), loss = 2.08751
I0815 02:13:18.403175 11101 solver.cpp:334]     Train net output #0: loss = 2.17502 (* 1 = 2.17502 loss)
I0815 02:13:18.403180 11101 sgd_solver.cpp:136] Iteration 146600, lr = 0.0541875, m = 0.9
I0815 02:13:33.439864 11101 solver.cpp:312] Iteration 146700 (6.65056 iter/s, 15.0363s/100 iter), loss = 1.86315
I0815 02:13:33.439891 11101 solver.cpp:334]     Train net output #0: loss = 2.07012 (* 1 = 2.07012 loss)
I0815 02:13:33.439898 11101 sgd_solver.cpp:136] Iteration 146700, lr = 0.0541563, m = 0.9
I0815 02:13:48.170984 11101 solver.cpp:312] Iteration 146800 (6.78854 iter/s, 14.7307s/100 iter), loss = 2.0123
I0815 02:13:48.171011 11101 solver.cpp:334]     Train net output #0: loss = 2.157 (* 1 = 2.157 loss)
I0815 02:13:48.171015 11101 sgd_solver.cpp:136] Iteration 146800, lr = 0.054125, m = 0.9
I0815 02:14:02.930863 11101 solver.cpp:312] Iteration 146900 (6.77531 iter/s, 14.7595s/100 iter), loss = 2.0417
I0815 02:14:02.930974 11101 solver.cpp:334]     Train net output #0: loss = 2.17058 (* 1 = 2.17058 loss)
I0815 02:14:02.930992 11101 sgd_solver.cpp:136] Iteration 146900, lr = 0.0540938, m = 0.9
I0815 02:14:17.324683 11101 solver.cpp:312] Iteration 147000 (6.94762 iter/s, 14.3934s/100 iter), loss = 2.71033
I0815 02:14:17.324712 11101 solver.cpp:334]     Train net output #0: loss = 2.85937 (* 1 = 2.85937 loss)
I0815 02:14:17.324717 11101 sgd_solver.cpp:136] Iteration 147000, lr = 0.0540625, m = 0.9
I0815 02:14:32.127986 11101 solver.cpp:312] Iteration 147100 (6.75543 iter/s, 14.8029s/100 iter), loss = 2.10788
I0815 02:14:32.128016 11101 solver.cpp:334]     Train net output #0: loss = 1.89046 (* 1 = 1.89046 loss)
I0815 02:14:32.128022 11101 sgd_solver.cpp:136] Iteration 147100, lr = 0.0540313, m = 0.9
I0815 02:14:46.827028 11101 solver.cpp:312] Iteration 147200 (6.80335 iter/s, 14.6986s/100 iter), loss = 2.25727
I0815 02:14:46.827270 11101 solver.cpp:334]     Train net output #0: loss = 2.56712 (* 1 = 2.56712 loss)
I0815 02:14:46.827380 11101 sgd_solver.cpp:136] Iteration 147200, lr = 0.054, m = 0.9
I0815 02:15:01.406476 11101 solver.cpp:312] Iteration 147300 (6.85916 iter/s, 14.5791s/100 iter), loss = 1.56603
I0815 02:15:01.406500 11101 solver.cpp:334]     Train net output #0: loss = 1.54032 (* 1 = 1.54032 loss)
I0815 02:15:01.406507 11101 sgd_solver.cpp:136] Iteration 147300, lr = 0.0539688, m = 0.9
I0815 02:15:16.184733 11101 solver.cpp:312] Iteration 147400 (6.76689 iter/s, 14.7778s/100 iter), loss = 2.04376
I0815 02:15:16.184792 11101 solver.cpp:334]     Train net output #0: loss = 2.1683 (* 1 = 2.1683 loss)
I0815 02:15:16.184810 11101 sgd_solver.cpp:136] Iteration 147400, lr = 0.0539375, m = 0.9
I0815 02:15:30.719559 11101 solver.cpp:312] Iteration 147500 (6.88022 iter/s, 14.5344s/100 iter), loss = 1.87519
I0815 02:15:30.719656 11101 solver.cpp:334]     Train net output #0: loss = 1.53614 (* 1 = 1.53614 loss)
I0815 02:15:30.719676 11101 sgd_solver.cpp:136] Iteration 147500, lr = 0.0539063, m = 0.9
I0815 02:15:45.339363 11101 solver.cpp:312] Iteration 147600 (6.84023 iter/s, 14.6194s/100 iter), loss = 1.80985
I0815 02:15:45.339431 11101 solver.cpp:334]     Train net output #0: loss = 1.72052 (* 1 = 1.72052 loss)
I0815 02:15:45.339448 11101 sgd_solver.cpp:136] Iteration 147600, lr = 0.053875, m = 0.9
I0815 02:15:59.762573 11101 solver.cpp:312] Iteration 147700 (6.93346 iter/s, 14.4228s/100 iter), loss = 2.37712
I0815 02:15:59.762598 11101 solver.cpp:334]     Train net output #0: loss = 2.55156 (* 1 = 2.55156 loss)
I0815 02:15:59.762601 11101 sgd_solver.cpp:136] Iteration 147700, lr = 0.0538437, m = 0.9
I0815 02:16:14.315820 11101 solver.cpp:312] Iteration 147800 (6.87151 iter/s, 14.5528s/100 iter), loss = 2.17403
I0815 02:16:14.315874 11101 solver.cpp:334]     Train net output #0: loss = 2.15156 (* 1 = 2.15156 loss)
I0815 02:16:14.315881 11101 sgd_solver.cpp:136] Iteration 147800, lr = 0.0538125, m = 0.9
I0815 02:16:28.888875 11101 solver.cpp:312] Iteration 147900 (6.86217 iter/s, 14.5727s/100 iter), loss = 2.29346
I0815 02:16:28.888900 11101 solver.cpp:334]     Train net output #0: loss = 2.48472 (* 1 = 2.48472 loss)
I0815 02:16:28.888905 11101 sgd_solver.cpp:136] Iteration 147900, lr = 0.0537812, m = 0.9
I0815 02:16:43.182718 11101 solver.cpp:509] Iteration 148000, Testing net (#0)
I0815 02:17:04.036792 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.460882
I0815 02:17:04.036866 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.723058
I0815 02:17:04.036873 11101 solver.cpp:594]     Test net output #2: loss = 2.41056 (* 1 = 2.41056 loss)
I0815 02:17:04.036890 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8536s
I0815 02:17:04.195768 11101 solver.cpp:312] Iteration 148000 (2.83239 iter/s, 35.3059s/100 iter), loss = 1.51552
I0815 02:17:04.195801 11101 solver.cpp:334]     Train net output #0: loss = 1.56551 (* 1 = 1.56551 loss)
I0815 02:17:04.195806 11101 sgd_solver.cpp:136] Iteration 148000, lr = 0.05375, m = 0.9
I0815 02:17:18.562496 11101 solver.cpp:312] Iteration 148100 (6.96072 iter/s, 14.3663s/100 iter), loss = 2.15583
I0815 02:17:18.562525 11101 solver.cpp:334]     Train net output #0: loss = 2.38146 (* 1 = 2.38146 loss)
I0815 02:17:18.562531 11101 sgd_solver.cpp:136] Iteration 148100, lr = 0.0537187, m = 0.9
I0815 02:17:32.932546 11101 solver.cpp:312] Iteration 148200 (6.95911 iter/s, 14.3697s/100 iter), loss = 1.83253
I0815 02:17:32.932572 11101 solver.cpp:334]     Train net output #0: loss = 1.84792 (* 1 = 1.84792 loss)
I0815 02:17:32.932577 11101 sgd_solver.cpp:136] Iteration 148200, lr = 0.0536875, m = 0.9
I0815 02:17:47.660447 11101 solver.cpp:312] Iteration 148300 (6.79002 iter/s, 14.7275s/100 iter), loss = 1.98733
I0815 02:17:47.660540 11101 solver.cpp:334]     Train net output #0: loss = 1.64796 (* 1 = 1.64796 loss)
I0815 02:17:47.660558 11101 sgd_solver.cpp:136] Iteration 148300, lr = 0.0536563, m = 0.9
I0815 02:18:02.235402 11101 solver.cpp:312] Iteration 148400 (6.86128 iter/s, 14.5745s/100 iter), loss = 2.11635
I0815 02:18:02.235432 11101 solver.cpp:334]     Train net output #0: loss = 2.27612 (* 1 = 2.27612 loss)
I0815 02:18:02.235440 11101 sgd_solver.cpp:136] Iteration 148400, lr = 0.053625, m = 0.9
I0815 02:18:16.920965 11101 solver.cpp:312] Iteration 148500 (6.8096 iter/s, 14.6852s/100 iter), loss = 1.66599
I0815 02:18:16.920994 11101 solver.cpp:334]     Train net output #0: loss = 1.60118 (* 1 = 1.60118 loss)
I0815 02:18:16.921000 11101 sgd_solver.cpp:136] Iteration 148500, lr = 0.0535938, m = 0.9
I0815 02:18:31.370281 11101 solver.cpp:312] Iteration 148600 (6.92093 iter/s, 14.4489s/100 iter), loss = 2.10461
I0815 02:18:31.370373 11101 solver.cpp:334]     Train net output #0: loss = 1.79283 (* 1 = 1.79283 loss)
I0815 02:18:31.370390 11101 sgd_solver.cpp:136] Iteration 148600, lr = 0.0535625, m = 0.9
I0815 02:18:46.041235 11101 solver.cpp:312] Iteration 148700 (6.81638 iter/s, 14.6705s/100 iter), loss = 2.04802
I0815 02:18:46.041316 11101 solver.cpp:334]     Train net output #0: loss = 2.21484 (* 1 = 2.21484 loss)
I0815 02:18:46.041335 11101 sgd_solver.cpp:136] Iteration 148700, lr = 0.0535313, m = 0.9
I0815 02:19:00.459453 11101 solver.cpp:312] Iteration 148800 (6.93586 iter/s, 14.4178s/100 iter), loss = 1.70426
I0815 02:19:00.459478 11101 solver.cpp:334]     Train net output #0: loss = 1.40257 (* 1 = 1.40257 loss)
I0815 02:19:00.459486 11101 sgd_solver.cpp:136] Iteration 148800, lr = 0.0535, m = 0.9
I0815 02:19:15.091084 11101 solver.cpp:312] Iteration 148900 (6.8347 iter/s, 14.6312s/100 iter), loss = 1.84993
I0815 02:19:15.091150 11101 solver.cpp:334]     Train net output #0: loss = 1.70294 (* 1 = 1.70294 loss)
I0815 02:19:15.091157 11101 sgd_solver.cpp:136] Iteration 148900, lr = 0.0534688, m = 0.9
I0815 02:19:29.760241 11101 solver.cpp:312] Iteration 149000 (6.81721 iter/s, 14.6688s/100 iter), loss = 2.13382
I0815 02:19:29.760268 11101 solver.cpp:334]     Train net output #0: loss = 2.05019 (* 1 = 2.05019 loss)
I0815 02:19:29.760273 11101 sgd_solver.cpp:136] Iteration 149000, lr = 0.0534375, m = 0.9
I0815 02:19:44.356317 11101 solver.cpp:312] Iteration 149100 (6.85135 iter/s, 14.5957s/100 iter), loss = 2.16902
I0815 02:19:44.356379 11101 solver.cpp:334]     Train net output #0: loss = 2.29929 (* 1 = 2.29929 loss)
I0815 02:19:44.356403 11101 sgd_solver.cpp:136] Iteration 149100, lr = 0.0534062, m = 0.9
I0815 02:19:58.877856 11101 solver.cpp:312] Iteration 149200 (6.88651 iter/s, 14.5211s/100 iter), loss = 2.10656
I0815 02:19:58.877930 11101 solver.cpp:334]     Train net output #0: loss = 2.42473 (* 1 = 2.42473 loss)
I0815 02:19:58.877938 11101 sgd_solver.cpp:136] Iteration 149200, lr = 0.053375, m = 0.9
I0815 02:20:13.758790 11101 solver.cpp:312] Iteration 149300 (6.72019 iter/s, 14.8805s/100 iter), loss = 2.02626
I0815 02:20:13.758816 11101 solver.cpp:334]     Train net output #0: loss = 1.93497 (* 1 = 1.93497 loss)
I0815 02:20:13.758822 11101 sgd_solver.cpp:136] Iteration 149300, lr = 0.0533438, m = 0.9
I0815 02:20:28.220837 11101 solver.cpp:312] Iteration 149400 (6.91484 iter/s, 14.4616s/100 iter), loss = 2.30564
I0815 02:20:28.220865 11101 solver.cpp:334]     Train net output #0: loss = 2.54672 (* 1 = 2.54672 loss)
I0815 02:20:28.220870 11101 sgd_solver.cpp:136] Iteration 149400, lr = 0.0533125, m = 0.9
I0815 02:20:42.549182 11101 solver.cpp:312] Iteration 149500 (6.97937 iter/s, 14.328s/100 iter), loss = 1.73851
I0815 02:20:42.549249 11101 solver.cpp:334]     Train net output #0: loss = 1.63326 (* 1 = 1.63326 loss)
I0815 02:20:42.549257 11101 sgd_solver.cpp:136] Iteration 149500, lr = 0.0532812, m = 0.9
I0815 02:20:56.996341 11101 solver.cpp:312] Iteration 149600 (6.92197 iter/s, 14.4468s/100 iter), loss = 2.01712
I0815 02:20:56.996368 11101 solver.cpp:334]     Train net output #0: loss = 2.06818 (* 1 = 2.06818 loss)
I0815 02:20:56.996376 11101 sgd_solver.cpp:136] Iteration 149600, lr = 0.05325, m = 0.9
I0815 02:21:11.743230 11101 solver.cpp:312] Iteration 149700 (6.78128 iter/s, 14.7465s/100 iter), loss = 2.14681
I0815 02:21:11.743260 11101 solver.cpp:334]     Train net output #0: loss = 2.10334 (* 1 = 2.10334 loss)
I0815 02:21:11.743266 11101 sgd_solver.cpp:136] Iteration 149700, lr = 0.0532187, m = 0.9
I0815 02:21:26.308456 11101 solver.cpp:312] Iteration 149800 (6.86586 iter/s, 14.5648s/100 iter), loss = 1.98137
I0815 02:21:26.308522 11101 solver.cpp:334]     Train net output #0: loss = 1.91795 (* 1 = 1.91795 loss)
I0815 02:21:26.308529 11101 sgd_solver.cpp:136] Iteration 149800, lr = 0.0531875, m = 0.9
I0815 02:21:40.754070 11101 solver.cpp:312] Iteration 149900 (6.92271 iter/s, 14.4452s/100 iter), loss = 1.89273
I0815 02:21:40.754134 11101 solver.cpp:334]     Train net output #0: loss = 2.0696 (* 1 = 2.0696 loss)
I0815 02:21:40.754153 11101 sgd_solver.cpp:136] Iteration 149900, lr = 0.0531563, m = 0.9
I0815 02:21:55.239208 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_150000.caffemodel
I0815 02:21:55.274425 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_150000.solverstate
I0815 02:21:55.280315 11101 solver.cpp:509] Iteration 150000, Testing net (#0)
I0815 02:22:14.146692 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 02:22:16.175271 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.412529
I0815 02:22:16.175297 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.673528
I0815 02:22:16.175304 11101 solver.cpp:594]     Test net output #2: loss = 2.69251 (* 1 = 2.69251 loss)
I0815 02:22:16.175379 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8945s
I0815 02:22:16.339773 11101 solver.cpp:312] Iteration 150000 (2.81019 iter/s, 35.5847s/100 iter), loss = 1.97643
I0815 02:22:16.339802 11101 solver.cpp:334]     Train net output #0: loss = 1.95264 (* 1 = 1.95264 loss)
I0815 02:22:16.339807 11101 sgd_solver.cpp:136] Iteration 150000, lr = 0.053125, m = 0.9
I0815 02:22:31.200826 11101 solver.cpp:312] Iteration 150100 (6.72918 iter/s, 14.8606s/100 iter), loss = 1.71912
I0815 02:22:31.200877 11101 solver.cpp:334]     Train net output #0: loss = 1.88551 (* 1 = 1.88551 loss)
I0815 02:22:31.200889 11101 sgd_solver.cpp:136] Iteration 150100, lr = 0.0530938, m = 0.9
I0815 02:22:45.774116 11101 solver.cpp:312] Iteration 150200 (6.86206 iter/s, 14.5729s/100 iter), loss = 1.76011
I0815 02:22:45.774173 11101 solver.cpp:334]     Train net output #0: loss = 1.82002 (* 1 = 1.82002 loss)
I0815 02:22:45.774214 11101 sgd_solver.cpp:136] Iteration 150200, lr = 0.0530625, m = 0.9
I0815 02:23:00.562034 11101 solver.cpp:312] Iteration 150300 (6.76246 iter/s, 14.7875s/100 iter), loss = 1.89986
I0815 02:23:00.562062 11101 solver.cpp:334]     Train net output #0: loss = 1.6618 (* 1 = 1.6618 loss)
I0815 02:23:00.562068 11101 sgd_solver.cpp:136] Iteration 150300, lr = 0.0530313, m = 0.9
I0815 02:23:15.304144 11101 solver.cpp:312] Iteration 150400 (6.78348 iter/s, 14.7417s/100 iter), loss = 1.93217
I0815 02:23:15.304172 11101 solver.cpp:334]     Train net output #0: loss = 1.8073 (* 1 = 1.8073 loss)
I0815 02:23:15.304177 11101 sgd_solver.cpp:136] Iteration 150400, lr = 0.053, m = 0.9
I0815 02:23:29.708894 11101 solver.cpp:312] Iteration 150500 (6.94235 iter/s, 14.4044s/100 iter), loss = 1.91701
I0815 02:23:29.709110 11101 solver.cpp:334]     Train net output #0: loss = 1.68644 (* 1 = 1.68644 loss)
I0815 02:23:29.709198 11101 sgd_solver.cpp:136] Iteration 150500, lr = 0.0529688, m = 0.9
I0815 02:23:44.119148 11101 solver.cpp:312] Iteration 150600 (6.9397 iter/s, 14.4099s/100 iter), loss = 1.59104
I0815 02:23:44.119174 11101 solver.cpp:334]     Train net output #0: loss = 1.63196 (* 1 = 1.63196 loss)
I0815 02:23:44.119180 11101 sgd_solver.cpp:136] Iteration 150600, lr = 0.0529375, m = 0.9
I0815 02:23:58.636904 11101 solver.cpp:312] Iteration 150700 (6.88831 iter/s, 14.5174s/100 iter), loss = 2.25513
I0815 02:23:58.636929 11101 solver.cpp:334]     Train net output #0: loss = 2.59924 (* 1 = 2.59924 loss)
I0815 02:23:58.636935 11101 sgd_solver.cpp:136] Iteration 150700, lr = 0.0529063, m = 0.9
I0815 02:24:13.100179 11101 solver.cpp:312] Iteration 150800 (6.91426 iter/s, 14.4629s/100 iter), loss = 2.48738
I0815 02:24:13.100240 11101 solver.cpp:334]     Train net output #0: loss = 2.73347 (* 1 = 2.73347 loss)
I0815 02:24:13.100245 11101 sgd_solver.cpp:136] Iteration 150800, lr = 0.052875, m = 0.9
I0815 02:24:27.927327 11101 solver.cpp:312] Iteration 150900 (6.74457 iter/s, 14.8267s/100 iter), loss = 1.72754
I0815 02:24:27.927398 11101 solver.cpp:334]     Train net output #0: loss = 1.70041 (* 1 = 1.70041 loss)
I0815 02:24:27.927418 11101 sgd_solver.cpp:136] Iteration 150900, lr = 0.0528437, m = 0.9
I0815 02:24:42.625957 11101 solver.cpp:312] Iteration 151000 (6.80354 iter/s, 14.6982s/100 iter), loss = 1.70701
I0815 02:24:42.625984 11101 solver.cpp:334]     Train net output #0: loss = 1.54867 (* 1 = 1.54867 loss)
I0815 02:24:42.625990 11101 sgd_solver.cpp:136] Iteration 151000, lr = 0.0528125, m = 0.9
I0815 02:24:57.454633 11101 solver.cpp:312] Iteration 151100 (6.74388 iter/s, 14.8283s/100 iter), loss = 1.96166
I0815 02:24:57.454864 11101 solver.cpp:334]     Train net output #0: loss = 1.95625 (* 1 = 1.95625 loss)
I0815 02:24:57.454874 11101 sgd_solver.cpp:136] Iteration 151100, lr = 0.0527813, m = 0.9
I0815 02:25:12.291692 11101 solver.cpp:312] Iteration 151200 (6.74007 iter/s, 14.8366s/100 iter), loss = 2.24488
I0815 02:25:12.291719 11101 solver.cpp:334]     Train net output #0: loss = 2.44227 (* 1 = 2.44227 loss)
I0815 02:25:12.291723 11101 sgd_solver.cpp:136] Iteration 151200, lr = 0.05275, m = 0.9
I0815 02:25:26.957798 11101 solver.cpp:312] Iteration 151300 (6.81863 iter/s, 14.6657s/100 iter), loss = 1.83663
I0815 02:25:26.957835 11101 solver.cpp:334]     Train net output #0: loss = 1.94953 (* 1 = 1.94953 loss)
I0815 02:25:26.957841 11101 sgd_solver.cpp:136] Iteration 151300, lr = 0.0527187, m = 0.9
I0815 02:25:41.733352 11101 solver.cpp:312] Iteration 151400 (6.76812 iter/s, 14.7751s/100 iter), loss = 1.72868
I0815 02:25:41.733407 11101 solver.cpp:334]     Train net output #0: loss = 1.98374 (* 1 = 1.98374 loss)
I0815 02:25:41.733413 11101 sgd_solver.cpp:136] Iteration 151400, lr = 0.0526875, m = 0.9
I0815 02:25:56.371037 11101 solver.cpp:312] Iteration 151500 (6.83187 iter/s, 14.6373s/100 iter), loss = 2.52693
I0815 02:25:56.371069 11101 solver.cpp:334]     Train net output #0: loss = 2.9072 (* 1 = 2.9072 loss)
I0815 02:25:56.371074 11101 sgd_solver.cpp:136] Iteration 151500, lr = 0.0526563, m = 0.9
I0815 02:26:10.778177 11101 solver.cpp:312] Iteration 151600 (6.94119 iter/s, 14.4067s/100 iter), loss = 2.01456
I0815 02:26:10.778336 11101 solver.cpp:334]     Train net output #0: loss = 1.85627 (* 1 = 1.85627 loss)
I0815 02:26:10.778355 11101 sgd_solver.cpp:136] Iteration 151600, lr = 0.052625, m = 0.9
I0815 02:26:25.366770 11101 solver.cpp:312] Iteration 151700 (6.85486 iter/s, 14.5882s/100 iter), loss = 2.26853
I0815 02:26:25.367024 11101 solver.cpp:334]     Train net output #0: loss = 2.35095 (* 1 = 2.35095 loss)
I0815 02:26:25.367044 11101 sgd_solver.cpp:136] Iteration 151700, lr = 0.0525937, m = 0.9
I0815 02:26:39.802439 11101 solver.cpp:312] Iteration 151800 (6.92748 iter/s, 14.4353s/100 iter), loss = 1.80266
I0815 02:26:39.802467 11101 solver.cpp:334]     Train net output #0: loss = 2.00477 (* 1 = 2.00477 loss)
I0815 02:26:39.802472 11101 sgd_solver.cpp:136] Iteration 151800, lr = 0.0525625, m = 0.9
I0815 02:26:54.698493 11101 solver.cpp:312] Iteration 151900 (6.71337 iter/s, 14.8956s/100 iter), loss = 2.41493
I0815 02:26:54.698521 11101 solver.cpp:334]     Train net output #0: loss = 2.99373 (* 1 = 2.99373 loss)
I0815 02:26:54.698529 11101 sgd_solver.cpp:136] Iteration 151900, lr = 0.0525313, m = 0.9
I0815 02:27:09.188618 11101 solver.cpp:509] Iteration 152000, Testing net (#0)
I0815 02:27:30.260918 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.418588
I0815 02:27:30.260944 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.671882
I0815 02:27:30.260951 11101 solver.cpp:594]     Test net output #2: loss = 2.72739 (* 1 = 2.72739 loss)
I0815 02:27:30.261004 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0718s
I0815 02:27:30.412931 11101 solver.cpp:312] Iteration 152000 (2.80006 iter/s, 35.7135s/100 iter), loss = 2.07553
I0815 02:27:30.412956 11101 solver.cpp:334]     Train net output #0: loss = 1.86531 (* 1 = 1.86531 loss)
I0815 02:27:30.412961 11101 sgd_solver.cpp:136] Iteration 152000, lr = 0.0525, m = 0.9
I0815 02:27:44.650543 11101 solver.cpp:312] Iteration 152100 (7.02385 iter/s, 14.2372s/100 iter), loss = 2.32777
I0815 02:27:44.650606 11101 solver.cpp:334]     Train net output #0: loss = 2.62416 (* 1 = 2.62416 loss)
I0815 02:27:44.650614 11101 sgd_solver.cpp:136] Iteration 152100, lr = 0.0524688, m = 0.9
I0815 02:27:59.422557 11101 solver.cpp:312] Iteration 152200 (6.76974 iter/s, 14.7716s/100 iter), loss = 1.87093
I0815 02:27:59.422580 11101 solver.cpp:334]     Train net output #0: loss = 1.89605 (* 1 = 1.89605 loss)
I0815 02:27:59.422585 11101 sgd_solver.cpp:136] Iteration 152200, lr = 0.0524375, m = 0.9
I0815 02:28:13.905359 11101 solver.cpp:312] Iteration 152300 (6.90493 iter/s, 14.4824s/100 iter), loss = 1.73494
I0815 02:28:13.905383 11101 solver.cpp:334]     Train net output #0: loss = 1.52813 (* 1 = 1.52813 loss)
I0815 02:28:13.905390 11101 sgd_solver.cpp:136] Iteration 152300, lr = 0.0524063, m = 0.9
I0815 02:28:28.428123 11101 solver.cpp:312] Iteration 152400 (6.88593 iter/s, 14.5224s/100 iter), loss = 2.20722
I0815 02:28:28.428230 11101 solver.cpp:334]     Train net output #0: loss = 2.00348 (* 1 = 2.00348 loss)
I0815 02:28:28.428246 11101 sgd_solver.cpp:136] Iteration 152400, lr = 0.052375, m = 0.9
I0815 02:28:43.080890 11101 solver.cpp:312] Iteration 152500 (6.82484 iter/s, 14.6524s/100 iter), loss = 1.91951
I0815 02:28:43.080921 11101 solver.cpp:334]     Train net output #0: loss = 2.00845 (* 1 = 2.00845 loss)
I0815 02:28:43.080927 11101 sgd_solver.cpp:136] Iteration 152500, lr = 0.0523438, m = 0.9
I0815 02:28:57.780725 11101 solver.cpp:312] Iteration 152600 (6.80298 iter/s, 14.6994s/100 iter), loss = 2.01815
I0815 02:28:57.780791 11101 solver.cpp:334]     Train net output #0: loss = 2.12423 (* 1 = 2.12423 loss)
I0815 02:28:57.780809 11101 sgd_solver.cpp:136] Iteration 152600, lr = 0.0523125, m = 0.9
I0815 02:29:12.629523 11101 solver.cpp:312] Iteration 152700 (6.73474 iter/s, 14.8484s/100 iter), loss = 1.48169
I0815 02:29:12.629580 11101 solver.cpp:334]     Train net output #0: loss = 1.69525 (* 1 = 1.69525 loss)
I0815 02:29:12.629585 11101 sgd_solver.cpp:136] Iteration 152700, lr = 0.0522812, m = 0.9
I0815 02:29:26.911448 11101 solver.cpp:312] Iteration 152800 (7.00205 iter/s, 14.2815s/100 iter), loss = 1.94585
I0815 02:29:26.911473 11101 solver.cpp:334]     Train net output #0: loss = 2.00256 (* 1 = 2.00256 loss)
I0815 02:29:26.911479 11101 sgd_solver.cpp:136] Iteration 152800, lr = 0.05225, m = 0.9
I0815 02:29:41.503448 11101 solver.cpp:312] Iteration 152900 (6.85606 iter/s, 14.5856s/100 iter), loss = 1.95871
I0815 02:29:41.503479 11101 solver.cpp:334]     Train net output #0: loss = 2.07749 (* 1 = 2.07749 loss)
I0815 02:29:41.503485 11101 sgd_solver.cpp:136] Iteration 152900, lr = 0.0522187, m = 0.9
I0815 02:29:56.114917 11101 solver.cpp:312] Iteration 153000 (6.84413 iter/s, 14.6111s/100 iter), loss = 2.48644
I0815 02:29:56.114979 11101 solver.cpp:334]     Train net output #0: loss = 2.48387 (* 1 = 2.48387 loss)
I0815 02:29:56.114985 11101 sgd_solver.cpp:136] Iteration 153000, lr = 0.0521875, m = 0.9
I0815 02:30:10.817548 11101 solver.cpp:312] Iteration 153100 (6.80169 iter/s, 14.7022s/100 iter), loss = 2.0495
I0815 02:30:10.817574 11101 solver.cpp:334]     Train net output #0: loss = 2.34354 (* 1 = 2.34354 loss)
I0815 02:30:10.817577 11101 sgd_solver.cpp:136] Iteration 153100, lr = 0.0521562, m = 0.9
I0815 02:30:25.372648 11101 solver.cpp:312] Iteration 153200 (6.87063 iter/s, 14.5547s/100 iter), loss = 1.92609
I0815 02:30:25.372671 11101 solver.cpp:334]     Train net output #0: loss = 1.83911 (* 1 = 1.83911 loss)
I0815 02:30:25.372675 11101 sgd_solver.cpp:136] Iteration 153200, lr = 0.052125, m = 0.9
I0815 02:30:40.021289 11101 solver.cpp:312] Iteration 153300 (6.82676 iter/s, 14.6482s/100 iter), loss = 2.02622
I0815 02:30:40.021344 11101 solver.cpp:334]     Train net output #0: loss = 1.99584 (* 1 = 1.99584 loss)
I0815 02:30:40.021355 11101 sgd_solver.cpp:136] Iteration 153300, lr = 0.0520938, m = 0.9
I0815 02:30:54.459695 11101 solver.cpp:312] Iteration 153400 (6.92616 iter/s, 14.438s/100 iter), loss = 2.13854
I0815 02:30:54.459723 11101 solver.cpp:334]     Train net output #0: loss = 1.96993 (* 1 = 1.96993 loss)
I0815 02:30:54.459730 11101 sgd_solver.cpp:136] Iteration 153400, lr = 0.0520625, m = 0.9
I0815 02:31:09.040362 11101 solver.cpp:312] Iteration 153500 (6.85858 iter/s, 14.5803s/100 iter), loss = 1.57167
I0815 02:31:09.040426 11101 solver.cpp:334]     Train net output #0: loss = 1.56839 (* 1 = 1.56839 loss)
I0815 02:31:09.040444 11101 sgd_solver.cpp:136] Iteration 153500, lr = 0.0520312, m = 0.9
I0815 02:31:24.089638 11101 solver.cpp:312] Iteration 153600 (6.64502 iter/s, 15.0489s/100 iter), loss = 1.66856
I0815 02:31:24.089699 11101 solver.cpp:334]     Train net output #0: loss = 1.54207 (* 1 = 1.54207 loss)
I0815 02:31:24.089705 11101 sgd_solver.cpp:136] Iteration 153600, lr = 0.052, m = 0.9
I0815 02:31:38.842947 11101 solver.cpp:312] Iteration 153700 (6.77833 iter/s, 14.7529s/100 iter), loss = 2.07669
I0815 02:31:38.842974 11101 solver.cpp:334]     Train net output #0: loss = 1.86728 (* 1 = 1.86728 loss)
I0815 02:31:38.842980 11101 sgd_solver.cpp:136] Iteration 153700, lr = 0.0519688, m = 0.9
I0815 02:31:53.782820 11101 solver.cpp:312] Iteration 153800 (6.69368 iter/s, 14.9395s/100 iter), loss = 1.9714
I0815 02:31:53.782847 11101 solver.cpp:334]     Train net output #0: loss = 2.15098 (* 1 = 2.15098 loss)
I0815 02:31:53.782852 11101 sgd_solver.cpp:136] Iteration 153800, lr = 0.0519375, m = 0.9
I0815 02:32:08.356617 11101 solver.cpp:312] Iteration 153900 (6.86182 iter/s, 14.5734s/100 iter), loss = 1.81632
I0815 02:32:08.356695 11101 solver.cpp:334]     Train net output #0: loss = 2.00768 (* 1 = 2.00768 loss)
I0815 02:32:08.356711 11101 sgd_solver.cpp:136] Iteration 153900, lr = 0.0519063, m = 0.9
I0815 02:32:23.539839 11101 solver.cpp:509] Iteration 154000, Testing net (#0)
I0815 02:32:44.576251 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.430529
I0815 02:32:44.576301 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.690646
I0815 02:32:44.576308 11101 solver.cpp:594]     Test net output #2: loss = 2.59505 (* 1 = 2.59505 loss)
I0815 02:32:44.576325 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0359s
I0815 02:32:44.717645 11101 solver.cpp:312] Iteration 154000 (2.75027 iter/s, 36.36s/100 iter), loss = 1.81332
I0815 02:32:44.717675 11101 solver.cpp:334]     Train net output #0: loss = 2.02564 (* 1 = 2.02564 loss)
I0815 02:32:44.717681 11101 sgd_solver.cpp:136] Iteration 154000, lr = 0.051875, m = 0.9
I0815 02:32:59.256512 11101 solver.cpp:312] Iteration 154100 (6.87831 iter/s, 14.5385s/100 iter), loss = 1.89713
I0815 02:32:59.256539 11101 solver.cpp:334]     Train net output #0: loss = 1.72237 (* 1 = 1.72237 loss)
I0815 02:32:59.256546 11101 sgd_solver.cpp:136] Iteration 154100, lr = 0.0518438, m = 0.9
I0815 02:33:13.668815 11101 solver.cpp:312] Iteration 154200 (6.93871 iter/s, 14.4119s/100 iter), loss = 2.09289
I0815 02:33:13.668848 11101 solver.cpp:334]     Train net output #0: loss = 2.13161 (* 1 = 2.13161 loss)
I0815 02:33:13.668853 11101 sgd_solver.cpp:136] Iteration 154200, lr = 0.0518125, m = 0.9
I0815 02:33:28.453510 11101 solver.cpp:312] Iteration 154300 (6.76394 iter/s, 14.7843s/100 iter), loss = 2.14771
I0815 02:33:28.453615 11101 solver.cpp:334]     Train net output #0: loss = 2.27067 (* 1 = 2.27067 loss)
I0815 02:33:28.453626 11101 sgd_solver.cpp:136] Iteration 154300, lr = 0.0517812, m = 0.9
I0815 02:33:43.160779 11101 solver.cpp:312] Iteration 154400 (6.79955 iter/s, 14.7069s/100 iter), loss = 2.00536
I0815 02:33:43.160805 11101 solver.cpp:334]     Train net output #0: loss = 2.11635 (* 1 = 2.11635 loss)
I0815 02:33:43.160810 11101 sgd_solver.cpp:136] Iteration 154400, lr = 0.05175, m = 0.9
I0815 02:33:57.895938 11101 solver.cpp:312] Iteration 154500 (6.78668 iter/s, 14.7348s/100 iter), loss = 2.02021
I0815 02:33:57.895967 11101 solver.cpp:334]     Train net output #0: loss = 2.12011 (* 1 = 2.12011 loss)
I0815 02:33:57.895973 11101 sgd_solver.cpp:136] Iteration 154500, lr = 0.0517187, m = 0.9
I0815 02:34:12.404769 11101 solver.cpp:312] Iteration 154600 (6.89254 iter/s, 14.5084s/100 iter), loss = 1.66718
I0815 02:34:12.404989 11101 solver.cpp:334]     Train net output #0: loss = 1.9355 (* 1 = 1.9355 loss)
I0815 02:34:12.404995 11101 sgd_solver.cpp:136] Iteration 154600, lr = 0.0516875, m = 0.9
I0815 02:34:26.751073 11101 solver.cpp:312] Iteration 154700 (6.97063 iter/s, 14.3459s/100 iter), loss = 2.36703
I0815 02:34:26.751106 11101 solver.cpp:334]     Train net output #0: loss = 2.12418 (* 1 = 2.12418 loss)
I0815 02:34:26.751112 11101 sgd_solver.cpp:136] Iteration 154700, lr = 0.0516562, m = 0.9
I0815 02:34:41.228093 11101 solver.cpp:312] Iteration 154800 (6.90769 iter/s, 14.4766s/100 iter), loss = 1.65926
I0815 02:34:41.228116 11101 solver.cpp:334]     Train net output #0: loss = 1.76709 (* 1 = 1.76709 loss)
I0815 02:34:41.228121 11101 sgd_solver.cpp:136] Iteration 154800, lr = 0.051625, m = 0.9
I0815 02:34:55.970751 11101 solver.cpp:312] Iteration 154900 (6.78323 iter/s, 14.7422s/100 iter), loss = 1.66448
I0815 02:34:55.970824 11101 solver.cpp:334]     Train net output #0: loss = 1.36272 (* 1 = 1.36272 loss)
I0815 02:34:55.970832 11101 sgd_solver.cpp:136] Iteration 154900, lr = 0.0515938, m = 0.9
I0815 02:35:10.596211 11101 solver.cpp:312] Iteration 155000 (6.83758 iter/s, 14.6251s/100 iter), loss = 1.95899
I0815 02:35:10.596262 11101 solver.cpp:334]     Train net output #0: loss = 1.8961 (* 1 = 1.8961 loss)
I0815 02:35:10.596278 11101 sgd_solver.cpp:136] Iteration 155000, lr = 0.0515625, m = 0.9
I0815 02:35:25.532726 11101 solver.cpp:312] Iteration 155100 (6.69519 iter/s, 14.9361s/100 iter), loss = 1.81676
I0815 02:35:25.536159 11101 solver.cpp:334]     Train net output #0: loss = 1.82477 (* 1 = 1.82477 loss)
I0815 02:35:25.536203 11101 sgd_solver.cpp:136] Iteration 155100, lr = 0.0515313, m = 0.9
I0815 02:35:40.052049 11101 solver.cpp:312] Iteration 155200 (6.88756 iter/s, 14.5189s/100 iter), loss = 1.71129
I0815 02:35:40.052156 11101 solver.cpp:334]     Train net output #0: loss = 1.43707 (* 1 = 1.43707 loss)
I0815 02:35:40.052177 11101 sgd_solver.cpp:136] Iteration 155200, lr = 0.0515, m = 0.9
I0815 02:35:54.683557 11101 solver.cpp:312] Iteration 155300 (6.83475 iter/s, 14.6311s/100 iter), loss = 1.86261
I0815 02:35:54.683586 11101 solver.cpp:334]     Train net output #0: loss = 1.76764 (* 1 = 1.76764 loss)
I0815 02:35:54.683593 11101 sgd_solver.cpp:136] Iteration 155300, lr = 0.0514688, m = 0.9
I0815 02:36:09.423744 11101 solver.cpp:312] Iteration 155400 (6.78436 iter/s, 14.7398s/100 iter), loss = 1.69951
I0815 02:36:09.423774 11101 solver.cpp:334]     Train net output #0: loss = 1.39472 (* 1 = 1.39472 loss)
I0815 02:36:09.423780 11101 sgd_solver.cpp:136] Iteration 155400, lr = 0.0514375, m = 0.9
I0815 02:36:23.898676 11101 solver.cpp:312] Iteration 155500 (6.90869 iter/s, 14.4745s/100 iter), loss = 1.91052
I0815 02:36:23.898753 11101 solver.cpp:334]     Train net output #0: loss = 2.03404 (* 1 = 2.03404 loss)
I0815 02:36:23.898761 11101 sgd_solver.cpp:136] Iteration 155500, lr = 0.0514063, m = 0.9
I0815 02:36:38.372755 11101 solver.cpp:312] Iteration 155600 (6.90909 iter/s, 14.4737s/100 iter), loss = 1.97708
I0815 02:36:38.372783 11101 solver.cpp:334]     Train net output #0: loss = 1.93341 (* 1 = 1.93341 loss)
I0815 02:36:38.372789 11101 sgd_solver.cpp:136] Iteration 155600, lr = 0.051375, m = 0.9
I0815 02:36:53.117322 11101 solver.cpp:312] Iteration 155700 (6.78234 iter/s, 14.7442s/100 iter), loss = 2.2097
I0815 02:36:53.117344 11101 solver.cpp:334]     Train net output #0: loss = 2.16555 (* 1 = 2.16555 loss)
I0815 02:36:53.117350 11101 sgd_solver.cpp:136] Iteration 155700, lr = 0.0513438, m = 0.9
I0815 02:37:08.002460 11101 solver.cpp:312] Iteration 155800 (6.7183 iter/s, 14.8847s/100 iter), loss = 1.98803
I0815 02:37:08.002522 11101 solver.cpp:334]     Train net output #0: loss = 2.732 (* 1 = 2.732 loss)
I0815 02:37:08.002528 11101 sgd_solver.cpp:136] Iteration 155800, lr = 0.0513125, m = 0.9
I0815 02:37:22.497714 11101 solver.cpp:312] Iteration 155900 (6.899 iter/s, 14.4949s/100 iter), loss = 1.97942
I0815 02:37:22.497745 11101 solver.cpp:334]     Train net output #0: loss = 2.07533 (* 1 = 2.07533 loss)
I0815 02:37:22.497750 11101 sgd_solver.cpp:136] Iteration 155900, lr = 0.0512813, m = 0.9
I0815 02:37:37.236438 11101 solver.cpp:509] Iteration 156000, Testing net (#0)
I0815 02:37:53.149019 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 02:37:57.890687 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.446765
I0815 02:37:57.890713 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.701411
I0815 02:37:57.890722 11101 solver.cpp:594]     Test net output #2: loss = 2.58752 (* 1 = 2.58752 loss)
I0815 02:37:57.890740 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6537s
I0815 02:37:58.055923 11101 solver.cpp:312] Iteration 156000 (2.81237 iter/s, 35.5572s/100 iter), loss = 1.88002
I0815 02:37:58.055987 11101 solver.cpp:334]     Train net output #0: loss = 1.47039 (* 1 = 1.47039 loss)
I0815 02:37:58.056007 11101 sgd_solver.cpp:136] Iteration 156000, lr = 0.05125, m = 0.9
I0815 02:38:12.480156 11101 solver.cpp:312] Iteration 156100 (6.93297 iter/s, 14.4238s/100 iter), loss = 2.38244
I0815 02:38:12.480222 11101 solver.cpp:334]     Train net output #0: loss = 2.04534 (* 1 = 2.04534 loss)
I0815 02:38:12.480239 11101 sgd_solver.cpp:136] Iteration 156100, lr = 0.0512187, m = 0.9
I0815 02:38:27.450321 11101 solver.cpp:312] Iteration 156200 (6.68014 iter/s, 14.9697s/100 iter), loss = 2.0287
I0815 02:38:27.450510 11101 solver.cpp:334]     Train net output #0: loss = 2.01127 (* 1 = 2.01127 loss)
I0815 02:38:27.450529 11101 sgd_solver.cpp:136] Iteration 156200, lr = 0.0511875, m = 0.9
I0815 02:38:42.149965 11101 solver.cpp:312] Iteration 156300 (6.80308 iter/s, 14.6992s/100 iter), loss = 2.2665
I0815 02:38:42.150025 11101 solver.cpp:334]     Train net output #0: loss = 2.61776 (* 1 = 2.61776 loss)
I0815 02:38:42.150037 11101 sgd_solver.cpp:136] Iteration 156300, lr = 0.0511562, m = 0.9
I0815 02:38:56.653170 11101 solver.cpp:312] Iteration 156400 (6.89522 iter/s, 14.5028s/100 iter), loss = 1.86682
I0815 02:38:56.653228 11101 solver.cpp:334]     Train net output #0: loss = 1.86627 (* 1 = 1.86627 loss)
I0815 02:38:56.653234 11101 sgd_solver.cpp:136] Iteration 156400, lr = 0.051125, m = 0.9
I0815 02:39:11.373411 11101 solver.cpp:312] Iteration 156500 (6.79356 iter/s, 14.7198s/100 iter), loss = 1.64007
I0815 02:39:11.373495 11101 solver.cpp:334]     Train net output #0: loss = 1.76513 (* 1 = 1.76513 loss)
I0815 02:39:11.373502 11101 sgd_solver.cpp:136] Iteration 156500, lr = 0.0510938, m = 0.9
I0815 02:39:26.119568 11101 solver.cpp:312] Iteration 156600 (6.78161 iter/s, 14.7458s/100 iter), loss = 1.82619
I0815 02:39:26.119596 11101 solver.cpp:334]     Train net output #0: loss = 2.00044 (* 1 = 2.00044 loss)
I0815 02:39:26.119601 11101 sgd_solver.cpp:136] Iteration 156600, lr = 0.0510625, m = 0.9
I0815 02:39:40.822387 11101 solver.cpp:312] Iteration 156700 (6.80161 iter/s, 14.7024s/100 iter), loss = 1.51491
I0815 02:39:40.822417 11101 solver.cpp:334]     Train net output #0: loss = 1.29423 (* 1 = 1.29423 loss)
I0815 02:39:40.822420 11101 sgd_solver.cpp:136] Iteration 156700, lr = 0.0510313, m = 0.9
I0815 02:39:55.396073 11101 solver.cpp:312] Iteration 156800 (6.86187 iter/s, 14.5733s/100 iter), loss = 2.19124
I0815 02:39:55.396142 11101 solver.cpp:334]     Train net output #0: loss = 2.21703 (* 1 = 2.21703 loss)
I0815 02:39:55.396150 11101 sgd_solver.cpp:136] Iteration 156800, lr = 0.051, m = 0.9
I0815 02:40:09.941570 11101 solver.cpp:312] Iteration 156900 (6.87517 iter/s, 14.5451s/100 iter), loss = 1.66719
I0815 02:40:09.941606 11101 solver.cpp:334]     Train net output #0: loss = 1.56864 (* 1 = 1.56864 loss)
I0815 02:40:09.941612 11101 sgd_solver.cpp:136] Iteration 156900, lr = 0.0509688, m = 0.9
I0815 02:40:24.785554 11101 solver.cpp:312] Iteration 157000 (6.73692 iter/s, 14.8436s/100 iter), loss = 1.96876
I0815 02:40:24.785620 11101 solver.cpp:334]     Train net output #0: loss = 1.86316 (* 1 = 1.86316 loss)
I0815 02:40:24.785637 11101 sgd_solver.cpp:136] Iteration 157000, lr = 0.0509375, m = 0.9
I0815 02:40:39.189769 11101 solver.cpp:312] Iteration 157100 (6.9426 iter/s, 14.4038s/100 iter), loss = 1.82223
I0815 02:40:39.189872 11101 solver.cpp:334]     Train net output #0: loss = 2.19178 (* 1 = 2.19178 loss)
I0815 02:40:39.189891 11101 sgd_solver.cpp:136] Iteration 157100, lr = 0.0509063, m = 0.9
I0815 02:40:53.632530 11101 solver.cpp:312] Iteration 157200 (6.92407 iter/s, 14.4424s/100 iter), loss = 2.15372
I0815 02:40:53.632599 11101 solver.cpp:334]     Train net output #0: loss = 2.62897 (* 1 = 2.62897 loss)
I0815 02:40:53.632619 11101 sgd_solver.cpp:136] Iteration 157200, lr = 0.050875, m = 0.9
I0815 02:41:08.084651 11101 solver.cpp:312] Iteration 157300 (6.91959 iter/s, 14.4517s/100 iter), loss = 2.27258
I0815 02:41:08.084676 11101 solver.cpp:334]     Train net output #0: loss = 2.09918 (* 1 = 2.09918 loss)
I0815 02:41:08.084681 11101 sgd_solver.cpp:136] Iteration 157300, lr = 0.0508438, m = 0.9
I0815 02:41:22.728524 11101 solver.cpp:312] Iteration 157400 (6.82898 iter/s, 14.6435s/100 iter), loss = 1.8835
I0815 02:41:22.728770 11101 solver.cpp:334]     Train net output #0: loss = 1.90165 (* 1 = 1.90165 loss)
I0815 02:41:22.728880 11101 sgd_solver.cpp:136] Iteration 157400, lr = 0.0508125, m = 0.9
I0815 02:41:37.146520 11101 solver.cpp:312] Iteration 157500 (6.93597 iter/s, 14.4176s/100 iter), loss = 1.72665
I0815 02:41:37.146553 11101 solver.cpp:334]     Train net output #0: loss = 1.9209 (* 1 = 1.9209 loss)
I0815 02:41:37.146559 11101 sgd_solver.cpp:136] Iteration 157500, lr = 0.0507812, m = 0.9
I0815 02:41:51.701408 11101 solver.cpp:312] Iteration 157600 (6.87073 iter/s, 14.5545s/100 iter), loss = 2.06443
I0815 02:41:51.701437 11101 solver.cpp:334]     Train net output #0: loss = 1.83981 (* 1 = 1.83981 loss)
I0815 02:41:51.701443 11101 sgd_solver.cpp:136] Iteration 157600, lr = 0.05075, m = 0.9
I0815 02:42:06.777251 11101 solver.cpp:312] Iteration 157700 (6.63331 iter/s, 15.0754s/100 iter), loss = 1.98199
I0815 02:42:06.777308 11101 solver.cpp:334]     Train net output #0: loss = 2.41227 (* 1 = 2.41227 loss)
I0815 02:42:06.777313 11101 sgd_solver.cpp:136] Iteration 157700, lr = 0.0507188, m = 0.9
I0815 02:42:21.430613 11101 solver.cpp:312] Iteration 157800 (6.82456 iter/s, 14.653s/100 iter), loss = 2.00216
I0815 02:42:21.430681 11101 solver.cpp:334]     Train net output #0: loss = 2.138 (* 1 = 2.138 loss)
I0815 02:42:21.430698 11101 sgd_solver.cpp:136] Iteration 157800, lr = 0.0506875, m = 0.9
I0815 02:42:35.852124 11101 solver.cpp:312] Iteration 157900 (6.93428 iter/s, 14.4211s/100 iter), loss = 2.02031
I0815 02:42:35.852159 11101 solver.cpp:334]     Train net output #0: loss = 1.74388 (* 1 = 1.74388 loss)
I0815 02:42:35.852165 11101 sgd_solver.cpp:136] Iteration 157900, lr = 0.0506562, m = 0.9
I0815 02:42:50.172230 11101 solver.cpp:509] Iteration 158000, Testing net (#0)
I0815 02:42:58.822458 11099 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 02:43:11.329831 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.449882
I0815 02:43:11.329859 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.702881
I0815 02:43:11.329866 11101 solver.cpp:594]     Test net output #2: loss = 2.52947 (* 1 = 2.52947 loss)
I0815 02:43:11.329888 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1571s
I0815 02:43:11.488935 11101 solver.cpp:312] Iteration 158000 (2.80616 iter/s, 35.6358s/100 iter), loss = 1.93965
I0815 02:43:11.488960 11101 solver.cpp:334]     Train net output #0: loss = 1.42379 (* 1 = 1.42379 loss)
I0815 02:43:11.488965 11101 sgd_solver.cpp:136] Iteration 158000, lr = 0.050625, m = 0.9
I0815 02:43:26.268648 11101 solver.cpp:312] Iteration 158100 (6.76622 iter/s, 14.7793s/100 iter), loss = 1.92561
I0815 02:43:26.268739 11101 solver.cpp:334]     Train net output #0: loss = 2.00675 (* 1 = 2.00675 loss)
I0815 02:43:26.268759 11101 sgd_solver.cpp:136] Iteration 158100, lr = 0.0505937, m = 0.9
I0815 02:43:40.738504 11101 solver.cpp:312] Iteration 158200 (6.91111 iter/s, 14.4695s/100 iter), loss = 1.86
I0815 02:43:40.738533 11101 solver.cpp:334]     Train net output #0: loss = 1.78696 (* 1 = 1.78696 loss)
I0815 02:43:40.738538 11101 sgd_solver.cpp:136] Iteration 158200, lr = 0.0505625, m = 0.9
I0815 02:43:55.532541 11101 solver.cpp:312] Iteration 158300 (6.75967 iter/s, 14.7936s/100 iter), loss = 2.06855
I0815 02:43:55.532572 11101 solver.cpp:334]     Train net output #0: loss = 2.25328 (* 1 = 2.25328 loss)
I0815 02:43:55.532578 11101 sgd_solver.cpp:136] Iteration 158300, lr = 0.0505312, m = 0.9
I0815 02:44:10.318951 11101 solver.cpp:312] Iteration 158400 (6.76315 iter/s, 14.786s/100 iter), loss = 2.29788
I0815 02:44:10.319046 11101 solver.cpp:334]     Train net output #0: loss = 1.96128 (* 1 = 1.96128 loss)
I0815 02:44:10.319066 11101 sgd_solver.cpp:136] Iteration 158400, lr = 0.0505, m = 0.9
I0815 02:44:24.985608 11101 solver.cpp:312] Iteration 158500 (6.81837 iter/s, 14.6663s/100 iter), loss = 1.88011
I0815 02:44:24.985633 11101 solver.cpp:334]     Train net output #0: loss = 2.0629 (* 1 = 2.0629 loss)
I0815 02:44:24.985637 11101 sgd_solver.cpp:136] Iteration 158500, lr = 0.0504688, m = 0.9
I0815 02:44:39.480128 11101 solver.cpp:312] Iteration 158600 (6.89935 iter/s, 14.4941s/100 iter), loss = 1.81964
I0815 02:44:39.480164 11101 solver.cpp:334]     Train net output #0: loss = 1.7796 (* 1 = 1.7796 loss)
I0815 02:44:39.480171 11101 sgd_solver.cpp:136] Iteration 158600, lr = 0.0504375, m = 0.9
I0815 02:44:54.108217 11101 solver.cpp:312] Iteration 158700 (6.83635 iter/s, 14.6277s/100 iter), loss = 1.87718
I0815 02:44:54.108273 11101 solver.cpp:334]     Train net output #0: loss = 1.80537 (* 1 = 1.80537 loss)
I0815 02:44:54.108278 11101 sgd_solver.cpp:136] Iteration 158700, lr = 0.0504063, m = 0.9
I0815 02:45:08.832964 11101 solver.cpp:312] Iteration 158800 (6.79148 iter/s, 14.7243s/100 iter), loss = 1.98979
I0815 02:45:08.833029 11101 solver.cpp:334]     Train net output #0: loss = 1.81289 (* 1 = 1.81289 loss)
I0815 02:45:08.833047 11101 sgd_solver.cpp:136] Iteration 158800, lr = 0.050375, m = 0.9
I0815 02:45:23.652274 11101 solver.cpp:312] Iteration 158900 (6.74814 iter/s, 14.8189s/100 iter), loss = 1.88008
I0815 02:45:23.652338 11101 solver.cpp:334]     Train net output #0: loss = 1.93983 (* 1 = 1.93983 loss)
I0815 02:45:23.652356 11101 sgd_solver.cpp:136] Iteration 158900, lr = 0.0503438, m = 0.9
I0815 02:45:38.292217 11101 solver.cpp:312] Iteration 159000 (6.83082 iter/s, 14.6395s/100 iter), loss = 2.4124
I0815 02:45:38.292328 11101 solver.cpp:334]     Train net output #0: loss = 2.76828 (* 1 = 2.76828 loss)
I0815 02:45:38.292348 11101 sgd_solver.cpp:136] Iteration 159000, lr = 0.0503125, m = 0.9
I0815 02:45:52.760220 11101 solver.cpp:312] Iteration 159100 (6.91199 iter/s, 14.4676s/100 iter), loss = 1.93236
I0815 02:45:52.760252 11101 solver.cpp:334]     Train net output #0: loss = 1.91831 (* 1 = 1.91831 loss)
I0815 02:45:52.760258 11101 sgd_solver.cpp:136] Iteration 159100, lr = 0.0502813, m = 0.9
I0815 02:46:07.629030 11101 solver.cpp:312] Iteration 159200 (6.72567 iter/s, 14.8684s/100 iter), loss = 2.10091
I0815 02:46:07.629243 11101 solver.cpp:334]     Train net output #0: loss = 2.37937 (* 1 = 2.37937 loss)
I0815 02:46:07.629353 11101 sgd_solver.cpp:136] Iteration 159200, lr = 0.05025, m = 0.9
I0815 02:46:22.691133 11101 solver.cpp:312] Iteration 159300 (6.63936 iter/s, 15.0617s/100 iter), loss = 2.05769
I0815 02:46:22.691229 11101 solver.cpp:334]     Train net output #0: loss = 2.01537 (* 1 = 2.01537 loss)
I0815 02:46:22.691248 11101 sgd_solver.cpp:136] Iteration 159300, lr = 0.0502187, m = 0.9
I0815 02:46:37.486227 11101 solver.cpp:312] Iteration 159400 (6.75918 iter/s, 14.7947s/100 iter), loss = 2.39649
I0815 02:46:37.486277 11101 solver.cpp:334]     Train net output #0: loss = 2.50336 (* 1 = 2.50336 loss)
I0815 02:46:37.486290 11101 sgd_solver.cpp:136] Iteration 159400, lr = 0.0501875, m = 0.9
I0815 02:46:52.044464 11101 solver.cpp:312] Iteration 159500 (6.86915 iter/s, 14.5578s/100 iter), loss = 1.99356
I0815 02:46:52.044663 11101 solver.cpp:334]     Train net output #0: loss = 1.84206 (* 1 = 1.84206 loss)
I0815 02:46:52.044772 11101 sgd_solver.cpp:136] Iteration 159500, lr = 0.0501562, m = 0.9
I0815 02:47:06.507045 11101 solver.cpp:312] Iteration 159600 (6.91459 iter/s, 14.4622s/100 iter), loss = 2.08401
I0815 02:47:06.507119 11101 solver.cpp:334]     Train net output #0: loss = 2.17573 (* 1 = 2.17573 loss)
I0815 02:47:06.507133 11101 sgd_solver.cpp:136] Iteration 159600, lr = 0.050125, m = 0.9
I0815 02:47:21.069834 11101 solver.cpp:312] Iteration 159700 (6.86701 iter/s, 14.5624s/100 iter), loss = 1.84751
I0815 02:47:21.069861 11101 solver.cpp:334]     Train net output #0: loss = 1.9138 (* 1 = 1.9138 loss)
I0815 02:47:21.069869 11101 sgd_solver.cpp:136] Iteration 159700, lr = 0.0500937, m = 0.9
I0815 02:47:35.777850 11101 solver.cpp:312] Iteration 159800 (6.7992 iter/s, 14.7076s/100 iter), loss = 2.10391
I0815 02:47:35.777879 11101 solver.cpp:334]     Train net output #0: loss = 2.02514 (* 1 = 2.02514 loss)
I0815 02:47:35.777885 11101 sgd_solver.cpp:136] Iteration 159800, lr = 0.0500625, m = 0.9
I0815 02:47:50.682049 11101 solver.cpp:312] Iteration 159900 (6.7097 iter/s, 14.9038s/100 iter), loss = 1.3935
I0815 02:47:50.682152 11101 solver.cpp:334]     Train net output #0: loss = 1.31387 (* 1 = 1.31387 loss)
I0815 02:47:50.682170 11101 sgd_solver.cpp:136] Iteration 159900, lr = 0.0500313, m = 0.9
I0815 02:48:05.137437 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_160000.caffemodel
I0815 02:48:05.156078 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_160000.solverstate
I0815 02:48:05.161319 11101 solver.cpp:509] Iteration 160000, Testing net (#0)
I0815 02:48:26.615269 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.39347
I0815 02:48:26.615324 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.645706
I0815 02:48:26.615331 11101 solver.cpp:594]     Test net output #2: loss = 2.87466 (* 1 = 2.87466 loss)
I0815 02:48:26.615352 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.4534s
I0815 02:48:26.782217 11101 solver.cpp:312] Iteration 160000 (2.77015 iter/s, 36.0992s/100 iter), loss = 1.89433
I0815 02:48:26.782243 11101 solver.cpp:334]     Train net output #0: loss = 2.16011 (* 1 = 2.16011 loss)
I0815 02:48:26.782248 11101 sgd_solver.cpp:136] Iteration 160000, lr = 0.05, m = 0.9
I0815 02:48:41.249524 11101 solver.cpp:312] Iteration 160100 (6.91233 iter/s, 14.4669s/100 iter), loss = 1.6527
I0815 02:48:41.249549 11101 solver.cpp:334]     Train net output #0: loss = 1.83921 (* 1 = 1.83921 loss)
I0815 02:48:41.249555 11101 sgd_solver.cpp:136] Iteration 160100, lr = 0.0499687, m = 0.9
I0815 02:48:56.115849 11101 solver.cpp:312] Iteration 160200 (6.7268 iter/s, 14.8659s/100 iter), loss = 2.0964
I0815 02:48:56.115875 11101 solver.cpp:334]     Train net output #0: loss = 1.81276 (* 1 = 1.81276 loss)
I0815 02:48:56.115881 11101 sgd_solver.cpp:136] Iteration 160200, lr = 0.0499375, m = 0.9
I0815 02:49:10.496299 11101 solver.cpp:312] Iteration 160300 (6.95408 iter/s, 14.38s/100 iter), loss = 1.97883
I0815 02:49:10.496412 11101 solver.cpp:334]     Train net output #0: loss = 2.27036 (* 1 = 2.27036 loss)
I0815 02:49:10.496428 11101 sgd_solver.cpp:136] Iteration 160300, lr = 0.0499063, m = 0.9
I0815 02:49:25.288777 11101 solver.cpp:312] Iteration 160400 (6.76038 iter/s, 14.7921s/100 iter), loss = 1.83497
I0815 02:49:25.288805 11101 solver.cpp:334]     Train net output #0: loss = 1.54969 (* 1 = 1.54969 loss)
I0815 02:49:25.288811 11101 sgd_solver.cpp:136] Iteration 160400, lr = 0.049875, m = 0.9
I0815 02:49:39.935947 11101 solver.cpp:312] Iteration 160500 (6.82745 iter/s, 14.6468s/100 iter), loss = 2.08958
I0815 02:49:39.935979 11101 solver.cpp:334]     Train net output #0: loss = 2.17947 (* 1 = 2.17947 loss)
I0815 02:49:39.935986 11101 sgd_solver.cpp:136] Iteration 160500, lr = 0.0498438, m = 0.9
I0815 02:49:54.729712 11101 solver.cpp:312] Iteration 160600 (6.75979 iter/s, 14.7934s/100 iter), loss = 1.79063
I0815 02:49:54.729764 11101 solver.cpp:334]     Train net output #0: loss = 1.87635 (* 1 = 1.87635 loss)
I0815 02:49:54.729771 11101 sgd_solver.cpp:136] Iteration 160600, lr = 0.0498125, m = 0.9
I0815 02:50:09.541515 11101 solver.cpp:312] Iteration 160700 (6.75156 iter/s, 14.8114s/100 iter), loss = 1.62913
I0815 02:50:09.541544 11101 solver.cpp:334]     Train net output #0: loss = 1.65424 (* 1 = 1.65424 loss)
I0815 02:50:09.541550 11101 sgd_solver.cpp:136] Iteration 160700, lr = 0.0497813, m = 0.9
I0815 02:50:24.297255 11101 solver.cpp:312] Iteration 160800 (6.77721 iter/s, 14.7553s/100 iter), loss = 2.16585
I0815 02:50:24.297283 11101 solver.cpp:334]     Train net output #0: loss = 2.54192 (* 1 = 2.54192 loss)
I0815 02:50:24.297289 11101 sgd_solver.cpp:136] Iteration 160800, lr = 0.04975, m = 0.9
I0815 02:50:38.995544 11101 solver.cpp:312] Iteration 160900 (6.8037 iter/s, 14.6979s/100 iter), loss = 1.76707
I0815 02:50:38.995627 11101 solver.cpp:334]     Train net output #0: loss = 1.83858 (* 1 = 1.83858 loss)
I0815 02:50:38.995641 11101 sgd_solver.cpp:136] Iteration 160900, lr = 0.0497187, m = 0.9
I0815 02:50:53.573001 11101 solver.cpp:312] Iteration 161000 (6.8601 iter/s, 14.5771s/100 iter), loss = 1.89682
I0815 02:50:53.573030 11101 solver.cpp:334]     Train net output #0: loss = 1.92606 (* 1 = 1.92606 loss)
I0815 02:50:53.573036 11101 sgd_solver.cpp:136] Iteration 161000, lr = 0.0496875, m = 0.9
I0815 02:51:08.472005 11101 solver.cpp:312] Iteration 161100 (6.71204 iter/s, 14.8986s/100 iter), loss = 2.06406
I0815 02:51:08.472033 11101 solver.cpp:334]     Train net output #0: loss = 1.95888 (* 1 = 1.95888 loss)
I0815 02:51:08.472038 11101 sgd_solver.cpp:136] Iteration 161100, lr = 0.0496562, m = 0.9
I0815 02:51:23.364384 11101 solver.cpp:312] Iteration 161200 (6.71503 iter/s, 14.892s/100 iter), loss = 1.79061
I0815 02:51:23.364485 11101 solver.cpp:334]     Train net output #0: loss = 1.7054 (* 1 = 1.7054 loss)
I0815 02:51:23.364506 11101 sgd_solver.cpp:136] Iteration 161200, lr = 0.049625, m = 0.9
I0815 02:51:37.918383 11101 solver.cpp:312] Iteration 161300 (6.87115 iter/s, 14.5536s/100 iter), loss = 2.05985
I0815 02:51:37.918411 11101 solver.cpp:334]     Train net output #0: loss = 2.10824 (* 1 = 2.10824 loss)
I0815 02:51:37.918414 11101 sgd_solver.cpp:136] Iteration 161300, lr = 0.0495938, m = 0.9
I0815 02:51:52.346936 11101 solver.cpp:312] Iteration 161400 (6.93089 iter/s, 14.4282s/100 iter), loss = 2.11135
I0815 02:51:52.346966 11101 solver.cpp:334]     Train net output #0: loss = 2.41158 (* 1 = 2.41158 loss)
I0815 02:51:52.346971 11101 sgd_solver.cpp:136] Iteration 161400, lr = 0.0495625, m = 0.9
I0815 02:52:06.849182 11101 solver.cpp:312] Iteration 161500 (6.89567 iter/s, 14.5018s/100 iter), loss = 2.01617
I0815 02:52:06.849273 11101 solver.cpp:334]     Train net output #0: loss = 1.91429 (* 1 = 1.91429 loss)
I0815 02:52:06.849287 11101 sgd_solver.cpp:136] Iteration 161500, lr = 0.0495313, m = 0.9
I0815 02:52:21.539062 11101 solver.cpp:312] Iteration 161600 (6.80759 iter/s, 14.6895s/100 iter), loss = 1.85839
I0815 02:52:21.539217 11101 solver.cpp:334]     Train net output #0: loss = 1.75753 (* 1 = 1.75753 loss)
I0815 02:52:21.539240 11101 sgd_solver.cpp:136] Iteration 161600, lr = 0.0495, m = 0.9
I0815 02:52:36.254848 11101 solver.cpp:312] Iteration 161700 (6.79561 iter/s, 14.7154s/100 iter), loss = 1.82809
I0815 02:52:36.254873 11101 solver.cpp:334]     Train net output #0: loss = 1.96351 (* 1 = 1.96351 loss)
I0815 02:52:36.254879 11101 sgd_solver.cpp:136] Iteration 161700, lr = 0.0494688, m = 0.9
I0815 02:52:50.755177 11101 solver.cpp:312] Iteration 161800 (6.89659 iter/s, 14.4999s/100 iter), loss = 1.92967
I0815 02:52:50.756139 11101 solver.cpp:334]     Train net output #0: loss = 1.80711 (* 1 = 1.80711 loss)
I0815 02:52:50.756147 11101 sgd_solver.cpp:136] Iteration 161800, lr = 0.0494375, m = 0.9
I0815 02:53:05.796283 11101 solver.cpp:312] Iteration 161900 (6.64863 iter/s, 15.0407s/100 iter), loss = 1.77443
I0815 02:53:05.796340 11101 solver.cpp:334]     Train net output #0: loss = 1.84822 (* 1 = 1.84822 loss)
I0815 02:53:05.796358 11101 sgd_solver.cpp:136] Iteration 161900, lr = 0.0494062, m = 0.9
I0815 02:53:20.601264 11101 solver.cpp:509] Iteration 162000, Testing net (#0)
I0815 02:53:33.449154 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 02:53:41.655036 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.465529
I0815 02:53:41.655062 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.719351
I0815 02:53:41.655071 11101 solver.cpp:594]     Test net output #2: loss = 2.44802 (* 1 = 2.44802 loss)
I0815 02:53:41.655092 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0533s
I0815 02:53:41.808677 11101 solver.cpp:312] Iteration 162000 (2.7769 iter/s, 36.0114s/100 iter), loss = 1.93714
I0815 02:53:41.808701 11101 solver.cpp:334]     Train net output #0: loss = 1.91103 (* 1 = 1.91103 loss)
I0815 02:53:41.808706 11101 sgd_solver.cpp:136] Iteration 162000, lr = 0.049375, m = 0.9
I0815 02:53:56.448005 11101 solver.cpp:312] Iteration 162100 (6.8311 iter/s, 14.6389s/100 iter), loss = 1.86392
I0815 02:53:56.448035 11101 solver.cpp:334]     Train net output #0: loss = 1.71095 (* 1 = 1.71095 loss)
I0815 02:53:56.448040 11101 sgd_solver.cpp:136] Iteration 162100, lr = 0.0493438, m = 0.9
I0815 02:54:10.972565 11101 solver.cpp:312] Iteration 162200 (6.88508 iter/s, 14.5242s/100 iter), loss = 1.92752
I0815 02:54:10.972631 11101 solver.cpp:334]     Train net output #0: loss = 2.07942 (* 1 = 2.07942 loss)
I0815 02:54:10.972638 11101 sgd_solver.cpp:136] Iteration 162200, lr = 0.0493125, m = 0.9
I0815 02:54:25.837069 11101 solver.cpp:312] Iteration 162300 (6.72762 iter/s, 14.8641s/100 iter), loss = 2.06283
I0815 02:54:25.837139 11101 solver.cpp:334]     Train net output #0: loss = 1.69176 (* 1 = 1.69176 loss)
I0815 02:54:25.837160 11101 sgd_solver.cpp:136] Iteration 162300, lr = 0.0492813, m = 0.9
I0815 02:54:40.459414 11101 solver.cpp:312] Iteration 162400 (6.83904 iter/s, 14.6219s/100 iter), loss = 2.05226
I0815 02:54:40.459475 11101 solver.cpp:334]     Train net output #0: loss = 2.35774 (* 1 = 2.35774 loss)
I0815 02:54:40.459489 11101 sgd_solver.cpp:136] Iteration 162400, lr = 0.04925, m = 0.9
I0815 02:54:55.158871 11101 solver.cpp:312] Iteration 162500 (6.80316 iter/s, 14.699s/100 iter), loss = 2.30497
I0815 02:54:55.159095 11101 solver.cpp:334]     Train net output #0: loss = 2.51611 (* 1 = 2.51611 loss)
I0815 02:54:55.159111 11101 sgd_solver.cpp:136] Iteration 162500, lr = 0.0492188, m = 0.9
I0815 02:55:09.795100 11101 solver.cpp:312] Iteration 162600 (6.83255 iter/s, 14.6358s/100 iter), loss = 1.5093
I0815 02:55:09.795128 11101 solver.cpp:334]     Train net output #0: loss = 1.74351 (* 1 = 1.74351 loss)
I0815 02:55:09.795135 11101 sgd_solver.cpp:136] Iteration 162600, lr = 0.0491875, m = 0.9
I0815 02:55:24.347170 11101 solver.cpp:312] Iteration 162700 (6.87206 iter/s, 14.5517s/100 iter), loss = 2.14289
I0815 02:55:24.347195 11101 solver.cpp:334]     Train net output #0: loss = 2.05709 (* 1 = 2.05709 loss)
I0815 02:55:24.347200 11101 sgd_solver.cpp:136] Iteration 162700, lr = 0.0491562, m = 0.9
I0815 02:55:38.711781 11101 solver.cpp:312] Iteration 162800 (6.96174 iter/s, 14.3642s/100 iter), loss = 2.0517
I0815 02:55:38.711858 11101 solver.cpp:334]     Train net output #0: loss = 1.81573 (* 1 = 1.81573 loss)
I0815 02:55:38.711868 11101 sgd_solver.cpp:136] Iteration 162800, lr = 0.049125, m = 0.9
I0815 02:55:53.097911 11101 solver.cpp:312] Iteration 162900 (6.95133 iter/s, 14.3857s/100 iter), loss = 1.95143
I0815 02:55:53.097937 11101 solver.cpp:334]     Train net output #0: loss = 2.21868 (* 1 = 2.21868 loss)
I0815 02:55:53.097942 11101 sgd_solver.cpp:136] Iteration 162900, lr = 0.0490937, m = 0.9
I0815 02:56:07.578594 11101 solver.cpp:312] Iteration 163000 (6.90594 iter/s, 14.4803s/100 iter), loss = 1.99129
I0815 02:56:07.578624 11101 solver.cpp:334]     Train net output #0: loss = 1.81917 (* 1 = 1.81917 loss)
I0815 02:56:07.578630 11101 sgd_solver.cpp:136] Iteration 163000, lr = 0.0490625, m = 0.9
I0815 02:56:22.154851 11101 solver.cpp:312] Iteration 163100 (6.86066 iter/s, 14.5759s/100 iter), loss = 2.04813
I0815 02:56:22.154986 11101 solver.cpp:334]     Train net output #0: loss = 1.98179 (* 1 = 1.98179 loss)
I0815 02:56:22.154994 11101 sgd_solver.cpp:136] Iteration 163100, lr = 0.0490313, m = 0.9
I0815 02:56:36.775558 11101 solver.cpp:312] Iteration 163200 (6.8398 iter/s, 14.6203s/100 iter), loss = 1.48298
I0815 02:56:36.775580 11101 solver.cpp:334]     Train net output #0: loss = 1.03215 (* 1 = 1.03215 loss)
I0815 02:56:36.775586 11101 sgd_solver.cpp:136] Iteration 163200, lr = 0.049, m = 0.9
I0815 02:56:51.248085 11101 solver.cpp:312] Iteration 163300 (6.90983 iter/s, 14.4721s/100 iter), loss = 2.05215
I0815 02:56:51.248114 11101 solver.cpp:334]     Train net output #0: loss = 2.20246 (* 1 = 2.20246 loss)
I0815 02:56:51.248121 11101 sgd_solver.cpp:136] Iteration 163300, lr = 0.0489688, m = 0.9
I0815 02:57:05.848387 11101 solver.cpp:312] Iteration 163400 (6.84936 iter/s, 14.5999s/100 iter), loss = 1.7294
I0815 02:57:05.848484 11101 solver.cpp:334]     Train net output #0: loss = 2.10271 (* 1 = 2.10271 loss)
I0815 02:57:05.848500 11101 sgd_solver.cpp:136] Iteration 163400, lr = 0.0489375, m = 0.9
I0815 02:57:20.379796 11101 solver.cpp:312] Iteration 163500 (6.88183 iter/s, 14.531s/100 iter), loss = 1.7835
I0815 02:57:20.379824 11101 solver.cpp:334]     Train net output #0: loss = 1.70079 (* 1 = 1.70079 loss)
I0815 02:57:20.379832 11101 sgd_solver.cpp:136] Iteration 163500, lr = 0.0489062, m = 0.9
I0815 02:57:34.983880 11101 solver.cpp:312] Iteration 163600 (6.84759 iter/s, 14.6037s/100 iter), loss = 1.66578
I0815 02:57:34.983903 11101 solver.cpp:334]     Train net output #0: loss = 1.98048 (* 1 = 1.98048 loss)
I0815 02:57:34.983909 11101 sgd_solver.cpp:136] Iteration 163600, lr = 0.048875, m = 0.9
I0815 02:57:49.456784 11101 solver.cpp:312] Iteration 163700 (6.90965 iter/s, 14.4725s/100 iter), loss = 1.87628
I0815 02:57:49.456861 11101 solver.cpp:334]     Train net output #0: loss = 1.65115 (* 1 = 1.65115 loss)
I0815 02:57:49.456872 11101 sgd_solver.cpp:136] Iteration 163700, lr = 0.0488437, m = 0.9
I0815 02:58:03.755612 11101 solver.cpp:312] Iteration 163800 (6.99377 iter/s, 14.2984s/100 iter), loss = 1.86207
I0815 02:58:03.755821 11101 solver.cpp:334]     Train net output #0: loss = 1.68198 (* 1 = 1.68198 loss)
I0815 02:58:03.755933 11101 sgd_solver.cpp:136] Iteration 163800, lr = 0.0488125, m = 0.9
I0815 02:58:18.340970 11101 solver.cpp:312] Iteration 163900 (6.85638 iter/s, 14.585s/100 iter), loss = 1.79034
I0815 02:58:18.341035 11101 solver.cpp:334]     Train net output #0: loss = 1.73941 (* 1 = 1.73941 loss)
I0815 02:58:18.341053 11101 sgd_solver.cpp:136] Iteration 163900, lr = 0.0487813, m = 0.9
I0815 02:58:32.832170 11101 solver.cpp:509] Iteration 164000, Testing net (#0)
I0815 02:58:53.734683 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.446
I0815 02:58:53.734704 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.701881
I0815 02:58:53.734709 11101 solver.cpp:594]     Test net output #2: loss = 2.53186 (* 1 = 2.53186 loss)
I0815 02:58:53.734724 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.902s
I0815 02:58:53.881767 11101 solver.cpp:312] Iteration 164000 (2.81374 iter/s, 35.5398s/100 iter), loss = 2.06451
I0815 02:58:53.881830 11101 solver.cpp:334]     Train net output #0: loss = 2.2752 (* 1 = 2.2752 loss)
I0815 02:58:53.881851 11101 sgd_solver.cpp:136] Iteration 164000, lr = 0.04875, m = 0.9
I0815 02:59:08.544044 11101 solver.cpp:312] Iteration 164100 (6.82041 iter/s, 14.6619s/100 iter), loss = 2.06839
I0815 02:59:08.544344 11101 solver.cpp:334]     Train net output #0: loss = 1.99021 (* 1 = 1.99021 loss)
I0815 02:59:08.544358 11101 sgd_solver.cpp:136] Iteration 164100, lr = 0.0487188, m = 0.9
I0815 02:59:23.072862 11101 solver.cpp:312] Iteration 164200 (6.88306 iter/s, 14.5284s/100 iter), loss = 2.07748
I0815 02:59:23.072934 11101 solver.cpp:334]     Train net output #0: loss = 2.25278 (* 1 = 2.25278 loss)
I0815 02:59:23.072954 11101 sgd_solver.cpp:136] Iteration 164200, lr = 0.0486875, m = 0.9
I0815 02:59:37.761451 11101 solver.cpp:312] Iteration 164300 (6.80819 iter/s, 14.6882s/100 iter), loss = 2.17374
I0815 02:59:37.761476 11101 solver.cpp:334]     Train net output #0: loss = 2.02952 (* 1 = 2.02952 loss)
I0815 02:59:37.761482 11101 sgd_solver.cpp:136] Iteration 164300, lr = 0.0486563, m = 0.9
I0815 02:59:52.242508 11101 solver.cpp:312] Iteration 164400 (6.90576 iter/s, 14.4807s/100 iter), loss = 1.75481
I0815 02:59:52.248154 11101 solver.cpp:334]     Train net output #0: loss = 1.60924 (* 1 = 1.60924 loss)
I0815 02:59:52.248163 11101 sgd_solver.cpp:136] Iteration 164400, lr = 0.048625, m = 0.9
I0815 03:00:06.794946 11101 solver.cpp:312] Iteration 164500 (6.87189 iter/s, 14.552s/100 iter), loss = 2.20443
I0815 03:00:06.794971 11101 solver.cpp:334]     Train net output #0: loss = 2.47618 (* 1 = 2.47618 loss)
I0815 03:00:06.794977 11101 sgd_solver.cpp:136] Iteration 164500, lr = 0.0485937, m = 0.9
I0815 03:00:21.246521 11101 solver.cpp:312] Iteration 164600 (6.91985 iter/s, 14.4512s/100 iter), loss = 1.9367
I0815 03:00:21.246589 11101 solver.cpp:334]     Train net output #0: loss = 2.46885 (* 1 = 2.46885 loss)
I0815 03:00:21.246606 11101 sgd_solver.cpp:136] Iteration 164600, lr = 0.0485625, m = 0.9
I0815 03:00:36.049073 11101 solver.cpp:312] Iteration 164700 (6.75578 iter/s, 14.8021s/100 iter), loss = 2.11446
I0815 03:00:36.049120 11101 solver.cpp:334]     Train net output #0: loss = 2.21891 (* 1 = 2.21891 loss)
I0815 03:00:36.049127 11101 sgd_solver.cpp:136] Iteration 164700, lr = 0.0485313, m = 0.9
I0815 03:00:50.820495 11101 solver.cpp:312] Iteration 164800 (6.77002 iter/s, 14.771s/100 iter), loss = 1.9007
I0815 03:00:50.820564 11101 solver.cpp:334]     Train net output #0: loss = 1.94393 (* 1 = 1.94393 loss)
I0815 03:00:50.820581 11101 sgd_solver.cpp:136] Iteration 164800, lr = 0.0485, m = 0.9
I0815 03:01:05.821903 11101 solver.cpp:312] Iteration 164900 (6.66623 iter/s, 15.001s/100 iter), loss = 2.02836
I0815 03:01:05.821930 11101 solver.cpp:334]     Train net output #0: loss = 1.88385 (* 1 = 1.88385 loss)
I0815 03:01:05.821934 11101 sgd_solver.cpp:136] Iteration 164900, lr = 0.0484687, m = 0.9
I0815 03:01:20.343039 11101 solver.cpp:312] Iteration 165000 (6.8867 iter/s, 14.5207s/100 iter), loss = 1.8133
I0815 03:01:20.344888 11101 solver.cpp:334]     Train net output #0: loss = 1.60162 (* 1 = 1.60162 loss)
I0815 03:01:20.344897 11101 sgd_solver.cpp:136] Iteration 165000, lr = 0.0484375, m = 0.9
I0815 03:01:34.851539 11101 solver.cpp:312] Iteration 165100 (6.8927 iter/s, 14.5081s/100 iter), loss = 1.84718
I0815 03:01:34.851567 11101 solver.cpp:334]     Train net output #0: loss = 2.05758 (* 1 = 2.05758 loss)
I0815 03:01:34.851572 11101 sgd_solver.cpp:136] Iteration 165100, lr = 0.0484063, m = 0.9
I0815 03:01:49.366984 11101 solver.cpp:312] Iteration 165200 (6.8894 iter/s, 14.515s/100 iter), loss = 1.902
I0815 03:01:49.367043 11101 solver.cpp:334]     Train net output #0: loss = 1.99686 (* 1 = 1.99686 loss)
I0815 03:01:49.367055 11101 sgd_solver.cpp:136] Iteration 165200, lr = 0.048375, m = 0.9
I0815 03:02:04.053467 11101 solver.cpp:312] Iteration 165300 (6.80917 iter/s, 14.6861s/100 iter), loss = 1.69146
I0815 03:02:04.053524 11101 solver.cpp:334]     Train net output #0: loss = 1.77635 (* 1 = 1.77635 loss)
I0815 03:02:04.053530 11101 sgd_solver.cpp:136] Iteration 165300, lr = 0.0483437, m = 0.9
I0815 03:02:18.769345 11101 solver.cpp:312] Iteration 165400 (6.79557 iter/s, 14.7155s/100 iter), loss = 2.1163
I0815 03:02:18.769415 11101 solver.cpp:334]     Train net output #0: loss = 2.24368 (* 1 = 2.24368 loss)
I0815 03:02:18.769438 11101 sgd_solver.cpp:136] Iteration 165400, lr = 0.0483125, m = 0.9
I0815 03:02:33.271209 11101 solver.cpp:312] Iteration 165500 (6.89585 iter/s, 14.5015s/100 iter), loss = 1.75681
I0815 03:02:33.271389 11101 solver.cpp:334]     Train net output #0: loss = 1.43483 (* 1 = 1.43483 loss)
I0815 03:02:33.271476 11101 sgd_solver.cpp:136] Iteration 165500, lr = 0.0482813, m = 0.9
I0815 03:02:47.818527 11101 solver.cpp:312] Iteration 165600 (6.87431 iter/s, 14.5469s/100 iter), loss = 2.00674
I0815 03:02:47.818593 11101 solver.cpp:334]     Train net output #0: loss = 1.74276 (* 1 = 1.74276 loss)
I0815 03:02:47.818599 11101 sgd_solver.cpp:136] Iteration 165600, lr = 0.04825, m = 0.9
I0815 03:03:02.375551 11101 solver.cpp:312] Iteration 165700 (6.86972 iter/s, 14.5566s/100 iter), loss = 1.61427
I0815 03:03:02.375618 11101 solver.cpp:334]     Train net output #0: loss = 1.67308 (* 1 = 1.67308 loss)
I0815 03:03:02.375635 11101 sgd_solver.cpp:136] Iteration 165700, lr = 0.0482188, m = 0.9
I0815 03:03:16.925724 11101 solver.cpp:312] Iteration 165800 (6.87296 iter/s, 14.5498s/100 iter), loss = 2.16858
I0815 03:03:16.925791 11101 solver.cpp:334]     Train net output #0: loss = 1.98961 (* 1 = 1.98961 loss)
I0815 03:03:16.925807 11101 sgd_solver.cpp:136] Iteration 165800, lr = 0.0481875, m = 0.9
I0815 03:03:31.300601 11101 solver.cpp:312] Iteration 165900 (6.95677 iter/s, 14.3745s/100 iter), loss = 1.71176
I0815 03:03:31.300660 11101 solver.cpp:334]     Train net output #0: loss = 1.42847 (* 1 = 1.42847 loss)
I0815 03:03:31.300668 11101 sgd_solver.cpp:136] Iteration 165900, lr = 0.0481563, m = 0.9
I0815 03:03:45.765714 11101 solver.cpp:509] Iteration 166000, Testing net (#0)
I0815 03:04:06.930613 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.436353
I0815 03:04:06.930666 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.693117
I0815 03:04:06.930675 11101 solver.cpp:594]     Test net output #2: loss = 2.66073 (* 1 = 2.66073 loss)
I0815 03:04:06.930697 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1644s
I0815 03:04:07.106292 11101 solver.cpp:312] Iteration 166000 (2.79293 iter/s, 35.8047s/100 iter), loss = 1.83763
I0815 03:04:07.106315 11101 solver.cpp:334]     Train net output #0: loss = 1.65142 (* 1 = 1.65142 loss)
I0815 03:04:07.106319 11101 sgd_solver.cpp:136] Iteration 166000, lr = 0.048125, m = 0.9
I0815 03:04:21.542387 11101 solver.cpp:312] Iteration 166100 (6.92728 iter/s, 14.4357s/100 iter), loss = 2.63443
I0815 03:04:21.542412 11101 solver.cpp:334]     Train net output #0: loss = 2.3597 (* 1 = 2.3597 loss)
I0815 03:04:21.542418 11101 sgd_solver.cpp:136] Iteration 166100, lr = 0.0480937, m = 0.9
I0815 03:04:35.913430 11101 solver.cpp:312] Iteration 166200 (6.95863 iter/s, 14.3706s/100 iter), loss = 2.04212
I0815 03:04:35.913492 11101 solver.cpp:334]     Train net output #0: loss = 1.90069 (* 1 = 1.90069 loss)
I0815 03:04:35.913517 11101 sgd_solver.cpp:136] Iteration 166200, lr = 0.0480625, m = 0.9
I0815 03:04:50.720564 11101 solver.cpp:312] Iteration 166300 (6.75369 iter/s, 14.8067s/100 iter), loss = 1.88599
I0815 03:04:50.720628 11101 solver.cpp:334]     Train net output #0: loss = 2.00371 (* 1 = 2.00371 loss)
I0815 03:04:50.720634 11101 sgd_solver.cpp:136] Iteration 166300, lr = 0.0480313, m = 0.9
I0815 03:05:05.542470 11101 solver.cpp:312] Iteration 166400 (6.74696 iter/s, 14.8215s/100 iter), loss = 1.95675
I0815 03:05:05.542500 11101 solver.cpp:334]     Train net output #0: loss = 2.14067 (* 1 = 2.14067 loss)
I0815 03:05:05.542506 11101 sgd_solver.cpp:136] Iteration 166400, lr = 0.048, m = 0.9
I0815 03:05:20.290611 11101 solver.cpp:312] Iteration 166500 (6.7807 iter/s, 14.7477s/100 iter), loss = 1.70561
I0815 03:05:20.290637 11101 solver.cpp:334]     Train net output #0: loss = 1.41168 (* 1 = 1.41168 loss)
I0815 03:05:20.290642 11101 sgd_solver.cpp:136] Iteration 166500, lr = 0.0479688, m = 0.9
I0815 03:05:35.037425 11101 solver.cpp:312] Iteration 166600 (6.78131 iter/s, 14.7464s/100 iter), loss = 1.71575
I0815 03:05:35.037480 11101 solver.cpp:334]     Train net output #0: loss = 2.13442 (* 1 = 2.13442 loss)
I0815 03:05:35.037485 11101 sgd_solver.cpp:136] Iteration 166600, lr = 0.0479375, m = 0.9
I0815 03:05:49.724705 11101 solver.cpp:312] Iteration 166700 (6.8088 iter/s, 14.6869s/100 iter), loss = 1.91113
I0815 03:05:49.724732 11101 solver.cpp:334]     Train net output #0: loss = 1.80944 (* 1 = 1.80944 loss)
I0815 03:05:49.724738 11101 sgd_solver.cpp:136] Iteration 166700, lr = 0.0479062, m = 0.9
I0815 03:06:04.258827 11101 solver.cpp:312] Iteration 166800 (6.88055 iter/s, 14.5337s/100 iter), loss = 1.91946
I0815 03:06:04.258857 11101 solver.cpp:334]     Train net output #0: loss = 1.9019 (* 1 = 1.9019 loss)
I0815 03:06:04.258862 11101 sgd_solver.cpp:136] Iteration 166800, lr = 0.047875, m = 0.9
I0815 03:06:18.868520 11101 solver.cpp:312] Iteration 166900 (6.84496 iter/s, 14.6093s/100 iter), loss = 1.74708
I0815 03:06:18.868584 11101 solver.cpp:334]     Train net output #0: loss = 1.60847 (* 1 = 1.60847 loss)
I0815 03:06:18.868592 11101 sgd_solver.cpp:136] Iteration 166900, lr = 0.0478438, m = 0.9
I0815 03:06:33.383098 11101 solver.cpp:312] Iteration 167000 (6.88981 iter/s, 14.5142s/100 iter), loss = 1.81093
I0815 03:06:33.383123 11101 solver.cpp:334]     Train net output #0: loss = 2.14458 (* 1 = 2.14458 loss)
I0815 03:06:33.383128 11101 sgd_solver.cpp:136] Iteration 167000, lr = 0.0478125, m = 0.9
I0815 03:06:48.010668 11101 solver.cpp:312] Iteration 167100 (6.8366 iter/s, 14.6272s/100 iter), loss = 2.19581
I0815 03:06:48.010696 11101 solver.cpp:334]     Train net output #0: loss = 2.19702 (* 1 = 2.19702 loss)
I0815 03:06:48.010701 11101 sgd_solver.cpp:136] Iteration 167100, lr = 0.0477813, m = 0.9
I0815 03:07:02.483922 11101 solver.cpp:312] Iteration 167200 (6.90949 iter/s, 14.4729s/100 iter), loss = 2.34253
I0815 03:07:02.484035 11101 solver.cpp:334]     Train net output #0: loss = 2.99289 (* 1 = 2.99289 loss)
I0815 03:07:02.484055 11101 sgd_solver.cpp:136] Iteration 167200, lr = 0.04775, m = 0.9
I0815 03:07:17.446959 11101 solver.cpp:312] Iteration 167300 (6.68332 iter/s, 14.9626s/100 iter), loss = 1.47051
I0815 03:07:17.446986 11101 solver.cpp:334]     Train net output #0: loss = 1.68468 (* 1 = 1.68468 loss)
I0815 03:07:17.446990 11101 sgd_solver.cpp:136] Iteration 167300, lr = 0.0477188, m = 0.9
I0815 03:07:32.072705 11101 solver.cpp:312] Iteration 167400 (6.83745 iter/s, 14.6253s/100 iter), loss = 2.2354
I0815 03:07:32.072772 11101 solver.cpp:334]     Train net output #0: loss = 2.20786 (* 1 = 2.20786 loss)
I0815 03:07:32.072788 11101 sgd_solver.cpp:136] Iteration 167400, lr = 0.0476875, m = 0.9
I0815 03:07:46.545750 11101 solver.cpp:312] Iteration 167500 (6.90959 iter/s, 14.4726s/100 iter), loss = 1.71394
I0815 03:07:46.546008 11101 solver.cpp:334]     Train net output #0: loss = 1.50354 (* 1 = 1.50354 loss)
I0815 03:07:46.546017 11101 sgd_solver.cpp:136] Iteration 167500, lr = 0.0476562, m = 0.9
I0815 03:08:01.182492 11101 solver.cpp:312] Iteration 167600 (6.83231 iter/s, 14.6363s/100 iter), loss = 1.83557
I0815 03:08:01.182564 11101 solver.cpp:334]     Train net output #0: loss = 1.84858 (* 1 = 1.84858 loss)
I0815 03:08:01.182584 11101 sgd_solver.cpp:136] Iteration 167600, lr = 0.047625, m = 0.9
I0815 03:08:15.801985 11101 solver.cpp:312] Iteration 167700 (6.84037 iter/s, 14.6191s/100 iter), loss = 1.83146
I0815 03:08:15.802011 11101 solver.cpp:334]     Train net output #0: loss = 1.57232 (* 1 = 1.57232 loss)
I0815 03:08:15.802017 11101 sgd_solver.cpp:136] Iteration 167700, lr = 0.0475938, m = 0.9
I0815 03:08:30.709480 11101 solver.cpp:312] Iteration 167800 (6.70822 iter/s, 14.9071s/100 iter), loss = 1.98044
I0815 03:08:30.709553 11101 solver.cpp:334]     Train net output #0: loss = 1.73879 (* 1 = 1.73879 loss)
I0815 03:08:30.709561 11101 sgd_solver.cpp:136] Iteration 167800, lr = 0.0475625, m = 0.9
I0815 03:08:45.237797 11101 solver.cpp:312] Iteration 167900 (6.8833 iter/s, 14.5279s/100 iter), loss = 1.74168
I0815 03:08:45.237851 11101 solver.cpp:334]     Train net output #0: loss = 1.73988 (* 1 = 1.73988 loss)
I0815 03:08:45.237865 11101 sgd_solver.cpp:136] Iteration 167900, lr = 0.0475312, m = 0.9
I0815 03:08:59.653100 11101 solver.cpp:509] Iteration 168000, Testing net (#0)
I0815 03:09:10.454790 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 03:09:20.975253 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.446706
I0815 03:09:20.975275 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.696235
I0815 03:09:20.975282 11101 solver.cpp:594]     Test net output #2: loss = 2.57478 (* 1 = 2.57478 loss)
I0815 03:09:20.975303 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3216s
I0815 03:09:21.139910 11101 solver.cpp:312] Iteration 168000 (2.78543 iter/s, 35.9011s/100 iter), loss = 2.14526
I0815 03:09:21.139938 11101 solver.cpp:334]     Train net output #0: loss = 2.31283 (* 1 = 2.31283 loss)
I0815 03:09:21.139943 11101 sgd_solver.cpp:136] Iteration 168000, lr = 0.0475, m = 0.9
I0815 03:09:35.456390 11101 solver.cpp:312] Iteration 168100 (6.98515 iter/s, 14.3161s/100 iter), loss = 1.87094
I0815 03:09:35.456418 11101 solver.cpp:334]     Train net output #0: loss = 1.90509 (* 1 = 1.90509 loss)
I0815 03:09:35.456421 11101 sgd_solver.cpp:136] Iteration 168100, lr = 0.0474688, m = 0.9
I0815 03:09:49.800611 11101 solver.cpp:312] Iteration 168200 (6.97164 iter/s, 14.3438s/100 iter), loss = 1.53349
I0815 03:09:49.800675 11101 solver.cpp:334]     Train net output #0: loss = 1.60455 (* 1 = 1.60455 loss)
I0815 03:09:49.800683 11101 sgd_solver.cpp:136] Iteration 168200, lr = 0.0474375, m = 0.9
I0815 03:10:04.465515 11101 solver.cpp:312] Iteration 168300 (6.81919 iter/s, 14.6645s/100 iter), loss = 1.76503
I0815 03:10:04.465584 11101 solver.cpp:334]     Train net output #0: loss = 1.64761 (* 1 = 1.64761 loss)
I0815 03:10:04.465600 11101 sgd_solver.cpp:136] Iteration 168300, lr = 0.0474063, m = 0.9
I0815 03:10:18.899971 11101 solver.cpp:312] Iteration 168400 (6.92806 iter/s, 14.4341s/100 iter), loss = 1.55746
I0815 03:10:18.899998 11101 solver.cpp:334]     Train net output #0: loss = 1.56905 (* 1 = 1.56905 loss)
I0815 03:10:18.900005 11101 sgd_solver.cpp:136] Iteration 168400, lr = 0.047375, m = 0.9
I0815 03:10:33.387578 11101 solver.cpp:312] Iteration 168500 (6.90264 iter/s, 14.4872s/100 iter), loss = 1.58716
I0815 03:10:33.387645 11101 solver.cpp:334]     Train net output #0: loss = 1.77283 (* 1 = 1.77283 loss)
I0815 03:10:33.387652 11101 sgd_solver.cpp:136] Iteration 168500, lr = 0.0473437, m = 0.9
I0815 03:10:47.925773 11101 solver.cpp:312] Iteration 168600 (6.87862 iter/s, 14.5378s/100 iter), loss = 2.07687
I0815 03:10:47.925798 11101 solver.cpp:334]     Train net output #0: loss = 1.93521 (* 1 = 1.93521 loss)
I0815 03:10:47.925804 11101 sgd_solver.cpp:136] Iteration 168600, lr = 0.0473125, m = 0.9
I0815 03:11:02.425190 11101 solver.cpp:312] Iteration 168700 (6.89702 iter/s, 14.499s/100 iter), loss = 1.86296
I0815 03:11:02.425369 11101 solver.cpp:334]     Train net output #0: loss = 1.6813 (* 1 = 1.6813 loss)
I0815 03:11:02.425467 11101 sgd_solver.cpp:136] Iteration 168700, lr = 0.0472812, m = 0.9
I0815 03:11:16.774024 11101 solver.cpp:312] Iteration 168800 (6.9694 iter/s, 14.3484s/100 iter), loss = 2.11666
I0815 03:11:16.774101 11101 solver.cpp:334]     Train net output #0: loss = 2.3162 (* 1 = 2.3162 loss)
I0815 03:11:16.774107 11101 sgd_solver.cpp:136] Iteration 168800, lr = 0.04725, m = 0.9
I0815 03:11:30.957423 11101 solver.cpp:312] Iteration 168900 (7.05069 iter/s, 14.183s/100 iter), loss = 1.9363
I0815 03:11:30.957471 11101 solver.cpp:334]     Train net output #0: loss = 1.68475 (* 1 = 1.68475 loss)
I0815 03:11:30.957484 11101 sgd_solver.cpp:136] Iteration 168900, lr = 0.0472188, m = 0.9
I0815 03:11:45.836825 11101 solver.cpp:312] Iteration 169000 (6.72089 iter/s, 14.879s/100 iter), loss = 1.65022
I0815 03:11:45.836879 11101 solver.cpp:334]     Train net output #0: loss = 1.69849 (* 1 = 1.69849 loss)
I0815 03:11:45.836892 11101 sgd_solver.cpp:136] Iteration 169000, lr = 0.0471875, m = 0.9
I0815 03:12:00.253201 11101 solver.cpp:312] Iteration 169100 (6.93675 iter/s, 14.416s/100 iter), loss = 2.20663
I0815 03:12:00.253306 11101 solver.cpp:334]     Train net output #0: loss = 2.79366 (* 1 = 2.79366 loss)
I0815 03:12:00.253325 11101 sgd_solver.cpp:136] Iteration 169100, lr = 0.0471563, m = 0.9
I0815 03:12:14.622440 11101 solver.cpp:312] Iteration 169200 (6.9595 iter/s, 14.3688s/100 iter), loss = 1.91166
I0815 03:12:14.622470 11101 solver.cpp:334]     Train net output #0: loss = 1.80779 (* 1 = 1.80779 loss)
I0815 03:12:14.622478 11101 sgd_solver.cpp:136] Iteration 169200, lr = 0.047125, m = 0.9
I0815 03:12:29.234097 11101 solver.cpp:312] Iteration 169300 (6.84404 iter/s, 14.6113s/100 iter), loss = 1.77831
I0815 03:12:29.234120 11101 solver.cpp:334]     Train net output #0: loss = 1.75109 (* 1 = 1.75109 loss)
I0815 03:12:29.234124 11101 sgd_solver.cpp:136] Iteration 169300, lr = 0.0470937, m = 0.9
I0815 03:12:43.919450 11101 solver.cpp:312] Iteration 169400 (6.80969 iter/s, 14.685s/100 iter), loss = 1.96172
I0815 03:12:43.919608 11101 solver.cpp:334]     Train net output #0: loss = 1.45079 (* 1 = 1.45079 loss)
I0815 03:12:43.919617 11101 sgd_solver.cpp:136] Iteration 169400, lr = 0.0470625, m = 0.9
I0815 03:12:58.395246 11101 solver.cpp:312] Iteration 169500 (6.90827 iter/s, 14.4754s/100 iter), loss = 1.74965
I0815 03:12:58.395272 11101 solver.cpp:334]     Train net output #0: loss = 1.62966 (* 1 = 1.62966 loss)
I0815 03:12:58.395306 11101 sgd_solver.cpp:136] Iteration 169500, lr = 0.0470312, m = 0.9
I0815 03:13:13.420708 11101 solver.cpp:312] Iteration 169600 (6.65555 iter/s, 15.025s/100 iter), loss = 2.29154
I0815 03:13:13.420783 11101 solver.cpp:334]     Train net output #0: loss = 1.52438 (* 1 = 1.52438 loss)
I0815 03:13:13.420802 11101 sgd_solver.cpp:136] Iteration 169600, lr = 0.047, m = 0.9
I0815 03:13:28.129863 11101 solver.cpp:312] Iteration 169700 (6.79867 iter/s, 14.7088s/100 iter), loss = 1.70039
I0815 03:13:28.129925 11101 solver.cpp:334]     Train net output #0: loss = 1.77942 (* 1 = 1.77942 loss)
I0815 03:13:28.129931 11101 sgd_solver.cpp:136] Iteration 169700, lr = 0.0469688, m = 0.9
I0815 03:13:42.507746 11101 solver.cpp:312] Iteration 169800 (6.95532 iter/s, 14.3775s/100 iter), loss = 1.7985
I0815 03:13:42.507773 11101 solver.cpp:334]     Train net output #0: loss = 2.24316 (* 1 = 2.24316 loss)
I0815 03:13:42.507777 11101 sgd_solver.cpp:136] Iteration 169800, lr = 0.0469375, m = 0.9
I0815 03:13:57.273875 11101 solver.cpp:312] Iteration 169900 (6.77244 iter/s, 14.7657s/100 iter), loss = 2.13906
I0815 03:13:57.273941 11101 solver.cpp:334]     Train net output #0: loss = 1.81651 (* 1 = 1.81651 loss)
I0815 03:13:57.273959 11101 sgd_solver.cpp:136] Iteration 169900, lr = 0.0469063, m = 0.9
I0815 03:14:11.428153 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_170000.caffemodel
I0815 03:14:11.440949 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_170000.solverstate
I0815 03:14:11.445340 11101 solver.cpp:509] Iteration 170000, Testing net (#0)
I0815 03:14:32.596645 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.467
I0815 03:14:32.596668 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.727233
I0815 03:14:32.596675 11101 solver.cpp:594]     Test net output #2: loss = 2.40386 (* 1 = 2.40386 loss)
I0815 03:14:32.596729 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.1508s
I0815 03:14:32.759420 11101 solver.cpp:312] Iteration 170000 (2.81813 iter/s, 35.4846s/100 iter), loss = 1.71941
I0815 03:14:32.759445 11101 solver.cpp:334]     Train net output #0: loss = 1.75722 (* 1 = 1.75722 loss)
I0815 03:14:32.759450 11101 sgd_solver.cpp:136] Iteration 170000, lr = 0.046875, m = 0.9
I0815 03:14:47.038069 11101 solver.cpp:312] Iteration 170100 (7.00366 iter/s, 14.2782s/100 iter), loss = 1.90973
I0815 03:14:47.038157 11101 solver.cpp:334]     Train net output #0: loss = 2.09464 (* 1 = 2.09464 loss)
I0815 03:14:47.038175 11101 sgd_solver.cpp:136] Iteration 170100, lr = 0.0468437, m = 0.9
I0815 03:15:01.599951 11101 solver.cpp:312] Iteration 170200 (6.86744 iter/s, 14.5615s/100 iter), loss = 2.06211
I0815 03:15:01.600028 11101 solver.cpp:334]     Train net output #0: loss = 2.03234 (* 1 = 2.03234 loss)
I0815 03:15:01.600049 11101 sgd_solver.cpp:136] Iteration 170200, lr = 0.0468125, m = 0.9
I0815 03:15:16.374605 11101 solver.cpp:312] Iteration 170300 (6.76853 iter/s, 14.7742s/100 iter), loss = 1.90253
I0815 03:15:16.374626 11101 solver.cpp:334]     Train net output #0: loss = 1.97703 (* 1 = 1.97703 loss)
I0815 03:15:16.374632 11101 sgd_solver.cpp:136] Iteration 170300, lr = 0.0467812, m = 0.9
I0815 03:15:30.774029 11101 solver.cpp:312] Iteration 170400 (6.94492 iter/s, 14.399s/100 iter), loss = 1.83474
I0815 03:15:30.774119 11101 solver.cpp:334]     Train net output #0: loss = 1.79378 (* 1 = 1.79378 loss)
I0815 03:15:30.774137 11101 sgd_solver.cpp:136] Iteration 170400, lr = 0.04675, m = 0.9
I0815 03:15:45.235728 11101 solver.cpp:312] Iteration 170500 (6.91501 iter/s, 14.4613s/100 iter), loss = 1.56862
I0815 03:15:45.235873 11101 solver.cpp:334]     Train net output #0: loss = 1.48822 (* 1 = 1.48822 loss)
I0815 03:15:45.235889 11101 sgd_solver.cpp:136] Iteration 170500, lr = 0.0467188, m = 0.9
I0815 03:15:59.842425 11101 solver.cpp:312] Iteration 170600 (6.84636 iter/s, 14.6063s/100 iter), loss = 1.89138
I0815 03:15:59.842478 11101 solver.cpp:334]     Train net output #0: loss = 1.42228 (* 1 = 1.42228 loss)
I0815 03:15:59.842490 11101 sgd_solver.cpp:136] Iteration 170600, lr = 0.0466875, m = 0.9
I0815 03:16:14.653201 11101 solver.cpp:312] Iteration 170700 (6.75203 iter/s, 14.8104s/100 iter), loss = 1.88456
I0815 03:16:14.653286 11101 solver.cpp:334]     Train net output #0: loss = 1.81847 (* 1 = 1.81847 loss)
I0815 03:16:14.653298 11101 sgd_solver.cpp:136] Iteration 170700, lr = 0.0466563, m = 0.9
I0815 03:16:29.562013 11101 solver.cpp:312] Iteration 170800 (6.70763 iter/s, 14.9084s/100 iter), loss = 1.87326
I0815 03:16:29.563416 11101 solver.cpp:334]     Train net output #0: loss = 1.8081 (* 1 = 1.8081 loss)
I0815 03:16:29.563443 11101 sgd_solver.cpp:136] Iteration 170800, lr = 0.046625, m = 0.9
I0815 03:16:44.019407 11101 solver.cpp:312] Iteration 170900 (6.91707 iter/s, 14.457s/100 iter), loss = 1.92573
I0815 03:16:44.019433 11101 solver.cpp:334]     Train net output #0: loss = 2.33727 (* 1 = 2.33727 loss)
I0815 03:16:44.019439 11101 sgd_solver.cpp:136] Iteration 170900, lr = 0.0465938, m = 0.9
I0815 03:16:58.257228 11101 solver.cpp:312] Iteration 171000 (7.02374 iter/s, 14.2374s/100 iter), loss = 2.0877
I0815 03:16:58.257282 11101 solver.cpp:334]     Train net output #0: loss = 2.14678 (* 1 = 2.14678 loss)
I0815 03:16:58.257288 11101 sgd_solver.cpp:136] Iteration 171000, lr = 0.0465625, m = 0.9
I0815 03:17:12.951313 11101 solver.cpp:312] Iteration 171100 (6.80565 iter/s, 14.6937s/100 iter), loss = 1.98966
I0815 03:17:12.951390 11101 solver.cpp:334]     Train net output #0: loss = 1.66509 (* 1 = 1.66509 loss)
I0815 03:17:12.951416 11101 sgd_solver.cpp:136] Iteration 171100, lr = 0.0465312, m = 0.9
I0815 03:17:27.501739 11101 solver.cpp:312] Iteration 171200 (6.87284 iter/s, 14.55s/100 iter), loss = 1.90353
I0815 03:17:27.501765 11101 solver.cpp:334]     Train net output #0: loss = 1.74335 (* 1 = 1.74335 loss)
I0815 03:17:27.501770 11101 sgd_solver.cpp:136] Iteration 171200, lr = 0.0465, m = 0.9
I0815 03:17:42.167381 11101 solver.cpp:312] Iteration 171300 (6.81885 iter/s, 14.6652s/100 iter), loss = 1.48054
I0815 03:17:42.167455 11101 solver.cpp:334]     Train net output #0: loss = 0.908005 (* 1 = 0.908005 loss)
I0815 03:17:42.167461 11101 sgd_solver.cpp:136] Iteration 171300, lr = 0.0464688, m = 0.9
I0815 03:17:56.800381 11101 solver.cpp:312] Iteration 171400 (6.83405 iter/s, 14.6326s/100 iter), loss = 2.08032
I0815 03:17:56.800410 11101 solver.cpp:334]     Train net output #0: loss = 2.08336 (* 1 = 2.08336 loss)
I0815 03:17:56.800415 11101 sgd_solver.cpp:136] Iteration 171400, lr = 0.0464375, m = 0.9
I0815 03:18:11.468250 11101 solver.cpp:312] Iteration 171500 (6.81781 iter/s, 14.6675s/100 iter), loss = 2.13218
I0815 03:18:11.468281 11101 solver.cpp:334]     Train net output #0: loss = 2.14487 (* 1 = 2.14487 loss)
I0815 03:18:11.468288 11101 sgd_solver.cpp:136] Iteration 171500, lr = 0.0464063, m = 0.9
I0815 03:18:26.066709 11101 solver.cpp:312] Iteration 171600 (6.85023 iter/s, 14.5981s/100 iter), loss = 2.07095
I0815 03:18:26.066771 11101 solver.cpp:334]     Train net output #0: loss = 2.49068 (* 1 = 2.49068 loss)
I0815 03:18:26.066776 11101 sgd_solver.cpp:136] Iteration 171600, lr = 0.046375, m = 0.9
I0815 03:18:40.760218 11101 solver.cpp:312] Iteration 171700 (6.80591 iter/s, 14.6931s/100 iter), loss = 1.82495
I0815 03:18:40.760246 11101 solver.cpp:334]     Train net output #0: loss = 2.01943 (* 1 = 2.01943 loss)
I0815 03:18:40.760252 11101 sgd_solver.cpp:136] Iteration 171700, lr = 0.0463438, m = 0.9
I0815 03:18:55.317764 11101 solver.cpp:312] Iteration 171800 (6.86948 iter/s, 14.5571s/100 iter), loss = 1.96553
I0815 03:18:55.317831 11101 solver.cpp:334]     Train net output #0: loss = 2.18221 (* 1 = 2.18221 loss)
I0815 03:18:55.317847 11101 sgd_solver.cpp:136] Iteration 171800, lr = 0.0463125, m = 0.9
I0815 03:19:10.204941 11101 solver.cpp:312] Iteration 171900 (6.71738 iter/s, 14.8868s/100 iter), loss = 1.91655
I0815 03:19:10.205188 11101 solver.cpp:334]     Train net output #0: loss = 2.12946 (* 1 = 2.12946 loss)
I0815 03:19:10.205298 11101 sgd_solver.cpp:136] Iteration 171900, lr = 0.0462812, m = 0.9
I0815 03:19:24.985432 11101 solver.cpp:509] Iteration 172000, Testing net (#0)
I0815 03:19:30.956017 11099 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 03:19:46.080232 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.468823
I0815 03:19:46.080283 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.725175
I0815 03:19:46.080289 11101 solver.cpp:594]     Test net output #2: loss = 2.39997 (* 1 = 2.39997 loss)
I0815 03:19:46.080309 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0943s
I0815 03:19:46.267760 11101 solver.cpp:312] Iteration 172000 (2.77302 iter/s, 36.0618s/100 iter), loss = 2.21658
I0815 03:19:46.267786 11101 solver.cpp:334]     Train net output #0: loss = 2.16614 (* 1 = 2.16614 loss)
I0815 03:19:46.267792 11101 sgd_solver.cpp:136] Iteration 172000, lr = 0.04625, m = 0.9
I0815 03:20:01.018749 11101 solver.cpp:312] Iteration 172100 (6.7794 iter/s, 14.7506s/100 iter), loss = 1.64084
I0815 03:20:01.018779 11101 solver.cpp:334]     Train net output #0: loss = 1.83074 (* 1 = 1.83074 loss)
I0815 03:20:01.018784 11101 sgd_solver.cpp:136] Iteration 172100, lr = 0.0462188, m = 0.9
I0815 03:20:15.712277 11101 solver.cpp:312] Iteration 172200 (6.8059 iter/s, 14.6931s/100 iter), loss = 1.78432
I0815 03:20:15.712344 11101 solver.cpp:334]     Train net output #0: loss = 1.65434 (* 1 = 1.65434 loss)
I0815 03:20:15.712362 11101 sgd_solver.cpp:136] Iteration 172200, lr = 0.0461875, m = 0.9
I0815 03:20:30.800743 11101 solver.cpp:312] Iteration 172300 (6.62777 iter/s, 15.088s/100 iter), loss = 2.33367
I0815 03:20:30.800855 11101 solver.cpp:334]     Train net output #0: loss = 3.03957 (* 1 = 3.03957 loss)
I0815 03:20:30.800871 11101 sgd_solver.cpp:136] Iteration 172300, lr = 0.0461563, m = 0.9
I0815 03:20:45.237388 11101 solver.cpp:312] Iteration 172400 (6.92701 iter/s, 14.4362s/100 iter), loss = 2.10555
I0815 03:20:45.237413 11101 solver.cpp:334]     Train net output #0: loss = 2.09432 (* 1 = 2.09432 loss)
I0815 03:20:45.237419 11101 sgd_solver.cpp:136] Iteration 172400, lr = 0.046125, m = 0.9
I0815 03:20:59.499730 11101 solver.cpp:312] Iteration 172500 (7.01166 iter/s, 14.2619s/100 iter), loss = 2.04799
I0815 03:20:59.499763 11101 solver.cpp:334]     Train net output #0: loss = 1.99367 (* 1 = 1.99367 loss)
I0815 03:20:59.499768 11101 sgd_solver.cpp:136] Iteration 172500, lr = 0.0460938, m = 0.9
I0815 03:21:14.243259 11101 solver.cpp:312] Iteration 172600 (6.78282 iter/s, 14.7431s/100 iter), loss = 1.8431
I0815 03:21:14.243319 11101 solver.cpp:334]     Train net output #0: loss = 1.78678 (* 1 = 1.78678 loss)
I0815 03:21:14.243325 11101 sgd_solver.cpp:136] Iteration 172600, lr = 0.0460625, m = 0.9
I0815 03:21:28.667728 11101 solver.cpp:312] Iteration 172700 (6.93286 iter/s, 14.4241s/100 iter), loss = 1.93874
I0815 03:21:28.667757 11101 solver.cpp:334]     Train net output #0: loss = 1.6875 (* 1 = 1.6875 loss)
I0815 03:21:28.667764 11101 sgd_solver.cpp:136] Iteration 172700, lr = 0.0460312, m = 0.9
I0815 03:21:43.169582 11101 solver.cpp:312] Iteration 172800 (6.89586 iter/s, 14.5015s/100 iter), loss = 2.0733
I0815 03:21:43.169610 11101 solver.cpp:334]     Train net output #0: loss = 1.93944 (* 1 = 1.93944 loss)
I0815 03:21:43.169616 11101 sgd_solver.cpp:136] Iteration 172800, lr = 0.046, m = 0.9
I0815 03:21:57.642135 11101 solver.cpp:312] Iteration 172900 (6.90982 iter/s, 14.4722s/100 iter), loss = 1.81044
I0815 03:21:57.642352 11101 solver.cpp:334]     Train net output #0: loss = 1.79094 (* 1 = 1.79094 loss)
I0815 03:21:57.642439 11101 sgd_solver.cpp:136] Iteration 172900, lr = 0.0459687, m = 0.9
I0815 03:22:12.235255 11101 solver.cpp:312] Iteration 173000 (6.85273 iter/s, 14.5927s/100 iter), loss = 1.79145
I0815 03:22:12.235283 11101 solver.cpp:334]     Train net output #0: loss = 1.86637 (* 1 = 1.86637 loss)
I0815 03:22:12.235291 11101 sgd_solver.cpp:136] Iteration 173000, lr = 0.0459375, m = 0.9
I0815 03:22:26.804055 11101 solver.cpp:312] Iteration 173100 (6.86417 iter/s, 14.5684s/100 iter), loss = 1.91822
I0815 03:22:26.804080 11101 solver.cpp:334]     Train net output #0: loss = 1.96799 (* 1 = 1.96799 loss)
I0815 03:22:26.804083 11101 sgd_solver.cpp:136] Iteration 173100, lr = 0.0459063, m = 0.9
I0815 03:22:41.537318 11101 solver.cpp:312] Iteration 173200 (6.78755 iter/s, 14.7329s/100 iter), loss = 2.2066
I0815 03:22:41.537384 11101 solver.cpp:334]     Train net output #0: loss = 2.25834 (* 1 = 2.25834 loss)
I0815 03:22:41.537391 11101 sgd_solver.cpp:136] Iteration 173200, lr = 0.045875, m = 0.9
I0815 03:22:56.229544 11101 solver.cpp:312] Iteration 173300 (6.80651 iter/s, 14.6918s/100 iter), loss = 2.04883
I0815 03:22:56.229569 11101 solver.cpp:334]     Train net output #0: loss = 1.47422 (* 1 = 1.47422 loss)
I0815 03:22:56.229573 11101 sgd_solver.cpp:136] Iteration 173300, lr = 0.0458438, m = 0.9
I0815 03:23:10.881819 11101 solver.cpp:312] Iteration 173400 (6.82507 iter/s, 14.6519s/100 iter), loss = 1.69456
I0815 03:23:10.881844 11101 solver.cpp:334]     Train net output #0: loss = 1.88067 (* 1 = 1.88067 loss)
I0815 03:23:10.881849 11101 sgd_solver.cpp:136] Iteration 173400, lr = 0.0458125, m = 0.9
I0815 03:23:25.554944 11101 solver.cpp:312] Iteration 173500 (6.81537 iter/s, 14.6727s/100 iter), loss = 2.01359
I0815 03:23:25.555042 11101 solver.cpp:334]     Train net output #0: loss = 1.92786 (* 1 = 1.92786 loss)
I0815 03:23:25.555064 11101 sgd_solver.cpp:136] Iteration 173500, lr = 0.0457813, m = 0.9
I0815 03:23:40.208575 11101 solver.cpp:312] Iteration 173600 (6.82443 iter/s, 14.6532s/100 iter), loss = 1.7274
I0815 03:23:40.208606 11101 solver.cpp:334]     Train net output #0: loss = 1.78154 (* 1 = 1.78154 loss)
I0815 03:23:40.208611 11101 sgd_solver.cpp:136] Iteration 173600, lr = 0.04575, m = 0.9
I0815 03:23:54.933308 11101 solver.cpp:312] Iteration 173700 (6.79148 iter/s, 14.7243s/100 iter), loss = 1.59793
I0815 03:23:54.933334 11101 solver.cpp:334]     Train net output #0: loss = 1.3137 (* 1 = 1.3137 loss)
I0815 03:23:54.933341 11101 sgd_solver.cpp:136] Iteration 173700, lr = 0.0457187, m = 0.9
I0815 03:24:09.492750 11101 solver.cpp:312] Iteration 173800 (6.86858 iter/s, 14.559s/100 iter), loss = 1.58992
I0815 03:24:09.492846 11101 solver.cpp:334]     Train net output #0: loss = 1.6728 (* 1 = 1.6728 loss)
I0815 03:24:09.492854 11101 sgd_solver.cpp:136] Iteration 173800, lr = 0.0456875, m = 0.9
I0815 03:24:24.196856 11101 solver.cpp:312] Iteration 173900 (6.80101 iter/s, 14.7037s/100 iter), loss = 2.22601
I0815 03:24:24.196884 11101 solver.cpp:334]     Train net output #0: loss = 2.66007 (* 1 = 2.66007 loss)
I0815 03:24:24.196890 11101 sgd_solver.cpp:136] Iteration 173900, lr = 0.0456563, m = 0.9
I0815 03:24:38.716224 11101 solver.cpp:509] Iteration 174000, Testing net (#0)
I0815 03:24:47.369158 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 03:24:59.946722 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.474824
I0815 03:24:59.946743 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.728939
I0815 03:24:59.946749 11101 solver.cpp:594]     Test net output #2: loss = 2.36012 (* 1 = 2.36012 loss)
I0815 03:24:59.946771 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.23s
I0815 03:25:00.106823 11101 solver.cpp:312] Iteration 174000 (2.78482 iter/s, 35.909s/100 iter), loss = 1.78522
I0815 03:25:00.106851 11101 solver.cpp:334]     Train net output #0: loss = 1.56029 (* 1 = 1.56029 loss)
I0815 03:25:00.106858 11101 sgd_solver.cpp:136] Iteration 174000, lr = 0.045625, m = 0.9
I0815 03:25:14.639895 11101 solver.cpp:312] Iteration 174100 (6.88105 iter/s, 14.5327s/100 iter), loss = 2.59253
I0815 03:25:14.639945 11101 solver.cpp:334]     Train net output #0: loss = 2.44594 (* 1 = 2.44594 loss)
I0815 03:25:14.639958 11101 sgd_solver.cpp:136] Iteration 174100, lr = 0.0455937, m = 0.9
I0815 03:25:28.906498 11101 solver.cpp:312] Iteration 174200 (7.00957 iter/s, 14.2662s/100 iter), loss = 1.78692
I0815 03:25:28.906558 11101 solver.cpp:334]     Train net output #0: loss = 1.73681 (* 1 = 1.73681 loss)
I0815 03:25:28.906565 11101 sgd_solver.cpp:136] Iteration 174200, lr = 0.0455625, m = 0.9
I0815 03:25:43.434023 11101 solver.cpp:312] Iteration 174300 (6.88368 iter/s, 14.5271s/100 iter), loss = 2.04807
I0815 03:25:43.434053 11101 solver.cpp:334]     Train net output #0: loss = 1.80697 (* 1 = 1.80697 loss)
I0815 03:25:43.434059 11101 sgd_solver.cpp:136] Iteration 174300, lr = 0.0455313, m = 0.9
I0815 03:25:58.069885 11101 solver.cpp:312] Iteration 174400 (6.83272 iter/s, 14.6355s/100 iter), loss = 1.91303
I0815 03:25:58.069952 11101 solver.cpp:334]     Train net output #0: loss = 2.15765 (* 1 = 2.15765 loss)
I0815 03:25:58.069972 11101 sgd_solver.cpp:136] Iteration 174400, lr = 0.0455, m = 0.9
I0815 03:26:12.418756 11101 solver.cpp:312] Iteration 174500 (6.96938 iter/s, 14.3485s/100 iter), loss = 1.84812
I0815 03:26:12.418860 11101 solver.cpp:334]     Train net output #0: loss = 1.79277 (* 1 = 1.79277 loss)
I0815 03:26:12.418881 11101 sgd_solver.cpp:136] Iteration 174500, lr = 0.0454687, m = 0.9
I0815 03:26:26.830688 11101 solver.cpp:312] Iteration 174600 (6.93889 iter/s, 14.4115s/100 iter), loss = 1.82051
I0815 03:26:26.830742 11101 solver.cpp:334]     Train net output #0: loss = 1.87357 (* 1 = 1.87357 loss)
I0815 03:26:26.830755 11101 sgd_solver.cpp:136] Iteration 174600, lr = 0.0454375, m = 0.9
I0815 03:26:41.407440 11101 solver.cpp:312] Iteration 174700 (6.86043 iter/s, 14.5764s/100 iter), loss = 1.81546
I0815 03:26:41.407493 11101 solver.cpp:334]     Train net output #0: loss = 1.55925 (* 1 = 1.55925 loss)
I0815 03:26:41.407506 11101 sgd_solver.cpp:136] Iteration 174700, lr = 0.0454063, m = 0.9
I0815 03:26:56.104460 11101 solver.cpp:312] Iteration 174800 (6.80429 iter/s, 14.6966s/100 iter), loss = 2.04046
I0815 03:26:56.104547 11101 solver.cpp:334]     Train net output #0: loss = 1.857 (* 1 = 1.857 loss)
I0815 03:26:56.104555 11101 sgd_solver.cpp:136] Iteration 174800, lr = 0.045375, m = 0.9
I0815 03:27:10.902588 11101 solver.cpp:312] Iteration 174900 (6.7578 iter/s, 14.7977s/100 iter), loss = 1.58954
I0815 03:27:10.902614 11101 solver.cpp:334]     Train net output #0: loss = 1.19674 (* 1 = 1.19674 loss)
I0815 03:27:10.902621 11101 sgd_solver.cpp:136] Iteration 174900, lr = 0.0453438, m = 0.9
I0815 03:27:25.498684 11101 solver.cpp:312] Iteration 175000 (6.85133 iter/s, 14.5957s/100 iter), loss = 1.66512
I0815 03:27:25.498710 11101 solver.cpp:334]     Train net output #0: loss = 1.2707 (* 1 = 1.2707 loss)
I0815 03:27:25.498714 11101 sgd_solver.cpp:136] Iteration 175000, lr = 0.0453125, m = 0.9
I0815 03:27:39.958020 11101 solver.cpp:312] Iteration 175100 (6.91614 iter/s, 14.4589s/100 iter), loss = 1.898
I0815 03:27:39.958117 11101 solver.cpp:334]     Train net output #0: loss = 2.08775 (* 1 = 2.08775 loss)
I0815 03:27:39.958137 11101 sgd_solver.cpp:136] Iteration 175100, lr = 0.0452813, m = 0.9
I0815 03:27:54.563379 11101 solver.cpp:312] Iteration 175200 (6.84699 iter/s, 14.605s/100 iter), loss = 1.92487
I0815 03:27:54.563442 11101 solver.cpp:334]     Train net output #0: loss = 2.19911 (* 1 = 2.19911 loss)
I0815 03:27:54.563463 11101 sgd_solver.cpp:136] Iteration 175200, lr = 0.04525, m = 0.9
I0815 03:28:09.112982 11101 solver.cpp:312] Iteration 175300 (6.87323 iter/s, 14.5492s/100 iter), loss = 2.16422
I0815 03:28:09.113008 11101 solver.cpp:334]     Train net output #0: loss = 2.13944 (* 1 = 2.13944 loss)
I0815 03:28:09.113015 11101 sgd_solver.cpp:136] Iteration 175300, lr = 0.0452187, m = 0.9
I0815 03:28:23.656127 11101 solver.cpp:312] Iteration 175400 (6.87628 iter/s, 14.5427s/100 iter), loss = 1.72595
I0815 03:28:23.656183 11101 solver.cpp:334]     Train net output #0: loss = 1.43647 (* 1 = 1.43647 loss)
I0815 03:28:23.656190 11101 sgd_solver.cpp:136] Iteration 175400, lr = 0.0451875, m = 0.9
I0815 03:28:38.266655 11101 solver.cpp:312] Iteration 175500 (6.84457 iter/s, 14.6101s/100 iter), loss = 1.90701
I0815 03:28:38.266687 11101 solver.cpp:334]     Train net output #0: loss = 1.74985 (* 1 = 1.74985 loss)
I0815 03:28:38.266695 11101 sgd_solver.cpp:136] Iteration 175500, lr = 0.0451563, m = 0.9
I0815 03:28:53.182932 11101 solver.cpp:312] Iteration 175600 (6.70427 iter/s, 14.9159s/100 iter), loss = 1.87548
I0815 03:28:53.182966 11101 solver.cpp:334]     Train net output #0: loss = 2.2033 (* 1 = 2.2033 loss)
I0815 03:28:53.182973 11101 sgd_solver.cpp:136] Iteration 175600, lr = 0.045125, m = 0.9
I0815 03:29:07.911716 11101 solver.cpp:312] Iteration 175700 (6.78961 iter/s, 14.7284s/100 iter), loss = 1.97353
I0815 03:29:07.911785 11101 solver.cpp:334]     Train net output #0: loss = 2.24784 (* 1 = 2.24784 loss)
I0815 03:29:07.911794 11101 sgd_solver.cpp:136] Iteration 175700, lr = 0.0450938, m = 0.9
I0815 03:29:22.432953 11101 solver.cpp:312] Iteration 175800 (6.88666 iter/s, 14.5208s/100 iter), loss = 1.94592
I0815 03:29:22.433025 11101 solver.cpp:334]     Train net output #0: loss = 2.01269 (* 1 = 2.01269 loss)
I0815 03:29:22.433045 11101 sgd_solver.cpp:136] Iteration 175800, lr = 0.0450625, m = 0.9
I0815 03:29:36.786727 11101 solver.cpp:312] Iteration 175900 (6.967 iter/s, 14.3534s/100 iter), loss = 2.00534
I0815 03:29:36.786756 11101 solver.cpp:334]     Train net output #0: loss = 2.29333 (* 1 = 2.29333 loss)
I0815 03:29:36.786762 11101 sgd_solver.cpp:136] Iteration 175900, lr = 0.0450312, m = 0.9
I0815 03:29:51.162395 11101 solver.cpp:509] Iteration 176000, Testing net (#0)
I0815 03:30:12.372071 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.416235
I0815 03:30:12.372092 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.676528
I0815 03:30:12.372099 11101 solver.cpp:594]     Test net output #2: loss = 2.66238 (* 1 = 2.66238 loss)
I0815 03:30:12.372120 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.2092s
I0815 03:30:12.525132 11101 solver.cpp:312] Iteration 176000 (2.79819 iter/s, 35.7374s/100 iter), loss = 2.03027
I0815 03:30:12.525161 11101 solver.cpp:334]     Train net output #0: loss = 2.17548 (* 1 = 2.17548 loss)
I0815 03:30:12.525166 11101 sgd_solver.cpp:136] Iteration 176000, lr = 0.045, m = 0.9
I0815 03:30:27.134692 11101 solver.cpp:312] Iteration 176100 (6.84502 iter/s, 14.6092s/100 iter), loss = 2.03247
I0815 03:30:27.134768 11101 solver.cpp:334]     Train net output #0: loss = 1.8647 (* 1 = 1.8647 loss)
I0815 03:30:27.134774 11101 sgd_solver.cpp:136] Iteration 176100, lr = 0.0449688, m = 0.9
I0815 03:30:41.853443 11101 solver.cpp:312] Iteration 176200 (6.79424 iter/s, 14.7183s/100 iter), loss = 1.85663
I0815 03:30:41.853469 11101 solver.cpp:334]     Train net output #0: loss = 1.97295 (* 1 = 1.97295 loss)
I0815 03:30:41.853473 11101 sgd_solver.cpp:136] Iteration 176200, lr = 0.0449375, m = 0.9
I0815 03:30:56.482976 11101 solver.cpp:312] Iteration 176300 (6.83568 iter/s, 14.6291s/100 iter), loss = 1.90013
I0815 03:30:56.483001 11101 solver.cpp:334]     Train net output #0: loss = 1.96323 (* 1 = 1.96323 loss)
I0815 03:30:56.483006 11101 sgd_solver.cpp:136] Iteration 176300, lr = 0.0449063, m = 0.9
I0815 03:31:11.264981 11101 solver.cpp:312] Iteration 176400 (6.76517 iter/s, 14.7816s/100 iter), loss = 1.93348
I0815 03:31:11.265077 11101 solver.cpp:334]     Train net output #0: loss = 1.82392 (* 1 = 1.82392 loss)
I0815 03:31:11.265099 11101 sgd_solver.cpp:136] Iteration 176400, lr = 0.044875, m = 0.9
I0815 03:31:26.384697 11101 solver.cpp:312] Iteration 176500 (6.61406 iter/s, 15.1193s/100 iter), loss = 1.62216
I0815 03:31:26.384753 11101 solver.cpp:334]     Train net output #0: loss = 2.11467 (* 1 = 2.11467 loss)
I0815 03:31:26.384768 11101 sgd_solver.cpp:136] Iteration 176500, lr = 0.0448438, m = 0.9
I0815 03:31:41.089275 11101 solver.cpp:312] Iteration 176600 (6.80079 iter/s, 14.7042s/100 iter), loss = 1.94178
I0815 03:31:41.089303 11101 solver.cpp:334]     Train net output #0: loss = 2.32311 (* 1 = 2.32311 loss)
I0815 03:31:41.089308 11101 sgd_solver.cpp:136] Iteration 176600, lr = 0.0448125, m = 0.9
I0815 03:31:55.550833 11101 solver.cpp:312] Iteration 176700 (6.91508 iter/s, 14.4612s/100 iter), loss = 1.8242
I0815 03:31:55.550904 11101 solver.cpp:334]     Train net output #0: loss = 1.67993 (* 1 = 1.67993 loss)
I0815 03:31:55.550915 11101 sgd_solver.cpp:136] Iteration 176700, lr = 0.0447812, m = 0.9
I0815 03:32:10.068706 11101 solver.cpp:312] Iteration 176800 (6.88825 iter/s, 14.5175s/100 iter), loss = 2.0719
I0815 03:32:10.068775 11101 solver.cpp:334]     Train net output #0: loss = 2.28047 (* 1 = 2.28047 loss)
I0815 03:32:10.068795 11101 sgd_solver.cpp:136] Iteration 176800, lr = 0.04475, m = 0.9
I0815 03:32:24.713876 11101 solver.cpp:312] Iteration 176900 (6.82838 iter/s, 14.6448s/100 iter), loss = 1.75732
I0815 03:32:24.713902 11101 solver.cpp:334]     Train net output #0: loss = 2.03385 (* 1 = 2.03385 loss)
I0815 03:32:24.713908 11101 sgd_solver.cpp:136] Iteration 176900, lr = 0.0447187, m = 0.9
I0815 03:32:39.336611 11101 solver.cpp:312] Iteration 177000 (6.83885 iter/s, 14.6223s/100 iter), loss = 1.95129
I0815 03:32:39.336663 11101 solver.cpp:334]     Train net output #0: loss = 2.10886 (* 1 = 2.10886 loss)
I0815 03:32:39.336669 11101 sgd_solver.cpp:136] Iteration 177000, lr = 0.0446875, m = 0.9
I0815 03:32:54.084012 11101 solver.cpp:312] Iteration 177100 (6.78104 iter/s, 14.747s/100 iter), loss = 1.73708
I0815 03:32:54.084038 11101 solver.cpp:334]     Train net output #0: loss = 1.99873 (* 1 = 1.99873 loss)
I0815 03:32:54.084044 11101 sgd_solver.cpp:136] Iteration 177100, lr = 0.0446563, m = 0.9
I0815 03:33:08.733374 11101 solver.cpp:312] Iteration 177200 (6.82642 iter/s, 14.649s/100 iter), loss = 1.7746
I0815 03:33:08.733402 11101 solver.cpp:334]     Train net output #0: loss = 2.14008 (* 1 = 2.14008 loss)
I0815 03:33:08.733408 11101 sgd_solver.cpp:136] Iteration 177200, lr = 0.044625, m = 0.9
I0815 03:33:23.084179 11101 solver.cpp:312] Iteration 177300 (6.96844 iter/s, 14.3504s/100 iter), loss = 2.10854
I0815 03:33:23.084245 11101 solver.cpp:334]     Train net output #0: loss = 1.8702 (* 1 = 1.8702 loss)
I0815 03:33:23.084251 11101 sgd_solver.cpp:136] Iteration 177300, lr = 0.0445938, m = 0.9
I0815 03:33:37.640981 11101 solver.cpp:312] Iteration 177400 (6.86983 iter/s, 14.5564s/100 iter), loss = 1.72201
I0815 03:33:37.641003 11101 solver.cpp:334]     Train net output #0: loss = 1.59974 (* 1 = 1.59974 loss)
I0815 03:33:37.641008 11101 sgd_solver.cpp:136] Iteration 177400, lr = 0.0445625, m = 0.9
I0815 03:33:52.127351 11101 solver.cpp:312] Iteration 177500 (6.90323 iter/s, 14.486s/100 iter), loss = 1.79027
I0815 03:33:52.127378 11101 solver.cpp:334]     Train net output #0: loss = 2.15451 (* 1 = 2.15451 loss)
I0815 03:33:52.127385 11101 sgd_solver.cpp:136] Iteration 177500, lr = 0.0445313, m = 0.9
I0815 03:34:06.693467 11101 solver.cpp:312] Iteration 177600 (6.86544 iter/s, 14.5657s/100 iter), loss = 1.74466
I0815 03:34:06.693531 11101 solver.cpp:334]     Train net output #0: loss = 1.93316 (* 1 = 1.93316 loss)
I0815 03:34:06.693542 11101 sgd_solver.cpp:136] Iteration 177600, lr = 0.0445, m = 0.9
I0815 03:34:21.109383 11101 solver.cpp:312] Iteration 177700 (6.93697 iter/s, 14.4155s/100 iter), loss = 1.60538
I0815 03:34:21.109410 11101 solver.cpp:334]     Train net output #0: loss = 1.84804 (* 1 = 1.84804 loss)
I0815 03:34:21.109416 11101 sgd_solver.cpp:136] Iteration 177700, lr = 0.0444687, m = 0.9
I0815 03:34:36.097975 11101 solver.cpp:312] Iteration 177800 (6.67193 iter/s, 14.9882s/100 iter), loss = 1.71064
I0815 03:34:36.098002 11101 solver.cpp:334]     Train net output #0: loss = 1.61682 (* 1 = 1.61682 loss)
I0815 03:34:36.098279 11101 sgd_solver.cpp:136] Iteration 177800, lr = 0.0444375, m = 0.9
I0815 03:34:50.539738 11101 solver.cpp:312] Iteration 177900 (6.92455 iter/s, 14.4414s/100 iter), loss = 1.59567
I0815 03:34:50.539842 11101 solver.cpp:334]     Train net output #0: loss = 1.86692 (* 1 = 1.86692 loss)
I0815 03:34:50.539861 11101 sgd_solver.cpp:136] Iteration 177900, lr = 0.0444062, m = 0.9
I0815 03:35:04.985210 11101 solver.cpp:509] Iteration 178000, Testing net (#0)
I0815 03:35:26.042270 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.433647
I0815 03:35:26.042353 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.686057
I0815 03:35:26.042362 11101 solver.cpp:594]     Test net output #2: loss = 2.64242 (* 1 = 2.64242 loss)
I0815 03:35:26.042382 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0566s
I0815 03:35:26.201009 11101 solver.cpp:312] Iteration 178000 (2.80424 iter/s, 35.6603s/100 iter), loss = 1.87756
I0815 03:35:26.201038 11101 solver.cpp:334]     Train net output #0: loss = 1.43398 (* 1 = 1.43398 loss)
I0815 03:35:26.201045 11101 sgd_solver.cpp:136] Iteration 178000, lr = 0.044375, m = 0.9
I0815 03:35:40.787477 11101 solver.cpp:312] Iteration 178100 (6.85586 iter/s, 14.5861s/100 iter), loss = 1.87549
I0815 03:35:40.787504 11101 solver.cpp:334]     Train net output #0: loss = 2.2151 (* 1 = 2.2151 loss)
I0815 03:35:40.787510 11101 sgd_solver.cpp:136] Iteration 178100, lr = 0.0443438, m = 0.9
I0815 03:35:55.403162 11101 solver.cpp:312] Iteration 178200 (6.84215 iter/s, 14.6153s/100 iter), loss = 2.54171
I0815 03:35:55.403190 11101 solver.cpp:334]     Train net output #0: loss = 2.79449 (* 1 = 2.79449 loss)
I0815 03:35:55.403198 11101 sgd_solver.cpp:136] Iteration 178200, lr = 0.0443125, m = 0.9
I0815 03:36:10.177628 11101 solver.cpp:312] Iteration 178300 (6.76862 iter/s, 14.7741s/100 iter), loss = 2.06659
I0815 03:36:10.177690 11101 solver.cpp:334]     Train net output #0: loss = 2.31163 (* 1 = 2.31163 loss)
I0815 03:36:10.177696 11101 sgd_solver.cpp:136] Iteration 178300, lr = 0.0442813, m = 0.9
I0815 03:36:24.718435 11101 solver.cpp:312] Iteration 178400 (6.87739 iter/s, 14.5404s/100 iter), loss = 1.94899
I0815 03:36:24.718514 11101 solver.cpp:334]     Train net output #0: loss = 1.83932 (* 1 = 1.83932 loss)
I0815 03:36:24.718533 11101 sgd_solver.cpp:136] Iteration 178400, lr = 0.04425, m = 0.9
I0815 03:36:39.316704 11101 solver.cpp:312] Iteration 178500 (6.85031 iter/s, 14.5979s/100 iter), loss = 1.46452
I0815 03:36:39.316927 11101 solver.cpp:334]     Train net output #0: loss = 1.41403 (* 1 = 1.41403 loss)
I0815 03:36:39.317040 11101 sgd_solver.cpp:136] Iteration 178500, lr = 0.0442187, m = 0.9
I0815 03:36:54.229565 11101 solver.cpp:312] Iteration 178600 (6.70581 iter/s, 14.9124s/100 iter), loss = 1.70062
I0815 03:36:54.229627 11101 solver.cpp:334]     Train net output #0: loss = 1.87348 (* 1 = 1.87348 loss)
I0815 03:36:54.229635 11101 sgd_solver.cpp:136] Iteration 178600, lr = 0.0441875, m = 0.9
I0815 03:37:08.949554 11101 solver.cpp:312] Iteration 178700 (6.79367 iter/s, 14.7196s/100 iter), loss = 1.86315
I0815 03:37:08.949587 11101 solver.cpp:334]     Train net output #0: loss = 2.18755 (* 1 = 2.18755 loss)
I0815 03:37:08.949594 11101 sgd_solver.cpp:136] Iteration 178700, lr = 0.0441562, m = 0.9
I0815 03:37:23.475251 11101 solver.cpp:312] Iteration 178800 (6.88454 iter/s, 14.5253s/100 iter), loss = 1.65792
I0815 03:37:23.475281 11101 solver.cpp:334]     Train net output #0: loss = 1.58605 (* 1 = 1.58605 loss)
I0815 03:37:23.475286 11101 sgd_solver.cpp:136] Iteration 178800, lr = 0.044125, m = 0.9
I0815 03:37:38.113369 11101 solver.cpp:312] Iteration 178900 (6.83167 iter/s, 14.6377s/100 iter), loss = 1.63956
I0815 03:37:38.113451 11101 solver.cpp:334]     Train net output #0: loss = 1.58731 (* 1 = 1.58731 loss)
I0815 03:37:38.113463 11101 sgd_solver.cpp:136] Iteration 178900, lr = 0.0440938, m = 0.9
I0815 03:37:52.484470 11101 solver.cpp:312] Iteration 179000 (6.9586 iter/s, 14.3707s/100 iter), loss = 2.06533
I0815 03:37:52.484519 11101 solver.cpp:334]     Train net output #0: loss = 2.05674 (* 1 = 2.05674 loss)
I0815 03:37:52.484532 11101 sgd_solver.cpp:136] Iteration 179000, lr = 0.0440625, m = 0.9
I0815 03:38:07.187388 11101 solver.cpp:312] Iteration 179100 (6.80156 iter/s, 14.7025s/100 iter), loss = 2.33622
I0815 03:38:07.187417 11101 solver.cpp:334]     Train net output #0: loss = 2.34543 (* 1 = 2.34543 loss)
I0815 03:38:07.187422 11101 sgd_solver.cpp:136] Iteration 179100, lr = 0.0440313, m = 0.9
I0815 03:38:21.929486 11101 solver.cpp:312] Iteration 179200 (6.78348 iter/s, 14.7417s/100 iter), loss = 1.39606
I0815 03:38:21.929579 11101 solver.cpp:334]     Train net output #0: loss = 1.38753 (* 1 = 1.38753 loss)
I0815 03:38:21.929596 11101 sgd_solver.cpp:136] Iteration 179200, lr = 0.044, m = 0.9
I0815 03:38:36.223472 11101 solver.cpp:312] Iteration 179300 (6.99614 iter/s, 14.2936s/100 iter), loss = 1.86842
I0815 03:38:36.223544 11101 solver.cpp:334]     Train net output #0: loss = 2.31082 (* 1 = 2.31082 loss)
I0815 03:38:36.223563 11101 sgd_solver.cpp:136] Iteration 179300, lr = 0.0439687, m = 0.9
I0815 03:38:50.938269 11101 solver.cpp:312] Iteration 179400 (6.79607 iter/s, 14.7144s/100 iter), loss = 1.97119
I0815 03:38:50.938333 11101 solver.cpp:334]     Train net output #0: loss = 1.98331 (* 1 = 1.98331 loss)
I0815 03:38:50.938352 11101 sgd_solver.cpp:136] Iteration 179400, lr = 0.0439375, m = 0.9
I0815 03:39:05.641757 11101 solver.cpp:312] Iteration 179500 (6.8013 iter/s, 14.7031s/100 iter), loss = 1.85846
I0815 03:39:05.641818 11101 solver.cpp:334]     Train net output #0: loss = 1.41067 (* 1 = 1.41067 loss)
I0815 03:39:05.641824 11101 sgd_solver.cpp:136] Iteration 179500, lr = 0.0439062, m = 0.9
I0815 03:39:20.156087 11101 solver.cpp:312] Iteration 179600 (6.88993 iter/s, 14.5139s/100 iter), loss = 2.06325
I0815 03:39:20.156118 11101 solver.cpp:334]     Train net output #0: loss = 1.85715 (* 1 = 1.85715 loss)
I0815 03:39:20.156124 11101 sgd_solver.cpp:136] Iteration 179600, lr = 0.043875, m = 0.9
I0815 03:39:34.550739 11101 solver.cpp:312] Iteration 179700 (6.94722 iter/s, 14.3943s/100 iter), loss = 2.36218
I0815 03:39:34.550765 11101 solver.cpp:334]     Train net output #0: loss = 2.33601 (* 1 = 2.33601 loss)
I0815 03:39:34.550768 11101 sgd_solver.cpp:136] Iteration 179700, lr = 0.0438438, m = 0.9
I0815 03:39:48.919983 11101 solver.cpp:312] Iteration 179800 (6.9595 iter/s, 14.3688s/100 iter), loss = 1.62918
I0815 03:39:48.920094 11101 solver.cpp:334]     Train net output #0: loss = 1.55989 (* 1 = 1.55989 loss)
I0815 03:39:48.920109 11101 sgd_solver.cpp:136] Iteration 179800, lr = 0.0438125, m = 0.9
I0815 03:40:03.488334 11101 solver.cpp:312] Iteration 179900 (6.86438 iter/s, 14.568s/100 iter), loss = 2.5283
I0815 03:40:03.488399 11101 solver.cpp:334]     Train net output #0: loss = 3.08088 (* 1 = 3.08088 loss)
I0815 03:40:03.488416 11101 sgd_solver.cpp:136] Iteration 179900, lr = 0.0437813, m = 0.9
I0815 03:40:17.857717 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_180000.caffemodel
I0815 03:40:18.000600 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_180000.solverstate
I0815 03:40:18.004899 11101 solver.cpp:509] Iteration 180000, Testing net (#0)
I0815 03:40:24.312666 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 03:40:38.881332 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.489882
I0815 03:40:38.881361 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.740881
I0815 03:40:38.881500 11101 solver.cpp:594]     Test net output #2: loss = 2.2871 (* 1 = 2.2871 loss)
I0815 03:40:38.881558 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8761s
I0815 03:40:39.056002 11101 solver.cpp:312] Iteration 180000 (2.81162 iter/s, 35.5667s/100 iter), loss = 1.88623
I0815 03:40:39.056030 11101 solver.cpp:334]     Train net output #0: loss = 1.63731 (* 1 = 1.63731 loss)
I0815 03:40:39.056035 11101 sgd_solver.cpp:136] Iteration 180000, lr = 0.04375, m = 0.9
I0815 03:40:53.725123 11101 solver.cpp:312] Iteration 180100 (6.81723 iter/s, 14.6687s/100 iter), loss = 1.43806
I0815 03:40:53.725147 11101 solver.cpp:334]     Train net output #0: loss = 1.59218 (* 1 = 1.59218 loss)
I0815 03:40:53.725152 11101 sgd_solver.cpp:136] Iteration 180100, lr = 0.0437188, m = 0.9
I0815 03:41:08.069180 11101 solver.cpp:312] Iteration 180200 (6.97172 iter/s, 14.3437s/100 iter), loss = 1.89094
I0815 03:41:08.069260 11101 solver.cpp:334]     Train net output #0: loss = 1.79806 (* 1 = 1.79806 loss)
I0815 03:41:08.069272 11101 sgd_solver.cpp:136] Iteration 180200, lr = 0.0436875, m = 0.9
I0815 03:41:22.441918 11101 solver.cpp:312] Iteration 180300 (6.95781 iter/s, 14.3723s/100 iter), loss = 2.15107
I0815 03:41:22.441969 11101 solver.cpp:334]     Train net output #0: loss = 1.93416 (* 1 = 1.93416 loss)
I0815 03:41:22.441983 11101 sgd_solver.cpp:136] Iteration 180300, lr = 0.0436562, m = 0.9
I0815 03:41:36.888267 11101 solver.cpp:312] Iteration 180400 (6.92236 iter/s, 14.4459s/100 iter), loss = 1.76876
I0815 03:41:36.888293 11101 solver.cpp:334]     Train net output #0: loss = 1.57122 (* 1 = 1.57122 loss)
I0815 03:41:36.888296 11101 sgd_solver.cpp:136] Iteration 180400, lr = 0.043625, m = 0.9
I0815 03:41:51.458216 11101 solver.cpp:312] Iteration 180500 (6.86363 iter/s, 14.5695s/100 iter), loss = 1.51987
I0815 03:41:51.458278 11101 solver.cpp:334]     Train net output #0: loss = 1.56315 (* 1 = 1.56315 loss)
I0815 03:41:51.458287 11101 sgd_solver.cpp:136] Iteration 180500, lr = 0.0435938, m = 0.9
I0815 03:42:05.825250 11101 solver.cpp:312] Iteration 180600 (6.96057 iter/s, 14.3666s/100 iter), loss = 1.97513
I0815 03:42:05.825278 11101 solver.cpp:334]     Train net output #0: loss = 2.29647 (* 1 = 2.29647 loss)
I0815 03:42:05.825284 11101 sgd_solver.cpp:136] Iteration 180600, lr = 0.0435625, m = 0.9
I0815 03:42:20.219307 11101 solver.cpp:312] Iteration 180700 (6.9475 iter/s, 14.3937s/100 iter), loss = 2.23171
I0815 03:42:20.219336 11101 solver.cpp:334]     Train net output #0: loss = 2.46216 (* 1 = 2.46216 loss)
I0815 03:42:20.219341 11101 sgd_solver.cpp:136] Iteration 180700, lr = 0.0435313, m = 0.9
I0815 03:42:34.573633 11101 solver.cpp:312] Iteration 180800 (6.96673 iter/s, 14.3539s/100 iter), loss = 2.2584
I0815 03:42:34.573720 11101 solver.cpp:334]     Train net output #0: loss = 2.53077 (* 1 = 2.53077 loss)
I0815 03:42:34.573727 11101 sgd_solver.cpp:136] Iteration 180800, lr = 0.0435, m = 0.9
I0815 03:42:49.476807 11101 solver.cpp:312] Iteration 180900 (6.71016 iter/s, 14.9028s/100 iter), loss = 1.91176
I0815 03:42:49.476832 11101 solver.cpp:334]     Train net output #0: loss = 1.47662 (* 1 = 1.47662 loss)
I0815 03:42:49.476836 11101 sgd_solver.cpp:136] Iteration 180900, lr = 0.0434688, m = 0.9
I0815 03:43:04.356161 11101 solver.cpp:312] Iteration 181000 (6.72091 iter/s, 14.8789s/100 iter), loss = 2.18447
I0815 03:43:04.356235 11101 solver.cpp:334]     Train net output #0: loss = 2.21557 (* 1 = 2.21557 loss)
I0815 03:43:04.356256 11101 sgd_solver.cpp:136] Iteration 181000, lr = 0.0434375, m = 0.9
I0815 03:43:19.145792 11101 solver.cpp:312] Iteration 181100 (6.76168 iter/s, 14.7892s/100 iter), loss = 1.95941
I0815 03:43:19.145853 11101 solver.cpp:334]     Train net output #0: loss = 2.03656 (* 1 = 2.03656 loss)
I0815 03:43:19.145859 11101 sgd_solver.cpp:136] Iteration 181100, lr = 0.0434062, m = 0.9
I0815 03:43:33.623545 11101 solver.cpp:312] Iteration 181200 (6.90734 iter/s, 14.4774s/100 iter), loss = 1.62122
I0815 03:43:33.623574 11101 solver.cpp:334]     Train net output #0: loss = 2.02829 (* 1 = 2.02829 loss)
I0815 03:43:33.623579 11101 sgd_solver.cpp:136] Iteration 181200, lr = 0.043375, m = 0.9
I0815 03:43:48.229840 11101 solver.cpp:312] Iteration 181300 (6.84655 iter/s, 14.6059s/100 iter), loss = 2.29115
I0815 03:43:48.229868 11101 solver.cpp:334]     Train net output #0: loss = 2.48881 (* 1 = 2.48881 loss)
I0815 03:43:48.229874 11101 sgd_solver.cpp:136] Iteration 181300, lr = 0.0433438, m = 0.9
I0815 03:44:03.059618 11101 solver.cpp:312] Iteration 181400 (6.74337 iter/s, 14.8294s/100 iter), loss = 1.83849
I0815 03:44:03.059708 11101 solver.cpp:334]     Train net output #0: loss = 1.80044 (* 1 = 1.80044 loss)
I0815 03:44:03.059726 11101 sgd_solver.cpp:136] Iteration 181400, lr = 0.0433125, m = 0.9
I0815 03:44:17.820781 11101 solver.cpp:312] Iteration 181500 (6.77472 iter/s, 14.7608s/100 iter), loss = 1.97633
I0815 03:44:17.820853 11101 solver.cpp:334]     Train net output #0: loss = 1.87913 (* 1 = 1.87913 loss)
I0815 03:44:17.820874 11101 sgd_solver.cpp:136] Iteration 181500, lr = 0.0432813, m = 0.9
I0815 03:44:32.982700 11101 solver.cpp:312] Iteration 181600 (6.59565 iter/s, 15.1615s/100 iter), loss = 1.75285
I0815 03:44:32.982779 11101 solver.cpp:334]     Train net output #0: loss = 1.8112 (* 1 = 1.8112 loss)
I0815 03:44:32.982796 11101 sgd_solver.cpp:136] Iteration 181600, lr = 0.04325, m = 0.9
I0815 03:44:48.017318 11101 solver.cpp:312] Iteration 181700 (6.6515 iter/s, 15.0342s/100 iter), loss = 2.4232
I0815 03:44:48.017382 11101 solver.cpp:334]     Train net output #0: loss = 2.49935 (* 1 = 2.49935 loss)
I0815 03:44:48.017390 11101 sgd_solver.cpp:136] Iteration 181700, lr = 0.0432188, m = 0.9
I0815 03:45:03.276757 11101 solver.cpp:312] Iteration 181800 (6.5535 iter/s, 15.259s/100 iter), loss = 1.64673
I0815 03:45:03.276829 11101 solver.cpp:334]     Train net output #0: loss = 1.527 (* 1 = 1.527 loss)
I0815 03:45:03.276849 11101 sgd_solver.cpp:136] Iteration 181800, lr = 0.0431875, m = 0.9
I0815 03:45:18.400154 11101 solver.cpp:312] Iteration 181900 (6.61245 iter/s, 15.123s/100 iter), loss = 2.15014
I0815 03:45:18.400212 11101 solver.cpp:334]     Train net output #0: loss = 2.07635 (* 1 = 2.07635 loss)
I0815 03:45:18.400218 11101 sgd_solver.cpp:136] Iteration 181900, lr = 0.0431562, m = 0.9
I0815 03:45:33.511621 11101 solver.cpp:509] Iteration 182000, Testing net (#0)
I0815 03:45:54.496835 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.468117
I0815 03:45:54.496918 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.721822
I0815 03:45:54.496925 11101 solver.cpp:594]     Test net output #2: loss = 2.40099 (* 1 = 2.40099 loss)
I0815 03:45:54.496944 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9848s
I0815 03:45:54.657517 11101 solver.cpp:312] Iteration 182000 (2.75814 iter/s, 36.2564s/100 iter), loss = 1.88558
I0815 03:45:54.657546 11101 solver.cpp:334]     Train net output #0: loss = 1.81944 (* 1 = 1.81944 loss)
I0815 03:45:54.657552 11101 sgd_solver.cpp:136] Iteration 182000, lr = 0.043125, m = 0.9
I0815 03:46:09.381536 11101 solver.cpp:312] Iteration 182100 (6.79181 iter/s, 14.7236s/100 iter), loss = 1.62196
I0815 03:46:09.381600 11101 solver.cpp:334]     Train net output #0: loss = 1.87693 (* 1 = 1.87693 loss)
I0815 03:46:09.381613 11101 sgd_solver.cpp:136] Iteration 182100, lr = 0.0430938, m = 0.9
I0815 03:46:23.998352 11101 solver.cpp:312] Iteration 182200 (6.84163 iter/s, 14.6164s/100 iter), loss = 1.56012
I0815 03:46:23.998385 11101 solver.cpp:334]     Train net output #0: loss = 1.62061 (* 1 = 1.62061 loss)
I0815 03:46:23.998391 11101 sgd_solver.cpp:136] Iteration 182200, lr = 0.0430625, m = 0.9
I0815 03:46:38.570194 11101 solver.cpp:312] Iteration 182300 (6.86274 iter/s, 14.5714s/100 iter), loss = 2.04143
I0815 03:46:38.570464 11101 solver.cpp:334]     Train net output #0: loss = 2.55902 (* 1 = 2.55902 loss)
I0815 03:46:38.570474 11101 sgd_solver.cpp:136] Iteration 182300, lr = 0.0430313, m = 0.9
I0815 03:46:53.312527 11101 solver.cpp:312] Iteration 182400 (6.78337 iter/s, 14.7419s/100 iter), loss = 1.46162
I0815 03:46:53.312553 11101 solver.cpp:334]     Train net output #0: loss = 1.42179 (* 1 = 1.42179 loss)
I0815 03:46:53.312557 11101 sgd_solver.cpp:136] Iteration 182400, lr = 0.043, m = 0.9
I0815 03:47:07.941433 11101 solver.cpp:312] Iteration 182500 (6.83597 iter/s, 14.6285s/100 iter), loss = 1.99851
I0815 03:47:07.941464 11101 solver.cpp:334]     Train net output #0: loss = 1.97474 (* 1 = 1.97474 loss)
I0815 03:47:07.941470 11101 sgd_solver.cpp:136] Iteration 182500, lr = 0.0429688, m = 0.9
I0815 03:47:22.636626 11101 solver.cpp:312] Iteration 182600 (6.80513 iter/s, 14.6948s/100 iter), loss = 1.48416
I0815 03:47:22.636705 11101 solver.cpp:334]     Train net output #0: loss = 1.33311 (* 1 = 1.33311 loss)
I0815 03:47:22.636719 11101 sgd_solver.cpp:136] Iteration 182600, lr = 0.0429375, m = 0.9
I0815 03:47:37.337236 11101 solver.cpp:312] Iteration 182700 (6.80263 iter/s, 14.7002s/100 iter), loss = 1.38852
I0815 03:47:37.337287 11101 solver.cpp:334]     Train net output #0: loss = 1.0588 (* 1 = 1.0588 loss)
I0815 03:47:37.337301 11101 sgd_solver.cpp:136] Iteration 182700, lr = 0.0429063, m = 0.9
I0815 03:47:52.407786 11101 solver.cpp:312] Iteration 182800 (6.63564 iter/s, 15.0701s/100 iter), loss = 2.03772
I0815 03:47:52.407860 11101 solver.cpp:334]     Train net output #0: loss = 2.04521 (* 1 = 2.04521 loss)
I0815 03:47:52.407881 11101 sgd_solver.cpp:136] Iteration 182800, lr = 0.042875, m = 0.9
I0815 03:48:07.240489 11101 solver.cpp:312] Iteration 182900 (6.74205 iter/s, 14.8323s/100 iter), loss = 2.03722
I0815 03:48:07.240548 11101 solver.cpp:334]     Train net output #0: loss = 1.95497 (* 1 = 1.95497 loss)
I0815 03:48:07.240557 11101 sgd_solver.cpp:136] Iteration 182900, lr = 0.0428437, m = 0.9
I0815 03:48:22.200861 11101 solver.cpp:312] Iteration 183000 (6.68451 iter/s, 14.96s/100 iter), loss = 2.06185
I0815 03:48:22.200888 11101 solver.cpp:334]     Train net output #0: loss = 2.40853 (* 1 = 2.40853 loss)
I0815 03:48:22.200893 11101 sgd_solver.cpp:136] Iteration 183000, lr = 0.0428125, m = 0.9
I0815 03:48:37.290329 11101 solver.cpp:312] Iteration 183100 (6.62732 iter/s, 15.0891s/100 iter), loss = 1.67955
I0815 03:48:37.290666 11101 solver.cpp:334]     Train net output #0: loss = 1.55583 (* 1 = 1.55583 loss)
I0815 03:48:37.290772 11101 sgd_solver.cpp:136] Iteration 183100, lr = 0.0427813, m = 0.9
I0815 03:48:51.880663 11101 solver.cpp:312] Iteration 183200 (6.85404 iter/s, 14.5899s/100 iter), loss = 2.09396
I0815 03:48:51.880693 11101 solver.cpp:334]     Train net output #0: loss = 2.25931 (* 1 = 2.25931 loss)
I0815 03:48:51.880699 11101 sgd_solver.cpp:136] Iteration 183200, lr = 0.04275, m = 0.9
I0815 03:49:06.738813 11101 solver.cpp:312] Iteration 183300 (6.7305 iter/s, 14.8577s/100 iter), loss = 2.11062
I0815 03:49:06.738840 11101 solver.cpp:334]     Train net output #0: loss = 1.751 (* 1 = 1.751 loss)
I0815 03:49:06.738847 11101 sgd_solver.cpp:136] Iteration 183300, lr = 0.0427187, m = 0.9
I0815 03:49:21.218164 11101 solver.cpp:312] Iteration 183400 (6.90658 iter/s, 14.479s/100 iter), loss = 1.89168
I0815 03:49:21.218247 11101 solver.cpp:334]     Train net output #0: loss = 1.7951 (* 1 = 1.7951 loss)
I0815 03:49:21.218255 11101 sgd_solver.cpp:136] Iteration 183400, lr = 0.0426875, m = 0.9
I0815 03:49:35.734469 11101 solver.cpp:312] Iteration 183500 (6.889 iter/s, 14.5159s/100 iter), loss = 2.05052
I0815 03:49:35.734496 11101 solver.cpp:334]     Train net output #0: loss = 2.20891 (* 1 = 2.20891 loss)
I0815 03:49:35.734503 11101 sgd_solver.cpp:136] Iteration 183500, lr = 0.0426563, m = 0.9
I0815 03:49:50.298632 11101 solver.cpp:312] Iteration 183600 (6.86636 iter/s, 14.5638s/100 iter), loss = 2.09296
I0815 03:49:50.298658 11101 solver.cpp:334]     Train net output #0: loss = 2.08733 (* 1 = 2.08733 loss)
I0815 03:49:50.298665 11101 sgd_solver.cpp:136] Iteration 183600, lr = 0.042625, m = 0.9
I0815 03:50:05.098076 11101 solver.cpp:312] Iteration 183700 (6.7572 iter/s, 14.799s/100 iter), loss = 1.84534
I0815 03:50:05.098137 11101 solver.cpp:334]     Train net output #0: loss = 1.8909 (* 1 = 1.8909 loss)
I0815 03:50:05.098145 11101 sgd_solver.cpp:136] Iteration 183700, lr = 0.0425937, m = 0.9
I0815 03:50:19.839890 11101 solver.cpp:312] Iteration 183800 (6.78361 iter/s, 14.7414s/100 iter), loss = 1.77787
I0815 03:50:19.839916 11101 solver.cpp:334]     Train net output #0: loss = 1.62672 (* 1 = 1.62672 loss)
I0815 03:50:19.839921 11101 sgd_solver.cpp:136] Iteration 183800, lr = 0.0425625, m = 0.9
I0815 03:50:34.562068 11101 solver.cpp:312] Iteration 183900 (6.79266 iter/s, 14.7218s/100 iter), loss = 1.67212
I0815 03:50:34.562098 11101 solver.cpp:334]     Train net output #0: loss = 1.66746 (* 1 = 1.66746 loss)
I0815 03:50:34.562103 11101 sgd_solver.cpp:136] Iteration 183900, lr = 0.0425313, m = 0.9
I0815 03:50:49.581403 11101 solver.cpp:509] Iteration 184000, Testing net (#0)
I0815 03:51:10.194681 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.446529
I0815 03:51:10.194702 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.697881
I0815 03:51:10.194710 11101 solver.cpp:594]     Test net output #2: loss = 2.6039 (* 1 = 2.6039 loss)
I0815 03:51:10.194738 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6128s
I0815 03:51:10.362413 11101 solver.cpp:312] Iteration 184000 (2.79335 iter/s, 35.7994s/100 iter), loss = 1.70149
I0815 03:51:10.362444 11101 solver.cpp:334]     Train net output #0: loss = 1.87373 (* 1 = 1.87373 loss)
I0815 03:51:10.362450 11101 sgd_solver.cpp:136] Iteration 184000, lr = 0.0425, m = 0.9
I0815 03:51:25.075037 11101 solver.cpp:312] Iteration 184100 (6.79707 iter/s, 14.7122s/100 iter), loss = 2.15558
I0815 03:51:25.075145 11101 solver.cpp:334]     Train net output #0: loss = 2.49185 (* 1 = 2.49185 loss)
I0815 03:51:25.075165 11101 sgd_solver.cpp:136] Iteration 184100, lr = 0.0424688, m = 0.9
I0815 03:51:39.907753 11101 solver.cpp:312] Iteration 184200 (6.74204 iter/s, 14.8323s/100 iter), loss = 1.74461
I0815 03:51:39.907780 11101 solver.cpp:334]     Train net output #0: loss = 1.57893 (* 1 = 1.57893 loss)
I0815 03:51:39.907785 11101 sgd_solver.cpp:136] Iteration 184200, lr = 0.0424375, m = 0.9
I0815 03:51:54.432386 11101 solver.cpp:312] Iteration 184300 (6.88505 iter/s, 14.5242s/100 iter), loss = 1.79255
I0815 03:51:54.432413 11101 solver.cpp:334]     Train net output #0: loss = 1.8773 (* 1 = 1.8773 loss)
I0815 03:51:54.432418 11101 sgd_solver.cpp:136] Iteration 184300, lr = 0.0424062, m = 0.9
I0815 03:52:09.044064 11101 solver.cpp:312] Iteration 184400 (6.84403 iter/s, 14.6113s/100 iter), loss = 1.56334
I0815 03:52:09.044117 11101 solver.cpp:334]     Train net output #0: loss = 1.73537 (* 1 = 1.73537 loss)
I0815 03:52:09.044157 11101 sgd_solver.cpp:136] Iteration 184400, lr = 0.042375, m = 0.9
I0815 03:52:23.678915 11101 solver.cpp:312] Iteration 184500 (6.83319 iter/s, 14.6345s/100 iter), loss = 2.10473
I0815 03:52:23.678978 11101 solver.cpp:334]     Train net output #0: loss = 2.02497 (* 1 = 2.02497 loss)
I0815 03:52:23.678995 11101 sgd_solver.cpp:136] Iteration 184500, lr = 0.0423437, m = 0.9
I0815 03:52:38.829810 11101 solver.cpp:312] Iteration 184600 (6.60045 iter/s, 15.1505s/100 iter), loss = 1.41781
I0815 03:52:38.829880 11101 solver.cpp:334]     Train net output #0: loss = 1.16332 (* 1 = 1.16332 loss)
I0815 03:52:38.829900 11101 sgd_solver.cpp:136] Iteration 184600, lr = 0.0423125, m = 0.9
I0815 03:52:53.827164 11101 solver.cpp:312] Iteration 184700 (6.66803 iter/s, 14.9969s/100 iter), loss = 1.73185
I0815 03:52:53.827262 11101 solver.cpp:334]     Train net output #0: loss = 1.73956 (* 1 = 1.73956 loss)
I0815 03:52:53.827280 11101 sgd_solver.cpp:136] Iteration 184700, lr = 0.0422813, m = 0.9
I0815 03:53:08.396109 11101 solver.cpp:312] Iteration 184800 (6.86411 iter/s, 14.5685s/100 iter), loss = 1.73108
I0815 03:53:08.396149 11101 solver.cpp:334]     Train net output #0: loss = 1.69106 (* 1 = 1.69106 loss)
I0815 03:53:08.396157 11101 sgd_solver.cpp:136] Iteration 184800, lr = 0.04225, m = 0.9
I0815 03:53:23.149380 11101 solver.cpp:312] Iteration 184900 (6.77834 iter/s, 14.7529s/100 iter), loss = 1.63088
I0815 03:53:23.149408 11101 solver.cpp:334]     Train net output #0: loss = 1.79222 (* 1 = 1.79222 loss)
I0815 03:53:23.149413 11101 sgd_solver.cpp:136] Iteration 184900, lr = 0.0422188, m = 0.9
I0815 03:53:37.863538 11101 solver.cpp:312] Iteration 185000 (6.79636 iter/s, 14.7137s/100 iter), loss = 2.03985
I0815 03:53:37.863601 11101 solver.cpp:334]     Train net output #0: loss = 2.28184 (* 1 = 2.28184 loss)
I0815 03:53:37.863610 11101 sgd_solver.cpp:136] Iteration 185000, lr = 0.0421875, m = 0.9
I0815 03:53:52.292157 11101 solver.cpp:312] Iteration 185100 (6.93086 iter/s, 14.4282s/100 iter), loss = 1.95184
I0815 03:53:52.292227 11101 solver.cpp:334]     Train net output #0: loss = 1.69241 (* 1 = 1.69241 loss)
I0815 03:53:52.292248 11101 sgd_solver.cpp:136] Iteration 185100, lr = 0.0421562, m = 0.9
I0815 03:54:06.813503 11101 solver.cpp:312] Iteration 185200 (6.8866 iter/s, 14.5209s/100 iter), loss = 2.02288
I0815 03:54:06.813568 11101 solver.cpp:334]     Train net output #0: loss = 2.09272 (* 1 = 2.09272 loss)
I0815 03:54:06.813586 11101 sgd_solver.cpp:136] Iteration 185200, lr = 0.042125, m = 0.9
I0815 03:54:21.288998 11101 solver.cpp:312] Iteration 185300 (6.90842 iter/s, 14.4751s/100 iter), loss = 1.80814
I0815 03:54:21.289059 11101 solver.cpp:334]     Train net output #0: loss = 2.03239 (* 1 = 2.03239 loss)
I0815 03:54:21.289067 11101 sgd_solver.cpp:136] Iteration 185300, lr = 0.0420938, m = 0.9
I0815 03:54:35.918058 11101 solver.cpp:312] Iteration 185400 (6.8359 iter/s, 14.6287s/100 iter), loss = 1.97731
I0815 03:54:35.918087 11101 solver.cpp:334]     Train net output #0: loss = 1.99372 (* 1 = 1.99372 loss)
I0815 03:54:35.918093 11101 sgd_solver.cpp:136] Iteration 185400, lr = 0.0420625, m = 0.9
I0815 03:54:50.635789 11101 solver.cpp:312] Iteration 185500 (6.79471 iter/s, 14.7173s/100 iter), loss = 1.91847
I0815 03:54:50.635843 11101 solver.cpp:334]     Train net output #0: loss = 2.00265 (* 1 = 2.00265 loss)
I0815 03:54:50.635856 11101 sgd_solver.cpp:136] Iteration 185500, lr = 0.0420313, m = 0.9
I0815 03:55:05.322140 11101 solver.cpp:312] Iteration 185600 (6.80923 iter/s, 14.6859s/100 iter), loss = 1.48139
I0815 03:55:05.322185 11101 solver.cpp:334]     Train net output #0: loss = 1.48264 (* 1 = 1.48264 loss)
I0815 03:55:05.322192 11101 sgd_solver.cpp:136] Iteration 185600, lr = 0.042, m = 0.9
I0815 03:55:19.801085 11101 solver.cpp:312] Iteration 185700 (6.90677 iter/s, 14.4785s/100 iter), loss = 1.56844
I0815 03:55:19.801110 11101 solver.cpp:334]     Train net output #0: loss = 1.59401 (* 1 = 1.59401 loss)
I0815 03:55:19.801116 11101 sgd_solver.cpp:136] Iteration 185700, lr = 0.0419688, m = 0.9
I0815 03:55:34.258064 11101 solver.cpp:312] Iteration 185800 (6.91726 iter/s, 14.4566s/100 iter), loss = 1.63395
I0815 03:55:34.258136 11101 solver.cpp:334]     Train net output #0: loss = 1.67289 (* 1 = 1.67289 loss)
I0815 03:55:34.258153 11101 sgd_solver.cpp:136] Iteration 185800, lr = 0.0419375, m = 0.9
I0815 03:55:48.968524 11101 solver.cpp:312] Iteration 185900 (6.79807 iter/s, 14.7101s/100 iter), loss = 1.75605
I0815 03:55:48.968600 11101 solver.cpp:334]     Train net output #0: loss = 1.78359 (* 1 = 1.78359 loss)
I0815 03:55:48.968610 11101 sgd_solver.cpp:136] Iteration 185900, lr = 0.0419062, m = 0.9
I0815 03:56:03.571130 11101 solver.cpp:509] Iteration 186000, Testing net (#0)
I0815 03:56:05.636119 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 03:56:06.694900 11099 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 03:56:24.439868 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.453235
I0815 03:56:24.439914 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.699998
I0815 03:56:24.439921 11101 solver.cpp:594]     Test net output #2: loss = 2.55425 (* 1 = 2.55425 loss)
I0815 03:56:24.439939 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8682s
I0815 03:56:24.578939 11101 solver.cpp:312] Iteration 186000 (2.80824 iter/s, 35.6094s/100 iter), loss = 1.81393
I0815 03:56:24.578964 11101 solver.cpp:334]     Train net output #0: loss = 1.94616 (* 1 = 1.94616 loss)
I0815 03:56:24.578970 11101 sgd_solver.cpp:136] Iteration 186000, lr = 0.041875, m = 0.9
I0815 03:56:39.046214 11101 solver.cpp:312] Iteration 186100 (6.91234 iter/s, 14.4669s/100 iter), loss = 1.96047
I0815 03:56:39.046239 11101 solver.cpp:334]     Train net output #0: loss = 1.95223 (* 1 = 1.95223 loss)
I0815 03:56:39.046247 11101 sgd_solver.cpp:136] Iteration 186100, lr = 0.0418437, m = 0.9
I0815 03:56:53.765780 11101 solver.cpp:312] Iteration 186200 (6.79387 iter/s, 14.7192s/100 iter), loss = 1.87848
I0815 03:56:53.765807 11101 solver.cpp:334]     Train net output #0: loss = 1.71828 (* 1 = 1.71828 loss)
I0815 03:56:53.765813 11101 sgd_solver.cpp:136] Iteration 186200, lr = 0.0418125, m = 0.9
I0815 03:57:08.642123 11101 solver.cpp:312] Iteration 186300 (6.72227 iter/s, 14.8759s/100 iter), loss = 1.91105
I0815 03:57:08.642333 11101 solver.cpp:334]     Train net output #0: loss = 1.80273 (* 1 = 1.80273 loss)
I0815 03:57:08.642339 11101 sgd_solver.cpp:136] Iteration 186300, lr = 0.0417813, m = 0.9
I0815 03:57:23.506122 11101 solver.cpp:312] Iteration 186400 (6.72785 iter/s, 14.8636s/100 iter), loss = 2.02303
I0815 03:57:23.506152 11101 solver.cpp:334]     Train net output #0: loss = 2.00231 (* 1 = 2.00231 loss)
I0815 03:57:23.506158 11101 sgd_solver.cpp:136] Iteration 186400, lr = 0.04175, m = 0.9
I0815 03:57:38.045413 11101 solver.cpp:312] Iteration 186500 (6.87811 iter/s, 14.5389s/100 iter), loss = 1.77678
I0815 03:57:38.045437 11101 solver.cpp:334]     Train net output #0: loss = 1.83472 (* 1 = 1.83472 loss)
I0815 03:57:38.045442 11101 sgd_solver.cpp:136] Iteration 186500, lr = 0.0417188, m = 0.9
I0815 03:57:52.557828 11101 solver.cpp:312] Iteration 186600 (6.89084 iter/s, 14.512s/100 iter), loss = 2.17489
I0815 03:57:52.557905 11101 solver.cpp:334]     Train net output #0: loss = 2.67802 (* 1 = 2.67802 loss)
I0815 03:57:52.557929 11101 sgd_solver.cpp:136] Iteration 186600, lr = 0.0416875, m = 0.9
I0815 03:58:07.259582 11101 solver.cpp:312] Iteration 186700 (6.8021 iter/s, 14.7013s/100 iter), loss = 1.9423
I0815 03:58:07.259609 11101 solver.cpp:334]     Train net output #0: loss = 1.82261 (* 1 = 1.82261 loss)
I0815 03:58:07.259615 11101 sgd_solver.cpp:136] Iteration 186700, lr = 0.0416563, m = 0.9
I0815 03:58:21.972239 11101 solver.cpp:312] Iteration 186800 (6.79706 iter/s, 14.7122s/100 iter), loss = 1.84832
I0815 03:58:21.972262 11101 solver.cpp:334]     Train net output #0: loss = 2.27167 (* 1 = 2.27167 loss)
I0815 03:58:21.972268 11101 sgd_solver.cpp:136] Iteration 186800, lr = 0.041625, m = 0.9
I0815 03:58:37.039880 11101 solver.cpp:312] Iteration 186900 (6.63692 iter/s, 15.0672s/100 iter), loss = 1.38231
I0815 03:58:37.039938 11101 solver.cpp:334]     Train net output #0: loss = 1.31951 (* 1 = 1.31951 loss)
I0815 03:58:37.039945 11101 sgd_solver.cpp:136] Iteration 186900, lr = 0.0415937, m = 0.9
I0815 03:58:51.581223 11101 solver.cpp:312] Iteration 187000 (6.87713 iter/s, 14.5409s/100 iter), loss = 2.09474
I0815 03:58:51.581250 11101 solver.cpp:334]     Train net output #0: loss = 1.9889 (* 1 = 1.9889 loss)
I0815 03:58:51.581254 11101 sgd_solver.cpp:136] Iteration 187000, lr = 0.0415625, m = 0.9
I0815 03:59:06.073827 11101 solver.cpp:312] Iteration 187100 (6.90026 iter/s, 14.4922s/100 iter), loss = 1.87066
I0815 03:59:06.073853 11101 solver.cpp:334]     Train net output #0: loss = 2.09043 (* 1 = 2.09043 loss)
I0815 03:59:06.073858 11101 sgd_solver.cpp:136] Iteration 187100, lr = 0.0415313, m = 0.9
I0815 03:59:20.574532 11101 solver.cpp:312] Iteration 187200 (6.89641 iter/s, 14.5003s/100 iter), loss = 1.97383
I0815 03:59:20.574652 11101 solver.cpp:334]     Train net output #0: loss = 1.68205 (* 1 = 1.68205 loss)
I0815 03:59:20.574673 11101 sgd_solver.cpp:136] Iteration 187200, lr = 0.0415, m = 0.9
I0815 03:59:35.224244 11101 solver.cpp:312] Iteration 187300 (6.82626 iter/s, 14.6493s/100 iter), loss = 1.86032
I0815 03:59:35.224310 11101 solver.cpp:334]     Train net output #0: loss = 1.82625 (* 1 = 1.82625 loss)
I0815 03:59:35.224331 11101 sgd_solver.cpp:136] Iteration 187300, lr = 0.0414688, m = 0.9
I0815 03:59:50.223399 11101 solver.cpp:312] Iteration 187400 (6.66723 iter/s, 14.9987s/100 iter), loss = 1.78144
I0815 03:59:50.223423 11101 solver.cpp:334]     Train net output #0: loss = 1.86571 (* 1 = 1.86571 loss)
I0815 03:59:50.223426 11101 sgd_solver.cpp:136] Iteration 187400, lr = 0.0414375, m = 0.9
I0815 04:00:04.814338 11101 solver.cpp:312] Iteration 187500 (6.85376 iter/s, 14.5905s/100 iter), loss = 1.94351
I0815 04:00:04.814400 11101 solver.cpp:334]     Train net output #0: loss = 1.66073 (* 1 = 1.66073 loss)
I0815 04:00:04.814409 11101 sgd_solver.cpp:136] Iteration 187500, lr = 0.0414063, m = 0.9
I0815 04:00:19.688674 11101 solver.cpp:312] Iteration 187600 (6.72317 iter/s, 14.8739s/100 iter), loss = 1.93673
I0815 04:00:19.688700 11101 solver.cpp:334]     Train net output #0: loss = 2.25391 (* 1 = 2.25391 loss)
I0815 04:00:19.688705 11101 sgd_solver.cpp:136] Iteration 187600, lr = 0.041375, m = 0.9
I0815 04:00:34.328641 11101 solver.cpp:312] Iteration 187700 (6.8308 iter/s, 14.6396s/100 iter), loss = 1.75738
I0815 04:00:34.328673 11101 solver.cpp:334]     Train net output #0: loss = 1.46509 (* 1 = 1.46509 loss)
I0815 04:00:34.328680 11101 sgd_solver.cpp:136] Iteration 187700, lr = 0.0413437, m = 0.9
I0815 04:00:49.204651 11101 solver.cpp:312] Iteration 187800 (6.72242 iter/s, 14.8756s/100 iter), loss = 1.83349
I0815 04:00:49.204715 11101 solver.cpp:334]     Train net output #0: loss = 2.31576 (* 1 = 2.31576 loss)
I0815 04:00:49.204726 11101 sgd_solver.cpp:136] Iteration 187800, lr = 0.0413125, m = 0.9
I0815 04:01:04.165838 11101 solver.cpp:312] Iteration 187900 (6.68415 iter/s, 14.9608s/100 iter), loss = 1.87775
I0815 04:01:04.165868 11101 solver.cpp:334]     Train net output #0: loss = 1.36734 (* 1 = 1.36734 loss)
I0815 04:01:04.165874 11101 sgd_solver.cpp:136] Iteration 187900, lr = 0.0412812, m = 0.9
I0815 04:01:18.720386 11101 solver.cpp:509] Iteration 188000, Testing net (#0)
I0815 04:01:39.629159 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.450941
I0815 04:01:39.629209 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.71194
I0815 04:01:39.629216 11101 solver.cpp:594]     Test net output #2: loss = 2.489 (* 1 = 2.489 loss)
I0815 04:01:39.629235 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9083s
I0815 04:01:39.785806 11101 solver.cpp:312] Iteration 188000 (2.80749 iter/s, 35.619s/100 iter), loss = 1.91321
I0815 04:01:39.785830 11101 solver.cpp:334]     Train net output #0: loss = 2.15298 (* 1 = 2.15298 loss)
I0815 04:01:39.785836 11101 sgd_solver.cpp:136] Iteration 188000, lr = 0.04125, m = 0.9
I0815 04:01:54.400162 11101 solver.cpp:312] Iteration 188100 (6.84278 iter/s, 14.6139s/100 iter), loss = 1.90583
I0815 04:01:54.400226 11101 solver.cpp:334]     Train net output #0: loss = 1.77478 (* 1 = 1.77478 loss)
I0815 04:01:54.400249 11101 sgd_solver.cpp:136] Iteration 188100, lr = 0.0412188, m = 0.9
I0815 04:02:08.882906 11101 solver.cpp:312] Iteration 188200 (6.90496 iter/s, 14.4823s/100 iter), loss = 2.01931
I0815 04:02:08.882935 11101 solver.cpp:334]     Train net output #0: loss = 2.03049 (* 1 = 2.03049 loss)
I0815 04:02:08.882941 11101 sgd_solver.cpp:136] Iteration 188200, lr = 0.0411875, m = 0.9
I0815 04:02:23.408756 11101 solver.cpp:312] Iteration 188300 (6.88447 iter/s, 14.5254s/100 iter), loss = 1.78093
I0815 04:02:23.408828 11101 solver.cpp:334]     Train net output #0: loss = 2.04188 (* 1 = 2.04188 loss)
I0815 04:02:23.408833 11101 sgd_solver.cpp:136] Iteration 188300, lr = 0.0411563, m = 0.9
I0815 04:02:38.332245 11101 solver.cpp:312] Iteration 188400 (6.70103 iter/s, 14.9231s/100 iter), loss = 2.24151
I0815 04:02:38.332273 11101 solver.cpp:334]     Train net output #0: loss = 2.29837 (* 1 = 2.29837 loss)
I0815 04:02:38.332276 11101 sgd_solver.cpp:136] Iteration 188400, lr = 0.041125, m = 0.9
I0815 04:02:53.203943 11101 solver.cpp:312] Iteration 188500 (6.72437 iter/s, 14.8713s/100 iter), loss = 2.15268
I0815 04:02:53.204011 11101 solver.cpp:334]     Train net output #0: loss = 2.7079 (* 1 = 2.7079 loss)
I0815 04:02:53.204028 11101 sgd_solver.cpp:136] Iteration 188500, lr = 0.0410937, m = 0.9
I0815 04:03:08.192867 11101 solver.cpp:312] Iteration 188600 (6.67178 iter/s, 14.9885s/100 iter), loss = 1.91741
I0815 04:03:08.193116 11101 solver.cpp:334]     Train net output #0: loss = 1.85006 (* 1 = 1.85006 loss)
I0815 04:03:08.193236 11101 sgd_solver.cpp:136] Iteration 188600, lr = 0.0410625, m = 0.9
I0815 04:03:23.207499 11101 solver.cpp:312] Iteration 188700 (6.66035 iter/s, 15.0142s/100 iter), loss = 1.50049
I0815 04:03:23.207527 11101 solver.cpp:334]     Train net output #0: loss = 1.71016 (* 1 = 1.71016 loss)
I0815 04:03:23.207533 11101 sgd_solver.cpp:136] Iteration 188700, lr = 0.0410312, m = 0.9
I0815 04:03:37.672700 11101 solver.cpp:312] Iteration 188800 (6.91333 iter/s, 14.4648s/100 iter), loss = 1.87608
I0815 04:03:37.672839 11101 solver.cpp:334]     Train net output #0: loss = 2.09296 (* 1 = 2.09296 loss)
I0815 04:03:37.672859 11101 sgd_solver.cpp:136] Iteration 188800, lr = 0.041, m = 0.9
I0815 04:03:52.186203 11101 solver.cpp:312] Iteration 188900 (6.89033 iter/s, 14.5131s/100 iter), loss = 1.73478
I0815 04:03:52.186305 11101 solver.cpp:334]     Train net output #0: loss = 1.50075 (* 1 = 1.50075 loss)
I0815 04:03:52.186324 11101 sgd_solver.cpp:136] Iteration 188900, lr = 0.0409688, m = 0.9
I0815 04:04:06.603888 11101 solver.cpp:312] Iteration 189000 (6.93612 iter/s, 14.4173s/100 iter), loss = 2.23509
I0815 04:04:06.603912 11101 solver.cpp:334]     Train net output #0: loss = 2.26031 (* 1 = 2.26031 loss)
I0815 04:04:06.603919 11101 sgd_solver.cpp:136] Iteration 189000, lr = 0.0409375, m = 0.9
I0815 04:04:21.393602 11101 solver.cpp:312] Iteration 189100 (6.76164 iter/s, 14.7893s/100 iter), loss = 1.58774
I0815 04:04:21.393822 11101 solver.cpp:334]     Train net output #0: loss = 1.59504 (* 1 = 1.59504 loss)
I0815 04:04:21.393931 11101 sgd_solver.cpp:136] Iteration 189100, lr = 0.0409063, m = 0.9
I0815 04:04:35.943586 11101 solver.cpp:312] Iteration 189200 (6.87305 iter/s, 14.5496s/100 iter), loss = 1.81523
I0815 04:04:35.943668 11101 solver.cpp:334]     Train net output #0: loss = 1.76107 (* 1 = 1.76107 loss)
I0815 04:04:35.943681 11101 sgd_solver.cpp:136] Iteration 189200, lr = 0.040875, m = 0.9
I0815 04:04:50.316465 11101 solver.cpp:312] Iteration 189300 (6.95774 iter/s, 14.3725s/100 iter), loss = 2.06769
I0815 04:04:50.316493 11101 solver.cpp:334]     Train net output #0: loss = 2.22009 (* 1 = 2.22009 loss)
I0815 04:04:50.316499 11101 sgd_solver.cpp:136] Iteration 189300, lr = 0.0408438, m = 0.9
I0815 04:05:04.817864 11101 solver.cpp:312] Iteration 189400 (6.89608 iter/s, 14.501s/100 iter), loss = 1.74769
I0815 04:05:04.817936 11101 solver.cpp:334]     Train net output #0: loss = 1.84651 (* 1 = 1.84651 loss)
I0815 04:05:04.817956 11101 sgd_solver.cpp:136] Iteration 189400, lr = 0.0408125, m = 0.9
I0815 04:05:19.475744 11101 solver.cpp:312] Iteration 189500 (6.82246 iter/s, 14.6575s/100 iter), loss = 1.78255
I0815 04:05:19.476017 11101 solver.cpp:334]     Train net output #0: loss = 1.53975 (* 1 = 1.53975 loss)
I0815 04:05:19.476160 11101 sgd_solver.cpp:136] Iteration 189500, lr = 0.0407812, m = 0.9
I0815 04:05:34.207257 11101 solver.cpp:312] Iteration 189600 (6.78836 iter/s, 14.7311s/100 iter), loss = 1.46784
I0815 04:05:34.207286 11101 solver.cpp:334]     Train net output #0: loss = 1.48791 (* 1 = 1.48791 loss)
I0815 04:05:34.207293 11101 sgd_solver.cpp:136] Iteration 189600, lr = 0.04075, m = 0.9
I0815 04:05:48.884112 11101 solver.cpp:312] Iteration 189700 (6.81364 iter/s, 14.6765s/100 iter), loss = 1.1957
I0815 04:05:48.884191 11101 solver.cpp:334]     Train net output #0: loss = 1.10596 (* 1 = 1.10596 loss)
I0815 04:05:48.884210 11101 sgd_solver.cpp:136] Iteration 189700, lr = 0.0407188, m = 0.9
I0815 04:06:03.244928 11101 solver.cpp:312] Iteration 189800 (6.96358 iter/s, 14.3604s/100 iter), loss = 1.96147
I0815 04:06:03.245149 11101 solver.cpp:334]     Train net output #0: loss = 1.83469 (* 1 = 1.83469 loss)
I0815 04:06:03.245263 11101 sgd_solver.cpp:136] Iteration 189800, lr = 0.0406875, m = 0.9
I0815 04:06:17.790920 11101 solver.cpp:312] Iteration 189900 (6.87494 iter/s, 14.5456s/100 iter), loss = 2.11492
I0815 04:06:17.790961 11101 solver.cpp:334]     Train net output #0: loss = 2.32984 (* 1 = 2.32984 loss)
I0815 04:06:17.790973 11101 sgd_solver.cpp:136] Iteration 189900, lr = 0.0406562, m = 0.9
I0815 04:06:32.066275 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_190000.caffemodel
I0815 04:06:32.105568 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_190000.solverstate
I0815 04:06:32.111122 11101 solver.cpp:509] Iteration 190000, Testing net (#0)
I0815 04:06:53.004305 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.47053
I0815 04:06:53.004359 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.728586
I0815 04:06:53.004366 11101 solver.cpp:594]     Test net output #2: loss = 2.43443 (* 1 = 2.43443 loss)
I0815 04:06:53.004385 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8927s
I0815 04:06:53.158430 11101 solver.cpp:312] Iteration 190000 (2.82753 iter/s, 35.3665s/100 iter), loss = 2.24046
I0815 04:06:53.158458 11101 solver.cpp:334]     Train net output #0: loss = 2.20433 (* 1 = 2.20433 loss)
I0815 04:06:53.158465 11101 sgd_solver.cpp:136] Iteration 190000, lr = 0.040625, m = 0.9
I0815 04:07:07.506160 11101 solver.cpp:312] Iteration 190100 (6.96994 iter/s, 14.3473s/100 iter), loss = 2.031
I0815 04:07:07.506230 11101 solver.cpp:334]     Train net output #0: loss = 1.78812 (* 1 = 1.78812 loss)
I0815 04:07:07.506249 11101 sgd_solver.cpp:136] Iteration 190100, lr = 0.0405938, m = 0.9
I0815 04:07:21.984498 11101 solver.cpp:312] Iteration 190200 (6.90706 iter/s, 14.4779s/100 iter), loss = 1.69487
I0815 04:07:21.984567 11101 solver.cpp:334]     Train net output #0: loss = 1.4894 (* 1 = 1.4894 loss)
I0815 04:07:21.984588 11101 sgd_solver.cpp:136] Iteration 190200, lr = 0.0405625, m = 0.9
I0815 04:07:36.705979 11101 solver.cpp:312] Iteration 190300 (6.79299 iter/s, 14.7211s/100 iter), loss = 1.76138
I0815 04:07:36.706107 11101 solver.cpp:334]     Train net output #0: loss = 1.55629 (* 1 = 1.55629 loss)
I0815 04:07:36.706133 11101 sgd_solver.cpp:136] Iteration 190300, lr = 0.0405312, m = 0.9
I0815 04:07:51.090807 11101 solver.cpp:312] Iteration 190400 (6.95196 iter/s, 14.3844s/100 iter), loss = 1.981
I0815 04:07:51.090878 11101 solver.cpp:334]     Train net output #0: loss = 1.96272 (* 1 = 1.96272 loss)
I0815 04:07:51.090898 11101 sgd_solver.cpp:136] Iteration 190400, lr = 0.0405, m = 0.9
I0815 04:08:05.577011 11101 solver.cpp:312] Iteration 190500 (6.90331 iter/s, 14.4858s/100 iter), loss = 2.06449
I0815 04:08:05.577239 11101 solver.cpp:334]     Train net output #0: loss = 2.06233 (* 1 = 2.06233 loss)
I0815 04:08:05.577352 11101 sgd_solver.cpp:136] Iteration 190500, lr = 0.0404688, m = 0.9
I0815 04:08:20.226306 11101 solver.cpp:312] Iteration 190600 (6.82646 iter/s, 14.6489s/100 iter), loss = 1.64161
I0815 04:08:20.226383 11101 solver.cpp:334]     Train net output #0: loss = 1.55469 (* 1 = 1.55469 loss)
I0815 04:08:20.226390 11101 sgd_solver.cpp:136] Iteration 190600, lr = 0.0404375, m = 0.9
I0815 04:08:34.774230 11101 solver.cpp:312] Iteration 190700 (6.87402 iter/s, 14.5475s/100 iter), loss = 1.94713
I0815 04:08:34.774257 11101 solver.cpp:334]     Train net output #0: loss = 2.14748 (* 1 = 2.14748 loss)
I0815 04:08:34.774265 11101 sgd_solver.cpp:136] Iteration 190700, lr = 0.0404063, m = 0.9
I0815 04:08:49.332029 11101 solver.cpp:312] Iteration 190800 (6.86936 iter/s, 14.5574s/100 iter), loss = 1.75361
I0815 04:08:49.332054 11101 solver.cpp:334]     Train net output #0: loss = 1.79388 (* 1 = 1.79388 loss)
I0815 04:08:49.332059 11101 sgd_solver.cpp:136] Iteration 190800, lr = 0.040375, m = 0.9
I0815 04:09:04.077419 11101 solver.cpp:312] Iteration 190900 (6.78197 iter/s, 14.745s/100 iter), loss = 1.77238
I0815 04:09:04.077658 11101 solver.cpp:334]     Train net output #0: loss = 1.97855 (* 1 = 1.97855 loss)
I0815 04:09:04.077767 11101 sgd_solver.cpp:136] Iteration 190900, lr = 0.0403438, m = 0.9
I0815 04:09:18.980379 11101 solver.cpp:312] Iteration 191000 (6.71026 iter/s, 14.9025s/100 iter), loss = 1.89576
I0815 04:09:18.980406 11101 solver.cpp:334]     Train net output #0: loss = 1.61613 (* 1 = 1.61613 loss)
I0815 04:09:18.980412 11101 sgd_solver.cpp:136] Iteration 191000, lr = 0.0403125, m = 0.9
I0815 04:09:33.461779 11101 solver.cpp:312] Iteration 191100 (6.9056 iter/s, 14.481s/100 iter), loss = 1.97585
I0815 04:09:33.461850 11101 solver.cpp:334]     Train net output #0: loss = 1.79685 (* 1 = 1.79685 loss)
I0815 04:09:33.461870 11101 sgd_solver.cpp:136] Iteration 191100, lr = 0.0402812, m = 0.9
I0815 04:09:47.939205 11101 solver.cpp:312] Iteration 191200 (6.9075 iter/s, 14.477s/100 iter), loss = 2.03129
I0815 04:09:47.939299 11101 solver.cpp:334]     Train net output #0: loss = 1.82856 (* 1 = 1.82856 loss)
I0815 04:09:47.939321 11101 sgd_solver.cpp:136] Iteration 191200, lr = 0.04025, m = 0.9
I0815 04:10:01.136914 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 04:10:02.622550 11101 solver.cpp:312] Iteration 191300 (6.81063 iter/s, 14.6829s/100 iter), loss = 2.03887
I0815 04:10:02.622576 11101 solver.cpp:334]     Train net output #0: loss = 1.7886 (* 1 = 1.7886 loss)
I0815 04:10:02.622582 11101 sgd_solver.cpp:136] Iteration 191300, lr = 0.0402188, m = 0.9
I0815 04:10:17.054404 11101 solver.cpp:312] Iteration 191400 (6.92931 iter/s, 14.4315s/100 iter), loss = 1.53767
I0815 04:10:17.054431 11101 solver.cpp:334]     Train net output #0: loss = 1.87332 (* 1 = 1.87332 loss)
I0815 04:10:17.054437 11101 sgd_solver.cpp:136] Iteration 191400, lr = 0.0401875, m = 0.9
I0815 04:10:31.739814 11101 solver.cpp:312] Iteration 191500 (6.80967 iter/s, 14.685s/100 iter), loss = 1.56454
I0815 04:10:31.739886 11101 solver.cpp:334]     Train net output #0: loss = 1.70457 (* 1 = 1.70457 loss)
I0815 04:10:31.739893 11101 sgd_solver.cpp:136] Iteration 191500, lr = 0.0401563, m = 0.9
I0815 04:10:46.596354 11101 solver.cpp:312] Iteration 191600 (6.73123 iter/s, 14.8561s/100 iter), loss = 1.74662
I0815 04:10:46.596408 11101 solver.cpp:334]     Train net output #0: loss = 1.67148 (* 1 = 1.67148 loss)
I0815 04:10:46.596421 11101 sgd_solver.cpp:136] Iteration 191600, lr = 0.040125, m = 0.9
I0815 04:11:01.167068 11101 solver.cpp:312] Iteration 191700 (6.86327 iter/s, 14.5703s/100 iter), loss = 1.83107
I0815 04:11:01.167098 11101 solver.cpp:334]     Train net output #0: loss = 2.04265 (* 1 = 2.04265 loss)
I0815 04:11:01.167104 11101 sgd_solver.cpp:136] Iteration 191700, lr = 0.0400937, m = 0.9
I0815 04:11:15.644985 11101 solver.cpp:312] Iteration 191800 (6.90726 iter/s, 14.4775s/100 iter), loss = 1.57622
I0815 04:11:15.645059 11101 solver.cpp:334]     Train net output #0: loss = 1.50939 (* 1 = 1.50939 loss)
I0815 04:11:15.645073 11101 sgd_solver.cpp:136] Iteration 191800, lr = 0.0400625, m = 0.9
I0815 04:11:30.135031 11101 solver.cpp:312] Iteration 191900 (6.90148 iter/s, 14.4896s/100 iter), loss = 1.90026
I0815 04:11:30.135097 11101 solver.cpp:334]     Train net output #0: loss = 1.72991 (* 1 = 1.72991 loss)
I0815 04:11:30.135113 11101 sgd_solver.cpp:136] Iteration 191900, lr = 0.0400313, m = 0.9
I0815 04:11:44.740157 11101 solver.cpp:509] Iteration 192000, Testing net (#0)
I0815 04:12:05.639206 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.461588
I0815 04:12:05.639271 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.721234
I0815 04:12:05.639276 11101 solver.cpp:594]     Test net output #2: loss = 2.43162 (* 1 = 2.43162 loss)
I0815 04:12:05.639330 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9012s
I0815 04:12:05.800652 11101 solver.cpp:312] Iteration 192000 (2.8039 iter/s, 35.6646s/100 iter), loss = 1.56059
I0815 04:12:05.800724 11101 solver.cpp:334]     Train net output #0: loss = 1.5062 (* 1 = 1.5062 loss)
I0815 04:12:05.800742 11101 sgd_solver.cpp:136] Iteration 192000, lr = 0.04, m = 0.9
I0815 04:12:20.335029 11101 solver.cpp:312] Iteration 192100 (6.88043 iter/s, 14.534s/100 iter), loss = 1.49596
I0815 04:12:20.335053 11101 solver.cpp:334]     Train net output #0: loss = 1.63583 (* 1 = 1.63583 loss)
I0815 04:12:20.335058 11101 sgd_solver.cpp:136] Iteration 192100, lr = 0.0399688, m = 0.9
I0815 04:12:34.817529 11101 solver.cpp:312] Iteration 192200 (6.90508 iter/s, 14.4821s/100 iter), loss = 2.12164
I0815 04:12:34.817556 11101 solver.cpp:334]     Train net output #0: loss = 2.21649 (* 1 = 2.21649 loss)
I0815 04:12:34.817564 11101 sgd_solver.cpp:136] Iteration 192200, lr = 0.0399375, m = 0.9
I0815 04:12:49.340515 11101 solver.cpp:312] Iteration 192300 (6.88583 iter/s, 14.5226s/100 iter), loss = 1.83203
I0815 04:12:49.340732 11101 solver.cpp:334]     Train net output #0: loss = 2.02342 (* 1 = 2.02342 loss)
I0815 04:12:49.340829 11101 sgd_solver.cpp:136] Iteration 192300, lr = 0.0399063, m = 0.9
I0815 04:13:03.807510 11101 solver.cpp:312] Iteration 192400 (6.91248 iter/s, 14.4666s/100 iter), loss = 1.76796
I0815 04:13:03.807539 11101 solver.cpp:334]     Train net output #0: loss = 1.83054 (* 1 = 1.83054 loss)
I0815 04:13:03.807545 11101 sgd_solver.cpp:136] Iteration 192400, lr = 0.039875, m = 0.9
I0815 04:13:18.534143 11101 solver.cpp:312] Iteration 192500 (6.79061 iter/s, 14.7262s/100 iter), loss = 1.6617
I0815 04:13:18.534168 11101 solver.cpp:334]     Train net output #0: loss = 1.57071 (* 1 = 1.57071 loss)
I0815 04:13:18.534174 11101 sgd_solver.cpp:136] Iteration 192500, lr = 0.0398437, m = 0.9
I0815 04:13:32.947607 11101 solver.cpp:312] Iteration 192600 (6.93815 iter/s, 14.4131s/100 iter), loss = 1.65181
I0815 04:13:32.947670 11101 solver.cpp:334]     Train net output #0: loss = 1.77389 (* 1 = 1.77389 loss)
I0815 04:13:32.947679 11101 sgd_solver.cpp:136] Iteration 192600, lr = 0.0398125, m = 0.9
I0815 04:13:47.480629 11101 solver.cpp:312] Iteration 192700 (6.88107 iter/s, 14.5326s/100 iter), loss = 1.95738
I0815 04:13:47.480657 11101 solver.cpp:334]     Train net output #0: loss = 1.63949 (* 1 = 1.63949 loss)
I0815 04:13:47.480664 11101 sgd_solver.cpp:136] Iteration 192700, lr = 0.0397813, m = 0.9
I0815 04:14:02.011915 11101 solver.cpp:312] Iteration 192800 (6.88189 iter/s, 14.5309s/100 iter), loss = 1.60964
I0815 04:14:02.011986 11101 solver.cpp:334]     Train net output #0: loss = 2.00796 (* 1 = 2.00796 loss)
I0815 04:14:02.012007 11101 sgd_solver.cpp:136] Iteration 192800, lr = 0.03975, m = 0.9
I0815 04:14:16.294862 11101 solver.cpp:312] Iteration 192900 (7.00155 iter/s, 14.2826s/100 iter), loss = 1.70752
I0815 04:14:16.294922 11101 solver.cpp:334]     Train net output #0: loss = 1.42164 (* 1 = 1.42164 loss)
I0815 04:14:16.294929 11101 sgd_solver.cpp:136] Iteration 192900, lr = 0.0397187, m = 0.9
I0815 04:14:31.030236 11101 solver.cpp:312] Iteration 193000 (6.78658 iter/s, 14.735s/100 iter), loss = 1.74149
I0815 04:14:31.030309 11101 solver.cpp:334]     Train net output #0: loss = 1.6631 (* 1 = 1.6631 loss)
I0815 04:14:31.030330 11101 sgd_solver.cpp:136] Iteration 193000, lr = 0.0396875, m = 0.9
I0815 04:14:46.012459 11101 solver.cpp:312] Iteration 193100 (6.67476 iter/s, 14.9818s/100 iter), loss = 2.24714
I0815 04:14:46.012517 11101 solver.cpp:334]     Train net output #0: loss = 2.19492 (* 1 = 2.19492 loss)
I0815 04:14:46.012529 11101 sgd_solver.cpp:136] Iteration 193100, lr = 0.0396563, m = 0.9
I0815 04:15:00.512995 11101 solver.cpp:312] Iteration 193200 (6.89649 iter/s, 14.5001s/100 iter), loss = 1.6247
I0815 04:15:00.513059 11101 solver.cpp:334]     Train net output #0: loss = 1.78516 (* 1 = 1.78516 loss)
I0815 04:15:00.513065 11101 sgd_solver.cpp:136] Iteration 193200, lr = 0.039625, m = 0.9
I0815 04:15:15.213250 11101 solver.cpp:312] Iteration 193300 (6.80279 iter/s, 14.6999s/100 iter), loss = 1.93241
I0815 04:15:15.213276 11101 solver.cpp:334]     Train net output #0: loss = 1.96451 (* 1 = 1.96451 loss)
I0815 04:15:15.213282 11101 sgd_solver.cpp:136] Iteration 193300, lr = 0.0395938, m = 0.9
I0815 04:15:29.755913 11101 solver.cpp:312] Iteration 193400 (6.87651 iter/s, 14.5423s/100 iter), loss = 1.94525
I0815 04:15:29.755937 11101 solver.cpp:334]     Train net output #0: loss = 2.14414 (* 1 = 2.14414 loss)
I0815 04:15:29.755942 11101 sgd_solver.cpp:136] Iteration 193400, lr = 0.0395625, m = 0.9
I0815 04:15:44.235808 11101 solver.cpp:312] Iteration 193500 (6.90632 iter/s, 14.4795s/100 iter), loss = 1.63187
I0815 04:15:44.235863 11101 solver.cpp:334]     Train net output #0: loss = 1.88419 (* 1 = 1.88419 loss)
I0815 04:15:44.235869 11101 sgd_solver.cpp:136] Iteration 193500, lr = 0.0395312, m = 0.9
I0815 04:15:59.096544 11101 solver.cpp:312] Iteration 193600 (6.72933 iter/s, 14.8603s/100 iter), loss = 1.97102
I0815 04:15:59.096576 11101 solver.cpp:334]     Train net output #0: loss = 2.09345 (* 1 = 2.09345 loss)
I0815 04:15:59.096582 11101 sgd_solver.cpp:136] Iteration 193600, lr = 0.0395, m = 0.9
I0815 04:16:14.023742 11101 solver.cpp:312] Iteration 193700 (6.69936 iter/s, 14.9268s/100 iter), loss = 1.6343
I0815 04:16:14.023767 11101 solver.cpp:334]     Train net output #0: loss = 1.75885 (* 1 = 1.75885 loss)
I0815 04:16:14.023773 11101 sgd_solver.cpp:136] Iteration 193700, lr = 0.0394687, m = 0.9
I0815 04:16:28.712617 11101 solver.cpp:312] Iteration 193800 (6.80806 iter/s, 14.6885s/100 iter), loss = 1.77187
I0815 04:16:28.724177 11101 solver.cpp:334]     Train net output #0: loss = 1.87544 (* 1 = 1.87544 loss)
I0815 04:16:28.724195 11101 sgd_solver.cpp:136] Iteration 193800, lr = 0.0394375, m = 0.9
I0815 04:16:43.154459 11101 solver.cpp:312] Iteration 193900 (6.92451 iter/s, 14.4414s/100 iter), loss = 1.78118
I0815 04:16:43.154531 11101 solver.cpp:334]     Train net output #0: loss = 1.88065 (* 1 = 1.88065 loss)
I0815 04:16:43.154551 11101 sgd_solver.cpp:136] Iteration 193900, lr = 0.0394063, m = 0.9
I0815 04:16:57.483448 11101 solver.cpp:509] Iteration 194000, Testing net (#0)
I0815 04:17:18.495429 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.447765
I0815 04:17:18.495513 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.705646
I0815 04:17:18.495525 11101 solver.cpp:594]     Test net output #2: loss = 2.52766 (* 1 = 2.52766 loss)
I0815 04:17:18.495579 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0116s
I0815 04:17:18.653230 11101 solver.cpp:312] Iteration 194000 (2.81708 iter/s, 35.4978s/100 iter), loss = 1.89112
I0815 04:17:18.653261 11101 solver.cpp:334]     Train net output #0: loss = 2.25443 (* 1 = 2.25443 loss)
I0815 04:17:18.653267 11101 sgd_solver.cpp:136] Iteration 194000, lr = 0.039375, m = 0.9
I0815 04:17:33.243294 11101 solver.cpp:312] Iteration 194100 (6.85417 iter/s, 14.5897s/100 iter), loss = 1.64299
I0815 04:17:33.243345 11101 solver.cpp:334]     Train net output #0: loss = 1.31858 (* 1 = 1.31858 loss)
I0815 04:17:33.243358 11101 sgd_solver.cpp:136] Iteration 194100, lr = 0.0393438, m = 0.9
I0815 04:17:47.645238 11101 solver.cpp:312] Iteration 194200 (6.9437 iter/s, 14.4015s/100 iter), loss = 1.61449
I0815 04:17:47.645261 11101 solver.cpp:334]     Train net output #0: loss = 1.73049 (* 1 = 1.73049 loss)
I0815 04:17:47.645267 11101 sgd_solver.cpp:136] Iteration 194200, lr = 0.0393125, m = 0.9
I0815 04:18:02.373420 11101 solver.cpp:312] Iteration 194300 (6.78989 iter/s, 14.7278s/100 iter), loss = 1.89834
I0815 04:18:02.373646 11101 solver.cpp:334]     Train net output #0: loss = 2.06949 (* 1 = 2.06949 loss)
I0815 04:18:02.373739 11101 sgd_solver.cpp:136] Iteration 194300, lr = 0.0392812, m = 0.9
I0815 04:18:16.882360 11101 solver.cpp:312] Iteration 194400 (6.89249 iter/s, 14.5085s/100 iter), loss = 2.00411
I0815 04:18:16.882387 11101 solver.cpp:334]     Train net output #0: loss = 1.89222 (* 1 = 1.89222 loss)
I0815 04:18:16.882391 11101 sgd_solver.cpp:136] Iteration 194400, lr = 0.03925, m = 0.9
I0815 04:18:31.468593 11101 solver.cpp:312] Iteration 194500 (6.85597 iter/s, 14.5858s/100 iter), loss = 1.68999
I0815 04:18:31.468626 11101 solver.cpp:334]     Train net output #0: loss = 1.91911 (* 1 = 1.91911 loss)
I0815 04:18:31.468631 11101 sgd_solver.cpp:136] Iteration 194500, lr = 0.0392187, m = 0.9
I0815 04:18:46.083076 11101 solver.cpp:312] Iteration 194600 (6.84272 iter/s, 14.6141s/100 iter), loss = 1.8157
I0815 04:18:46.083133 11101 solver.cpp:334]     Train net output #0: loss = 1.8008 (* 1 = 1.8008 loss)
I0815 04:18:46.083138 11101 sgd_solver.cpp:136] Iteration 194600, lr = 0.0391875, m = 0.9
I0815 04:19:00.613826 11101 solver.cpp:312] Iteration 194700 (6.88215 iter/s, 14.5303s/100 iter), loss = 1.64926
I0815 04:19:00.613857 11101 solver.cpp:334]     Train net output #0: loss = 1.53398 (* 1 = 1.53398 loss)
I0815 04:19:00.613862 11101 sgd_solver.cpp:136] Iteration 194700, lr = 0.0391563, m = 0.9
I0815 04:19:15.140516 11101 solver.cpp:312] Iteration 194800 (6.88407 iter/s, 14.5263s/100 iter), loss = 2.15426
I0815 04:19:15.140671 11101 solver.cpp:334]     Train net output #0: loss = 2.59856 (* 1 = 2.59856 loss)
I0815 04:19:15.140691 11101 sgd_solver.cpp:136] Iteration 194800, lr = 0.039125, m = 0.9
I0815 04:19:29.834748 11101 solver.cpp:312] Iteration 194900 (6.80558 iter/s, 14.6938s/100 iter), loss = 2.41301
I0815 04:19:29.834820 11101 solver.cpp:334]     Train net output #0: loss = 2.3557 (* 1 = 2.3557 loss)
I0815 04:19:29.834830 11101 sgd_solver.cpp:136] Iteration 194900, lr = 0.0390938, m = 0.9
I0815 04:19:44.295588 11101 solver.cpp:312] Iteration 195000 (6.91542 iter/s, 14.4604s/100 iter), loss = 1.87493
I0815 04:19:44.295614 11101 solver.cpp:334]     Train net output #0: loss = 2.03293 (* 1 = 2.03293 loss)
I0815 04:19:44.295617 11101 sgd_solver.cpp:136] Iteration 195000, lr = 0.0390625, m = 0.9
I0815 04:19:59.015542 11101 solver.cpp:312] Iteration 195100 (6.79369 iter/s, 14.7195s/100 iter), loss = 1.90541
I0815 04:19:59.015570 11101 solver.cpp:334]     Train net output #0: loss = 1.76429 (* 1 = 1.76429 loss)
I0815 04:19:59.015576 11101 sgd_solver.cpp:136] Iteration 195100, lr = 0.0390312, m = 0.9
I0815 04:20:13.393756 11101 solver.cpp:312] Iteration 195200 (6.95516 iter/s, 14.3778s/100 iter), loss = 1.73348
I0815 04:20:13.393853 11101 solver.cpp:334]     Train net output #0: loss = 1.89006 (* 1 = 1.89006 loss)
I0815 04:20:13.393872 11101 sgd_solver.cpp:136] Iteration 195200, lr = 0.039, m = 0.9
I0815 04:20:28.214123 11101 solver.cpp:312] Iteration 195300 (6.74766 iter/s, 14.82s/100 iter), loss = 1.86524
I0815 04:20:28.214149 11101 solver.cpp:334]     Train net output #0: loss = 1.68954 (* 1 = 1.68954 loss)
I0815 04:20:28.214155 11101 sgd_solver.cpp:136] Iteration 195300, lr = 0.0389687, m = 0.9
I0815 04:20:42.808183 11101 solver.cpp:312] Iteration 195400 (6.85229 iter/s, 14.5937s/100 iter), loss = 1.8336
I0815 04:20:42.808257 11101 solver.cpp:334]     Train net output #0: loss = 1.79762 (* 1 = 1.79762 loss)
I0815 04:20:42.808279 11101 sgd_solver.cpp:136] Iteration 195400, lr = 0.0389375, m = 0.9
I0815 04:20:57.209025 11101 solver.cpp:312] Iteration 195500 (6.94423 iter/s, 14.4004s/100 iter), loss = 1.99665
I0815 04:20:57.209089 11101 solver.cpp:334]     Train net output #0: loss = 2.21746 (* 1 = 2.21746 loss)
I0815 04:20:57.209096 11101 sgd_solver.cpp:136] Iteration 195500, lr = 0.0389063, m = 0.9
I0815 04:21:11.628625 11101 solver.cpp:312] Iteration 195600 (6.9352 iter/s, 14.4192s/100 iter), loss = 2.13958
I0815 04:21:11.628690 11101 solver.cpp:334]     Train net output #0: loss = 2.016 (* 1 = 2.016 loss)
I0815 04:21:11.628712 11101 sgd_solver.cpp:136] Iteration 195600, lr = 0.038875, m = 0.9
I0815 04:21:26.206856 11101 solver.cpp:312] Iteration 195700 (6.85973 iter/s, 14.5778s/100 iter), loss = 1.93279
I0815 04:21:26.207010 11101 solver.cpp:334]     Train net output #0: loss = 1.86422 (* 1 = 1.86422 loss)
I0815 04:21:26.207029 11101 sgd_solver.cpp:136] Iteration 195700, lr = 0.0388438, m = 0.9
I0815 04:21:40.641242 11101 solver.cpp:312] Iteration 195800 (6.92809 iter/s, 14.434s/100 iter), loss = 1.89876
I0815 04:21:40.641302 11101 solver.cpp:334]     Train net output #0: loss = 1.59866 (* 1 = 1.59866 loss)
I0815 04:21:40.641309 11101 sgd_solver.cpp:136] Iteration 195800, lr = 0.0388125, m = 0.9
I0815 04:21:55.209254 11101 solver.cpp:312] Iteration 195900 (6.86454 iter/s, 14.5676s/100 iter), loss = 1.73133
I0815 04:21:55.209285 11101 solver.cpp:334]     Train net output #0: loss = 1.90096 (* 1 = 1.90096 loss)
I0815 04:21:55.209291 11101 sgd_solver.cpp:136] Iteration 195900, lr = 0.0387813, m = 0.9
I0815 04:22:09.581553 11101 solver.cpp:509] Iteration 196000, Testing net (#0)
I0815 04:22:30.584965 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.449177
I0815 04:22:30.585021 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.705528
I0815 04:22:30.585026 11101 solver.cpp:594]     Test net output #2: loss = 2.52187 (* 1 = 2.52187 loss)
I0815 04:22:30.585078 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.003s
I0815 04:22:30.761955 11101 solver.cpp:312] Iteration 196000 (2.8128 iter/s, 35.5517s/100 iter), loss = 1.81789
I0815 04:22:30.761988 11101 solver.cpp:334]     Train net output #0: loss = 1.85608 (* 1 = 1.85608 loss)
I0815 04:22:30.761996 11101 sgd_solver.cpp:136] Iteration 196000, lr = 0.03875, m = 0.9
I0815 04:22:44.240799 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 04:22:45.270556 11101 solver.cpp:312] Iteration 196100 (6.89265 iter/s, 14.5082s/100 iter), loss = 1.50258
I0815 04:22:45.270582 11101 solver.cpp:334]     Train net output #0: loss = 1.19408 (* 1 = 1.19408 loss)
I0815 04:22:45.270586 11101 sgd_solver.cpp:136] Iteration 196100, lr = 0.0387187, m = 0.9
I0815 04:22:59.964309 11101 solver.cpp:312] Iteration 196200 (6.8058 iter/s, 14.6933s/100 iter), loss = 1.91399
I0815 04:22:59.964340 11101 solver.cpp:334]     Train net output #0: loss = 1.73399 (* 1 = 1.73399 loss)
I0815 04:22:59.964347 11101 sgd_solver.cpp:136] Iteration 196200, lr = 0.0386875, m = 0.9
I0815 04:23:14.567528 11101 solver.cpp:312] Iteration 196300 (6.848 iter/s, 14.6028s/100 iter), loss = 2.0238
I0815 04:23:14.567589 11101 solver.cpp:334]     Train net output #0: loss = 2.01184 (* 1 = 2.01184 loss)
I0815 04:23:14.567595 11101 sgd_solver.cpp:136] Iteration 196300, lr = 0.0386563, m = 0.9
I0815 04:23:29.400979 11101 solver.cpp:312] Iteration 196400 (6.74171 iter/s, 14.833s/100 iter), loss = 2.04804
I0815 04:23:29.401010 11101 solver.cpp:334]     Train net output #0: loss = 2.12643 (* 1 = 2.12643 loss)
I0815 04:23:29.401016 11101 sgd_solver.cpp:136] Iteration 196400, lr = 0.038625, m = 0.9
I0815 04:23:44.122037 11101 solver.cpp:312] Iteration 196500 (6.79318 iter/s, 14.7207s/100 iter), loss = 1.91907
I0815 04:23:44.122108 11101 solver.cpp:334]     Train net output #0: loss = 1.71793 (* 1 = 1.71793 loss)
I0815 04:23:44.122128 11101 sgd_solver.cpp:136] Iteration 196500, lr = 0.0385938, m = 0.9
I0815 04:23:58.478438 11101 solver.cpp:312] Iteration 196600 (6.96573 iter/s, 14.356s/100 iter), loss = 1.67102
I0815 04:23:58.478494 11101 solver.cpp:334]     Train net output #0: loss = 1.52899 (* 1 = 1.52899 loss)
I0815 04:23:58.478500 11101 sgd_solver.cpp:136] Iteration 196600, lr = 0.0385625, m = 0.9
I0815 04:24:13.126562 11101 solver.cpp:312] Iteration 196700 (6.827 iter/s, 14.6477s/100 iter), loss = 1.90583
I0815 04:24:13.126687 11101 solver.cpp:334]     Train net output #0: loss = 2.14257 (* 1 = 2.14257 loss)
I0815 04:24:13.126700 11101 sgd_solver.cpp:136] Iteration 196700, lr = 0.0385313, m = 0.9
I0815 04:24:27.871479 11101 solver.cpp:312] Iteration 196800 (6.78218 iter/s, 14.7445s/100 iter), loss = 1.87223
I0815 04:24:27.871544 11101 solver.cpp:334]     Train net output #0: loss = 1.49892 (* 1 = 1.49892 loss)
I0815 04:24:27.871562 11101 sgd_solver.cpp:136] Iteration 196800, lr = 0.0385, m = 0.9
I0815 04:24:42.252197 11101 solver.cpp:312] Iteration 196900 (6.95395 iter/s, 14.3803s/100 iter), loss = 1.88243
I0815 04:24:42.252264 11101 solver.cpp:334]     Train net output #0: loss = 1.46682 (* 1 = 1.46682 loss)
I0815 04:24:42.252358 11101 sgd_solver.cpp:136] Iteration 196900, lr = 0.0384687, m = 0.9
I0815 04:24:56.787544 11101 solver.cpp:312] Iteration 197000 (6.87997 iter/s, 14.5349s/100 iter), loss = 1.66626
I0815 04:24:56.787572 11101 solver.cpp:334]     Train net output #0: loss = 1.45772 (* 1 = 1.45772 loss)
I0815 04:24:56.787578 11101 sgd_solver.cpp:136] Iteration 197000, lr = 0.0384375, m = 0.9
I0815 04:25:11.406899 11101 solver.cpp:312] Iteration 197100 (6.84043 iter/s, 14.619s/100 iter), loss = 1.69487
I0815 04:25:11.406973 11101 solver.cpp:334]     Train net output #0: loss = 1.76731 (* 1 = 1.76731 loss)
I0815 04:25:11.406992 11101 sgd_solver.cpp:136] Iteration 197100, lr = 0.0384063, m = 0.9
I0815 04:25:25.973641 11101 solver.cpp:312] Iteration 197200 (6.86514 iter/s, 14.5663s/100 iter), loss = 1.63301
I0815 04:25:25.973709 11101 solver.cpp:334]     Train net output #0: loss = 1.76211 (* 1 = 1.76211 loss)
I0815 04:25:25.973716 11101 sgd_solver.cpp:136] Iteration 197200, lr = 0.038375, m = 0.9
I0815 04:25:40.378891 11101 solver.cpp:312] Iteration 197300 (6.9421 iter/s, 14.4049s/100 iter), loss = 2.066
I0815 04:25:40.378914 11101 solver.cpp:334]     Train net output #0: loss = 1.9166 (* 1 = 1.9166 loss)
I0815 04:25:40.378918 11101 sgd_solver.cpp:136] Iteration 197300, lr = 0.0383438, m = 0.9
I0815 04:25:54.984583 11101 solver.cpp:312] Iteration 197400 (6.84683 iter/s, 14.6053s/100 iter), loss = 1.98591
I0815 04:25:54.988150 11101 solver.cpp:334]     Train net output #0: loss = 2.1642 (* 1 = 2.1642 loss)
I0815 04:25:54.988178 11101 sgd_solver.cpp:136] Iteration 197400, lr = 0.0383125, m = 0.9
I0815 04:26:09.622018 11101 solver.cpp:312] Iteration 197500 (6.83199 iter/s, 14.637s/100 iter), loss = 1.66485
I0815 04:26:09.622069 11101 solver.cpp:334]     Train net output #0: loss = 1.7054 (* 1 = 1.7054 loss)
I0815 04:26:09.622076 11101 sgd_solver.cpp:136] Iteration 197500, lr = 0.0382813, m = 0.9
I0815 04:26:24.447871 11101 solver.cpp:312] Iteration 197600 (6.74516 iter/s, 14.8254s/100 iter), loss = 2.16531
I0815 04:26:24.447902 11101 solver.cpp:334]     Train net output #0: loss = 1.9524 (* 1 = 1.9524 loss)
I0815 04:26:24.447907 11101 sgd_solver.cpp:136] Iteration 197600, lr = 0.03825, m = 0.9
I0815 04:26:38.809226 11101 solver.cpp:312] Iteration 197700 (6.96332 iter/s, 14.361s/100 iter), loss = 1.71939
I0815 04:26:38.809253 11101 solver.cpp:334]     Train net output #0: loss = 1.41029 (* 1 = 1.41029 loss)
I0815 04:26:38.809257 11101 sgd_solver.cpp:136] Iteration 197700, lr = 0.0382187, m = 0.9
I0815 04:26:53.400179 11101 solver.cpp:312] Iteration 197800 (6.85375 iter/s, 14.5906s/100 iter), loss = 1.84586
I0815 04:26:53.400269 11101 solver.cpp:334]     Train net output #0: loss = 1.7835 (* 1 = 1.7835 loss)
I0815 04:26:53.400286 11101 sgd_solver.cpp:136] Iteration 197800, lr = 0.0381875, m = 0.9
I0815 04:27:08.030776 11101 solver.cpp:312] Iteration 197900 (6.83518 iter/s, 14.6302s/100 iter), loss = 1.78181
I0815 04:27:08.030799 11101 solver.cpp:334]     Train net output #0: loss = 1.7065 (* 1 = 1.7065 loss)
I0815 04:27:08.030803 11101 sgd_solver.cpp:136] Iteration 197900, lr = 0.0381562, m = 0.9
I0815 04:27:22.409785 11101 solver.cpp:509] Iteration 198000, Testing net (#0)
I0815 04:27:43.291143 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.474294
I0815 04:27:43.291188 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.727234
I0815 04:27:43.291196 11101 solver.cpp:594]     Test net output #2: loss = 2.36951 (* 1 = 2.36951 loss)
I0815 04:27:43.291218 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8809s
I0815 04:27:43.460788 11101 solver.cpp:312] Iteration 198000 (2.82254 iter/s, 35.4291s/100 iter), loss = 1.70334
I0815 04:27:43.460816 11101 solver.cpp:334]     Train net output #0: loss = 1.39247 (* 1 = 1.39247 loss)
I0815 04:27:43.460822 11101 sgd_solver.cpp:136] Iteration 198000, lr = 0.038125, m = 0.9
I0815 04:27:57.795197 11101 solver.cpp:312] Iteration 198100 (6.97642 iter/s, 14.334s/100 iter), loss = 2.11547
I0815 04:27:57.798785 11101 solver.cpp:334]     Train net output #0: loss = 1.54965 (* 1 = 1.54965 loss)
I0815 04:27:57.800144 11101 sgd_solver.cpp:136] Iteration 198100, lr = 0.0380938, m = 0.9
I0815 04:28:12.178418 11101 solver.cpp:312] Iteration 198200 (6.95274 iter/s, 14.3828s/100 iter), loss = 1.77728
I0815 04:28:12.178442 11101 solver.cpp:334]     Train net output #0: loss = 1.91621 (* 1 = 1.91621 loss)
I0815 04:28:12.178448 11101 sgd_solver.cpp:136] Iteration 198200, lr = 0.0380625, m = 0.9
I0815 04:28:26.908486 11101 solver.cpp:312] Iteration 198300 (6.78902 iter/s, 14.7297s/100 iter), loss = 1.97187
I0815 04:28:26.908582 11101 solver.cpp:334]     Train net output #0: loss = 1.83426 (* 1 = 1.83426 loss)
I0815 04:28:26.908596 11101 sgd_solver.cpp:136] Iteration 198300, lr = 0.0380313, m = 0.9
I0815 04:28:41.769665 11101 solver.cpp:312] Iteration 198400 (6.72913 iter/s, 14.8608s/100 iter), loss = 1.63474
I0815 04:28:41.769716 11101 solver.cpp:334]     Train net output #0: loss = 1.40144 (* 1 = 1.40144 loss)
I0815 04:28:41.769728 11101 sgd_solver.cpp:136] Iteration 198400, lr = 0.038, m = 0.9
I0815 04:28:56.701331 11101 solver.cpp:312] Iteration 198500 (6.69736 iter/s, 14.9313s/100 iter), loss = 1.92249
I0815 04:28:56.701359 11101 solver.cpp:334]     Train net output #0: loss = 1.58291 (* 1 = 1.58291 loss)
I0815 04:28:56.701362 11101 sgd_solver.cpp:136] Iteration 198500, lr = 0.0379688, m = 0.9
I0815 04:29:11.217620 11101 solver.cpp:312] Iteration 198600 (6.889 iter/s, 14.5159s/100 iter), loss = 1.8942
I0815 04:29:11.217685 11101 solver.cpp:334]     Train net output #0: loss = 1.91442 (* 1 = 1.91442 loss)
I0815 04:29:11.217691 11101 sgd_solver.cpp:136] Iteration 198600, lr = 0.0379375, m = 0.9
I0815 04:29:25.821107 11101 solver.cpp:312] Iteration 198700 (6.84787 iter/s, 14.6031s/100 iter), loss = 1.45934
I0815 04:29:25.821132 11101 solver.cpp:334]     Train net output #0: loss = 1.4395 (* 1 = 1.4395 loss)
I0815 04:29:25.821138 11101 sgd_solver.cpp:136] Iteration 198700, lr = 0.0379062, m = 0.9
I0815 04:29:40.653527 11101 solver.cpp:312] Iteration 198800 (6.74217 iter/s, 14.832s/100 iter), loss = 1.92282
I0815 04:29:40.653556 11101 solver.cpp:334]     Train net output #0: loss = 2.0394 (* 1 = 2.0394 loss)
I0815 04:29:40.653563 11101 sgd_solver.cpp:136] Iteration 198800, lr = 0.037875, m = 0.9
I0815 04:29:55.624207 11101 solver.cpp:312] Iteration 198900 (6.67991 iter/s, 14.9703s/100 iter), loss = 1.86774
I0815 04:29:55.624302 11101 solver.cpp:334]     Train net output #0: loss = 1.87753 (* 1 = 1.87753 loss)
I0815 04:29:55.624308 11101 sgd_solver.cpp:136] Iteration 198900, lr = 0.0378438, m = 0.9
I0815 04:30:10.366448 11101 solver.cpp:312] Iteration 199000 (6.78342 iter/s, 14.7418s/100 iter), loss = 2.07343
I0815 04:30:10.366477 11101 solver.cpp:334]     Train net output #0: loss = 2.15937 (* 1 = 2.15937 loss)
I0815 04:30:10.366483 11101 sgd_solver.cpp:136] Iteration 199000, lr = 0.0378125, m = 0.9
I0815 04:30:24.810075 11101 solver.cpp:312] Iteration 199100 (6.92366 iter/s, 14.4432s/100 iter), loss = 1.75855
I0815 04:30:24.810099 11101 solver.cpp:334]     Train net output #0: loss = 1.70106 (* 1 = 1.70106 loss)
I0815 04:30:24.810104 11101 sgd_solver.cpp:136] Iteration 199100, lr = 0.0377812, m = 0.9
I0815 04:30:39.334702 11101 solver.cpp:312] Iteration 199200 (6.88505 iter/s, 14.5242s/100 iter), loss = 1.69596
I0815 04:30:39.334764 11101 solver.cpp:334]     Train net output #0: loss = 1.63718 (* 1 = 1.63718 loss)
I0815 04:30:39.334769 11101 sgd_solver.cpp:136] Iteration 199200, lr = 0.03775, m = 0.9
I0815 04:30:53.750643 11101 solver.cpp:312] Iteration 199300 (6.93696 iter/s, 14.4155s/100 iter), loss = 1.90159
I0815 04:30:53.750666 11101 solver.cpp:334]     Train net output #0: loss = 1.9151 (* 1 = 1.9151 loss)
I0815 04:30:53.750672 11101 sgd_solver.cpp:136] Iteration 199300, lr = 0.0377188, m = 0.9
I0815 04:31:08.317193 11101 solver.cpp:312] Iteration 199400 (6.86523 iter/s, 14.5661s/100 iter), loss = 1.76271
I0815 04:31:08.317221 11101 solver.cpp:334]     Train net output #0: loss = 1.86328 (* 1 = 1.86328 loss)
I0815 04:31:08.317227 11101 sgd_solver.cpp:136] Iteration 199400, lr = 0.0376875, m = 0.9
I0815 04:31:22.937099 11101 solver.cpp:312] Iteration 199500 (6.84018 iter/s, 14.6195s/100 iter), loss = 1.8992
I0815 04:31:22.937191 11101 solver.cpp:334]     Train net output #0: loss = 2.08252 (* 1 = 2.08252 loss)
I0815 04:31:22.937203 11101 sgd_solver.cpp:136] Iteration 199500, lr = 0.0376562, m = 0.9
I0815 04:31:37.450639 11101 solver.cpp:312] Iteration 199600 (6.89031 iter/s, 14.5131s/100 iter), loss = 2.08492
I0815 04:31:37.450690 11101 solver.cpp:334]     Train net output #0: loss = 2.39894 (* 1 = 2.39894 loss)
I0815 04:31:37.450702 11101 sgd_solver.cpp:136] Iteration 199600, lr = 0.037625, m = 0.9
I0815 04:31:52.182484 11101 solver.cpp:312] Iteration 199700 (6.7882 iter/s, 14.7314s/100 iter), loss = 1.82856
I0815 04:31:52.182510 11101 solver.cpp:334]     Train net output #0: loss = 1.87506 (* 1 = 1.87506 loss)
I0815 04:31:52.182516 11101 sgd_solver.cpp:136] Iteration 199700, lr = 0.0375938, m = 0.9
I0815 04:32:06.932947 11101 solver.cpp:312] Iteration 199800 (6.77964 iter/s, 14.7501s/100 iter), loss = 1.84293
I0815 04:32:06.933045 11101 solver.cpp:334]     Train net output #0: loss = 1.75614 (* 1 = 1.75614 loss)
I0815 04:32:06.933063 11101 sgd_solver.cpp:136] Iteration 199800, lr = 0.0375625, m = 0.9
I0815 04:32:21.725767 11101 solver.cpp:312] Iteration 199900 (6.76022 iter/s, 14.7924s/100 iter), loss = 1.67536
I0815 04:32:21.725791 11101 solver.cpp:334]     Train net output #0: loss = 1.80578 (* 1 = 1.80578 loss)
I0815 04:32:21.725796 11101 sgd_solver.cpp:136] Iteration 199900, lr = 0.0375313, m = 0.9
I0815 04:32:36.212005 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_200000.caffemodel
I0815 04:32:36.225029 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_200000.solverstate
I0815 04:32:36.229424 11101 solver.cpp:509] Iteration 200000, Testing net (#0)
I0815 04:32:36.574503 11099 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 04:32:57.110810 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.481529
I0815 04:32:57.110862 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.728763
I0815 04:32:57.110869 11101 solver.cpp:594]     Test net output #2: loss = 2.3673 (* 1 = 2.3673 loss)
I0815 04:32:57.110888 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8809s
I0815 04:32:57.271986 11101 solver.cpp:312] Iteration 200000 (2.81332 iter/s, 35.5452s/100 iter), loss = 1.59241
I0815 04:32:57.272013 11101 solver.cpp:334]     Train net output #0: loss = 1.57491 (* 1 = 1.57491 loss)
I0815 04:32:57.272017 11101 sgd_solver.cpp:136] Iteration 200000, lr = 0.0375, m = 0.9
I0815 04:33:12.055070 11101 solver.cpp:312] Iteration 200100 (6.76468 iter/s, 14.7827s/100 iter), loss = 1.91971
I0815 04:33:12.055115 11101 solver.cpp:334]     Train net output #0: loss = 1.81056 (* 1 = 1.81056 loss)
I0815 04:33:12.055133 11101 sgd_solver.cpp:136] Iteration 200100, lr = 0.0374688, m = 0.9
I0815 04:33:26.636919 11101 solver.cpp:312] Iteration 200200 (6.85803 iter/s, 14.5814s/100 iter), loss = 1.87316
I0815 04:33:26.636992 11101 solver.cpp:334]     Train net output #0: loss = 1.67328 (* 1 = 1.67328 loss)
I0815 04:33:26.637017 11101 sgd_solver.cpp:136] Iteration 200200, lr = 0.0374375, m = 0.9
I0815 04:33:41.124958 11101 solver.cpp:312] Iteration 200300 (6.90244 iter/s, 14.4876s/100 iter), loss = 1.89537
I0815 04:33:41.125063 11101 solver.cpp:334]     Train net output #0: loss = 1.84507 (* 1 = 1.84507 loss)
I0815 04:33:41.125084 11101 sgd_solver.cpp:136] Iteration 200300, lr = 0.0374062, m = 0.9
I0815 04:33:55.806340 11101 solver.cpp:312] Iteration 200400 (6.81154 iter/s, 14.681s/100 iter), loss = 1.75173
I0815 04:33:55.806368 11101 solver.cpp:334]     Train net output #0: loss = 2.14227 (* 1 = 2.14227 loss)
I0815 04:33:55.806375 11101 sgd_solver.cpp:136] Iteration 200400, lr = 0.037375, m = 0.9
I0815 04:34:10.192441 11101 solver.cpp:312] Iteration 200500 (6.95134 iter/s, 14.3857s/100 iter), loss = 1.80277
I0815 04:34:10.192469 11101 solver.cpp:334]     Train net output #0: loss = 1.52807 (* 1 = 1.52807 loss)
I0815 04:34:10.192476 11101 sgd_solver.cpp:136] Iteration 200500, lr = 0.0373438, m = 0.9
I0815 04:34:24.846626 11101 solver.cpp:312] Iteration 200600 (6.82418 iter/s, 14.6538s/100 iter), loss = 1.37644
I0815 04:34:24.846740 11101 solver.cpp:334]     Train net output #0: loss = 1.29484 (* 1 = 1.29484 loss)
I0815 04:34:24.846763 11101 sgd_solver.cpp:136] Iteration 200600, lr = 0.0373125, m = 0.9
I0815 04:34:39.555402 11101 solver.cpp:312] Iteration 200700 (6.79885 iter/s, 14.7084s/100 iter), loss = 1.8161
I0815 04:34:39.555472 11101 solver.cpp:334]     Train net output #0: loss = 2.19492 (* 1 = 2.19492 loss)
I0815 04:34:39.555495 11101 sgd_solver.cpp:136] Iteration 200700, lr = 0.0372813, m = 0.9
I0815 04:34:54.116531 11101 solver.cpp:312] Iteration 200800 (6.86779 iter/s, 14.5607s/100 iter), loss = 1.81479
I0815 04:34:54.116559 11101 solver.cpp:334]     Train net output #0: loss = 1.85791 (* 1 = 1.85791 loss)
I0815 04:34:54.116564 11101 sgd_solver.cpp:136] Iteration 200800, lr = 0.03725, m = 0.9
I0815 04:35:08.581125 11101 solver.cpp:312] Iteration 200900 (6.91362 iter/s, 14.4642s/100 iter), loss = 2.04619
I0815 04:35:08.581184 11101 solver.cpp:334]     Train net output #0: loss = 1.76041 (* 1 = 1.76041 loss)
I0815 04:35:08.581192 11101 sgd_solver.cpp:136] Iteration 200900, lr = 0.0372187, m = 0.9
I0815 04:35:23.134829 11101 solver.cpp:312] Iteration 201000 (6.87129 iter/s, 14.5533s/100 iter), loss = 1.63098
I0815 04:35:23.134897 11101 solver.cpp:334]     Train net output #0: loss = 1.15749 (* 1 = 1.15749 loss)
I0815 04:35:23.134914 11101 sgd_solver.cpp:136] Iteration 201000, lr = 0.0371875, m = 0.9
I0815 04:35:37.735658 11101 solver.cpp:312] Iteration 201100 (6.84912 iter/s, 14.6004s/100 iter), loss = 1.96759
I0815 04:35:37.735689 11101 solver.cpp:334]     Train net output #0: loss = 1.83068 (* 1 = 1.83068 loss)
I0815 04:35:37.735695 11101 sgd_solver.cpp:136] Iteration 201100, lr = 0.0371563, m = 0.9
I0815 04:35:52.113064 11101 solver.cpp:312] Iteration 201200 (6.95555 iter/s, 14.377s/100 iter), loss = 1.85805
I0815 04:35:52.113126 11101 solver.cpp:334]     Train net output #0: loss = 1.79041 (* 1 = 1.79041 loss)
I0815 04:35:52.113132 11101 sgd_solver.cpp:136] Iteration 201200, lr = 0.037125, m = 0.9
I0815 04:36:06.479180 11101 solver.cpp:312] Iteration 201300 (6.96102 iter/s, 14.3657s/100 iter), loss = 1.61505
I0815 04:36:06.479207 11101 solver.cpp:334]     Train net output #0: loss = 1.89544 (* 1 = 1.89544 loss)
I0815 04:36:06.479214 11101 sgd_solver.cpp:136] Iteration 201300, lr = 0.0370938, m = 0.9
I0815 04:36:21.012310 11101 solver.cpp:312] Iteration 201400 (6.88102 iter/s, 14.5327s/100 iter), loss = 1.90963
I0815 04:36:21.012379 11101 solver.cpp:334]     Train net output #0: loss = 2.30935 (* 1 = 2.30935 loss)
I0815 04:36:21.012398 11101 sgd_solver.cpp:136] Iteration 201400, lr = 0.0370625, m = 0.9
I0815 04:36:35.546385 11101 solver.cpp:312] Iteration 201500 (6.88057 iter/s, 14.5337s/100 iter), loss = 1.75674
I0815 04:36:35.546439 11101 solver.cpp:334]     Train net output #0: loss = 1.91665 (* 1 = 1.91665 loss)
I0815 04:36:35.548277 11101 sgd_solver.cpp:136] Iteration 201500, lr = 0.0370313, m = 0.9
I0815 04:36:50.124141 11101 solver.cpp:312] Iteration 201600 (6.85996 iter/s, 14.5774s/100 iter), loss = 1.50077
I0815 04:36:50.124168 11101 solver.cpp:334]     Train net output #0: loss = 1.60964 (* 1 = 1.60964 loss)
I0815 04:36:50.124174 11101 sgd_solver.cpp:136] Iteration 201600, lr = 0.037, m = 0.9
I0815 04:37:04.744709 11101 solver.cpp:312] Iteration 201700 (6.83987 iter/s, 14.6202s/100 iter), loss = 1.57798
I0815 04:37:04.744735 11101 solver.cpp:334]     Train net output #0: loss = 1.46318 (* 1 = 1.46318 loss)
I0815 04:37:04.744740 11101 sgd_solver.cpp:136] Iteration 201700, lr = 0.0369687, m = 0.9
I0815 04:37:19.459661 11101 solver.cpp:312] Iteration 201800 (6.79599 iter/s, 14.7145s/100 iter), loss = 2.44682
I0815 04:37:19.459758 11101 solver.cpp:334]     Train net output #0: loss = 2.77862 (* 1 = 2.77862 loss)
I0815 04:37:19.459776 11101 sgd_solver.cpp:136] Iteration 201800, lr = 0.0369375, m = 0.9
I0815 04:37:34.213858 11101 solver.cpp:312] Iteration 201900 (6.77792 iter/s, 14.7538s/100 iter), loss = 1.65581
I0815 04:37:34.213884 11101 solver.cpp:334]     Train net output #0: loss = 1.73877 (* 1 = 1.73877 loss)
I0815 04:37:34.213888 11101 sgd_solver.cpp:136] Iteration 201900, lr = 0.0369062, m = 0.9
I0815 04:37:48.575139 11101 solver.cpp:509] Iteration 202000, Testing net (#0)
I0815 04:38:06.392634 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 04:38:09.330901 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.468059
I0815 04:38:09.330927 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.723175
I0815 04:38:09.330935 11101 solver.cpp:594]     Test net output #2: loss = 2.40331 (* 1 = 2.40331 loss)
I0815 04:38:09.330967 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7553s
I0815 04:38:09.488474 11101 solver.cpp:312] Iteration 202000 (2.83498 iter/s, 35.2737s/100 iter), loss = 1.87759
I0815 04:38:09.488653 11101 solver.cpp:334]     Train net output #0: loss = 1.86449 (* 1 = 1.86449 loss)
I0815 04:38:09.488739 11101 sgd_solver.cpp:136] Iteration 202000, lr = 0.036875, m = 0.9
I0815 04:38:24.096765 11101 solver.cpp:312] Iteration 202100 (6.84562 iter/s, 14.6079s/100 iter), loss = 1.83784
I0815 04:38:24.096794 11101 solver.cpp:334]     Train net output #0: loss = 1.74006 (* 1 = 1.74006 loss)
I0815 04:38:24.096799 11101 sgd_solver.cpp:136] Iteration 202100, lr = 0.0368438, m = 0.9
I0815 04:38:38.488342 11101 solver.cpp:312] Iteration 202200 (6.9487 iter/s, 14.3912s/100 iter), loss = 1.98739
I0815 04:38:38.488401 11101 solver.cpp:334]     Train net output #0: loss = 1.79043 (* 1 = 1.79043 loss)
I0815 04:38:38.488409 11101 sgd_solver.cpp:136] Iteration 202200, lr = 0.0368125, m = 0.9
I0815 04:38:53.095515 11101 solver.cpp:312] Iteration 202300 (6.84614 iter/s, 14.6068s/100 iter), loss = 1.93615
I0815 04:38:53.095541 11101 solver.cpp:334]     Train net output #0: loss = 2.12464 (* 1 = 2.12464 loss)
I0815 04:38:53.095546 11101 sgd_solver.cpp:136] Iteration 202300, lr = 0.0367813, m = 0.9
I0815 04:39:07.594925 11101 solver.cpp:312] Iteration 202400 (6.89702 iter/s, 14.499s/100 iter), loss = 1.73105
I0815 04:39:07.594957 11101 solver.cpp:334]     Train net output #0: loss = 1.41332 (* 1 = 1.41332 loss)
I0815 04:39:07.594964 11101 sgd_solver.cpp:136] Iteration 202400, lr = 0.03675, m = 0.9
I0815 04:39:22.415918 11101 solver.cpp:312] Iteration 202500 (6.74737 iter/s, 14.8206s/100 iter), loss = 1.94396
I0815 04:39:22.415968 11101 solver.cpp:334]     Train net output #0: loss = 1.74898 (* 1 = 1.74898 loss)
I0815 04:39:22.415973 11101 sgd_solver.cpp:136] Iteration 202500, lr = 0.0367188, m = 0.9
I0815 04:39:36.889307 11101 solver.cpp:312] Iteration 202600 (6.90942 iter/s, 14.473s/100 iter), loss = 1.58359
I0815 04:39:36.889525 11101 solver.cpp:334]     Train net output #0: loss = 1.66429 (* 1 = 1.66429 loss)
I0815 04:39:36.889643 11101 sgd_solver.cpp:136] Iteration 202600, lr = 0.0366875, m = 0.9
I0815 04:39:51.438061 11101 solver.cpp:312] Iteration 202700 (6.87363 iter/s, 14.5484s/100 iter), loss = 1.95675
I0815 04:39:51.438130 11101 solver.cpp:334]     Train net output #0: loss = 1.88379 (* 1 = 1.88379 loss)
I0815 04:39:51.438155 11101 sgd_solver.cpp:136] Iteration 202700, lr = 0.0366562, m = 0.9
I0815 04:40:05.988518 11101 solver.cpp:312] Iteration 202800 (6.87283 iter/s, 14.5501s/100 iter), loss = 2.06254
I0815 04:40:05.988988 11101 solver.cpp:334]     Train net output #0: loss = 2.07997 (* 1 = 2.07997 loss)
I0815 04:40:05.989009 11101 sgd_solver.cpp:136] Iteration 202800, lr = 0.036625, m = 0.9
I0815 04:40:20.732524 11101 solver.cpp:312] Iteration 202900 (6.7826 iter/s, 14.7436s/100 iter), loss = 1.70787
I0815 04:40:20.732547 11101 solver.cpp:334]     Train net output #0: loss = 1.67556 (* 1 = 1.67556 loss)
I0815 04:40:20.732553 11101 sgd_solver.cpp:136] Iteration 202900, lr = 0.0365937, m = 0.9
I0815 04:40:34.999246 11101 solver.cpp:312] Iteration 203000 (7.00951 iter/s, 14.2663s/100 iter), loss = 1.47441
I0815 04:40:34.999418 11101 solver.cpp:334]     Train net output #0: loss = 1.70951 (* 1 = 1.70951 loss)
I0815 04:40:34.999536 11101 sgd_solver.cpp:136] Iteration 203000, lr = 0.0365625, m = 0.9
I0815 04:40:49.488931 11101 solver.cpp:312] Iteration 203100 (6.90165 iter/s, 14.4893s/100 iter), loss = 1.36355
I0815 04:40:49.489037 11101 solver.cpp:334]     Train net output #0: loss = 1.18016 (* 1 = 1.18016 loss)
I0815 04:40:49.489056 11101 sgd_solver.cpp:136] Iteration 203100, lr = 0.0365313, m = 0.9
I0815 04:41:03.963919 11101 solver.cpp:312] Iteration 203200 (6.90866 iter/s, 14.4746s/100 iter), loss = 1.84778
I0815 04:41:03.963994 11101 solver.cpp:334]     Train net output #0: loss = 2.11475 (* 1 = 2.11475 loss)
I0815 04:41:03.964013 11101 sgd_solver.cpp:136] Iteration 203200, lr = 0.0365, m = 0.9
I0815 04:41:18.444607 11101 solver.cpp:312] Iteration 203300 (6.90594 iter/s, 14.4803s/100 iter), loss = 1.88358
I0815 04:41:18.444633 11101 solver.cpp:334]     Train net output #0: loss = 1.92221 (* 1 = 1.92221 loss)
I0815 04:41:18.444638 11101 sgd_solver.cpp:136] Iteration 203300, lr = 0.0364688, m = 0.9
I0815 04:41:32.996537 11101 solver.cpp:312] Iteration 203400 (6.87213 iter/s, 14.5515s/100 iter), loss = 2.29092
I0815 04:41:32.996630 11101 solver.cpp:334]     Train net output #0: loss = 2.51462 (* 1 = 2.51462 loss)
I0815 04:41:32.996649 11101 sgd_solver.cpp:136] Iteration 203400, lr = 0.0364375, m = 0.9
I0815 04:41:47.498327 11101 solver.cpp:312] Iteration 203500 (6.89589 iter/s, 14.5014s/100 iter), loss = 2.00829
I0815 04:41:47.499111 11101 solver.cpp:334]     Train net output #0: loss = 2.2129 (* 1 = 2.2129 loss)
I0815 04:41:47.499133 11101 sgd_solver.cpp:136] Iteration 203500, lr = 0.0364062, m = 0.9
I0815 04:42:01.862383 11101 solver.cpp:312] Iteration 203600 (6.96201 iter/s, 14.3637s/100 iter), loss = 1.83959
I0815 04:42:01.862457 11101 solver.cpp:334]     Train net output #0: loss = 2.1474 (* 1 = 2.1474 loss)
I0815 04:42:01.862475 11101 sgd_solver.cpp:136] Iteration 203600, lr = 0.036375, m = 0.9
I0815 04:42:16.727804 11101 solver.cpp:312] Iteration 203700 (6.72721 iter/s, 14.865s/100 iter), loss = 2.17031
I0815 04:42:16.727898 11101 solver.cpp:334]     Train net output #0: loss = 2.33926 (* 1 = 2.33926 loss)
I0815 04:42:16.727918 11101 sgd_solver.cpp:136] Iteration 203700, lr = 0.0363437, m = 0.9
I0815 04:42:31.245054 11101 solver.cpp:312] Iteration 203800 (6.88855 iter/s, 14.5168s/100 iter), loss = 1.65677
I0815 04:42:31.245131 11101 solver.cpp:334]     Train net output #0: loss = 1.75086 (* 1 = 1.75086 loss)
I0815 04:42:31.245151 11101 sgd_solver.cpp:136] Iteration 203800, lr = 0.0363125, m = 0.9
I0815 04:42:45.986764 11101 solver.cpp:312] Iteration 203900 (6.78366 iter/s, 14.7413s/100 iter), loss = 1.61041
I0815 04:42:45.986794 11101 solver.cpp:334]     Train net output #0: loss = 1.2458 (* 1 = 1.2458 loss)
I0815 04:42:45.986801 11101 sgd_solver.cpp:136] Iteration 203900, lr = 0.0362813, m = 0.9
I0815 04:43:00.776077 11101 solver.cpp:509] Iteration 204000, Testing net (#0)
I0815 04:43:21.692142 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.460118
I0815 04:43:21.692164 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.71294
I0815 04:43:21.692169 11101 solver.cpp:594]     Test net output #2: loss = 2.4497 (* 1 = 2.4497 loss)
I0815 04:43:21.692190 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9155s
I0815 04:43:21.840993 11101 solver.cpp:312] Iteration 204000 (2.78915 iter/s, 35.8533s/100 iter), loss = 2.21602
I0815 04:43:21.841017 11101 solver.cpp:334]     Train net output #0: loss = 2.2542 (* 1 = 2.2542 loss)
I0815 04:43:21.841020 11101 sgd_solver.cpp:136] Iteration 204000, lr = 0.03625, m = 0.9
I0815 04:43:36.253260 11101 solver.cpp:312] Iteration 204100 (6.93873 iter/s, 14.4119s/100 iter), loss = 1.80731
I0815 04:43:36.253346 11101 solver.cpp:334]     Train net output #0: loss = 1.79248 (* 1 = 1.79248 loss)
I0815 04:43:36.253355 11101 sgd_solver.cpp:136] Iteration 204100, lr = 0.0362188, m = 0.9
I0815 04:43:50.912111 11101 solver.cpp:312] Iteration 204200 (6.82201 iter/s, 14.6584s/100 iter), loss = 1.83316
I0815 04:43:50.912145 11101 solver.cpp:334]     Train net output #0: loss = 1.64183 (* 1 = 1.64183 loss)
I0815 04:43:50.912149 11101 sgd_solver.cpp:136] Iteration 204200, lr = 0.0361875, m = 0.9
I0815 04:44:05.638336 11101 solver.cpp:312] Iteration 204300 (6.79079 iter/s, 14.7258s/100 iter), loss = 1.89666
I0815 04:44:05.638363 11101 solver.cpp:334]     Train net output #0: loss = 1.98374 (* 1 = 1.98374 loss)
I0815 04:44:05.638367 11101 sgd_solver.cpp:136] Iteration 204300, lr = 0.0361562, m = 0.9
I0815 04:44:20.360003 11101 solver.cpp:312] Iteration 204400 (6.7929 iter/s, 14.7213s/100 iter), loss = 1.69999
I0815 04:44:20.360047 11101 solver.cpp:334]     Train net output #0: loss = 1.74765 (* 1 = 1.74765 loss)
I0815 04:44:20.360052 11101 sgd_solver.cpp:136] Iteration 204400, lr = 0.036125, m = 0.9
I0815 04:44:34.803571 11101 solver.cpp:312] Iteration 204500 (6.92369 iter/s, 14.4432s/100 iter), loss = 2.02481
I0815 04:44:34.803597 11101 solver.cpp:334]     Train net output #0: loss = 1.8901 (* 1 = 1.8901 loss)
I0815 04:44:34.803603 11101 sgd_solver.cpp:136] Iteration 204500, lr = 0.0360937, m = 0.9
I0815 04:44:49.567720 11101 solver.cpp:312] Iteration 204600 (6.77335 iter/s, 14.7637s/100 iter), loss = 1.62353
I0815 04:44:49.567787 11101 solver.cpp:334]     Train net output #0: loss = 1.7803 (* 1 = 1.7803 loss)
I0815 04:44:49.567804 11101 sgd_solver.cpp:136] Iteration 204600, lr = 0.0360625, m = 0.9
I0815 04:45:03.990051 11101 solver.cpp:312] Iteration 204700 (6.93389 iter/s, 14.4219s/100 iter), loss = 2.0252
I0815 04:45:03.990160 11101 solver.cpp:334]     Train net output #0: loss = 2.13115 (* 1 = 2.13115 loss)
I0815 04:45:03.990177 11101 sgd_solver.cpp:136] Iteration 204700, lr = 0.0360313, m = 0.9
I0815 04:45:18.632622 11101 solver.cpp:312] Iteration 204800 (6.82959 iter/s, 14.6422s/100 iter), loss = 1.69635
I0815 04:45:18.632650 11101 solver.cpp:334]     Train net output #0: loss = 1.48092 (* 1 = 1.48092 loss)
I0815 04:45:18.632655 11101 sgd_solver.cpp:136] Iteration 204800, lr = 0.036, m = 0.9
I0815 04:45:33.308159 11101 solver.cpp:312] Iteration 204900 (6.81425 iter/s, 14.6751s/100 iter), loss = 2.20276
I0815 04:45:33.308223 11101 solver.cpp:334]     Train net output #0: loss = 2.39503 (* 1 = 2.39503 loss)
I0815 04:45:33.308240 11101 sgd_solver.cpp:136] Iteration 204900, lr = 0.0359688, m = 0.9
I0815 04:45:48.222651 11101 solver.cpp:312] Iteration 205000 (6.70507 iter/s, 14.9141s/100 iter), loss = 1.87387
I0815 04:45:48.222709 11101 solver.cpp:334]     Train net output #0: loss = 1.69898 (* 1 = 1.69898 loss)
I0815 04:45:48.222715 11101 sgd_solver.cpp:136] Iteration 205000, lr = 0.0359375, m = 0.9
I0815 04:46:02.713357 11101 solver.cpp:312] Iteration 205100 (6.90117 iter/s, 14.4903s/100 iter), loss = 1.90623
I0815 04:46:02.713383 11101 solver.cpp:334]     Train net output #0: loss = 1.92715 (* 1 = 1.92715 loss)
I0815 04:46:02.713387 11101 sgd_solver.cpp:136] Iteration 205100, lr = 0.0359063, m = 0.9
I0815 04:46:17.063025 11101 solver.cpp:312] Iteration 205200 (6.96899 iter/s, 14.3493s/100 iter), loss = 1.9985
I0815 04:46:17.063052 11101 solver.cpp:334]     Train net output #0: loss = 1.78656 (* 1 = 1.78656 loss)
I0815 04:46:17.063058 11101 sgd_solver.cpp:136] Iteration 205200, lr = 0.035875, m = 0.9
I0815 04:46:31.223309 11101 solver.cpp:312] Iteration 205300 (7.0622 iter/s, 14.1599s/100 iter), loss = 1.86094
I0815 04:46:31.223386 11101 solver.cpp:334]     Train net output #0: loss = 1.89179 (* 1 = 1.89179 loss)
I0815 04:46:31.223399 11101 sgd_solver.cpp:136] Iteration 205300, lr = 0.0358437, m = 0.9
I0815 04:46:45.939061 11101 solver.cpp:312] Iteration 205400 (6.79563 iter/s, 14.7153s/100 iter), loss = 2.04808
I0815 04:46:45.939126 11101 solver.cpp:334]     Train net output #0: loss = 1.85114 (* 1 = 1.85114 loss)
I0815 04:46:45.939144 11101 sgd_solver.cpp:136] Iteration 205400, lr = 0.0358125, m = 0.9
I0815 04:47:00.962337 11101 solver.cpp:312] Iteration 205500 (6.65652 iter/s, 15.0229s/100 iter), loss = 1.644
I0815 04:47:00.962406 11101 solver.cpp:334]     Train net output #0: loss = 1.50604 (* 1 = 1.50604 loss)
I0815 04:47:00.962424 11101 sgd_solver.cpp:136] Iteration 205500, lr = 0.0357813, m = 0.9
I0815 04:47:15.715291 11101 solver.cpp:312] Iteration 205600 (6.77849 iter/s, 14.7526s/100 iter), loss = 2.03791
I0815 04:47:15.715361 11101 solver.cpp:334]     Train net output #0: loss = 1.97313 (* 1 = 1.97313 loss)
I0815 04:47:15.715369 11101 sgd_solver.cpp:136] Iteration 205600, lr = 0.03575, m = 0.9
I0815 04:47:30.340492 11101 solver.cpp:312] Iteration 205700 (6.8377 iter/s, 14.6248s/100 iter), loss = 2.128
I0815 04:47:30.340520 11101 solver.cpp:334]     Train net output #0: loss = 2.04962 (* 1 = 2.04962 loss)
I0815 04:47:30.340526 11101 sgd_solver.cpp:136] Iteration 205700, lr = 0.0357188, m = 0.9
I0815 04:47:44.880520 11101 solver.cpp:312] Iteration 205800 (6.87775 iter/s, 14.5396s/100 iter), loss = 1.81961
I0815 04:47:44.880550 11101 solver.cpp:334]     Train net output #0: loss = 1.94591 (* 1 = 1.94591 loss)
I0815 04:47:44.880556 11101 sgd_solver.cpp:136] Iteration 205800, lr = 0.0356875, m = 0.9
I0815 04:47:59.311630 11101 solver.cpp:312] Iteration 205900 (6.92967 iter/s, 14.4307s/100 iter), loss = 1.59856
I0815 04:47:59.311724 11101 solver.cpp:334]     Train net output #0: loss = 1.50266 (* 1 = 1.50266 loss)
I0815 04:47:59.311743 11101 sgd_solver.cpp:136] Iteration 205900, lr = 0.0356563, m = 0.9
I0815 04:48:13.766870 11101 solver.cpp:509] Iteration 206000, Testing net (#0)
I0815 04:48:34.724992 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.447706
I0815 04:48:34.725044 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.706822
I0815 04:48:34.725054 11101 solver.cpp:594]     Test net output #2: loss = 2.52976 (* 1 = 2.52976 loss)
I0815 04:48:34.725075 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9576s
I0815 04:48:34.896303 11101 solver.cpp:312] Iteration 206000 (2.81028 iter/s, 35.5837s/100 iter), loss = 2.06466
I0815 04:48:34.896332 11101 solver.cpp:334]     Train net output #0: loss = 2.13486 (* 1 = 2.13486 loss)
I0815 04:48:34.896337 11101 sgd_solver.cpp:136] Iteration 206000, lr = 0.035625, m = 0.9
I0815 04:48:49.495263 11101 solver.cpp:312] Iteration 206100 (6.84999 iter/s, 14.5986s/100 iter), loss = 1.8255
I0815 04:48:49.495484 11101 solver.cpp:334]     Train net output #0: loss = 1.54035 (* 1 = 1.54035 loss)
I0815 04:48:49.495595 11101 sgd_solver.cpp:136] Iteration 206100, lr = 0.0355937, m = 0.9
I0815 04:49:04.282668 11101 solver.cpp:312] Iteration 206200 (6.7627 iter/s, 14.787s/100 iter), loss = 1.31329
I0815 04:49:04.282694 11101 solver.cpp:334]     Train net output #0: loss = 1.52189 (* 1 = 1.52189 loss)
I0815 04:49:04.282701 11101 sgd_solver.cpp:136] Iteration 206200, lr = 0.0355625, m = 0.9
I0815 04:49:19.007334 11101 solver.cpp:312] Iteration 206300 (6.79151 iter/s, 14.7243s/100 iter), loss = 1.96271
I0815 04:49:19.007395 11101 solver.cpp:334]     Train net output #0: loss = 1.80867 (* 1 = 1.80867 loss)
I0815 04:49:19.007402 11101 sgd_solver.cpp:136] Iteration 206300, lr = 0.0355313, m = 0.9
I0815 04:49:33.490152 11101 solver.cpp:312] Iteration 206400 (6.90493 iter/s, 14.4824s/100 iter), loss = 1.7036
I0815 04:49:33.490216 11101 solver.cpp:334]     Train net output #0: loss = 1.54272 (* 1 = 1.54272 loss)
I0815 04:49:33.490234 11101 sgd_solver.cpp:136] Iteration 206400, lr = 0.0355, m = 0.9
I0815 04:49:48.077683 11101 solver.cpp:312] Iteration 206500 (6.85536 iter/s, 14.5871s/100 iter), loss = 1.8355
I0815 04:49:48.077711 11101 solver.cpp:334]     Train net output #0: loss = 1.73089 (* 1 = 1.73089 loss)
I0815 04:49:48.077718 11101 sgd_solver.cpp:136] Iteration 206500, lr = 0.0354688, m = 0.9
I0815 04:50:02.936161 11101 solver.cpp:312] Iteration 206600 (6.73035 iter/s, 14.8581s/100 iter), loss = 1.58352
I0815 04:50:02.936260 11101 solver.cpp:334]     Train net output #0: loss = 1.42917 (* 1 = 1.42917 loss)
I0815 04:50:02.936273 11101 sgd_solver.cpp:136] Iteration 206600, lr = 0.0354375, m = 0.9
I0815 04:50:17.950863 11101 solver.cpp:312] Iteration 206700 (6.66032 iter/s, 15.0143s/100 iter), loss = 1.81364
I0815 04:50:17.950892 11101 solver.cpp:334]     Train net output #0: loss = 1.62913 (* 1 = 1.62913 loss)
I0815 04:50:17.950898 11101 sgd_solver.cpp:136] Iteration 206700, lr = 0.0354063, m = 0.9
I0815 04:50:32.404469 11101 solver.cpp:312] Iteration 206800 (6.91888 iter/s, 14.4532s/100 iter), loss = 1.88943
I0815 04:50:32.404495 11101 solver.cpp:334]     Train net output #0: loss = 1.8978 (* 1 = 1.8978 loss)
I0815 04:50:32.404502 11101 sgd_solver.cpp:136] Iteration 206800, lr = 0.035375, m = 0.9
I0815 04:50:46.856994 11101 solver.cpp:312] Iteration 206900 (6.9194 iter/s, 14.4521s/100 iter), loss = 1.33096
I0815 04:50:46.858017 11101 solver.cpp:334]     Train net output #0: loss = 1.47292 (* 1 = 1.47292 loss)
I0815 04:50:46.858047 11101 sgd_solver.cpp:136] Iteration 206900, lr = 0.0353437, m = 0.9
I0815 04:51:01.679375 11101 solver.cpp:312] Iteration 207000 (6.74674 iter/s, 14.822s/100 iter), loss = 1.64541
I0815 04:51:01.679407 11101 solver.cpp:334]     Train net output #0: loss = 1.67422 (* 1 = 1.67422 loss)
I0815 04:51:01.679414 11101 sgd_solver.cpp:136] Iteration 207000, lr = 0.0353125, m = 0.9
I0815 04:51:16.273785 11101 solver.cpp:312] Iteration 207100 (6.85213 iter/s, 14.594s/100 iter), loss = 2.19821
I0815 04:51:16.273814 11101 solver.cpp:334]     Train net output #0: loss = 2.14338 (* 1 = 2.14338 loss)
I0815 04:51:16.273820 11101 sgd_solver.cpp:136] Iteration 207100, lr = 0.0352813, m = 0.9
I0815 04:51:31.436190 11101 solver.cpp:312] Iteration 207200 (6.59544 iter/s, 15.162s/100 iter), loss = 1.53621
I0815 04:51:31.436269 11101 solver.cpp:334]     Train net output #0: loss = 1.61752 (* 1 = 1.61752 loss)
I0815 04:51:31.436278 11101 sgd_solver.cpp:136] Iteration 207200, lr = 0.03525, m = 0.9
I0815 04:51:46.062711 11101 solver.cpp:312] Iteration 207300 (6.83709 iter/s, 14.6261s/100 iter), loss = 1.51247
I0815 04:51:46.062736 11101 solver.cpp:334]     Train net output #0: loss = 1.4493 (* 1 = 1.4493 loss)
I0815 04:51:46.062741 11101 sgd_solver.cpp:136] Iteration 207300, lr = 0.0352188, m = 0.9
I0815 04:52:00.626030 11101 solver.cpp:312] Iteration 207400 (6.86676 iter/s, 14.5629s/100 iter), loss = 1.62775
I0815 04:52:00.626080 11101 solver.cpp:334]     Train net output #0: loss = 1.70929 (* 1 = 1.70929 loss)
I0815 04:52:00.626093 11101 sgd_solver.cpp:136] Iteration 207400, lr = 0.0351875, m = 0.9
I0815 04:52:15.033700 11101 solver.cpp:312] Iteration 207500 (6.94094 iter/s, 14.4073s/100 iter), loss = 1.46072
I0815 04:52:15.033792 11101 solver.cpp:334]     Train net output #0: loss = 1.54844 (* 1 = 1.54844 loss)
I0815 04:52:15.033812 11101 sgd_solver.cpp:136] Iteration 207500, lr = 0.0351562, m = 0.9
I0815 04:52:29.330988 11101 solver.cpp:312] Iteration 207600 (6.99453 iter/s, 14.2969s/100 iter), loss = 1.50471
I0815 04:52:29.331015 11101 solver.cpp:334]     Train net output #0: loss = 1.95782 (* 1 = 1.95782 loss)
I0815 04:52:29.331022 11101 sgd_solver.cpp:136] Iteration 207600, lr = 0.035125, m = 0.9
I0815 04:52:44.117347 11101 solver.cpp:312] Iteration 207700 (6.76318 iter/s, 14.786s/100 iter), loss = 2.1147
I0815 04:52:44.117375 11101 solver.cpp:334]     Train net output #0: loss = 2.52766 (* 1 = 2.52766 loss)
I0815 04:52:44.117383 11101 sgd_solver.cpp:136] Iteration 207700, lr = 0.0350938, m = 0.9
I0815 04:52:59.001387 11101 solver.cpp:312] Iteration 207800 (6.71879 iter/s, 14.8836s/100 iter), loss = 1.76917
I0815 04:52:59.001468 11101 solver.cpp:334]     Train net output #0: loss = 1.69044 (* 1 = 1.69044 loss)
I0815 04:52:59.001485 11101 sgd_solver.cpp:136] Iteration 207800, lr = 0.0350625, m = 0.9
I0815 04:53:13.402973 11101 solver.cpp:312] Iteration 207900 (6.94387 iter/s, 14.4012s/100 iter), loss = 1.66491
I0815 04:53:13.402997 11101 solver.cpp:334]     Train net output #0: loss = 1.86175 (* 1 = 1.86175 loss)
I0815 04:53:13.403004 11101 sgd_solver.cpp:136] Iteration 207900, lr = 0.0350312, m = 0.9
I0815 04:53:28.029708 11101 solver.cpp:509] Iteration 208000, Testing net (#0)
I0815 04:53:43.582535 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 04:53:48.961026 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.478765
I0815 04:53:48.961051 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.730527
I0815 04:53:48.961058 11101 solver.cpp:594]     Test net output #2: loss = 2.34657 (* 1 = 2.34657 loss)
I0815 04:53:48.961077 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9308s
I0815 04:53:49.112486 11101 solver.cpp:312] Iteration 208000 (2.80045 iter/s, 35.7085s/100 iter), loss = 1.79011
I0815 04:53:49.112509 11101 solver.cpp:334]     Train net output #0: loss = 2.02763 (* 1 = 2.02763 loss)
I0815 04:53:49.112515 11101 sgd_solver.cpp:136] Iteration 208000, lr = 0.035, m = 0.9
I0815 04:54:03.584447 11101 solver.cpp:312] Iteration 208100 (6.91011 iter/s, 14.4716s/100 iter), loss = 1.76353
I0815 04:54:03.588171 11101 solver.cpp:334]     Train net output #0: loss = 1.23782 (* 1 = 1.23782 loss)
I0815 04:54:03.588177 11101 sgd_solver.cpp:136] Iteration 208100, lr = 0.0349688, m = 0.9
I0815 04:54:18.209625 11101 solver.cpp:312] Iteration 208200 (6.83772 iter/s, 14.6248s/100 iter), loss = 1.43385
I0815 04:54:18.209712 11101 solver.cpp:334]     Train net output #0: loss = 1.72343 (* 1 = 1.72343 loss)
I0815 04:54:18.209718 11101 sgd_solver.cpp:136] Iteration 208200, lr = 0.0349375, m = 0.9
I0815 04:54:33.067010 11101 solver.cpp:312] Iteration 208300 (6.73084 iter/s, 14.857s/100 iter), loss = 1.77631
I0815 04:54:33.067032 11101 solver.cpp:334]     Train net output #0: loss = 1.95017 (* 1 = 1.95017 loss)
I0815 04:54:33.067036 11101 sgd_solver.cpp:136] Iteration 208300, lr = 0.0349062, m = 0.9
I0815 04:54:47.787974 11101 solver.cpp:312] Iteration 208400 (6.79322 iter/s, 14.7206s/100 iter), loss = 1.8016
I0815 04:54:47.788035 11101 solver.cpp:334]     Train net output #0: loss = 1.44871 (* 1 = 1.44871 loss)
I0815 04:54:47.788053 11101 sgd_solver.cpp:136] Iteration 208400, lr = 0.034875, m = 0.9
I0815 04:55:02.288790 11101 solver.cpp:312] Iteration 208500 (6.89636 iter/s, 14.5004s/100 iter), loss = 1.89411
I0815 04:55:02.288874 11101 solver.cpp:334]     Train net output #0: loss = 1.87258 (* 1 = 1.87258 loss)
I0815 04:55:02.288890 11101 sgd_solver.cpp:136] Iteration 208500, lr = 0.0348438, m = 0.9
I0815 04:55:17.120759 11101 solver.cpp:312] Iteration 208600 (6.74238 iter/s, 14.8316s/100 iter), loss = 2.06319
I0815 04:55:17.120785 11101 solver.cpp:334]     Train net output #0: loss = 1.82997 (* 1 = 1.82997 loss)
I0815 04:55:17.120791 11101 sgd_solver.cpp:136] Iteration 208600, lr = 0.0348125, m = 0.9
I0815 04:55:31.939869 11101 solver.cpp:312] Iteration 208700 (6.74823 iter/s, 14.8187s/100 iter), loss = 1.55024
I0815 04:55:31.939896 11101 solver.cpp:334]     Train net output #0: loss = 1.64783 (* 1 = 1.64783 loss)
I0815 04:55:31.939901 11101 sgd_solver.cpp:136] Iteration 208700, lr = 0.0347812, m = 0.9
I0815 04:55:46.713505 11101 solver.cpp:312] Iteration 208800 (6.769 iter/s, 14.7732s/100 iter), loss = 1.73435
I0815 04:55:46.713712 11101 solver.cpp:334]     Train net output #0: loss = 1.69755 (* 1 = 1.69755 loss)
I0815 04:55:46.713799 11101 sgd_solver.cpp:136] Iteration 208800, lr = 0.03475, m = 0.9
I0815 04:56:01.522786 11101 solver.cpp:312] Iteration 208900 (6.75271 iter/s, 14.8089s/100 iter), loss = 2.17053
I0815 04:56:01.522857 11101 solver.cpp:334]     Train net output #0: loss = 2.48754 (* 1 = 2.48754 loss)
I0815 04:56:01.522878 11101 sgd_solver.cpp:136] Iteration 208900, lr = 0.0347188, m = 0.9
I0815 04:56:15.909281 11101 solver.cpp:312] Iteration 209000 (6.95115 iter/s, 14.3861s/100 iter), loss = 1.87667
I0815 04:56:15.909344 11101 solver.cpp:334]     Train net output #0: loss = 1.2536 (* 1 = 1.2536 loss)
I0815 04:56:15.909366 11101 sgd_solver.cpp:136] Iteration 209000, lr = 0.0346875, m = 0.9
I0815 04:56:30.583509 11101 solver.cpp:312] Iteration 209100 (6.81486 iter/s, 14.6738s/100 iter), loss = 1.60981
I0815 04:56:30.583600 11101 solver.cpp:334]     Train net output #0: loss = 1.54633 (* 1 = 1.54633 loss)
I0815 04:56:30.583613 11101 sgd_solver.cpp:136] Iteration 209100, lr = 0.0346563, m = 0.9
I0815 04:56:45.443011 11101 solver.cpp:312] Iteration 209200 (6.72989 iter/s, 14.8591s/100 iter), loss = 1.81498
I0815 04:56:45.443038 11101 solver.cpp:334]     Train net output #0: loss = 1.78514 (* 1 = 1.78514 loss)
I0815 04:56:45.443044 11101 sgd_solver.cpp:136] Iteration 209200, lr = 0.034625, m = 0.9
I0815 04:57:00.147290 11101 solver.cpp:312] Iteration 209300 (6.80093 iter/s, 14.7039s/100 iter), loss = 1.65194
I0815 04:57:00.147341 11101 solver.cpp:334]     Train net output #0: loss = 1.51322 (* 1 = 1.51322 loss)
I0815 04:57:00.147354 11101 sgd_solver.cpp:136] Iteration 209300, lr = 0.0345937, m = 0.9
I0815 04:57:14.691471 11101 solver.cpp:312] Iteration 209400 (6.87579 iter/s, 14.5438s/100 iter), loss = 1.61832
I0815 04:57:14.696166 11101 solver.cpp:334]     Train net output #0: loss = 1.66906 (* 1 = 1.66906 loss)
I0815 04:57:14.696177 11101 sgd_solver.cpp:136] Iteration 209400, lr = 0.0345625, m = 0.9
I0815 04:57:29.283068 11101 solver.cpp:312] Iteration 209500 (6.85345 iter/s, 14.5912s/100 iter), loss = 1.84926
I0815 04:57:29.283243 11101 solver.cpp:334]     Train net output #0: loss = 1.95901 (* 1 = 1.95901 loss)
I0815 04:57:29.283332 11101 sgd_solver.cpp:136] Iteration 209500, lr = 0.0345312, m = 0.9
I0815 04:57:43.981783 11101 solver.cpp:312] Iteration 209600 (6.8035 iter/s, 14.6983s/100 iter), loss = 2.13166
I0815 04:57:43.981858 11101 solver.cpp:334]     Train net output #0: loss = 1.74924 (* 1 = 1.74924 loss)
I0815 04:57:43.981878 11101 sgd_solver.cpp:136] Iteration 209600, lr = 0.0345, m = 0.9
I0815 04:57:58.890202 11101 solver.cpp:312] Iteration 209700 (6.70781 iter/s, 14.908s/100 iter), loss = 1.66435
I0815 04:57:58.890277 11101 solver.cpp:334]     Train net output #0: loss = 1.6201 (* 1 = 1.6201 loss)
I0815 04:57:58.890291 11101 sgd_solver.cpp:136] Iteration 209700, lr = 0.0344688, m = 0.9
I0815 04:58:13.725219 11101 solver.cpp:312] Iteration 209800 (6.74099 iter/s, 14.8346s/100 iter), loss = 1.70514
I0815 04:58:13.725286 11101 solver.cpp:334]     Train net output #0: loss = 1.66455 (* 1 = 1.66455 loss)
I0815 04:58:13.725303 11101 sgd_solver.cpp:136] Iteration 209800, lr = 0.0344375, m = 0.9
I0815 04:58:28.169101 11101 solver.cpp:312] Iteration 209900 (6.92354 iter/s, 14.4435s/100 iter), loss = 1.80762
I0815 04:58:28.169153 11101 solver.cpp:334]     Train net output #0: loss = 2.03997 (* 1 = 2.03997 loss)
I0815 04:58:28.169167 11101 sgd_solver.cpp:136] Iteration 209900, lr = 0.0344063, m = 0.9
I0815 04:58:42.548090 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_210000.caffemodel
I0815 04:58:42.559172 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_210000.solverstate
I0815 04:58:42.565765 11101 solver.cpp:509] Iteration 210000, Testing net (#0)
I0815 04:59:03.423641 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.470412
I0815 04:59:03.423661 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.717352
I0815 04:59:03.423669 11101 solver.cpp:594]     Test net output #2: loss = 2.41618 (* 1 = 2.41618 loss)
I0815 04:59:03.423691 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8574s
I0815 04:59:03.582028 11101 solver.cpp:312] Iteration 210000 (2.8239 iter/s, 35.412s/100 iter), loss = 1.95363
I0815 04:59:03.582056 11101 solver.cpp:334]     Train net output #0: loss = 1.91442 (* 1 = 1.91442 loss)
I0815 04:59:03.582062 11101 sgd_solver.cpp:136] Iteration 210000, lr = 0.034375, m = 0.9
I0815 04:59:18.224275 11101 solver.cpp:312] Iteration 210100 (6.82974 iter/s, 14.6418s/100 iter), loss = 1.74381
I0815 04:59:18.224503 11101 solver.cpp:334]     Train net output #0: loss = 2.10778 (* 1 = 2.10778 loss)
I0815 04:59:18.224512 11101 sgd_solver.cpp:136] Iteration 210100, lr = 0.0343437, m = 0.9
I0815 04:59:32.777398 11101 solver.cpp:312] Iteration 210200 (6.87157 iter/s, 14.5527s/100 iter), loss = 1.69864
I0815 04:59:32.777428 11101 solver.cpp:334]     Train net output #0: loss = 2.11678 (* 1 = 2.11678 loss)
I0815 04:59:32.777434 11101 sgd_solver.cpp:136] Iteration 210200, lr = 0.0343125, m = 0.9
I0815 04:59:47.203119 11101 solver.cpp:312] Iteration 210300 (6.93226 iter/s, 14.4253s/100 iter), loss = 2.12011
I0815 04:59:47.203148 11101 solver.cpp:334]     Train net output #0: loss = 2.21037 (* 1 = 2.21037 loss)
I0815 04:59:47.203155 11101 sgd_solver.cpp:136] Iteration 210300, lr = 0.0342813, m = 0.9
I0815 05:00:01.825757 11101 solver.cpp:312] Iteration 210400 (6.8389 iter/s, 14.6222s/100 iter), loss = 1.73085
I0815 05:00:01.825860 11101 solver.cpp:334]     Train net output #0: loss = 1.81123 (* 1 = 1.81123 loss)
I0815 05:00:01.825881 11101 sgd_solver.cpp:136] Iteration 210400, lr = 0.03425, m = 0.9
I0815 05:00:16.505092 11101 solver.cpp:312] Iteration 210500 (6.81249 iter/s, 14.6789s/100 iter), loss = 1.69986
I0815 05:00:16.505117 11101 solver.cpp:334]     Train net output #0: loss = 1.8762 (* 1 = 1.8762 loss)
I0815 05:00:16.505121 11101 sgd_solver.cpp:136] Iteration 210500, lr = 0.0342188, m = 0.9
I0815 05:00:31.426054 11101 solver.cpp:312] Iteration 210600 (6.70217 iter/s, 14.9205s/100 iter), loss = 1.33916
I0815 05:00:31.426081 11101 solver.cpp:334]     Train net output #0: loss = 1.47535 (* 1 = 1.47535 loss)
I0815 05:00:31.426087 11101 sgd_solver.cpp:136] Iteration 210600, lr = 0.0341875, m = 0.9
I0815 05:00:46.302448 11101 solver.cpp:312] Iteration 210700 (6.72224 iter/s, 14.876s/100 iter), loss = 1.8379
I0815 05:00:46.302520 11101 solver.cpp:334]     Train net output #0: loss = 1.94374 (* 1 = 1.94374 loss)
I0815 05:00:46.302526 11101 sgd_solver.cpp:136] Iteration 210700, lr = 0.0341563, m = 0.9
I0815 05:01:00.977591 11101 solver.cpp:312] Iteration 210800 (6.81443 iter/s, 14.6747s/100 iter), loss = 2.04002
I0815 05:01:00.977617 11101 solver.cpp:334]     Train net output #0: loss = 1.77544 (* 1 = 1.77544 loss)
I0815 05:01:00.977623 11101 sgd_solver.cpp:136] Iteration 210800, lr = 0.034125, m = 0.9
I0815 05:01:15.872689 11101 solver.cpp:312] Iteration 210900 (6.7138 iter/s, 14.8947s/100 iter), loss = 1.72286
I0815 05:01:15.872758 11101 solver.cpp:334]     Train net output #0: loss = 1.55173 (* 1 = 1.55173 loss)
I0815 05:01:15.872778 11101 sgd_solver.cpp:136] Iteration 210900, lr = 0.0340937, m = 0.9
I0815 05:01:30.270774 11101 solver.cpp:312] Iteration 211000 (6.94556 iter/s, 14.3977s/100 iter), loss = 1.6282
I0815 05:01:30.270853 11101 solver.cpp:334]     Train net output #0: loss = 1.78539 (* 1 = 1.78539 loss)
I0815 05:01:30.270866 11101 sgd_solver.cpp:136] Iteration 211000, lr = 0.0340625, m = 0.9
I0815 05:01:45.097285 11101 solver.cpp:312] Iteration 211100 (6.74486 iter/s, 14.8261s/100 iter), loss = 1.97896
I0815 05:01:45.097312 11101 solver.cpp:334]     Train net output #0: loss = 1.91171 (* 1 = 1.91171 loss)
I0815 05:01:45.097318 11101 sgd_solver.cpp:136] Iteration 211100, lr = 0.0340312, m = 0.9
I0815 05:01:59.616817 11101 solver.cpp:312] Iteration 211200 (6.88747 iter/s, 14.5191s/100 iter), loss = 1.71035
I0815 05:01:59.616853 11101 solver.cpp:334]     Train net output #0: loss = 1.94375 (* 1 = 1.94375 loss)
I0815 05:01:59.616859 11101 sgd_solver.cpp:136] Iteration 211200, lr = 0.034, m = 0.9
I0815 05:02:14.178277 11101 solver.cpp:312] Iteration 211300 (6.86763 iter/s, 14.5611s/100 iter), loss = 1.72823
I0815 05:02:14.178373 11101 solver.cpp:334]     Train net output #0: loss = 1.93693 (* 1 = 1.93693 loss)
I0815 05:02:14.178398 11101 sgd_solver.cpp:136] Iteration 211300, lr = 0.0339688, m = 0.9
I0815 05:02:28.793714 11101 solver.cpp:312] Iteration 211400 (6.84227 iter/s, 14.615s/100 iter), loss = 1.71887
I0815 05:02:28.793746 11101 solver.cpp:334]     Train net output #0: loss = 1.66345 (* 1 = 1.66345 loss)
I0815 05:02:28.793752 11101 sgd_solver.cpp:136] Iteration 211400, lr = 0.0339375, m = 0.9
I0815 05:02:43.823292 11101 solver.cpp:312] Iteration 211500 (6.65373 iter/s, 15.0292s/100 iter), loss = 1.73674
I0815 05:02:43.823315 11101 solver.cpp:334]     Train net output #0: loss = 1.57404 (* 1 = 1.57404 loss)
I0815 05:02:43.823319 11101 sgd_solver.cpp:136] Iteration 211500, lr = 0.0339063, m = 0.9
I0815 05:02:58.480813 11101 solver.cpp:312] Iteration 211600 (6.82262 iter/s, 14.6571s/100 iter), loss = 1.52291
I0815 05:02:58.480883 11101 solver.cpp:334]     Train net output #0: loss = 1.46519 (* 1 = 1.46519 loss)
I0815 05:02:58.480890 11101 sgd_solver.cpp:136] Iteration 211600, lr = 0.033875, m = 0.9
I0815 05:03:13.292366 11101 solver.cpp:312] Iteration 211700 (6.75167 iter/s, 14.8111s/100 iter), loss = 1.63258
I0815 05:03:13.292392 11101 solver.cpp:334]     Train net output #0: loss = 1.99787 (* 1 = 1.99787 loss)
I0815 05:03:13.292397 11101 sgd_solver.cpp:136] Iteration 211700, lr = 0.0338438, m = 0.9
I0815 05:03:28.313154 11101 solver.cpp:312] Iteration 211800 (6.65762 iter/s, 15.0204s/100 iter), loss = 1.86216
I0815 05:03:28.313179 11101 solver.cpp:334]     Train net output #0: loss = 1.51624 (* 1 = 1.51624 loss)
I0815 05:03:28.313184 11101 sgd_solver.cpp:136] Iteration 211800, lr = 0.0338125, m = 0.9
I0815 05:03:42.718246 11101 solver.cpp:312] Iteration 211900 (6.94218 iter/s, 14.4047s/100 iter), loss = 1.89006
I0815 05:03:42.718312 11101 solver.cpp:334]     Train net output #0: loss = 1.6911 (* 1 = 1.6911 loss)
I0815 05:03:42.718319 11101 sgd_solver.cpp:136] Iteration 211900, lr = 0.0337812, m = 0.9
I0815 05:03:57.159934 11101 solver.cpp:509] Iteration 212000, Testing net (#0)
I0815 05:04:18.029836 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.424647
I0815 05:04:18.029939 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.682587
I0815 05:04:18.029948 11101 solver.cpp:594]     Test net output #2: loss = 2.72643 (* 1 = 2.72643 loss)
I0815 05:04:18.029969 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8695s
I0815 05:04:18.188361 11101 solver.cpp:312] Iteration 212000 (2.81935 iter/s, 35.4691s/100 iter), loss = 1.60988
I0815 05:04:18.188416 11101 solver.cpp:334]     Train net output #0: loss = 1.5441 (* 1 = 1.5441 loss)
I0815 05:04:18.188427 11101 sgd_solver.cpp:136] Iteration 212000, lr = 0.03375, m = 0.9
I0815 05:04:32.942409 11101 solver.cpp:312] Iteration 212100 (6.77799 iter/s, 14.7536s/100 iter), loss = 2.07409
I0815 05:04:32.942433 11101 solver.cpp:334]     Train net output #0: loss = 2.15467 (* 1 = 2.15467 loss)
I0815 05:04:32.942437 11101 sgd_solver.cpp:136] Iteration 212100, lr = 0.0337188, m = 0.9
I0815 05:04:48.064502 11101 solver.cpp:312] Iteration 212200 (6.61302 iter/s, 15.1217s/100 iter), loss = 1.65666
I0815 05:04:48.064592 11101 solver.cpp:334]     Train net output #0: loss = 1.77404 (* 1 = 1.77404 loss)
I0815 05:04:48.064610 11101 sgd_solver.cpp:136] Iteration 212200, lr = 0.0336875, m = 0.9
I0815 05:05:03.108162 11101 solver.cpp:312] Iteration 212300 (6.6475 iter/s, 15.0432s/100 iter), loss = 2.01858
I0815 05:05:03.108194 11101 solver.cpp:334]     Train net output #0: loss = 1.9402 (* 1 = 1.9402 loss)
I0815 05:05:03.108201 11101 sgd_solver.cpp:136] Iteration 212300, lr = 0.0336563, m = 0.9
I0815 05:05:17.739869 11101 solver.cpp:312] Iteration 212400 (6.83466 iter/s, 14.6313s/100 iter), loss = 1.7707
I0815 05:05:17.739902 11101 solver.cpp:334]     Train net output #0: loss = 1.75363 (* 1 = 1.75363 loss)
I0815 05:05:17.739943 11101 sgd_solver.cpp:136] Iteration 212400, lr = 0.033625, m = 0.9
I0815 05:05:32.115988 11101 solver.cpp:312] Iteration 212500 (6.95617 iter/s, 14.3757s/100 iter), loss = 1.83167
I0815 05:05:32.116046 11101 solver.cpp:334]     Train net output #0: loss = 2.17837 (* 1 = 2.17837 loss)
I0815 05:05:32.116053 11101 sgd_solver.cpp:136] Iteration 212500, lr = 0.0335938, m = 0.9
I0815 05:05:46.621330 11101 solver.cpp:312] Iteration 212600 (6.8942 iter/s, 14.5049s/100 iter), loss = 1.61256
I0815 05:05:46.621393 11101 solver.cpp:334]     Train net output #0: loss = 1.51686 (* 1 = 1.51686 loss)
I0815 05:05:46.621417 11101 sgd_solver.cpp:136] Iteration 212600, lr = 0.0335625, m = 0.9
I0815 05:06:00.975502 11101 solver.cpp:312] Iteration 212700 (6.96681 iter/s, 14.3538s/100 iter), loss = 1.40262
I0815 05:06:00.975559 11101 solver.cpp:334]     Train net output #0: loss = 1.6013 (* 1 = 1.6013 loss)
I0815 05:06:00.975577 11101 sgd_solver.cpp:136] Iteration 212700, lr = 0.0335312, m = 0.9
I0815 05:06:15.844708 11101 solver.cpp:312] Iteration 212800 (6.72549 iter/s, 14.8688s/100 iter), loss = 1.80845
I0815 05:06:15.844789 11101 solver.cpp:334]     Train net output #0: loss = 1.73025 (* 1 = 1.73025 loss)
I0815 05:06:15.844796 11101 sgd_solver.cpp:136] Iteration 212800, lr = 0.0335, m = 0.9
I0815 05:06:30.336588 11101 solver.cpp:312] Iteration 212900 (6.90061 iter/s, 14.4915s/100 iter), loss = 1.83453
I0815 05:06:30.336654 11101 solver.cpp:334]     Train net output #0: loss = 1.94192 (* 1 = 1.94192 loss)
I0815 05:06:30.336671 11101 sgd_solver.cpp:136] Iteration 212900, lr = 0.0334687, m = 0.9
I0815 05:06:44.888687 11101 solver.cpp:312] Iteration 213000 (6.87205 iter/s, 14.5517s/100 iter), loss = 2.07706
I0815 05:06:44.888717 11101 solver.cpp:334]     Train net output #0: loss = 2.45284 (* 1 = 2.45284 loss)
I0815 05:06:44.888725 11101 sgd_solver.cpp:136] Iteration 213000, lr = 0.0334375, m = 0.9
I0815 05:06:59.318512 11101 solver.cpp:312] Iteration 213100 (6.93028 iter/s, 14.4294s/100 iter), loss = 1.61783
I0815 05:06:59.318577 11101 solver.cpp:334]     Train net output #0: loss = 1.69293 (* 1 = 1.69293 loss)
I0815 05:06:59.318584 11101 sgd_solver.cpp:136] Iteration 213100, lr = 0.0334063, m = 0.9
I0815 05:07:13.727990 11101 solver.cpp:312] Iteration 213200 (6.94007 iter/s, 14.4091s/100 iter), loss = 1.89108
I0815 05:07:13.728197 11101 solver.cpp:334]     Train net output #0: loss = 1.90795 (* 1 = 1.90795 loss)
I0815 05:07:13.728209 11101 sgd_solver.cpp:136] Iteration 213200, lr = 0.033375, m = 0.9
I0815 05:07:28.266042 11101 solver.cpp:312] Iteration 213300 (6.87869 iter/s, 14.5376s/100 iter), loss = 1.91983
I0815 05:07:28.266072 11101 solver.cpp:334]     Train net output #0: loss = 1.58974 (* 1 = 1.58974 loss)
I0815 05:07:28.266078 11101 sgd_solver.cpp:136] Iteration 213300, lr = 0.0333438, m = 0.9
I0815 05:07:42.635473 11101 solver.cpp:312] Iteration 213400 (6.95941 iter/s, 14.369s/100 iter), loss = 1.55121
I0815 05:07:42.636183 11101 solver.cpp:334]     Train net output #0: loss = 1.85905 (* 1 = 1.85905 loss)
I0815 05:07:42.636201 11101 sgd_solver.cpp:136] Iteration 213400, lr = 0.0333125, m = 0.9
I0815 05:07:57.059218 11101 solver.cpp:312] Iteration 213500 (6.9332 iter/s, 14.4233s/100 iter), loss = 1.85433
I0815 05:07:57.059247 11101 solver.cpp:334]     Train net output #0: loss = 1.73595 (* 1 = 1.73595 loss)
I0815 05:07:57.059252 11101 sgd_solver.cpp:136] Iteration 213500, lr = 0.0332812, m = 0.9
I0815 05:08:00.772078 11065 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 05:08:11.658624 11101 solver.cpp:312] Iteration 213600 (6.84978 iter/s, 14.599s/100 iter), loss = 1.6636
I0815 05:08:11.658651 11101 solver.cpp:334]     Train net output #0: loss = 1.72141 (* 1 = 1.72141 loss)
I0815 05:08:11.658655 11101 sgd_solver.cpp:136] Iteration 213600, lr = 0.03325, m = 0.9
I0815 05:08:26.125082 11101 solver.cpp:312] Iteration 213700 (6.91273 iter/s, 14.4661s/100 iter), loss = 1.56157
I0815 05:08:26.125138 11101 solver.cpp:334]     Train net output #0: loss = 1.58607 (* 1 = 1.58607 loss)
I0815 05:08:26.125144 11101 sgd_solver.cpp:136] Iteration 213700, lr = 0.0332187, m = 0.9
I0815 05:08:40.665603 11101 solver.cpp:312] Iteration 213800 (6.87752 iter/s, 14.5401s/100 iter), loss = 2.28156
I0815 05:08:40.665669 11101 solver.cpp:334]     Train net output #0: loss = 1.95798 (* 1 = 1.95798 loss)
I0815 05:08:40.665693 11101 sgd_solver.cpp:136] Iteration 213800, lr = 0.0331875, m = 0.9
I0815 05:08:55.213201 11101 solver.cpp:312] Iteration 213900 (6.87418 iter/s, 14.5472s/100 iter), loss = 1.95293
I0815 05:08:55.213254 11101 solver.cpp:334]     Train net output #0: loss = 2.24838 (* 1 = 2.24838 loss)
I0815 05:08:55.213275 11101 sgd_solver.cpp:136] Iteration 213900, lr = 0.0331563, m = 0.9
I0815 05:09:09.618254 11101 solver.cpp:509] Iteration 214000, Testing net (#0)
I0815 05:09:22.676486 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 05:09:31.015532 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.468471
I0815 05:09:31.015552 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.724823
I0815 05:09:31.015558 11101 solver.cpp:594]     Test net output #2: loss = 2.39305 (* 1 = 2.39305 loss)
I0815 05:09:31.015579 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3967s
I0815 05:09:31.179179 11101 solver.cpp:312] Iteration 214000 (2.78048 iter/s, 35.965s/100 iter), loss = 1.18378
I0815 05:09:31.179201 11101 solver.cpp:334]     Train net output #0: loss = 1.22803 (* 1 = 1.22803 loss)
I0815 05:09:31.179205 11101 sgd_solver.cpp:136] Iteration 214000, lr = 0.033125, m = 0.9
I0815 05:09:45.606015 11101 solver.cpp:312] Iteration 214100 (6.93172 iter/s, 14.4264s/100 iter), loss = 1.80748
I0815 05:09:45.606266 11101 solver.cpp:334]     Train net output #0: loss = 1.93317 (* 1 = 1.93317 loss)
I0815 05:09:45.606374 11101 sgd_solver.cpp:136] Iteration 214100, lr = 0.0330938, m = 0.9
I0815 05:10:00.120162 11101 solver.cpp:312] Iteration 214200 (6.89002 iter/s, 14.5137s/100 iter), loss = 2.12388
I0815 05:10:00.120213 11101 solver.cpp:334]     Train net output #0: loss = 2.07041 (* 1 = 2.07041 loss)
I0815 05:10:00.120219 11101 sgd_solver.cpp:136] Iteration 214200, lr = 0.0330625, m = 0.9
I0815 05:10:14.657330 11101 solver.cpp:312] Iteration 214300 (6.87911 iter/s, 14.5368s/100 iter), loss = 1.51566
I0815 05:10:14.657359 11101 solver.cpp:334]     Train net output #0: loss = 1.35912 (* 1 = 1.35912 loss)
I0815 05:10:14.657366 11101 sgd_solver.cpp:136] Iteration 214300, lr = 0.0330313, m = 0.9
I0815 05:10:29.254374 11101 solver.cpp:312] Iteration 214400 (6.85089 iter/s, 14.5966s/100 iter), loss = 1.63119
I0815 05:10:29.254604 11101 solver.cpp:334]     Train net output #0: loss = 2.09826 (* 1 = 2.09826 loss)
I0815 05:10:29.254709 11101 sgd_solver.cpp:136] Iteration 214400, lr = 0.033, m = 0.9
I0815 05:10:43.764219 11101 solver.cpp:312] Iteration 214500 (6.89206 iter/s, 14.5094s/100 iter), loss = 1.47981
I0815 05:10:43.764245 11101 solver.cpp:334]     Train net output #0: loss = 1.83271 (* 1 = 1.83271 loss)
I0815 05:10:43.764250 11101 sgd_solver.cpp:136] Iteration 214500, lr = 0.0329687, m = 0.9
I0815 05:10:58.376637 11101 solver.cpp:312] Iteration 214600 (6.84368 iter/s, 14.612s/100 iter), loss = 2.56969
I0815 05:10:58.376663 11101 solver.cpp:334]     Train net output #0: loss = 2.30348 (* 1 = 2.30348 loss)
I0815 05:10:58.376669 11101 sgd_solver.cpp:136] Iteration 214600, lr = 0.0329375, m = 0.9
I0815 05:11:12.841537 11101 solver.cpp:312] Iteration 214700 (6.91348 iter/s, 14.4645s/100 iter), loss = 1.96228
I0815 05:11:12.841603 11101 solver.cpp:334]     Train net output #0: loss = 1.6757 (* 1 = 1.6757 loss)
I0815 05:11:12.841609 11101 sgd_solver.cpp:136] Iteration 214700, lr = 0.0329063, m = 0.9
I0815 05:11:27.333729 11101 solver.cpp:312] Iteration 214800 (6.90046 iter/s, 14.4918s/100 iter), loss = 2.1574
I0815 05:11:27.333778 11101 solver.cpp:334]     Train net output #0: loss = 2.3395 (* 1 = 2.3395 loss)
I0815 05:11:27.333792 11101 sgd_solver.cpp:136] Iteration 214800, lr = 0.032875, m = 0.9
I0815 05:11:42.181879 11101 solver.cpp:312] Iteration 214900 (6.73503 iter/s, 14.8477s/100 iter), loss = 1.97875
I0815 05:11:42.181942 11101 solver.cpp:334]     Train net output #0: loss = 1.61789 (* 1 = 1.61789 loss)
I0815 05:11:42.181959 11101 sgd_solver.cpp:136] Iteration 214900, lr = 0.0328437, m = 0.9
I0815 05:11:56.784749 11101 solver.cpp:312] Iteration 215000 (6.84816 iter/s, 14.6025s/100 iter), loss = 1.65833
I0815 05:11:56.784814 11101 solver.cpp:334]     Train net output #0: loss = 1.29703 (* 1 = 1.29703 loss)
I0815 05:11:56.784821 11101 sgd_solver.cpp:136] Iteration 215000, lr = 0.0328125, m = 0.9
I0815 05:12:11.501276 11101 solver.cpp:312] Iteration 215100 (6.79527 iter/s, 14.7161s/100 iter), loss = 1.97568
I0815 05:12:11.501354 11101 solver.cpp:334]     Train net output #0: loss = 1.93462 (* 1 = 1.93462 loss)
I0815 05:12:11.501372 11101 sgd_solver.cpp:136] Iteration 215100, lr = 0.0327813, m = 0.9
I0815 05:12:26.214082 11101 solver.cpp:312] Iteration 215200 (6.79699 iter/s, 14.7124s/100 iter), loss = 1.46071
I0815 05:12:26.214109 11101 solver.cpp:334]     Train net output #0: loss = 1.47114 (* 1 = 1.47114 loss)
I0815 05:12:26.214115 11101 sgd_solver.cpp:136] Iteration 215200, lr = 0.03275, m = 0.9
I0815 05:12:40.938050 11101 solver.cpp:312] Iteration 215300 (6.79184 iter/s, 14.7236s/100 iter), loss = 1.86562
I0815 05:12:40.944146 11101 solver.cpp:334]     Train net output #0: loss = 1.74278 (* 1 = 1.74278 loss)
I0815 05:12:40.944175 11101 sgd_solver.cpp:136] Iteration 215300, lr = 0.0327187, m = 0.9
I0815 05:12:55.509292 11101 solver.cpp:312] Iteration 215400 (6.86302 iter/s, 14.5708s/100 iter), loss = 2.62466
I0815 05:12:55.509332 11101 solver.cpp:334]     Train net output #0: loss = 2.43802 (* 1 = 2.43802 loss)
I0815 05:12:55.509353 11101 sgd_solver.cpp:136] Iteration 215400, lr = 0.0326875, m = 0.9
I0815 05:13:10.214706 11101 solver.cpp:312] Iteration 215500 (6.8004 iter/s, 14.705s/100 iter), loss = 1.60939
I0815 05:13:10.214771 11101 solver.cpp:334]     Train net output #0: loss = 1.57393 (* 1 = 1.57393 loss)
I0815 05:13:10.214787 11101 sgd_solver.cpp:136] Iteration 215500, lr = 0.0326563, m = 0.9
I0815 05:13:24.922133 11101 solver.cpp:312] Iteration 215600 (6.79947 iter/s, 14.707s/100 iter), loss = 1.8944
I0815 05:13:24.922196 11101 solver.cpp:334]     Train net output #0: loss = 1.97946 (* 1 = 1.97946 loss)
I0815 05:13:24.922204 11101 sgd_solver.cpp:136] Iteration 215600, lr = 0.032625, m = 0.9
I0815 05:13:39.798766 11101 solver.cpp:312] Iteration 215700 (6.72214 iter/s, 14.8762s/100 iter), loss = 1.88058
I0815 05:13:39.798794 11101 solver.cpp:334]     Train net output #0: loss = 1.57472 (* 1 = 1.57472 loss)
I0815 05:13:39.798800 11101 sgd_solver.cpp:136] Iteration 215700, lr = 0.0325938, m = 0.9
I0815 05:13:54.489656 11101 solver.cpp:312] Iteration 215800 (6.80713 iter/s, 14.6905s/100 iter), loss = 1.93778
I0815 05:13:54.489677 11101 solver.cpp:334]     Train net output #0: loss = 1.93272 (* 1 = 1.93272 loss)
I0815 05:13:54.489681 11101 sgd_solver.cpp:136] Iteration 215800, lr = 0.0325625, m = 0.9
I0815 05:14:09.332922 11101 solver.cpp:312] Iteration 215900 (6.73725 iter/s, 14.8429s/100 iter), loss = 1.77136
I0815 05:14:09.332981 11101 solver.cpp:334]     Train net output #0: loss = 1.94941 (* 1 = 1.94941 loss)
I0815 05:14:09.332990 11101 sgd_solver.cpp:136] Iteration 215900, lr = 0.0325313, m = 0.9
I0815 05:14:24.267855 11101 solver.cpp:509] Iteration 216000, Testing net (#0)
I0815 05:14:45.661628 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.492412
I0815 05:14:45.661681 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.738763
I0815 05:14:45.661687 11101 solver.cpp:594]     Test net output #2: loss = 2.29742 (* 1 = 2.29742 loss)
I0815 05:14:45.661737 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3933s
I0815 05:14:45.821851 11101 solver.cpp:312] Iteration 216000 (2.74063 iter/s, 36.4879s/100 iter), loss = 1.33757
I0815 05:14:45.821874 11101 solver.cpp:334]     Train net output #0: loss = 1.47096 (* 1 = 1.47096 loss)
I0815 05:14:45.821878 11101 sgd_solver.cpp:136] Iteration 216000, lr = 0.0325, m = 0.9
I0815 05:15:00.453932 11101 solver.cpp:312] Iteration 216100 (6.83449 iter/s, 14.6317s/100 iter), loss = 1.61896
I0815 05:15:00.454175 11101 solver.cpp:334]     Train net output #0: loss = 1.61605 (* 1 = 1.61605 loss)
I0815 05:15:00.454185 11101 sgd_solver.cpp:136] Iteration 216100, lr = 0.0324687, m = 0.9
I0815 05:15:15.320312 11101 solver.cpp:312] Iteration 216200 (6.72678 iter/s, 14.866s/100 iter), loss = 1.56233
I0815 05:15:15.320348 11101 solver.cpp:334]     Train net output #0: loss = 1.53492 (* 1 = 1.53492 loss)
I0815 05:15:15.320356 11101 sgd_solver.cpp:136] Iteration 216200, lr = 0.0324375, m = 0.9
I0815 05:15:29.832612 11101 solver.cpp:312] Iteration 216300 (6.8909 iter/s, 14.5119s/100 iter), loss = 2.04649
I0815 05:15:29.833037 11101 solver.cpp:334]     Train net output #0: loss = 1.83676 (* 1 = 1.83676 loss)
I0815 05:15:29.833045 11101 sgd_solver.cpp:136] Iteration 216300, lr = 0.0324063, m = 0.9
I0815 05:15:44.352702 11101 solver.cpp:312] Iteration 216400 (6.8872 iter/s, 14.5197s/100 iter), loss = 1.82177
I0815 05:15:44.352727 11101 solver.cpp:334]     Train net output #0: loss = 2.13432 (* 1 = 2.13432 loss)
I0815 05:15:44.352732 11101 sgd_solver.cpp:136] Iteration 216400, lr = 0.032375, m = 0.9
I0815 05:15:59.293556 11101 solver.cpp:312] Iteration 216500 (6.69324 iter/s, 14.9404s/100 iter), loss = 1.96446
I0815 05:15:59.293583 11101 solver.cpp:334]     Train net output #0: loss = 1.76152 (* 1 = 1.76152 loss)
I0815 05:15:59.293589 11101 sgd_solver.cpp:136] Iteration 216500, lr = 0.0323438, m = 0.9
I0815 05:16:13.791015 11101 solver.cpp:312] Iteration 216600 (6.89795 iter/s, 14.4971s/100 iter), loss = 2.1894
I0815 05:16:13.791101 11101 solver.cpp:334]     Train net output #0: loss = 2.78701 (* 1 = 2.78701 loss)
I0815 05:16:13.791115 11101 sgd_solver.cpp:136] Iteration 216600, lr = 0.0323125, m = 0.9
I0815 05:16:28.383008 11101 solver.cpp:312] Iteration 216700 (6.85326 iter/s, 14.5916s/100 iter), loss = 1.79321
I0815 05:16:28.383078 11101 solver.cpp:334]     Train net output #0: loss = 1.98926 (* 1 = 1.98926 loss)
I0815 05:16:28.383096 11101 sgd_solver.cpp:136] Iteration 216700, lr = 0.0322812, m = 0.9
I0815 05:16:42.944142 11101 solver.cpp:312] Iteration 216800 (6.86779 iter/s, 14.5607s/100 iter), loss = 1.92704
I0815 05:16:42.944170 11101 solver.cpp:334]     Train net output #0: loss = 1.85993 (* 1 = 1.85993 loss)
I0815 05:16:42.944176 11101 sgd_solver.cpp:136] Iteration 216800, lr = 0.03225, m = 0.9
I0815 05:16:57.523342 11101 solver.cpp:312] Iteration 216900 (6.85928 iter/s, 14.5788s/100 iter), loss = 1.52615
I0815 05:16:57.523419 11101 solver.cpp:334]     Train net output #0: loss = 1.33826 (* 1 = 1.33826 loss)
I0815 05:16:57.523427 11101 sgd_solver.cpp:136] Iteration 216900, lr = 0.0322188, m = 0.9
I0815 05:17:12.352665 11101 solver.cpp:312] Iteration 217000 (6.74358 iter/s, 14.8289s/100 iter), loss = 1.59182
I0815 05:17:12.352691 11101 solver.cpp:334]     Train net output #0: loss = 1.73182 (* 1 = 1.73182 loss)
I0815 05:17:12.352695 11101 sgd_solver.cpp:136] Iteration 217000, lr = 0.0321875, m = 0.9
I0815 05:17:26.953397 11101 solver.cpp:312] Iteration 217100 (6.84916 iter/s, 14.6003s/100 iter), loss = 1.65963
I0815 05:17:26.953469 11101 solver.cpp:334]     Train net output #0: loss = 1.61912 (* 1 = 1.61912 loss)
I0815 05:17:26.953490 11101 sgd_solver.cpp:136] Iteration 217100, lr = 0.0321563, m = 0.9
I0815 05:17:41.450577 11101 solver.cpp:312] Iteration 217200 (6.89809 iter/s, 14.4968s/100 iter), loss = 1.67702
I0815 05:17:41.450673 11101 solver.cpp:334]     Train net output #0: loss = 1.97241 (* 1 = 1.97241 loss)
I0815 05:17:41.450685 11101 sgd_solver.cpp:136] Iteration 217200, lr = 0.032125, m = 0.9
I0815 05:17:56.080991 11101 solver.cpp:312] Iteration 217300 (6.83526 iter/s, 14.63s/100 iter), loss = 1.83997
I0815 05:17:56.081013 11101 solver.cpp:334]     Train net output #0: loss = 1.96866 (* 1 = 1.96866 loss)
I0815 05:17:56.081018 11101 sgd_solver.cpp:136] Iteration 217300, lr = 0.0320938, m = 0.9
I0815 05:18:10.878676 11101 solver.cpp:312] Iteration 217400 (6.758 iter/s, 14.7973s/100 iter), loss = 1.97811
I0815 05:18:10.878697 11101 solver.cpp:334]     Train net output #0: loss = 1.99107 (* 1 = 1.99107 loss)
I0815 05:18:10.878702 11101 sgd_solver.cpp:136] Iteration 217400, lr = 0.0320625, m = 0.9
I0815 05:18:25.669481 11101 solver.cpp:312] Iteration 217500 (6.76114 iter/s, 14.7904s/100 iter), loss = 1.76654
I0815 05:18:25.669728 11101 solver.cpp:334]     Train net output #0: loss = 1.68549 (* 1 = 1.68549 loss)
I0815 05:18:25.669838 11101 sgd_solver.cpp:136] Iteration 217500, lr = 0.0320312, m = 0.9
I0815 05:18:40.348744 11101 solver.cpp:312] Iteration 217600 (6.81252 iter/s, 14.6789s/100 iter), loss = 2.10358
I0815 05:18:40.348899 11101 solver.cpp:334]     Train net output #0: loss = 1.76898 (* 1 = 1.76898 loss)
I0815 05:18:40.348925 11101 sgd_solver.cpp:136] Iteration 217600, lr = 0.032, m = 0.9
I0815 05:18:55.076011 11101 solver.cpp:312] Iteration 217700 (6.79031 iter/s, 14.7269s/100 iter), loss = 1.44883
I0815 05:18:55.076040 11101 solver.cpp:334]     Train net output #0: loss = 1.56866 (* 1 = 1.56866 loss)
I0815 05:18:55.076045 11101 sgd_solver.cpp:136] Iteration 217700, lr = 0.0319688, m = 0.9
I0815 05:19:09.744359 11101 solver.cpp:312] Iteration 217800 (6.81759 iter/s, 14.6679s/100 iter), loss = 1.88099
I0815 05:19:09.744436 11101 solver.cpp:334]     Train net output #0: loss = 2.01753 (* 1 = 2.01753 loss)
I0815 05:19:09.744442 11101 sgd_solver.cpp:136] Iteration 217800, lr = 0.0319375, m = 0.9
I0815 05:19:24.668107 11101 solver.cpp:312] Iteration 217900 (6.70091 iter/s, 14.9233s/100 iter), loss = 1.68278
I0815 05:19:24.668136 11101 solver.cpp:334]     Train net output #0: loss = 1.62489 (* 1 = 1.62489 loss)
I0815 05:19:24.668143 11101 sgd_solver.cpp:136] Iteration 217900, lr = 0.0319062, m = 0.9
I0815 05:19:39.098917 11101 solver.cpp:509] Iteration 218000, Testing net (#0)
I0815 05:20:00.167595 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.472706
I0815 05:20:00.167649 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.72841
I0815 05:20:00.167656 11101 solver.cpp:594]     Test net output #2: loss = 2.36172 (* 1 = 2.36172 loss)
I0815 05:20:00.167703 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0682s
I0815 05:20:00.330319 11101 solver.cpp:312] Iteration 218000 (2.80417 iter/s, 35.6612s/100 iter), loss = 2.11681
I0815 05:20:00.330379 11101 solver.cpp:334]     Train net output #0: loss = 1.74832 (* 1 = 1.74832 loss)
I0815 05:20:00.330392 11101 sgd_solver.cpp:136] Iteration 218000, lr = 0.031875, m = 0.9
I0815 05:20:14.922142 11101 solver.cpp:312] Iteration 218100 (6.85335 iter/s, 14.5914s/100 iter), loss = 1.76107
I0815 05:20:14.922166 11101 solver.cpp:334]     Train net output #0: loss = 1.24614 (* 1 = 1.24614 loss)
I0815 05:20:14.922171 11101 sgd_solver.cpp:136] Iteration 218100, lr = 0.0318438, m = 0.9
I0815 05:20:29.909844 11101 solver.cpp:312] Iteration 218200 (6.67232 iter/s, 14.9873s/100 iter), loss = 1.99714
I0815 05:20:29.909870 11101 solver.cpp:334]     Train net output #0: loss = 1.67117 (* 1 = 1.67117 loss)
I0815 05:20:29.909876 11101 sgd_solver.cpp:136] Iteration 218200, lr = 0.0318125, m = 0.9
I0815 05:20:44.954759 11101 solver.cpp:312] Iteration 218300 (6.64695 iter/s, 15.0445s/100 iter), loss = 1.8576
I0815 05:20:44.954844 11101 solver.cpp:334]     Train net output #0: loss = 1.77835 (* 1 = 1.77835 loss)
I0815 05:20:44.954861 11101 sgd_solver.cpp:136] Iteration 218300, lr = 0.0317813, m = 0.9
I0815 05:20:59.479703 11101 solver.cpp:312] Iteration 218400 (6.8849 iter/s, 14.5245s/100 iter), loss = 1.73594
I0815 05:20:59.479732 11101 solver.cpp:334]     Train net output #0: loss = 2.03172 (* 1 = 2.03172 loss)
I0815 05:20:59.479737 11101 sgd_solver.cpp:136] Iteration 218400, lr = 0.03175, m = 0.9
I0815 05:21:13.889875 11101 solver.cpp:312] Iteration 218500 (6.93974 iter/s, 14.4098s/100 iter), loss = 1.86734
I0815 05:21:13.889902 11101 solver.cpp:334]     Train net output #0: loss = 1.32239 (* 1 = 1.32239 loss)
I0815 05:21:13.889909 11101 sgd_solver.cpp:136] Iteration 218500, lr = 0.0317187, m = 0.9
I0815 05:21:28.371217 11101 solver.cpp:312] Iteration 218600 (6.90563 iter/s, 14.4809s/100 iter), loss = 1.71996
I0815 05:21:28.371281 11101 solver.cpp:334]     Train net output #0: loss = 1.91781 (* 1 = 1.91781 loss)
I0815 05:21:28.371289 11101 sgd_solver.cpp:136] Iteration 218600, lr = 0.0316875, m = 0.9
I0815 05:21:42.787567 11101 solver.cpp:312] Iteration 218700 (6.93676 iter/s, 14.4159s/100 iter), loss = 1.75872
I0815 05:21:42.787595 11101 solver.cpp:334]     Train net output #0: loss = 1.76092 (* 1 = 1.76092 loss)
I0815 05:21:42.787600 11101 sgd_solver.cpp:136] Iteration 218700, lr = 0.0316562, m = 0.9
I0815 05:21:57.294437 11101 solver.cpp:312] Iteration 218800 (6.89348 iter/s, 14.5065s/100 iter), loss = 1.99243
I0815 05:21:57.294509 11101 solver.cpp:334]     Train net output #0: loss = 1.99602 (* 1 = 1.99602 loss)
I0815 05:21:57.294528 11101 sgd_solver.cpp:136] Iteration 218800, lr = 0.031625, m = 0.9
I0815 05:22:11.915345 11101 solver.cpp:312] Iteration 218900 (6.83971 iter/s, 14.6205s/100 iter), loss = 2.04053
I0815 05:22:11.915457 11101 solver.cpp:334]     Train net output #0: loss = 1.83353 (* 1 = 1.83353 loss)
I0815 05:22:11.915475 11101 sgd_solver.cpp:136] Iteration 218900, lr = 0.0315938, m = 0.9
I0815 05:22:26.587726 11101 solver.cpp:312] Iteration 219000 (6.81572 iter/s, 14.672s/100 iter), loss = 1.81577
I0815 05:22:26.587781 11101 solver.cpp:334]     Train net output #0: loss = 1.95475 (* 1 = 1.95475 loss)
I0815 05:22:26.587795 11101 sgd_solver.cpp:136] Iteration 219000, lr = 0.0315625, m = 0.9
I0815 05:22:41.223440 11101 solver.cpp:312] Iteration 219100 (6.83279 iter/s, 14.6353s/100 iter), loss = 1.50645
I0815 05:22:41.223464 11101 solver.cpp:334]     Train net output #0: loss = 1.1952 (* 1 = 1.1952 loss)
I0815 05:22:41.223469 11101 sgd_solver.cpp:136] Iteration 219100, lr = 0.0315313, m = 0.9
I0815 05:22:55.857000 11101 solver.cpp:312] Iteration 219200 (6.8338 iter/s, 14.6332s/100 iter), loss = 1.59564
I0815 05:22:55.857101 11101 solver.cpp:334]     Train net output #0: loss = 1.69063 (* 1 = 1.69063 loss)
I0815 05:22:55.857120 11101 sgd_solver.cpp:136] Iteration 219200, lr = 0.0315, m = 0.9
I0815 05:23:10.802561 11101 solver.cpp:312] Iteration 219300 (6.69114 iter/s, 14.9451s/100 iter), loss = 1.87665
I0815 05:23:10.802587 11101 solver.cpp:334]     Train net output #0: loss = 1.74547 (* 1 = 1.74547 loss)
I0815 05:23:10.802590 11101 sgd_solver.cpp:136] Iteration 219300, lr = 0.0314687, m = 0.9
I0815 05:23:25.279284 11101 solver.cpp:312] Iteration 219400 (6.90783 iter/s, 14.4763s/100 iter), loss = 1.6792
I0815 05:23:25.279312 11101 solver.cpp:334]     Train net output #0: loss = 1.72965 (* 1 = 1.72965 loss)
I0815 05:23:25.279317 11101 sgd_solver.cpp:136] Iteration 219400, lr = 0.0314375, m = 0.9
I0815 05:23:39.795713 11101 solver.cpp:312] Iteration 219500 (6.88894 iter/s, 14.516s/100 iter), loss = 1.77784
I0815 05:23:39.795807 11101 solver.cpp:334]     Train net output #0: loss = 1.81841 (* 1 = 1.81841 loss)
I0815 05:23:39.795825 11101 sgd_solver.cpp:136] Iteration 219500, lr = 0.0314062, m = 0.9
I0815 05:23:54.270067 11101 solver.cpp:312] Iteration 219600 (6.90896 iter/s, 14.474s/100 iter), loss = 1.57263
I0815 05:23:54.270098 11101 solver.cpp:334]     Train net output #0: loss = 1.6279 (* 1 = 1.6279 loss)
I0815 05:23:54.270105 11101 sgd_solver.cpp:136] Iteration 219600, lr = 0.031375, m = 0.9
I0815 05:24:08.997864 11101 solver.cpp:312] Iteration 219700 (6.79007 iter/s, 14.7274s/100 iter), loss = 1.54783
I0815 05:24:08.997932 11101 solver.cpp:334]     Train net output #0: loss = 1.53357 (* 1 = 1.53357 loss)
I0815 05:24:08.997952 11101 sgd_solver.cpp:136] Iteration 219700, lr = 0.0313438, m = 0.9
I0815 05:24:23.921447 11101 solver.cpp:312] Iteration 219800 (6.70099 iter/s, 14.9232s/100 iter), loss = 1.96268
I0815 05:24:23.921505 11101 solver.cpp:334]     Train net output #0: loss = 1.83962 (* 1 = 1.83962 loss)
I0815 05:24:23.921512 11101 sgd_solver.cpp:136] Iteration 219800, lr = 0.0313125, m = 0.9
I0815 05:24:39.074590 11101 solver.cpp:312] Iteration 219900 (6.59947 iter/s, 15.1527s/100 iter), loss = 2.17742
I0815 05:24:39.074615 11101 solver.cpp:334]     Train net output #0: loss = 2.22129 (* 1 = 2.22129 loss)
I0815 05:24:39.074620 11101 sgd_solver.cpp:136] Iteration 219900, lr = 0.0312813, m = 0.9
I0815 05:24:53.913584 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_220000.caffemodel
I0815 05:24:53.954215 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_220000.solverstate
I0815 05:24:53.958767 11101 solver.cpp:509] Iteration 220000, Testing net (#0)
I0815 05:25:03.985817 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 05:25:15.316123 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.491706
I0815 05:25:15.316155 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.743057
I0815 05:25:15.316164 11101 solver.cpp:594]     Test net output #2: loss = 2.28817 (* 1 = 2.28817 loss)
I0815 05:25:15.316184 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3568s
I0815 05:25:15.463239 11101 solver.cpp:312] Iteration 220000 (2.74818 iter/s, 36.3877s/100 iter), loss = 1.74307
I0815 05:25:15.463266 11101 solver.cpp:334]     Train net output #0: loss = 1.80733 (* 1 = 1.80733 loss)
I0815 05:25:15.463273 11101 sgd_solver.cpp:136] Iteration 220000, lr = 0.03125, m = 0.9
I0815 05:25:30.004716 11101 solver.cpp:312] Iteration 220100 (6.87707 iter/s, 14.5411s/100 iter), loss = 1.99889
I0815 05:25:30.004796 11101 solver.cpp:334]     Train net output #0: loss = 2.61968 (* 1 = 2.61968 loss)
I0815 05:25:30.004808 11101 sgd_solver.cpp:136] Iteration 220100, lr = 0.0312188, m = 0.9
I0815 05:25:44.960815 11101 solver.cpp:312] Iteration 220200 (6.68642 iter/s, 14.9557s/100 iter), loss = 2.00195
I0815 05:25:44.960891 11101 solver.cpp:334]     Train net output #0: loss = 1.96332 (* 1 = 1.96332 loss)
I0815 05:25:44.960911 11101 sgd_solver.cpp:136] Iteration 220200, lr = 0.0311875, m = 0.9
I0815 05:25:59.461263 11101 solver.cpp:312] Iteration 220300 (6.89653 iter/s, 14.5s/100 iter), loss = 1.87603
I0815 05:25:59.461283 11101 solver.cpp:334]     Train net output #0: loss = 1.80628 (* 1 = 1.80628 loss)
I0815 05:25:59.461289 11101 sgd_solver.cpp:136] Iteration 220300, lr = 0.0311562, m = 0.9
I0815 05:26:14.319433 11101 solver.cpp:312] Iteration 220400 (6.73049 iter/s, 14.8578s/100 iter), loss = 1.6717
I0815 05:26:14.324185 11101 solver.cpp:334]     Train net output #0: loss = 1.52266 (* 1 = 1.52266 loss)
I0815 05:26:14.324204 11101 sgd_solver.cpp:136] Iteration 220400, lr = 0.031125, m = 0.9
I0815 05:26:29.099135 11101 solver.cpp:312] Iteration 220500 (6.76622 iter/s, 14.7793s/100 iter), loss = 1.69314
I0815 05:26:29.099207 11101 solver.cpp:334]     Train net output #0: loss = 1.78761 (* 1 = 1.78761 loss)
I0815 05:26:29.099230 11101 sgd_solver.cpp:136] Iteration 220500, lr = 0.0310938, m = 0.9
I0815 05:26:44.152096 11101 solver.cpp:312] Iteration 220600 (6.6434 iter/s, 15.0525s/100 iter), loss = 1.76446
I0815 05:26:44.154199 11101 solver.cpp:334]     Train net output #0: loss = 1.72344 (* 1 = 1.72344 loss)
I0815 05:26:44.154218 11101 sgd_solver.cpp:136] Iteration 220600, lr = 0.0310625, m = 0.9
I0815 05:26:58.907348 11101 solver.cpp:312] Iteration 220700 (6.77743 iter/s, 14.7548s/100 iter), loss = 1.7892
I0815 05:26:58.907429 11101 solver.cpp:334]     Train net output #0: loss = 2.00535 (* 1 = 2.00535 loss)
I0815 05:26:58.907441 11101 sgd_solver.cpp:136] Iteration 220700, lr = 0.0310313, m = 0.9
I0815 05:27:13.310698 11101 solver.cpp:312] Iteration 220800 (6.94302 iter/s, 14.403s/100 iter), loss = 1.51171
I0815 05:27:13.310765 11101 solver.cpp:334]     Train net output #0: loss = 1.43804 (* 1 = 1.43804 loss)
I0815 05:27:13.310782 11101 sgd_solver.cpp:136] Iteration 220800, lr = 0.031, m = 0.9
I0815 05:27:27.875109 11101 solver.cpp:312] Iteration 220900 (6.86624 iter/s, 14.564s/100 iter), loss = 1.97684
I0815 05:27:27.875135 11101 solver.cpp:334]     Train net output #0: loss = 2.33338 (* 1 = 2.33338 loss)
I0815 05:27:27.875139 11101 sgd_solver.cpp:136] Iteration 220900, lr = 0.0309687, m = 0.9
I0815 05:27:42.566288 11101 solver.cpp:312] Iteration 221000 (6.80699 iter/s, 14.6908s/100 iter), loss = 1.42604
I0815 05:27:42.566382 11101 solver.cpp:334]     Train net output #0: loss = 1.38101 (* 1 = 1.38101 loss)
I0815 05:27:42.566401 11101 sgd_solver.cpp:136] Iteration 221000, lr = 0.0309375, m = 0.9
I0815 05:27:56.970294 11101 solver.cpp:312] Iteration 221100 (6.9427 iter/s, 14.4036s/100 iter), loss = 1.73733
I0815 05:27:56.970360 11101 solver.cpp:334]     Train net output #0: loss = 1.55741 (* 1 = 1.55741 loss)
I0815 05:27:56.970378 11101 sgd_solver.cpp:136] Iteration 221100, lr = 0.0309062, m = 0.9
I0815 05:28:11.905272 11101 solver.cpp:312] Iteration 221200 (6.69588 iter/s, 14.9346s/100 iter), loss = 1.6546
I0815 05:28:11.905299 11101 solver.cpp:334]     Train net output #0: loss = 1.87197 (* 1 = 1.87197 loss)
I0815 05:28:11.905308 11101 sgd_solver.cpp:136] Iteration 221200, lr = 0.030875, m = 0.9
I0815 05:28:26.863373 11101 solver.cpp:312] Iteration 221300 (6.68553 iter/s, 14.9577s/100 iter), loss = 1.36686
I0815 05:28:26.864214 11101 solver.cpp:334]     Train net output #0: loss = 1.14619 (* 1 = 1.14619 loss)
I0815 05:28:26.864223 11101 sgd_solver.cpp:136] Iteration 221300, lr = 0.0308438, m = 0.9
I0815 05:28:41.206678 11101 solver.cpp:312] Iteration 221400 (6.97209 iter/s, 14.3429s/100 iter), loss = 1.52328
I0815 05:28:41.206894 11101 solver.cpp:334]     Train net output #0: loss = 1.7227 (* 1 = 1.7227 loss)
I0815 05:28:41.207002 11101 sgd_solver.cpp:136] Iteration 221400, lr = 0.0308125, m = 0.9
I0815 05:28:55.625071 11101 solver.cpp:312] Iteration 221500 (6.93578 iter/s, 14.418s/100 iter), loss = 1.70692
I0815 05:28:55.625097 11101 solver.cpp:334]     Train net output #0: loss = 1.72109 (* 1 = 1.72109 loss)
I0815 05:28:55.625102 11101 sgd_solver.cpp:136] Iteration 221500, lr = 0.0307813, m = 0.9
I0815 05:29:10.289151 11101 solver.cpp:312] Iteration 221600 (6.81957 iter/s, 14.6637s/100 iter), loss = 1.60554
I0815 05:29:10.289212 11101 solver.cpp:334]     Train net output #0: loss = 1.46526 (* 1 = 1.46526 loss)
I0815 05:29:10.289219 11101 sgd_solver.cpp:136] Iteration 221600, lr = 0.03075, m = 0.9
I0815 05:29:24.825659 11101 solver.cpp:312] Iteration 221700 (6.87942 iter/s, 14.5361s/100 iter), loss = 1.58374
I0815 05:29:24.825687 11101 solver.cpp:334]     Train net output #0: loss = 1.41128 (* 1 = 1.41128 loss)
I0815 05:29:24.825695 11101 sgd_solver.cpp:136] Iteration 221700, lr = 0.0307187, m = 0.9
I0815 05:29:39.302359 11101 solver.cpp:312] Iteration 221800 (6.90784 iter/s, 14.4763s/100 iter), loss = 1.64686
I0815 05:29:39.302538 11101 solver.cpp:334]     Train net output #0: loss = 1.72693 (* 1 = 1.72693 loss)
I0815 05:29:39.302621 11101 sgd_solver.cpp:136] Iteration 221800, lr = 0.0306875, m = 0.9
I0815 05:29:53.905375 11101 solver.cpp:312] Iteration 221900 (6.84809 iter/s, 14.6026s/100 iter), loss = 1.759
I0815 05:29:53.905553 11101 solver.cpp:334]     Train net output #0: loss = 1.71822 (* 1 = 1.71822 loss)
I0815 05:29:53.905575 11101 sgd_solver.cpp:136] Iteration 221900, lr = 0.0306562, m = 0.9
I0815 05:30:08.656580 11101 solver.cpp:509] Iteration 222000, Testing net (#0)
I0815 05:30:29.737303 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.498294
I0815 05:30:29.737385 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.750175
I0815 05:30:29.737395 11101 solver.cpp:594]     Test net output #2: loss = 2.24356 (* 1 = 2.24356 loss)
I0815 05:30:29.737416 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0803s
I0815 05:30:29.890552 11101 solver.cpp:312] Iteration 222000 (2.779 iter/s, 35.9842s/100 iter), loss = 2.06052
I0815 05:30:29.890612 11101 solver.cpp:334]     Train net output #0: loss = 1.87978 (* 1 = 1.87978 loss)
I0815 05:30:29.890630 11101 sgd_solver.cpp:136] Iteration 222000, lr = 0.030625, m = 0.9
I0815 05:30:44.518252 11101 solver.cpp:312] Iteration 222100 (6.83653 iter/s, 14.6273s/100 iter), loss = 1.66223
I0815 05:30:44.518280 11101 solver.cpp:334]     Train net output #0: loss = 1.36735 (* 1 = 1.36735 loss)
I0815 05:30:44.518285 11101 sgd_solver.cpp:136] Iteration 222100, lr = 0.0305938, m = 0.9
I0815 05:30:59.311844 11101 solver.cpp:312] Iteration 222200 (6.75987 iter/s, 14.7932s/100 iter), loss = 1.64563
I0815 05:30:59.311880 11101 solver.cpp:334]     Train net output #0: loss = 1.26667 (* 1 = 1.26667 loss)
I0815 05:30:59.311887 11101 sgd_solver.cpp:136] Iteration 222200, lr = 0.0305625, m = 0.9
I0815 05:31:13.959410 11101 solver.cpp:312] Iteration 222300 (6.82726 iter/s, 14.6472s/100 iter), loss = 1.6333
I0815 05:31:13.959477 11101 solver.cpp:334]     Train net output #0: loss = 1.4157 (* 1 = 1.4157 loss)
I0815 05:31:13.959482 11101 sgd_solver.cpp:136] Iteration 222300, lr = 0.0305313, m = 0.9
I0815 05:31:28.519189 11101 solver.cpp:312] Iteration 222400 (6.86843 iter/s, 14.5594s/100 iter), loss = 1.6288
I0815 05:31:28.519261 11101 solver.cpp:334]     Train net output #0: loss = 1.6734 (* 1 = 1.6734 loss)
I0815 05:31:28.519280 11101 sgd_solver.cpp:136] Iteration 222400, lr = 0.0305, m = 0.9
I0815 05:31:43.436177 11101 solver.cpp:312] Iteration 222500 (6.70395 iter/s, 14.9166s/100 iter), loss = 1.86334
I0815 05:31:43.436208 11101 solver.cpp:334]     Train net output #0: loss = 2.17272 (* 1 = 2.17272 loss)
I0815 05:31:43.436213 11101 sgd_solver.cpp:136] Iteration 222500, lr = 0.0304688, m = 0.9
I0815 05:31:58.259589 11101 solver.cpp:312] Iteration 222600 (6.74627 iter/s, 14.823s/100 iter), loss = 1.84632
I0815 05:31:58.259654 11101 solver.cpp:334]     Train net output #0: loss = 1.86398 (* 1 = 1.86398 loss)
I0815 05:31:58.259660 11101 sgd_solver.cpp:136] Iteration 222600, lr = 0.0304375, m = 0.9
I0815 05:32:13.153239 11101 solver.cpp:312] Iteration 222700 (6.71446 iter/s, 14.8932s/100 iter), loss = 1.55608
I0815 05:32:13.153316 11101 solver.cpp:334]     Train net output #0: loss = 1.25774 (* 1 = 1.25774 loss)
I0815 05:32:13.153337 11101 sgd_solver.cpp:136] Iteration 222700, lr = 0.0304062, m = 0.9
I0815 05:32:27.806308 11101 solver.cpp:312] Iteration 222800 (6.8247 iter/s, 14.6527s/100 iter), loss = 1.85468
I0815 05:32:27.806331 11101 solver.cpp:334]     Train net output #0: loss = 1.92267 (* 1 = 1.92267 loss)
I0815 05:32:27.806337 11101 sgd_solver.cpp:136] Iteration 222800, lr = 0.030375, m = 0.9
I0815 05:32:42.505633 11101 solver.cpp:312] Iteration 222900 (6.80322 iter/s, 14.6989s/100 iter), loss = 1.83887
I0815 05:32:42.505741 11101 solver.cpp:334]     Train net output #0: loss = 2.10536 (* 1 = 2.10536 loss)
I0815 05:32:42.505759 11101 sgd_solver.cpp:136] Iteration 222900, lr = 0.0303437, m = 0.9
I0815 05:32:57.275215 11101 solver.cpp:312] Iteration 223000 (6.77086 iter/s, 14.7692s/100 iter), loss = 1.62267
I0815 05:32:57.275243 11101 solver.cpp:334]     Train net output #0: loss = 1.95104 (* 1 = 1.95104 loss)
I0815 05:32:57.275250 11101 sgd_solver.cpp:136] Iteration 223000, lr = 0.0303125, m = 0.9
I0815 05:33:12.065238 11101 solver.cpp:312] Iteration 223100 (6.7615 iter/s, 14.7896s/100 iter), loss = 1.71725
I0815 05:33:12.065264 11101 solver.cpp:334]     Train net output #0: loss = 1.67314 (* 1 = 1.67314 loss)
I0815 05:33:12.065270 11101 sgd_solver.cpp:136] Iteration 223100, lr = 0.0302813, m = 0.9
I0815 05:33:26.684289 11101 solver.cpp:312] Iteration 223200 (6.84058 iter/s, 14.6186s/100 iter), loss = 1.57914
I0815 05:33:26.684358 11101 solver.cpp:334]     Train net output #0: loss = 1.33279 (* 1 = 1.33279 loss)
I0815 05:33:26.684365 11101 sgd_solver.cpp:136] Iteration 223200, lr = 0.03025, m = 0.9
I0815 05:33:41.450837 11101 solver.cpp:312] Iteration 223300 (6.77225 iter/s, 14.7661s/100 iter), loss = 1.83265
I0815 05:33:41.450865 11101 solver.cpp:334]     Train net output #0: loss = 2.18542 (* 1 = 2.18542 loss)
I0815 05:33:41.450870 11101 sgd_solver.cpp:136] Iteration 223300, lr = 0.0302188, m = 0.9
I0815 05:33:55.916519 11101 solver.cpp:312] Iteration 223400 (6.9131 iter/s, 14.4653s/100 iter), loss = 1.93232
I0815 05:33:55.916553 11101 solver.cpp:334]     Train net output #0: loss = 2.38252 (* 1 = 2.38252 loss)
I0815 05:33:55.916558 11101 sgd_solver.cpp:136] Iteration 223400, lr = 0.0301875, m = 0.9
I0815 05:34:10.660374 11101 solver.cpp:312] Iteration 223500 (6.78267 iter/s, 14.7435s/100 iter), loss = 1.7286
I0815 05:34:10.660435 11101 solver.cpp:334]     Train net output #0: loss = 1.60253 (* 1 = 1.60253 loss)
I0815 05:34:10.660442 11101 sgd_solver.cpp:136] Iteration 223500, lr = 0.0301562, m = 0.9
I0815 05:34:25.385294 11101 solver.cpp:312] Iteration 223600 (6.79139 iter/s, 14.7245s/100 iter), loss = 1.79841
I0815 05:34:25.385372 11101 solver.cpp:334]     Train net output #0: loss = 2.39234 (* 1 = 2.39234 loss)
I0815 05:34:25.385392 11101 sgd_solver.cpp:136] Iteration 223600, lr = 0.030125, m = 0.9
I0815 05:34:39.679010 11101 solver.cpp:312] Iteration 223700 (6.99627 iter/s, 14.2933s/100 iter), loss = 1.6805
I0815 05:34:39.679039 11101 solver.cpp:334]     Train net output #0: loss = 1.75447 (* 1 = 1.75447 loss)
I0815 05:34:39.679044 11101 sgd_solver.cpp:136] Iteration 223700, lr = 0.0300937, m = 0.9
I0815 05:34:54.151055 11101 solver.cpp:312] Iteration 223800 (6.91006 iter/s, 14.4717s/100 iter), loss = 2.05133
I0815 05:34:54.151134 11101 solver.cpp:334]     Train net output #0: loss = 2.13917 (* 1 = 2.13917 loss)
I0815 05:34:54.151141 11101 sgd_solver.cpp:136] Iteration 223800, lr = 0.0300625, m = 0.9
I0815 05:35:08.635375 11101 solver.cpp:312] Iteration 223900 (6.90421 iter/s, 14.4839s/100 iter), loss = 1.59682
I0815 05:35:08.635440 11101 solver.cpp:334]     Train net output #0: loss = 1.50562 (* 1 = 1.50562 loss)
I0815 05:35:08.635457 11101 sgd_solver.cpp:136] Iteration 223900, lr = 0.0300313, m = 0.9
I0815 05:35:23.159358 11101 solver.cpp:509] Iteration 224000, Testing net (#0)
I0815 05:35:43.862238 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.489235
I0815 05:35:43.862313 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.743292
I0815 05:35:43.862321 11101 solver.cpp:594]     Test net output #2: loss = 2.27409 (* 1 = 2.27409 loss)
I0815 05:35:43.862340 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7024s
I0815 05:35:44.021380 11101 solver.cpp:312] Iteration 224000 (2.82605 iter/s, 35.385s/100 iter), loss = 1.80002
I0815 05:35:44.021404 11101 solver.cpp:334]     Train net output #0: loss = 1.34828 (* 1 = 1.34828 loss)
I0815 05:35:44.021409 11101 sgd_solver.cpp:136] Iteration 224000, lr = 0.03, m = 0.9
I0815 05:35:58.555001 11101 solver.cpp:312] Iteration 224100 (6.88079 iter/s, 14.5332s/100 iter), loss = 1.51238
I0815 05:35:58.555029 11101 solver.cpp:334]     Train net output #0: loss = 1.32336 (* 1 = 1.32336 loss)
I0815 05:35:58.555035 11101 sgd_solver.cpp:136] Iteration 224100, lr = 0.0299688, m = 0.9
I0815 05:36:12.952777 11101 solver.cpp:312] Iteration 224200 (6.94571 iter/s, 14.3974s/100 iter), loss = 1.55167
I0815 05:36:12.952829 11101 solver.cpp:334]     Train net output #0: loss = 1.60954 (* 1 = 1.60954 loss)
I0815 05:36:12.952842 11101 sgd_solver.cpp:136] Iteration 224200, lr = 0.0299375, m = 0.9
I0815 05:36:27.231884 11101 solver.cpp:312] Iteration 224300 (7.00343 iter/s, 14.2787s/100 iter), loss = 1.66928
I0815 05:36:27.231943 11101 solver.cpp:334]     Train net output #0: loss = 1.89312 (* 1 = 1.89312 loss)
I0815 05:36:27.231950 11101 sgd_solver.cpp:136] Iteration 224300, lr = 0.0299062, m = 0.9
I0815 05:36:42.418856 11101 solver.cpp:312] Iteration 224400 (6.58477 iter/s, 15.1866s/100 iter), loss = 1.75859
I0815 05:36:42.418922 11101 solver.cpp:334]     Train net output #0: loss = 2.09992 (* 1 = 2.09992 loss)
I0815 05:36:42.418941 11101 sgd_solver.cpp:136] Iteration 224400, lr = 0.029875, m = 0.9
I0815 05:36:57.409904 11101 solver.cpp:312] Iteration 224500 (6.67083 iter/s, 14.9906s/100 iter), loss = 1.45955
I0815 05:36:57.416182 11101 solver.cpp:334]     Train net output #0: loss = 1.28019 (* 1 = 1.28019 loss)
I0815 05:36:57.416188 11101 sgd_solver.cpp:136] Iteration 224500, lr = 0.0298437, m = 0.9
I0815 05:37:12.616626 11101 solver.cpp:312] Iteration 224600 (6.57622 iter/s, 15.2063s/100 iter), loss = 1.69261
I0815 05:37:12.616659 11101 solver.cpp:334]     Train net output #0: loss = 1.3647 (* 1 = 1.3647 loss)
I0815 05:37:12.616665 11101 sgd_solver.cpp:136] Iteration 224600, lr = 0.0298125, m = 0.9
I0815 05:37:27.488865 11101 solver.cpp:312] Iteration 224700 (6.72412 iter/s, 14.8718s/100 iter), loss = 1.97482
I0815 05:37:27.488970 11101 solver.cpp:334]     Train net output #0: loss = 1.57451 (* 1 = 1.57451 loss)
I0815 05:37:27.488989 11101 sgd_solver.cpp:136] Iteration 224700, lr = 0.0297813, m = 0.9
I0815 05:37:42.533175 11101 solver.cpp:312] Iteration 224800 (6.64721 iter/s, 15.0439s/100 iter), loss = 1.59286
I0815 05:37:42.533247 11101 solver.cpp:334]     Train net output #0: loss = 1.71221 (* 1 = 1.71221 loss)
I0815 05:37:42.533265 11101 sgd_solver.cpp:136] Iteration 224800, lr = 0.02975, m = 0.9
I0815 05:37:58.002192 11101 solver.cpp:312] Iteration 224900 (6.46471 iter/s, 15.4686s/100 iter), loss = 1.97167
I0815 05:37:58.002255 11101 solver.cpp:334]     Train net output #0: loss = 1.88747 (* 1 = 1.88747 loss)
I0815 05:37:58.002262 11101 sgd_solver.cpp:136] Iteration 224900, lr = 0.0297188, m = 0.9
I0815 05:38:13.605509 11101 solver.cpp:312] Iteration 225000 (6.40907 iter/s, 15.6029s/100 iter), loss = 1.93004
I0815 05:38:13.605538 11101 solver.cpp:334]     Train net output #0: loss = 1.91055 (* 1 = 1.91055 loss)
I0815 05:38:13.605571 11101 sgd_solver.cpp:136] Iteration 225000, lr = 0.0296875, m = 0.9
I0815 05:38:29.241264 11101 solver.cpp:312] Iteration 225100 (6.39577 iter/s, 15.6353s/100 iter), loss = 1.84929
I0815 05:38:29.241349 11101 solver.cpp:334]     Train net output #0: loss = 1.79592 (* 1 = 1.79592 loss)
I0815 05:38:29.241355 11101 sgd_solver.cpp:136] Iteration 225100, lr = 0.0296563, m = 0.9
I0815 05:38:44.517091 11101 solver.cpp:312] Iteration 225200 (6.54647 iter/s, 15.2754s/100 iter), loss = 1.84886
I0815 05:38:44.517123 11101 solver.cpp:334]     Train net output #0: loss = 1.81923 (* 1 = 1.81923 loss)
I0815 05:38:44.517129 11101 sgd_solver.cpp:136] Iteration 225200, lr = 0.029625, m = 0.9
I0815 05:39:00.031427 11101 solver.cpp:312] Iteration 225300 (6.44583 iter/s, 15.5139s/100 iter), loss = 2.0232
I0815 05:39:00.031483 11101 solver.cpp:334]     Train net output #0: loss = 1.95022 (* 1 = 1.95022 loss)
I0815 05:39:00.031491 11101 sgd_solver.cpp:136] Iteration 225300, lr = 0.0295937, m = 0.9
I0815 05:39:15.830271 11101 solver.cpp:312] Iteration 225400 (6.32975 iter/s, 15.7984s/100 iter), loss = 1.86331
I0815 05:39:15.830299 11101 solver.cpp:334]     Train net output #0: loss = 1.8694 (* 1 = 1.8694 loss)
I0815 05:39:15.830305 11101 sgd_solver.cpp:136] Iteration 225400, lr = 0.0295625, m = 0.9
I0815 05:39:30.791642 11101 solver.cpp:312] Iteration 225500 (6.68406 iter/s, 14.961s/100 iter), loss = 1.55773
I0815 05:39:30.792250 11101 solver.cpp:334]     Train net output #0: loss = 1.61469 (* 1 = 1.61469 loss)
I0815 05:39:30.792258 11101 sgd_solver.cpp:136] Iteration 225500, lr = 0.0295313, m = 0.9
I0815 05:39:45.571458 11101 solver.cpp:312] Iteration 225600 (6.76617 iter/s, 14.7794s/100 iter), loss = 2.05379
I0815 05:39:45.571540 11101 solver.cpp:334]     Train net output #0: loss = 2.47573 (* 1 = 2.47573 loss)
I0815 05:39:45.571565 11101 sgd_solver.cpp:136] Iteration 225600, lr = 0.0295, m = 0.9
I0815 05:40:00.641118 11101 solver.cpp:312] Iteration 225700 (6.63603 iter/s, 15.0692s/100 iter), loss = 1.68635
I0815 05:40:00.641144 11101 solver.cpp:334]     Train net output #0: loss = 2.03616 (* 1 = 2.03616 loss)
I0815 05:40:00.641150 11101 sgd_solver.cpp:136] Iteration 225700, lr = 0.0294688, m = 0.9
I0815 05:40:15.872158 11101 solver.cpp:312] Iteration 225800 (6.56572 iter/s, 15.2306s/100 iter), loss = 1.9459
I0815 05:40:15.872251 11101 solver.cpp:334]     Train net output #0: loss = 1.83729 (* 1 = 1.83729 loss)
I0815 05:40:15.872269 11101 sgd_solver.cpp:136] Iteration 225800, lr = 0.0294375, m = 0.9
I0815 05:40:30.915225 11101 solver.cpp:312] Iteration 225900 (6.64776 iter/s, 15.0427s/100 iter), loss = 1.4672
I0815 05:40:30.915294 11101 solver.cpp:334]     Train net output #0: loss = 1.70666 (* 1 = 1.70666 loss)
I0815 05:40:30.915313 11101 sgd_solver.cpp:136] Iteration 225900, lr = 0.0294062, m = 0.9
I0815 05:40:46.161720 11101 solver.cpp:509] Iteration 226000, Testing net (#0)
I0815 05:40:51.176829 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 05:41:09.514885 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.510412
I0815 05:41:09.514907 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.756998
I0815 05:41:09.514914 11101 solver.cpp:594]     Test net output #2: loss = 2.1936 (* 1 = 2.1936 loss)
I0815 05:41:09.514932 11101 solver.cpp:264] [MultiGPU] Tests completed in 23.3526s
I0815 05:41:09.676187 11101 solver.cpp:312] Iteration 226000 (2.57999 iter/s, 38.7599s/100 iter), loss = 1.75171
I0815 05:41:09.676211 11101 solver.cpp:334]     Train net output #0: loss = 1.61324 (* 1 = 1.61324 loss)
I0815 05:41:09.676215 11101 sgd_solver.cpp:136] Iteration 226000, lr = 0.029375, m = 0.9
I0815 05:41:24.908113 11101 solver.cpp:312] Iteration 226100 (6.56534 iter/s, 15.2315s/100 iter), loss = 1.79308
I0815 05:41:24.908218 11101 solver.cpp:334]     Train net output #0: loss = 1.54458 (* 1 = 1.54458 loss)
I0815 05:41:24.908236 11101 sgd_solver.cpp:136] Iteration 226100, lr = 0.0293437, m = 0.9
I0815 05:41:39.986274 11101 solver.cpp:312] Iteration 226200 (6.63229 iter/s, 15.0777s/100 iter), loss = 1.50405
I0815 05:41:39.986302 11101 solver.cpp:334]     Train net output #0: loss = 1.78734 (* 1 = 1.78734 loss)
I0815 05:41:39.986308 11101 sgd_solver.cpp:136] Iteration 226200, lr = 0.0293125, m = 0.9
I0815 05:41:55.192061 11101 solver.cpp:312] Iteration 226300 (6.57662 iter/s, 15.2054s/100 iter), loss = 1.75595
I0815 05:41:55.192124 11101 solver.cpp:334]     Train net output #0: loss = 1.83627 (* 1 = 1.83627 loss)
I0815 05:41:55.192137 11101 sgd_solver.cpp:136] Iteration 226300, lr = 0.0292813, m = 0.9
I0815 05:42:10.046530 11101 solver.cpp:312] Iteration 226400 (6.73217 iter/s, 14.8541s/100 iter), loss = 1.87734
I0815 05:42:10.046555 11101 solver.cpp:334]     Train net output #0: loss = 1.6159 (* 1 = 1.6159 loss)
I0815 05:42:10.046561 11101 sgd_solver.cpp:136] Iteration 226400, lr = 0.02925, m = 0.9
I0815 05:42:25.324409 11101 solver.cpp:312] Iteration 226500 (6.54559 iter/s, 15.2775s/100 iter), loss = 1.98723
I0815 05:42:25.324468 11101 solver.cpp:334]     Train net output #0: loss = 2.23077 (* 1 = 2.23077 loss)
I0815 05:42:25.324477 11101 sgd_solver.cpp:136] Iteration 226500, lr = 0.0292188, m = 0.9
I0815 05:42:40.948750 11101 solver.cpp:312] Iteration 226600 (6.40045 iter/s, 15.6239s/100 iter), loss = 1.94738
I0815 05:42:40.948777 11101 solver.cpp:334]     Train net output #0: loss = 1.98576 (* 1 = 1.98576 loss)
I0815 05:42:40.948783 11101 sgd_solver.cpp:136] Iteration 226600, lr = 0.0291875, m = 0.9
I0815 05:42:55.919492 11101 solver.cpp:312] Iteration 226700 (6.67988 iter/s, 14.9703s/100 iter), loss = 1.82961
I0815 05:42:55.919549 11101 solver.cpp:334]     Train net output #0: loss = 2.07683 (* 1 = 2.07683 loss)
I0815 05:42:55.919554 11101 sgd_solver.cpp:136] Iteration 226700, lr = 0.0291563, m = 0.9
I0815 05:43:10.554785 11101 solver.cpp:312] Iteration 226800 (6.83299 iter/s, 14.6349s/100 iter), loss = 1.85636
I0815 05:43:10.554811 11101 solver.cpp:334]     Train net output #0: loss = 1.70141 (* 1 = 1.70141 loss)
I0815 05:43:10.554816 11101 sgd_solver.cpp:136] Iteration 226800, lr = 0.029125, m = 0.9
I0815 05:43:25.365629 11101 solver.cpp:312] Iteration 226900 (6.752 iter/s, 14.8104s/100 iter), loss = 1.94313
I0815 05:43:25.365698 11101 solver.cpp:334]     Train net output #0: loss = 1.83469 (* 1 = 1.83469 loss)
I0815 05:43:25.365715 11101 sgd_solver.cpp:136] Iteration 226900, lr = 0.0290937, m = 0.9
I0815 05:43:40.361629 11101 solver.cpp:312] Iteration 227000 (6.66863 iter/s, 14.9956s/100 iter), loss = 1.74268
I0815 05:43:40.361719 11101 solver.cpp:334]     Train net output #0: loss = 1.66586 (* 1 = 1.66586 loss)
I0815 05:43:40.361727 11101 sgd_solver.cpp:136] Iteration 227000, lr = 0.0290625, m = 0.9
I0815 05:43:55.384790 11101 solver.cpp:312] Iteration 227100 (6.65657 iter/s, 15.0227s/100 iter), loss = 1.70582
I0815 05:43:55.384819 11101 solver.cpp:334]     Train net output #0: loss = 2.11582 (* 1 = 2.11582 loss)
I0815 05:43:55.384825 11101 sgd_solver.cpp:136] Iteration 227100, lr = 0.0290313, m = 0.9
I0815 05:44:10.376642 11101 solver.cpp:312] Iteration 227200 (6.67047 iter/s, 14.9914s/100 iter), loss = 1.55688
I0815 05:44:10.376737 11101 solver.cpp:334]     Train net output #0: loss = 1.53753 (* 1 = 1.53753 loss)
I0815 05:44:10.376755 11101 sgd_solver.cpp:136] Iteration 227200, lr = 0.029, m = 0.9
I0815 05:44:25.126570 11101 solver.cpp:312] Iteration 227300 (6.77988 iter/s, 14.7495s/100 iter), loss = 1.81886
I0815 05:44:25.126626 11101 solver.cpp:334]     Train net output #0: loss = 1.90833 (* 1 = 1.90833 loss)
I0815 05:44:25.126639 11101 sgd_solver.cpp:136] Iteration 227300, lr = 0.0289688, m = 0.9
I0815 05:44:40.118165 11101 solver.cpp:312] Iteration 227400 (6.67059 iter/s, 14.9912s/100 iter), loss = 1.56622
I0815 05:44:40.118194 11101 solver.cpp:334]     Train net output #0: loss = 1.72714 (* 1 = 1.72714 loss)
I0815 05:44:40.118201 11101 sgd_solver.cpp:136] Iteration 227400, lr = 0.0289375, m = 0.9
I0815 05:44:55.333545 11101 solver.cpp:312] Iteration 227500 (6.57248 iter/s, 15.215s/100 iter), loss = 1.7367
I0815 05:44:55.333638 11101 solver.cpp:334]     Train net output #0: loss = 1.69784 (* 1 = 1.69784 loss)
I0815 05:44:55.333654 11101 sgd_solver.cpp:136] Iteration 227500, lr = 0.0289063, m = 0.9
I0815 05:45:10.407052 11101 solver.cpp:312] Iteration 227600 (6.63434 iter/s, 15.0731s/100 iter), loss = 1.36451
I0815 05:45:10.407079 11101 solver.cpp:334]     Train net output #0: loss = 1.46437 (* 1 = 1.46437 loss)
I0815 05:45:10.407085 11101 sgd_solver.cpp:136] Iteration 227600, lr = 0.028875, m = 0.9
I0815 05:45:25.410392 11101 solver.cpp:312] Iteration 227700 (6.66537 iter/s, 15.0029s/100 iter), loss = 1.50883
I0815 05:45:25.410445 11101 solver.cpp:334]     Train net output #0: loss = 1.50993 (* 1 = 1.50993 loss)
I0815 05:45:25.410451 11101 sgd_solver.cpp:136] Iteration 227700, lr = 0.0288437, m = 0.9
I0815 05:45:40.375519 11101 solver.cpp:312] Iteration 227800 (6.68238 iter/s, 14.9647s/100 iter), loss = 1.88484
I0815 05:45:40.375543 11101 solver.cpp:334]     Train net output #0: loss = 1.81632 (* 1 = 1.81632 loss)
I0815 05:45:40.375548 11101 sgd_solver.cpp:136] Iteration 227800, lr = 0.0288125, m = 0.9
I0815 05:45:55.419486 11101 solver.cpp:312] Iteration 227900 (6.64737 iter/s, 15.0436s/100 iter), loss = 1.93302
I0815 05:45:55.419550 11101 solver.cpp:334]     Train net output #0: loss = 1.93748 (* 1 = 1.93748 loss)
I0815 05:45:55.419558 11101 sgd_solver.cpp:136] Iteration 227900, lr = 0.0287812, m = 0.9
I0815 05:46:10.786126 11101 solver.cpp:509] Iteration 228000, Testing net (#0)
I0815 05:46:26.470286 11099 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 05:46:33.247802 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.484706
I0815 05:46:33.247822 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.738999
I0815 05:46:33.247828 11101 solver.cpp:594]     Test net output #2: loss = 2.28846 (* 1 = 2.28846 loss)
I0815 05:46:33.247859 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.4611s
I0815 05:46:33.388595 11101 solver.cpp:312] Iteration 228000 (2.63379 iter/s, 37.9681s/100 iter), loss = 2.05538
I0815 05:46:33.388620 11101 solver.cpp:334]     Train net output #0: loss = 2.03763 (* 1 = 2.03763 loss)
I0815 05:46:33.388624 11101 sgd_solver.cpp:136] Iteration 228000, lr = 0.02875, m = 0.9
I0815 05:46:48.086571 11101 solver.cpp:312] Iteration 228100 (6.80385 iter/s, 14.6976s/100 iter), loss = 1.8017
I0815 05:46:48.086596 11101 solver.cpp:334]     Train net output #0: loss = 1.97874 (* 1 = 1.97874 loss)
I0815 05:46:48.086601 11101 sgd_solver.cpp:136] Iteration 228100, lr = 0.0287188, m = 0.9
I0815 05:47:02.621794 11101 solver.cpp:312] Iteration 228200 (6.88003 iter/s, 14.5348s/100 iter), loss = 1.78253
I0815 05:47:02.621858 11101 solver.cpp:334]     Train net output #0: loss = 1.69649 (* 1 = 1.69649 loss)
I0815 05:47:02.621866 11101 sgd_solver.cpp:136] Iteration 228200, lr = 0.0286875, m = 0.9
I0815 05:47:17.538218 11101 solver.cpp:312] Iteration 228300 (6.70421 iter/s, 14.916s/100 iter), loss = 1.89653
I0815 05:47:17.538283 11101 solver.cpp:334]     Train net output #0: loss = 2.01153 (* 1 = 2.01153 loss)
I0815 05:47:17.538300 11101 sgd_solver.cpp:136] Iteration 228300, lr = 0.0286562, m = 0.9
I0815 05:47:32.107648 11101 solver.cpp:312] Iteration 228400 (6.86388 iter/s, 14.569s/100 iter), loss = 1.61536
I0815 05:47:32.107712 11101 solver.cpp:334]     Train net output #0: loss = 2.08468 (* 1 = 2.08468 loss)
I0815 05:47:32.107738 11101 sgd_solver.cpp:136] Iteration 228400, lr = 0.028625, m = 0.9
I0815 05:47:46.746127 11101 solver.cpp:312] Iteration 228500 (6.8315 iter/s, 14.6381s/100 iter), loss = 1.63672
I0815 05:47:46.747679 11101 solver.cpp:334]     Train net output #0: loss = 1.40771 (* 1 = 1.40771 loss)
I0815 05:47:46.747704 11101 sgd_solver.cpp:136] Iteration 228500, lr = 0.0285937, m = 0.9
I0815 05:48:01.527420 11101 solver.cpp:312] Iteration 228600 (6.76549 iter/s, 14.7809s/100 iter), loss = 1.80293
I0815 05:48:01.527443 11101 solver.cpp:334]     Train net output #0: loss = 1.46773 (* 1 = 1.46773 loss)
I0815 05:48:01.527448 11101 sgd_solver.cpp:136] Iteration 228600, lr = 0.0285625, m = 0.9
I0815 05:48:16.275048 11101 solver.cpp:312] Iteration 228700 (6.78094 iter/s, 14.7472s/100 iter), loss = 1.79494
I0815 05:48:16.275118 11101 solver.cpp:334]     Train net output #0: loss = 1.76253 (* 1 = 1.76253 loss)
I0815 05:48:16.275137 11101 sgd_solver.cpp:136] Iteration 228700, lr = 0.0285312, m = 0.9
I0815 05:48:31.034500 11101 solver.cpp:312] Iteration 228800 (6.77551 iter/s, 14.759s/100 iter), loss = 1.67187
I0815 05:48:31.034556 11101 solver.cpp:334]     Train net output #0: loss = 1.6622 (* 1 = 1.6622 loss)
I0815 05:48:31.034564 11101 sgd_solver.cpp:136] Iteration 228800, lr = 0.0285, m = 0.9
I0815 05:48:45.550933 11101 solver.cpp:312] Iteration 228900 (6.88893 iter/s, 14.516s/100 iter), loss = 0.951544
I0815 05:48:45.550958 11101 solver.cpp:334]     Train net output #0: loss = 0.899619 (* 1 = 0.899619 loss)
I0815 05:48:45.550964 11101 sgd_solver.cpp:136] Iteration 228900, lr = 0.0284688, m = 0.9
I0815 05:49:00.129268 11101 solver.cpp:312] Iteration 229000 (6.85968 iter/s, 14.5779s/100 iter), loss = 1.52297
I0815 05:49:00.129295 11101 solver.cpp:334]     Train net output #0: loss = 1.43603 (* 1 = 1.43603 loss)
I0815 05:49:00.129302 11101 sgd_solver.cpp:136] Iteration 229000, lr = 0.0284375, m = 0.9
I0815 05:49:14.568241 11101 solver.cpp:312] Iteration 229100 (6.92589 iter/s, 14.4386s/100 iter), loss = 1.74478
I0815 05:49:14.568300 11101 solver.cpp:334]     Train net output #0: loss = 1.96975 (* 1 = 1.96975 loss)
I0815 05:49:14.568306 11101 sgd_solver.cpp:136] Iteration 229100, lr = 0.0284063, m = 0.9
I0815 05:49:28.942941 11101 solver.cpp:312] Iteration 229200 (6.95686 iter/s, 14.3743s/100 iter), loss = 1.56097
I0815 05:49:28.942965 11101 solver.cpp:334]     Train net output #0: loss = 2.0873 (* 1 = 2.0873 loss)
I0815 05:49:28.942972 11101 sgd_solver.cpp:136] Iteration 229200, lr = 0.028375, m = 0.9
I0815 05:49:43.331008 11101 solver.cpp:312] Iteration 229300 (6.95039 iter/s, 14.3877s/100 iter), loss = 1.69098
I0815 05:49:43.331037 11101 solver.cpp:334]     Train net output #0: loss = 1.7111 (* 1 = 1.7111 loss)
I0815 05:49:43.331043 11101 sgd_solver.cpp:136] Iteration 229300, lr = 0.0283438, m = 0.9
I0815 05:49:57.777295 11101 solver.cpp:312] Iteration 229400 (6.92238 iter/s, 14.4459s/100 iter), loss = 2.19805
I0815 05:49:57.777390 11101 solver.cpp:334]     Train net output #0: loss = 2.68796 (* 1 = 2.68796 loss)
I0815 05:49:57.777407 11101 sgd_solver.cpp:136] Iteration 229400, lr = 0.0283125, m = 0.9
I0815 05:50:12.480463 11101 solver.cpp:312] Iteration 229500 (6.80144 iter/s, 14.7028s/100 iter), loss = 1.89021
I0815 05:50:12.480535 11101 solver.cpp:334]     Train net output #0: loss = 1.91198 (* 1 = 1.91198 loss)
I0815 05:50:12.480554 11101 sgd_solver.cpp:136] Iteration 229500, lr = 0.0282812, m = 0.9
I0815 05:50:27.235939 11101 solver.cpp:312] Iteration 229600 (6.77733 iter/s, 14.7551s/100 iter), loss = 1.56207
I0815 05:50:27.236167 11101 solver.cpp:334]     Train net output #0: loss = 1.19145 (* 1 = 1.19145 loss)
I0815 05:50:27.236281 11101 sgd_solver.cpp:136] Iteration 229600, lr = 0.02825, m = 0.9
I0815 05:50:42.104084 11101 solver.cpp:312] Iteration 229700 (6.72597 iter/s, 14.8677s/100 iter), loss = 1.84873
I0815 05:50:42.104142 11101 solver.cpp:334]     Train net output #0: loss = 1.93985 (* 1 = 1.93985 loss)
I0815 05:50:42.104151 11101 sgd_solver.cpp:136] Iteration 229700, lr = 0.0282188, m = 0.9
I0815 05:50:56.773434 11101 solver.cpp:312] Iteration 229800 (6.81712 iter/s, 14.6689s/100 iter), loss = 1.71731
I0815 05:50:56.773463 11101 solver.cpp:334]     Train net output #0: loss = 1.60974 (* 1 = 1.60974 loss)
I0815 05:50:56.773469 11101 sgd_solver.cpp:136] Iteration 229800, lr = 0.0281875, m = 0.9
I0815 05:51:11.542271 11101 solver.cpp:312] Iteration 229900 (6.7712 iter/s, 14.7684s/100 iter), loss = 1.8152
I0815 05:51:11.542300 11101 solver.cpp:334]     Train net output #0: loss = 1.89136 (* 1 = 1.89136 loss)
I0815 05:51:11.542306 11101 sgd_solver.cpp:136] Iteration 229900, lr = 0.0281563, m = 0.9
I0815 05:51:26.419416 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_230000.caffemodel
I0815 05:51:26.484216 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_230000.solverstate
I0815 05:51:26.489861 11101 solver.cpp:509] Iteration 230000, Testing net (#0)
I0815 05:51:47.553899 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.501941
I0815 05:51:47.553922 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.74741
I0815 05:51:47.553928 11101 solver.cpp:594]     Test net output #2: loss = 2.2286 (* 1 = 2.2286 loss)
I0815 05:51:47.553956 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0635s
I0815 05:51:47.715656 11101 solver.cpp:312] Iteration 230000 (2.76454 iter/s, 36.1724s/100 iter), loss = 1.91757
I0815 05:51:47.715682 11101 solver.cpp:334]     Train net output #0: loss = 1.54125 (* 1 = 1.54125 loss)
I0815 05:51:47.715688 11101 sgd_solver.cpp:136] Iteration 230000, lr = 0.028125, m = 0.9
I0815 05:52:02.448719 11101 solver.cpp:312] Iteration 230100 (6.78764 iter/s, 14.7327s/100 iter), loss = 1.66799
I0815 05:52:02.448782 11101 solver.cpp:334]     Train net output #0: loss = 1.26456 (* 1 = 1.26456 loss)
I0815 05:52:02.448789 11101 sgd_solver.cpp:136] Iteration 230100, lr = 0.0280937, m = 0.9
I0815 05:52:17.073539 11101 solver.cpp:312] Iteration 230200 (6.83788 iter/s, 14.6244s/100 iter), loss = 1.95188
I0815 05:52:17.073568 11101 solver.cpp:334]     Train net output #0: loss = 2.29537 (* 1 = 2.29537 loss)
I0815 05:52:17.073575 11101 sgd_solver.cpp:136] Iteration 230200, lr = 0.0280625, m = 0.9
I0815 05:52:31.585678 11101 solver.cpp:312] Iteration 230300 (6.89097 iter/s, 14.5117s/100 iter), loss = 1.55161
I0815 05:52:31.585706 11101 solver.cpp:334]     Train net output #0: loss = 2.04941 (* 1 = 2.04941 loss)
I0815 05:52:31.585713 11101 sgd_solver.cpp:136] Iteration 230300, lr = 0.0280312, m = 0.9
I0815 05:52:45.983705 11101 solver.cpp:312] Iteration 230400 (6.94559 iter/s, 14.3976s/100 iter), loss = 1.37849
I0815 05:52:45.983938 11101 solver.cpp:334]     Train net output #0: loss = 1.34495 (* 1 = 1.34495 loss)
I0815 05:52:45.983952 11101 sgd_solver.cpp:136] Iteration 230400, lr = 0.028, m = 0.9
I0815 05:53:00.477679 11101 solver.cpp:312] Iteration 230500 (6.89961 iter/s, 14.4936s/100 iter), loss = 1.69459
I0815 05:53:00.477753 11101 solver.cpp:334]     Train net output #0: loss = 1.51382 (* 1 = 1.51382 loss)
I0815 05:53:00.477774 11101 sgd_solver.cpp:136] Iteration 230500, lr = 0.0279688, m = 0.9
I0815 05:53:15.289258 11101 solver.cpp:312] Iteration 230600 (6.75166 iter/s, 14.8112s/100 iter), loss = 1.37348
I0815 05:53:15.289286 11101 solver.cpp:334]     Train net output #0: loss = 1.35592 (* 1 = 1.35592 loss)
I0815 05:53:15.289293 11101 sgd_solver.cpp:136] Iteration 230600, lr = 0.0279375, m = 0.9
I0815 05:53:30.403600 11101 solver.cpp:312] Iteration 230700 (6.61641 iter/s, 15.1139s/100 iter), loss = 1.6672
I0815 05:53:30.403693 11101 solver.cpp:334]     Train net output #0: loss = 1.97236 (* 1 = 1.97236 loss)
I0815 05:53:30.403712 11101 sgd_solver.cpp:136] Iteration 230700, lr = 0.0279063, m = 0.9
I0815 05:53:44.940207 11101 solver.cpp:312] Iteration 230800 (6.87938 iter/s, 14.5362s/100 iter), loss = 1.26781
I0815 05:53:44.942833 11101 solver.cpp:334]     Train net output #0: loss = 1.49312 (* 1 = 1.49312 loss)
I0815 05:53:44.942844 11101 sgd_solver.cpp:136] Iteration 230800, lr = 0.027875, m = 0.9
I0815 05:53:59.496609 11101 solver.cpp:312] Iteration 230900 (6.87002 iter/s, 14.556s/100 iter), loss = 1.60634
I0815 05:53:59.496634 11101 solver.cpp:334]     Train net output #0: loss = 1.47294 (* 1 = 1.47294 loss)
I0815 05:53:59.496640 11101 sgd_solver.cpp:136] Iteration 230900, lr = 0.0278437, m = 0.9
I0815 05:54:13.813000 11101 solver.cpp:312] Iteration 231000 (6.98519 iter/s, 14.316s/100 iter), loss = 1.95739
I0815 05:54:13.813073 11101 solver.cpp:334]     Train net output #0: loss = 1.99808 (* 1 = 1.99808 loss)
I0815 05:54:13.813081 11101 sgd_solver.cpp:136] Iteration 231000, lr = 0.0278125, m = 0.9
I0815 05:54:28.005252 11101 solver.cpp:312] Iteration 231100 (7.04629 iter/s, 14.1919s/100 iter), loss = 2.00119
I0815 05:54:28.005280 11101 solver.cpp:334]     Train net output #0: loss = 2.01768 (* 1 = 2.01768 loss)
I0815 05:54:28.005286 11101 sgd_solver.cpp:136] Iteration 231100, lr = 0.0277812, m = 0.9
I0815 05:54:42.496280 11101 solver.cpp:312] Iteration 231200 (6.90101 iter/s, 14.4906s/100 iter), loss = 2.09445
I0815 05:54:42.496306 11101 solver.cpp:334]     Train net output #0: loss = 1.83311 (* 1 = 1.83311 loss)
I0815 05:54:42.496310 11101 sgd_solver.cpp:136] Iteration 231200, lr = 0.02775, m = 0.9
I0815 05:54:57.129920 11101 solver.cpp:312] Iteration 231300 (6.83376 iter/s, 14.6332s/100 iter), loss = 1.95893
I0815 05:54:57.129982 11101 solver.cpp:334]     Train net output #0: loss = 2.06537 (* 1 = 2.06537 loss)
I0815 05:54:57.129992 11101 sgd_solver.cpp:136] Iteration 231300, lr = 0.0277188, m = 0.9
I0815 05:55:11.925168 11101 solver.cpp:312] Iteration 231400 (6.75911 iter/s, 14.7948s/100 iter), loss = 1.94106
I0815 05:55:11.925233 11101 solver.cpp:334]     Train net output #0: loss = 1.76119 (* 1 = 1.76119 loss)
I0815 05:55:11.925251 11101 sgd_solver.cpp:136] Iteration 231400, lr = 0.0276875, m = 0.9
I0815 05:55:26.872359 11101 solver.cpp:312] Iteration 231500 (6.69041 iter/s, 14.9468s/100 iter), loss = 1.60125
I0815 05:55:26.872413 11101 solver.cpp:334]     Train net output #0: loss = 1.69473 (* 1 = 1.69473 loss)
I0815 05:55:26.872427 11101 sgd_solver.cpp:136] Iteration 231500, lr = 0.0276563, m = 0.9
I0815 05:55:41.313262 11101 solver.cpp:312] Iteration 231600 (6.92496 iter/s, 14.4405s/100 iter), loss = 1.63379
I0815 05:55:41.313344 11101 solver.cpp:334]     Train net output #0: loss = 1.36932 (* 1 = 1.36932 loss)
I0815 05:55:41.313352 11101 sgd_solver.cpp:136] Iteration 231600, lr = 0.027625, m = 0.9
I0815 05:55:55.828547 11101 solver.cpp:312] Iteration 231700 (6.88948 iter/s, 14.5149s/100 iter), loss = 1.89545
I0815 05:55:55.828619 11101 solver.cpp:334]     Train net output #0: loss = 2.14842 (* 1 = 2.14842 loss)
I0815 05:55:55.828639 11101 sgd_solver.cpp:136] Iteration 231700, lr = 0.0275938, m = 0.9
I0815 05:56:10.464149 11101 solver.cpp:312] Iteration 231800 (6.83284 iter/s, 14.6352s/100 iter), loss = 1.88676
I0815 05:56:10.464179 11101 solver.cpp:334]     Train net output #0: loss = 1.48538 (* 1 = 1.48538 loss)
I0815 05:56:10.464185 11101 sgd_solver.cpp:136] Iteration 231800, lr = 0.0275625, m = 0.9
I0815 05:56:25.282724 11101 solver.cpp:312] Iteration 231900 (6.74847 iter/s, 14.8182s/100 iter), loss = 1.79645
I0815 05:56:25.282791 11101 solver.cpp:334]     Train net output #0: loss = 1.44329 (* 1 = 1.44329 loss)
I0815 05:56:25.282824 11101 sgd_solver.cpp:136] Iteration 231900, lr = 0.0275312, m = 0.9
I0815 05:56:39.775192 11101 solver.cpp:509] Iteration 232000, Testing net (#0)
I0815 05:56:39.847004 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 05:57:00.355444 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.509882
I0815 05:57:00.355500 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.76241
I0815 05:57:00.355507 11101 solver.cpp:594]     Test net output #2: loss = 2.16727 (* 1 = 2.16727 loss)
I0815 05:57:00.355557 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.5798s
I0815 05:57:00.513121 11101 solver.cpp:312] Iteration 232000 (2.83854 iter/s, 35.2294s/100 iter), loss = 1.65145
I0815 05:57:00.513149 11101 solver.cpp:334]     Train net output #0: loss = 1.22781 (* 1 = 1.22781 loss)
I0815 05:57:00.513155 11101 sgd_solver.cpp:136] Iteration 232000, lr = 0.0275, m = 0.9
I0815 05:57:15.110546 11101 solver.cpp:312] Iteration 232100 (6.85071 iter/s, 14.597s/100 iter), loss = 1.58263
I0815 05:57:15.110576 11101 solver.cpp:334]     Train net output #0: loss = 1.54646 (* 1 = 1.54646 loss)
I0815 05:57:15.110581 11101 sgd_solver.cpp:136] Iteration 232100, lr = 0.0274688, m = 0.9
I0815 05:57:30.009018 11101 solver.cpp:312] Iteration 232200 (6.71228 iter/s, 14.8981s/100 iter), loss = 1.35409
I0815 05:57:30.009074 11101 solver.cpp:334]     Train net output #0: loss = 1.41929 (* 1 = 1.41929 loss)
I0815 05:57:30.009088 11101 sgd_solver.cpp:136] Iteration 232200, lr = 0.0274375, m = 0.9
I0815 05:57:44.807152 11101 solver.cpp:312] Iteration 232300 (6.7578 iter/s, 14.7977s/100 iter), loss = 1.74644
I0815 05:57:44.807859 11101 solver.cpp:334]     Train net output #0: loss = 1.82635 (* 1 = 1.82635 loss)
I0815 05:57:44.807868 11101 sgd_solver.cpp:136] Iteration 232300, lr = 0.0274063, m = 0.9
I0815 05:57:59.124104 11101 solver.cpp:312] Iteration 232400 (6.98492 iter/s, 14.3166s/100 iter), loss = 1.41943
I0815 05:57:59.124164 11101 solver.cpp:334]     Train net output #0: loss = 1.25949 (* 1 = 1.25949 loss)
I0815 05:57:59.124177 11101 sgd_solver.cpp:136] Iteration 232400, lr = 0.027375, m = 0.9
I0815 05:58:13.829368 11101 solver.cpp:312] Iteration 232500 (6.80047 iter/s, 14.7049s/100 iter), loss = 1.82351
I0815 05:58:13.829399 11101 solver.cpp:334]     Train net output #0: loss = 1.81505 (* 1 = 1.81505 loss)
I0815 05:58:13.829406 11101 sgd_solver.cpp:136] Iteration 232500, lr = 0.0273438, m = 0.9
I0815 05:58:28.400480 11101 solver.cpp:312] Iteration 232600 (6.86308 iter/s, 14.5707s/100 iter), loss = 1.45376
I0815 05:58:28.400562 11101 solver.cpp:334]     Train net output #0: loss = 1.23977 (* 1 = 1.23977 loss)
I0815 05:58:28.400574 11101 sgd_solver.cpp:136] Iteration 232600, lr = 0.0273125, m = 0.9
I0815 05:58:43.021309 11101 solver.cpp:312] Iteration 232700 (6.83975 iter/s, 14.6204s/100 iter), loss = 2.2545
I0815 05:58:43.021333 11101 solver.cpp:334]     Train net output #0: loss = 2.58215 (* 1 = 2.58215 loss)
I0815 05:58:43.021339 11101 sgd_solver.cpp:136] Iteration 232700, lr = 0.0272812, m = 0.9
I0815 05:58:57.566498 11101 solver.cpp:312] Iteration 232800 (6.87531 iter/s, 14.5448s/100 iter), loss = 1.38168
I0815 05:58:57.566541 11101 solver.cpp:334]     Train net output #0: loss = 1.27083 (* 1 = 1.27083 loss)
I0815 05:58:57.566555 11101 sgd_solver.cpp:136] Iteration 232800, lr = 0.02725, m = 0.9
I0815 05:59:12.051189 11101 solver.cpp:312] Iteration 232900 (6.90403 iter/s, 14.4843s/100 iter), loss = 1.48519
I0815 05:59:12.051244 11101 solver.cpp:334]     Train net output #0: loss = 1.71586 (* 1 = 1.71586 loss)
I0815 05:59:12.051250 11101 sgd_solver.cpp:136] Iteration 232900, lr = 0.0272187, m = 0.9
I0815 05:59:26.408051 11101 solver.cpp:312] Iteration 233000 (6.9655 iter/s, 14.3565s/100 iter), loss = 1.91981
I0815 05:59:26.408080 11101 solver.cpp:334]     Train net output #0: loss = 1.55623 (* 1 = 1.55623 loss)
I0815 05:59:26.408087 11101 sgd_solver.cpp:136] Iteration 233000, lr = 0.0271875, m = 0.9
I0815 05:59:40.973297 11101 solver.cpp:312] Iteration 233100 (6.86585 iter/s, 14.5648s/100 iter), loss = 1.98223
I0815 05:59:40.973322 11101 solver.cpp:334]     Train net output #0: loss = 2.08514 (* 1 = 2.08514 loss)
I0815 05:59:40.973327 11101 sgd_solver.cpp:136] Iteration 233100, lr = 0.0271563, m = 0.9
I0815 05:59:55.199019 11101 solver.cpp:312] Iteration 233200 (7.02971 iter/s, 14.2253s/100 iter), loss = 1.64098
I0815 05:59:55.199105 11101 solver.cpp:334]     Train net output #0: loss = 1.87261 (* 1 = 1.87261 loss)
I0815 05:59:55.199117 11101 sgd_solver.cpp:136] Iteration 233200, lr = 0.027125, m = 0.9
I0815 06:00:09.741610 11101 solver.cpp:312] Iteration 233300 (6.87654 iter/s, 14.5422s/100 iter), loss = 1.72429
I0815 06:00:09.741672 11101 solver.cpp:334]     Train net output #0: loss = 1.92747 (* 1 = 1.92747 loss)
I0815 06:00:09.741698 11101 sgd_solver.cpp:136] Iteration 233300, lr = 0.0270938, m = 0.9
I0815 06:00:24.202611 11101 solver.cpp:312] Iteration 233400 (6.91534 iter/s, 14.4606s/100 iter), loss = 1.6058
I0815 06:00:24.202685 11101 solver.cpp:334]     Train net output #0: loss = 1.39585 (* 1 = 1.39585 loss)
I0815 06:00:24.202711 11101 sgd_solver.cpp:136] Iteration 233400, lr = 0.0270625, m = 0.9
I0815 06:00:38.769520 11101 solver.cpp:312] Iteration 233500 (6.86507 iter/s, 14.5665s/100 iter), loss = 1.61634
I0815 06:00:38.769595 11101 solver.cpp:334]     Train net output #0: loss = 1.46661 (* 1 = 1.46661 loss)
I0815 06:00:38.769601 11101 sgd_solver.cpp:136] Iteration 233500, lr = 0.0270312, m = 0.9
I0815 06:00:53.461606 11101 solver.cpp:312] Iteration 233600 (6.80657 iter/s, 14.6917s/100 iter), loss = 2.06161
I0815 06:00:53.461633 11101 solver.cpp:334]     Train net output #0: loss = 2.19598 (* 1 = 2.19598 loss)
I0815 06:00:53.461639 11101 sgd_solver.cpp:136] Iteration 233600, lr = 0.027, m = 0.9
I0815 06:01:08.045075 11101 solver.cpp:312] Iteration 233700 (6.85727 iter/s, 14.5831s/100 iter), loss = 1.71211
I0815 06:01:08.045143 11101 solver.cpp:334]     Train net output #0: loss = 1.1434 (* 1 = 1.1434 loss)
I0815 06:01:08.045162 11101 sgd_solver.cpp:136] Iteration 233700, lr = 0.0269687, m = 0.9
I0815 06:01:22.732223 11101 solver.cpp:312] Iteration 233800 (6.80886 iter/s, 14.6867s/100 iter), loss = 1.95762
I0815 06:01:22.736249 11101 solver.cpp:334]     Train net output #0: loss = 1.96933 (* 1 = 1.96933 loss)
I0815 06:01:22.736276 11101 sgd_solver.cpp:136] Iteration 233800, lr = 0.0269375, m = 0.9
I0815 06:01:37.401737 11101 solver.cpp:312] Iteration 233900 (6.81704 iter/s, 14.6691s/100 iter), loss = 1.62385
I0815 06:01:37.401765 11101 solver.cpp:334]     Train net output #0: loss = 1.31387 (* 1 = 1.31387 loss)
I0815 06:01:37.401772 11101 sgd_solver.cpp:136] Iteration 233900, lr = 0.0269063, m = 0.9
I0815 06:01:51.658980 11101 solver.cpp:509] Iteration 234000, Testing net (#0)
I0815 06:02:12.960391 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.506176
I0815 06:02:12.960450 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.757468
I0815 06:02:12.960458 11101 solver.cpp:594]     Test net output #2: loss = 2.2115 (* 1 = 2.2115 loss)
I0815 06:02:12.960480 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.3009s
I0815 06:02:13.151397 11101 solver.cpp:312] Iteration 234000 (2.79731 iter/s, 35.7487s/100 iter), loss = 1.64618
I0815 06:02:13.151423 11101 solver.cpp:334]     Train net output #0: loss = 1.1316 (* 1 = 1.1316 loss)
I0815 06:02:13.151428 11101 sgd_solver.cpp:136] Iteration 234000, lr = 0.026875, m = 0.9
I0815 06:02:27.960660 11101 solver.cpp:312] Iteration 234100 (6.75272 iter/s, 14.8089s/100 iter), loss = 1.52347
I0815 06:02:27.960731 11101 solver.cpp:334]     Train net output #0: loss = 1.46883 (* 1 = 1.46883 loss)
I0815 06:02:27.960750 11101 sgd_solver.cpp:136] Iteration 234100, lr = 0.0268438, m = 0.9
I0815 06:02:42.499431 11101 solver.cpp:312] Iteration 234200 (6.87835 iter/s, 14.5384s/100 iter), loss = 1.46029
I0815 06:02:42.499459 11101 solver.cpp:334]     Train net output #0: loss = 1.3479 (* 1 = 1.3479 loss)
I0815 06:02:42.499464 11101 sgd_solver.cpp:136] Iteration 234200, lr = 0.0268125, m = 0.9
I0815 06:02:57.038678 11101 solver.cpp:312] Iteration 234300 (6.87813 iter/s, 14.5388s/100 iter), loss = 1.52715
I0815 06:02:57.038759 11101 solver.cpp:334]     Train net output #0: loss = 1.64169 (* 1 = 1.64169 loss)
I0815 06:02:57.038772 11101 sgd_solver.cpp:136] Iteration 234300, lr = 0.0267812, m = 0.9
I0815 06:03:11.793407 11101 solver.cpp:312] Iteration 234400 (6.77768 iter/s, 14.7543s/100 iter), loss = 1.64404
I0815 06:03:11.793427 11101 solver.cpp:334]     Train net output #0: loss = 1.71563 (* 1 = 1.71563 loss)
I0815 06:03:11.793433 11101 sgd_solver.cpp:136] Iteration 234400, lr = 0.02675, m = 0.9
I0815 06:03:26.706248 11101 solver.cpp:312] Iteration 234500 (6.70582 iter/s, 14.9124s/100 iter), loss = 1.58302
I0815 06:03:26.706274 11101 solver.cpp:334]     Train net output #0: loss = 1.32723 (* 1 = 1.32723 loss)
I0815 06:03:26.706279 11101 sgd_solver.cpp:136] Iteration 234500, lr = 0.0267187, m = 0.9
I0815 06:03:41.293170 11101 solver.cpp:312] Iteration 234600 (6.85564 iter/s, 14.5865s/100 iter), loss = 1.45775
I0815 06:03:41.293236 11101 solver.cpp:334]     Train net output #0: loss = 1.50044 (* 1 = 1.50044 loss)
I0815 06:03:41.293243 11101 sgd_solver.cpp:136] Iteration 234600, lr = 0.0266875, m = 0.9
I0815 06:03:55.856999 11101 solver.cpp:312] Iteration 234700 (6.86652 iter/s, 14.5634s/100 iter), loss = 1.66222
I0815 06:03:55.857072 11101 solver.cpp:334]     Train net output #0: loss = 1.4899 (* 1 = 1.4899 loss)
I0815 06:03:55.857092 11101 sgd_solver.cpp:136] Iteration 234700, lr = 0.0266563, m = 0.9
I0815 06:04:10.280269 11101 solver.cpp:312] Iteration 234800 (6.93343 iter/s, 14.4229s/100 iter), loss = 1.96384
I0815 06:04:10.280341 11101 solver.cpp:334]     Train net output #0: loss = 2.02222 (* 1 = 2.02222 loss)
I0815 06:04:10.280360 11101 sgd_solver.cpp:136] Iteration 234800, lr = 0.026625, m = 0.9
I0815 06:04:24.563174 11101 solver.cpp:312] Iteration 234900 (7.00157 iter/s, 14.2825s/100 iter), loss = 1.86499
I0815 06:04:24.563235 11101 solver.cpp:334]     Train net output #0: loss = 1.69735 (* 1 = 1.69735 loss)
I0815 06:04:24.563241 11101 sgd_solver.cpp:136] Iteration 234900, lr = 0.0265938, m = 0.9
I0815 06:04:39.218626 11101 solver.cpp:312] Iteration 235000 (6.82359 iter/s, 14.6551s/100 iter), loss = 1.83993
I0815 06:04:39.218693 11101 solver.cpp:334]     Train net output #0: loss = 1.54189 (* 1 = 1.54189 loss)
I0815 06:04:39.218710 11101 sgd_solver.cpp:136] Iteration 235000, lr = 0.0265625, m = 0.9
I0815 06:04:53.806639 11101 solver.cpp:312] Iteration 235100 (6.85513 iter/s, 14.5876s/100 iter), loss = 1.06461
I0815 06:04:53.806666 11101 solver.cpp:334]     Train net output #0: loss = 1.07375 (* 1 = 1.07375 loss)
I0815 06:04:53.806671 11101 sgd_solver.cpp:136] Iteration 235100, lr = 0.0265312, m = 0.9
I0815 06:05:08.666728 11101 solver.cpp:312] Iteration 235200 (6.72962 iter/s, 14.8597s/100 iter), loss = 1.73114
I0815 06:05:08.672214 11101 solver.cpp:334]     Train net output #0: loss = 1.5749 (* 1 = 1.5749 loss)
I0815 06:05:08.672241 11101 sgd_solver.cpp:136] Iteration 235200, lr = 0.0265, m = 0.9
I0815 06:05:23.116884 11101 solver.cpp:312] Iteration 235300 (6.92053 iter/s, 14.4498s/100 iter), loss = 1.5965
I0815 06:05:23.116909 11101 solver.cpp:334]     Train net output #0: loss = 1.56439 (* 1 = 1.56439 loss)
I0815 06:05:23.116914 11101 sgd_solver.cpp:136] Iteration 235300, lr = 0.0264687, m = 0.9
I0815 06:05:37.981544 11101 solver.cpp:312] Iteration 235400 (6.72756 iter/s, 14.8642s/100 iter), loss = 1.69342
I0815 06:05:37.981799 11101 solver.cpp:334]     Train net output #0: loss = 1.52786 (* 1 = 1.52786 loss)
I0815 06:05:37.981919 11101 sgd_solver.cpp:136] Iteration 235400, lr = 0.0264375, m = 0.9
I0815 06:05:52.692605 11101 solver.cpp:312] Iteration 235500 (6.79779 iter/s, 14.7107s/100 iter), loss = 1.61384
I0815 06:05:52.696177 11101 solver.cpp:334]     Train net output #0: loss = 1.61055 (* 1 = 1.61055 loss)
I0815 06:05:52.696203 11101 sgd_solver.cpp:136] Iteration 235500, lr = 0.0264063, m = 0.9
I0815 06:06:07.321390 11101 solver.cpp:312] Iteration 235600 (6.83602 iter/s, 14.6284s/100 iter), loss = 1.80659
I0815 06:06:07.321419 11101 solver.cpp:334]     Train net output #0: loss = 1.81766 (* 1 = 1.81766 loss)
I0815 06:06:07.321425 11101 sgd_solver.cpp:136] Iteration 235600, lr = 0.026375, m = 0.9
I0815 06:06:22.109614 11101 solver.cpp:312] Iteration 235700 (6.76232 iter/s, 14.7878s/100 iter), loss = 1.5589
I0815 06:06:22.109639 11101 solver.cpp:334]     Train net output #0: loss = 1.07077 (* 1 = 1.07077 loss)
I0815 06:06:22.109645 11101 sgd_solver.cpp:136] Iteration 235700, lr = 0.0263438, m = 0.9
I0815 06:06:36.618723 11101 solver.cpp:312] Iteration 235800 (6.89241 iter/s, 14.5087s/100 iter), loss = 1.63683
I0815 06:06:36.618774 11101 solver.cpp:334]     Train net output #0: loss = 1.56247 (* 1 = 1.56247 loss)
I0815 06:06:36.618782 11101 sgd_solver.cpp:136] Iteration 235800, lr = 0.0263125, m = 0.9
I0815 06:06:51.316553 11101 solver.cpp:312] Iteration 235900 (6.80391 iter/s, 14.6974s/100 iter), loss = 1.76691
I0815 06:06:51.316627 11101 solver.cpp:334]     Train net output #0: loss = 1.89395 (* 1 = 1.89395 loss)
I0815 06:06:51.316651 11101 sgd_solver.cpp:136] Iteration 235900, lr = 0.0262813, m = 0.9
I0815 06:07:05.960253 11101 solver.cpp:509] Iteration 236000, Testing net (#0)
I0815 06:07:26.857048 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.496117
I0815 06:07:26.857110 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.744528
I0815 06:07:26.857116 11101 solver.cpp:594]     Test net output #2: loss = 2.27844 (* 1 = 2.27844 loss)
I0815 06:07:26.857136 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8963s
I0815 06:07:27.012221 11101 solver.cpp:312] Iteration 236000 (2.80154 iter/s, 35.6947s/100 iter), loss = 1.6819
I0815 06:07:27.012250 11101 solver.cpp:334]     Train net output #0: loss = 1.76923 (* 1 = 1.76923 loss)
I0815 06:07:27.012256 11101 sgd_solver.cpp:136] Iteration 236000, lr = 0.02625, m = 0.9
I0815 06:07:41.934903 11101 solver.cpp:312] Iteration 236100 (6.70139 iter/s, 14.9223s/100 iter), loss = 2.00207
I0815 06:07:41.934931 11101 solver.cpp:334]     Train net output #0: loss = 2.41357 (* 1 = 2.41357 loss)
I0815 06:07:41.934936 11101 sgd_solver.cpp:136] Iteration 236100, lr = 0.0262187, m = 0.9
I0815 06:07:56.668464 11101 solver.cpp:312] Iteration 236200 (6.78741 iter/s, 14.7332s/100 iter), loss = 1.76163
I0815 06:07:56.668491 11101 solver.cpp:334]     Train net output #0: loss = 1.67591 (* 1 = 1.67591 loss)
I0815 06:07:56.668498 11101 sgd_solver.cpp:136] Iteration 236200, lr = 0.0261875, m = 0.9
I0815 06:08:05.405798 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 06:08:11.314064 11101 solver.cpp:312] Iteration 236300 (6.82818 iter/s, 14.6452s/100 iter), loss = 1.40463
I0815 06:08:11.314090 11101 solver.cpp:334]     Train net output #0: loss = 1.10529 (* 1 = 1.10529 loss)
I0815 06:08:11.314095 11101 sgd_solver.cpp:136] Iteration 236300, lr = 0.0261563, m = 0.9
I0815 06:08:26.091830 11101 solver.cpp:312] Iteration 236400 (6.76711 iter/s, 14.7774s/100 iter), loss = 1.60208
I0815 06:08:26.091894 11101 solver.cpp:334]     Train net output #0: loss = 1.50199 (* 1 = 1.50199 loss)
I0815 06:08:26.091912 11101 sgd_solver.cpp:136] Iteration 236400, lr = 0.026125, m = 0.9
I0815 06:08:40.868141 11101 solver.cpp:312] Iteration 236500 (6.76778 iter/s, 14.7759s/100 iter), loss = 1.88437
I0815 06:08:40.868203 11101 solver.cpp:334]     Train net output #0: loss = 2.10427 (* 1 = 2.10427 loss)
I0815 06:08:40.868211 11101 sgd_solver.cpp:136] Iteration 236500, lr = 0.0260938, m = 0.9
I0815 06:08:55.472369 11101 solver.cpp:312] Iteration 236600 (6.84752 iter/s, 14.6038s/100 iter), loss = 1.71794
I0815 06:08:55.472398 11101 solver.cpp:334]     Train net output #0: loss = 1.77312 (* 1 = 1.77312 loss)
I0815 06:08:55.472405 11101 sgd_solver.cpp:136] Iteration 236600, lr = 0.0260625, m = 0.9
I0815 06:09:10.399268 11101 solver.cpp:312] Iteration 236700 (6.6995 iter/s, 14.9265s/100 iter), loss = 2.08238
I0815 06:09:10.399296 11101 solver.cpp:334]     Train net output #0: loss = 2.23688 (* 1 = 2.23688 loss)
I0815 06:09:10.399302 11101 sgd_solver.cpp:136] Iteration 236700, lr = 0.0260313, m = 0.9
I0815 06:09:24.991083 11101 solver.cpp:312] Iteration 236800 (6.85335 iter/s, 14.5914s/100 iter), loss = 1.79844
I0815 06:09:24.991154 11101 solver.cpp:334]     Train net output #0: loss = 1.77163 (* 1 = 1.77163 loss)
I0815 06:09:24.991161 11101 sgd_solver.cpp:136] Iteration 236800, lr = 0.026, m = 0.9
I0815 06:09:39.484113 11101 solver.cpp:312] Iteration 236900 (6.90006 iter/s, 14.4926s/100 iter), loss = 1.78446
I0815 06:09:39.484151 11101 solver.cpp:334]     Train net output #0: loss = 1.92624 (* 1 = 1.92624 loss)
I0815 06:09:39.484158 11101 sgd_solver.cpp:136] Iteration 236900, lr = 0.0259687, m = 0.9
I0815 06:09:53.993512 11101 solver.cpp:312] Iteration 237000 (6.89227 iter/s, 14.509s/100 iter), loss = 1.30397
I0815 06:09:53.993541 11101 solver.cpp:334]     Train net output #0: loss = 1.38796 (* 1 = 1.38796 loss)
I0815 06:09:53.993547 11101 sgd_solver.cpp:136] Iteration 237000, lr = 0.0259375, m = 0.9
I0815 06:10:08.749187 11101 solver.cpp:312] Iteration 237100 (6.77724 iter/s, 14.7553s/100 iter), loss = 1.902
I0815 06:10:08.749299 11101 solver.cpp:334]     Train net output #0: loss = 1.83113 (* 1 = 1.83113 loss)
I0815 06:10:08.749317 11101 sgd_solver.cpp:136] Iteration 237100, lr = 0.0259063, m = 0.9
I0815 06:10:23.682927 11101 solver.cpp:312] Iteration 237200 (6.69643 iter/s, 14.9333s/100 iter), loss = 1.68914
I0815 06:10:23.682998 11101 solver.cpp:334]     Train net output #0: loss = 2.03437 (* 1 = 2.03437 loss)
I0815 06:10:23.683017 11101 sgd_solver.cpp:136] Iteration 237200, lr = 0.025875, m = 0.9
I0815 06:10:38.401284 11101 solver.cpp:312] Iteration 237300 (6.79443 iter/s, 14.7179s/100 iter), loss = 1.81965
I0815 06:10:38.401307 11101 solver.cpp:334]     Train net output #0: loss = 1.52777 (* 1 = 1.52777 loss)
I0815 06:10:38.401314 11101 sgd_solver.cpp:136] Iteration 237300, lr = 0.0258438, m = 0.9
I0815 06:10:52.886363 11101 solver.cpp:312] Iteration 237400 (6.90384 iter/s, 14.4847s/100 iter), loss = 1.73472
I0815 06:10:52.886426 11101 solver.cpp:334]     Train net output #0: loss = 1.7691 (* 1 = 1.7691 loss)
I0815 06:10:52.886432 11101 sgd_solver.cpp:136] Iteration 237400, lr = 0.0258125, m = 0.9
I0815 06:11:07.278396 11101 solver.cpp:312] Iteration 237500 (6.94848 iter/s, 14.3916s/100 iter), loss = 1.58957
I0815 06:11:07.278461 11101 solver.cpp:334]     Train net output #0: loss = 1.68062 (* 1 = 1.68062 loss)
I0815 06:11:07.278479 11101 sgd_solver.cpp:136] Iteration 237500, lr = 0.0257812, m = 0.9
I0815 06:11:21.674937 11101 solver.cpp:312] Iteration 237600 (6.9463 iter/s, 14.3961s/100 iter), loss = 1.6369
I0815 06:11:21.674963 11101 solver.cpp:334]     Train net output #0: loss = 1.36986 (* 1 = 1.36986 loss)
I0815 06:11:21.674969 11101 sgd_solver.cpp:136] Iteration 237600, lr = 0.02575, m = 0.9
I0815 06:11:36.085191 11101 solver.cpp:312] Iteration 237700 (6.93969 iter/s, 14.4099s/100 iter), loss = 1.94808
I0815 06:11:36.085271 11101 solver.cpp:334]     Train net output #0: loss = 1.92971 (* 1 = 1.92971 loss)
I0815 06:11:36.085280 11101 sgd_solver.cpp:136] Iteration 237700, lr = 0.0257187, m = 0.9
I0815 06:11:50.868345 11101 solver.cpp:312] Iteration 237800 (6.76464 iter/s, 14.7827s/100 iter), loss = 1.9011
I0815 06:11:50.868371 11101 solver.cpp:334]     Train net output #0: loss = 1.73409 (* 1 = 1.73409 loss)
I0815 06:11:50.868376 11101 sgd_solver.cpp:136] Iteration 237800, lr = 0.0256875, m = 0.9
I0815 06:12:05.552301 11101 solver.cpp:312] Iteration 237900 (6.81034 iter/s, 14.6836s/100 iter), loss = 1.71515
I0815 06:12:05.552331 11101 solver.cpp:334]     Train net output #0: loss = 1.56891 (* 1 = 1.56891 loss)
I0815 06:12:05.552336 11101 sgd_solver.cpp:136] Iteration 237900, lr = 0.0256562, m = 0.9
I0815 06:12:19.914839 11101 solver.cpp:509] Iteration 238000, Testing net (#0)
I0815 06:12:40.482658 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.486412
I0815 06:12:40.482682 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.737057
I0815 06:12:40.482687 11101 solver.cpp:594]     Test net output #2: loss = 2.31633 (* 1 = 2.31633 loss)
I0815 06:12:40.482707 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.5673s
I0815 06:12:40.657240 11101 solver.cpp:312] Iteration 238000 (2.84868 iter/s, 35.104s/100 iter), loss = 1.55565
I0815 06:12:40.659427 11101 solver.cpp:334]     Train net output #0: loss = 1.78176 (* 1 = 1.78176 loss)
I0815 06:12:40.659452 11101 sgd_solver.cpp:136] Iteration 238000, lr = 0.025625, m = 0.9
I0815 06:12:55.103597 11101 solver.cpp:312] Iteration 238100 (6.92235 iter/s, 14.446s/100 iter), loss = 1.82116
I0815 06:12:55.103649 11101 solver.cpp:334]     Train net output #0: loss = 1.95697 (* 1 = 1.95697 loss)
I0815 06:12:55.103654 11101 sgd_solver.cpp:136] Iteration 238100, lr = 0.0255938, m = 0.9
I0815 06:13:09.697703 11101 solver.cpp:312] Iteration 238200 (6.85227 iter/s, 14.5937s/100 iter), loss = 2.11808
I0815 06:13:09.697729 11101 solver.cpp:334]     Train net output #0: loss = 2.75554 (* 1 = 2.75554 loss)
I0815 06:13:09.697734 11101 sgd_solver.cpp:136] Iteration 238200, lr = 0.0255625, m = 0.9
I0815 06:13:24.352705 11101 solver.cpp:312] Iteration 238300 (6.8238 iter/s, 14.6546s/100 iter), loss = 1.58982
I0815 06:13:24.352730 11101 solver.cpp:334]     Train net output #0: loss = 1.3902 (* 1 = 1.3902 loss)
I0815 06:13:24.352736 11101 sgd_solver.cpp:136] Iteration 238300, lr = 0.0255313, m = 0.9
I0815 06:13:39.219849 11101 solver.cpp:312] Iteration 238400 (6.72643 iter/s, 14.8667s/100 iter), loss = 1.88971
I0815 06:13:39.219921 11101 solver.cpp:334]     Train net output #0: loss = 2.02117 (* 1 = 2.02117 loss)
I0815 06:13:39.219930 11101 sgd_solver.cpp:136] Iteration 238400, lr = 0.0255, m = 0.9
I0815 06:13:54.446893 11101 solver.cpp:312] Iteration 238500 (6.56744 iter/s, 15.2266s/100 iter), loss = 1.63598
I0815 06:13:54.446965 11101 solver.cpp:334]     Train net output #0: loss = 1.82205 (* 1 = 1.82205 loss)
I0815 06:13:54.446985 11101 sgd_solver.cpp:136] Iteration 238500, lr = 0.0254687, m = 0.9
I0815 06:14:09.398672 11101 solver.cpp:312] Iteration 238600 (6.68835 iter/s, 14.9514s/100 iter), loss = 1.20877
I0815 06:14:09.398769 11101 solver.cpp:334]     Train net output #0: loss = 1.01695 (* 1 = 1.01695 loss)
I0815 06:14:09.398789 11101 sgd_solver.cpp:136] Iteration 238600, lr = 0.0254375, m = 0.9
I0815 06:14:24.001119 11101 solver.cpp:312] Iteration 238700 (6.84836 iter/s, 14.602s/100 iter), loss = 1.48495
I0815 06:14:24.001183 11101 solver.cpp:334]     Train net output #0: loss = 1.22946 (* 1 = 1.22946 loss)
I0815 06:14:24.001201 11101 sgd_solver.cpp:136] Iteration 238700, lr = 0.0254062, m = 0.9
I0815 06:14:38.342120 11101 solver.cpp:312] Iteration 238800 (6.97321 iter/s, 14.3406s/100 iter), loss = 1.98262
I0815 06:14:38.342278 11101 solver.cpp:334]     Train net output #0: loss = 1.92459 (* 1 = 1.92459 loss)
I0815 06:14:38.342300 11101 sgd_solver.cpp:136] Iteration 238800, lr = 0.025375, m = 0.9
I0815 06:14:53.461511 11101 solver.cpp:312] Iteration 238900 (6.61421 iter/s, 15.119s/100 iter), loss = 1.4623
I0815 06:14:53.461791 11101 solver.cpp:334]     Train net output #0: loss = 1.73103 (* 1 = 1.73103 loss)
I0815 06:14:53.461808 11101 sgd_solver.cpp:136] Iteration 238900, lr = 0.0253438, m = 0.9
I0815 06:15:08.222652 11101 solver.cpp:312] Iteration 239000 (6.77473 iter/s, 14.7607s/100 iter), loss = 1.52141
I0815 06:15:08.222681 11101 solver.cpp:334]     Train net output #0: loss = 1.71321 (* 1 = 1.71321 loss)
I0815 06:15:08.222687 11101 sgd_solver.cpp:136] Iteration 239000, lr = 0.0253125, m = 0.9
I0815 06:15:22.809897 11101 solver.cpp:312] Iteration 239100 (6.85549 iter/s, 14.5868s/100 iter), loss = 1.84358
I0815 06:15:22.809924 11101 solver.cpp:334]     Train net output #0: loss = 1.62314 (* 1 = 1.62314 loss)
I0815 06:15:22.809931 11101 sgd_solver.cpp:136] Iteration 239100, lr = 0.0252813, m = 0.9
I0815 06:15:37.504127 11101 solver.cpp:312] Iteration 239200 (6.80558 iter/s, 14.6938s/100 iter), loss = 1.25809
I0815 06:15:37.504231 11101 solver.cpp:334]     Train net output #0: loss = 0.966161 (* 1 = 0.966161 loss)
I0815 06:15:37.504251 11101 sgd_solver.cpp:136] Iteration 239200, lr = 0.02525, m = 0.9
I0815 06:15:52.308218 11101 solver.cpp:312] Iteration 239300 (6.75508 iter/s, 14.8037s/100 iter), loss = 1.51793
I0815 06:15:52.308269 11101 solver.cpp:334]     Train net output #0: loss = 1.26919 (* 1 = 1.26919 loss)
I0815 06:15:52.308281 11101 sgd_solver.cpp:136] Iteration 239300, lr = 0.0252187, m = 0.9
I0815 06:16:06.750617 11101 solver.cpp:312] Iteration 239400 (6.92425 iter/s, 14.442s/100 iter), loss = 1.64556
I0815 06:16:06.750681 11101 solver.cpp:334]     Train net output #0: loss = 1.77812 (* 1 = 1.77812 loss)
I0815 06:16:06.750699 11101 sgd_solver.cpp:136] Iteration 239400, lr = 0.0251875, m = 0.9
I0815 06:16:21.254098 11101 solver.cpp:312] Iteration 239500 (6.89509 iter/s, 14.5031s/100 iter), loss = 1.45676
I0815 06:16:21.254154 11101 solver.cpp:334]     Train net output #0: loss = 1.52155 (* 1 = 1.52155 loss)
I0815 06:16:21.254159 11101 sgd_solver.cpp:136] Iteration 239500, lr = 0.0251562, m = 0.9
I0815 06:16:35.909071 11101 solver.cpp:312] Iteration 239600 (6.82381 iter/s, 14.6546s/100 iter), loss = 1.6481
I0815 06:16:35.909101 11101 solver.cpp:334]     Train net output #0: loss = 1.49964 (* 1 = 1.49964 loss)
I0815 06:16:35.909106 11101 sgd_solver.cpp:136] Iteration 239600, lr = 0.025125, m = 0.9
I0815 06:16:50.808385 11101 solver.cpp:312] Iteration 239700 (6.7119 iter/s, 14.8989s/100 iter), loss = 1.92259
I0815 06:16:50.808409 11101 solver.cpp:334]     Train net output #0: loss = 2.065 (* 1 = 2.065 loss)
I0815 06:16:50.808413 11101 sgd_solver.cpp:136] Iteration 239700, lr = 0.0250938, m = 0.9
I0815 06:17:05.644743 11101 solver.cpp:312] Iteration 239800 (6.74038 iter/s, 14.836s/100 iter), loss = 1.44273
I0815 06:17:05.644822 11101 solver.cpp:334]     Train net output #0: loss = 1.53091 (* 1 = 1.53091 loss)
I0815 06:17:05.644829 11101 sgd_solver.cpp:136] Iteration 239800, lr = 0.0250625, m = 0.9
I0815 06:17:20.739500 11101 solver.cpp:312] Iteration 239900 (6.625 iter/s, 15.0943s/100 iter), loss = 1.52676
I0815 06:17:20.739569 11101 solver.cpp:334]     Train net output #0: loss = 1.59175 (* 1 = 1.59175 loss)
I0815 06:17:20.739589 11101 sgd_solver.cpp:136] Iteration 239900, lr = 0.0250313, m = 0.9
I0815 06:17:35.392765 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_240000.caffemodel
I0815 06:17:35.480537 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_240000.solverstate
I0815 06:17:35.486225 11101 solver.cpp:509] Iteration 240000, Testing net (#0)
I0815 06:17:56.347338 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.49753
I0815 06:17:56.347419 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.742469
I0815 06:17:56.347427 11101 solver.cpp:594]     Test net output #2: loss = 2.26978 (* 1 = 2.26978 loss)
I0815 06:17:56.347447 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8607s
I0815 06:17:56.488225 11101 solver.cpp:312] Iteration 240000 (2.79738 iter/s, 35.7477s/100 iter), loss = 1.66908
I0815 06:17:56.488251 11101 solver.cpp:334]     Train net output #0: loss = 1.59523 (* 1 = 1.59523 loss)
I0815 06:17:56.488257 11101 sgd_solver.cpp:136] Iteration 240000, lr = 0.025, m = 0.9
I0815 06:18:10.870095 11101 solver.cpp:312] Iteration 240100 (6.95339 iter/s, 14.3815s/100 iter), loss = 1.88306
I0815 06:18:10.870123 11101 solver.cpp:334]     Train net output #0: loss = 2.12934 (* 1 = 2.12934 loss)
I0815 06:18:10.870131 11101 sgd_solver.cpp:136] Iteration 240100, lr = 0.0249687, m = 0.9
I0815 06:18:25.298918 11101 solver.cpp:312] Iteration 240200 (6.93076 iter/s, 14.4284s/100 iter), loss = 1.89318
I0815 06:18:25.298946 11101 solver.cpp:334]     Train net output #0: loss = 2.03239 (* 1 = 2.03239 loss)
I0815 06:18:25.298954 11101 sgd_solver.cpp:136] Iteration 240200, lr = 0.0249375, m = 0.9
I0815 06:18:39.756623 11101 solver.cpp:312] Iteration 240300 (6.91692 iter/s, 14.4573s/100 iter), loss = 1.61113
I0815 06:18:39.756719 11101 solver.cpp:334]     Train net output #0: loss = 1.88683 (* 1 = 1.88683 loss)
I0815 06:18:39.756736 11101 sgd_solver.cpp:136] Iteration 240300, lr = 0.0249062, m = 0.9
I0815 06:18:54.441941 11101 solver.cpp:312] Iteration 240400 (6.80971 iter/s, 14.6849s/100 iter), loss = 2.02505
I0815 06:18:54.441967 11101 solver.cpp:334]     Train net output #0: loss = 2.00602 (* 1 = 2.00602 loss)
I0815 06:18:54.441974 11101 sgd_solver.cpp:136] Iteration 240400, lr = 0.024875, m = 0.9
I0815 06:19:09.232208 11101 solver.cpp:312] Iteration 240500 (6.76139 iter/s, 14.7899s/100 iter), loss = 1.52049
I0815 06:19:09.232237 11101 solver.cpp:334]     Train net output #0: loss = 1.73093 (* 1 = 1.73093 loss)
I0815 06:19:09.232244 11101 sgd_solver.cpp:136] Iteration 240500, lr = 0.0248438, m = 0.9
I0815 06:19:23.808166 11101 solver.cpp:312] Iteration 240600 (6.8608 iter/s, 14.5756s/100 iter), loss = 1.77504
I0815 06:19:23.808243 11101 solver.cpp:334]     Train net output #0: loss = 1.71214 (* 1 = 1.71214 loss)
I0815 06:19:23.808257 11101 sgd_solver.cpp:136] Iteration 240600, lr = 0.0248125, m = 0.9
I0815 06:19:38.641453 11101 solver.cpp:312] Iteration 240700 (6.74178 iter/s, 14.8329s/100 iter), loss = 1.76398
I0815 06:19:38.641477 11101 solver.cpp:334]     Train net output #0: loss = 1.41623 (* 1 = 1.41623 loss)
I0815 06:19:38.641482 11101 sgd_solver.cpp:136] Iteration 240700, lr = 0.0247813, m = 0.9
I0815 06:19:53.339618 11101 solver.cpp:312] Iteration 240800 (6.80377 iter/s, 14.6977s/100 iter), loss = 2.17815
I0815 06:19:53.339643 11101 solver.cpp:334]     Train net output #0: loss = 2.44743 (* 1 = 2.44743 loss)
I0815 06:19:53.339648 11101 sgd_solver.cpp:136] Iteration 240800, lr = 0.02475, m = 0.9
I0815 06:20:07.690284 11101 solver.cpp:312] Iteration 240900 (6.96851 iter/s, 14.3503s/100 iter), loss = 1.87516
I0815 06:20:07.690390 11101 solver.cpp:334]     Train net output #0: loss = 1.8601 (* 1 = 1.8601 loss)
I0815 06:20:07.690410 11101 sgd_solver.cpp:136] Iteration 240900, lr = 0.0247188, m = 0.9
I0815 06:20:22.362107 11101 solver.cpp:312] Iteration 241000 (6.81597 iter/s, 14.6714s/100 iter), loss = 1.72758
I0815 06:20:22.362169 11101 solver.cpp:334]     Train net output #0: loss = 1.7845 (* 1 = 1.7845 loss)
I0815 06:20:22.362176 11101 sgd_solver.cpp:136] Iteration 241000, lr = 0.0246875, m = 0.9
I0815 06:20:36.753612 11101 solver.cpp:312] Iteration 241100 (6.94873 iter/s, 14.3911s/100 iter), loss = 1.6211
I0815 06:20:36.753640 11101 solver.cpp:334]     Train net output #0: loss = 1.40754 (* 1 = 1.40754 loss)
I0815 06:20:36.753645 11101 sgd_solver.cpp:136] Iteration 241100, lr = 0.0246562, m = 0.9
I0815 06:20:51.567111 11101 solver.cpp:312] Iteration 241200 (6.75078 iter/s, 14.8131s/100 iter), loss = 1.65334
I0815 06:20:51.567184 11101 solver.cpp:334]     Train net output #0: loss = 1.95761 (* 1 = 1.95761 loss)
I0815 06:20:51.567191 11101 sgd_solver.cpp:136] Iteration 241200, lr = 0.024625, m = 0.9
I0815 06:21:06.122134 11101 solver.cpp:312] Iteration 241300 (6.87067 iter/s, 14.5546s/100 iter), loss = 1.93441
I0815 06:21:06.122156 11101 solver.cpp:334]     Train net output #0: loss = 1.64255 (* 1 = 1.64255 loss)
I0815 06:21:06.122160 11101 sgd_solver.cpp:136] Iteration 241300, lr = 0.0245938, m = 0.9
I0815 06:21:20.806254 11101 solver.cpp:312] Iteration 241400 (6.81027 iter/s, 14.6837s/100 iter), loss = 1.81199
I0815 06:21:20.806279 11101 solver.cpp:334]     Train net output #0: loss = 1.65322 (* 1 = 1.65322 loss)
I0815 06:21:20.806283 11101 sgd_solver.cpp:136] Iteration 241400, lr = 0.0245625, m = 0.9
I0815 06:21:35.451413 11101 solver.cpp:312] Iteration 241500 (6.82838 iter/s, 14.6448s/100 iter), loss = 1.54849
I0815 06:21:35.451488 11101 solver.cpp:334]     Train net output #0: loss = 1.49523 (* 1 = 1.49523 loss)
I0815 06:21:35.451495 11101 sgd_solver.cpp:136] Iteration 241500, lr = 0.0245313, m = 0.9
I0815 06:21:49.947417 11101 solver.cpp:312] Iteration 241600 (6.89864 iter/s, 14.4956s/100 iter), loss = 1.88995
I0815 06:21:49.947441 11101 solver.cpp:334]     Train net output #0: loss = 1.8294 (* 1 = 1.8294 loss)
I0815 06:21:49.947445 11101 sgd_solver.cpp:136] Iteration 241600, lr = 0.0245, m = 0.9
I0815 06:22:04.271759 11101 solver.cpp:312] Iteration 241700 (6.98132 iter/s, 14.3239s/100 iter), loss = 1.44808
I0815 06:22:04.271976 11101 solver.cpp:334]     Train net output #0: loss = 1.56928 (* 1 = 1.56928 loss)
I0815 06:22:04.272083 11101 sgd_solver.cpp:136] Iteration 241700, lr = 0.0244687, m = 0.9
I0815 06:22:18.755923 11101 solver.cpp:312] Iteration 241800 (6.90428 iter/s, 14.4838s/100 iter), loss = 1.45961
I0815 06:22:18.755987 11101 solver.cpp:334]     Train net output #0: loss = 1.61564 (* 1 = 1.61564 loss)
I0815 06:22:18.755995 11101 sgd_solver.cpp:136] Iteration 241800, lr = 0.0244375, m = 0.9
I0815 06:22:33.103525 11101 solver.cpp:312] Iteration 241900 (6.97 iter/s, 14.3472s/100 iter), loss = 1.8244
I0815 06:22:33.103552 11101 solver.cpp:334]     Train net output #0: loss = 1.72093 (* 1 = 1.72093 loss)
I0815 06:22:33.103598 11101 sgd_solver.cpp:136] Iteration 241900, lr = 0.0244062, m = 0.9
I0815 06:22:47.318699 11101 solver.cpp:509] Iteration 242000, Testing net (#0)
I0815 06:22:59.104153 11099 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 06:23:05.090103 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 06:23:07.983862 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.506941
I0815 06:23:07.983883 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.757469
I0815 06:23:07.983888 11101 solver.cpp:594]     Test net output #2: loss = 2.18566 (* 1 = 2.18566 loss)
I0815 06:23:07.983906 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6646s
I0815 06:23:08.124124 11101 solver.cpp:312] Iteration 242000 (2.85554 iter/s, 35.0196s/100 iter), loss = 1.67457
I0815 06:23:08.124307 11101 solver.cpp:334]     Train net output #0: loss = 1.4493 (* 1 = 1.4493 loss)
I0815 06:23:08.124390 11101 sgd_solver.cpp:136] Iteration 242000, lr = 0.024375, m = 0.9
I0815 06:23:22.738632 11101 solver.cpp:312] Iteration 242100 (6.8427 iter/s, 14.6141s/100 iter), loss = 1.54537
I0815 06:23:22.738662 11101 solver.cpp:334]     Train net output #0: loss = 1.529 (* 1 = 1.529 loss)
I0815 06:23:22.738668 11101 sgd_solver.cpp:136] Iteration 242100, lr = 0.0243438, m = 0.9
I0815 06:23:37.443025 11101 solver.cpp:312] Iteration 242200 (6.80088 iter/s, 14.704s/100 iter), loss = 1.88207
I0815 06:23:37.443117 11101 solver.cpp:334]     Train net output #0: loss = 1.57852 (* 1 = 1.57852 loss)
I0815 06:23:37.443130 11101 sgd_solver.cpp:136] Iteration 242200, lr = 0.0243125, m = 0.9
I0815 06:23:52.117399 11101 solver.cpp:312] Iteration 242300 (6.81479 iter/s, 14.674s/100 iter), loss = 1.75822
I0815 06:23:52.117429 11101 solver.cpp:334]     Train net output #0: loss = 1.96147 (* 1 = 1.96147 loss)
I0815 06:23:52.117434 11101 sgd_solver.cpp:136] Iteration 242300, lr = 0.0242813, m = 0.9
I0815 06:24:06.676041 11101 solver.cpp:312] Iteration 242400 (6.86896 iter/s, 14.5582s/100 iter), loss = 1.7386
I0815 06:24:06.676066 11101 solver.cpp:334]     Train net output #0: loss = 1.69987 (* 1 = 1.69987 loss)
I0815 06:24:06.676071 11101 sgd_solver.cpp:136] Iteration 242400, lr = 0.02425, m = 0.9
I0815 06:24:21.528722 11101 solver.cpp:312] Iteration 242500 (6.73298 iter/s, 14.8523s/100 iter), loss = 1.78196
I0815 06:24:21.528805 11101 solver.cpp:334]     Train net output #0: loss = 1.63268 (* 1 = 1.63268 loss)
I0815 06:24:21.528816 11101 sgd_solver.cpp:136] Iteration 242500, lr = 0.0242188, m = 0.9
I0815 06:24:36.372915 11101 solver.cpp:312] Iteration 242600 (6.73683 iter/s, 14.8438s/100 iter), loss = 1.69294
I0815 06:24:36.372946 11101 solver.cpp:334]     Train net output #0: loss = 1.77614 (* 1 = 1.77614 loss)
I0815 06:24:36.372954 11101 sgd_solver.cpp:136] Iteration 242600, lr = 0.0241875, m = 0.9
I0815 06:24:51.271975 11101 solver.cpp:312] Iteration 242700 (6.71202 iter/s, 14.8986s/100 iter), loss = 1.38837
I0815 06:24:51.272042 11101 solver.cpp:334]     Train net output #0: loss = 1.52939 (* 1 = 1.52939 loss)
I0815 06:24:51.272059 11101 sgd_solver.cpp:136] Iteration 242700, lr = 0.0241562, m = 0.9
I0815 06:25:06.446578 11101 solver.cpp:312] Iteration 242800 (6.59014 iter/s, 15.1742s/100 iter), loss = 1.45032
I0815 06:25:06.446657 11101 solver.cpp:334]     Train net output #0: loss = 1.86935 (* 1 = 1.86935 loss)
I0815 06:25:06.446665 11101 sgd_solver.cpp:136] Iteration 242800, lr = 0.024125, m = 0.9
I0815 06:25:20.932319 11101 solver.cpp:312] Iteration 242900 (6.90353 iter/s, 14.4853s/100 iter), loss = 1.58065
I0815 06:25:20.932384 11101 solver.cpp:334]     Train net output #0: loss = 1.57965 (* 1 = 1.57965 loss)
I0815 06:25:20.932401 11101 sgd_solver.cpp:136] Iteration 242900, lr = 0.0240937, m = 0.9
I0815 06:25:35.736239 11101 solver.cpp:312] Iteration 243000 (6.75516 iter/s, 14.8035s/100 iter), loss = 1.75026
I0815 06:25:35.736310 11101 solver.cpp:334]     Train net output #0: loss = 1.76651 (* 1 = 1.76651 loss)
I0815 06:25:35.736330 11101 sgd_solver.cpp:136] Iteration 243000, lr = 0.0240625, m = 0.9
I0815 06:25:50.620908 11101 solver.cpp:312] Iteration 243100 (6.71851 iter/s, 14.8843s/100 iter), loss = 1.75514
I0815 06:25:50.621124 11101 solver.cpp:334]     Train net output #0: loss = 2.12403 (* 1 = 2.12403 loss)
I0815 06:25:50.621217 11101 sgd_solver.cpp:136] Iteration 243100, lr = 0.0240313, m = 0.9
I0815 06:26:04.979698 11101 solver.cpp:312] Iteration 243200 (6.96457 iter/s, 14.3584s/100 iter), loss = 1.83585
I0815 06:26:04.979727 11101 solver.cpp:334]     Train net output #0: loss = 1.9884 (* 1 = 1.9884 loss)
I0815 06:26:04.979732 11101 sgd_solver.cpp:136] Iteration 243200, lr = 0.024, m = 0.9
I0815 06:26:19.721464 11101 solver.cpp:312] Iteration 243300 (6.78364 iter/s, 14.7414s/100 iter), loss = 1.84952
I0815 06:26:19.721516 11101 solver.cpp:334]     Train net output #0: loss = 1.78972 (* 1 = 1.78972 loss)
I0815 06:26:19.721529 11101 sgd_solver.cpp:136] Iteration 243300, lr = 0.0239688, m = 0.9
I0815 06:26:34.566232 11101 solver.cpp:312] Iteration 243400 (6.73657 iter/s, 14.8444s/100 iter), loss = 1.54489
I0815 06:26:34.566329 11101 solver.cpp:334]     Train net output #0: loss = 1.63291 (* 1 = 1.63291 loss)
I0815 06:26:34.566349 11101 sgd_solver.cpp:136] Iteration 243400, lr = 0.0239375, m = 0.9
I0815 06:26:49.240319 11101 solver.cpp:312] Iteration 243500 (6.81492 iter/s, 14.6737s/100 iter), loss = 1.87271
I0815 06:26:49.240348 11101 solver.cpp:334]     Train net output #0: loss = 1.65812 (* 1 = 1.65812 loss)
I0815 06:26:49.240353 11101 sgd_solver.cpp:136] Iteration 243500, lr = 0.0239062, m = 0.9
I0815 06:27:04.210921 11101 solver.cpp:312] Iteration 243600 (6.67994 iter/s, 14.9702s/100 iter), loss = 1.61609
I0815 06:27:04.211135 11101 solver.cpp:334]     Train net output #0: loss = 1.61533 (* 1 = 1.61533 loss)
I0815 06:27:04.211244 11101 sgd_solver.cpp:136] Iteration 243600, lr = 0.023875, m = 0.9
I0815 06:27:18.866281 11101 solver.cpp:312] Iteration 243700 (6.82363 iter/s, 14.655s/100 iter), loss = 1.3007
I0815 06:27:18.866353 11101 solver.cpp:334]     Train net output #0: loss = 1.49938 (* 1 = 1.49938 loss)
I0815 06:27:18.866358 11101 sgd_solver.cpp:136] Iteration 243700, lr = 0.0238437, m = 0.9
I0815 06:27:33.572306 11101 solver.cpp:312] Iteration 243800 (6.80012 iter/s, 14.7056s/100 iter), loss = 1.6161
I0815 06:27:33.572336 11101 solver.cpp:334]     Train net output #0: loss = 1.44921 (* 1 = 1.44921 loss)
I0815 06:27:33.572343 11101 sgd_solver.cpp:136] Iteration 243800, lr = 0.0238125, m = 0.9
I0815 06:27:48.105517 11101 solver.cpp:312] Iteration 243900 (6.88098 iter/s, 14.5328s/100 iter), loss = 1.49637
I0815 06:27:48.105545 11101 solver.cpp:334]     Train net output #0: loss = 1.05997 (* 1 = 1.05997 loss)
I0815 06:27:48.105551 11101 sgd_solver.cpp:136] Iteration 243900, lr = 0.0237813, m = 0.9
I0815 06:28:02.590684 11101 solver.cpp:509] Iteration 244000, Testing net (#0)
I0815 06:28:23.362679 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.504471
I0815 06:28:23.362704 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.753057
I0815 06:28:23.362711 11101 solver.cpp:594]     Test net output #2: loss = 2.21599 (* 1 = 2.21599 loss)
I0815 06:28:23.362731 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7715s
I0815 06:28:23.502315 11101 solver.cpp:312] Iteration 244000 (2.82519 iter/s, 35.3958s/100 iter), loss = 1.75566
I0815 06:28:23.502341 11101 solver.cpp:334]     Train net output #0: loss = 1.60223 (* 1 = 1.60223 loss)
I0815 06:28:23.502344 11101 sgd_solver.cpp:136] Iteration 244000, lr = 0.02375, m = 0.9
I0815 06:28:38.114594 11101 solver.cpp:312] Iteration 244100 (6.84375 iter/s, 14.6119s/100 iter), loss = 1.81816
I0815 06:28:38.114662 11101 solver.cpp:334]     Train net output #0: loss = 1.81454 (* 1 = 1.81454 loss)
I0815 06:28:38.114670 11101 sgd_solver.cpp:136] Iteration 244100, lr = 0.0237188, m = 0.9
I0815 06:28:52.859336 11101 solver.cpp:312] Iteration 244200 (6.78227 iter/s, 14.7443s/100 iter), loss = 1.58483
I0815 06:28:52.859410 11101 solver.cpp:334]     Train net output #0: loss = 1.79082 (* 1 = 1.79082 loss)
I0815 06:28:52.859437 11101 sgd_solver.cpp:136] Iteration 244200, lr = 0.0236875, m = 0.9
I0815 06:29:07.830687 11101 solver.cpp:312] Iteration 244300 (6.67961 iter/s, 14.9709s/100 iter), loss = 1.68204
I0815 06:29:07.830912 11101 solver.cpp:334]     Train net output #0: loss = 1.268 (* 1 = 1.268 loss)
I0815 06:29:07.831032 11101 sgd_solver.cpp:136] Iteration 244300, lr = 0.0236562, m = 0.9
I0815 06:29:22.444869 11101 solver.cpp:312] Iteration 244400 (6.84286 iter/s, 14.6138s/100 iter), loss = 1.67321
I0815 06:29:22.444952 11101 solver.cpp:334]     Train net output #0: loss = 1.64546 (* 1 = 1.64546 loss)
I0815 06:29:22.444958 11101 sgd_solver.cpp:136] Iteration 244400, lr = 0.023625, m = 0.9
I0815 06:29:36.910585 11101 solver.cpp:312] Iteration 244500 (6.91309 iter/s, 14.4653s/100 iter), loss = 1.77108
I0815 06:29:36.910609 11101 solver.cpp:334]     Train net output #0: loss = 1.61626 (* 1 = 1.61626 loss)
I0815 06:29:36.910615 11101 sgd_solver.cpp:136] Iteration 244500, lr = 0.0235937, m = 0.9
I0815 06:29:51.430102 11101 solver.cpp:312] Iteration 244600 (6.88747 iter/s, 14.5191s/100 iter), loss = 1.7032
I0815 06:29:51.430132 11101 solver.cpp:334]     Train net output #0: loss = 1.5518 (* 1 = 1.5518 loss)
I0815 06:29:51.430138 11101 sgd_solver.cpp:136] Iteration 244600, lr = 0.0235625, m = 0.9
I0815 06:30:06.063840 11101 solver.cpp:312] Iteration 244700 (6.83371 iter/s, 14.6333s/100 iter), loss = 1.66782
I0815 06:30:06.063890 11101 solver.cpp:334]     Train net output #0: loss = 1.52713 (* 1 = 1.52713 loss)
I0815 06:30:06.063896 11101 sgd_solver.cpp:136] Iteration 244700, lr = 0.0235313, m = 0.9
I0815 06:30:20.625213 11101 solver.cpp:312] Iteration 244800 (6.86767 iter/s, 14.561s/100 iter), loss = 1.48936
I0815 06:30:20.625244 11101 solver.cpp:334]     Train net output #0: loss = 1.45509 (* 1 = 1.45509 loss)
I0815 06:30:20.625250 11101 sgd_solver.cpp:136] Iteration 244800, lr = 0.0235, m = 0.9
I0815 06:30:35.046751 11101 solver.cpp:312] Iteration 244900 (6.93426 iter/s, 14.4211s/100 iter), loss = 1.78382
I0815 06:30:35.046819 11101 solver.cpp:334]     Train net output #0: loss = 1.92073 (* 1 = 1.92073 loss)
I0815 06:30:35.046839 11101 sgd_solver.cpp:136] Iteration 244900, lr = 0.0234688, m = 0.9
I0815 06:30:49.873178 11101 solver.cpp:312] Iteration 245000 (6.7449 iter/s, 14.826s/100 iter), loss = 1.76954
I0815 06:30:49.873239 11101 solver.cpp:334]     Train net output #0: loss = 2.1779 (* 1 = 2.1779 loss)
I0815 06:30:49.873245 11101 sgd_solver.cpp:136] Iteration 245000, lr = 0.0234375, m = 0.9
I0815 06:31:04.713083 11101 solver.cpp:312] Iteration 245100 (6.73877 iter/s, 14.8395s/100 iter), loss = 1.73101
I0815 06:31:04.713107 11101 solver.cpp:334]     Train net output #0: loss = 2.01444 (* 1 = 2.01444 loss)
I0815 06:31:04.713112 11101 sgd_solver.cpp:136] Iteration 245100, lr = 0.0234063, m = 0.9
I0815 06:31:19.024821 11101 solver.cpp:312] Iteration 245200 (6.98746 iter/s, 14.3113s/100 iter), loss = 1.67864
I0815 06:31:19.024883 11101 solver.cpp:334]     Train net output #0: loss = 1.6529 (* 1 = 1.6529 loss)
I0815 06:31:19.024900 11101 sgd_solver.cpp:136] Iteration 245200, lr = 0.023375, m = 0.9
I0815 06:31:33.337571 11101 solver.cpp:312] Iteration 245300 (6.98697 iter/s, 14.3124s/100 iter), loss = 1.73749
I0815 06:31:33.337688 11101 solver.cpp:334]     Train net output #0: loss = 1.70026 (* 1 = 1.70026 loss)
I0815 06:31:33.337705 11101 sgd_solver.cpp:136] Iteration 245300, lr = 0.0233437, m = 0.9
I0815 06:31:48.571058 11101 solver.cpp:312] Iteration 245400 (6.56467 iter/s, 15.2331s/100 iter), loss = 1.47488
I0815 06:31:48.571120 11101 solver.cpp:334]     Train net output #0: loss = 1.52019 (* 1 = 1.52019 loss)
I0815 06:31:48.571137 11101 sgd_solver.cpp:136] Iteration 245400, lr = 0.0233125, m = 0.9
I0815 06:32:03.731374 11101 solver.cpp:312] Iteration 245500 (6.59635 iter/s, 15.1599s/100 iter), loss = 1.75761
I0815 06:32:03.731473 11101 solver.cpp:334]     Train net output #0: loss = 1.99316 (* 1 = 1.99316 loss)
I0815 06:32:03.731494 11101 sgd_solver.cpp:136] Iteration 245500, lr = 0.0232813, m = 0.9
I0815 06:32:18.458410 11101 solver.cpp:312] Iteration 245600 (6.79042 iter/s, 14.7266s/100 iter), loss = 1.39237
I0815 06:32:18.458570 11101 solver.cpp:334]     Train net output #0: loss = 1.38462 (* 1 = 1.38462 loss)
I0815 06:32:18.458642 11101 sgd_solver.cpp:136] Iteration 245600, lr = 0.02325, m = 0.9
I0815 06:32:32.960053 11101 solver.cpp:312] Iteration 245700 (6.89596 iter/s, 14.5012s/100 iter), loss = 1.74163
I0815 06:32:32.960081 11101 solver.cpp:334]     Train net output #0: loss = 1.66032 (* 1 = 1.66032 loss)
I0815 06:32:32.960085 11101 sgd_solver.cpp:136] Iteration 245700, lr = 0.0232188, m = 0.9
I0815 06:32:47.605134 11101 solver.cpp:312] Iteration 245800 (6.82842 iter/s, 14.6447s/100 iter), loss = 1.56626
I0815 06:32:47.605211 11101 solver.cpp:334]     Train net output #0: loss = 1.65395 (* 1 = 1.65395 loss)
I0815 06:32:47.605217 11101 sgd_solver.cpp:136] Iteration 245800, lr = 0.0231875, m = 0.9
I0815 06:33:02.244334 11101 solver.cpp:312] Iteration 245900 (6.83116 iter/s, 14.6388s/100 iter), loss = 1.79006
I0815 06:33:02.244362 11101 solver.cpp:334]     Train net output #0: loss = 2.01438 (* 1 = 2.01438 loss)
I0815 06:33:02.244369 11101 sgd_solver.cpp:136] Iteration 245900, lr = 0.0231562, m = 0.9
I0815 06:33:16.701408 11101 solver.cpp:509] Iteration 246000, Testing net (#0)
I0815 06:33:37.414432 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.51953
I0815 06:33:37.414485 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.76794
I0815 06:33:37.414494 11101 solver.cpp:594]     Test net output #2: loss = 2.13936 (* 1 = 2.13936 loss)
I0815 06:33:37.414517 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7125s
I0815 06:33:37.588747 11101 solver.cpp:312] Iteration 246000 (2.82938 iter/s, 35.3435s/100 iter), loss = 1.50421
I0815 06:33:37.588776 11101 solver.cpp:334]     Train net output #0: loss = 1.42327 (* 1 = 1.42327 loss)
I0815 06:33:37.588783 11101 sgd_solver.cpp:136] Iteration 246000, lr = 0.023125, m = 0.9
I0815 06:33:52.010916 11101 solver.cpp:312] Iteration 246100 (6.93396 iter/s, 14.4218s/100 iter), loss = 1.67141
I0815 06:33:52.011129 11101 solver.cpp:334]     Train net output #0: loss = 1.7056 (* 1 = 1.7056 loss)
I0815 06:33:52.011238 11101 sgd_solver.cpp:136] Iteration 246100, lr = 0.0230937, m = 0.9
I0815 06:34:06.501606 11101 solver.cpp:312] Iteration 246200 (6.90117 iter/s, 14.4903s/100 iter), loss = 1.58411
I0815 06:34:06.501678 11101 solver.cpp:334]     Train net output #0: loss = 1.87024 (* 1 = 1.87024 loss)
I0815 06:34:06.501698 11101 sgd_solver.cpp:136] Iteration 246200, lr = 0.0230625, m = 0.9
I0815 06:34:21.279852 11101 solver.cpp:312] Iteration 246300 (6.76689 iter/s, 14.7778s/100 iter), loss = 1.64391
I0815 06:34:21.279947 11101 solver.cpp:334]     Train net output #0: loss = 2.00846 (* 1 = 2.00846 loss)
I0815 06:34:21.279965 11101 sgd_solver.cpp:136] Iteration 246300, lr = 0.0230313, m = 0.9
I0815 06:34:35.969277 11101 solver.cpp:312] Iteration 246400 (6.80781 iter/s, 14.689s/100 iter), loss = 1.57957
I0815 06:34:35.969329 11101 solver.cpp:334]     Train net output #0: loss = 1.62355 (* 1 = 1.62355 loss)
I0815 06:34:35.969342 11101 sgd_solver.cpp:136] Iteration 246400, lr = 0.023, m = 0.9
I0815 06:34:50.527277 11101 solver.cpp:312] Iteration 246500 (6.86926 iter/s, 14.5576s/100 iter), loss = 1.79855
I0815 06:34:50.527304 11101 solver.cpp:334]     Train net output #0: loss = 1.19732 (* 1 = 1.19732 loss)
I0815 06:34:50.527308 11101 sgd_solver.cpp:136] Iteration 246500, lr = 0.0229688, m = 0.9
I0815 06:35:05.356822 11101 solver.cpp:312] Iteration 246600 (6.74348 iter/s, 14.8291s/100 iter), loss = 1.62171
I0815 06:35:05.364181 11101 solver.cpp:334]     Train net output #0: loss = 1.55749 (* 1 = 1.55749 loss)
I0815 06:35:05.364200 11101 sgd_solver.cpp:136] Iteration 246600, lr = 0.0229375, m = 0.9
I0815 06:35:19.833176 11101 solver.cpp:312] Iteration 246700 (6.90801 iter/s, 14.476s/100 iter), loss = 1.62692
I0815 06:35:19.833242 11101 solver.cpp:334]     Train net output #0: loss = 2.13277 (* 1 = 2.13277 loss)
I0815 06:35:19.833259 11101 sgd_solver.cpp:136] Iteration 246700, lr = 0.0229062, m = 0.9
I0815 06:35:34.387776 11101 solver.cpp:312] Iteration 246800 (6.87087 iter/s, 14.5542s/100 iter), loss = 1.69429
I0815 06:35:34.387850 11101 solver.cpp:334]     Train net output #0: loss = 1.68498 (* 1 = 1.68498 loss)
I0815 06:35:34.387871 11101 sgd_solver.cpp:136] Iteration 246800, lr = 0.022875, m = 0.9
I0815 06:35:48.750111 11101 solver.cpp:312] Iteration 246900 (6.96285 iter/s, 14.3619s/100 iter), loss = 1.41032
I0815 06:35:48.750231 11101 solver.cpp:334]     Train net output #0: loss = 1.56783 (* 1 = 1.56783 loss)
I0815 06:35:48.750250 11101 sgd_solver.cpp:136] Iteration 246900, lr = 0.0228437, m = 0.9
I0815 06:36:03.291251 11101 solver.cpp:312] Iteration 247000 (6.87723 iter/s, 14.5407s/100 iter), loss = 1.57988
I0815 06:36:03.291280 11101 solver.cpp:334]     Train net output #0: loss = 1.83635 (* 1 = 1.83635 loss)
I0815 06:36:03.291285 11101 sgd_solver.cpp:136] Iteration 247000, lr = 0.0228125, m = 0.9
I0815 06:36:17.842607 11101 solver.cpp:312] Iteration 247100 (6.8724 iter/s, 14.551s/100 iter), loss = 1.6502
I0815 06:36:17.842631 11101 solver.cpp:334]     Train net output #0: loss = 1.44534 (* 1 = 1.44534 loss)
I0815 06:36:17.842636 11101 sgd_solver.cpp:136] Iteration 247100, lr = 0.0227813, m = 0.9
I0815 06:36:32.429579 11101 solver.cpp:312] Iteration 247200 (6.85562 iter/s, 14.5866s/100 iter), loss = 1.26848
I0815 06:36:32.429646 11101 solver.cpp:334]     Train net output #0: loss = 1.28413 (* 1 = 1.28413 loss)
I0815 06:36:32.429653 11101 sgd_solver.cpp:136] Iteration 247200, lr = 0.02275, m = 0.9
I0815 06:36:47.090755 11101 solver.cpp:312] Iteration 247300 (6.82092 iter/s, 14.6608s/100 iter), loss = 1.93986
I0815 06:36:47.090785 11101 solver.cpp:334]     Train net output #0: loss = 1.88312 (* 1 = 1.88312 loss)
I0815 06:36:47.090790 11101 sgd_solver.cpp:136] Iteration 247300, lr = 0.0227188, m = 0.9
I0815 06:37:01.603854 11101 solver.cpp:312] Iteration 247400 (6.89052 iter/s, 14.5127s/100 iter), loss = 1.77371
I0815 06:37:01.603881 11101 solver.cpp:334]     Train net output #0: loss = 1.45429 (* 1 = 1.45429 loss)
I0815 06:37:01.603888 11101 sgd_solver.cpp:136] Iteration 247400, lr = 0.0226875, m = 0.9
I0815 06:37:15.995417 11101 solver.cpp:312] Iteration 247500 (6.94871 iter/s, 14.3912s/100 iter), loss = 1.77346
I0815 06:37:15.995514 11101 solver.cpp:334]     Train net output #0: loss = 1.86916 (* 1 = 1.86916 loss)
I0815 06:37:15.995532 11101 sgd_solver.cpp:136] Iteration 247500, lr = 0.0226563, m = 0.9
I0815 06:37:30.823400 11101 solver.cpp:312] Iteration 247600 (6.74419 iter/s, 14.8276s/100 iter), loss = 1.94092
I0815 06:37:30.823433 11101 solver.cpp:334]     Train net output #0: loss = 2.07137 (* 1 = 2.07137 loss)
I0815 06:37:30.823438 11101 sgd_solver.cpp:136] Iteration 247600, lr = 0.022625, m = 0.9
I0815 06:37:45.573478 11101 solver.cpp:312] Iteration 247700 (6.77981 iter/s, 14.7497s/100 iter), loss = 1.62771
I0815 06:37:45.573504 11101 solver.cpp:334]     Train net output #0: loss = 1.80637 (* 1 = 1.80637 loss)
I0815 06:37:45.573510 11101 sgd_solver.cpp:136] Iteration 247700, lr = 0.0225937, m = 0.9
I0815 06:38:00.299695 11101 solver.cpp:312] Iteration 247800 (6.7908 iter/s, 14.7258s/100 iter), loss = 1.87791
I0815 06:38:00.299777 11101 solver.cpp:334]     Train net output #0: loss = 1.88321 (* 1 = 1.88321 loss)
I0815 06:38:00.299790 11101 sgd_solver.cpp:136] Iteration 247800, lr = 0.0225625, m = 0.9
I0815 06:38:15.019618 11101 solver.cpp:312] Iteration 247900 (6.7937 iter/s, 14.7195s/100 iter), loss = 2.0611
I0815 06:38:15.019670 11101 solver.cpp:334]     Train net output #0: loss = 2.30873 (* 1 = 2.30873 loss)
I0815 06:38:15.019683 11101 sgd_solver.cpp:136] Iteration 247900, lr = 0.0225312, m = 0.9
I0815 06:38:29.607725 11101 solver.cpp:509] Iteration 248000, Testing net (#0)
I0815 06:38:44.008827 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 06:38:50.232787 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.500235
I0815 06:38:50.232812 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.749116
I0815 06:38:50.232820 11101 solver.cpp:594]     Test net output #2: loss = 2.22554 (* 1 = 2.22554 loss)
I0815 06:38:50.232839 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.6246s
I0815 06:38:50.389099 11101 solver.cpp:312] Iteration 248000 (2.82737 iter/s, 35.3685s/100 iter), loss = 1.56538
I0815 06:38:50.389123 11101 solver.cpp:334]     Train net output #0: loss = 1.42107 (* 1 = 1.42107 loss)
I0815 06:38:50.389129 11101 sgd_solver.cpp:136] Iteration 248000, lr = 0.0225, m = 0.9
I0815 06:39:04.996140 11101 solver.cpp:312] Iteration 248100 (6.84621 iter/s, 14.6066s/100 iter), loss = 1.43267
I0815 06:39:04.996204 11101 solver.cpp:334]     Train net output #0: loss = 1.44937 (* 1 = 1.44937 loss)
I0815 06:39:04.996223 11101 sgd_solver.cpp:136] Iteration 248100, lr = 0.0224688, m = 0.9
I0815 06:39:19.601152 11101 solver.cpp:312] Iteration 248200 (6.84715 iter/s, 14.6046s/100 iter), loss = 1.64981
I0815 06:39:19.601267 11101 solver.cpp:334]     Train net output #0: loss = 1.57463 (* 1 = 1.57463 loss)
I0815 06:39:19.601274 11101 sgd_solver.cpp:136] Iteration 248200, lr = 0.0224375, m = 0.9
I0815 06:39:34.199146 11101 solver.cpp:312] Iteration 248300 (6.85045 iter/s, 14.5976s/100 iter), loss = 1.92391
I0815 06:39:34.199175 11101 solver.cpp:334]     Train net output #0: loss = 1.87068 (* 1 = 1.87068 loss)
I0815 06:39:34.199182 11101 sgd_solver.cpp:136] Iteration 248300, lr = 0.0224063, m = 0.9
I0815 06:39:49.308611 11101 solver.cpp:312] Iteration 248400 (6.61855 iter/s, 15.109s/100 iter), loss = 1.68007
I0815 06:39:49.308658 11101 solver.cpp:334]     Train net output #0: loss = 1.87463 (* 1 = 1.87463 loss)
I0815 06:39:49.308671 11101 sgd_solver.cpp:136] Iteration 248400, lr = 0.022375, m = 0.9
I0815 06:40:03.917498 11101 solver.cpp:312] Iteration 248500 (6.84534 iter/s, 14.6085s/100 iter), loss = 1.56725
I0815 06:40:03.919078 11101 solver.cpp:334]     Train net output #0: loss = 1.93674 (* 1 = 1.93674 loss)
I0815 06:40:03.919098 11101 sgd_solver.cpp:136] Iteration 248500, lr = 0.0223437, m = 0.9
I0815 06:40:18.711127 11101 solver.cpp:312] Iteration 248600 (6.75985 iter/s, 14.7932s/100 iter), loss = 1.97253
I0815 06:40:18.711154 11101 solver.cpp:334]     Train net output #0: loss = 2.2537 (* 1 = 2.2537 loss)
I0815 06:40:18.711159 11101 sgd_solver.cpp:136] Iteration 248600, lr = 0.0223125, m = 0.9
I0815 06:40:33.898239 11101 solver.cpp:312] Iteration 248700 (6.58471 iter/s, 15.1867s/100 iter), loss = 1.21205
I0815 06:40:33.898305 11101 solver.cpp:334]     Train net output #0: loss = 0.971934 (* 1 = 0.971934 loss)
I0815 06:40:33.898324 11101 sgd_solver.cpp:136] Iteration 248700, lr = 0.0222812, m = 0.9
I0815 06:40:48.604221 11101 solver.cpp:312] Iteration 248800 (6.80014 iter/s, 14.7056s/100 iter), loss = 1.57919
I0815 06:40:48.605036 11101 solver.cpp:334]     Train net output #0: loss = 1.62819 (* 1 = 1.62819 loss)
I0815 06:40:48.605042 11101 sgd_solver.cpp:136] Iteration 248800, lr = 0.02225, m = 0.9
I0815 06:41:03.259925 11101 solver.cpp:312] Iteration 248900 (6.82347 iter/s, 14.6553s/100 iter), loss = 1.92295
I0815 06:41:03.259956 11101 solver.cpp:334]     Train net output #0: loss = 1.74742 (* 1 = 1.74742 loss)
I0815 06:41:03.259963 11101 sgd_solver.cpp:136] Iteration 248900, lr = 0.0222188, m = 0.9
I0815 06:41:17.839378 11101 solver.cpp:312] Iteration 249000 (6.85916 iter/s, 14.5791s/100 iter), loss = 1.6935
I0815 06:41:17.839406 11101 solver.cpp:334]     Train net output #0: loss = 1.71692 (* 1 = 1.71692 loss)
I0815 06:41:17.839413 11101 sgd_solver.cpp:136] Iteration 249000, lr = 0.0221875, m = 0.9
I0815 06:41:32.423372 11101 solver.cpp:312] Iteration 249100 (6.85702 iter/s, 14.5836s/100 iter), loss = 1.55696
I0815 06:41:32.423475 11101 solver.cpp:334]     Train net output #0: loss = 1.4596 (* 1 = 1.4596 loss)
I0815 06:41:32.423493 11101 sgd_solver.cpp:136] Iteration 249100, lr = 0.0221563, m = 0.9
I0815 06:41:46.824846 11101 solver.cpp:312] Iteration 249200 (6.94392 iter/s, 14.4011s/100 iter), loss = 1.72936
I0815 06:41:46.824913 11101 solver.cpp:334]     Train net output #0: loss = 1.79452 (* 1 = 1.79452 loss)
I0815 06:41:46.824929 11101 sgd_solver.cpp:136] Iteration 249200, lr = 0.022125, m = 0.9
I0815 06:42:01.408998 11101 solver.cpp:312] Iteration 249300 (6.85695 iter/s, 14.5837s/100 iter), loss = 1.58015
I0815 06:42:01.409024 11101 solver.cpp:334]     Train net output #0: loss = 1.60519 (* 1 = 1.60519 loss)
I0815 06:42:01.409031 11101 sgd_solver.cpp:136] Iteration 249300, lr = 0.0220937, m = 0.9
I0815 06:42:16.163990 11101 solver.cpp:312] Iteration 249400 (6.77755 iter/s, 14.7546s/100 iter), loss = 2.01391
I0815 06:42:16.164101 11101 solver.cpp:334]     Train net output #0: loss = 1.94208 (* 1 = 1.94208 loss)
I0815 06:42:16.164121 11101 sgd_solver.cpp:136] Iteration 249400, lr = 0.0220625, m = 0.9
I0815 06:42:30.936480 11101 solver.cpp:312] Iteration 249500 (6.76953 iter/s, 14.7721s/100 iter), loss = 1.98678
I0815 06:42:30.936509 11101 solver.cpp:334]     Train net output #0: loss = 1.94387 (* 1 = 1.94387 loss)
I0815 06:42:30.936516 11101 sgd_solver.cpp:136] Iteration 249500, lr = 0.0220312, m = 0.9
I0815 06:42:45.199738 11101 solver.cpp:312] Iteration 249600 (7.01121 iter/s, 14.2629s/100 iter), loss = 1.40746
I0815 06:42:45.199764 11101 solver.cpp:334]     Train net output #0: loss = 1.23437 (* 1 = 1.23437 loss)
I0815 06:42:45.199771 11101 sgd_solver.cpp:136] Iteration 249600, lr = 0.022, m = 0.9
I0815 06:42:59.948504 11101 solver.cpp:312] Iteration 249700 (6.78041 iter/s, 14.7484s/100 iter), loss = 1.77128
I0815 06:42:59.948582 11101 solver.cpp:334]     Train net output #0: loss = 1.58305 (* 1 = 1.58305 loss)
I0815 06:42:59.948596 11101 sgd_solver.cpp:136] Iteration 249700, lr = 0.0219688, m = 0.9
I0815 06:43:14.879400 11101 solver.cpp:312] Iteration 249800 (6.69771 iter/s, 14.9305s/100 iter), loss = 1.1952
I0815 06:43:14.879451 11101 solver.cpp:334]     Train net output #0: loss = 1.30014 (* 1 = 1.30014 loss)
I0815 06:43:14.879565 11101 sgd_solver.cpp:136] Iteration 249800, lr = 0.0219375, m = 0.9
I0815 06:43:29.427414 11101 solver.cpp:312] Iteration 249900 (6.87398 iter/s, 14.5476s/100 iter), loss = 1.90137
I0815 06:43:29.427443 11101 solver.cpp:334]     Train net output #0: loss = 2.03168 (* 1 = 2.03168 loss)
I0815 06:43:29.427446 11101 sgd_solver.cpp:136] Iteration 249900, lr = 0.0219063, m = 0.9
I0815 06:43:43.897778 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_250000.caffemodel
I0815 06:43:43.923441 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_250000.solverstate
I0815 06:43:43.929349 11101 solver.cpp:509] Iteration 250000, Testing net (#0)
I0815 06:44:04.789669 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.499647
I0815 06:44:04.789691 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.746528
I0815 06:44:04.789696 11101 solver.cpp:594]     Test net output #2: loss = 2.26183 (* 1 = 2.26183 loss)
I0815 06:44:04.789808 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8598s
I0815 06:44:04.941208 11101 solver.cpp:312] Iteration 250000 (2.81588 iter/s, 35.5128s/100 iter), loss = 1.88378
I0815 06:44:04.941231 11101 solver.cpp:334]     Train net output #0: loss = 1.5901 (* 1 = 1.5901 loss)
I0815 06:44:04.941237 11101 sgd_solver.cpp:136] Iteration 250000, lr = 0.021875, m = 0.9
I0815 06:44:19.237066 11101 solver.cpp:312] Iteration 250100 (6.99523 iter/s, 14.2955s/100 iter), loss = 1.73342
I0815 06:44:19.237130 11101 solver.cpp:334]     Train net output #0: loss = 1.871 (* 1 = 1.871 loss)
I0815 06:44:19.237139 11101 sgd_solver.cpp:136] Iteration 250100, lr = 0.0218438, m = 0.9
I0815 06:44:34.072257 11101 solver.cpp:312] Iteration 250200 (6.74091 iter/s, 14.8348s/100 iter), loss = 1.78009
I0815 06:44:34.072280 11101 solver.cpp:334]     Train net output #0: loss = 1.9007 (* 1 = 1.9007 loss)
I0815 06:44:34.072286 11101 sgd_solver.cpp:136] Iteration 250200, lr = 0.0218125, m = 0.9
I0815 06:44:48.773056 11101 solver.cpp:312] Iteration 250300 (6.80254 iter/s, 14.7004s/100 iter), loss = 1.80652
I0815 06:44:48.775086 11101 solver.cpp:334]     Train net output #0: loss = 2.13894 (* 1 = 2.13894 loss)
I0815 06:44:48.775115 11101 sgd_solver.cpp:136] Iteration 250300, lr = 0.0217812, m = 0.9
I0815 06:45:03.371784 11101 solver.cpp:312] Iteration 250400 (6.8501 iter/s, 14.5983s/100 iter), loss = 1.47007
I0815 06:45:03.371901 11101 solver.cpp:334]     Train net output #0: loss = 1.35374 (* 1 = 1.35374 loss)
I0815 06:45:03.371924 11101 sgd_solver.cpp:136] Iteration 250400, lr = 0.02175, m = 0.9
I0815 06:45:17.958662 11101 solver.cpp:312] Iteration 250500 (6.85567 iter/s, 14.5865s/100 iter), loss = 1.70069
I0815 06:45:17.958685 11101 solver.cpp:334]     Train net output #0: loss = 1.64291 (* 1 = 1.64291 loss)
I0815 06:45:17.958689 11101 sgd_solver.cpp:136] Iteration 250500, lr = 0.0217188, m = 0.9
I0815 06:45:32.546205 11101 solver.cpp:312] Iteration 250600 (6.85535 iter/s, 14.5871s/100 iter), loss = 1.6194
I0815 06:45:32.546234 11101 solver.cpp:334]     Train net output #0: loss = 1.41771 (* 1 = 1.41771 loss)
I0815 06:45:32.546241 11101 sgd_solver.cpp:136] Iteration 250600, lr = 0.0216875, m = 0.9
I0815 06:45:47.089493 11101 solver.cpp:312] Iteration 250700 (6.87621 iter/s, 14.5429s/100 iter), loss = 1.72327
I0815 06:45:47.089555 11101 solver.cpp:334]     Train net output #0: loss = 1.7656 (* 1 = 1.7656 loss)
I0815 06:45:47.089561 11101 sgd_solver.cpp:136] Iteration 250700, lr = 0.0216563, m = 0.9
I0815 06:46:02.041477 11101 solver.cpp:312] Iteration 250800 (6.68826 iter/s, 14.9516s/100 iter), loss = 1.53229
I0815 06:46:02.041501 11101 solver.cpp:334]     Train net output #0: loss = 1.19736 (* 1 = 1.19736 loss)
I0815 06:46:02.041507 11101 sgd_solver.cpp:136] Iteration 250800, lr = 0.021625, m = 0.9
I0815 06:46:17.161025 11101 solver.cpp:312] Iteration 250900 (6.61414 iter/s, 15.1191s/100 iter), loss = 1.83381
I0815 06:46:17.161104 11101 solver.cpp:334]     Train net output #0: loss = 2.28445 (* 1 = 2.28445 loss)
I0815 06:46:17.161113 11101 sgd_solver.cpp:136] Iteration 250900, lr = 0.0215937, m = 0.9
I0815 06:46:31.785457 11101 solver.cpp:312] Iteration 251000 (6.83806 iter/s, 14.624s/100 iter), loss = 1.31208
I0815 06:46:31.785521 11101 solver.cpp:334]     Train net output #0: loss = 0.980422 (* 1 = 0.980422 loss)
I0815 06:46:31.785539 11101 sgd_solver.cpp:136] Iteration 251000, lr = 0.0215625, m = 0.9
I0815 06:46:46.378242 11101 solver.cpp:312] Iteration 251100 (6.85289 iter/s, 14.5924s/100 iter), loss = 1.54724
I0815 06:46:46.378295 11101 solver.cpp:334]     Train net output #0: loss = 1.48327 (* 1 = 1.48327 loss)
I0815 06:46:46.378309 11101 sgd_solver.cpp:136] Iteration 251100, lr = 0.0215312, m = 0.9
I0815 06:47:01.091228 11101 solver.cpp:312] Iteration 251200 (6.79691 iter/s, 14.7126s/100 iter), loss = 1.67305
I0815 06:47:01.091316 11101 solver.cpp:334]     Train net output #0: loss = 1.59782 (* 1 = 1.59782 loss)
I0815 06:47:01.091336 11101 sgd_solver.cpp:136] Iteration 251200, lr = 0.0215, m = 0.9
I0815 06:47:15.440599 11101 solver.cpp:312] Iteration 251300 (6.96914 iter/s, 14.349s/100 iter), loss = 1.62428
I0815 06:47:15.440726 11101 solver.cpp:334]     Train net output #0: loss = 1.73928 (* 1 = 1.73928 loss)
I0815 06:47:15.440793 11101 sgd_solver.cpp:136] Iteration 251300, lr = 0.0214688, m = 0.9
I0815 06:47:30.033229 11101 solver.cpp:312] Iteration 251400 (6.85296 iter/s, 14.5922s/100 iter), loss = 1.93306
I0815 06:47:30.033251 11101 solver.cpp:334]     Train net output #0: loss = 2.28418 (* 1 = 2.28418 loss)
I0815 06:47:30.033254 11101 sgd_solver.cpp:136] Iteration 251400, lr = 0.0214375, m = 0.9
I0815 06:47:44.499598 11101 solver.cpp:312] Iteration 251500 (6.91278 iter/s, 14.466s/100 iter), loss = 1.53085
I0815 06:47:44.499658 11101 solver.cpp:334]     Train net output #0: loss = 1.76904 (* 1 = 1.76904 loss)
I0815 06:47:44.499665 11101 sgd_solver.cpp:136] Iteration 251500, lr = 0.0214063, m = 0.9
I0815 06:47:59.066112 11101 solver.cpp:312] Iteration 251600 (6.86525 iter/s, 14.5661s/100 iter), loss = 1.46608
I0815 06:47:59.066181 11101 solver.cpp:334]     Train net output #0: loss = 1.33793 (* 1 = 1.33793 loss)
I0815 06:47:59.066200 11101 sgd_solver.cpp:136] Iteration 251600, lr = 0.021375, m = 0.9
I0815 06:48:13.324189 11101 solver.cpp:312] Iteration 251700 (7.01376 iter/s, 14.2577s/100 iter), loss = 1.46766
I0815 06:48:13.324254 11101 solver.cpp:334]     Train net output #0: loss = 1.43153 (* 1 = 1.43153 loss)
I0815 06:48:13.324271 11101 sgd_solver.cpp:136] Iteration 251700, lr = 0.0213438, m = 0.9
I0815 06:48:28.164290 11101 solver.cpp:312] Iteration 251800 (6.73868 iter/s, 14.8397s/100 iter), loss = 1.57756
I0815 06:48:28.164368 11101 solver.cpp:334]     Train net output #0: loss = 1.56352 (* 1 = 1.56352 loss)
I0815 06:48:28.164441 11101 sgd_solver.cpp:136] Iteration 251800, lr = 0.0213125, m = 0.9
I0815 06:48:42.839159 11101 solver.cpp:312] Iteration 251900 (6.81456 iter/s, 14.6745s/100 iter), loss = 1.85746
I0815 06:48:42.839222 11101 solver.cpp:334]     Train net output #0: loss = 1.82956 (* 1 = 1.82956 loss)
I0815 06:48:42.839241 11101 sgd_solver.cpp:136] Iteration 251900, lr = 0.0212812, m = 0.9
I0815 06:48:57.290537 11101 solver.cpp:509] Iteration 252000, Testing net (#0)
I0815 06:49:18.292753 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.531235
I0815 06:49:18.292811 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.774233
I0815 06:49:18.292820 11101 solver.cpp:594]     Test net output #2: loss = 2.06953 (* 1 = 2.06953 loss)
I0815 06:49:18.292840 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0017s
I0815 06:49:18.456570 11101 solver.cpp:312] Iteration 252000 (2.80769 iter/s, 35.6164s/100 iter), loss = 1.47641
I0815 06:49:18.456598 11101 solver.cpp:334]     Train net output #0: loss = 1.78187 (* 1 = 1.78187 loss)
I0815 06:49:18.456605 11101 sgd_solver.cpp:136] Iteration 252000, lr = 0.02125, m = 0.9
I0815 06:49:33.169344 11101 solver.cpp:312] Iteration 252100 (6.797 iter/s, 14.7124s/100 iter), loss = 1.82027
I0815 06:49:33.169373 11101 solver.cpp:334]     Train net output #0: loss = 2.03279 (* 1 = 2.03279 loss)
I0815 06:49:33.169378 11101 sgd_solver.cpp:136] Iteration 252100, lr = 0.0212188, m = 0.9
I0815 06:49:47.599817 11101 solver.cpp:312] Iteration 252200 (6.92997 iter/s, 14.4301s/100 iter), loss = 2.32379
I0815 06:49:47.599845 11101 solver.cpp:334]     Train net output #0: loss = 2.24327 (* 1 = 2.24327 loss)
I0815 06:49:47.599851 11101 sgd_solver.cpp:136] Iteration 252200, lr = 0.0211875, m = 0.9
I0815 06:50:02.268189 11101 solver.cpp:312] Iteration 252300 (6.81758 iter/s, 14.668s/100 iter), loss = 1.92073
I0815 06:50:02.268267 11101 solver.cpp:334]     Train net output #0: loss = 1.70643 (* 1 = 1.70643 loss)
I0815 06:50:02.268280 11101 sgd_solver.cpp:136] Iteration 252300, lr = 0.0211563, m = 0.9
I0815 06:50:17.030163 11101 solver.cpp:312] Iteration 252400 (6.77435 iter/s, 14.7616s/100 iter), loss = 1.5051
I0815 06:50:17.030212 11101 solver.cpp:334]     Train net output #0: loss = 1.58403 (* 1 = 1.58403 loss)
I0815 06:50:17.030226 11101 sgd_solver.cpp:136] Iteration 252400, lr = 0.021125, m = 0.9
I0815 06:50:32.153604 11101 solver.cpp:312] Iteration 252500 (6.61244 iter/s, 15.123s/100 iter), loss = 1.77694
I0815 06:50:32.153645 11101 solver.cpp:334]     Train net output #0: loss = 1.53641 (* 1 = 1.53641 loss)
I0815 06:50:32.153656 11101 sgd_solver.cpp:136] Iteration 252500, lr = 0.0210938, m = 0.9
I0815 06:50:46.699981 11101 solver.cpp:312] Iteration 252600 (6.87475 iter/s, 14.546s/100 iter), loss = 1.30685
I0815 06:50:46.700078 11101 solver.cpp:334]     Train net output #0: loss = 1.28562 (* 1 = 1.28562 loss)
I0815 06:50:46.700094 11101 sgd_solver.cpp:136] Iteration 252600, lr = 0.0210625, m = 0.9
I0815 06:51:01.177937 11101 solver.cpp:312] Iteration 252700 (6.90724 iter/s, 14.4776s/100 iter), loss = 1.52175
I0815 06:51:01.177959 11101 solver.cpp:334]     Train net output #0: loss = 1.56148 (* 1 = 1.56148 loss)
I0815 06:51:01.177963 11101 sgd_solver.cpp:136] Iteration 252700, lr = 0.0210312, m = 0.9
I0815 06:51:15.837016 11101 solver.cpp:312] Iteration 252800 (6.8219 iter/s, 14.6587s/100 iter), loss = 1.49356
I0815 06:51:15.837045 11101 solver.cpp:334]     Train net output #0: loss = 1.88817 (* 1 = 1.88817 loss)
I0815 06:51:15.837051 11101 sgd_solver.cpp:136] Iteration 252800, lr = 0.021, m = 0.9
I0815 06:51:30.332510 11101 solver.cpp:312] Iteration 252900 (6.89889 iter/s, 14.4951s/100 iter), loss = 1.57057
I0815 06:51:30.332690 11101 solver.cpp:334]     Train net output #0: loss = 1.23128 (* 1 = 1.23128 loss)
I0815 06:51:30.332708 11101 sgd_solver.cpp:136] Iteration 252900, lr = 0.0209687, m = 0.9
I0815 06:51:44.893875 11101 solver.cpp:312] Iteration 253000 (6.86768 iter/s, 14.561s/100 iter), loss = 1.44269
I0815 06:51:44.893903 11101 solver.cpp:334]     Train net output #0: loss = 1.36253 (* 1 = 1.36253 loss)
I0815 06:51:44.893908 11101 sgd_solver.cpp:136] Iteration 253000, lr = 0.0209375, m = 0.9
I0815 06:51:59.898062 11101 solver.cpp:312] Iteration 253100 (6.66499 iter/s, 15.0038s/100 iter), loss = 1.26455
I0815 06:51:59.898090 11101 solver.cpp:334]     Train net output #0: loss = 1.32714 (* 1 = 1.32714 loss)
I0815 06:51:59.898097 11101 sgd_solver.cpp:136] Iteration 253100, lr = 0.0209063, m = 0.9
I0815 06:52:14.461554 11101 solver.cpp:312] Iteration 253200 (6.86668 iter/s, 14.5631s/100 iter), loss = 1.53224
I0815 06:52:14.468158 11101 solver.cpp:334]     Train net output #0: loss = 1.45437 (* 1 = 1.45437 loss)
I0815 06:52:14.468171 11101 sgd_solver.cpp:136] Iteration 253200, lr = 0.020875, m = 0.9
I0815 06:52:28.916277 11101 solver.cpp:312] Iteration 253300 (6.91834 iter/s, 14.4543s/100 iter), loss = 1.93822
I0815 06:52:28.916496 11101 solver.cpp:334]     Train net output #0: loss = 1.77296 (* 1 = 1.77296 loss)
I0815 06:52:28.916604 11101 sgd_solver.cpp:136] Iteration 253300, lr = 0.0208437, m = 0.9
I0815 06:52:43.602484 11101 solver.cpp:312] Iteration 253400 (6.8093 iter/s, 14.6858s/100 iter), loss = 1.9451
I0815 06:52:43.602512 11101 solver.cpp:334]     Train net output #0: loss = 2.04593 (* 1 = 2.04593 loss)
I0815 06:52:43.602516 11101 sgd_solver.cpp:136] Iteration 253400, lr = 0.0208125, m = 0.9
I0815 06:52:58.205924 11101 solver.cpp:312] Iteration 253500 (6.84789 iter/s, 14.603s/100 iter), loss = 1.90156
I0815 06:52:58.206014 11101 solver.cpp:334]     Train net output #0: loss = 1.41487 (* 1 = 1.41487 loss)
I0815 06:52:58.206032 11101 sgd_solver.cpp:136] Iteration 253500, lr = 0.0207812, m = 0.9
I0815 06:53:12.763730 11101 solver.cpp:312] Iteration 253600 (6.86936 iter/s, 14.5574s/100 iter), loss = 1.81832
I0815 06:53:12.763758 11101 solver.cpp:334]     Train net output #0: loss = 1.56335 (* 1 = 1.56335 loss)
I0815 06:53:12.763766 11101 sgd_solver.cpp:136] Iteration 253600, lr = 0.02075, m = 0.9
I0815 06:53:27.548468 11101 solver.cpp:312] Iteration 253700 (6.76392 iter/s, 14.7843s/100 iter), loss = 1.20558
I0815 06:53:27.548533 11101 solver.cpp:334]     Train net output #0: loss = 1.50693 (* 1 = 1.50693 loss)
I0815 06:53:27.548550 11101 sgd_solver.cpp:136] Iteration 253700, lr = 0.0207187, m = 0.9
I0815 06:53:42.401654 11101 solver.cpp:312] Iteration 253800 (6.73275 iter/s, 14.8528s/100 iter), loss = 1.43431
I0815 06:53:42.401737 11101 solver.cpp:334]     Train net output #0: loss = 1.57411 (* 1 = 1.57411 loss)
I0815 06:53:42.401753 11101 sgd_solver.cpp:136] Iteration 253800, lr = 0.0206875, m = 0.9
I0815 06:53:57.272097 11101 solver.cpp:312] Iteration 253900 (6.72493 iter/s, 14.87s/100 iter), loss = 1.73926
I0815 06:53:57.272127 11101 solver.cpp:334]     Train net output #0: loss = 1.61163 (* 1 = 1.61163 loss)
I0815 06:53:57.272143 11101 sgd_solver.cpp:136] Iteration 253900, lr = 0.0206563, m = 0.9
I0815 06:54:11.911345 11101 solver.cpp:509] Iteration 254000, Testing net (#0)
I0815 06:54:23.105538 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 06:54:32.796519 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.522176
I0815 06:54:32.796543 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.767645
I0815 06:54:32.796548 11101 solver.cpp:594]     Test net output #2: loss = 2.1237 (* 1 = 2.1237 loss)
I0815 06:54:32.796573 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8847s
I0815 06:54:32.946017 11101 solver.cpp:312] Iteration 254000 (2.80324 iter/s, 35.6729s/100 iter), loss = 1.65958
I0815 06:54:32.946043 11101 solver.cpp:334]     Train net output #0: loss = 1.60249 (* 1 = 1.60249 loss)
I0815 06:54:32.946049 11101 sgd_solver.cpp:136] Iteration 254000, lr = 0.020625, m = 0.9
I0815 06:54:47.652986 11101 solver.cpp:312] Iteration 254100 (6.79969 iter/s, 14.7066s/100 iter), loss = 1.63584
I0815 06:54:47.653018 11101 solver.cpp:334]     Train net output #0: loss = 1.46373 (* 1 = 1.46373 loss)
I0815 06:54:47.653024 11101 sgd_solver.cpp:136] Iteration 254100, lr = 0.0205938, m = 0.9
I0815 06:55:02.826544 11101 solver.cpp:312] Iteration 254200 (6.59059 iter/s, 15.1731s/100 iter), loss = 1.90482
I0815 06:55:02.826619 11101 solver.cpp:334]     Train net output #0: loss = 1.60551 (* 1 = 1.60551 loss)
I0815 06:55:02.826627 11101 sgd_solver.cpp:136] Iteration 254200, lr = 0.0205625, m = 0.9
I0815 06:55:17.749176 11101 solver.cpp:312] Iteration 254300 (6.70142 iter/s, 14.9222s/100 iter), loss = 1.51682
I0815 06:55:17.749204 11101 solver.cpp:334]     Train net output #0: loss = 1.63901 (* 1 = 1.63901 loss)
I0815 06:55:17.749210 11101 sgd_solver.cpp:136] Iteration 254300, lr = 0.0205313, m = 0.9
I0815 06:55:32.246719 11101 solver.cpp:312] Iteration 254400 (6.89791 iter/s, 14.4971s/100 iter), loss = 2.08169
I0815 06:55:32.246750 11101 solver.cpp:334]     Train net output #0: loss = 2.116 (* 1 = 2.116 loss)
I0815 06:55:32.246757 11101 sgd_solver.cpp:136] Iteration 254400, lr = 0.0205, m = 0.9
I0815 06:55:46.912585 11101 solver.cpp:312] Iteration 254500 (6.81874 iter/s, 14.6655s/100 iter), loss = 2.08563
I0815 06:55:46.912770 11101 solver.cpp:334]     Train net output #0: loss = 2.07059 (* 1 = 2.07059 loss)
I0815 06:55:46.912789 11101 sgd_solver.cpp:136] Iteration 254500, lr = 0.0204687, m = 0.9
I0815 06:56:01.564163 11101 solver.cpp:312] Iteration 254600 (6.82539 iter/s, 14.6512s/100 iter), loss = 1.89285
I0815 06:56:01.564205 11101 solver.cpp:334]     Train net output #0: loss = 1.77115 (* 1 = 1.77115 loss)
I0815 06:56:01.564214 11101 sgd_solver.cpp:136] Iteration 254600, lr = 0.0204375, m = 0.9
I0815 06:56:16.041275 11101 solver.cpp:312] Iteration 254700 (6.90764 iter/s, 14.4767s/100 iter), loss = 1.71396
I0815 06:56:16.041337 11101 solver.cpp:334]     Train net output #0: loss = 1.47478 (* 1 = 1.47478 loss)
I0815 06:56:16.041354 11101 sgd_solver.cpp:136] Iteration 254700, lr = 0.0204063, m = 0.9
I0815 06:56:30.509620 11101 solver.cpp:312] Iteration 254800 (6.91183 iter/s, 14.4679s/100 iter), loss = 1.7888
I0815 06:56:30.509675 11101 solver.cpp:334]     Train net output #0: loss = 1.74789 (* 1 = 1.74789 loss)
I0815 06:56:30.509680 11101 sgd_solver.cpp:136] Iteration 254800, lr = 0.020375, m = 0.9
I0815 06:56:45.105020 11101 solver.cpp:312] Iteration 254900 (6.85166 iter/s, 14.595s/100 iter), loss = 1.5965
I0815 06:56:45.105046 11101 solver.cpp:334]     Train net output #0: loss = 1.7125 (* 1 = 1.7125 loss)
I0815 06:56:45.105051 11101 sgd_solver.cpp:136] Iteration 254900, lr = 0.0203438, m = 0.9
I0815 06:57:00.188138 11101 solver.cpp:312] Iteration 255000 (6.63011 iter/s, 15.0827s/100 iter), loss = 1.71167
I0815 06:57:00.188204 11101 solver.cpp:334]     Train net output #0: loss = 1.92128 (* 1 = 1.92128 loss)
I0815 06:57:00.188220 11101 sgd_solver.cpp:136] Iteration 255000, lr = 0.0203125, m = 0.9
I0815 06:57:14.742314 11101 solver.cpp:312] Iteration 255100 (6.87107 iter/s, 14.5538s/100 iter), loss = 1.70886
I0815 06:57:14.742393 11101 solver.cpp:334]     Train net output #0: loss = 2.09508 (* 1 = 2.09508 loss)
I0815 06:57:14.742400 11101 sgd_solver.cpp:136] Iteration 255100, lr = 0.0202812, m = 0.9
I0815 06:57:29.390622 11101 solver.cpp:312] Iteration 255200 (6.82691 iter/s, 14.6479s/100 iter), loss = 1.65212
I0815 06:57:29.390674 11101 solver.cpp:334]     Train net output #0: loss = 1.78941 (* 1 = 1.78941 loss)
I0815 06:57:29.390687 11101 sgd_solver.cpp:136] Iteration 255200, lr = 0.02025, m = 0.9
I0815 06:57:44.423655 11101 solver.cpp:312] Iteration 255300 (6.6522 iter/s, 15.0326s/100 iter), loss = 1.58125
I0815 06:57:44.423684 11101 solver.cpp:334]     Train net output #0: loss = 1.76268 (* 1 = 1.76268 loss)
I0815 06:57:44.423691 11101 sgd_solver.cpp:136] Iteration 255300, lr = 0.0202187, m = 0.9
I0815 06:57:59.432581 11101 solver.cpp:312] Iteration 255400 (6.66289 iter/s, 15.0085s/100 iter), loss = 1.68006
I0815 06:57:59.432665 11101 solver.cpp:334]     Train net output #0: loss = 1.38093 (* 1 = 1.38093 loss)
I0815 06:57:59.432674 11101 sgd_solver.cpp:136] Iteration 255400, lr = 0.0201875, m = 0.9
I0815 06:58:14.432725 11101 solver.cpp:312] Iteration 255500 (6.66679 iter/s, 14.9997s/100 iter), loss = 1.50978
I0815 06:58:14.432799 11101 solver.cpp:334]     Train net output #0: loss = 1.65491 (* 1 = 1.65491 loss)
I0815 06:58:14.432818 11101 sgd_solver.cpp:136] Iteration 255500, lr = 0.0201563, m = 0.9
I0815 06:58:29.439129 11101 solver.cpp:312] Iteration 255600 (6.66401 iter/s, 15.006s/100 iter), loss = 1.52111
I0815 06:58:29.439195 11101 solver.cpp:334]     Train net output #0: loss = 1.39615 (* 1 = 1.39615 loss)
I0815 06:58:29.439203 11101 sgd_solver.cpp:136] Iteration 255600, lr = 0.020125, m = 0.9
I0815 06:58:44.018095 11101 solver.cpp:312] Iteration 255700 (6.85939 iter/s, 14.5786s/100 iter), loss = 1.78149
I0815 06:58:44.018172 11101 solver.cpp:334]     Train net output #0: loss = 1.4946 (* 1 = 1.4946 loss)
I0815 06:58:44.018193 11101 sgd_solver.cpp:136] Iteration 255700, lr = 0.0200938, m = 0.9
I0815 06:58:58.645251 11101 solver.cpp:312] Iteration 255800 (6.83679 iter/s, 14.6268s/100 iter), loss = 1.63023
I0815 06:58:58.645321 11101 solver.cpp:334]     Train net output #0: loss = 1.52974 (* 1 = 1.52974 loss)
I0815 06:58:58.645340 11101 sgd_solver.cpp:136] Iteration 255800, lr = 0.0200625, m = 0.9
I0815 06:59:13.259191 11101 solver.cpp:312] Iteration 255900 (6.84297 iter/s, 14.6135s/100 iter), loss = 1.80992
I0815 06:59:13.259256 11101 solver.cpp:334]     Train net output #0: loss = 1.50545 (* 1 = 1.50545 loss)
I0815 06:59:13.259263 11101 sgd_solver.cpp:136] Iteration 255900, lr = 0.0200312, m = 0.9
I0815 06:59:27.695849 11101 solver.cpp:509] Iteration 256000, Testing net (#0)
I0815 06:59:36.621322 11099 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 06:59:48.519528 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.510529
I0815 06:59:48.519582 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.759763
I0815 06:59:48.519588 11101 solver.cpp:594]     Test net output #2: loss = 2.17379 (* 1 = 2.17379 loss)
I0815 06:59:48.519606 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8232s
I0815 06:59:48.690382 11101 solver.cpp:312] Iteration 256000 (2.82245 iter/s, 35.4302s/100 iter), loss = 1.69713
I0815 06:59:48.690445 11101 solver.cpp:334]     Train net output #0: loss = 1.97406 (* 1 = 1.97406 loss)
I0815 06:59:48.690466 11101 sgd_solver.cpp:136] Iteration 256000, lr = 0.02, m = 0.9
I0815 07:00:03.341487 11101 solver.cpp:312] Iteration 256100 (6.82561 iter/s, 14.6507s/100 iter), loss = 1.34439
I0815 07:00:03.341516 11101 solver.cpp:334]     Train net output #0: loss = 1.3897 (* 1 = 1.3897 loss)
I0815 07:00:03.341521 11101 sgd_solver.cpp:136] Iteration 256100, lr = 0.0199687, m = 0.9
I0815 07:00:17.958075 11101 solver.cpp:312] Iteration 256200 (6.84173 iter/s, 14.6162s/100 iter), loss = 1.63151
I0815 07:00:17.958106 11101 solver.cpp:334]     Train net output #0: loss = 1.75171 (* 1 = 1.75171 loss)
I0815 07:00:17.958111 11101 sgd_solver.cpp:136] Iteration 256200, lr = 0.0199375, m = 0.9
I0815 07:00:32.888706 11101 solver.cpp:312] Iteration 256300 (6.69782 iter/s, 14.9302s/100 iter), loss = 1.98118
I0815 07:00:32.888788 11101 solver.cpp:334]     Train net output #0: loss = 1.70761 (* 1 = 1.70761 loss)
I0815 07:00:32.888801 11101 sgd_solver.cpp:136] Iteration 256300, lr = 0.0199063, m = 0.9
I0815 07:00:47.475250 11101 solver.cpp:312] Iteration 256400 (6.85582 iter/s, 14.5861s/100 iter), loss = 1.62924
I0815 07:00:47.475317 11101 solver.cpp:334]     Train net output #0: loss = 1.90773 (* 1 = 1.90773 loss)
I0815 07:00:47.475337 11101 sgd_solver.cpp:136] Iteration 256400, lr = 0.019875, m = 0.9
I0815 07:01:02.132263 11101 solver.cpp:312] Iteration 256500 (6.82286 iter/s, 14.6566s/100 iter), loss = 1.53256
I0815 07:01:02.132289 11101 solver.cpp:334]     Train net output #0: loss = 1.40857 (* 1 = 1.40857 loss)
I0815 07:01:02.132293 11101 sgd_solver.cpp:136] Iteration 256500, lr = 0.0198438, m = 0.9
I0815 07:01:16.745831 11101 solver.cpp:312] Iteration 256600 (6.84314 iter/s, 14.6132s/100 iter), loss = 1.24293
I0815 07:01:16.745944 11101 solver.cpp:334]     Train net output #0: loss = 1.35572 (* 1 = 1.35572 loss)
I0815 07:01:16.745964 11101 sgd_solver.cpp:136] Iteration 256600, lr = 0.0198125, m = 0.9
I0815 07:01:31.363731 11101 solver.cpp:312] Iteration 256700 (6.84112 iter/s, 14.6175s/100 iter), loss = 1.30605
I0815 07:01:31.363760 11101 solver.cpp:334]     Train net output #0: loss = 1.29356 (* 1 = 1.29356 loss)
I0815 07:01:31.363766 11101 sgd_solver.cpp:136] Iteration 256700, lr = 0.0197813, m = 0.9
I0815 07:01:46.136634 11101 solver.cpp:312] Iteration 256800 (6.76934 iter/s, 14.7725s/100 iter), loss = 1.06815
I0815 07:01:46.136658 11101 solver.cpp:334]     Train net output #0: loss = 1.34919 (* 1 = 1.34919 loss)
I0815 07:01:46.136664 11101 sgd_solver.cpp:136] Iteration 256800, lr = 0.01975, m = 0.9
I0815 07:02:00.761596 11101 solver.cpp:312] Iteration 256900 (6.83781 iter/s, 14.6246s/100 iter), loss = 1.9632
I0815 07:02:00.761693 11101 solver.cpp:334]     Train net output #0: loss = 1.48444 (* 1 = 1.48444 loss)
I0815 07:02:00.761713 11101 sgd_solver.cpp:136] Iteration 256900, lr = 0.0197187, m = 0.9
I0815 07:02:15.211669 11101 solver.cpp:312] Iteration 257000 (6.92057 iter/s, 14.4497s/100 iter), loss = 1.50856
I0815 07:02:15.211694 11101 solver.cpp:334]     Train net output #0: loss = 1.56865 (* 1 = 1.56865 loss)
I0815 07:02:15.211699 11101 sgd_solver.cpp:136] Iteration 257000, lr = 0.0196875, m = 0.9
I0815 07:02:29.761567 11101 solver.cpp:312] Iteration 257100 (6.87309 iter/s, 14.5495s/100 iter), loss = 1.63054
I0815 07:02:29.761597 11101 solver.cpp:334]     Train net output #0: loss = 1.95711 (* 1 = 1.95711 loss)
I0815 07:02:29.761603 11101 sgd_solver.cpp:136] Iteration 257100, lr = 0.0196563, m = 0.9
I0815 07:02:44.484724 11101 solver.cpp:312] Iteration 257200 (6.79221 iter/s, 14.7228s/100 iter), loss = 1.40162
I0815 07:02:44.484777 11101 solver.cpp:334]     Train net output #0: loss = 1.53444 (* 1 = 1.53444 loss)
I0815 07:02:44.484784 11101 sgd_solver.cpp:136] Iteration 257200, lr = 0.019625, m = 0.9
I0815 07:02:58.959422 11101 solver.cpp:312] Iteration 257300 (6.9088 iter/s, 14.4743s/100 iter), loss = 1.53145
I0815 07:02:58.959452 11101 solver.cpp:334]     Train net output #0: loss = 1.45722 (* 1 = 1.45722 loss)
I0815 07:02:58.959458 11101 sgd_solver.cpp:136] Iteration 257300, lr = 0.0195938, m = 0.9
I0815 07:03:13.599222 11101 solver.cpp:312] Iteration 257400 (6.83088 iter/s, 14.6394s/100 iter), loss = 1.70675
I0815 07:03:13.599270 11101 solver.cpp:334]     Train net output #0: loss = 1.39628 (* 1 = 1.39628 loss)
I0815 07:03:13.599284 11101 sgd_solver.cpp:136] Iteration 257400, lr = 0.0195625, m = 0.9
I0815 07:03:28.407938 11101 solver.cpp:312] Iteration 257500 (6.75297 iter/s, 14.8083s/100 iter), loss = 1.94845
I0815 07:03:28.407991 11101 solver.cpp:334]     Train net output #0: loss = 2.3632 (* 1 = 2.3632 loss)
I0815 07:03:28.407999 11101 sgd_solver.cpp:136] Iteration 257500, lr = 0.0195312, m = 0.9
I0815 07:03:42.863054 11101 solver.cpp:312] Iteration 257600 (6.91815 iter/s, 14.4547s/100 iter), loss = 1.35473
I0815 07:03:42.863121 11101 solver.cpp:334]     Train net output #0: loss = 1.20276 (* 1 = 1.20276 loss)
I0815 07:03:42.863138 11101 sgd_solver.cpp:136] Iteration 257600, lr = 0.0195, m = 0.9
I0815 07:03:57.485574 11101 solver.cpp:312] Iteration 257700 (6.83895 iter/s, 14.6221s/100 iter), loss = 1.42314
I0815 07:03:57.486340 11101 solver.cpp:334]     Train net output #0: loss = 1.68217 (* 1 = 1.68217 loss)
I0815 07:03:57.486346 11101 sgd_solver.cpp:136] Iteration 257700, lr = 0.0194687, m = 0.9
I0815 07:04:12.253563 11101 solver.cpp:312] Iteration 257800 (6.77159 iter/s, 14.7676s/100 iter), loss = 1.68381
I0815 07:04:12.253631 11101 solver.cpp:334]     Train net output #0: loss = 1.52676 (* 1 = 1.52676 loss)
I0815 07:04:12.253639 11101 sgd_solver.cpp:136] Iteration 257800, lr = 0.0194375, m = 0.9
I0815 07:04:26.965143 11101 solver.cpp:312] Iteration 257900 (6.79755 iter/s, 14.7112s/100 iter), loss = 1.42191
I0815 07:04:26.965173 11101 solver.cpp:334]     Train net output #0: loss = 1.32564 (* 1 = 1.32564 loss)
I0815 07:04:26.965179 11101 sgd_solver.cpp:136] Iteration 257900, lr = 0.0194062, m = 0.9
I0815 07:04:41.690943 11101 solver.cpp:509] Iteration 258000, Testing net (#0)
I0815 07:05:02.659687 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.52247
I0815 07:05:02.659812 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.76988
I0815 07:05:02.659821 11101 solver.cpp:594]     Test net output #2: loss = 2.08668 (* 1 = 2.08668 loss)
I0815 07:05:02.659839 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9683s
I0815 07:05:02.816979 11101 solver.cpp:312] Iteration 258000 (2.78933 iter/s, 35.8509s/100 iter), loss = 1.64151
I0815 07:05:02.817006 11101 solver.cpp:334]     Train net output #0: loss = 1.65156 (* 1 = 1.65156 loss)
I0815 07:05:02.817013 11101 sgd_solver.cpp:136] Iteration 258000, lr = 0.019375, m = 0.9
I0815 07:05:17.235754 11101 solver.cpp:312] Iteration 258100 (6.9356 iter/s, 14.4184s/100 iter), loss = 1.48612
I0815 07:05:17.235780 11101 solver.cpp:334]     Train net output #0: loss = 1.65802 (* 1 = 1.65802 loss)
I0815 07:05:17.235787 11101 sgd_solver.cpp:136] Iteration 258100, lr = 0.0193438, m = 0.9
I0815 07:05:31.717767 11101 solver.cpp:312] Iteration 258200 (6.90531 iter/s, 14.4816s/100 iter), loss = 1.8829
I0815 07:05:31.717792 11101 solver.cpp:334]     Train net output #0: loss = 1.85192 (* 1 = 1.85192 loss)
I0815 07:05:31.717797 11101 sgd_solver.cpp:136] Iteration 258200, lr = 0.0193125, m = 0.9
I0815 07:05:46.285907 11101 solver.cpp:312] Iteration 258300 (6.86448 iter/s, 14.5677s/100 iter), loss = 1.75288
I0815 07:05:46.285969 11101 solver.cpp:334]     Train net output #0: loss = 1.83564 (* 1 = 1.83564 loss)
I0815 07:05:46.285976 11101 sgd_solver.cpp:136] Iteration 258300, lr = 0.0192813, m = 0.9
I0815 07:06:00.883795 11101 solver.cpp:312] Iteration 258400 (6.85049 iter/s, 14.5975s/100 iter), loss = 1.63314
I0815 07:06:00.883821 11101 solver.cpp:334]     Train net output #0: loss = 2.11644 (* 1 = 2.11644 loss)
I0815 07:06:00.883826 11101 sgd_solver.cpp:136] Iteration 258400, lr = 0.01925, m = 0.9
I0815 07:06:15.433529 11101 solver.cpp:312] Iteration 258500 (6.87317 iter/s, 14.5493s/100 iter), loss = 1.4286
I0815 07:06:15.433553 11101 solver.cpp:334]     Train net output #0: loss = 1.41614 (* 1 = 1.41614 loss)
I0815 07:06:15.433557 11101 sgd_solver.cpp:136] Iteration 258500, lr = 0.0192187, m = 0.9
I0815 07:06:29.864997 11101 solver.cpp:312] Iteration 258600 (6.92949 iter/s, 14.4311s/100 iter), loss = 1.89523
I0815 07:06:29.865097 11101 solver.cpp:334]     Train net output #0: loss = 2.14264 (* 1 = 2.14264 loss)
I0815 07:06:29.865115 11101 sgd_solver.cpp:136] Iteration 258600, lr = 0.0191875, m = 0.9
I0815 07:06:44.662858 11101 solver.cpp:312] Iteration 258700 (6.75792 iter/s, 14.7975s/100 iter), loss = 1.32778
I0815 07:06:44.662886 11101 solver.cpp:334]     Train net output #0: loss = 1.72499 (* 1 = 1.72499 loss)
I0815 07:06:44.662892 11101 sgd_solver.cpp:136] Iteration 258700, lr = 0.0191562, m = 0.9
I0815 07:06:59.681304 11101 solver.cpp:312] Iteration 258800 (6.65866 iter/s, 15.018s/100 iter), loss = 1.84596
I0815 07:06:59.681331 11101 solver.cpp:334]     Train net output #0: loss = 1.78315 (* 1 = 1.78315 loss)
I0815 07:06:59.681337 11101 sgd_solver.cpp:136] Iteration 258800, lr = 0.019125, m = 0.9
I0815 07:07:14.296025 11101 solver.cpp:312] Iteration 258900 (6.8426 iter/s, 14.6143s/100 iter), loss = 1.59353
I0815 07:07:14.300251 11101 solver.cpp:334]     Train net output #0: loss = 1.59412 (* 1 = 1.59412 loss)
I0815 07:07:14.300268 11101 sgd_solver.cpp:136] Iteration 258900, lr = 0.0190938, m = 0.9
I0815 07:07:29.108264 11101 solver.cpp:312] Iteration 259000 (6.75136 iter/s, 14.8118s/100 iter), loss = 1.49815
I0815 07:07:29.108292 11101 solver.cpp:334]     Train net output #0: loss = 1.48278 (* 1 = 1.48278 loss)
I0815 07:07:29.108299 11101 sgd_solver.cpp:136] Iteration 259000, lr = 0.0190625, m = 0.9
I0815 07:07:43.910647 11101 solver.cpp:312] Iteration 259100 (6.75585 iter/s, 14.802s/100 iter), loss = 1.40743
I0815 07:07:43.910866 11101 solver.cpp:334]     Train net output #0: loss = 1.42326 (* 1 = 1.42326 loss)
I0815 07:07:43.910979 11101 sgd_solver.cpp:136] Iteration 259100, lr = 0.0190313, m = 0.9
I0815 07:07:58.869686 11101 solver.cpp:312] Iteration 259200 (6.68511 iter/s, 14.9586s/100 iter), loss = 1.73792
I0815 07:07:58.869901 11101 solver.cpp:334]     Train net output #0: loss = 1.90626 (* 1 = 1.90626 loss)
I0815 07:07:58.869997 11101 sgd_solver.cpp:136] Iteration 259200, lr = 0.019, m = 0.9
I0815 07:08:13.316601 11101 solver.cpp:312] Iteration 259300 (6.92208 iter/s, 14.4465s/100 iter), loss = 1.73367
I0815 07:08:13.316629 11101 solver.cpp:334]     Train net output #0: loss = 1.83156 (* 1 = 1.83156 loss)
I0815 07:08:13.316633 11101 sgd_solver.cpp:136] Iteration 259300, lr = 0.0189687, m = 0.9
I0815 07:08:28.197285 11101 solver.cpp:312] Iteration 259400 (6.72031 iter/s, 14.8803s/100 iter), loss = 1.75605
I0815 07:08:28.197312 11101 solver.cpp:334]     Train net output #0: loss = 1.92568 (* 1 = 1.92568 loss)
I0815 07:08:28.197316 11101 sgd_solver.cpp:136] Iteration 259400, lr = 0.0189375, m = 0.9
I0815 07:08:42.644639 11101 solver.cpp:312] Iteration 259500 (6.92187 iter/s, 14.447s/100 iter), loss = 1.46628
I0815 07:08:42.644717 11101 solver.cpp:334]     Train net output #0: loss = 1.83247 (* 1 = 1.83247 loss)
I0815 07:08:42.644733 11101 sgd_solver.cpp:136] Iteration 259500, lr = 0.0189062, m = 0.9
I0815 07:08:57.320694 11101 solver.cpp:312] Iteration 259600 (6.81401 iter/s, 14.6757s/100 iter), loss = 1.5279
I0815 07:08:57.320722 11101 solver.cpp:334]     Train net output #0: loss = 1.51169 (* 1 = 1.51169 loss)
I0815 07:08:57.320729 11101 sgd_solver.cpp:136] Iteration 259600, lr = 0.018875, m = 0.9
I0815 07:09:12.010740 11101 solver.cpp:312] Iteration 259700 (6.80752 iter/s, 14.6896s/100 iter), loss = 1.6037
I0815 07:09:12.010766 11101 solver.cpp:334]     Train net output #0: loss = 1.75289 (* 1 = 1.75289 loss)
I0815 07:09:12.010814 11101 sgd_solver.cpp:136] Iteration 259700, lr = 0.0188438, m = 0.9
I0815 07:09:26.620537 11101 solver.cpp:312] Iteration 259800 (6.84491 iter/s, 14.6094s/100 iter), loss = 1.44457
I0815 07:09:26.620594 11101 solver.cpp:334]     Train net output #0: loss = 1.53282 (* 1 = 1.53282 loss)
I0815 07:09:26.620601 11101 sgd_solver.cpp:136] Iteration 259800, lr = 0.0188125, m = 0.9
I0815 07:09:41.282598 11101 solver.cpp:312] Iteration 259900 (6.82051 iter/s, 14.6617s/100 iter), loss = 1.35851
I0815 07:09:41.282624 11101 solver.cpp:334]     Train net output #0: loss = 1.2094 (* 1 = 1.2094 loss)
I0815 07:09:41.282630 11101 sgd_solver.cpp:136] Iteration 259900, lr = 0.0187813, m = 0.9
I0815 07:09:55.628384 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_260000.caffemodel
I0815 07:09:55.652271 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_260000.solverstate
I0815 07:09:55.658164 11101 solver.cpp:509] Iteration 260000, Testing net (#0)
I0815 07:10:02.824008 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 07:10:16.454840 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.494177
I0815 07:10:16.454859 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.743116
I0815 07:10:16.454865 11101 solver.cpp:594]     Test net output #2: loss = 2.27313 (* 1 = 2.27313 loss)
I0815 07:10:16.464174 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8054s
I0815 07:10:16.605195 11101 solver.cpp:312] Iteration 260000 (2.83113 iter/s, 35.3216s/100 iter), loss = 1.19571
I0815 07:10:16.605218 11101 solver.cpp:334]     Train net output #0: loss = 1.20941 (* 1 = 1.20941 loss)
I0815 07:10:16.605224 11101 sgd_solver.cpp:136] Iteration 260000, lr = 0.01875, m = 0.9
I0815 07:10:31.183534 11101 solver.cpp:312] Iteration 260100 (6.85968 iter/s, 14.5779s/100 iter), loss = 1.42998
I0815 07:10:31.183563 11101 solver.cpp:334]     Train net output #0: loss = 1.55944 (* 1 = 1.55944 loss)
I0815 07:10:31.183569 11101 sgd_solver.cpp:136] Iteration 260100, lr = 0.0187187, m = 0.9
I0815 07:10:45.925940 11101 solver.cpp:312] Iteration 260200 (6.78334 iter/s, 14.742s/100 iter), loss = 1.76038
I0815 07:10:45.926036 11101 solver.cpp:334]     Train net output #0: loss = 1.64853 (* 1 = 1.64853 loss)
I0815 07:10:45.926053 11101 sgd_solver.cpp:136] Iteration 260200, lr = 0.0186875, m = 0.9
I0815 07:11:00.431427 11101 solver.cpp:312] Iteration 260300 (6.89413 iter/s, 14.5051s/100 iter), loss = 1.51009
I0815 07:11:00.431502 11101 solver.cpp:334]     Train net output #0: loss = 1.646 (* 1 = 1.646 loss)
I0815 07:11:00.431522 11101 sgd_solver.cpp:136] Iteration 260300, lr = 0.0186562, m = 0.9
I0815 07:11:14.940742 11101 solver.cpp:312] Iteration 260400 (6.89232 iter/s, 14.5089s/100 iter), loss = 1.40801
I0815 07:11:14.940817 11101 solver.cpp:334]     Train net output #0: loss = 1.03513 (* 1 = 1.03513 loss)
I0815 07:11:14.940837 11101 sgd_solver.cpp:136] Iteration 260400, lr = 0.018625, m = 0.9
I0815 07:11:29.548322 11101 solver.cpp:312] Iteration 260500 (6.84595 iter/s, 14.6072s/100 iter), loss = 1.5622
I0815 07:11:29.548398 11101 solver.cpp:334]     Train net output #0: loss = 1.5822 (* 1 = 1.5822 loss)
I0815 07:11:29.548411 11101 sgd_solver.cpp:136] Iteration 260500, lr = 0.0185938, m = 0.9
I0815 07:11:44.159845 11101 solver.cpp:312] Iteration 260600 (6.8441 iter/s, 14.6111s/100 iter), loss = 1.21975
I0815 07:11:44.159874 11101 solver.cpp:334]     Train net output #0: loss = 1.18197 (* 1 = 1.18197 loss)
I0815 07:11:44.159880 11101 sgd_solver.cpp:136] Iteration 260600, lr = 0.0185625, m = 0.9
I0815 07:11:58.694413 11101 solver.cpp:312] Iteration 260700 (6.88034 iter/s, 14.5342s/100 iter), loss = 1.34968
I0815 07:11:58.694471 11101 solver.cpp:334]     Train net output #0: loss = 1.31022 (* 1 = 1.31022 loss)
I0815 07:11:58.694484 11101 sgd_solver.cpp:136] Iteration 260700, lr = 0.0185313, m = 0.9
I0815 07:12:13.553732 11101 solver.cpp:312] Iteration 260800 (6.72997 iter/s, 14.8589s/100 iter), loss = 1.60584
I0815 07:12:13.553789 11101 solver.cpp:334]     Train net output #0: loss = 1.59093 (* 1 = 1.59093 loss)
I0815 07:12:13.553797 11101 sgd_solver.cpp:136] Iteration 260800, lr = 0.0185, m = 0.9
I0815 07:12:28.443882 11101 solver.cpp:312] Iteration 260900 (6.71604 iter/s, 14.8897s/100 iter), loss = 1.5956
I0815 07:12:28.443912 11101 solver.cpp:334]     Train net output #0: loss = 1.74163 (* 1 = 1.74163 loss)
I0815 07:12:28.443917 11101 sgd_solver.cpp:136] Iteration 260900, lr = 0.0184688, m = 0.9
I0815 07:12:43.172605 11101 solver.cpp:312] Iteration 261000 (6.78964 iter/s, 14.7283s/100 iter), loss = 1.73896
I0815 07:12:43.172667 11101 solver.cpp:334]     Train net output #0: loss = 1.41445 (* 1 = 1.41445 loss)
I0815 07:12:43.172685 11101 sgd_solver.cpp:136] Iteration 261000, lr = 0.0184375, m = 0.9
I0815 07:12:57.732436 11101 solver.cpp:312] Iteration 261100 (6.8684 iter/s, 14.5594s/100 iter), loss = 1.42541
I0815 07:12:57.732563 11101 solver.cpp:334]     Train net output #0: loss = 1.80099 (* 1 = 1.80099 loss)
I0815 07:12:57.732586 11101 sgd_solver.cpp:136] Iteration 261100, lr = 0.0184062, m = 0.9
I0815 07:13:12.618834 11101 solver.cpp:312] Iteration 261200 (6.71773 iter/s, 14.886s/100 iter), loss = 1.58431
I0815 07:13:12.618898 11101 solver.cpp:334]     Train net output #0: loss = 1.55343 (* 1 = 1.55343 loss)
I0815 07:13:12.618916 11101 sgd_solver.cpp:136] Iteration 261200, lr = 0.018375, m = 0.9
I0815 07:13:27.092363 11101 solver.cpp:312] Iteration 261300 (6.90935 iter/s, 14.4731s/100 iter), loss = 2.15881
I0815 07:13:27.092392 11101 solver.cpp:334]     Train net output #0: loss = 1.56584 (* 1 = 1.56584 loss)
I0815 07:13:27.092398 11101 sgd_solver.cpp:136] Iteration 261300, lr = 0.0183438, m = 0.9
I0815 07:13:41.619896 11101 solver.cpp:312] Iteration 261400 (6.88367 iter/s, 14.5271s/100 iter), loss = 1.62336
I0815 07:13:41.620036 11101 solver.cpp:334]     Train net output #0: loss = 1.53352 (* 1 = 1.53352 loss)
I0815 07:13:41.620059 11101 sgd_solver.cpp:136] Iteration 261400, lr = 0.0183125, m = 0.9
I0815 07:13:56.300843 11101 solver.cpp:312] Iteration 261500 (6.81174 iter/s, 14.6805s/100 iter), loss = 1.43243
I0815 07:13:56.300868 11101 solver.cpp:334]     Train net output #0: loss = 1.42651 (* 1 = 1.42651 loss)
I0815 07:13:56.300874 11101 sgd_solver.cpp:136] Iteration 261500, lr = 0.0182813, m = 0.9
I0815 07:14:10.867544 11101 solver.cpp:312] Iteration 261600 (6.86516 iter/s, 14.5663s/100 iter), loss = 1.76012
I0815 07:14:10.867621 11101 solver.cpp:334]     Train net output #0: loss = 2.11936 (* 1 = 2.11936 loss)
I0815 07:14:10.867642 11101 sgd_solver.cpp:136] Iteration 261600, lr = 0.01825, m = 0.9
I0815 07:14:25.801954 11101 solver.cpp:312] Iteration 261700 (6.69613 iter/s, 14.934s/100 iter), loss = 1.56637
I0815 07:14:25.802021 11101 solver.cpp:334]     Train net output #0: loss = 1.08026 (* 1 = 1.08026 loss)
I0815 07:14:25.802027 11101 sgd_solver.cpp:136] Iteration 261700, lr = 0.0182188, m = 0.9
I0815 07:14:40.680409 11101 solver.cpp:312] Iteration 261800 (6.72131 iter/s, 14.878s/100 iter), loss = 1.5429
I0815 07:14:40.680436 11101 solver.cpp:334]     Train net output #0: loss = 1.49405 (* 1 = 1.49405 loss)
I0815 07:14:40.680443 11101 sgd_solver.cpp:136] Iteration 261800, lr = 0.0181875, m = 0.9
I0815 07:14:55.359688 11101 solver.cpp:312] Iteration 261900 (6.81251 iter/s, 14.6789s/100 iter), loss = 1.2779
I0815 07:14:55.359750 11101 solver.cpp:334]     Train net output #0: loss = 1.2955 (* 1 = 1.2955 loss)
I0815 07:14:55.359768 11101 sgd_solver.cpp:136] Iteration 261900, lr = 0.0181562, m = 0.9
I0815 07:15:10.026940 11101 solver.cpp:509] Iteration 262000, Testing net (#0)
I0815 07:15:30.861208 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.519882
I0815 07:15:30.861235 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.76541
I0815 07:15:30.861243 11101 solver.cpp:594]     Test net output #2: loss = 2.13944 (* 1 = 2.13944 loss)
I0815 07:15:30.861263 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8338s
I0815 07:15:31.025717 11101 solver.cpp:312] Iteration 262000 (2.80386 iter/s, 35.6651s/100 iter), loss = 1.53371
I0815 07:15:31.025741 11101 solver.cpp:334]     Train net output #0: loss = 1.5939 (* 1 = 1.5939 loss)
I0815 07:15:31.025746 11101 sgd_solver.cpp:136] Iteration 262000, lr = 0.018125, m = 0.9
I0815 07:15:45.278596 11101 solver.cpp:312] Iteration 262100 (7.01632 iter/s, 14.2525s/100 iter), loss = 1.32701
I0815 07:15:45.278653 11101 solver.cpp:334]     Train net output #0: loss = 1.60987 (* 1 = 1.60987 loss)
I0815 07:15:45.278661 11101 sgd_solver.cpp:136] Iteration 262100, lr = 0.0180938, m = 0.9
I0815 07:15:59.820413 11101 solver.cpp:312] Iteration 262200 (6.87691 iter/s, 14.5414s/100 iter), loss = 1.83043
I0815 07:15:59.820441 11101 solver.cpp:334]     Train net output #0: loss = 2.21533 (* 1 = 2.21533 loss)
I0815 07:15:59.820448 11101 sgd_solver.cpp:136] Iteration 262200, lr = 0.0180625, m = 0.9
I0815 07:16:14.526688 11101 solver.cpp:312] Iteration 262300 (6.80001 iter/s, 14.7059s/100 iter), loss = 1.73913
I0815 07:16:14.526752 11101 solver.cpp:334]     Train net output #0: loss = 1.80921 (* 1 = 1.80921 loss)
I0815 07:16:14.526770 11101 sgd_solver.cpp:136] Iteration 262300, lr = 0.0180313, m = 0.9
I0815 07:16:29.146718 11101 solver.cpp:312] Iteration 262400 (6.84012 iter/s, 14.6196s/100 iter), loss = 1.57534
I0815 07:16:29.146811 11101 solver.cpp:334]     Train net output #0: loss = 1.1736 (* 1 = 1.1736 loss)
I0815 07:16:29.146828 11101 sgd_solver.cpp:136] Iteration 262400, lr = 0.018, m = 0.9
I0815 07:16:44.054190 11101 solver.cpp:312] Iteration 262500 (6.70823 iter/s, 14.9071s/100 iter), loss = 1.31284
I0815 07:16:44.054265 11101 solver.cpp:334]     Train net output #0: loss = 1.26622 (* 1 = 1.26622 loss)
I0815 07:16:44.054283 11101 sgd_solver.cpp:136] Iteration 262500, lr = 0.0179687, m = 0.9
I0815 07:16:58.829797 11101 solver.cpp:312] Iteration 262600 (6.7681 iter/s, 14.7752s/100 iter), loss = 1.68076
I0815 07:16:58.829867 11101 solver.cpp:334]     Train net output #0: loss = 2.05249 (* 1 = 2.05249 loss)
I0815 07:16:58.829885 11101 sgd_solver.cpp:136] Iteration 262600, lr = 0.0179375, m = 0.9
I0815 07:17:13.491866 11101 solver.cpp:312] Iteration 262700 (6.82051 iter/s, 14.6617s/100 iter), loss = 1.14559
I0815 07:17:13.491982 11101 solver.cpp:334]     Train net output #0: loss = 1.10512 (* 1 = 1.10512 loss)
I0815 07:17:13.492002 11101 sgd_solver.cpp:136] Iteration 262700, lr = 0.0179062, m = 0.9
I0815 07:17:28.103034 11101 solver.cpp:312] Iteration 262800 (6.84427 iter/s, 14.6108s/100 iter), loss = 1.57124
I0815 07:17:28.103106 11101 solver.cpp:334]     Train net output #0: loss = 1.14302 (* 1 = 1.14302 loss)
I0815 07:17:28.103127 11101 sgd_solver.cpp:136] Iteration 262800, lr = 0.017875, m = 0.9
I0815 07:17:42.962680 11101 solver.cpp:312] Iteration 262900 (6.72982 iter/s, 14.8592s/100 iter), loss = 1.40396
I0815 07:17:42.962709 11101 solver.cpp:334]     Train net output #0: loss = 1.51716 (* 1 = 1.51716 loss)
I0815 07:17:42.962715 11101 sgd_solver.cpp:136] Iteration 262900, lr = 0.0178437, m = 0.9
I0815 07:17:57.371372 11101 solver.cpp:312] Iteration 263000 (6.94045 iter/s, 14.4083s/100 iter), loss = 1.53672
I0815 07:17:57.371585 11101 solver.cpp:334]     Train net output #0: loss = 1.64152 (* 1 = 1.64152 loss)
I0815 07:17:57.371673 11101 sgd_solver.cpp:136] Iteration 263000, lr = 0.0178125, m = 0.9
I0815 07:18:11.921221 11101 solver.cpp:312] Iteration 263100 (6.87311 iter/s, 14.5495s/100 iter), loss = 1.72195
I0815 07:18:11.921442 11101 solver.cpp:334]     Train net output #0: loss = 1.53618 (* 1 = 1.53618 loss)
I0815 07:18:11.921553 11101 sgd_solver.cpp:136] Iteration 263100, lr = 0.0177813, m = 0.9
I0815 07:18:26.535491 11101 solver.cpp:312] Iteration 263200 (6.84282 iter/s, 14.6139s/100 iter), loss = 1.5581
I0815 07:18:26.535558 11101 solver.cpp:334]     Train net output #0: loss = 1.46863 (* 1 = 1.46863 loss)
I0815 07:18:26.535576 11101 sgd_solver.cpp:136] Iteration 263200, lr = 0.01775, m = 0.9
I0815 07:18:40.928758 11101 solver.cpp:312] Iteration 263300 (6.94788 iter/s, 14.3929s/100 iter), loss = 1.73937
I0815 07:18:40.928827 11101 solver.cpp:334]     Train net output #0: loss = 2.13007 (* 1 = 2.13007 loss)
I0815 07:18:40.928835 11101 sgd_solver.cpp:136] Iteration 263300, lr = 0.0177188, m = 0.9
I0815 07:18:55.423141 11101 solver.cpp:312] Iteration 263400 (6.89942 iter/s, 14.494s/100 iter), loss = 1.48404
I0815 07:18:55.423171 11101 solver.cpp:334]     Train net output #0: loss = 1.56636 (* 1 = 1.56636 loss)
I0815 07:18:55.423177 11101 sgd_solver.cpp:136] Iteration 263400, lr = 0.0176875, m = 0.9
I0815 07:19:10.047462 11101 solver.cpp:312] Iteration 263500 (6.83811 iter/s, 14.6239s/100 iter), loss = 1.72921
I0815 07:19:10.047685 11101 solver.cpp:334]     Train net output #0: loss = 2.01509 (* 1 = 2.01509 loss)
I0815 07:19:10.047797 11101 sgd_solver.cpp:136] Iteration 263500, lr = 0.0176562, m = 0.9
I0815 07:19:24.635916 11101 solver.cpp:312] Iteration 263600 (6.85492 iter/s, 14.5881s/100 iter), loss = 1.69891
I0815 07:19:24.635977 11101 solver.cpp:334]     Train net output #0: loss = 1.86295 (* 1 = 1.86295 loss)
I0815 07:19:24.635983 11101 sgd_solver.cpp:136] Iteration 263600, lr = 0.017625, m = 0.9
I0815 07:19:39.035248 11101 solver.cpp:312] Iteration 263700 (6.94496 iter/s, 14.3989s/100 iter), loss = 1.74541
I0815 07:19:39.035274 11101 solver.cpp:334]     Train net output #0: loss = 1.76 (* 1 = 1.76 loss)
I0815 07:19:39.035280 11101 sgd_solver.cpp:136] Iteration 263700, lr = 0.0175937, m = 0.9
I0815 07:19:53.334278 11101 solver.cpp:312] Iteration 263800 (6.99367 iter/s, 14.2986s/100 iter), loss = 1.54494
I0815 07:19:53.334305 11101 solver.cpp:334]     Train net output #0: loss = 1.61951 (* 1 = 1.61951 loss)
I0815 07:19:53.334311 11101 sgd_solver.cpp:136] Iteration 263800, lr = 0.0175625, m = 0.9
I0815 07:20:07.991998 11101 solver.cpp:312] Iteration 263900 (6.82253 iter/s, 14.6573s/100 iter), loss = 1.70426
I0815 07:20:07.992056 11101 solver.cpp:334]     Train net output #0: loss = 1.58474 (* 1 = 1.58474 loss)
I0815 07:20:07.992063 11101 sgd_solver.cpp:136] Iteration 263900, lr = 0.0175313, m = 0.9
I0815 07:20:22.286922 11101 solver.cpp:509] Iteration 264000, Testing net (#0)
I0815 07:20:43.243111 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.518
I0815 07:20:43.243175 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.765822
I0815 07:20:43.243183 11101 solver.cpp:594]     Test net output #2: loss = 2.13424 (* 1 = 2.13424 loss)
I0815 07:20:43.243234 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9557s
I0815 07:20:43.415814 11101 solver.cpp:312] Iteration 264000 (2.82304 iter/s, 35.4229s/100 iter), loss = 1.82048
I0815 07:20:43.415861 11101 solver.cpp:334]     Train net output #0: loss = 1.57912 (* 1 = 1.57912 loss)
I0815 07:20:43.415874 11101 sgd_solver.cpp:136] Iteration 264000, lr = 0.0175, m = 0.9
I0815 07:20:57.915813 11101 solver.cpp:312] Iteration 264100 (6.89674 iter/s, 14.4996s/100 iter), loss = 2.0688
I0815 07:20:57.915876 11101 solver.cpp:334]     Train net output #0: loss = 1.92853 (* 1 = 1.92853 loss)
I0815 07:20:57.915894 11101 sgd_solver.cpp:136] Iteration 264100, lr = 0.0174688, m = 0.9
I0815 07:21:12.737146 11101 solver.cpp:312] Iteration 264200 (6.74722 iter/s, 14.8209s/100 iter), loss = 1.6595
I0815 07:21:12.737174 11101 solver.cpp:334]     Train net output #0: loss = 1.7586 (* 1 = 1.7586 loss)
I0815 07:21:12.737179 11101 sgd_solver.cpp:136] Iteration 264200, lr = 0.0174375, m = 0.9
I0815 07:21:27.721251 11101 solver.cpp:312] Iteration 264300 (6.67392 iter/s, 14.9837s/100 iter), loss = 1.68662
I0815 07:21:27.721778 11101 solver.cpp:334]     Train net output #0: loss = 1.67445 (* 1 = 1.67445 loss)
I0815 07:21:27.721786 11101 sgd_solver.cpp:136] Iteration 264300, lr = 0.0174062, m = 0.9
I0815 07:21:42.974217 11101 solver.cpp:312] Iteration 264400 (6.55628 iter/s, 15.2525s/100 iter), loss = 1.54707
I0815 07:21:42.974246 11101 solver.cpp:334]     Train net output #0: loss = 1.66308 (* 1 = 1.66308 loss)
I0815 07:21:42.974251 11101 sgd_solver.cpp:136] Iteration 264400, lr = 0.017375, m = 0.9
I0815 07:21:57.963412 11101 solver.cpp:312] Iteration 264500 (6.67166 iter/s, 14.9888s/100 iter), loss = 1.62415
I0815 07:21:57.963466 11101 solver.cpp:334]     Train net output #0: loss = 1.49063 (* 1 = 1.49063 loss)
I0815 07:21:57.963471 11101 sgd_solver.cpp:136] Iteration 264500, lr = 0.0173437, m = 0.9
I0815 07:22:12.362123 11101 solver.cpp:312] Iteration 264600 (6.94526 iter/s, 14.3983s/100 iter), loss = 1.69905
I0815 07:22:12.362149 11101 solver.cpp:334]     Train net output #0: loss = 1.7719 (* 1 = 1.7719 loss)
I0815 07:22:12.362152 11101 sgd_solver.cpp:136] Iteration 264600, lr = 0.0173125, m = 0.9
I0815 07:22:27.119829 11101 solver.cpp:312] Iteration 264700 (6.77631 iter/s, 14.7573s/100 iter), loss = 1.6563
I0815 07:22:27.119855 11101 solver.cpp:334]     Train net output #0: loss = 1.77412 (* 1 = 1.77412 loss)
I0815 07:22:27.119859 11101 sgd_solver.cpp:136] Iteration 264700, lr = 0.0172813, m = 0.9
I0815 07:22:41.845062 11101 solver.cpp:312] Iteration 264800 (6.79125 iter/s, 14.7248s/100 iter), loss = 1.34546
I0815 07:22:41.845270 11101 solver.cpp:334]     Train net output #0: loss = 1.49888 (* 1 = 1.49888 loss)
I0815 07:22:41.845357 11101 sgd_solver.cpp:136] Iteration 264800, lr = 0.01725, m = 0.9
I0815 07:22:56.526783 11101 solver.cpp:312] Iteration 264900 (6.81138 iter/s, 14.6813s/100 iter), loss = 1.73003
I0815 07:22:56.526836 11101 solver.cpp:334]     Train net output #0: loss = 1.37744 (* 1 = 1.37744 loss)
I0815 07:22:56.526849 11101 sgd_solver.cpp:136] Iteration 264900, lr = 0.0172188, m = 0.9
I0815 07:23:11.223589 11101 solver.cpp:312] Iteration 265000 (6.80439 iter/s, 14.6964s/100 iter), loss = 1.97909
I0815 07:23:11.223664 11101 solver.cpp:334]     Train net output #0: loss = 1.89646 (* 1 = 1.89646 loss)
I0815 07:23:11.223685 11101 sgd_solver.cpp:136] Iteration 265000, lr = 0.0171875, m = 0.9
I0815 07:23:25.971740 11101 solver.cpp:312] Iteration 265100 (6.7807 iter/s, 14.7477s/100 iter), loss = 1.46144
I0815 07:23:25.971838 11101 solver.cpp:334]     Train net output #0: loss = 1.45999 (* 1 = 1.45999 loss)
I0815 07:23:25.971856 11101 sgd_solver.cpp:136] Iteration 265100, lr = 0.0171562, m = 0.9
I0815 07:23:40.440609 11101 solver.cpp:312] Iteration 265200 (6.91158 iter/s, 14.4685s/100 iter), loss = 1.56196
I0815 07:23:40.440631 11101 solver.cpp:334]     Train net output #0: loss = 1.09876 (* 1 = 1.09876 loss)
I0815 07:23:40.440637 11101 sgd_solver.cpp:136] Iteration 265200, lr = 0.017125, m = 0.9
I0815 07:23:54.901981 11101 solver.cpp:312] Iteration 265300 (6.91516 iter/s, 14.461s/100 iter), loss = 1.43158
I0815 07:23:54.902011 11101 solver.cpp:334]     Train net output #0: loss = 1.02406 (* 1 = 1.02406 loss)
I0815 07:23:54.902017 11101 sgd_solver.cpp:136] Iteration 265300, lr = 0.0170937, m = 0.9
I0815 07:24:09.464331 11101 solver.cpp:312] Iteration 265400 (6.86721 iter/s, 14.5619s/100 iter), loss = 1.82355
I0815 07:24:09.464721 11101 solver.cpp:334]     Train net output #0: loss = 2.35347 (* 1 = 2.35347 loss)
I0815 07:24:09.464833 11101 sgd_solver.cpp:136] Iteration 265400, lr = 0.0170625, m = 0.9
I0815 07:24:23.954036 11101 solver.cpp:312] Iteration 265500 (6.90164 iter/s, 14.4893s/100 iter), loss = 1.66622
I0815 07:24:23.954109 11101 solver.cpp:334]     Train net output #0: loss = 1.32481 (* 1 = 1.32481 loss)
I0815 07:24:23.954129 11101 sgd_solver.cpp:136] Iteration 265500, lr = 0.0170313, m = 0.9
I0815 07:24:38.529367 11101 solver.cpp:312] Iteration 265600 (6.8611 iter/s, 14.5749s/100 iter), loss = 1.76974
I0815 07:24:38.529397 11101 solver.cpp:334]     Train net output #0: loss = 1.51483 (* 1 = 1.51483 loss)
I0815 07:24:38.529402 11101 sgd_solver.cpp:136] Iteration 265600, lr = 0.017, m = 0.9
I0815 07:24:52.966753 11101 solver.cpp:312] Iteration 265700 (6.92665 iter/s, 14.437s/100 iter), loss = 1.47876
I0815 07:24:52.966846 11101 solver.cpp:334]     Train net output #0: loss = 1.6655 (* 1 = 1.6655 loss)
I0815 07:24:52.966863 11101 sgd_solver.cpp:136] Iteration 265700, lr = 0.0169688, m = 0.9
I0815 07:25:07.380422 11101 solver.cpp:312] Iteration 265800 (6.93805 iter/s, 14.4133s/100 iter), loss = 1.46566
I0815 07:25:07.380448 11101 solver.cpp:334]     Train net output #0: loss = 1.25205 (* 1 = 1.25205 loss)
I0815 07:25:07.380452 11101 sgd_solver.cpp:136] Iteration 265800, lr = 0.0169375, m = 0.9
I0815 07:25:21.967315 11101 solver.cpp:312] Iteration 265900 (6.85566 iter/s, 14.5865s/100 iter), loss = 1.66096
I0815 07:25:21.967344 11101 solver.cpp:334]     Train net output #0: loss = 1.44545 (* 1 = 1.44545 loss)
I0815 07:25:21.967351 11101 sgd_solver.cpp:136] Iteration 265900, lr = 0.0169063, m = 0.9
I0815 07:25:36.315011 11101 solver.cpp:509] Iteration 266000, Testing net (#0)
I0815 07:25:40.220361 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 07:25:57.253763 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.521235
I0815 07:25:57.253792 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.766881
I0815 07:25:57.253798 11101 solver.cpp:594]     Test net output #2: loss = 2.11145 (* 1 = 2.11145 loss)
I0815 07:25:57.253893 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9383s
I0815 07:25:57.408709 11101 solver.cpp:312] Iteration 266000 (2.82164 iter/s, 35.4404s/100 iter), loss = 1.36644
I0815 07:25:57.408927 11101 solver.cpp:334]     Train net output #0: loss = 1.38636 (* 1 = 1.38636 loss)
I0815 07:25:57.409034 11101 sgd_solver.cpp:136] Iteration 266000, lr = 0.016875, m = 0.9
I0815 07:26:12.013794 11101 solver.cpp:312] Iteration 266100 (6.84712 iter/s, 14.6047s/100 iter), loss = 1.61692
I0815 07:26:12.013885 11101 solver.cpp:334]     Train net output #0: loss = 1.48754 (* 1 = 1.48754 loss)
I0815 07:26:12.013901 11101 sgd_solver.cpp:136] Iteration 266100, lr = 0.0168437, m = 0.9
I0815 07:26:26.700422 11101 solver.cpp:312] Iteration 266200 (6.8091 iter/s, 14.6862s/100 iter), loss = 1.91026
I0815 07:26:26.700446 11101 solver.cpp:334]     Train net output #0: loss = 1.9591 (* 1 = 1.9591 loss)
I0815 07:26:26.700453 11101 sgd_solver.cpp:136] Iteration 266200, lr = 0.0168125, m = 0.9
I0815 07:26:41.098520 11101 solver.cpp:312] Iteration 266300 (6.94555 iter/s, 14.3977s/100 iter), loss = 1.60734
I0815 07:26:41.098546 11101 solver.cpp:334]     Train net output #0: loss = 1.35829 (* 1 = 1.35829 loss)
I0815 07:26:41.098551 11101 sgd_solver.cpp:136] Iteration 266300, lr = 0.0167813, m = 0.9
I0815 07:26:55.506680 11101 solver.cpp:312] Iteration 266400 (6.9407 iter/s, 14.4078s/100 iter), loss = 1.54932
I0815 07:26:55.506779 11101 solver.cpp:334]     Train net output #0: loss = 1.53319 (* 1 = 1.53319 loss)
I0815 07:26:55.506791 11101 sgd_solver.cpp:136] Iteration 266400, lr = 0.01675, m = 0.9
I0815 07:27:10.253353 11101 solver.cpp:312] Iteration 266500 (6.78138 iter/s, 14.7463s/100 iter), loss = 1.66522
I0815 07:27:10.253381 11101 solver.cpp:334]     Train net output #0: loss = 1.39968 (* 1 = 1.39968 loss)
I0815 07:27:10.253384 11101 sgd_solver.cpp:136] Iteration 266500, lr = 0.0167188, m = 0.9
I0815 07:27:24.777284 11101 solver.cpp:312] Iteration 266600 (6.88538 iter/s, 14.5235s/100 iter), loss = 1.44379
I0815 07:27:24.777351 11101 solver.cpp:334]     Train net output #0: loss = 1.37189 (* 1 = 1.37189 loss)
I0815 07:27:24.777370 11101 sgd_solver.cpp:136] Iteration 266600, lr = 0.0166875, m = 0.9
I0815 07:27:39.833529 11101 solver.cpp:312] Iteration 266700 (6.64194 iter/s, 15.0558s/100 iter), loss = 1.44855
I0815 07:27:39.833629 11101 solver.cpp:334]     Train net output #0: loss = 1.49708 (* 1 = 1.49708 loss)
I0815 07:27:39.833649 11101 sgd_solver.cpp:136] Iteration 266700, lr = 0.0166562, m = 0.9
I0815 07:27:54.613420 11101 solver.cpp:312] Iteration 266800 (6.76614 iter/s, 14.7795s/100 iter), loss = 1.7655
I0815 07:27:54.613451 11101 solver.cpp:334]     Train net output #0: loss = 1.93856 (* 1 = 1.93856 loss)
I0815 07:27:54.613456 11101 sgd_solver.cpp:136] Iteration 266800, lr = 0.016625, m = 0.9
I0815 07:28:09.323597 11101 solver.cpp:312] Iteration 266900 (6.7982 iter/s, 14.7098s/100 iter), loss = 1.6551
I0815 07:28:09.323668 11101 solver.cpp:334]     Train net output #0: loss = 1.7065 (* 1 = 1.7065 loss)
I0815 07:28:09.323688 11101 sgd_solver.cpp:136] Iteration 266900, lr = 0.0165937, m = 0.9
I0815 07:28:23.946650 11101 solver.cpp:312] Iteration 267000 (6.83871 iter/s, 14.6226s/100 iter), loss = 1.38732
I0815 07:28:23.946713 11101 solver.cpp:334]     Train net output #0: loss = 1.07186 (* 1 = 1.07186 loss)
I0815 07:28:23.946720 11101 sgd_solver.cpp:136] Iteration 267000, lr = 0.0165625, m = 0.9
I0815 07:28:38.773898 11101 solver.cpp:312] Iteration 267100 (6.74453 iter/s, 14.8268s/100 iter), loss = 1.72383
I0815 07:28:38.773921 11101 solver.cpp:334]     Train net output #0: loss = 1.90456 (* 1 = 1.90456 loss)
I0815 07:28:38.773926 11101 sgd_solver.cpp:136] Iteration 267100, lr = 0.0165313, m = 0.9
I0815 07:28:53.376860 11101 solver.cpp:312] Iteration 267200 (6.84811 iter/s, 14.6026s/100 iter), loss = 1.24199
I0815 07:28:53.376884 11101 solver.cpp:334]     Train net output #0: loss = 1.23804 (* 1 = 1.23804 loss)
I0815 07:28:53.376888 11101 sgd_solver.cpp:136] Iteration 267200, lr = 0.0165, m = 0.9
I0815 07:29:07.904551 11101 solver.cpp:312] Iteration 267300 (6.8836 iter/s, 14.5273s/100 iter), loss = 1.38707
I0815 07:29:07.904613 11101 solver.cpp:334]     Train net output #0: loss = 1.36008 (* 1 = 1.36008 loss)
I0815 07:29:07.904621 11101 sgd_solver.cpp:136] Iteration 267300, lr = 0.0164688, m = 0.9
I0815 07:29:22.694365 11101 solver.cpp:312] Iteration 267400 (6.7616 iter/s, 14.7894s/100 iter), loss = 1.26229
I0815 07:29:22.694391 11101 solver.cpp:334]     Train net output #0: loss = 1.35559 (* 1 = 1.35559 loss)
I0815 07:29:22.694394 11101 sgd_solver.cpp:136] Iteration 267400, lr = 0.0164375, m = 0.9
I0815 07:29:37.077661 11101 solver.cpp:312] Iteration 267500 (6.9527 iter/s, 14.3829s/100 iter), loss = 1.76984
I0815 07:29:37.077728 11101 solver.cpp:334]     Train net output #0: loss = 1.91591 (* 1 = 1.91591 loss)
I0815 07:29:37.077747 11101 sgd_solver.cpp:136] Iteration 267500, lr = 0.0164063, m = 0.9
I0815 07:29:51.717600 11101 solver.cpp:312] Iteration 267600 (6.83082 iter/s, 14.6395s/100 iter), loss = 1.62364
I0815 07:29:51.717700 11101 solver.cpp:334]     Train net output #0: loss = 1.31832 (* 1 = 1.31832 loss)
I0815 07:29:51.717718 11101 sgd_solver.cpp:136] Iteration 267600, lr = 0.016375, m = 0.9
I0815 07:30:06.460553 11101 solver.cpp:312] Iteration 267700 (6.78309 iter/s, 14.7426s/100 iter), loss = 1.62769
I0815 07:30:06.460578 11101 solver.cpp:334]     Train net output #0: loss = 2.01223 (* 1 = 2.01223 loss)
I0815 07:30:06.460583 11101 sgd_solver.cpp:136] Iteration 267700, lr = 0.0163437, m = 0.9
I0815 07:30:21.427279 11101 solver.cpp:312] Iteration 267800 (6.68167 iter/s, 14.9663s/100 iter), loss = 1.63218
I0815 07:30:21.427305 11101 solver.cpp:334]     Train net output #0: loss = 1.35934 (* 1 = 1.35934 loss)
I0815 07:30:21.427311 11101 sgd_solver.cpp:136] Iteration 267800, lr = 0.0163125, m = 0.9
I0815 07:30:36.024379 11101 solver.cpp:312] Iteration 267900 (6.85086 iter/s, 14.5967s/100 iter), loss = 1.75711
I0815 07:30:36.024441 11101 solver.cpp:334]     Train net output #0: loss = 1.97562 (* 1 = 1.97562 loss)
I0815 07:30:36.024449 11101 sgd_solver.cpp:136] Iteration 267900, lr = 0.0162812, m = 0.9
I0815 07:30:50.565116 11101 solver.cpp:509] Iteration 268000, Testing net (#0)
I0815 07:31:11.368155 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.532882
I0815 07:31:11.368206 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.778174
I0815 07:31:11.368212 11101 solver.cpp:594]     Test net output #2: loss = 2.03274 (* 1 = 2.03274 loss)
I0815 07:31:11.368230 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8026s
I0815 07:31:11.528700 11101 solver.cpp:312] Iteration 268000 (2.81664 iter/s, 35.5034s/100 iter), loss = 1.85497
I0815 07:31:11.528728 11101 solver.cpp:334]     Train net output #0: loss = 1.97662 (* 1 = 1.97662 loss)
I0815 07:31:11.528733 11101 sgd_solver.cpp:136] Iteration 268000, lr = 0.01625, m = 0.9
I0815 07:31:25.871583 11101 solver.cpp:312] Iteration 268100 (6.97229 iter/s, 14.3425s/100 iter), loss = 1.56242
I0815 07:31:25.871609 11101 solver.cpp:334]     Train net output #0: loss = 1.36213 (* 1 = 1.36213 loss)
I0815 07:31:25.871616 11101 sgd_solver.cpp:136] Iteration 268100, lr = 0.0162188, m = 0.9
I0815 07:31:40.515111 11101 solver.cpp:312] Iteration 268200 (6.82914 iter/s, 14.6431s/100 iter), loss = 1.60007
I0815 07:31:40.515138 11101 solver.cpp:334]     Train net output #0: loss = 1.5145 (* 1 = 1.5145 loss)
I0815 07:31:40.515144 11101 sgd_solver.cpp:136] Iteration 268200, lr = 0.0161875, m = 0.9
I0815 07:31:55.074533 11101 solver.cpp:312] Iteration 268300 (6.8686 iter/s, 14.559s/100 iter), loss = 1.65926
I0815 07:31:55.074594 11101 solver.cpp:334]     Train net output #0: loss = 1.92993 (* 1 = 1.92993 loss)
I0815 07:31:55.074601 11101 sgd_solver.cpp:136] Iteration 268300, lr = 0.0161563, m = 0.9
I0815 07:32:09.570652 11101 solver.cpp:312] Iteration 268400 (6.89859 iter/s, 14.4957s/100 iter), loss = 1.62029
I0815 07:32:09.570678 11101 solver.cpp:334]     Train net output #0: loss = 1.51494 (* 1 = 1.51494 loss)
I0815 07:32:09.570685 11101 sgd_solver.cpp:136] Iteration 268400, lr = 0.016125, m = 0.9
I0815 07:32:24.227356 11101 solver.cpp:312] Iteration 268500 (6.823 iter/s, 14.6563s/100 iter), loss = 1.99421
I0815 07:32:24.227390 11101 solver.cpp:334]     Train net output #0: loss = 2.0822 (* 1 = 2.0822 loss)
I0815 07:32:24.227396 11101 sgd_solver.cpp:136] Iteration 268500, lr = 0.0160937, m = 0.9
I0815 07:32:39.036115 11101 solver.cpp:312] Iteration 268600 (6.75295 iter/s, 14.8083s/100 iter), loss = 1.23651
I0815 07:32:39.036327 11101 solver.cpp:334]     Train net output #0: loss = 1.3401 (* 1 = 1.3401 loss)
I0815 07:32:39.036414 11101 sgd_solver.cpp:136] Iteration 268600, lr = 0.0160625, m = 0.9
I0815 07:32:53.835194 11101 solver.cpp:312] Iteration 268700 (6.75736 iter/s, 14.7987s/100 iter), loss = 1.71053
I0815 07:32:53.835268 11101 solver.cpp:334]     Train net output #0: loss = 1.75279 (* 1 = 1.75279 loss)
I0815 07:32:53.835285 11101 sgd_solver.cpp:136] Iteration 268700, lr = 0.0160312, m = 0.9
I0815 07:33:08.339784 11101 solver.cpp:312] Iteration 268800 (6.89456 iter/s, 14.5042s/100 iter), loss = 1.57808
I0815 07:33:08.339812 11101 solver.cpp:334]     Train net output #0: loss = 1.66737 (* 1 = 1.66737 loss)
I0815 07:33:08.339819 11101 sgd_solver.cpp:136] Iteration 268800, lr = 0.016, m = 0.9
I0815 07:33:23.110877 11101 solver.cpp:312] Iteration 268900 (6.77017 iter/s, 14.7707s/100 iter), loss = 1.74876
I0815 07:33:23.110940 11101 solver.cpp:334]     Train net output #0: loss = 1.48909 (* 1 = 1.48909 loss)
I0815 07:33:23.110949 11101 sgd_solver.cpp:136] Iteration 268900, lr = 0.0159688, m = 0.9
I0815 07:33:37.786100 11101 solver.cpp:312] Iteration 269000 (6.81439 iter/s, 14.6748s/100 iter), loss = 1.56393
I0815 07:33:37.786128 11101 solver.cpp:334]     Train net output #0: loss = 1.73748 (* 1 = 1.73748 loss)
I0815 07:33:37.786134 11101 sgd_solver.cpp:136] Iteration 269000, lr = 0.0159375, m = 0.9
I0815 07:33:52.317677 11101 solver.cpp:312] Iteration 269100 (6.88176 iter/s, 14.5312s/100 iter), loss = 1.93573
I0815 07:33:52.317704 11101 solver.cpp:334]     Train net output #0: loss = 1.92461 (* 1 = 1.92461 loss)
I0815 07:33:52.317708 11101 sgd_solver.cpp:136] Iteration 269100, lr = 0.0159063, m = 0.9
I0815 07:34:07.064443 11101 solver.cpp:312] Iteration 269200 (6.78134 iter/s, 14.7464s/100 iter), loss = 1.73728
I0815 07:34:07.064532 11101 solver.cpp:334]     Train net output #0: loss = 1.77914 (* 1 = 1.77914 loss)
I0815 07:34:07.064554 11101 sgd_solver.cpp:136] Iteration 269200, lr = 0.015875, m = 0.9
I0815 07:34:21.775238 11101 solver.cpp:312] Iteration 269300 (6.79792 iter/s, 14.7104s/100 iter), loss = 1.1938
I0815 07:34:21.775308 11101 solver.cpp:334]     Train net output #0: loss = 0.895617 (* 1 = 0.895617 loss)
I0815 07:34:21.775328 11101 sgd_solver.cpp:136] Iteration 269300, lr = 0.0158437, m = 0.9
I0815 07:34:36.605792 11101 solver.cpp:312] Iteration 269400 (6.74302 iter/s, 14.8301s/100 iter), loss = 1.43199
I0815 07:34:36.605849 11101 solver.cpp:334]     Train net output #0: loss = 1.11611 (* 1 = 1.11611 loss)
I0815 07:34:36.605860 11101 sgd_solver.cpp:136] Iteration 269400, lr = 0.0158125, m = 0.9
I0815 07:34:51.439867 11101 solver.cpp:312] Iteration 269500 (6.74142 iter/s, 14.8337s/100 iter), loss = 1.46449
I0815 07:34:51.439962 11101 solver.cpp:334]     Train net output #0: loss = 1.19085 (* 1 = 1.19085 loss)
I0815 07:34:51.439980 11101 sgd_solver.cpp:136] Iteration 269500, lr = 0.0157812, m = 0.9
I0815 07:35:06.187932 11101 solver.cpp:312] Iteration 269600 (6.78073 iter/s, 14.7477s/100 iter), loss = 1.0804
I0815 07:35:06.187994 11101 solver.cpp:334]     Train net output #0: loss = 1.1066 (* 1 = 1.1066 loss)
I0815 07:35:06.188011 11101 sgd_solver.cpp:136] Iteration 269600, lr = 0.01575, m = 0.9
I0815 07:35:21.257115 11101 solver.cpp:312] Iteration 269700 (6.63624 iter/s, 15.0688s/100 iter), loss = 1.40767
I0815 07:35:21.257186 11101 solver.cpp:334]     Train net output #0: loss = 1.45565 (* 1 = 1.45565 loss)
I0815 07:35:21.257205 11101 sgd_solver.cpp:136] Iteration 269700, lr = 0.0157188, m = 0.9
I0815 07:35:36.106238 11101 solver.cpp:312] Iteration 269800 (6.73459 iter/s, 14.8487s/100 iter), loss = 1.31134
I0815 07:35:36.106326 11101 solver.cpp:334]     Train net output #0: loss = 1.2245 (* 1 = 1.2245 loss)
I0815 07:35:36.106343 11101 sgd_solver.cpp:136] Iteration 269800, lr = 0.0156875, m = 0.9
I0815 07:35:50.676055 11101 solver.cpp:312] Iteration 269900 (6.86369 iter/s, 14.5694s/100 iter), loss = 1.6831
I0815 07:35:50.676084 11101 solver.cpp:334]     Train net output #0: loss = 1.60042 (* 1 = 1.60042 loss)
I0815 07:35:50.676090 11101 sgd_solver.cpp:136] Iteration 269900, lr = 0.0156563, m = 0.9
I0815 07:36:05.251034 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_270000.caffemodel
I0815 07:36:05.264380 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_270000.solverstate
I0815 07:36:05.269067 11101 solver.cpp:509] Iteration 270000, Testing net (#0)
I0815 07:36:11.557646 11099 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 07:36:26.208026 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.533412
I0815 07:36:26.208047 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.777469
I0815 07:36:26.208055 11101 solver.cpp:594]     Test net output #2: loss = 2.06835 (* 1 = 2.06835 loss)
I0815 07:36:26.208081 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9384s
I0815 07:36:26.384330 11101 solver.cpp:312] Iteration 270000 (2.80055 iter/s, 35.7073s/100 iter), loss = 1.72303
I0815 07:36:26.384359 11101 solver.cpp:334]     Train net output #0: loss = 1.68926 (* 1 = 1.68926 loss)
I0815 07:36:26.384366 11101 sgd_solver.cpp:136] Iteration 270000, lr = 0.015625, m = 0.9
I0815 07:36:40.833127 11101 solver.cpp:312] Iteration 270100 (6.92118 iter/s, 14.4484s/100 iter), loss = 1.32055
I0815 07:36:40.833313 11101 solver.cpp:334]     Train net output #0: loss = 1.63945 (* 1 = 1.63945 loss)
I0815 07:36:40.833429 11101 sgd_solver.cpp:136] Iteration 270100, lr = 0.0155937, m = 0.9
I0815 07:36:55.241045 11101 solver.cpp:312] Iteration 270200 (6.94082 iter/s, 14.4075s/100 iter), loss = 1.56974
I0815 07:36:55.243228 11101 solver.cpp:334]     Train net output #0: loss = 1.41702 (* 1 = 1.41702 loss)
I0815 07:36:55.243237 11101 sgd_solver.cpp:136] Iteration 270200, lr = 0.0155625, m = 0.9
I0815 07:37:09.649461 11101 solver.cpp:312] Iteration 270300 (6.94058 iter/s, 14.408s/100 iter), loss = 1.24956
I0815 07:37:09.649530 11101 solver.cpp:334]     Train net output #0: loss = 1.32025 (* 1 = 1.32025 loss)
I0815 07:37:09.649547 11101 sgd_solver.cpp:136] Iteration 270300, lr = 0.0155312, m = 0.9
I0815 07:37:24.430475 11101 solver.cpp:312] Iteration 270400 (6.76562 iter/s, 14.7806s/100 iter), loss = 1.30386
I0815 07:37:24.430527 11101 solver.cpp:334]     Train net output #0: loss = 0.99141 (* 1 = 0.99141 loss)
I0815 07:37:24.430539 11101 sgd_solver.cpp:136] Iteration 270400, lr = 0.0155, m = 0.9
I0815 07:37:39.003088 11101 solver.cpp:312] Iteration 270500 (6.86238 iter/s, 14.5722s/100 iter), loss = 1.65812
I0815 07:37:39.003172 11101 solver.cpp:334]     Train net output #0: loss = 1.6639 (* 1 = 1.6639 loss)
I0815 07:37:39.003180 11101 sgd_solver.cpp:136] Iteration 270500, lr = 0.0154688, m = 0.9
I0815 07:37:53.519938 11101 solver.cpp:312] Iteration 270600 (6.88874 iter/s, 14.5164s/100 iter), loss = 1.74972
I0815 07:37:53.520004 11101 solver.cpp:334]     Train net output #0: loss = 1.60214 (* 1 = 1.60214 loss)
I0815 07:37:53.520021 11101 sgd_solver.cpp:136] Iteration 270600, lr = 0.0154375, m = 0.9
I0815 07:38:07.972476 11101 solver.cpp:312] Iteration 270700 (6.91939 iter/s, 14.4521s/100 iter), loss = 1.87237
I0815 07:38:07.972512 11101 solver.cpp:334]     Train net output #0: loss = 2.01487 (* 1 = 2.01487 loss)
I0815 07:38:07.972518 11101 sgd_solver.cpp:136] Iteration 270700, lr = 0.0154063, m = 0.9
I0815 07:38:22.420145 11101 solver.cpp:312] Iteration 270800 (6.92173 iter/s, 14.4473s/100 iter), loss = 1.39194
I0815 07:38:22.420193 11101 solver.cpp:334]     Train net output #0: loss = 1.20379 (* 1 = 1.20379 loss)
I0815 07:38:22.420202 11101 sgd_solver.cpp:136] Iteration 270800, lr = 0.015375, m = 0.9
I0815 07:38:36.909508 11101 solver.cpp:312] Iteration 270900 (6.9018 iter/s, 14.489s/100 iter), loss = 1.63277
I0815 07:38:36.911312 11101 solver.cpp:334]     Train net output #0: loss = 1.74341 (* 1 = 1.74341 loss)
I0815 07:38:36.911339 11101 sgd_solver.cpp:136] Iteration 270900, lr = 0.0153437, m = 0.9
I0815 07:38:51.374773 11101 solver.cpp:312] Iteration 271000 (6.9133 iter/s, 14.4649s/100 iter), loss = 1.57205
I0815 07:38:51.388176 11101 solver.cpp:334]     Train net output #0: loss = 1.61442 (* 1 = 1.61442 loss)
I0815 07:38:51.388276 11101 sgd_solver.cpp:136] Iteration 271000, lr = 0.0153125, m = 0.9
I0815 07:39:06.083415 11101 solver.cpp:312] Iteration 271100 (6.79891 iter/s, 14.7082s/100 iter), loss = 1.66116
I0815 07:39:06.083511 11101 solver.cpp:334]     Train net output #0: loss = 1.48806 (* 1 = 1.48806 loss)
I0815 07:39:06.083529 11101 sgd_solver.cpp:136] Iteration 271100, lr = 0.0152812, m = 0.9
I0815 07:39:20.406051 11101 solver.cpp:312] Iteration 271200 (6.98215 iter/s, 14.3222s/100 iter), loss = 1.83872
I0815 07:39:20.406080 11101 solver.cpp:334]     Train net output #0: loss = 1.85905 (* 1 = 1.85905 loss)
I0815 07:39:20.406085 11101 sgd_solver.cpp:136] Iteration 271200, lr = 0.01525, m = 0.9
I0815 07:39:35.122747 11101 solver.cpp:312] Iteration 271300 (6.79519 iter/s, 14.7163s/100 iter), loss = 1.84158
I0815 07:39:35.122819 11101 solver.cpp:334]     Train net output #0: loss = 1.84761 (* 1 = 1.84761 loss)
I0815 07:39:35.122838 11101 sgd_solver.cpp:136] Iteration 271300, lr = 0.0152188, m = 0.9
I0815 07:39:49.946431 11101 solver.cpp:312] Iteration 271400 (6.74615 iter/s, 14.8233s/100 iter), loss = 1.75999
I0815 07:39:49.946542 11101 solver.cpp:334]     Train net output #0: loss = 1.67264 (* 1 = 1.67264 loss)
I0815 07:39:49.946560 11101 sgd_solver.cpp:136] Iteration 271400, lr = 0.0151875, m = 0.9
I0815 07:40:04.317195 11101 solver.cpp:312] Iteration 271500 (6.95876 iter/s, 14.3704s/100 iter), loss = 1.4991
I0815 07:40:04.317227 11101 solver.cpp:334]     Train net output #0: loss = 1.65182 (* 1 = 1.65182 loss)
I0815 07:40:04.317234 11101 sgd_solver.cpp:136] Iteration 271500, lr = 0.0151563, m = 0.9
I0815 07:40:19.157490 11101 solver.cpp:312] Iteration 271600 (6.7386 iter/s, 14.8399s/100 iter), loss = 1.29405
I0815 07:40:19.157562 11101 solver.cpp:334]     Train net output #0: loss = 1.33413 (* 1 = 1.33413 loss)
I0815 07:40:19.157579 11101 sgd_solver.cpp:136] Iteration 271600, lr = 0.015125, m = 0.9
I0815 07:40:34.564651 11101 solver.cpp:312] Iteration 271700 (6.49067 iter/s, 15.4067s/100 iter), loss = 1.48935
I0815 07:40:34.564704 11101 solver.cpp:334]     Train net output #0: loss = 1.76787 (* 1 = 1.76787 loss)
I0815 07:40:34.564710 11101 sgd_solver.cpp:136] Iteration 271700, lr = 0.0150938, m = 0.9
I0815 07:40:49.571274 11101 solver.cpp:312] Iteration 271800 (6.66391 iter/s, 15.0062s/100 iter), loss = 1.65835
I0815 07:40:49.571293 11101 solver.cpp:334]     Train net output #0: loss = 1.78498 (* 1 = 1.78498 loss)
I0815 07:40:49.571297 11101 sgd_solver.cpp:136] Iteration 271800, lr = 0.0150625, m = 0.9
I0815 07:41:04.374352 11101 solver.cpp:312] Iteration 271900 (6.75554 iter/s, 14.8027s/100 iter), loss = 1.54297
I0815 07:41:04.374377 11101 solver.cpp:334]     Train net output #0: loss = 1.62527 (* 1 = 1.62527 loss)
I0815 07:41:04.374382 11101 sgd_solver.cpp:136] Iteration 271900, lr = 0.0150312, m = 0.9
I0815 07:41:19.168841 11101 solver.cpp:509] Iteration 272000, Testing net (#0)
I0815 07:41:19.204354 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 07:41:40.219856 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.535706
I0815 07:41:40.219877 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.773822
I0815 07:41:40.219883 11101 solver.cpp:594]     Test net output #2: loss = 2.07347 (* 1 = 2.07347 loss)
I0815 07:41:40.219904 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0505s
I0815 07:41:40.370594 11101 solver.cpp:312] Iteration 272000 (2.77814 iter/s, 35.9953s/100 iter), loss = 1.6592
I0815 07:41:40.370640 11101 solver.cpp:334]     Train net output #0: loss = 1.86402 (* 1 = 1.86402 loss)
I0815 07:41:40.370652 11101 sgd_solver.cpp:136] Iteration 272000, lr = 0.015, m = 0.9
I0815 07:41:55.142374 11101 solver.cpp:312] Iteration 272100 (6.76985 iter/s, 14.7714s/100 iter), loss = 1.43018
I0815 07:41:55.143136 11101 solver.cpp:334]     Train net output #0: loss = 1.52999 (* 1 = 1.52999 loss)
I0815 07:41:55.143157 11101 sgd_solver.cpp:136] Iteration 272100, lr = 0.0149688, m = 0.9
I0815 07:42:09.789976 11101 solver.cpp:312] Iteration 272200 (6.82725 iter/s, 14.6472s/100 iter), loss = 1.58534
I0815 07:42:09.790062 11101 solver.cpp:334]     Train net output #0: loss = 1.43467 (* 1 = 1.43467 loss)
I0815 07:42:09.790088 11101 sgd_solver.cpp:136] Iteration 272200, lr = 0.0149375, m = 0.9
I0815 07:42:24.608093 11101 solver.cpp:312] Iteration 272300 (6.74868 iter/s, 14.8177s/100 iter), loss = 1.71257
I0815 07:42:24.608120 11101 solver.cpp:334]     Train net output #0: loss = 1.87692 (* 1 = 1.87692 loss)
I0815 07:42:24.608124 11101 sgd_solver.cpp:136] Iteration 272300, lr = 0.0149063, m = 0.9
I0815 07:42:39.422715 11101 solver.cpp:312] Iteration 272400 (6.75027 iter/s, 14.8142s/100 iter), loss = 1.68375
I0815 07:42:39.422816 11101 solver.cpp:334]     Train net output #0: loss = 1.20817 (* 1 = 1.20817 loss)
I0815 07:42:39.422833 11101 sgd_solver.cpp:136] Iteration 272400, lr = 0.014875, m = 0.9
I0815 07:42:54.080286 11101 solver.cpp:312] Iteration 272500 (6.8226 iter/s, 14.6572s/100 iter), loss = 1.58167
I0815 07:42:54.080440 11101 solver.cpp:334]     Train net output #0: loss = 1.50251 (* 1 = 1.50251 loss)
I0815 07:42:54.080461 11101 sgd_solver.cpp:136] Iteration 272500, lr = 0.0148437, m = 0.9
I0815 07:43:09.743031 11101 solver.cpp:312] Iteration 272600 (6.38475 iter/s, 15.6623s/100 iter), loss = 1.14666
I0815 07:43:09.743165 11101 solver.cpp:334]     Train net output #0: loss = 1.00193 (* 1 = 1.00193 loss)
I0815 07:43:09.743196 11101 sgd_solver.cpp:136] Iteration 272600, lr = 0.0148125, m = 0.9
I0815 07:43:27.832756 11101 solver.cpp:312] Iteration 272700 (5.52815 iter/s, 18.0892s/100 iter), loss = 1.85889
I0815 07:43:27.832783 11101 solver.cpp:334]     Train net output #0: loss = 1.88698 (* 1 = 1.88698 loss)
I0815 07:43:27.832789 11101 sgd_solver.cpp:136] Iteration 272700, lr = 0.0147812, m = 0.9
I0815 07:43:44.843541 11101 solver.cpp:312] Iteration 272800 (5.87879 iter/s, 17.0103s/100 iter), loss = 1.66715
I0815 07:43:44.843605 11101 solver.cpp:334]     Train net output #0: loss = 1.145 (* 1 = 1.145 loss)
I0815 07:43:44.843611 11101 sgd_solver.cpp:136] Iteration 272800, lr = 0.01475, m = 0.9
I0815 07:43:59.969832 11101 solver.cpp:312] Iteration 272900 (6.61119 iter/s, 15.1259s/100 iter), loss = 1.71541
I0815 07:43:59.969862 11101 solver.cpp:334]     Train net output #0: loss = 1.42971 (* 1 = 1.42971 loss)
I0815 07:43:59.969869 11101 sgd_solver.cpp:136] Iteration 272900, lr = 0.0147187, m = 0.9
I0815 07:44:15.213948 11101 solver.cpp:312] Iteration 273000 (6.56009 iter/s, 15.2437s/100 iter), loss = 1.97811
I0815 07:44:15.214001 11101 solver.cpp:334]     Train net output #0: loss = 1.38199 (* 1 = 1.38199 loss)
I0815 07:44:15.214009 11101 sgd_solver.cpp:136] Iteration 273000, lr = 0.0146875, m = 0.9
I0815 07:44:30.037556 11101 solver.cpp:312] Iteration 273100 (6.74618 iter/s, 14.8232s/100 iter), loss = 1.64372
I0815 07:44:30.037580 11101 solver.cpp:334]     Train net output #0: loss = 1.3953 (* 1 = 1.3953 loss)
I0815 07:44:30.037585 11101 sgd_solver.cpp:136] Iteration 273100, lr = 0.0146563, m = 0.9
I0815 07:44:44.886591 11101 solver.cpp:312] Iteration 273200 (6.73463 iter/s, 14.8486s/100 iter), loss = 1.37753
I0815 07:44:44.886638 11101 solver.cpp:334]     Train net output #0: loss = 1.3313 (* 1 = 1.3313 loss)
I0815 07:44:44.886651 11101 sgd_solver.cpp:136] Iteration 273200, lr = 0.014625, m = 0.9
I0815 07:44:59.367954 11101 solver.cpp:312] Iteration 273300 (6.90562 iter/s, 14.481s/100 iter), loss = 1.56684
I0815 07:44:59.368023 11101 solver.cpp:334]     Train net output #0: loss = 1.51412 (* 1 = 1.51412 loss)
I0815 07:44:59.368029 11101 sgd_solver.cpp:136] Iteration 273300, lr = 0.0145938, m = 0.9
I0815 07:45:13.829198 11101 solver.cpp:312] Iteration 273400 (6.91523 iter/s, 14.4608s/100 iter), loss = 1.65701
I0815 07:45:13.829241 11101 solver.cpp:334]     Train net output #0: loss = 1.6775 (* 1 = 1.6775 loss)
I0815 07:45:13.829252 11101 sgd_solver.cpp:136] Iteration 273400, lr = 0.0145625, m = 0.9
I0815 07:45:28.712435 11101 solver.cpp:312] Iteration 273500 (6.71915 iter/s, 14.8828s/100 iter), loss = 1.46395
I0815 07:45:28.712463 11101 solver.cpp:334]     Train net output #0: loss = 1.4503 (* 1 = 1.4503 loss)
I0815 07:45:28.712469 11101 sgd_solver.cpp:136] Iteration 273500, lr = 0.0145312, m = 0.9
I0815 07:45:43.318681 11101 solver.cpp:312] Iteration 273600 (6.84658 iter/s, 14.6058s/100 iter), loss = 1.43515
I0815 07:45:43.318742 11101 solver.cpp:334]     Train net output #0: loss = 1.47069 (* 1 = 1.47069 loss)
I0815 07:45:43.318749 11101 sgd_solver.cpp:136] Iteration 273600, lr = 0.0145, m = 0.9
I0815 07:45:57.736937 11101 solver.cpp:312] Iteration 273700 (6.93584 iter/s, 14.4179s/100 iter), loss = 1.53083
I0815 07:45:57.736966 11101 solver.cpp:334]     Train net output #0: loss = 1.21168 (* 1 = 1.21168 loss)
I0815 07:45:57.737013 11101 sgd_solver.cpp:136] Iteration 273700, lr = 0.0144687, m = 0.9
I0815 07:46:12.118669 11101 solver.cpp:312] Iteration 273800 (6.95345 iter/s, 14.3813s/100 iter), loss = 1.59254
I0815 07:46:12.118695 11101 solver.cpp:334]     Train net output #0: loss = 1.62669 (* 1 = 1.62669 loss)
I0815 07:46:12.118718 11101 sgd_solver.cpp:136] Iteration 273800, lr = 0.0144375, m = 0.9
I0815 07:46:26.762720 11101 solver.cpp:312] Iteration 273900 (6.8289 iter/s, 14.6436s/100 iter), loss = 1.58553
I0815 07:46:26.762969 11101 solver.cpp:334]     Train net output #0: loss = 1.8856 (* 1 = 1.8856 loss)
I0815 07:46:26.763080 11101 sgd_solver.cpp:136] Iteration 273900, lr = 0.0144063, m = 0.9
I0815 07:46:41.301802 11101 solver.cpp:509] Iteration 274000, Testing net (#0)
I0815 07:47:02.309198 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.535353
I0815 07:47:02.309259 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.776822
I0815 07:47:02.309268 11101 solver.cpp:594]     Test net output #2: loss = 2.03198 (* 1 = 2.03198 loss)
I0815 07:47:02.309319 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.0069s
I0815 07:47:02.478492 11101 solver.cpp:312] Iteration 274000 (2.79996 iter/s, 35.7148s/100 iter), loss = 1.35567
I0815 07:47:02.478516 11101 solver.cpp:334]     Train net output #0: loss = 1.33765 (* 1 = 1.33765 loss)
I0815 07:47:02.478520 11101 sgd_solver.cpp:136] Iteration 274000, lr = 0.014375, m = 0.9
I0815 07:47:17.187921 11101 solver.cpp:312] Iteration 274100 (6.79855 iter/s, 14.709s/100 iter), loss = 1.60389
I0815 07:47:17.187947 11101 solver.cpp:334]     Train net output #0: loss = 1.79965 (* 1 = 1.79965 loss)
I0815 07:47:17.187953 11101 sgd_solver.cpp:136] Iteration 274100, lr = 0.0143438, m = 0.9
I0815 07:47:31.924851 11101 solver.cpp:312] Iteration 274200 (6.78586 iter/s, 14.7365s/100 iter), loss = 1.5796
I0815 07:47:31.924880 11101 solver.cpp:334]     Train net output #0: loss = 1.63216 (* 1 = 1.63216 loss)
I0815 07:47:31.924883 11101 sgd_solver.cpp:136] Iteration 274200, lr = 0.0143125, m = 0.9
I0815 07:47:46.458199 11101 solver.cpp:312] Iteration 274300 (6.88092 iter/s, 14.5329s/100 iter), loss = 1.50116
I0815 07:47:46.458444 11101 solver.cpp:334]     Train net output #0: loss = 1.24094 (* 1 = 1.24094 loss)
I0815 07:47:46.458463 11101 sgd_solver.cpp:136] Iteration 274300, lr = 0.0142812, m = 0.9
I0815 07:48:00.934697 11101 solver.cpp:312] Iteration 274400 (6.90794 iter/s, 14.4761s/100 iter), loss = 1.44204
I0815 07:48:00.934725 11101 solver.cpp:334]     Train net output #0: loss = 1.54346 (* 1 = 1.54346 loss)
I0815 07:48:00.934729 11101 sgd_solver.cpp:136] Iteration 274400, lr = 0.01425, m = 0.9
I0815 07:48:15.661480 11101 solver.cpp:312] Iteration 274500 (6.79054 iter/s, 14.7264s/100 iter), loss = 1.51303
I0815 07:48:15.661510 11101 solver.cpp:334]     Train net output #0: loss = 1.44364 (* 1 = 1.44364 loss)
I0815 07:48:15.661517 11101 sgd_solver.cpp:136] Iteration 274500, lr = 0.0142187, m = 0.9
I0815 07:48:30.295191 11101 solver.cpp:312] Iteration 274600 (6.83372 iter/s, 14.6333s/100 iter), loss = 1.46186
I0815 07:48:30.295254 11101 solver.cpp:334]     Train net output #0: loss = 1.82289 (* 1 = 1.82289 loss)
I0815 07:48:30.295261 11101 sgd_solver.cpp:136] Iteration 274600, lr = 0.0141875, m = 0.9
I0815 07:48:44.858091 11101 solver.cpp:312] Iteration 274700 (6.86695 iter/s, 14.5625s/100 iter), loss = 1.5397
I0815 07:48:44.858119 11101 solver.cpp:334]     Train net output #0: loss = 1.42304 (* 1 = 1.42304 loss)
I0815 07:48:44.858124 11101 sgd_solver.cpp:136] Iteration 274700, lr = 0.0141563, m = 0.9
I0815 07:48:59.449494 11101 solver.cpp:312] Iteration 274800 (6.85354 iter/s, 14.591s/100 iter), loss = 1.83378
I0815 07:48:59.449522 11101 solver.cpp:334]     Train net output #0: loss = 1.84914 (* 1 = 1.84914 loss)
I0815 07:48:59.449527 11101 sgd_solver.cpp:136] Iteration 274800, lr = 0.014125, m = 0.9
I0815 07:49:13.935549 11101 solver.cpp:312] Iteration 274900 (6.90338 iter/s, 14.4857s/100 iter), loss = 1.43929
I0815 07:49:13.935658 11101 solver.cpp:334]     Train net output #0: loss = 1.30808 (* 1 = 1.30808 loss)
I0815 07:49:13.935681 11101 sgd_solver.cpp:136] Iteration 274900, lr = 0.0140938, m = 0.9
I0815 07:49:28.491531 11101 solver.cpp:312] Iteration 275000 (6.87022 iter/s, 14.5556s/100 iter), loss = 1.64029
I0815 07:49:28.491567 11101 solver.cpp:334]     Train net output #0: loss = 1.60667 (* 1 = 1.60667 loss)
I0815 07:49:28.491572 11101 sgd_solver.cpp:136] Iteration 275000, lr = 0.0140625, m = 0.9
I0815 07:49:43.380682 11101 solver.cpp:312] Iteration 275100 (6.71649 iter/s, 14.8887s/100 iter), loss = 1.77176
I0815 07:49:43.380712 11101 solver.cpp:334]     Train net output #0: loss = 1.73293 (* 1 = 1.73293 loss)
I0815 07:49:43.380717 11101 sgd_solver.cpp:136] Iteration 275100, lr = 0.0140312, m = 0.9
I0815 07:49:58.146108 11101 solver.cpp:312] Iteration 275200 (6.77277 iter/s, 14.765s/100 iter), loss = 1.5366
I0815 07:49:58.152016 11101 solver.cpp:334]     Train net output #0: loss = 1.31771 (* 1 = 1.31771 loss)
I0815 07:49:58.152045 11101 sgd_solver.cpp:136] Iteration 275200, lr = 0.014, m = 0.9
I0815 07:50:12.800060 11101 solver.cpp:312] Iteration 275300 (6.82428 iter/s, 14.6536s/100 iter), loss = 1.66706
I0815 07:50:12.800143 11101 solver.cpp:334]     Train net output #0: loss = 1.54468 (* 1 = 1.54468 loss)
I0815 07:50:12.800168 11101 sgd_solver.cpp:136] Iteration 275300, lr = 0.0139687, m = 0.9
I0815 07:50:27.625526 11101 solver.cpp:312] Iteration 275400 (6.74533 iter/s, 14.8251s/100 iter), loss = 1.53806
I0815 07:50:27.625555 11101 solver.cpp:334]     Train net output #0: loss = 1.48533 (* 1 = 1.48533 loss)
I0815 07:50:27.625561 11101 sgd_solver.cpp:136] Iteration 275400, lr = 0.0139375, m = 0.9
I0815 07:50:42.696097 11101 solver.cpp:312] Iteration 275500 (6.63563 iter/s, 15.0702s/100 iter), loss = 1.53986
I0815 07:50:42.696174 11101 solver.cpp:334]     Train net output #0: loss = 1.73801 (* 1 = 1.73801 loss)
I0815 07:50:42.696183 11101 sgd_solver.cpp:136] Iteration 275500, lr = 0.0139063, m = 0.9
I0815 07:50:57.676560 11101 solver.cpp:312] Iteration 275600 (6.67554 iter/s, 14.9801s/100 iter), loss = 1.81644
I0815 07:50:57.676630 11101 solver.cpp:334]     Train net output #0: loss = 2.0381 (* 1 = 2.0381 loss)
I0815 07:50:57.676648 11101 sgd_solver.cpp:136] Iteration 275600, lr = 0.013875, m = 0.9
I0815 07:51:12.451877 11101 solver.cpp:312] Iteration 275700 (6.76823 iter/s, 14.7749s/100 iter), loss = 1.7831
I0815 07:51:12.452098 11101 solver.cpp:334]     Train net output #0: loss = 1.43684 (* 1 = 1.43684 loss)
I0815 07:51:12.452155 11101 sgd_solver.cpp:136] Iteration 275700, lr = 0.0138438, m = 0.9
I0815 07:51:27.319932 11101 solver.cpp:312] Iteration 275800 (6.72601 iter/s, 14.8676s/100 iter), loss = 1.33941
I0815 07:51:27.332159 11101 solver.cpp:334]     Train net output #0: loss = 1.3048 (* 1 = 1.3048 loss)
I0815 07:51:27.332173 11101 sgd_solver.cpp:136] Iteration 275800, lr = 0.0138125, m = 0.9
I0815 07:51:41.974325 11101 solver.cpp:312] Iteration 275900 (6.82408 iter/s, 14.654s/100 iter), loss = 1.8052
I0815 07:51:41.974390 11101 solver.cpp:334]     Train net output #0: loss = 2.01643 (* 1 = 2.01643 loss)
I0815 07:51:41.974408 11101 sgd_solver.cpp:136] Iteration 275900, lr = 0.0137812, m = 0.9
I0815 07:51:56.533834 11101 solver.cpp:509] Iteration 276000, Testing net (#0)
I0815 07:52:15.835048 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 07:52:17.368188 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.530588
I0815 07:52:17.368208 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.772528
I0815 07:52:17.368216 11101 solver.cpp:594]     Test net output #2: loss = 2.09221 (* 1 = 2.09221 loss)
I0815 07:52:17.368273 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8339s
I0815 07:52:17.514345 11101 solver.cpp:312] Iteration 276000 (2.81381 iter/s, 35.5391s/100 iter), loss = 1.59104
I0815 07:52:17.514401 11101 solver.cpp:334]     Train net output #0: loss = 1.46686 (* 1 = 1.46686 loss)
I0815 07:52:17.514418 11101 sgd_solver.cpp:136] Iteration 276000, lr = 0.01375, m = 0.9
I0815 07:52:32.019518 11101 solver.cpp:312] Iteration 276100 (6.89428 iter/s, 14.5048s/100 iter), loss = 1.46227
I0815 07:52:32.019546 11101 solver.cpp:334]     Train net output #0: loss = 1.31979 (* 1 = 1.31979 loss)
I0815 07:52:32.019551 11101 sgd_solver.cpp:136] Iteration 276100, lr = 0.0137187, m = 0.9
I0815 07:52:46.929153 11101 solver.cpp:312] Iteration 276200 (6.70726 iter/s, 14.9092s/100 iter), loss = 1.79615
I0815 07:52:46.929258 11101 solver.cpp:334]     Train net output #0: loss = 1.18688 (* 1 = 1.18688 loss)
I0815 07:52:46.929275 11101 sgd_solver.cpp:136] Iteration 276200, lr = 0.0136875, m = 0.9
I0815 07:53:01.850379 11101 solver.cpp:312] Iteration 276300 (6.70205 iter/s, 14.9208s/100 iter), loss = 1.44916
I0815 07:53:01.850414 11101 solver.cpp:334]     Train net output #0: loss = 1.61223 (* 1 = 1.61223 loss)
I0815 07:53:01.850420 11101 sgd_solver.cpp:136] Iteration 276300, lr = 0.0136563, m = 0.9
I0815 07:53:16.979629 11101 solver.cpp:312] Iteration 276400 (6.60989 iter/s, 15.1288s/100 iter), loss = 2.01714
I0815 07:53:16.979717 11101 solver.cpp:334]     Train net output #0: loss = 2.10171 (* 1 = 2.10171 loss)
I0815 07:53:16.979737 11101 sgd_solver.cpp:136] Iteration 276400, lr = 0.013625, m = 0.9
I0815 07:53:31.590875 11101 solver.cpp:312] Iteration 276500 (6.84423 iter/s, 14.6108s/100 iter), loss = 1.43744
I0815 07:53:31.590904 11101 solver.cpp:334]     Train net output #0: loss = 1.52208 (* 1 = 1.52208 loss)
I0815 07:53:31.590910 11101 sgd_solver.cpp:136] Iteration 276500, lr = 0.0135938, m = 0.9
I0815 07:53:46.210460 11101 solver.cpp:312] Iteration 276600 (6.84033 iter/s, 14.6192s/100 iter), loss = 1.51724
I0815 07:53:46.210513 11101 solver.cpp:334]     Train net output #0: loss = 1.39391 (* 1 = 1.39391 loss)
I0815 07:53:46.210526 11101 sgd_solver.cpp:136] Iteration 276600, lr = 0.0135625, m = 0.9
I0815 07:54:00.943302 11101 solver.cpp:312] Iteration 276700 (6.78775 iter/s, 14.7324s/100 iter), loss = 1.33012
I0815 07:54:00.943390 11101 solver.cpp:334]     Train net output #0: loss = 1.3891 (* 1 = 1.3891 loss)
I0815 07:54:00.943404 11101 sgd_solver.cpp:136] Iteration 276700, lr = 0.0135312, m = 0.9
I0815 07:54:15.667115 11101 solver.cpp:312] Iteration 276800 (6.7919 iter/s, 14.7234s/100 iter), loss = 1.6554
I0815 07:54:15.667168 11101 solver.cpp:334]     Train net output #0: loss = 1.89591 (* 1 = 1.89591 loss)
I0815 07:54:15.667181 11101 sgd_solver.cpp:136] Iteration 276800, lr = 0.0135, m = 0.9
I0815 07:54:30.715939 11101 solver.cpp:312] Iteration 276900 (6.64522 iter/s, 15.0484s/100 iter), loss = 1.52851
I0815 07:54:30.716006 11101 solver.cpp:334]     Train net output #0: loss = 1.56483 (* 1 = 1.56483 loss)
I0815 07:54:30.716023 11101 sgd_solver.cpp:136] Iteration 276900, lr = 0.0134687, m = 0.9
I0815 07:54:45.622932 11101 solver.cpp:312] Iteration 277000 (6.70845 iter/s, 14.9066s/100 iter), loss = 1.93476
I0815 07:54:45.623183 11101 solver.cpp:334]     Train net output #0: loss = 2.28551 (* 1 = 2.28551 loss)
I0815 07:54:45.623203 11101 sgd_solver.cpp:136] Iteration 277000, lr = 0.0134375, m = 0.9
I0815 07:55:00.301753 11101 solver.cpp:312] Iteration 277100 (6.81272 iter/s, 14.6784s/100 iter), loss = 1.69175
I0815 07:55:00.301816 11101 solver.cpp:334]     Train net output #0: loss = 1.71096 (* 1 = 1.71096 loss)
I0815 07:55:00.301832 11101 sgd_solver.cpp:136] Iteration 277100, lr = 0.0134063, m = 0.9
I0815 07:55:15.940964 11101 solver.cpp:312] Iteration 277200 (6.39436 iter/s, 15.6388s/100 iter), loss = 1.68958
I0815 07:55:15.941027 11101 solver.cpp:334]     Train net output #0: loss = 1.87427 (* 1 = 1.87427 loss)
I0815 07:55:15.941033 11101 sgd_solver.cpp:136] Iteration 277200, lr = 0.013375, m = 0.9
I0815 07:55:30.746835 11101 solver.cpp:312] Iteration 277300 (6.75426 iter/s, 14.8055s/100 iter), loss = 1.62902
I0815 07:55:30.746865 11101 solver.cpp:334]     Train net output #0: loss = 1.7224 (* 1 = 1.7224 loss)
I0815 07:55:30.746872 11101 sgd_solver.cpp:136] Iteration 277300, lr = 0.0133438, m = 0.9
I0815 07:55:45.483856 11101 solver.cpp:312] Iteration 277400 (6.78582 iter/s, 14.7366s/100 iter), loss = 1.81874
I0815 07:55:45.483885 11101 solver.cpp:334]     Train net output #0: loss = 2.00966 (* 1 = 2.00966 loss)
I0815 07:55:45.483891 11101 sgd_solver.cpp:136] Iteration 277400, lr = 0.0133125, m = 0.9
I0815 07:56:00.088636 11101 solver.cpp:312] Iteration 277500 (6.84726 iter/s, 14.6044s/100 iter), loss = 1.64191
I0815 07:56:00.088749 11101 solver.cpp:334]     Train net output #0: loss = 2.03527 (* 1 = 2.03527 loss)
I0815 07:56:00.088769 11101 sgd_solver.cpp:136] Iteration 277500, lr = 0.0132813, m = 0.9
I0815 07:56:14.730891 11101 solver.cpp:312] Iteration 277600 (6.82974 iter/s, 14.6419s/100 iter), loss = 1.81762
I0815 07:56:14.730919 11101 solver.cpp:334]     Train net output #0: loss = 1.92966 (* 1 = 1.92966 loss)
I0815 07:56:14.730926 11101 sgd_solver.cpp:136] Iteration 277600, lr = 0.01325, m = 0.9
I0815 07:56:29.398576 11101 solver.cpp:312] Iteration 277700 (6.81789 iter/s, 14.6673s/100 iter), loss = 1.19409
I0815 07:56:29.398797 11101 solver.cpp:334]     Train net output #0: loss = 1.3164 (* 1 = 1.3164 loss)
I0815 07:56:29.398916 11101 sgd_solver.cpp:136] Iteration 277700, lr = 0.0132187, m = 0.9
I0815 07:56:44.406873 11101 solver.cpp:312] Iteration 277800 (6.66317 iter/s, 15.0079s/100 iter), loss = 1.48569
I0815 07:56:44.406958 11101 solver.cpp:334]     Train net output #0: loss = 1.5971 (* 1 = 1.5971 loss)
I0815 07:56:44.406971 11101 sgd_solver.cpp:136] Iteration 277800, lr = 0.0131875, m = 0.9
I0815 07:56:59.253419 11101 solver.cpp:312] Iteration 277900 (6.73576 iter/s, 14.8461s/100 iter), loss = 1.95399
I0815 07:56:59.253448 11101 solver.cpp:334]     Train net output #0: loss = 1.95752 (* 1 = 1.95752 loss)
I0815 07:56:59.253453 11101 sgd_solver.cpp:136] Iteration 277900, lr = 0.0131562, m = 0.9
I0815 07:57:13.931771 11101 solver.cpp:509] Iteration 278000, Testing net (#0)
I0815 07:57:34.893543 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.541412
I0815 07:57:34.893597 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.783116
I0815 07:57:34.893604 11101 solver.cpp:594]     Test net output #2: loss = 2.01299 (* 1 = 2.01299 loss)
I0815 07:57:34.893621 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.9613s
I0815 07:57:35.044100 11101 solver.cpp:312] Iteration 278000 (2.7941 iter/s, 35.7897s/100 iter), loss = 1.59236
I0815 07:57:35.044127 11101 solver.cpp:334]     Train net output #0: loss = 1.71993 (* 1 = 1.71993 loss)
I0815 07:57:35.044138 11101 sgd_solver.cpp:136] Iteration 278000, lr = 0.013125, m = 0.9
I0815 07:57:49.721570 11101 solver.cpp:312] Iteration 278100 (6.81335 iter/s, 14.6771s/100 iter), loss = 1.22737
I0815 07:57:49.721596 11101 solver.cpp:334]     Train net output #0: loss = 0.865203 (* 1 = 0.865203 loss)
I0815 07:57:49.721601 11101 sgd_solver.cpp:136] Iteration 278100, lr = 0.0130938, m = 0.9
I0815 07:58:04.667083 11101 solver.cpp:312] Iteration 278200 (6.69116 iter/s, 14.9451s/100 iter), loss = 1.70794
I0815 07:58:04.667109 11101 solver.cpp:334]     Train net output #0: loss = 1.72223 (* 1 = 1.72223 loss)
I0815 07:58:04.667115 11101 sgd_solver.cpp:136] Iteration 278200, lr = 0.0130625, m = 0.9
I0815 07:58:19.666319 11101 solver.cpp:312] Iteration 278300 (6.66719 iter/s, 14.9988s/100 iter), loss = 2.26493
I0815 07:58:19.666374 11101 solver.cpp:334]     Train net output #0: loss = 2.13846 (* 1 = 2.13846 loss)
I0815 07:58:19.666379 11101 sgd_solver.cpp:136] Iteration 278300, lr = 0.0130313, m = 0.9
I0815 07:58:34.238881 11101 solver.cpp:312] Iteration 278400 (6.8624 iter/s, 14.5722s/100 iter), loss = 1.44814
I0815 07:58:34.238910 11101 solver.cpp:334]     Train net output #0: loss = 1.41853 (* 1 = 1.41853 loss)
I0815 07:58:34.238916 11101 sgd_solver.cpp:136] Iteration 278400, lr = 0.013, m = 0.9
I0815 07:58:48.900625 11101 solver.cpp:312] Iteration 278500 (6.82066 iter/s, 14.6613s/100 iter), loss = 1.48574
I0815 07:58:48.900691 11101 solver.cpp:334]     Train net output #0: loss = 1.39261 (* 1 = 1.39261 loss)
I0815 07:58:48.900714 11101 sgd_solver.cpp:136] Iteration 278500, lr = 0.0129687, m = 0.9
I0815 07:59:03.767220 11101 solver.cpp:312] Iteration 278600 (6.72668 iter/s, 14.8662s/100 iter), loss = 1.25043
I0815 07:59:03.767302 11101 solver.cpp:334]     Train net output #0: loss = 1.2235 (* 1 = 1.2235 loss)
I0815 07:59:03.767312 11101 sgd_solver.cpp:136] Iteration 278600, lr = 0.0129375, m = 0.9
I0815 07:59:18.350826 11101 solver.cpp:312] Iteration 278700 (6.8572 iter/s, 14.5832s/100 iter), loss = 1.81846
I0815 07:59:18.350853 11101 solver.cpp:334]     Train net output #0: loss = 1.48532 (* 1 = 1.48532 loss)
I0815 07:59:18.350857 11101 sgd_solver.cpp:136] Iteration 278700, lr = 0.0129062, m = 0.9
I0815 07:59:33.034605 11101 solver.cpp:312] Iteration 278800 (6.81042 iter/s, 14.6834s/100 iter), loss = 1.54445
I0815 07:59:33.034634 11101 solver.cpp:334]     Train net output #0: loss = 1.50981 (* 1 = 1.50981 loss)
I0815 07:59:33.034641 11101 sgd_solver.cpp:136] Iteration 278800, lr = 0.012875, m = 0.9
I0815 07:59:47.883808 11101 solver.cpp:312] Iteration 278900 (6.73455 iter/s, 14.8488s/100 iter), loss = 1.39821
I0815 07:59:47.883893 11101 solver.cpp:334]     Train net output #0: loss = 1.3288 (* 1 = 1.3288 loss)
I0815 07:59:47.883910 11101 sgd_solver.cpp:136] Iteration 278900, lr = 0.0128438, m = 0.9
I0815 08:00:02.735828 11101 solver.cpp:312] Iteration 279000 (6.73328 iter/s, 14.8516s/100 iter), loss = 1.46503
I0815 08:00:02.735854 11101 solver.cpp:334]     Train net output #0: loss = 1.26387 (* 1 = 1.26387 loss)
I0815 08:00:02.736136 11101 sgd_solver.cpp:136] Iteration 279000, lr = 0.0128125, m = 0.9
I0815 08:00:17.415056 11101 solver.cpp:312] Iteration 279100 (6.81253 iter/s, 14.6788s/100 iter), loss = 1.7015
I0815 08:00:17.415083 11101 solver.cpp:334]     Train net output #0: loss = 1.66574 (* 1 = 1.66574 loss)
I0815 08:00:17.415089 11101 sgd_solver.cpp:136] Iteration 279100, lr = 0.0127813, m = 0.9
I0815 08:00:32.472477 11101 solver.cpp:312] Iteration 279200 (6.64143 iter/s, 15.057s/100 iter), loss = 1.48459
I0815 08:00:32.472530 11101 solver.cpp:334]     Train net output #0: loss = 1.18454 (* 1 = 1.18454 loss)
I0815 08:00:32.472537 11101 sgd_solver.cpp:136] Iteration 279200, lr = 0.01275, m = 0.9
I0815 08:00:46.899392 11101 solver.cpp:312] Iteration 279300 (6.93168 iter/s, 14.4265s/100 iter), loss = 1.79124
I0815 08:00:46.899425 11101 solver.cpp:334]     Train net output #0: loss = 1.60766 (* 1 = 1.60766 loss)
I0815 08:00:46.899430 11101 sgd_solver.cpp:136] Iteration 279300, lr = 0.0127187, m = 0.9
I0815 08:01:01.261612 11101 solver.cpp:312] Iteration 279400 (6.9629 iter/s, 14.3618s/100 iter), loss = 1.68774
I0815 08:01:01.261668 11101 solver.cpp:334]     Train net output #0: loss = 1.60685 (* 1 = 1.60685 loss)
I0815 08:01:01.261682 11101 sgd_solver.cpp:136] Iteration 279400, lr = 0.0126875, m = 0.9
I0815 08:01:15.804138 11101 solver.cpp:312] Iteration 279500 (6.87657 iter/s, 14.5421s/100 iter), loss = 1.72159
I0815 08:01:15.804215 11101 solver.cpp:334]     Train net output #0: loss = 1.86513 (* 1 = 1.86513 loss)
I0815 08:01:15.804229 11101 sgd_solver.cpp:136] Iteration 279500, lr = 0.0126562, m = 0.9
I0815 08:01:30.518889 11101 solver.cpp:312] Iteration 279600 (6.79609 iter/s, 14.7143s/100 iter), loss = 1.54036
I0815 08:01:30.518949 11101 solver.cpp:334]     Train net output #0: loss = 1.59837 (* 1 = 1.59837 loss)
I0815 08:01:30.518965 11101 sgd_solver.cpp:136] Iteration 279600, lr = 0.012625, m = 0.9
I0815 08:01:45.177922 11101 solver.cpp:312] Iteration 279700 (6.82192 iter/s, 14.6586s/100 iter), loss = 1.86301
I0815 08:01:45.177986 11101 solver.cpp:334]     Train net output #0: loss = 1.89981 (* 1 = 1.89981 loss)
I0815 08:01:45.178004 11101 sgd_solver.cpp:136] Iteration 279700, lr = 0.0125938, m = 0.9
I0815 08:01:59.878298 11101 solver.cpp:312] Iteration 279800 (6.80274 iter/s, 14.7s/100 iter), loss = 1.51667
I0815 08:01:59.878361 11101 solver.cpp:334]     Train net output #0: loss = 1.19732 (* 1 = 1.19732 loss)
I0815 08:01:59.878366 11101 sgd_solver.cpp:136] Iteration 279800, lr = 0.0125625, m = 0.9
I0815 08:02:14.660581 11101 solver.cpp:312] Iteration 279900 (6.76504 iter/s, 14.7819s/100 iter), loss = 1.30078
I0815 08:02:14.660609 11101 solver.cpp:334]     Train net output #0: loss = 1.34947 (* 1 = 1.34947 loss)
I0815 08:02:14.660614 11101 sgd_solver.cpp:136] Iteration 279900, lr = 0.0125313, m = 0.9
I0815 08:02:29.226450 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_280000.caffemodel
I0815 08:02:29.282922 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_280000.solverstate
I0815 08:02:29.287426 11101 solver.cpp:509] Iteration 280000, Testing net (#0)
I0815 08:02:50.424824 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.546941
I0815 08:02:50.424953 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.787645
I0815 08:02:50.424962 11101 solver.cpp:594]     Test net output #2: loss = 2.00062 (* 1 = 2.00062 loss)
I0815 08:02:50.424980 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.137s
I0815 08:02:50.576633 11101 solver.cpp:312] Iteration 280000 (2.78435 iter/s, 35.9151s/100 iter), loss = 1.34444
I0815 08:02:50.576663 11101 solver.cpp:334]     Train net output #0: loss = 1.80192 (* 1 = 1.80192 loss)
I0815 08:02:50.576668 11101 sgd_solver.cpp:136] Iteration 280000, lr = 0.0125, m = 0.9
I0815 08:03:05.001649 11101 solver.cpp:312] Iteration 280100 (6.93259 iter/s, 14.4246s/100 iter), loss = 1.59979
I0815 08:03:05.001679 11101 solver.cpp:334]     Train net output #0: loss = 1.31981 (* 1 = 1.31981 loss)
I0815 08:03:05.001683 11101 sgd_solver.cpp:136] Iteration 280100, lr = 0.0124687, m = 0.9
I0815 08:03:19.484438 11101 solver.cpp:312] Iteration 280200 (6.90494 iter/s, 14.4824s/100 iter), loss = 1.65026
I0815 08:03:19.484503 11101 solver.cpp:334]     Train net output #0: loss = 1.75516 (* 1 = 1.75516 loss)
I0815 08:03:19.484521 11101 sgd_solver.cpp:136] Iteration 280200, lr = 0.0124375, m = 0.9
I0815 08:03:33.927692 11101 solver.cpp:312] Iteration 280300 (6.92384 iter/s, 14.4429s/100 iter), loss = 1.3074
I0815 08:03:33.934027 11101 solver.cpp:334]     Train net output #0: loss = 1.72661 (* 1 = 1.72661 loss)
I0815 08:03:33.934046 11101 sgd_solver.cpp:136] Iteration 280300, lr = 0.0124062, m = 0.9
I0815 08:03:48.629849 11101 solver.cpp:312] Iteration 280400 (6.80191 iter/s, 14.7017s/100 iter), loss = 1.3121
I0815 08:03:48.629876 11101 solver.cpp:334]     Train net output #0: loss = 1.52601 (* 1 = 1.52601 loss)
I0815 08:03:48.629881 11101 sgd_solver.cpp:136] Iteration 280400, lr = 0.012375, m = 0.9
I0815 08:04:03.709369 11101 solver.cpp:312] Iteration 280500 (6.63169 iter/s, 15.0791s/100 iter), loss = 1.50722
I0815 08:04:03.709396 11101 solver.cpp:334]     Train net output #0: loss = 1.78791 (* 1 = 1.78791 loss)
I0815 08:04:03.709403 11101 sgd_solver.cpp:136] Iteration 280500, lr = 0.0123438, m = 0.9
I0815 08:04:18.743508 11101 solver.cpp:312] Iteration 280600 (6.65171 iter/s, 15.0337s/100 iter), loss = 1.64223
I0815 08:04:18.743579 11101 solver.cpp:334]     Train net output #0: loss = 1.55666 (* 1 = 1.55666 loss)
I0815 08:04:18.743587 11101 sgd_solver.cpp:136] Iteration 280600, lr = 0.0123125, m = 0.9
I0815 08:04:33.939218 11101 solver.cpp:312] Iteration 280700 (6.58099 iter/s, 15.1953s/100 iter), loss = 1.74425
I0815 08:04:33.939286 11101 solver.cpp:334]     Train net output #0: loss = 1.74375 (* 1 = 1.74375 loss)
I0815 08:04:33.939304 11101 sgd_solver.cpp:136] Iteration 280700, lr = 0.0122813, m = 0.9
I0815 08:04:48.667163 11101 solver.cpp:312] Iteration 280800 (6.79 iter/s, 14.7275s/100 iter), loss = 1.55846
I0815 08:04:48.667227 11101 solver.cpp:334]     Train net output #0: loss = 1.52837 (* 1 = 1.52837 loss)
I0815 08:04:48.667244 11101 sgd_solver.cpp:136] Iteration 280800, lr = 0.01225, m = 0.9
I0815 08:05:03.074162 11101 solver.cpp:312] Iteration 280900 (6.94126 iter/s, 14.4066s/100 iter), loss = 1.47279
I0815 08:05:03.074264 11101 solver.cpp:334]     Train net output #0: loss = 1.33529 (* 1 = 1.33529 loss)
I0815 08:05:03.074287 11101 sgd_solver.cpp:136] Iteration 280900, lr = 0.0122188, m = 0.9
I0815 08:05:17.923715 11101 solver.cpp:312] Iteration 281000 (6.7344 iter/s, 14.8491s/100 iter), loss = 1.5232
I0815 08:05:17.923743 11101 solver.cpp:334]     Train net output #0: loss = 1.56202 (* 1 = 1.56202 loss)
I0815 08:05:17.923750 11101 sgd_solver.cpp:136] Iteration 281000, lr = 0.0121875, m = 0.9
I0815 08:05:32.471164 11101 solver.cpp:312] Iteration 281100 (6.87425 iter/s, 14.547s/100 iter), loss = 1.33695
I0815 08:05:32.471192 11101 solver.cpp:334]     Train net output #0: loss = 1.40539 (* 1 = 1.40539 loss)
I0815 08:05:32.471199 11101 sgd_solver.cpp:136] Iteration 281100, lr = 0.0121562, m = 0.9
I0815 08:05:47.014521 11101 solver.cpp:312] Iteration 281200 (6.87618 iter/s, 14.543s/100 iter), loss = 1.46723
I0815 08:05:47.014601 11101 solver.cpp:334]     Train net output #0: loss = 1.4494 (* 1 = 1.4494 loss)
I0815 08:05:47.014607 11101 sgd_solver.cpp:136] Iteration 281200, lr = 0.012125, m = 0.9
I0815 08:06:01.632989 11101 solver.cpp:312] Iteration 281300 (6.84085 iter/s, 14.6181s/100 iter), loss = 1.74682
I0815 08:06:01.633015 11101 solver.cpp:334]     Train net output #0: loss = 1.72937 (* 1 = 1.72937 loss)
I0815 08:06:01.633020 11101 sgd_solver.cpp:136] Iteration 281300, lr = 0.0120938, m = 0.9
I0815 08:06:16.408162 11101 solver.cpp:312] Iteration 281400 (6.7683 iter/s, 14.7748s/100 iter), loss = 1.57329
I0815 08:06:16.408226 11101 solver.cpp:334]     Train net output #0: loss = 1.45963 (* 1 = 1.45963 loss)
I0815 08:06:16.408244 11101 sgd_solver.cpp:136] Iteration 281400, lr = 0.0120625, m = 0.9
I0815 08:06:30.872789 11101 solver.cpp:312] Iteration 281500 (6.91361 iter/s, 14.4642s/100 iter), loss = 1.47412
I0815 08:06:30.873044 11101 solver.cpp:334]     Train net output #0: loss = 1.2972 (* 1 = 1.2972 loss)
I0815 08:06:30.873152 11101 sgd_solver.cpp:136] Iteration 281500, lr = 0.0120313, m = 0.9
I0815 08:06:45.642973 11101 solver.cpp:312] Iteration 281600 (6.77058 iter/s, 14.7698s/100 iter), loss = 1.52036
I0815 08:06:45.643038 11101 solver.cpp:334]     Train net output #0: loss = 1.51327 (* 1 = 1.51327 loss)
I0815 08:06:45.643054 11101 sgd_solver.cpp:136] Iteration 281600, lr = 0.012, m = 0.9
I0815 08:07:00.332362 11101 solver.cpp:312] Iteration 281700 (6.80782 iter/s, 14.689s/100 iter), loss = 1.42701
I0815 08:07:00.332406 11101 solver.cpp:334]     Train net output #0: loss = 1.63408 (* 1 = 1.63408 loss)
I0815 08:07:00.332415 11101 sgd_solver.cpp:136] Iteration 281700, lr = 0.0119687, m = 0.9
I0815 08:07:15.058183 11101 solver.cpp:312] Iteration 281800 (6.79098 iter/s, 14.7254s/100 iter), loss = 1.36824
I0815 08:07:15.058279 11101 solver.cpp:334]     Train net output #0: loss = 1.44905 (* 1 = 1.44905 loss)
I0815 08:07:15.058300 11101 sgd_solver.cpp:136] Iteration 281800, lr = 0.0119375, m = 0.9
I0815 08:07:29.743808 11101 solver.cpp:312] Iteration 281900 (6.80957 iter/s, 14.6852s/100 iter), loss = 1.44723
I0815 08:07:29.743834 11101 solver.cpp:334]     Train net output #0: loss = 1.05957 (* 1 = 1.05957 loss)
I0815 08:07:29.743839 11101 sgd_solver.cpp:136] Iteration 281900, lr = 0.0119062, m = 0.9
I0815 08:07:44.568157 11101 solver.cpp:509] Iteration 282000, Testing net (#0)
I0815 08:08:00.343964 11103 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 08:08:06.269649 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.525765
I0815 08:08:06.269670 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.770469
I0815 08:08:06.269677 11101 solver.cpp:594]     Test net output #2: loss = 2.09194 (* 1 = 2.09194 loss)
I0815 08:08:06.269708 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.701s
I0815 08:08:06.440160 11101 solver.cpp:312] Iteration 282000 (2.72514 iter/s, 36.6954s/100 iter), loss = 1.24113
I0815 08:08:06.440207 11101 solver.cpp:334]     Train net output #0: loss = 1.20365 (* 1 = 1.20365 loss)
I0815 08:08:06.440218 11101 sgd_solver.cpp:136] Iteration 282000, lr = 0.011875, m = 0.9
I0815 08:08:21.096313 11101 solver.cpp:312] Iteration 282100 (6.82326 iter/s, 14.6557s/100 iter), loss = 1.1742
I0815 08:08:21.096346 11101 solver.cpp:334]     Train net output #0: loss = 1.28218 (* 1 = 1.28218 loss)
I0815 08:08:21.096351 11101 sgd_solver.cpp:136] Iteration 282100, lr = 0.0118438, m = 0.9
I0815 08:08:35.541239 11101 solver.cpp:312] Iteration 282200 (6.92304 iter/s, 14.4445s/100 iter), loss = 2.01821
I0815 08:08:35.541301 11101 solver.cpp:334]     Train net output #0: loss = 1.8821 (* 1 = 1.8821 loss)
I0815 08:08:35.541307 11101 sgd_solver.cpp:136] Iteration 282200, lr = 0.0118125, m = 0.9
I0815 08:08:50.030354 11101 solver.cpp:312] Iteration 282300 (6.90192 iter/s, 14.4887s/100 iter), loss = 1.4327
I0815 08:08:50.030380 11101 solver.cpp:334]     Train net output #0: loss = 1.5625 (* 1 = 1.5625 loss)
I0815 08:08:50.030386 11101 sgd_solver.cpp:136] Iteration 282300, lr = 0.0117813, m = 0.9
I0815 08:09:04.830070 11101 solver.cpp:312] Iteration 282400 (6.75707 iter/s, 14.7993s/100 iter), loss = 1.61719
I0815 08:09:04.830097 11101 solver.cpp:334]     Train net output #0: loss = 1.54997 (* 1 = 1.54997 loss)
I0815 08:09:04.830104 11101 sgd_solver.cpp:136] Iteration 282400, lr = 0.01175, m = 0.9
I0815 08:09:19.606241 11101 solver.cpp:312] Iteration 282500 (6.76784 iter/s, 14.7758s/100 iter), loss = 1.74106
I0815 08:09:19.606294 11101 solver.cpp:334]     Train net output #0: loss = 1.93038 (* 1 = 1.93038 loss)
I0815 08:09:19.606302 11101 sgd_solver.cpp:136] Iteration 282500, lr = 0.0117188, m = 0.9
I0815 08:09:34.161062 11101 solver.cpp:312] Iteration 282600 (6.87077 iter/s, 14.5544s/100 iter), loss = 1.36734
I0815 08:09:34.161090 11101 solver.cpp:334]     Train net output #0: loss = 1.58615 (* 1 = 1.58615 loss)
I0815 08:09:34.161094 11101 sgd_solver.cpp:136] Iteration 282600, lr = 0.0116875, m = 0.9
I0815 08:09:48.704951 11101 solver.cpp:312] Iteration 282700 (6.87593 iter/s, 14.5435s/100 iter), loss = 1.81724
I0815 08:09:48.705023 11101 solver.cpp:334]     Train net output #0: loss = 2.02351 (* 1 = 2.02351 loss)
I0815 08:09:48.705042 11101 sgd_solver.cpp:136] Iteration 282700, lr = 0.0116562, m = 0.9
I0815 08:10:03.098111 11101 solver.cpp:312] Iteration 282800 (6.94793 iter/s, 14.3928s/100 iter), loss = 1.23485
I0815 08:10:03.098176 11101 solver.cpp:334]     Train net output #0: loss = 1.45478 (* 1 = 1.45478 loss)
I0815 08:10:03.098187 11101 sgd_solver.cpp:136] Iteration 282800, lr = 0.011625, m = 0.9
I0815 08:10:17.506217 11101 solver.cpp:312] Iteration 282900 (6.94073 iter/s, 14.4077s/100 iter), loss = 1.66725
I0815 08:10:17.506281 11101 solver.cpp:334]     Train net output #0: loss = 1.3784 (* 1 = 1.3784 loss)
I0815 08:10:17.506299 11101 sgd_solver.cpp:136] Iteration 282900, lr = 0.0115937, m = 0.9
I0815 08:10:32.209786 11101 solver.cpp:312] Iteration 283000 (6.80126 iter/s, 14.7032s/100 iter), loss = 1.45014
I0815 08:10:32.209856 11101 solver.cpp:334]     Train net output #0: loss = 1.23588 (* 1 = 1.23588 loss)
I0815 08:10:32.209875 11101 sgd_solver.cpp:136] Iteration 283000, lr = 0.0115625, m = 0.9
I0815 08:10:46.950922 11101 solver.cpp:312] Iteration 283100 (6.78393 iter/s, 14.7407s/100 iter), loss = 1.25333
I0815 08:10:46.951026 11101 solver.cpp:334]     Train net output #0: loss = 1.13475 (* 1 = 1.13475 loss)
I0815 08:10:46.951045 11101 sgd_solver.cpp:136] Iteration 283100, lr = 0.0115313, m = 0.9
I0815 08:11:01.788316 11101 solver.cpp:312] Iteration 283200 (6.73991 iter/s, 14.837s/100 iter), loss = 1.32746
I0815 08:11:01.788347 11101 solver.cpp:334]     Train net output #0: loss = 1.45338 (* 1 = 1.45338 loss)
I0815 08:11:01.788353 11101 sgd_solver.cpp:136] Iteration 283200, lr = 0.0115, m = 0.9
I0815 08:11:16.371037 11101 solver.cpp:312] Iteration 283300 (6.85762 iter/s, 14.5823s/100 iter), loss = 1.65953
I0815 08:11:16.371064 11101 solver.cpp:334]     Train net output #0: loss = 1.49394 (* 1 = 1.49394 loss)
I0815 08:11:16.371070 11101 sgd_solver.cpp:136] Iteration 283300, lr = 0.0114688, m = 0.9
I0815 08:11:31.246752 11101 solver.cpp:312] Iteration 283400 (6.72255 iter/s, 14.8753s/100 iter), loss = 1.64639
I0815 08:11:31.246812 11101 solver.cpp:334]     Train net output #0: loss = 1.85224 (* 1 = 1.85224 loss)
I0815 08:11:31.246819 11101 sgd_solver.cpp:136] Iteration 283400, lr = 0.0114375, m = 0.9
I0815 08:11:45.976404 11101 solver.cpp:312] Iteration 283500 (6.78921 iter/s, 14.7292s/100 iter), loss = 1.46821
I0815 08:11:45.976481 11101 solver.cpp:334]     Train net output #0: loss = 1.46829 (* 1 = 1.46829 loss)
I0815 08:11:45.976502 11101 sgd_solver.cpp:136] Iteration 283500, lr = 0.0114062, m = 0.9
I0815 08:12:00.358136 11101 solver.cpp:312] Iteration 283600 (6.95346 iter/s, 14.3813s/100 iter), loss = 1.30261
I0815 08:12:00.358166 11101 solver.cpp:334]     Train net output #0: loss = 1.2396 (* 1 = 1.2396 loss)
I0815 08:12:00.358173 11101 sgd_solver.cpp:136] Iteration 283600, lr = 0.011375, m = 0.9
I0815 08:12:14.990479 11101 solver.cpp:312] Iteration 283700 (6.83436 iter/s, 14.6319s/100 iter), loss = 1.35302
I0815 08:12:14.990547 11101 solver.cpp:334]     Train net output #0: loss = 1.19879 (* 1 = 1.19879 loss)
I0815 08:12:14.990556 11101 sgd_solver.cpp:136] Iteration 283700, lr = 0.0113437, m = 0.9
I0815 08:12:29.920892 11101 solver.cpp:312] Iteration 283800 (6.69792 iter/s, 14.93s/100 iter), loss = 1.4128
I0815 08:12:29.920917 11101 solver.cpp:334]     Train net output #0: loss = 1.42478 (* 1 = 1.42478 loss)
I0815 08:12:29.920922 11101 sgd_solver.cpp:136] Iteration 283800, lr = 0.0113125, m = 0.9
I0815 08:12:44.524916 11101 solver.cpp:312] Iteration 283900 (6.84762 iter/s, 14.6036s/100 iter), loss = 1.47946
I0815 08:12:44.524943 11101 solver.cpp:334]     Train net output #0: loss = 1.21941 (* 1 = 1.21941 loss)
I0815 08:12:44.524948 11101 sgd_solver.cpp:136] Iteration 283900, lr = 0.0112813, m = 0.9
I0815 08:12:58.892988 11101 solver.cpp:509] Iteration 284000, Testing net (#0)
I0815 08:13:02.380390 11099 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 08:13:19.607452 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.551588
I0815 08:13:19.607475 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790233
I0815 08:13:19.607486 11101 solver.cpp:594]     Test net output #2: loss = 1.95689 (* 1 = 1.95689 loss)
I0815 08:13:19.607509 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.714s
I0815 08:13:19.780995 11101 solver.cpp:312] Iteration 284000 (2.83647 iter/s, 35.2551s/100 iter), loss = 1.35618
I0815 08:13:19.781021 11101 solver.cpp:334]     Train net output #0: loss = 0.986408 (* 1 = 0.986408 loss)
I0815 08:13:19.781028 11101 sgd_solver.cpp:136] Iteration 284000, lr = 0.01125, m = 0.9
I0815 08:13:34.316668 11101 solver.cpp:312] Iteration 284100 (6.87982 iter/s, 14.5353s/100 iter), loss = 1.35101
I0815 08:13:34.316721 11101 solver.cpp:334]     Train net output #0: loss = 1.01737 (* 1 = 1.01737 loss)
I0815 08:13:34.316728 11101 sgd_solver.cpp:136] Iteration 284100, lr = 0.0112188, m = 0.9
I0815 08:13:49.109133 11101 solver.cpp:312] Iteration 284200 (6.76039 iter/s, 14.7921s/100 iter), loss = 1.53173
I0815 08:13:49.109352 11101 solver.cpp:334]     Train net output #0: loss = 1.31681 (* 1 = 1.31681 loss)
I0815 08:13:49.109459 11101 sgd_solver.cpp:136] Iteration 284200, lr = 0.0111875, m = 0.9
I0815 08:14:03.727313 11101 solver.cpp:312] Iteration 284300 (6.84098 iter/s, 14.6178s/100 iter), loss = 1.4505
I0815 08:14:03.727378 11101 solver.cpp:334]     Train net output #0: loss = 1.12708 (* 1 = 1.12708 loss)
I0815 08:14:03.727396 11101 sgd_solver.cpp:136] Iteration 284300, lr = 0.0111562, m = 0.9
I0815 08:14:18.237990 11101 solver.cpp:312] Iteration 284400 (6.89167 iter/s, 14.5103s/100 iter), loss = 1.8006
I0815 08:14:18.238083 11101 solver.cpp:334]     Train net output #0: loss = 1.6951 (* 1 = 1.6951 loss)
I0815 08:14:18.238101 11101 sgd_solver.cpp:136] Iteration 284400, lr = 0.011125, m = 0.9
I0815 08:14:32.636454 11101 solver.cpp:312] Iteration 284500 (6.94538 iter/s, 14.3981s/100 iter), loss = 1.65856
I0815 08:14:32.636476 11101 solver.cpp:334]     Train net output #0: loss = 2.32291 (* 1 = 2.32291 loss)
I0815 08:14:32.636481 11101 sgd_solver.cpp:136] Iteration 284500, lr = 0.0110937, m = 0.9
I0815 08:14:46.997341 11101 solver.cpp:312] Iteration 284600 (6.96355 iter/s, 14.3605s/100 iter), loss = 1.32238
I0815 08:14:46.997370 11101 solver.cpp:334]     Train net output #0: loss = 1.51917 (* 1 = 1.51917 loss)
I0815 08:14:46.997376 11101 sgd_solver.cpp:136] Iteration 284600, lr = 0.0110625, m = 0.9
I0815 08:15:01.758993 11101 solver.cpp:312] Iteration 284700 (6.7745 iter/s, 14.7612s/100 iter), loss = 1.18419
I0815 08:15:01.759066 11101 solver.cpp:334]     Train net output #0: loss = 1.1502 (* 1 = 1.1502 loss)
I0815 08:15:01.759071 11101 sgd_solver.cpp:136] Iteration 284700, lr = 0.0110313, m = 0.9
I0815 08:15:16.477767 11101 solver.cpp:312] Iteration 284800 (6.79423 iter/s, 14.7184s/100 iter), loss = 1.46885
I0815 08:15:16.477794 11101 solver.cpp:334]     Train net output #0: loss = 1.61784 (* 1 = 1.61784 loss)
I0815 08:15:16.477800 11101 sgd_solver.cpp:136] Iteration 284800, lr = 0.011, m = 0.9
I0815 08:15:30.928449 11101 solver.cpp:312] Iteration 284900 (6.92028 iter/s, 14.4503s/100 iter), loss = 1.75403
I0815 08:15:30.928503 11101 solver.cpp:334]     Train net output #0: loss = 2.01908 (* 1 = 2.01908 loss)
I0815 08:15:30.928516 11101 sgd_solver.cpp:136] Iteration 284900, lr = 0.0109688, m = 0.9
I0815 08:15:45.600558 11101 solver.cpp:312] Iteration 285000 (6.81584 iter/s, 14.6717s/100 iter), loss = 1.80166
I0815 08:15:45.600641 11101 solver.cpp:334]     Train net output #0: loss = 1.91329 (* 1 = 1.91329 loss)
I0815 08:15:45.600656 11101 sgd_solver.cpp:136] Iteration 285000, lr = 0.0109375, m = 0.9
I0815 08:16:00.124579 11101 solver.cpp:312] Iteration 285100 (6.88534 iter/s, 14.5236s/100 iter), loss = 1.54829
I0815 08:16:00.124608 11101 solver.cpp:334]     Train net output #0: loss = 1.61961 (* 1 = 1.61961 loss)
I0815 08:16:00.124614 11101 sgd_solver.cpp:136] Iteration 285100, lr = 0.0109062, m = 0.9
I0815 08:16:15.044809 11101 solver.cpp:312] Iteration 285200 (6.70249 iter/s, 14.9198s/100 iter), loss = 1.55579
I0815 08:16:15.044834 11101 solver.cpp:334]     Train net output #0: loss = 1.54911 (* 1 = 1.54911 loss)
I0815 08:16:15.044841 11101 sgd_solver.cpp:136] Iteration 285200, lr = 0.010875, m = 0.9
I0815 08:16:29.795188 11101 solver.cpp:312] Iteration 285300 (6.77967 iter/s, 14.75s/100 iter), loss = 1.32119
I0815 08:16:29.795248 11101 solver.cpp:334]     Train net output #0: loss = 1.22831 (* 1 = 1.22831 loss)
I0815 08:16:29.795254 11101 sgd_solver.cpp:136] Iteration 285300, lr = 0.0108437, m = 0.9
I0815 08:16:44.992774 11101 solver.cpp:312] Iteration 285400 (6.58017 iter/s, 15.1972s/100 iter), loss = 1.4088
I0815 08:16:44.992810 11101 solver.cpp:334]     Train net output #0: loss = 1.67918 (* 1 = 1.67918 loss)
I0815 08:16:44.992817 11101 sgd_solver.cpp:136] Iteration 285400, lr = 0.0108125, m = 0.9
I0815 08:16:59.504392 11101 solver.cpp:312] Iteration 285500 (6.89122 iter/s, 14.5112s/100 iter), loss = 1.43041
I0815 08:16:59.504457 11101 solver.cpp:334]     Train net output #0: loss = 1.53113 (* 1 = 1.53113 loss)
I0815 08:16:59.504474 11101 sgd_solver.cpp:136] Iteration 285500, lr = 0.0107813, m = 0.9
I0815 08:17:13.869457 11101 solver.cpp:312] Iteration 285600 (6.96153 iter/s, 14.3647s/100 iter), loss = 1.87404
I0815 08:17:13.869518 11101 solver.cpp:334]     Train net output #0: loss = 1.55817 (* 1 = 1.55817 loss)
I0815 08:17:13.869524 11101 sgd_solver.cpp:136] Iteration 285600, lr = 0.01075, m = 0.9
I0815 08:17:28.594519 11101 solver.cpp:312] Iteration 285700 (6.79133 iter/s, 14.7247s/100 iter), loss = 1.65613
I0815 08:17:28.594549 11101 solver.cpp:334]     Train net output #0: loss = 1.68449 (* 1 = 1.68449 loss)
I0815 08:17:28.594555 11101 sgd_solver.cpp:136] Iteration 285700, lr = 0.0107188, m = 0.9
I0815 08:17:42.924578 11101 solver.cpp:312] Iteration 285800 (6.97853 iter/s, 14.3297s/100 iter), loss = 1.30145
I0815 08:17:42.924607 11101 solver.cpp:334]     Train net output #0: loss = 1.3739 (* 1 = 1.3739 loss)
I0815 08:17:42.924613 11101 sgd_solver.cpp:136] Iteration 285800, lr = 0.0106875, m = 0.9
I0815 08:17:57.634399 11101 solver.cpp:312] Iteration 285900 (6.79837 iter/s, 14.7094s/100 iter), loss = 1.38283
I0815 08:17:57.634455 11101 solver.cpp:334]     Train net output #0: loss = 1.16456 (* 1 = 1.16456 loss)
I0815 08:17:57.634461 11101 sgd_solver.cpp:136] Iteration 285900, lr = 0.0106562, m = 0.9
I0815 08:18:12.558652 11101 solver.cpp:509] Iteration 286000, Testing net (#0)
I0815 08:18:33.393862 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.557706
I0815 08:18:33.393932 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.79588
I0815 08:18:33.393939 11101 solver.cpp:594]     Test net output #2: loss = 1.94363 (* 1 = 1.94363 loss)
I0815 08:18:33.393959 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.8347s
I0815 08:18:33.568205 11101 solver.cpp:312] Iteration 286000 (2.78297 iter/s, 35.9328s/100 iter), loss = 1.36651
I0815 08:18:33.568231 11101 solver.cpp:334]     Train net output #0: loss = 1.09381 (* 1 = 1.09381 loss)
I0815 08:18:33.568238 11101 sgd_solver.cpp:136] Iteration 286000, lr = 0.010625, m = 0.9
I0815 08:18:47.902104 11101 solver.cpp:312] Iteration 286100 (6.97666 iter/s, 14.3335s/100 iter), loss = 1.04913
I0815 08:18:47.902132 11101 solver.cpp:334]     Train net output #0: loss = 0.922948 (* 1 = 0.922948 loss)
I0815 08:18:47.902135 11101 sgd_solver.cpp:136] Iteration 286100, lr = 0.0105937, m = 0.9
I0815 08:19:02.558279 11101 solver.cpp:312] Iteration 286200 (6.82325 iter/s, 14.6558s/100 iter), loss = 1.43306
I0815 08:19:02.558305 11101 solver.cpp:334]     Train net output #0: loss = 1.58313 (* 1 = 1.58313 loss)
I0815 08:19:02.558311 11101 sgd_solver.cpp:136] Iteration 286200, lr = 0.0105625, m = 0.9
I0815 08:19:17.249017 11101 solver.cpp:312] Iteration 286300 (6.8072 iter/s, 14.6903s/100 iter), loss = 1.44713
I0815 08:19:17.249110 11101 solver.cpp:334]     Train net output #0: loss = 1.68391 (* 1 = 1.68391 loss)
I0815 08:19:17.249128 11101 sgd_solver.cpp:136] Iteration 286300, lr = 0.0105313, m = 0.9
I0815 08:19:31.976924 11101 solver.cpp:312] Iteration 286400 (6.79002 iter/s, 14.7275s/100 iter), loss = 1.36344
I0815 08:19:31.976953 11101 solver.cpp:334]     Train net output #0: loss = 1.18256 (* 1 = 1.18256 loss)
I0815 08:19:31.976958 11101 sgd_solver.cpp:136] Iteration 286400, lr = 0.0105, m = 0.9
I0815 08:19:46.226411 11101 solver.cpp:312] Iteration 286500 (7.01799 iter/s, 14.2491s/100 iter), loss = 1.58031
I0815 08:19:46.226470 11101 solver.cpp:334]     Train net output #0: loss = 1.46661 (* 1 = 1.46661 loss)
I0815 08:19:46.226490 11101 sgd_solver.cpp:136] Iteration 286500, lr = 0.0104688, m = 0.9
I0815 08:20:00.855976 11101 solver.cpp:312] Iteration 286600 (6.83566 iter/s, 14.6292s/100 iter), loss = 1.21641
I0815 08:20:00.856078 11101 solver.cpp:334]     Train net output #0: loss = 1.41484 (* 1 = 1.41484 loss)
I0815 08:20:00.856097 11101 sgd_solver.cpp:136] Iteration 286600, lr = 0.0104375, m = 0.9
I0815 08:20:15.740468 11101 solver.cpp:312] Iteration 286700 (6.71859 iter/s, 14.8841s/100 iter), loss = 1.45663
I0815 08:20:15.740492 11101 solver.cpp:334]     Train net output #0: loss = 1.52154 (* 1 = 1.52154 loss)
I0815 08:20:15.740499 11101 sgd_solver.cpp:136] Iteration 286700, lr = 0.0104063, m = 0.9
I0815 08:20:31.026325 11101 solver.cpp:312] Iteration 286800 (6.54217 iter/s, 15.2854s/100 iter), loss = 1.76101
I0815 08:20:31.026378 11101 solver.cpp:334]     Train net output #0: loss = 1.8324 (* 1 = 1.8324 loss)
I0815 08:20:31.026384 11101 sgd_solver.cpp:136] Iteration 286800, lr = 0.010375, m = 0.9
I0815 08:20:46.231837 11101 solver.cpp:312] Iteration 286900 (6.57674 iter/s, 15.2051s/100 iter), loss = 1.54203
I0815 08:20:46.231905 11101 solver.cpp:334]     Train net output #0: loss = 1.65488 (* 1 = 1.65488 loss)
I0815 08:20:46.231925 11101 sgd_solver.cpp:136] Iteration 286900, lr = 0.0103437, m = 0.9
I0815 08:21:00.639832 11101 solver.cpp:312] Iteration 287000 (6.94078 iter/s, 14.4076s/100 iter), loss = 1.91973
I0815 08:21:00.639858 11101 solver.cpp:334]     Train net output #0: loss = 1.64855 (* 1 = 1.64855 loss)
I0815 08:21:00.639863 11101 sgd_solver.cpp:136] Iteration 287000, lr = 0.0103125, m = 0.9
I0815 08:21:15.059003 11101 solver.cpp:312] Iteration 287100 (6.9354 iter/s, 14.4188s/100 iter), loss = 1.25242
I0815 08:21:15.059100 11101 solver.cpp:334]     Train net output #0: loss = 1.41485 (* 1 = 1.41485 loss)
I0815 08:21:15.059119 11101 sgd_solver.cpp:136] Iteration 287100, lr = 0.0102813, m = 0.9
I0815 08:21:29.684257 11101 solver.cpp:312] Iteration 287200 (6.83768 iter/s, 14.6249s/100 iter), loss = 0.975545
I0815 08:21:29.684288 11101 solver.cpp:334]     Train net output #0: loss = 1.07177 (* 1 = 1.07177 loss)
I0815 08:21:29.684294 11101 sgd_solver.cpp:136] Iteration 287200, lr = 0.01025, m = 0.9
I0815 08:21:44.175977 11101 solver.cpp:312] Iteration 287300 (6.90068 iter/s, 14.4913s/100 iter), loss = 1.79487
I0815 08:21:44.176164 11101 solver.cpp:334]     Train net output #0: loss = 1.85932 (* 1 = 1.85932 loss)
I0815 08:21:44.176251 11101 sgd_solver.cpp:136] Iteration 287300, lr = 0.0102188, m = 0.9
I0815 08:21:59.178267 11101 solver.cpp:312] Iteration 287400 (6.66583 iter/s, 15.0019s/100 iter), loss = 1.97977
I0815 08:21:59.180176 11101 solver.cpp:334]     Train net output #0: loss = 1.90111 (* 1 = 1.90111 loss)
I0815 08:21:59.180311 11101 sgd_solver.cpp:136] Iteration 287400, lr = 0.0101875, m = 0.9
I0815 08:22:13.673323 11101 solver.cpp:312] Iteration 287500 (6.89909 iter/s, 14.4947s/100 iter), loss = 1.5997
I0815 08:22:13.673351 11101 solver.cpp:334]     Train net output #0: loss = 1.40319 (* 1 = 1.40319 loss)
I0815 08:22:13.673357 11101 sgd_solver.cpp:136] Iteration 287500, lr = 0.0101563, m = 0.9
I0815 08:22:28.312121 11101 solver.cpp:312] Iteration 287600 (6.83135 iter/s, 14.6384s/100 iter), loss = 1.81979
I0815 08:22:28.312162 11101 solver.cpp:334]     Train net output #0: loss = 2.32571 (* 1 = 2.32571 loss)
I0815 08:22:28.312168 11101 sgd_solver.cpp:136] Iteration 287600, lr = 0.010125, m = 0.9
I0815 08:22:42.857318 11101 solver.cpp:312] Iteration 287700 (6.87531 iter/s, 14.5448s/100 iter), loss = 1.60786
I0815 08:22:42.857421 11101 solver.cpp:334]     Train net output #0: loss = 1.63827 (* 1 = 1.63827 loss)
I0815 08:22:42.857440 11101 sgd_solver.cpp:136] Iteration 287700, lr = 0.0100937, m = 0.9
I0815 08:22:57.407618 11101 solver.cpp:312] Iteration 287800 (6.8729 iter/s, 14.5499s/100 iter), loss = 1.75614
I0815 08:22:57.407680 11101 solver.cpp:334]     Train net output #0: loss = 1.91887 (* 1 = 1.91887 loss)
I0815 08:22:57.407696 11101 sgd_solver.cpp:136] Iteration 287800, lr = 0.0100625, m = 0.9
I0815 08:23:12.209120 11101 solver.cpp:312] Iteration 287900 (6.75626 iter/s, 14.8011s/100 iter), loss = 1.3757
I0815 08:23:12.209148 11101 solver.cpp:334]     Train net output #0: loss = 1.6526 (* 1 = 1.6526 loss)
I0815 08:23:12.209154 11101 sgd_solver.cpp:136] Iteration 287900, lr = 0.0100312, m = 0.9
I0815 08:23:26.373420 11101 solver.cpp:509] Iteration 288000, Testing net (#0)
I0815 08:23:37.856361 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 08:23:47.153110 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.559059
I0815 08:23:47.153139 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.792057
I0815 08:23:47.153146 11101 solver.cpp:594]     Test net output #2: loss = 1.92227 (* 1 = 1.92227 loss)
I0815 08:23:47.153168 11101 solver.cpp:264] [MultiGPU] Tests completed in 20.7792s
I0815 08:23:47.294652 11101 solver.cpp:312] Iteration 288000 (2.85026 iter/s, 35.0846s/100 iter), loss = 1.36801
I0815 08:23:47.294715 11101 solver.cpp:334]     Train net output #0: loss = 1.43001 (* 1 = 1.43001 loss)
I0815 08:23:47.294733 11101 sgd_solver.cpp:136] Iteration 288000, lr = 0.01, m = 0.9
I0815 08:24:02.142217 11101 solver.cpp:312] Iteration 288100 (6.7353 iter/s, 14.8472s/100 iter), loss = 1.46365
I0815 08:24:02.142318 11101 solver.cpp:334]     Train net output #0: loss = 1.51457 (* 1 = 1.51457 loss)
I0815 08:24:02.142338 11101 sgd_solver.cpp:136] Iteration 288100, lr = 0.00996875, m = 0.9
I0815 08:24:16.855273 11101 solver.cpp:312] Iteration 288200 (6.79687 iter/s, 14.7127s/100 iter), loss = 1.30802
I0815 08:24:16.855298 11101 solver.cpp:334]     Train net output #0: loss = 1.15383 (* 1 = 1.15383 loss)
I0815 08:24:16.855304 11101 sgd_solver.cpp:136] Iteration 288200, lr = 0.0099375, m = 0.9
I0815 08:24:31.532737 11101 solver.cpp:312] Iteration 288300 (6.81336 iter/s, 14.6771s/100 iter), loss = 1.80428
I0815 08:24:31.532800 11101 solver.cpp:334]     Train net output #0: loss = 1.62387 (* 1 = 1.62387 loss)
I0815 08:24:31.532824 11101 sgd_solver.cpp:136] Iteration 288300, lr = 0.00990625, m = 0.9
I0815 08:24:46.154279 11101 solver.cpp:312] Iteration 288400 (6.83941 iter/s, 14.6211s/100 iter), loss = 1.96203
I0815 08:24:46.166764 11101 solver.cpp:334]     Train net output #0: loss = 1.90124 (* 1 = 1.90124 loss)
I0815 08:24:46.166780 11101 sgd_solver.cpp:136] Iteration 288400, lr = 0.009875, m = 0.9
I0815 08:25:01.007447 11101 solver.cpp:312] Iteration 288500 (6.73276 iter/s, 14.8528s/100 iter), loss = 1.5448
I0815 08:25:01.007472 11101 solver.cpp:334]     Train net output #0: loss = 1.60952 (* 1 = 1.60952 loss)
I0815 08:25:01.007479 11101 sgd_solver.cpp:136] Iteration 288500, lr = 0.00984375, m = 0.9
I0815 08:25:16.185065 11101 solver.cpp:312] Iteration 288600 (6.58883 iter/s, 15.1772s/100 iter), loss = 1.50537
I0815 08:25:16.185129 11101 solver.cpp:334]     Train net output #0: loss = 1.70485 (* 1 = 1.70485 loss)
I0815 08:25:16.185137 11101 sgd_solver.cpp:136] Iteration 288600, lr = 0.0098125, m = 0.9
I0815 08:25:31.509748 11101 solver.cpp:312] Iteration 288700 (6.5256 iter/s, 15.3243s/100 iter), loss = 1.35592
I0815 08:25:31.509775 11101 solver.cpp:334]     Train net output #0: loss = 1.11698 (* 1 = 1.11698 loss)
I0815 08:25:31.509779 11101 sgd_solver.cpp:136] Iteration 288700, lr = 0.00978125, m = 0.9
I0815 08:25:46.397074 11101 solver.cpp:312] Iteration 288800 (6.71731 iter/s, 14.8869s/100 iter), loss = 1.26973
I0815 08:25:46.397176 11101 solver.cpp:334]     Train net output #0: loss = 1.15723 (* 1 = 1.15723 loss)
I0815 08:25:46.397194 11101 sgd_solver.cpp:136] Iteration 288800, lr = 0.00975, m = 0.9
I0815 08:26:01.139878 11101 solver.cpp:312] Iteration 288900 (6.78316 iter/s, 14.7424s/100 iter), loss = 1.60491
I0815 08:26:01.139902 11101 solver.cpp:334]     Train net output #0: loss = 1.78262 (* 1 = 1.78262 loss)
I0815 08:26:01.139909 11101 sgd_solver.cpp:136] Iteration 288900, lr = 0.00971875, m = 0.9
I0815 08:26:16.319432 11101 solver.cpp:312] Iteration 289000 (6.58799 iter/s, 15.1791s/100 iter), loss = 1.42389
I0815 08:26:16.319502 11101 solver.cpp:334]     Train net output #0: loss = 1.43029 (* 1 = 1.43029 loss)
I0815 08:26:16.319521 11101 sgd_solver.cpp:136] Iteration 289000, lr = 0.0096875, m = 0.9
I0815 08:26:31.063740 11101 solver.cpp:312] Iteration 289100 (6.78246 iter/s, 14.7439s/100 iter), loss = 1.47503
I0815 08:26:31.063818 11101 solver.cpp:334]     Train net output #0: loss = 1.78138 (* 1 = 1.78138 loss)
I0815 08:26:31.063832 11101 sgd_solver.cpp:136] Iteration 289100, lr = 0.00965625, m = 0.9
I0815 08:26:46.084769 11101 solver.cpp:312] Iteration 289200 (6.65752 iter/s, 15.0206s/100 iter), loss = 1.47237
I0815 08:26:46.084797 11101 solver.cpp:334]     Train net output #0: loss = 1.64085 (* 1 = 1.64085 loss)
I0815 08:26:46.084803 11101 sgd_solver.cpp:136] Iteration 289200, lr = 0.009625, m = 0.9
I0815 08:27:00.902709 11101 solver.cpp:312] Iteration 289300 (6.74876 iter/s, 14.8175s/100 iter), loss = 1.21735
I0815 08:27:00.902743 11101 solver.cpp:334]     Train net output #0: loss = 1.06863 (* 1 = 1.06863 loss)
I0815 08:27:00.902748 11101 sgd_solver.cpp:136] Iteration 289300, lr = 0.00959375, m = 0.9
I0815 08:27:15.695446 11101 solver.cpp:312] Iteration 289400 (6.76026 iter/s, 14.7923s/100 iter), loss = 2.08425
I0815 08:27:15.695644 11101 solver.cpp:334]     Train net output #0: loss = 1.97147 (* 1 = 1.97147 loss)
I0815 08:27:15.695665 11101 sgd_solver.cpp:136] Iteration 289400, lr = 0.0095625, m = 0.9
I0815 08:27:30.780966 11101 solver.cpp:312] Iteration 289500 (6.62906 iter/s, 15.0851s/100 iter), loss = 1.77802
I0815 08:27:30.780995 11101 solver.cpp:334]     Train net output #0: loss = 1.53608 (* 1 = 1.53608 loss)
I0815 08:27:30.781002 11101 sgd_solver.cpp:136] Iteration 289500, lr = 0.00953125, m = 0.9
I0815 08:27:45.777591 11101 solver.cpp:312] Iteration 289600 (6.66835 iter/s, 14.9962s/100 iter), loss = 1.29529
I0815 08:27:45.784157 11101 solver.cpp:334]     Train net output #0: loss = 1.15376 (* 1 = 1.15376 loss)
I0815 08:27:45.784170 11101 sgd_solver.cpp:136] Iteration 289600, lr = 0.0095, m = 0.9
I0815 08:28:00.723832 11101 solver.cpp:312] Iteration 289700 (6.69083 iter/s, 14.9458s/100 iter), loss = 1.43015
I0815 08:28:00.724053 11101 solver.cpp:334]     Train net output #0: loss = 1.22404 (* 1 = 1.22404 loss)
I0815 08:28:00.724169 11101 sgd_solver.cpp:136] Iteration 289700, lr = 0.00946875, m = 0.9
I0815 08:28:15.778640 11101 solver.cpp:312] Iteration 289800 (6.64258 iter/s, 15.0544s/100 iter), loss = 1.78322
I0815 08:28:15.784202 11101 solver.cpp:334]     Train net output #0: loss = 2.36498 (* 1 = 2.36498 loss)
I0815 08:28:15.784224 11101 sgd_solver.cpp:136] Iteration 289800, lr = 0.0094375, m = 0.9
I0815 08:28:30.453366 11101 solver.cpp:312] Iteration 289900 (6.81462 iter/s, 14.6743s/100 iter), loss = 1.49639
I0815 08:28:30.453394 11101 solver.cpp:334]     Train net output #0: loss = 1.58153 (* 1 = 1.58153 loss)
I0815 08:28:30.453400 11101 sgd_solver.cpp:136] Iteration 289900, lr = 0.00940625, m = 0.9
I0815 08:28:45.346230 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_290000.caffemodel
I0815 08:28:45.451431 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_290000.solverstate
I0815 08:28:45.457423 11101 solver.cpp:509] Iteration 290000, Testing net (#0)
I0815 08:29:07.865065 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.555647
I0815 08:29:07.865183 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.795291
I0815 08:29:07.865206 11101 solver.cpp:594]     Test net output #2: loss = 1.96063 (* 1 = 1.96063 loss)
I0815 08:29:07.865247 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.4072s
I0815 08:29:08.049973 11101 solver.cpp:312] Iteration 290000 (2.65989 iter/s, 37.5956s/100 iter), loss = 1.61377
I0815 08:29:08.050040 11101 solver.cpp:334]     Train net output #0: loss = 1.94587 (* 1 = 1.94587 loss)
I0815 08:29:08.050060 11101 sgd_solver.cpp:136] Iteration 290000, lr = 0.009375, m = 0.9
I0815 08:29:23.067606 11101 solver.cpp:312] Iteration 290100 (6.65902 iter/s, 15.0172s/100 iter), loss = 1.49613
I0815 08:29:23.067672 11101 solver.cpp:334]     Train net output #0: loss = 2.00129 (* 1 = 2.00129 loss)
I0815 08:29:23.067690 11101 sgd_solver.cpp:136] Iteration 290100, lr = 0.00934375, m = 0.9
I0815 08:29:37.644479 11101 solver.cpp:312] Iteration 290200 (6.86037 iter/s, 14.5765s/100 iter), loss = 1.35807
I0815 08:29:37.644505 11101 solver.cpp:334]     Train net output #0: loss = 1.30456 (* 1 = 1.30456 loss)
I0815 08:29:37.644510 11101 sgd_solver.cpp:136] Iteration 290200, lr = 0.0093125, m = 0.9
I0815 08:29:52.484838 11101 solver.cpp:312] Iteration 290300 (6.73857 iter/s, 14.84s/100 iter), loss = 1.34092
I0815 08:29:52.485234 11101 solver.cpp:334]     Train net output #0: loss = 1.41133 (* 1 = 1.41133 loss)
I0815 08:29:52.485342 11101 sgd_solver.cpp:136] Iteration 290300, lr = 0.00928125, m = 0.9
I0815 08:30:07.250928 11101 solver.cpp:312] Iteration 290400 (6.77246 iter/s, 14.7657s/100 iter), loss = 1.36334
I0815 08:30:07.251124 11101 solver.cpp:334]     Train net output #0: loss = 1.49796 (* 1 = 1.49796 loss)
I0815 08:30:07.251214 11101 sgd_solver.cpp:136] Iteration 290400, lr = 0.00925, m = 0.9
I0815 08:30:22.104702 11101 solver.cpp:312] Iteration 290500 (6.73248 iter/s, 14.8534s/100 iter), loss = 1.5105
I0815 08:30:22.104732 11101 solver.cpp:334]     Train net output #0: loss = 1.50659 (* 1 = 1.50659 loss)
I0815 08:30:22.104737 11101 sgd_solver.cpp:136] Iteration 290500, lr = 0.00921875, m = 0.9
I0815 08:30:36.779819 11101 solver.cpp:312] Iteration 290600 (6.81444 iter/s, 14.6747s/100 iter), loss = 1.29198
I0815 08:30:36.779884 11101 solver.cpp:334]     Train net output #0: loss = 1.41709 (* 1 = 1.41709 loss)
I0815 08:30:36.779891 11101 sgd_solver.cpp:136] Iteration 290600, lr = 0.0091875, m = 0.9
I0815 08:30:51.508886 11101 solver.cpp:312] Iteration 290700 (6.78948 iter/s, 14.7287s/100 iter), loss = 1.36797
I0815 08:30:51.508951 11101 solver.cpp:334]     Train net output #0: loss = 1.2113 (* 1 = 1.2113 loss)
I0815 08:30:51.508975 11101 sgd_solver.cpp:136] Iteration 290700, lr = 0.00915625, m = 0.9
I0815 08:31:06.246264 11101 solver.cpp:312] Iteration 290800 (6.78565 iter/s, 14.737s/100 iter), loss = 1.08724
I0815 08:31:06.246289 11101 solver.cpp:334]     Train net output #0: loss = 1.10004 (* 1 = 1.10004 loss)
I0815 08:31:06.246295 11101 sgd_solver.cpp:136] Iteration 290800, lr = 0.009125, m = 0.9
I0815 08:31:21.445667 11101 solver.cpp:312] Iteration 290900 (6.57939 iter/s, 15.199s/100 iter), loss = 1.4494
I0815 08:31:21.445888 11101 solver.cpp:334]     Train net output #0: loss = 1.57162 (* 1 = 1.57162 loss)
I0815 08:31:21.445907 11101 sgd_solver.cpp:136] Iteration 290900, lr = 0.00909375, m = 0.9
I0815 08:31:36.691148 11101 solver.cpp:312] Iteration 291000 (6.5595 iter/s, 15.2451s/100 iter), loss = 1.18396
I0815 08:31:36.691172 11101 solver.cpp:334]     Train net output #0: loss = 1.30968 (* 1 = 1.30968 loss)
I0815 08:31:36.691176 11101 sgd_solver.cpp:136] Iteration 291000, lr = 0.0090625, m = 0.9
I0815 08:31:51.431736 11101 solver.cpp:312] Iteration 291100 (6.78418 iter/s, 14.7402s/100 iter), loss = 1.63747
I0815 08:31:51.431764 11101 solver.cpp:334]     Train net output #0: loss = 1.65068 (* 1 = 1.65068 loss)
I0815 08:31:51.431816 11101 sgd_solver.cpp:136] Iteration 291100, lr = 0.00903125, m = 0.9
I0815 08:32:06.269707 11101 solver.cpp:312] Iteration 291200 (6.73965 iter/s, 14.8376s/100 iter), loss = 1.46731
I0815 08:32:06.269937 11101 solver.cpp:334]     Train net output #0: loss = 1.26411 (* 1 = 1.26411 loss)
I0815 08:32:06.269951 11101 sgd_solver.cpp:136] Iteration 291200, lr = 0.009, m = 0.9
I0815 08:32:20.943567 11101 solver.cpp:312] Iteration 291300 (6.81503 iter/s, 14.6735s/100 iter), loss = 1.55041
I0815 08:32:20.943594 11101 solver.cpp:334]     Train net output #0: loss = 1.68908 (* 1 = 1.68908 loss)
I0815 08:32:20.943599 11101 sgd_solver.cpp:136] Iteration 291300, lr = 0.00896875, m = 0.9
I0815 08:32:36.021385 11101 solver.cpp:312] Iteration 291400 (6.63244 iter/s, 15.0774s/100 iter), loss = 1.21963
I0815 08:32:36.021446 11101 solver.cpp:334]     Train net output #0: loss = 1.22375 (* 1 = 1.22375 loss)
I0815 08:32:36.021462 11101 sgd_solver.cpp:136] Iteration 291400, lr = 0.0089375, m = 0.9
I0815 08:32:51.565549 11101 solver.cpp:312] Iteration 291500 (6.43346 iter/s, 15.5437s/100 iter), loss = 1.38828
I0815 08:32:51.565618 11101 solver.cpp:334]     Train net output #0: loss = 1.66726 (* 1 = 1.66726 loss)
I0815 08:32:51.565626 11101 sgd_solver.cpp:136] Iteration 291500, lr = 0.00890625, m = 0.9
I0815 08:33:07.065466 11101 solver.cpp:312] Iteration 291600 (6.45183 iter/s, 15.4995s/100 iter), loss = 1.39891
I0815 08:33:07.065497 11101 solver.cpp:334]     Train net output #0: loss = 1.28926 (* 1 = 1.28926 loss)
I0815 08:33:07.065503 11101 sgd_solver.cpp:136] Iteration 291600, lr = 0.008875, m = 0.9
I0815 08:33:22.300381 11101 solver.cpp:312] Iteration 291700 (6.56405 iter/s, 15.2345s/100 iter), loss = 1.25095
I0815 08:33:22.300468 11101 solver.cpp:334]     Train net output #0: loss = 1.48002 (* 1 = 1.48002 loss)
I0815 08:33:22.300482 11101 sgd_solver.cpp:136] Iteration 291700, lr = 0.00884375, m = 0.9
I0815 08:33:37.293803 11101 solver.cpp:312] Iteration 291800 (6.66977 iter/s, 14.993s/100 iter), loss = 1.23117
I0815 08:33:37.293854 11101 solver.cpp:334]     Train net output #0: loss = 1.39517 (* 1 = 1.39517 loss)
I0815 08:33:37.293869 11101 sgd_solver.cpp:136] Iteration 291800, lr = 0.0088125, m = 0.9
I0815 08:33:52.398402 11101 solver.cpp:312] Iteration 291900 (6.62068 iter/s, 15.1042s/100 iter), loss = 1.49303
I0815 08:33:52.398492 11101 solver.cpp:334]     Train net output #0: loss = 1.4793 (* 1 = 1.4793 loss)
I0815 08:33:52.398506 11101 sgd_solver.cpp:136] Iteration 291900, lr = 0.00878125, m = 0.9
I0815 08:34:07.477521 11101 solver.cpp:509] Iteration 292000, Testing net (#0)
I0815 08:34:31.957649 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.544235
I0815 08:34:31.957777 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790939
I0815 08:34:31.957787 11101 solver.cpp:594]     Test net output #2: loss = 1.97085 (* 1 = 1.97085 loss)
I0815 08:34:31.957808 11101 solver.cpp:264] [MultiGPU] Tests completed in 24.4796s
I0815 08:34:32.113224 11101 solver.cpp:312] Iteration 292000 (2.51802 iter/s, 39.7137s/100 iter), loss = 1.28834
I0815 08:34:32.113248 11101 solver.cpp:334]     Train net output #0: loss = 1.63483 (* 1 = 1.63483 loss)
I0815 08:34:32.113255 11101 sgd_solver.cpp:136] Iteration 292000, lr = 0.00875, m = 0.9
I0815 08:34:47.105535 11101 solver.cpp:312] Iteration 292100 (6.67027 iter/s, 14.9919s/100 iter), loss = 1.82399
I0815 08:34:47.105733 11101 solver.cpp:334]     Train net output #0: loss = 1.95452 (* 1 = 1.95452 loss)
I0815 08:34:47.105742 11101 sgd_solver.cpp:136] Iteration 292100, lr = 0.00871875, m = 0.9
I0815 08:35:02.669492 11101 solver.cpp:312] Iteration 292200 (6.42528 iter/s, 15.5635s/100 iter), loss = 1.4924
I0815 08:35:02.669558 11101 solver.cpp:334]     Train net output #0: loss = 1.82199 (* 1 = 1.82199 loss)
I0815 08:35:02.669564 11101 sgd_solver.cpp:136] Iteration 292200, lr = 0.0086875, m = 0.9
I0815 08:35:17.916434 11101 solver.cpp:312] Iteration 292300 (6.55887 iter/s, 15.2465s/100 iter), loss = 1.22432
I0815 08:35:17.916463 11101 solver.cpp:334]     Train net output #0: loss = 1.18514 (* 1 = 1.18514 loss)
I0815 08:35:17.916470 11101 sgd_solver.cpp:136] Iteration 292300, lr = 0.00865625, m = 0.9
I0815 08:35:33.157243 11101 solver.cpp:312] Iteration 292400 (6.56151 iter/s, 15.2404s/100 iter), loss = 1.01676
I0815 08:35:33.157332 11101 solver.cpp:334]     Train net output #0: loss = 0.997671 (* 1 = 0.997671 loss)
I0815 08:35:33.157351 11101 sgd_solver.cpp:136] Iteration 292400, lr = 0.008625, m = 0.9
I0815 08:35:48.430141 11101 solver.cpp:312] Iteration 292500 (6.54773 iter/s, 15.2725s/100 iter), loss = 1.34216
I0815 08:35:48.430169 11101 solver.cpp:334]     Train net output #0: loss = 1.57852 (* 1 = 1.57852 loss)
I0815 08:35:48.430176 11101 sgd_solver.cpp:136] Iteration 292500, lr = 0.00859375, m = 0.9
I0815 08:36:03.724962 11101 solver.cpp:312] Iteration 292600 (6.53834 iter/s, 15.2944s/100 iter), loss = 1.6801
I0815 08:36:03.725049 11101 solver.cpp:334]     Train net output #0: loss = 1.64904 (* 1 = 1.64904 loss)
I0815 08:36:03.725066 11101 sgd_solver.cpp:136] Iteration 292600, lr = 0.0085625, m = 0.9
I0815 08:36:18.786054 11101 solver.cpp:312] Iteration 292700 (6.63981 iter/s, 15.0607s/100 iter), loss = 1.1709
I0815 08:36:18.786084 11101 solver.cpp:334]     Train net output #0: loss = 1.29042 (* 1 = 1.29042 loss)
I0815 08:36:18.786090 11101 sgd_solver.cpp:136] Iteration 292700, lr = 0.00853125, m = 0.9
I0815 08:36:33.849433 11101 solver.cpp:312] Iteration 292800 (6.6388 iter/s, 15.063s/100 iter), loss = 1.4557
I0815 08:36:33.849526 11101 solver.cpp:334]     Train net output #0: loss = 0.85909 (* 1 = 0.85909 loss)
I0815 08:36:33.849534 11101 sgd_solver.cpp:136] Iteration 292800, lr = 0.0085, m = 0.9
I0815 08:36:49.101985 11101 solver.cpp:312] Iteration 292900 (6.55646 iter/s, 15.2521s/100 iter), loss = 1.05882
I0815 08:36:49.102054 11101 solver.cpp:334]     Train net output #0: loss = 1.1591 (* 1 = 1.1591 loss)
I0815 08:36:49.102073 11101 sgd_solver.cpp:136] Iteration 292900, lr = 0.00846875, m = 0.9
I0815 08:37:04.119460 11101 solver.cpp:312] Iteration 293000 (6.65909 iter/s, 15.0171s/100 iter), loss = 1.40636
I0815 08:37:04.119524 11101 solver.cpp:334]     Train net output #0: loss = 1.33697 (* 1 = 1.33697 loss)
I0815 08:37:04.119530 11101 sgd_solver.cpp:136] Iteration 293000, lr = 0.0084375, m = 0.9
I0815 08:37:19.402595 11101 solver.cpp:312] Iteration 293100 (6.54334 iter/s, 15.2827s/100 iter), loss = 1.46447
I0815 08:37:19.402631 11101 solver.cpp:334]     Train net output #0: loss = 1.67676 (* 1 = 1.67676 loss)
I0815 08:37:19.402637 11101 sgd_solver.cpp:136] Iteration 293100, lr = 0.00840625, m = 0.9
I0815 08:37:34.453838 11101 solver.cpp:312] Iteration 293200 (6.64415 iter/s, 15.0508s/100 iter), loss = 1.0247
I0815 08:37:34.453912 11101 solver.cpp:334]     Train net output #0: loss = 1.07819 (* 1 = 1.07819 loss)
I0815 08:37:34.453920 11101 sgd_solver.cpp:136] Iteration 293200, lr = 0.008375, m = 0.9
I0815 08:37:50.000303 11101 solver.cpp:312] Iteration 293300 (6.43251 iter/s, 15.546s/100 iter), loss = 1.6768
I0815 08:37:50.000524 11101 solver.cpp:334]     Train net output #0: loss = 1.56609 (* 1 = 1.56609 loss)
I0815 08:37:50.000633 11101 sgd_solver.cpp:136] Iteration 293300, lr = 0.00834375, m = 0.9
I0815 08:38:05.766366 11101 solver.cpp:312] Iteration 293400 (6.34291 iter/s, 15.7656s/100 iter), loss = 1.427
I0815 08:38:05.766469 11101 solver.cpp:334]     Train net output #0: loss = 1.23712 (* 1 = 1.23712 loss)
I0815 08:38:05.766489 11101 sgd_solver.cpp:136] Iteration 293400, lr = 0.0083125, m = 0.9
I0815 08:38:21.115314 11101 solver.cpp:312] Iteration 293500 (6.51528 iter/s, 15.3485s/100 iter), loss = 1.64456
I0815 08:38:21.115389 11101 solver.cpp:334]     Train net output #0: loss = 1.39136 (* 1 = 1.39136 loss)
I0815 08:38:21.115408 11101 sgd_solver.cpp:136] Iteration 293500, lr = 0.00828125, m = 0.9
I0815 08:38:36.507060 11101 solver.cpp:312] Iteration 293600 (6.49717 iter/s, 15.3913s/100 iter), loss = 1.75527
I0815 08:38:36.507143 11101 solver.cpp:334]     Train net output #0: loss = 1.73011 (* 1 = 1.73011 loss)
I0815 08:38:36.507161 11101 sgd_solver.cpp:136] Iteration 293600, lr = 0.00825, m = 0.9
I0815 08:38:51.784728 11101 solver.cpp:312] Iteration 293700 (6.54568 iter/s, 15.2773s/100 iter), loss = 1.49386
I0815 08:38:51.784793 11101 solver.cpp:334]     Train net output #0: loss = 1.5646 (* 1 = 1.5646 loss)
I0815 08:38:51.784811 11101 sgd_solver.cpp:136] Iteration 293700, lr = 0.00821875, m = 0.9
I0815 08:39:07.263855 11101 solver.cpp:312] Iteration 293800 (6.46049 iter/s, 15.4787s/100 iter), loss = 1.51379
I0815 08:39:07.263932 11101 solver.cpp:334]     Train net output #0: loss = 1.67078 (* 1 = 1.67078 loss)
I0815 08:39:07.263944 11101 sgd_solver.cpp:136] Iteration 293800, lr = 0.0081875, m = 0.9
I0815 08:39:22.160192 11101 solver.cpp:312] Iteration 293900 (6.71324 iter/s, 14.8959s/100 iter), loss = 1.68508
I0815 08:39:22.160220 11101 solver.cpp:334]     Train net output #0: loss = 1.55688 (* 1 = 1.55688 loss)
I0815 08:39:22.160225 11101 sgd_solver.cpp:136] Iteration 293900, lr = 0.00815625, m = 0.9
I0815 08:39:37.144096 11101 solver.cpp:509] Iteration 294000, Testing net (#0)
I0815 08:39:41.389462 11102 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 08:39:58.929867 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.573588
I0815 08:39:58.929893 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807232
I0815 08:39:58.929899 11101 solver.cpp:594]     Test net output #2: loss = 1.85295 (* 1 = 1.85295 loss)
I0815 08:39:58.929924 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.7852s
I0815 08:39:59.078752 11101 solver.cpp:312] Iteration 294000 (2.70874 iter/s, 36.9176s/100 iter), loss = 1.18775
I0815 08:39:59.078776 11101 solver.cpp:334]     Train net output #0: loss = 1.14224 (* 1 = 1.14224 loss)
I0815 08:39:59.078783 11101 sgd_solver.cpp:136] Iteration 294000, lr = 0.008125, m = 0.9
I0815 08:40:14.313984 11101 solver.cpp:312] Iteration 294100 (6.56391 iter/s, 15.2348s/100 iter), loss = 1.36171
I0815 08:40:14.316148 11101 solver.cpp:334]     Train net output #0: loss = 1.36508 (* 1 = 1.36508 loss)
I0815 08:40:14.316172 11101 sgd_solver.cpp:136] Iteration 294100, lr = 0.00809375, m = 0.9
I0815 08:40:29.273550 11101 solver.cpp:312] Iteration 294200 (6.68487 iter/s, 14.9591s/100 iter), loss = 1.20671
I0815 08:40:29.273577 11101 solver.cpp:334]     Train net output #0: loss = 1.27979 (* 1 = 1.27979 loss)
I0815 08:40:29.273615 11101 sgd_solver.cpp:136] Iteration 294200, lr = 0.0080625, m = 0.9
I0815 08:40:44.888155 11101 solver.cpp:312] Iteration 294300 (6.40444 iter/s, 15.6142s/100 iter), loss = 1.36931
I0815 08:40:44.888455 11101 solver.cpp:334]     Train net output #0: loss = 1.59355 (* 1 = 1.59355 loss)
I0815 08:40:44.888489 11101 sgd_solver.cpp:136] Iteration 294300, lr = 0.00803125, m = 0.9
I0815 08:41:00.383749 11101 solver.cpp:312] Iteration 294400 (6.45362 iter/s, 15.4952s/100 iter), loss = 1.5569
I0815 08:41:00.383826 11101 solver.cpp:334]     Train net output #0: loss = 1.26729 (* 1 = 1.26729 loss)
I0815 08:41:00.383847 11101 sgd_solver.cpp:136] Iteration 294400, lr = 0.008, m = 0.9
I0815 08:41:15.748284 11101 solver.cpp:312] Iteration 294500 (6.50868 iter/s, 15.3641s/100 iter), loss = 1.26148
I0815 08:41:15.748387 11101 solver.cpp:334]     Train net output #0: loss = 1.33139 (* 1 = 1.33139 loss)
I0815 08:41:15.748416 11101 sgd_solver.cpp:136] Iteration 294500, lr = 0.00796875, m = 0.9
I0815 08:41:31.003588 11101 solver.cpp:312] Iteration 294600 (6.55528 iter/s, 15.2549s/100 iter), loss = 0.877081
I0815 08:41:31.003828 11101 solver.cpp:334]     Train net output #0: loss = 1.01304 (* 1 = 1.01304 loss)
I0815 08:41:31.003950 11101 sgd_solver.cpp:136] Iteration 294600, lr = 0.0079375, m = 0.9
I0815 08:41:46.266803 11101 solver.cpp:312] Iteration 294700 (6.55188 iter/s, 15.2628s/100 iter), loss = 1.55782
I0815 08:41:46.267020 11101 solver.cpp:334]     Train net output #0: loss = 1.78724 (* 1 = 1.78724 loss)
I0815 08:41:46.267027 11101 sgd_solver.cpp:136] Iteration 294700, lr = 0.00790625, m = 0.9
I0815 08:42:01.336447 11101 solver.cpp:312] Iteration 294800 (6.63604 iter/s, 15.0692s/100 iter), loss = 1.40701
I0815 08:42:01.336477 11101 solver.cpp:334]     Train net output #0: loss = 1.42207 (* 1 = 1.42207 loss)
I0815 08:42:01.336483 11101 sgd_solver.cpp:136] Iteration 294800, lr = 0.007875, m = 0.9
I0815 08:42:16.279844 11101 solver.cpp:312] Iteration 294900 (6.6921 iter/s, 14.943s/100 iter), loss = 1.45617
I0815 08:42:16.279904 11101 solver.cpp:334]     Train net output #0: loss = 1.47046 (* 1 = 1.47046 loss)
I0815 08:42:16.279911 11101 sgd_solver.cpp:136] Iteration 294900, lr = 0.00784375, m = 0.9
I0815 08:42:31.486296 11101 solver.cpp:312] Iteration 295000 (6.57634 iter/s, 15.206s/100 iter), loss = 1.26619
I0815 08:42:31.486368 11101 solver.cpp:334]     Train net output #0: loss = 1.49149 (* 1 = 1.49149 loss)
I0815 08:42:31.486388 11101 sgd_solver.cpp:136] Iteration 295000, lr = 0.0078125, m = 0.9
I0815 08:42:46.692260 11101 solver.cpp:312] Iteration 295100 (6.57655 iter/s, 15.2055s/100 iter), loss = 1.2944
I0815 08:42:46.693965 11101 solver.cpp:334]     Train net output #0: loss = 1.59108 (* 1 = 1.59108 loss)
I0815 08:42:46.693991 11101 sgd_solver.cpp:136] Iteration 295100, lr = 0.00778125, m = 0.9
I0815 08:43:01.637579 11101 solver.cpp:312] Iteration 295200 (6.69124 iter/s, 14.9449s/100 iter), loss = 1.16133
I0815 08:43:01.637614 11101 solver.cpp:334]     Train net output #0: loss = 1.29625 (* 1 = 1.29625 loss)
I0815 08:43:01.637621 11101 sgd_solver.cpp:136] Iteration 295200, lr = 0.00775, m = 0.9
I0815 08:43:16.655740 11101 solver.cpp:312] Iteration 295300 (6.65879 iter/s, 15.0177s/100 iter), loss = 1.73328
I0815 08:43:16.655791 11101 solver.cpp:334]     Train net output #0: loss = 1.48416 (* 1 = 1.48416 loss)
I0815 08:43:16.655804 11101 sgd_solver.cpp:136] Iteration 295300, lr = 0.00771875, m = 0.9
I0815 08:43:31.644397 11101 solver.cpp:312] Iteration 295400 (6.67189 iter/s, 14.9882s/100 iter), loss = 1.59516
I0815 08:43:31.644661 11101 solver.cpp:334]     Train net output #0: loss = 1.62868 (* 1 = 1.62868 loss)
I0815 08:43:31.644670 11101 sgd_solver.cpp:136] Iteration 295400, lr = 0.0076875, m = 0.9
I0815 08:43:46.467921 11101 solver.cpp:312] Iteration 295500 (6.74622 iter/s, 14.8231s/100 iter), loss = 1.39022
I0815 08:43:46.467947 11101 solver.cpp:334]     Train net output #0: loss = 1.13623 (* 1 = 1.13623 loss)
I0815 08:43:46.467953 11101 sgd_solver.cpp:136] Iteration 295500, lr = 0.00765625, m = 0.9
I0815 08:44:01.486636 11101 solver.cpp:312] Iteration 295600 (6.65854 iter/s, 15.0183s/100 iter), loss = 1.27298
I0815 08:44:01.486855 11101 solver.cpp:334]     Train net output #0: loss = 1.34977 (* 1 = 1.34977 loss)
I0815 08:44:01.486970 11101 sgd_solver.cpp:136] Iteration 295600, lr = 0.007625, m = 0.9
I0815 08:44:16.409155 11101 solver.cpp:312] Iteration 295700 (6.70147 iter/s, 14.9221s/100 iter), loss = 1.56362
I0815 08:44:16.409263 11101 solver.cpp:334]     Train net output #0: loss = 1.47945 (* 1 = 1.47945 loss)
I0815 08:44:16.409284 11101 sgd_solver.cpp:136] Iteration 295700, lr = 0.00759375, m = 0.9
I0815 08:44:31.446794 11101 solver.cpp:312] Iteration 295800 (6.65016 iter/s, 15.0372s/100 iter), loss = 1.29769
I0815 08:44:31.446821 11101 solver.cpp:334]     Train net output #0: loss = 1.32191 (* 1 = 1.32191 loss)
I0815 08:44:31.446825 11101 sgd_solver.cpp:136] Iteration 295800, lr = 0.0075625, m = 0.9
I0815 08:44:46.338688 11101 solver.cpp:312] Iteration 295900 (6.71525 iter/s, 14.8915s/100 iter), loss = 1.24836
I0815 08:44:46.338747 11101 solver.cpp:334]     Train net output #0: loss = 1.28669 (* 1 = 1.28669 loss)
I0815 08:44:46.338759 11101 sgd_solver.cpp:136] Iteration 295900, lr = 0.00753125, m = 0.9
I0815 08:45:00.840509 11101 solver.cpp:509] Iteration 296000, Testing net (#0)
I0815 08:45:22.328508 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.568941
I0815 08:45:22.328531 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805233
I0815 08:45:22.328537 11101 solver.cpp:594]     Test net output #2: loss = 1.88216 (* 1 = 1.88216 loss)
I0815 08:45:22.328558 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.4875s
I0815 08:45:22.476126 11101 solver.cpp:312] Iteration 296000 (2.76729 iter/s, 36.1365s/100 iter), loss = 1.57351
I0815 08:45:22.476161 11101 solver.cpp:334]     Train net output #0: loss = 1.3909 (* 1 = 1.3909 loss)
I0815 08:45:22.476266 11101 sgd_solver.cpp:136] Iteration 296000, lr = 0.0075, m = 0.9
I0815 08:45:37.028342 11101 solver.cpp:312] Iteration 296100 (6.872 iter/s, 14.5518s/100 iter), loss = 1.18006
I0815 08:45:37.028656 11101 solver.cpp:334]     Train net output #0: loss = 0.871522 (* 1 = 0.871522 loss)
I0815 08:45:37.028676 11101 sgd_solver.cpp:136] Iteration 296100, lr = 0.00746875, m = 0.9
I0815 08:45:51.524513 11101 solver.cpp:312] Iteration 296200 (6.89856 iter/s, 14.4958s/100 iter), loss = 1.67136
I0815 08:45:51.524582 11101 solver.cpp:334]     Train net output #0: loss = 1.80645 (* 1 = 1.80645 loss)
I0815 08:45:51.524601 11101 sgd_solver.cpp:136] Iteration 296200, lr = 0.0074375, m = 0.9
I0815 08:46:06.278022 11101 solver.cpp:312] Iteration 296300 (6.77823 iter/s, 14.7531s/100 iter), loss = 1.23798
I0815 08:46:06.278045 11101 solver.cpp:334]     Train net output #0: loss = 1.16084 (* 1 = 1.16084 loss)
I0815 08:46:06.278051 11101 sgd_solver.cpp:136] Iteration 296300, lr = 0.00740625, m = 0.9
I0815 08:46:20.923933 11101 solver.cpp:312] Iteration 296400 (6.82803 iter/s, 14.6455s/100 iter), loss = 1.54984
I0815 08:46:20.924041 11101 solver.cpp:334]     Train net output #0: loss = 1.4774 (* 1 = 1.4774 loss)
I0815 08:46:20.924063 11101 sgd_solver.cpp:136] Iteration 296400, lr = 0.007375, m = 0.9
I0815 08:46:35.804453 11101 solver.cpp:312] Iteration 296500 (6.72038 iter/s, 14.8801s/100 iter), loss = 1.52934
I0815 08:46:35.804477 11101 solver.cpp:334]     Train net output #0: loss = 1.40293 (* 1 = 1.40293 loss)
I0815 08:46:35.804481 11101 sgd_solver.cpp:136] Iteration 296500, lr = 0.00734375, m = 0.9
I0815 08:46:50.447923 11101 solver.cpp:312] Iteration 296600 (6.82917 iter/s, 14.6431s/100 iter), loss = 1.36925
I0815 08:46:50.447988 11101 solver.cpp:334]     Train net output #0: loss = 1.39484 (* 1 = 1.39484 loss)
I0815 08:46:50.448005 11101 sgd_solver.cpp:136] Iteration 296600, lr = 0.0073125, m = 0.9
I0815 08:47:05.129549 11101 solver.cpp:312] Iteration 296700 (6.81142 iter/s, 14.6812s/100 iter), loss = 1.34888
I0815 08:47:05.129652 11101 solver.cpp:334]     Train net output #0: loss = 1.07929 (* 1 = 1.07929 loss)
I0815 08:47:05.129664 11101 sgd_solver.cpp:136] Iteration 296700, lr = 0.00728125, m = 0.9
I0815 08:47:20.103554 11101 solver.cpp:312] Iteration 296800 (6.67843 iter/s, 14.9736s/100 iter), loss = 1.53293
I0815 08:47:20.103597 11101 solver.cpp:334]     Train net output #0: loss = 1.77321 (* 1 = 1.77321 loss)
I0815 08:47:20.103695 11101 sgd_solver.cpp:136] Iteration 296800, lr = 0.00725, m = 0.9
I0815 08:47:35.281153 11101 solver.cpp:312] Iteration 296900 (6.58884 iter/s, 15.1772s/100 iter), loss = 1.82737
I0815 08:47:35.281261 11101 solver.cpp:334]     Train net output #0: loss = 1.73077 (* 1 = 1.73077 loss)
I0815 08:47:35.281270 11101 sgd_solver.cpp:136] Iteration 296900, lr = 0.00721875, m = 0.9
I0815 08:47:50.530685 11101 solver.cpp:312] Iteration 297000 (6.55776 iter/s, 15.2491s/100 iter), loss = 1.8405
I0815 08:47:50.530750 11101 solver.cpp:334]     Train net output #0: loss = 1.52408 (* 1 = 1.52408 loss)
I0815 08:47:50.530772 11101 sgd_solver.cpp:136] Iteration 297000, lr = 0.0071875, m = 0.9
I0815 08:48:05.628543 11101 solver.cpp:312] Iteration 297100 (6.62364 iter/s, 15.0974s/100 iter), loss = 1.7532
I0815 08:48:05.628607 11101 solver.cpp:334]     Train net output #0: loss = 1.86355 (* 1 = 1.86355 loss)
I0815 08:48:05.628617 11101 sgd_solver.cpp:136] Iteration 297100, lr = 0.00715625, m = 0.9
I0815 08:48:20.434242 11101 solver.cpp:312] Iteration 297200 (6.75434 iter/s, 14.8053s/100 iter), loss = 1.30602
I0815 08:48:20.434273 11101 solver.cpp:334]     Train net output #0: loss = 1.34111 (* 1 = 1.34111 loss)
I0815 08:48:20.434279 11101 sgd_solver.cpp:136] Iteration 297200, lr = 0.007125, m = 0.9
I0815 08:48:35.691707 11101 solver.cpp:312] Iteration 297300 (6.55435 iter/s, 15.257s/100 iter), loss = 1.28476
I0815 08:48:35.691769 11101 solver.cpp:334]     Train net output #0: loss = 1.1925 (* 1 = 1.1925 loss)
I0815 08:48:35.691776 11101 sgd_solver.cpp:136] Iteration 297300, lr = 0.00709375, m = 0.9
I0815 08:48:51.006075 11101 solver.cpp:312] Iteration 297400 (6.52999 iter/s, 15.3139s/100 iter), loss = 1.35618
I0815 08:48:51.006099 11101 solver.cpp:334]     Train net output #0: loss = 1.34799 (* 1 = 1.34799 loss)
I0815 08:48:51.006105 11101 sgd_solver.cpp:136] Iteration 297400, lr = 0.0070625, m = 0.9
I0815 08:49:05.968895 11101 solver.cpp:312] Iteration 297500 (6.68342 iter/s, 14.9624s/100 iter), loss = 1.47182
I0815 08:49:05.968976 11101 solver.cpp:334]     Train net output #0: loss = 1.48923 (* 1 = 1.48923 loss)
I0815 08:49:05.968988 11101 sgd_solver.cpp:136] Iteration 297500, lr = 0.00703125, m = 0.9
I0815 08:49:21.149215 11101 solver.cpp:312] Iteration 297600 (6.58766 iter/s, 15.1799s/100 iter), loss = 1.36543
I0815 08:49:21.149246 11101 solver.cpp:334]     Train net output #0: loss = 1.08316 (* 1 = 1.08316 loss)
I0815 08:49:21.149252 11101 sgd_solver.cpp:136] Iteration 297600, lr = 0.007, m = 0.9
I0815 08:49:36.860561 11101 solver.cpp:312] Iteration 297700 (6.365 iter/s, 15.7109s/100 iter), loss = 1.41886
I0815 08:49:36.860661 11101 solver.cpp:334]     Train net output #0: loss = 1.09705 (* 1 = 1.09705 loss)
I0815 08:49:36.860680 11101 sgd_solver.cpp:136] Iteration 297700, lr = 0.00696875, m = 0.9
I0815 08:49:52.403870 11101 solver.cpp:312] Iteration 297800 (6.43381 iter/s, 15.5429s/100 iter), loss = 1.08623
I0815 08:49:52.403897 11101 solver.cpp:334]     Train net output #0: loss = 0.769717 (* 1 = 0.769717 loss)
I0815 08:49:52.403903 11101 sgd_solver.cpp:136] Iteration 297800, lr = 0.0069375, m = 0.9
I0815 08:50:07.696966 11101 solver.cpp:312] Iteration 297900 (6.53908 iter/s, 15.2927s/100 iter), loss = 1.20259
I0815 08:50:07.697029 11101 solver.cpp:334]     Train net output #0: loss = 0.972112 (* 1 = 0.972112 loss)
I0815 08:50:07.697036 11101 sgd_solver.cpp:136] Iteration 297900, lr = 0.00690625, m = 0.9
I0815 08:50:22.643939 11101 solver.cpp:509] Iteration 298000, Testing net (#0)
I0815 08:50:23.491421 11099 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 08:50:45.812010 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.570588
I0815 08:50:45.812055 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.80888
I0815 08:50:45.812062 11101 solver.cpp:594]     Test net output #2: loss = 1.83849 (* 1 = 1.83849 loss)
I0815 08:50:45.812084 11101 solver.cpp:264] [MultiGPU] Tests completed in 23.1675s
I0815 08:50:45.975641 11101 solver.cpp:312] Iteration 298000 (2.61249 iter/s, 38.2776s/100 iter), loss = 1.31843
I0815 08:50:45.975667 11101 solver.cpp:334]     Train net output #0: loss = 1.1399 (* 1 = 1.1399 loss)
I0815 08:50:45.975672 11101 sgd_solver.cpp:136] Iteration 298000, lr = 0.006875, m = 0.9
I0815 08:51:01.288157 11101 solver.cpp:312] Iteration 298100 (6.53079 iter/s, 15.3121s/100 iter), loss = 1.60208
I0815 08:51:01.288224 11101 solver.cpp:334]     Train net output #0: loss = 1.13118 (* 1 = 1.13118 loss)
I0815 08:51:01.288239 11101 sgd_solver.cpp:136] Iteration 298100, lr = 0.00684375, m = 0.9
I0815 08:51:16.671038 11101 solver.cpp:312] Iteration 298200 (6.50091 iter/s, 15.3825s/100 iter), loss = 1.70174
I0815 08:51:16.671103 11101 solver.cpp:334]     Train net output #0: loss = 1.86989 (* 1 = 1.86989 loss)
I0815 08:51:16.671116 11101 sgd_solver.cpp:136] Iteration 298200, lr = 0.0068125, m = 0.9
I0815 08:51:31.897791 11101 solver.cpp:312] Iteration 298300 (6.56757 iter/s, 15.2263s/100 iter), loss = 1.52383
I0815 08:51:31.897815 11101 solver.cpp:334]     Train net output #0: loss = 1.47376 (* 1 = 1.47376 loss)
I0815 08:51:31.897821 11101 sgd_solver.cpp:136] Iteration 298300, lr = 0.00678125, m = 0.9
I0815 08:51:46.830482 11101 solver.cpp:312] Iteration 298400 (6.6969 iter/s, 14.9323s/100 iter), loss = 1.50808
I0815 08:51:46.830574 11101 solver.cpp:334]     Train net output #0: loss = 1.61733 (* 1 = 1.61733 loss)
I0815 08:51:46.830591 11101 sgd_solver.cpp:136] Iteration 298400, lr = 0.00675, m = 0.9
I0815 08:52:01.809209 11101 solver.cpp:312] Iteration 298500 (6.67632 iter/s, 14.9783s/100 iter), loss = 1.61117
I0815 08:52:01.809237 11101 solver.cpp:334]     Train net output #0: loss = 1.70468 (* 1 = 1.70468 loss)
I0815 08:52:01.809244 11101 sgd_solver.cpp:136] Iteration 298500, lr = 0.00671875, m = 0.9
I0815 08:52:14.053930 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 08:52:17.040029 11101 solver.cpp:312] Iteration 298600 (6.56582 iter/s, 15.2304s/100 iter), loss = 1.62346
I0815 08:52:17.040112 11101 solver.cpp:334]     Train net output #0: loss = 1.94446 (* 1 = 1.94446 loss)
I0815 08:52:17.040127 11101 sgd_solver.cpp:136] Iteration 298600, lr = 0.0066875, m = 0.9
I0815 08:52:31.889822 11101 solver.cpp:312] Iteration 298700 (6.73429 iter/s, 14.8494s/100 iter), loss = 1.40502
I0815 08:52:31.889885 11101 solver.cpp:334]     Train net output #0: loss = 1.13793 (* 1 = 1.13793 loss)
I0815 08:52:31.889902 11101 sgd_solver.cpp:136] Iteration 298700, lr = 0.00665625, m = 0.9
I0815 08:52:47.007637 11101 solver.cpp:312] Iteration 298800 (6.6149 iter/s, 15.1174s/100 iter), loss = 1.20996
I0815 08:52:47.007666 11101 solver.cpp:334]     Train net output #0: loss = 1.28715 (* 1 = 1.28715 loss)
I0815 08:52:47.007673 11101 sgd_solver.cpp:136] Iteration 298800, lr = 0.006625, m = 0.9
I0815 08:53:02.241647 11101 solver.cpp:312] Iteration 298900 (6.56444 iter/s, 15.2336s/100 iter), loss = 1.25963
I0815 08:53:02.241736 11101 solver.cpp:334]     Train net output #0: loss = 0.781807 (* 1 = 0.781807 loss)
I0815 08:53:02.241755 11101 sgd_solver.cpp:136] Iteration 298900, lr = 0.00659375, m = 0.9
I0815 08:53:17.364152 11101 solver.cpp:312] Iteration 299000 (6.61284 iter/s, 15.1221s/100 iter), loss = 1.91114
I0815 08:53:17.364179 11101 solver.cpp:334]     Train net output #0: loss = 1.61768 (* 1 = 1.61768 loss)
I0815 08:53:17.364184 11101 sgd_solver.cpp:136] Iteration 299000, lr = 0.0065625, m = 0.9
I0815 08:53:32.621183 11101 solver.cpp:312] Iteration 299100 (6.55454 iter/s, 15.2566s/100 iter), loss = 1.40671
I0815 08:53:32.632207 11101 solver.cpp:334]     Train net output #0: loss = 1.49518 (* 1 = 1.49518 loss)
I0815 08:53:32.632236 11101 sgd_solver.cpp:136] Iteration 299100, lr = 0.00653125, m = 0.9
I0815 08:53:47.936626 11101 solver.cpp:312] Iteration 299200 (6.52954 iter/s, 15.315s/100 iter), loss = 1.53366
I0815 08:53:47.936691 11101 solver.cpp:334]     Train net output #0: loss = 1.23171 (* 1 = 1.23171 loss)
I0815 08:53:47.936708 11101 sgd_solver.cpp:136] Iteration 299200, lr = 0.0065, m = 0.9
I0815 08:54:03.034952 11101 solver.cpp:312] Iteration 299300 (6.62343 iter/s, 15.0979s/100 iter), loss = 1.56972
I0815 08:54:03.035049 11101 solver.cpp:334]     Train net output #0: loss = 1.54696 (* 1 = 1.54696 loss)
I0815 08:54:03.035068 11101 sgd_solver.cpp:136] Iteration 299300, lr = 0.00646875, m = 0.9
I0815 08:54:18.268187 11101 solver.cpp:312] Iteration 299400 (6.56478 iter/s, 15.2328s/100 iter), loss = 1.31023
I0815 08:54:18.268244 11101 solver.cpp:334]     Train net output #0: loss = 1.34308 (* 1 = 1.34308 loss)
I0815 08:54:18.268257 11101 sgd_solver.cpp:136] Iteration 299400, lr = 0.0064375, m = 0.9
I0815 08:54:33.901227 11101 solver.cpp:312] Iteration 299500 (6.39688 iter/s, 15.6326s/100 iter), loss = 1.49247
I0815 08:54:33.901294 11101 solver.cpp:334]     Train net output #0: loss = 1.60562 (* 1 = 1.60562 loss)
I0815 08:54:33.901301 11101 sgd_solver.cpp:136] Iteration 299500, lr = 0.00640625, m = 0.9
I0815 08:54:49.157068 11101 solver.cpp:312] Iteration 299600 (6.55505 iter/s, 15.2554s/100 iter), loss = 1.27987
I0815 08:54:49.157095 11101 solver.cpp:334]     Train net output #0: loss = 1.28312 (* 1 = 1.28312 loss)
I0815 08:54:49.157100 11101 sgd_solver.cpp:136] Iteration 299600, lr = 0.006375, m = 0.9
I0815 08:55:04.235157 11101 solver.cpp:312] Iteration 299700 (6.63232 iter/s, 15.0777s/100 iter), loss = 1.31395
I0815 08:55:04.235260 11101 solver.cpp:334]     Train net output #0: loss = 1.56956 (* 1 = 1.56956 loss)
I0815 08:55:04.235267 11101 sgd_solver.cpp:136] Iteration 299700, lr = 0.00634375, m = 0.9
I0815 08:55:19.157133 11101 solver.cpp:312] Iteration 299800 (6.70171 iter/s, 14.9216s/100 iter), loss = 1.08752
I0815 08:55:19.157157 11101 solver.cpp:334]     Train net output #0: loss = 1.15294 (* 1 = 1.15294 loss)
I0815 08:55:19.157161 11101 sgd_solver.cpp:136] Iteration 299800, lr = 0.0063125, m = 0.9
I0815 08:55:34.406930 11101 solver.cpp:312] Iteration 299900 (6.55765 iter/s, 15.2494s/100 iter), loss = 1.22734
I0815 08:55:34.407018 11101 solver.cpp:334]     Train net output #0: loss = 1.13797 (* 1 = 1.13797 loss)
I0815 08:55:34.407025 11101 sgd_solver.cpp:136] Iteration 299900, lr = 0.00628125, m = 0.9
I0815 08:55:49.640918 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_300000.caffemodel
I0815 08:55:49.710674 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_300000.solverstate
I0815 08:55:49.715265 11101 solver.cpp:509] Iteration 300000, Testing net (#0)
I0815 08:56:12.192889 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.578824
I0815 08:56:12.192945 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.809762
I0815 08:56:12.192950 11101 solver.cpp:594]     Test net output #2: loss = 1.8282 (* 1 = 1.8282 loss)
I0815 08:56:12.192967 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.4771s
I0815 08:56:12.355219 11101 solver.cpp:312] Iteration 300000 (2.63524 iter/s, 37.9473s/100 iter), loss = 1.19606
I0815 08:56:12.355242 11101 solver.cpp:334]     Train net output #0: loss = 1.27352 (* 1 = 1.27352 loss)
I0815 08:56:12.355247 11101 sgd_solver.cpp:136] Iteration 300000, lr = 0.00625, m = 0.9
I0815 08:56:27.727365 11101 solver.cpp:312] Iteration 300100 (6.50546 iter/s, 15.3717s/100 iter), loss = 1.24725
I0815 08:56:27.727455 11101 solver.cpp:334]     Train net output #0: loss = 1.20193 (* 1 = 1.20193 loss)
I0815 08:56:27.727476 11101 sgd_solver.cpp:136] Iteration 300100, lr = 0.00621875, m = 0.9
I0815 08:56:43.095193 11101 solver.cpp:312] Iteration 300200 (6.50728 iter/s, 15.3674s/100 iter), loss = 1.75157
I0815 08:56:43.095297 11101 solver.cpp:334]     Train net output #0: loss = 1.71523 (* 1 = 1.71523 loss)
I0815 08:56:43.095317 11101 sgd_solver.cpp:136] Iteration 300200, lr = 0.0061875, m = 0.9
I0815 08:56:58.307492 11101 solver.cpp:312] Iteration 300300 (6.57381 iter/s, 15.2119s/100 iter), loss = 1.26816
I0815 08:56:58.307559 11101 solver.cpp:334]     Train net output #0: loss = 1.29213 (* 1 = 1.29213 loss)
I0815 08:56:58.307576 11101 sgd_solver.cpp:136] Iteration 300300, lr = 0.00615625, m = 0.9
I0815 08:57:13.337877 11101 solver.cpp:312] Iteration 300400 (6.65337 iter/s, 15.03s/100 iter), loss = 1.50238
I0815 08:57:13.337986 11101 solver.cpp:334]     Train net output #0: loss = 1.27605 (* 1 = 1.27605 loss)
I0815 08:57:13.338007 11101 sgd_solver.cpp:136] Iteration 300400, lr = 0.006125, m = 0.9
I0815 08:57:28.426334 11101 solver.cpp:312] Iteration 300500 (6.62777 iter/s, 15.088s/100 iter), loss = 1.31883
I0815 08:57:28.426409 11101 solver.cpp:334]     Train net output #0: loss = 1.18019 (* 1 = 1.18019 loss)
I0815 08:57:28.426429 11101 sgd_solver.cpp:136] Iteration 300500, lr = 0.00609375, m = 0.9
I0815 08:57:43.586544 11101 solver.cpp:312] Iteration 300600 (6.5964 iter/s, 15.1598s/100 iter), loss = 1.25235
I0815 08:57:43.586792 11101 solver.cpp:334]     Train net output #0: loss = 1.10337 (* 1 = 1.10337 loss)
I0815 08:57:43.586905 11101 sgd_solver.cpp:136] Iteration 300600, lr = 0.0060625, m = 0.9
I0815 08:57:58.478737 11101 solver.cpp:312] Iteration 300700 (6.71511 iter/s, 14.8918s/100 iter), loss = 1.48749
I0815 08:57:58.478770 11101 solver.cpp:334]     Train net output #0: loss = 1.43407 (* 1 = 1.43407 loss)
I0815 08:57:58.478775 11101 sgd_solver.cpp:136] Iteration 300700, lr = 0.00603125, m = 0.9
I0815 08:58:13.357662 11101 solver.cpp:312] Iteration 300800 (6.7211 iter/s, 14.8785s/100 iter), loss = 1.57624
I0815 08:58:13.357868 11101 solver.cpp:334]     Train net output #0: loss = 1.48978 (* 1 = 1.48978 loss)
I0815 08:58:13.357977 11101 sgd_solver.cpp:136] Iteration 300800, lr = 0.006, m = 0.9
I0815 08:58:28.434190 11101 solver.cpp:312] Iteration 300900 (6.63301 iter/s, 15.0761s/100 iter), loss = 1.47081
I0815 08:58:28.434262 11101 solver.cpp:334]     Train net output #0: loss = 1.52788 (* 1 = 1.52788 loss)
I0815 08:58:28.434268 11101 sgd_solver.cpp:136] Iteration 300900, lr = 0.00596875, m = 0.9
I0815 08:58:43.344465 11101 solver.cpp:312] Iteration 301000 (6.70697 iter/s, 14.9099s/100 iter), loss = 1.65127
I0815 08:58:43.344492 11101 solver.cpp:334]     Train net output #0: loss = 1.78756 (* 1 = 1.78756 loss)
I0815 08:58:43.344501 11101 sgd_solver.cpp:136] Iteration 301000, lr = 0.0059375, m = 0.9
I0815 08:58:58.259248 11101 solver.cpp:312] Iteration 301100 (6.70494 iter/s, 14.9144s/100 iter), loss = 1.26403
I0815 08:58:58.264227 11101 solver.cpp:334]     Train net output #0: loss = 1.10618 (* 1 = 1.10618 loss)
I0815 08:58:58.264246 11101 sgd_solver.cpp:136] Iteration 301100, lr = 0.00590625, m = 0.9
I0815 08:59:13.180557 11101 solver.cpp:312] Iteration 301200 (6.70201 iter/s, 14.9209s/100 iter), loss = 1.13819
I0815 08:59:13.180642 11101 solver.cpp:334]     Train net output #0: loss = 0.944432 (* 1 = 0.944432 loss)
I0815 08:59:13.180649 11101 sgd_solver.cpp:136] Iteration 301200, lr = 0.005875, m = 0.9
I0815 08:59:27.925977 11101 solver.cpp:312] Iteration 301300 (6.78195 iter/s, 14.745s/100 iter), loss = 1.44307
I0815 08:59:27.926192 11101 solver.cpp:334]     Train net output #0: loss = 1.69788 (* 1 = 1.69788 loss)
I0815 08:59:27.926302 11101 sgd_solver.cpp:136] Iteration 301300, lr = 0.00584375, m = 0.9
I0815 08:59:42.936540 11101 solver.cpp:312] Iteration 301400 (6.66216 iter/s, 15.0102s/100 iter), loss = 1.45745
I0815 08:59:42.936758 11101 solver.cpp:334]     Train net output #0: loss = 1.39424 (* 1 = 1.39424 loss)
I0815 08:59:42.936868 11101 sgd_solver.cpp:136] Iteration 301400, lr = 0.0058125, m = 0.9
I0815 08:59:58.072160 11101 solver.cpp:312] Iteration 301500 (6.60712 iter/s, 15.1352s/100 iter), loss = 1.43543
I0815 08:59:58.072268 11101 solver.cpp:334]     Train net output #0: loss = 1.27268 (* 1 = 1.27268 loss)
I0815 08:59:58.072286 11101 sgd_solver.cpp:136] Iteration 301500, lr = 0.00578125, m = 0.9
I0815 09:00:12.821396 11101 solver.cpp:312] Iteration 301600 (6.7802 iter/s, 14.7488s/100 iter), loss = 1.4672
I0815 09:00:12.821424 11101 solver.cpp:334]     Train net output #0: loss = 1.48856 (* 1 = 1.48856 loss)
I0815 09:00:12.821431 11101 sgd_solver.cpp:136] Iteration 301600, lr = 0.00575, m = 0.9
I0815 09:00:27.592882 11101 solver.cpp:312] Iteration 301700 (6.76998 iter/s, 14.7711s/100 iter), loss = 1.45695
I0815 09:00:27.593045 11101 solver.cpp:334]     Train net output #0: loss = 1.04107 (* 1 = 1.04107 loss)
I0815 09:00:27.593065 11101 sgd_solver.cpp:136] Iteration 301700, lr = 0.00571875, m = 0.9
I0815 09:00:42.542924 11101 solver.cpp:312] Iteration 301800 (6.68913 iter/s, 14.9496s/100 iter), loss = 1.25126
I0815 09:00:42.543038 11101 solver.cpp:334]     Train net output #0: loss = 1.18038 (* 1 = 1.18038 loss)
I0815 09:00:42.543058 11101 sgd_solver.cpp:136] Iteration 301800, lr = 0.0056875, m = 0.9
I0815 09:00:57.619019 11101 solver.cpp:312] Iteration 301900 (6.6332 iter/s, 15.0757s/100 iter), loss = 1.24755
I0815 09:00:57.619081 11101 solver.cpp:334]     Train net output #0: loss = 1.52203 (* 1 = 1.52203 loss)
I0815 09:00:57.619096 11101 sgd_solver.cpp:136] Iteration 301900, lr = 0.00565625, m = 0.9
I0815 09:01:12.296156 11101 solver.cpp:509] Iteration 302000, Testing net (#0)
I0815 09:01:34.570744 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.574529
I0815 09:01:34.570853 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807704
I0815 09:01:34.570863 11101 solver.cpp:594]     Test net output #2: loss = 1.84439 (* 1 = 1.84439 loss)
I0815 09:01:34.570883 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.2741s
I0815 09:01:34.729882 11101 solver.cpp:312] Iteration 302000 (2.6947 iter/s, 37.1099s/100 iter), loss = 1.18223
I0815 09:01:34.729913 11101 solver.cpp:334]     Train net output #0: loss = 0.963409 (* 1 = 0.963409 loss)
I0815 09:01:34.729919 11101 sgd_solver.cpp:136] Iteration 302000, lr = 0.005625, m = 0.9
I0815 09:01:49.636900 11101 solver.cpp:312] Iteration 302100 (6.70844 iter/s, 14.9066s/100 iter), loss = 1.14157
I0815 09:01:49.636927 11101 solver.cpp:334]     Train net output #0: loss = 1.16083 (* 1 = 1.16083 loss)
I0815 09:01:49.636931 11101 sgd_solver.cpp:136] Iteration 302100, lr = 0.00559375, m = 0.9
I0815 09:02:04.309233 11101 solver.cpp:312] Iteration 302200 (6.81574 iter/s, 14.6719s/100 iter), loss = 1.28591
I0815 09:02:04.309285 11101 solver.cpp:334]     Train net output #0: loss = 1.41798 (* 1 = 1.41798 loss)
I0815 09:02:04.309299 11101 sgd_solver.cpp:136] Iteration 302200, lr = 0.0055625, m = 0.9
I0815 09:02:19.130506 11101 solver.cpp:312] Iteration 302300 (6.74725 iter/s, 14.8209s/100 iter), loss = 1.35048
I0815 09:02:19.136183 11101 solver.cpp:334]     Train net output #0: loss = 0.973145 (* 1 = 0.973145 loss)
I0815 09:02:19.136203 11101 sgd_solver.cpp:136] Iteration 302300, lr = 0.00553125, m = 0.9
I0815 09:02:33.792549 11101 solver.cpp:312] Iteration 302400 (6.82052 iter/s, 14.6616s/100 iter), loss = 1.07915
I0815 09:02:33.792614 11101 solver.cpp:334]     Train net output #0: loss = 1.342 (* 1 = 1.342 loss)
I0815 09:02:33.792631 11101 sgd_solver.cpp:136] Iteration 302400, lr = 0.0055, m = 0.9
I0815 09:02:48.431099 11101 solver.cpp:312] Iteration 302500 (6.83147 iter/s, 14.6381s/100 iter), loss = 1.41276
I0815 09:02:48.431164 11101 solver.cpp:334]     Train net output #0: loss = 1.53266 (* 1 = 1.53266 loss)
I0815 09:02:48.431181 11101 sgd_solver.cpp:136] Iteration 302500, lr = 0.00546875, m = 0.9
I0815 09:03:03.587571 11101 solver.cpp:312] Iteration 302600 (6.59802 iter/s, 15.1561s/100 iter), loss = 1.11009
I0815 09:03:03.587654 11101 solver.cpp:334]     Train net output #0: loss = 1.51783 (* 1 = 1.51783 loss)
I0815 09:03:03.587662 11101 sgd_solver.cpp:136] Iteration 302600, lr = 0.0054375, m = 0.9
I0815 09:03:18.719676 11101 solver.cpp:312] Iteration 302700 (6.60865 iter/s, 15.1317s/100 iter), loss = 1.45569
I0815 09:03:18.719740 11101 solver.cpp:334]     Train net output #0: loss = 1.52355 (* 1 = 1.52355 loss)
I0815 09:03:18.719758 11101 sgd_solver.cpp:136] Iteration 302700, lr = 0.00540625, m = 0.9
I0815 09:03:33.983145 11101 solver.cpp:312] Iteration 302800 (6.55177 iter/s, 15.263s/100 iter), loss = 1.07036
I0815 09:03:33.983212 11101 solver.cpp:334]     Train net output #0: loss = 1.11227 (* 1 = 1.11227 loss)
I0815 09:03:33.983219 11101 sgd_solver.cpp:136] Iteration 302800, lr = 0.005375, m = 0.9
I0815 09:03:48.972226 11101 solver.cpp:312] Iteration 302900 (6.67171 iter/s, 14.9887s/100 iter), loss = 1.58563
I0815 09:03:48.974445 11101 solver.cpp:334]     Train net output #0: loss = 2.0511 (* 1 = 2.0511 loss)
I0815 09:03:48.974458 11101 sgd_solver.cpp:136] Iteration 302900, lr = 0.00534375, m = 0.9
I0815 09:04:03.736639 11101 solver.cpp:312] Iteration 303000 (6.77323 iter/s, 14.764s/100 iter), loss = 1.07947
I0815 09:04:03.736663 11101 solver.cpp:334]     Train net output #0: loss = 1.22435 (* 1 = 1.22435 loss)
I0815 09:04:03.736668 11101 sgd_solver.cpp:136] Iteration 303000, lr = 0.0053125, m = 0.9
I0815 09:04:18.990841 11101 solver.cpp:312] Iteration 303100 (6.55575 iter/s, 15.2538s/100 iter), loss = 1.04119
I0815 09:04:18.992178 11101 solver.cpp:334]     Train net output #0: loss = 0.928648 (* 1 = 0.928648 loss)
I0815 09:04:18.992193 11101 sgd_solver.cpp:136] Iteration 303100, lr = 0.00528125, m = 0.9
I0815 09:04:34.341975 11101 solver.cpp:312] Iteration 303200 (6.51436 iter/s, 15.3507s/100 iter), loss = 1.30238
I0815 09:04:34.342003 11101 solver.cpp:334]     Train net output #0: loss = 1.19539 (* 1 = 1.19539 loss)
I0815 09:04:34.342010 11101 sgd_solver.cpp:136] Iteration 303200, lr = 0.00525, m = 0.9
I0815 09:04:50.123548 11101 solver.cpp:312] Iteration 303300 (6.33668 iter/s, 15.7811s/100 iter), loss = 1.89714
I0815 09:04:50.123798 11101 solver.cpp:334]     Train net output #0: loss = 1.94808 (* 1 = 1.94808 loss)
I0815 09:04:50.123818 11101 sgd_solver.cpp:136] Iteration 303300, lr = 0.00521875, m = 0.9
I0815 09:05:04.829905 11101 solver.cpp:312] Iteration 303400 (6.79997 iter/s, 14.706s/100 iter), loss = 1.44791
I0815 09:05:04.829980 11101 solver.cpp:334]     Train net output #0: loss = 1.53898 (* 1 = 1.53898 loss)
I0815 09:05:04.830000 11101 sgd_solver.cpp:136] Iteration 303400, lr = 0.0051875, m = 0.9
I0815 09:05:19.849514 11101 solver.cpp:312] Iteration 303500 (6.65815 iter/s, 15.0192s/100 iter), loss = 1.57441
I0815 09:05:19.849539 11101 solver.cpp:334]     Train net output #0: loss = 1.5867 (* 1 = 1.5867 loss)
I0815 09:05:19.849545 11101 sgd_solver.cpp:136] Iteration 303500, lr = 0.00515625, m = 0.9
I0815 09:05:35.126068 11101 solver.cpp:312] Iteration 303600 (6.54616 iter/s, 15.2761s/100 iter), loss = 1.60187
I0815 09:05:35.126134 11101 solver.cpp:334]     Train net output #0: loss = 1.50475 (* 1 = 1.50475 loss)
I0815 09:05:35.126142 11101 sgd_solver.cpp:136] Iteration 303600, lr = 0.005125, m = 0.9
I0815 09:05:50.180344 11101 solver.cpp:312] Iteration 303700 (6.64281 iter/s, 15.0539s/100 iter), loss = 1.14366
I0815 09:05:50.180373 11101 solver.cpp:334]     Train net output #0: loss = 0.943634 (* 1 = 0.943634 loss)
I0815 09:05:50.180379 11101 sgd_solver.cpp:136] Iteration 303700, lr = 0.00509375, m = 0.9
I0815 09:06:05.157958 11101 solver.cpp:312] Iteration 303800 (6.67681 iter/s, 14.9772s/100 iter), loss = 1.33644
I0815 09:06:05.158382 11101 solver.cpp:334]     Train net output #0: loss = 1.50368 (* 1 = 1.50368 loss)
I0815 09:06:05.158399 11101 sgd_solver.cpp:136] Iteration 303800, lr = 0.0050625, m = 0.9
I0815 09:06:19.823071 11101 solver.cpp:312] Iteration 303900 (6.81909 iter/s, 14.6647s/100 iter), loss = 1.16789
I0815 09:06:19.823101 11101 solver.cpp:334]     Train net output #0: loss = 0.715244 (* 1 = 0.715244 loss)
I0815 09:06:19.823107 11101 sgd_solver.cpp:136] Iteration 303900, lr = 0.00503125, m = 0.9
I0815 09:06:34.885658 11101 solver.cpp:509] Iteration 304000, Testing net (#0)
I0815 09:06:51.597138 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:06:58.098305 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.580177
I0815 09:06:58.098330 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812056
I0815 09:06:58.098338 11101 solver.cpp:594]     Test net output #2: loss = 1.81129 (* 1 = 1.81129 loss)
I0815 09:06:58.098387 11101 solver.cpp:264] [MultiGPU] Tests completed in 23.2121s
I0815 09:06:58.247465 11101 solver.cpp:312] Iteration 304000 (2.60258 iter/s, 38.4233s/100 iter), loss = 0.914315
I0815 09:06:58.247493 11101 solver.cpp:334]     Train net output #0: loss = 0.728111 (* 1 = 0.728111 loss)
I0815 09:06:58.247500 11101 sgd_solver.cpp:136] Iteration 304000, lr = 0.005, m = 0.9
I0815 09:07:13.120196 11101 solver.cpp:312] Iteration 304100 (6.7239 iter/s, 14.8723s/100 iter), loss = 1.36312
I0815 09:07:13.120247 11101 solver.cpp:334]     Train net output #0: loss = 1.41292 (* 1 = 1.41292 loss)
I0815 09:07:13.120280 11101 sgd_solver.cpp:136] Iteration 304100, lr = 0.00496875, m = 0.9
I0815 09:07:28.198282 11101 solver.cpp:312] Iteration 304200 (6.63233 iter/s, 15.0777s/100 iter), loss = 1.32249
I0815 09:07:28.198355 11101 solver.cpp:334]     Train net output #0: loss = 1.1156 (* 1 = 1.1156 loss)
I0815 09:07:28.198366 11101 sgd_solver.cpp:136] Iteration 304200, lr = 0.0049375, m = 0.9
I0815 09:07:43.689785 11101 solver.cpp:312] Iteration 304300 (6.45533 iter/s, 15.4911s/100 iter), loss = 1.55319
I0815 09:07:43.689810 11101 solver.cpp:334]     Train net output #0: loss = 1.63802 (* 1 = 1.63802 loss)
I0815 09:07:43.689816 11101 sgd_solver.cpp:136] Iteration 304300, lr = 0.00490625, m = 0.9
I0815 09:07:59.144086 11101 solver.cpp:312] Iteration 304400 (6.47087 iter/s, 15.4539s/100 iter), loss = 1.41189
I0815 09:07:59.144150 11101 solver.cpp:334]     Train net output #0: loss = 1.38927 (* 1 = 1.38927 loss)
I0815 09:07:59.144156 11101 sgd_solver.cpp:136] Iteration 304400, lr = 0.004875, m = 0.9
I0815 09:08:14.178792 11101 solver.cpp:312] Iteration 304500 (6.65146 iter/s, 15.0343s/100 iter), loss = 1.35202
I0815 09:08:14.178820 11101 solver.cpp:334]     Train net output #0: loss = 1.03403 (* 1 = 1.03403 loss)
I0815 09:08:14.178824 11101 sgd_solver.cpp:136] Iteration 304500, lr = 0.00484375, m = 0.9
I0815 09:08:29.267612 11101 solver.cpp:312] Iteration 304600 (6.62761 iter/s, 15.0884s/100 iter), loss = 1.01
I0815 09:08:29.267675 11101 solver.cpp:334]     Train net output #0: loss = 1.18102 (* 1 = 1.18102 loss)
I0815 09:08:29.267681 11101 sgd_solver.cpp:136] Iteration 304600, lr = 0.0048125, m = 0.9
I0815 09:08:44.087864 11101 solver.cpp:312] Iteration 304700 (6.74771 iter/s, 14.8198s/100 iter), loss = 1.35108
I0815 09:08:44.087937 11101 solver.cpp:334]     Train net output #0: loss = 1.35013 (* 1 = 1.35013 loss)
I0815 09:08:44.087956 11101 sgd_solver.cpp:136] Iteration 304700, lr = 0.00478125, m = 0.9
I0815 09:08:59.293171 11101 solver.cpp:312] Iteration 304800 (6.57683 iter/s, 15.2049s/100 iter), loss = 1.25326
I0815 09:08:59.293261 11101 solver.cpp:334]     Train net output #0: loss = 1.31016 (* 1 = 1.31016 loss)
I0815 09:08:59.293277 11101 sgd_solver.cpp:136] Iteration 304800, lr = 0.00475, m = 0.9
I0815 09:09:14.977847 11101 solver.cpp:312] Iteration 304900 (6.37583 iter/s, 15.6842s/100 iter), loss = 1.05244
I0815 09:09:14.977871 11101 solver.cpp:334]     Train net output #0: loss = 0.983572 (* 1 = 0.983572 loss)
I0815 09:09:14.977875 11101 sgd_solver.cpp:136] Iteration 304900, lr = 0.00471875, m = 0.9
I0815 09:09:30.609102 11101 solver.cpp:312] Iteration 305000 (6.39762 iter/s, 15.6308s/100 iter), loss = 0.866667
I0815 09:09:30.612282 11101 solver.cpp:334]     Train net output #0: loss = 0.903294 (* 1 = 0.903294 loss)
I0815 09:09:30.612294 11101 sgd_solver.cpp:136] Iteration 305000, lr = 0.0046875, m = 0.9
I0815 09:09:45.443660 11101 solver.cpp:312] Iteration 305100 (6.7412 iter/s, 14.8341s/100 iter), loss = 1.29684
I0815 09:09:45.443732 11101 solver.cpp:334]     Train net output #0: loss = 1.42337 (* 1 = 1.42337 loss)
I0815 09:09:45.443752 11101 sgd_solver.cpp:136] Iteration 305100, lr = 0.00465625, m = 0.9
I0815 09:10:00.397858 11101 solver.cpp:312] Iteration 305200 (6.68727 iter/s, 14.9538s/100 iter), loss = 1.12717
I0815 09:10:00.397886 11101 solver.cpp:334]     Train net output #0: loss = 1.11428 (* 1 = 1.11428 loss)
I0815 09:10:00.397892 11101 sgd_solver.cpp:136] Iteration 305200, lr = 0.004625, m = 0.9
I0815 09:10:15.492954 11101 solver.cpp:312] Iteration 305300 (6.62485 iter/s, 15.0947s/100 iter), loss = 1.38714
I0815 09:10:15.495440 11101 solver.cpp:334]     Train net output #0: loss = 1.44841 (* 1 = 1.44841 loss)
I0815 09:10:15.495457 11101 sgd_solver.cpp:136] Iteration 305300, lr = 0.00459375, m = 0.9
I0815 09:10:30.722061 11101 solver.cpp:312] Iteration 305400 (6.56655 iter/s, 15.2287s/100 iter), loss = 1.34691
I0815 09:10:30.722088 11101 solver.cpp:334]     Train net output #0: loss = 1.20713 (* 1 = 1.20713 loss)
I0815 09:10:30.722095 11101 sgd_solver.cpp:136] Iteration 305400, lr = 0.0045625, m = 0.9
I0815 09:10:45.792521 11101 solver.cpp:312] Iteration 305500 (6.63568 iter/s, 15.07s/100 iter), loss = 1.39871
I0815 09:10:45.792618 11101 solver.cpp:334]     Train net output #0: loss = 1.06048 (* 1 = 1.06048 loss)
I0815 09:10:45.792632 11101 sgd_solver.cpp:136] Iteration 305500, lr = 0.00453125, m = 0.9
I0815 09:11:01.025816 11101 solver.cpp:312] Iteration 305600 (6.56475 iter/s, 15.2329s/100 iter), loss = 1.71801
I0815 09:11:01.025866 11101 solver.cpp:334]     Train net output #0: loss = 1.82662 (* 1 = 1.82662 loss)
I0815 09:11:01.025879 11101 sgd_solver.cpp:136] Iteration 305600, lr = 0.0045, m = 0.9
I0815 09:11:15.962419 11101 solver.cpp:312] Iteration 305700 (6.69515 iter/s, 14.9362s/100 iter), loss = 1.2539
I0815 09:11:15.962512 11101 solver.cpp:334]     Train net output #0: loss = 1.16028 (* 1 = 1.16028 loss)
I0815 09:11:15.962530 11101 sgd_solver.cpp:136] Iteration 305700, lr = 0.00446875, m = 0.9
I0815 09:11:31.352149 11101 solver.cpp:312] Iteration 305800 (6.49802 iter/s, 15.3893s/100 iter), loss = 1.27741
I0815 09:11:31.352177 11101 solver.cpp:334]     Train net output #0: loss = 1.50995 (* 1 = 1.50995 loss)
I0815 09:11:31.352183 11101 sgd_solver.cpp:136] Iteration 305800, lr = 0.0044375, m = 0.9
I0815 09:11:46.395421 11101 solver.cpp:312] Iteration 305900 (6.64768 iter/s, 15.0429s/100 iter), loss = 1.38224
I0815 09:11:46.395529 11101 solver.cpp:334]     Train net output #0: loss = 1.07715 (* 1 = 1.07715 loss)
I0815 09:11:46.395546 11101 sgd_solver.cpp:136] Iteration 305900, lr = 0.00440625, m = 0.9
I0815 09:12:01.114678 11101 solver.cpp:509] Iteration 306000, Testing net (#0)
I0815 09:12:24.612382 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.586412
I0815 09:12:24.612440 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817703
I0815 09:12:24.612447 11101 solver.cpp:594]     Test net output #2: loss = 1.80876 (* 1 = 1.80876 loss)
I0815 09:12:24.612464 11101 solver.cpp:264] [MultiGPU] Tests completed in 23.4992s
I0815 09:12:24.804800 11101 solver.cpp:312] Iteration 306000 (2.6036 iter/s, 38.4083s/100 iter), loss = 1.2878
I0815 09:12:24.804841 11101 solver.cpp:334]     Train net output #0: loss = 1.35017 (* 1 = 1.35017 loss)
I0815 09:12:24.804853 11101 sgd_solver.cpp:136] Iteration 306000, lr = 0.004375, m = 0.9
I0815 09:12:39.886340 11101 solver.cpp:312] Iteration 306100 (6.63081 iter/s, 15.0811s/100 iter), loss = 1.36836
I0815 09:12:39.886523 11101 solver.cpp:334]     Train net output #0: loss = 1.38432 (* 1 = 1.38432 loss)
I0815 09:12:39.886607 11101 sgd_solver.cpp:136] Iteration 306100, lr = 0.00434375, m = 0.9
I0815 09:12:54.867527 11101 solver.cpp:312] Iteration 306200 (6.67523 iter/s, 14.9808s/100 iter), loss = 1.3055
I0815 09:12:54.867588 11101 solver.cpp:334]     Train net output #0: loss = 1.30938 (* 1 = 1.30938 loss)
I0815 09:12:54.867594 11101 sgd_solver.cpp:136] Iteration 306200, lr = 0.0043125, m = 0.9
I0815 09:13:09.894512 11101 solver.cpp:312] Iteration 306300 (6.65488 iter/s, 15.0266s/100 iter), loss = 1.1805
I0815 09:13:09.894534 11101 solver.cpp:334]     Train net output #0: loss = 1.27016 (* 1 = 1.27016 loss)
I0815 09:13:09.894539 11101 sgd_solver.cpp:136] Iteration 306300, lr = 0.00428125, m = 0.9
I0815 09:13:24.875735 11101 solver.cpp:312] Iteration 306400 (6.67521 iter/s, 14.9808s/100 iter), loss = 1.16555
I0815 09:13:24.876026 11101 solver.cpp:334]     Train net output #0: loss = 1.27648 (* 1 = 1.27648 loss)
I0815 09:13:24.876152 11101 sgd_solver.cpp:136] Iteration 306400, lr = 0.00425, m = 0.9
I0815 09:13:39.843677 11101 solver.cpp:312] Iteration 306500 (6.68113 iter/s, 14.9675s/100 iter), loss = 1.04471
I0815 09:13:39.843719 11101 solver.cpp:334]     Train net output #0: loss = 0.874588 (* 1 = 0.874588 loss)
I0815 09:13:39.843727 11101 sgd_solver.cpp:136] Iteration 306500, lr = 0.00421875, m = 0.9
I0815 09:13:54.714543 11101 solver.cpp:312] Iteration 306600 (6.72474 iter/s, 14.8705s/100 iter), loss = 1.62249
I0815 09:13:54.714617 11101 solver.cpp:334]     Train net output #0: loss = 1.72572 (* 1 = 1.72572 loss)
I0815 09:13:54.714644 11101 sgd_solver.cpp:136] Iteration 306600, lr = 0.0041875, m = 0.9
I0815 09:14:09.719236 11101 solver.cpp:312] Iteration 306700 (6.66477 iter/s, 15.0043s/100 iter), loss = 1.32009
I0815 09:14:09.719312 11101 solver.cpp:334]     Train net output #0: loss = 1.50042 (* 1 = 1.50042 loss)
I0815 09:14:09.719321 11101 sgd_solver.cpp:136] Iteration 306700, lr = 0.00415625, m = 0.9
I0815 09:14:24.729640 11101 solver.cpp:312] Iteration 306800 (6.66223 iter/s, 15.01s/100 iter), loss = 1.4252
I0815 09:14:24.729701 11101 solver.cpp:334]     Train net output #0: loss = 1.25004 (* 1 = 1.25004 loss)
I0815 09:14:24.729719 11101 sgd_solver.cpp:136] Iteration 306800, lr = 0.004125, m = 0.9
I0815 09:14:39.740723 11101 solver.cpp:312] Iteration 306900 (6.66193 iter/s, 15.0107s/100 iter), loss = 1.28017
I0815 09:14:39.740828 11101 solver.cpp:334]     Train net output #0: loss = 1.22212 (* 1 = 1.22212 loss)
I0815 09:14:39.740842 11101 sgd_solver.cpp:136] Iteration 306900, lr = 0.00409375, m = 0.9
I0815 09:14:54.665153 11101 solver.cpp:312] Iteration 307000 (6.70061 iter/s, 14.924s/100 iter), loss = 1.45891
I0815 09:14:54.665180 11101 solver.cpp:334]     Train net output #0: loss = 1.23545 (* 1 = 1.23545 loss)
I0815 09:14:54.665185 11101 sgd_solver.cpp:136] Iteration 307000, lr = 0.0040625, m = 0.9
I0815 09:15:09.652199 11101 solver.cpp:312] Iteration 307100 (6.67261 iter/s, 14.9866s/100 iter), loss = 1.95752
I0815 09:15:09.652227 11101 solver.cpp:334]     Train net output #0: loss = 1.56812 (* 1 = 1.56812 loss)
I0815 09:15:09.652233 11101 sgd_solver.cpp:136] Iteration 307100, lr = 0.00403125, m = 0.9
I0815 09:15:24.378013 11101 solver.cpp:312] Iteration 307200 (6.79098 iter/s, 14.7254s/100 iter), loss = 1.44302
I0815 09:15:24.378100 11101 solver.cpp:334]     Train net output #0: loss = 1.50853 (* 1 = 1.50853 loss)
I0815 09:15:24.378108 11101 sgd_solver.cpp:136] Iteration 307200, lr = 0.004, m = 0.9
I0815 09:15:39.380437 11101 solver.cpp:312] Iteration 307300 (6.66577 iter/s, 15.002s/100 iter), loss = 1.40499
I0815 09:15:39.380509 11101 solver.cpp:334]     Train net output #0: loss = 1.20896 (* 1 = 1.20896 loss)
I0815 09:15:39.380527 11101 sgd_solver.cpp:136] Iteration 307300, lr = 0.00396875, m = 0.9
I0815 09:15:54.100196 11101 solver.cpp:312] Iteration 307400 (6.79378 iter/s, 14.7194s/100 iter), loss = 1.13606
I0815 09:15:54.100236 11101 solver.cpp:334]     Train net output #0: loss = 1.46904 (* 1 = 1.46904 loss)
I0815 09:15:54.100244 11101 sgd_solver.cpp:136] Iteration 307400, lr = 0.0039375, m = 0.9
I0815 09:16:08.702915 11101 solver.cpp:312] Iteration 307500 (6.84823 iter/s, 14.6023s/100 iter), loss = 1.07464
I0815 09:16:08.702976 11101 solver.cpp:334]     Train net output #0: loss = 1.26228 (* 1 = 1.26228 loss)
I0815 09:16:08.702983 11101 sgd_solver.cpp:136] Iteration 307500, lr = 0.00390625, m = 0.9
I0815 09:16:23.286262 11101 solver.cpp:312] Iteration 307600 (6.85733 iter/s, 14.5829s/100 iter), loss = 1.26329
I0815 09:16:23.286288 11101 solver.cpp:334]     Train net output #0: loss = 1.41406 (* 1 = 1.41406 loss)
I0815 09:16:23.286293 11101 sgd_solver.cpp:136] Iteration 307600, lr = 0.003875, m = 0.9
I0815 09:16:38.018386 11101 solver.cpp:312] Iteration 307700 (6.78807 iter/s, 14.7317s/100 iter), loss = 1.46877
I0815 09:16:38.018414 11101 solver.cpp:334]     Train net output #0: loss = 1.10904 (* 1 = 1.10904 loss)
I0815 09:16:38.018420 11101 sgd_solver.cpp:136] Iteration 307700, lr = 0.00384375, m = 0.9
I0815 09:16:53.036059 11101 solver.cpp:312] Iteration 307800 (6.65901 iter/s, 15.0172s/100 iter), loss = 1.17753
I0815 09:16:53.036175 11101 solver.cpp:334]     Train net output #0: loss = 1.11643 (* 1 = 1.11643 loss)
I0815 09:16:53.036200 11101 sgd_solver.cpp:136] Iteration 307800, lr = 0.0038125, m = 0.9
I0815 09:17:07.769353 11101 solver.cpp:312] Iteration 307900 (6.78753 iter/s, 14.7329s/100 iter), loss = 1.33596
I0815 09:17:07.769407 11101 solver.cpp:334]     Train net output #0: loss = 1.38503 (* 1 = 1.38503 loss)
I0815 09:17:07.769419 11101 sgd_solver.cpp:136] Iteration 307900, lr = 0.00378125, m = 0.9
I0815 09:17:22.403947 11101 solver.cpp:509] Iteration 308000, Testing net (#0)
I0815 09:17:44.344354 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.589765
I0815 09:17:44.344471 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.821232
I0815 09:17:44.344480 11101 solver.cpp:594]     Test net output #2: loss = 1.76524 (* 1 = 1.76524 loss)
I0815 09:17:44.344499 11101 solver.cpp:264] [MultiGPU] Tests completed in 21.94s
I0815 09:17:44.492671 11101 solver.cpp:312] Iteration 308000 (2.72314 iter/s, 36.7223s/100 iter), loss = 1.12173
I0815 09:17:44.492694 11101 solver.cpp:334]     Train net output #0: loss = 0.966677 (* 1 = 0.966677 loss)
I0815 09:17:44.492698 11101 sgd_solver.cpp:136] Iteration 308000, lr = 0.00375, m = 0.9
I0815 09:17:59.359210 11101 solver.cpp:312] Iteration 308100 (6.7267 iter/s, 14.8661s/100 iter), loss = 1.22439
I0815 09:17:59.359236 11101 solver.cpp:334]     Train net output #0: loss = 1.25921 (* 1 = 1.25921 loss)
I0815 09:17:59.359242 11101 sgd_solver.cpp:136] Iteration 308100, lr = 0.00371875, m = 0.9
I0815 09:18:14.320878 11101 solver.cpp:312] Iteration 308200 (6.68394 iter/s, 14.9612s/100 iter), loss = 1.26618
I0815 09:18:14.320932 11101 solver.cpp:334]     Train net output #0: loss = 1.14072 (* 1 = 1.14072 loss)
I0815 09:18:14.320947 11101 sgd_solver.cpp:136] Iteration 308200, lr = 0.0036875, m = 0.9
I0815 09:18:29.268923 11101 solver.cpp:312] Iteration 308300 (6.69002 iter/s, 14.9476s/100 iter), loss = 0.981826
I0815 09:18:29.269186 11101 solver.cpp:334]     Train net output #0: loss = 1.1403 (* 1 = 1.1403 loss)
I0815 09:18:29.269294 11101 sgd_solver.cpp:136] Iteration 308300, lr = 0.00365625, m = 0.9
I0815 09:18:44.772344 11101 solver.cpp:312] Iteration 308400 (6.45037 iter/s, 15.503s/100 iter), loss = 0.837437
I0815 09:18:44.772370 11101 solver.cpp:334]     Train net output #0: loss = 0.574464 (* 1 = 0.574464 loss)
I0815 09:18:44.772374 11101 sgd_solver.cpp:136] Iteration 308400, lr = 0.003625, m = 0.9
I0815 09:19:00.592154 11101 solver.cpp:312] Iteration 308500 (6.32136 iter/s, 15.8194s/100 iter), loss = 1.52459
I0815 09:19:00.592211 11101 solver.cpp:334]     Train net output #0: loss = 1.46384 (* 1 = 1.46384 loss)
I0815 09:19:00.592219 11101 sgd_solver.cpp:136] Iteration 308500, lr = 0.00359375, m = 0.9
I0815 09:19:15.960268 11101 solver.cpp:312] Iteration 308600 (6.50716 iter/s, 15.3677s/100 iter), loss = 1.54022
I0815 09:19:15.960294 11101 solver.cpp:334]     Train net output #0: loss = 1.80734 (* 1 = 1.80734 loss)
I0815 09:19:15.960299 11101 sgd_solver.cpp:136] Iteration 308600, lr = 0.0035625, m = 0.9
I0815 09:19:31.148183 11101 solver.cpp:312] Iteration 308700 (6.58437 iter/s, 15.1875s/100 iter), loss = 1.20125
I0815 09:19:31.148771 11101 solver.cpp:334]     Train net output #0: loss = 1.33822 (* 1 = 1.33822 loss)
I0815 09:19:31.148795 11101 sgd_solver.cpp:136] Iteration 308700, lr = 0.00353125, m = 0.9
I0815 09:19:46.308162 11101 solver.cpp:312] Iteration 308800 (6.5965 iter/s, 15.1596s/100 iter), loss = 1.81255
I0815 09:19:46.308251 11101 solver.cpp:334]     Train net output #0: loss = 1.98382 (* 1 = 1.98382 loss)
I0815 09:19:46.308277 11101 sgd_solver.cpp:136] Iteration 308800, lr = 0.0035, m = 0.9
I0815 09:20:01.399081 11101 solver.cpp:312] Iteration 308900 (6.62668 iter/s, 15.0905s/100 iter), loss = 1.42757
I0815 09:20:01.399157 11101 solver.cpp:334]     Train net output #0: loss = 1.32764 (* 1 = 1.32764 loss)
I0815 09:20:01.399168 11101 sgd_solver.cpp:136] Iteration 308900, lr = 0.00346875, m = 0.9
I0815 09:20:16.734521 11101 solver.cpp:312] Iteration 309000 (6.52102 iter/s, 15.335s/100 iter), loss = 1.23162
I0815 09:20:16.734585 11101 solver.cpp:334]     Train net output #0: loss = 1.09735 (* 1 = 1.09735 loss)
I0815 09:20:16.734602 11101 sgd_solver.cpp:136] Iteration 309000, lr = 0.0034375, m = 0.9
I0815 09:20:31.652797 11101 solver.cpp:312] Iteration 309100 (6.70337 iter/s, 14.9179s/100 iter), loss = 1.24842
I0815 09:20:31.652914 11101 solver.cpp:334]     Train net output #0: loss = 1.63566 (* 1 = 1.63566 loss)
I0815 09:20:31.652935 11101 sgd_solver.cpp:136] Iteration 309100, lr = 0.00340625, m = 0.9
I0815 09:20:47.174695 11101 solver.cpp:312] Iteration 309200 (6.44269 iter/s, 15.5215s/100 iter), loss = 1.66282
I0815 09:20:47.174731 11101 solver.cpp:334]     Train net output #0: loss = 1.86351 (* 1 = 1.86351 loss)
I0815 09:20:47.174736 11101 sgd_solver.cpp:136] Iteration 309200, lr = 0.003375, m = 0.9
I0815 09:21:02.701764 11101 solver.cpp:312] Iteration 309300 (6.44054 iter/s, 15.5266s/100 iter), loss = 1.33386
I0815 09:21:02.701831 11101 solver.cpp:334]     Train net output #0: loss = 1.65791 (* 1 = 1.65791 loss)
I0815 09:21:02.701838 11101 sgd_solver.cpp:136] Iteration 309300, lr = 0.00334375, m = 0.9
I0815 09:21:17.997050 11101 solver.cpp:312] Iteration 309400 (6.53814 iter/s, 15.2949s/100 iter), loss = 1.25764
I0815 09:21:17.997076 11101 solver.cpp:334]     Train net output #0: loss = 1.36314 (* 1 = 1.36314 loss)
I0815 09:21:17.997083 11101 sgd_solver.cpp:136] Iteration 309400, lr = 0.0033125, m = 0.9
I0815 09:21:33.410607 11101 solver.cpp:312] Iteration 309500 (6.48798 iter/s, 15.4131s/100 iter), loss = 1.17216
I0815 09:21:33.410696 11101 solver.cpp:334]     Train net output #0: loss = 1.13485 (* 1 = 1.13485 loss)
I0815 09:21:33.410706 11101 sgd_solver.cpp:136] Iteration 309500, lr = 0.00328125, m = 0.9
I0815 09:21:48.472681 11101 solver.cpp:312] Iteration 309600 (6.63937 iter/s, 15.0617s/100 iter), loss = 1.19358
I0815 09:21:48.472745 11101 solver.cpp:334]     Train net output #0: loss = 1.09453 (* 1 = 1.09453 loss)
I0815 09:21:48.472764 11101 sgd_solver.cpp:136] Iteration 309600, lr = 0.00325, m = 0.9
I0815 09:22:03.522356 11101 solver.cpp:312] Iteration 309700 (6.64484 iter/s, 15.0493s/100 iter), loss = 1.41249
I0815 09:22:03.522413 11101 solver.cpp:334]     Train net output #0: loss = 1.52104 (* 1 = 1.52104 loss)
I0815 09:22:03.522420 11101 sgd_solver.cpp:136] Iteration 309700, lr = 0.00321875, m = 0.9
I0815 09:22:19.336496 11101 solver.cpp:312] Iteration 309800 (6.32363 iter/s, 15.8137s/100 iter), loss = 1.6522
I0815 09:22:19.336546 11101 solver.cpp:334]     Train net output #0: loss = 1.61927 (* 1 = 1.61927 loss)
I0815 09:22:19.336561 11101 sgd_solver.cpp:136] Iteration 309800, lr = 0.0031875, m = 0.9
I0815 09:22:34.772163 11101 solver.cpp:312] Iteration 309900 (6.47868 iter/s, 15.4352s/100 iter), loss = 1.28226
I0815 09:22:34.772220 11101 solver.cpp:334]     Train net output #0: loss = 0.829913 (* 1 = 0.829913 loss)
I0815 09:22:34.772228 11101 sgd_solver.cpp:136] Iteration 309900, lr = 0.00315625, m = 0.9
I0815 09:22:49.688650 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_310000.caffemodel
I0815 09:22:49.966343 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_310000.solverstate
I0815 09:22:49.972059 11101 solver.cpp:509] Iteration 310000, Testing net (#0)
I0815 09:22:57.594815 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:23:12.760649 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.591352
I0815 09:23:12.760707 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819115
I0815 09:23:12.760716 11101 solver.cpp:594]     Test net output #2: loss = 1.76892 (* 1 = 1.76892 loss)
I0815 09:23:12.760772 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.7881s
I0815 09:23:12.934756 11101 solver.cpp:312] Iteration 310000 (2.62044 iter/s, 38.1615s/100 iter), loss = 1.31071
I0815 09:23:12.934826 11101 solver.cpp:334]     Train net output #0: loss = 1.50223 (* 1 = 1.50223 loss)
I0815 09:23:12.934845 11101 sgd_solver.cpp:136] Iteration 310000, lr = 0.003125, m = 0.9
I0815 09:23:28.349297 11101 solver.cpp:312] Iteration 310100 (6.48756 iter/s, 15.4141s/100 iter), loss = 1.57181
I0815 09:23:28.349323 11101 solver.cpp:334]     Train net output #0: loss = 1.67932 (* 1 = 1.67932 loss)
I0815 09:23:28.349328 11101 sgd_solver.cpp:136] Iteration 310100, lr = 0.00309375, m = 0.9
I0815 09:23:43.939574 11101 solver.cpp:312] Iteration 310200 (6.41443 iter/s, 15.5898s/100 iter), loss = 1.33696
I0815 09:23:43.939669 11101 solver.cpp:334]     Train net output #0: loss = 1.59871 (* 1 = 1.59871 loss)
I0815 09:23:43.939677 11101 sgd_solver.cpp:136] Iteration 310200, lr = 0.0030625, m = 0.9
I0815 09:23:59.506759 11101 solver.cpp:312] Iteration 310300 (6.42395 iter/s, 15.5668s/100 iter), loss = 1.44621
I0815 09:23:59.506795 11101 solver.cpp:334]     Train net output #0: loss = 1.26842 (* 1 = 1.26842 loss)
I0815 09:23:59.506801 11101 sgd_solver.cpp:136] Iteration 310300, lr = 0.00303125, m = 0.9
I0815 09:24:14.913570 11101 solver.cpp:312] Iteration 310400 (6.49082 iter/s, 15.4064s/100 iter), loss = 1.07423
I0815 09:24:14.913625 11101 solver.cpp:334]     Train net output #0: loss = 1.04255 (* 1 = 1.04255 loss)
I0815 09:24:14.913631 11101 sgd_solver.cpp:136] Iteration 310400, lr = 0.003, m = 0.9
I0815 09:24:30.110025 11101 solver.cpp:312] Iteration 310500 (6.58066 iter/s, 15.196s/100 iter), loss = 1.39791
I0815 09:24:30.110054 11101 solver.cpp:334]     Train net output #0: loss = 1.0376 (* 1 = 1.0376 loss)
I0815 09:24:30.110059 11101 sgd_solver.cpp:136] Iteration 310500, lr = 0.00296875, m = 0.9
I0815 09:24:45.229033 11101 solver.cpp:312] Iteration 310600 (6.61437 iter/s, 15.1186s/100 iter), loss = 1.00665
I0815 09:24:45.229266 11101 solver.cpp:334]     Train net output #0: loss = 1.21892 (* 1 = 1.21892 loss)
I0815 09:24:45.229282 11101 sgd_solver.cpp:136] Iteration 310600, lr = 0.0029375, m = 0.9
I0815 09:25:00.444149 11101 solver.cpp:312] Iteration 310700 (6.57259 iter/s, 15.2147s/100 iter), loss = 1.40967
I0815 09:25:00.444180 11101 solver.cpp:334]     Train net output #0: loss = 1.08174 (* 1 = 1.08174 loss)
I0815 09:25:00.444186 11101 sgd_solver.cpp:136] Iteration 310700, lr = 0.00290625, m = 0.9
I0815 09:25:15.720284 11101 solver.cpp:312] Iteration 310800 (6.54634 iter/s, 15.2757s/100 iter), loss = 1.48026
I0815 09:25:15.720347 11101 solver.cpp:334]     Train net output #0: loss = 1.5155 (* 1 = 1.5155 loss)
I0815 09:25:15.720352 11101 sgd_solver.cpp:136] Iteration 310800, lr = 0.002875, m = 0.9
I0815 09:25:31.021699 11101 solver.cpp:312] Iteration 310900 (6.53552 iter/s, 15.301s/100 iter), loss = 1.16307
I0815 09:25:31.021723 11101 solver.cpp:334]     Train net output #0: loss = 1.27691 (* 1 = 1.27691 loss)
I0815 09:25:31.021728 11101 sgd_solver.cpp:136] Iteration 310900, lr = 0.00284375, m = 0.9
I0815 09:25:46.319777 11101 solver.cpp:312] Iteration 311000 (6.53695 iter/s, 15.2977s/100 iter), loss = 1.12235
I0815 09:25:46.319880 11101 solver.cpp:334]     Train net output #0: loss = 0.938069 (* 1 = 0.938069 loss)
I0815 09:25:46.319900 11101 sgd_solver.cpp:136] Iteration 311000, lr = 0.0028125, m = 0.9
I0815 09:26:01.254137 11101 solver.cpp:312] Iteration 311100 (6.69615 iter/s, 14.9339s/100 iter), loss = 1.34112
I0815 09:26:01.254163 11101 solver.cpp:334]     Train net output #0: loss = 1.54254 (* 1 = 1.54254 loss)
I0815 09:26:01.254169 11101 sgd_solver.cpp:136] Iteration 311100, lr = 0.00278125, m = 0.9
I0815 09:26:16.701900 11101 solver.cpp:312] Iteration 311200 (6.47361 iter/s, 15.4473s/100 iter), loss = 1.23792
I0815 09:26:16.702057 11101 solver.cpp:334]     Train net output #0: loss = 1.06738 (* 1 = 1.06738 loss)
I0815 09:26:16.702086 11101 sgd_solver.cpp:136] Iteration 311200, lr = 0.00275, m = 0.9
I0815 09:26:32.041971 11101 solver.cpp:312] Iteration 311300 (6.51905 iter/s, 15.3396s/100 iter), loss = 1.18288
I0815 09:26:32.042044 11101 solver.cpp:334]     Train net output #0: loss = 1.3377 (* 1 = 1.3377 loss)
I0815 09:26:32.042064 11101 sgd_solver.cpp:136] Iteration 311300, lr = 0.00271875, m = 0.9
I0815 09:26:47.275806 11101 solver.cpp:312] Iteration 311400 (6.56452 iter/s, 15.2334s/100 iter), loss = 1.25433
I0815 09:26:47.275862 11101 solver.cpp:334]     Train net output #0: loss = 1.4222 (* 1 = 1.4222 loss)
I0815 09:26:47.275867 11101 sgd_solver.cpp:136] Iteration 311400, lr = 0.0026875, m = 0.9
I0815 09:27:02.452522 11101 solver.cpp:312] Iteration 311500 (6.58922 iter/s, 15.1763s/100 iter), loss = 1.57011
I0815 09:27:02.452672 11101 solver.cpp:334]     Train net output #0: loss = 1.57771 (* 1 = 1.57771 loss)
I0815 09:27:02.452756 11101 sgd_solver.cpp:136] Iteration 311500, lr = 0.00265625, m = 0.9
I0815 09:27:17.563994 11101 solver.cpp:312] Iteration 311600 (6.61767 iter/s, 15.1111s/100 iter), loss = 1.28785
I0815 09:27:17.564074 11101 solver.cpp:334]     Train net output #0: loss = 1.2638 (* 1 = 1.2638 loss)
I0815 09:27:17.564080 11101 sgd_solver.cpp:136] Iteration 311600, lr = 0.002625, m = 0.9
I0815 09:27:32.906682 11101 solver.cpp:312] Iteration 311700 (6.51794 iter/s, 15.3423s/100 iter), loss = 1.64965
I0815 09:27:32.906703 11101 solver.cpp:334]     Train net output #0: loss = 2.10718 (* 1 = 2.10718 loss)
I0815 09:27:32.906708 11101 sgd_solver.cpp:136] Iteration 311700, lr = 0.00259375, m = 0.9
I0815 09:27:48.307777 11101 solver.cpp:312] Iteration 311800 (6.49322 iter/s, 15.4007s/100 iter), loss = 0.981369
I0815 09:27:48.307881 11101 solver.cpp:334]     Train net output #0: loss = 0.833355 (* 1 = 0.833355 loss)
I0815 09:27:48.307904 11101 sgd_solver.cpp:136] Iteration 311800, lr = 0.0025625, m = 0.9
I0815 09:28:03.516069 11101 solver.cpp:312] Iteration 311900 (6.57554 iter/s, 15.2079s/100 iter), loss = 1.51195
I0815 09:28:03.516098 11101 solver.cpp:334]     Train net output #0: loss = 1.76054 (* 1 = 1.76054 loss)
I0815 09:28:03.516104 11101 sgd_solver.cpp:136] Iteration 311900, lr = 0.00253125, m = 0.9
I0815 09:28:18.464910 11101 solver.cpp:509] Iteration 312000, Testing net (#0)
I0815 09:28:37.843510 11099 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 09:28:41.151789 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.595235
I0815 09:28:41.151829 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.82282
I0815 09:28:41.151844 11101 solver.cpp:594]     Test net output #2: loss = 1.74445 (* 1 = 1.74445 loss)
I0815 09:28:41.151888 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.6864s
I0815 09:28:41.364179 11101 solver.cpp:312] Iteration 312000 (2.64221 iter/s, 37.8471s/100 iter), loss = 1.6177
I0815 09:28:41.364261 11101 solver.cpp:334]     Train net output #0: loss = 1.51968 (* 1 = 1.51968 loss)
I0815 09:28:41.364282 11101 sgd_solver.cpp:136] Iteration 312000, lr = 0.0025, m = 0.9
I0815 09:28:56.680384 11101 solver.cpp:312] Iteration 312100 (6.52921 iter/s, 15.3158s/100 iter), loss = 1.23158
I0815 09:28:56.680450 11101 solver.cpp:334]     Train net output #0: loss = 1.11683 (* 1 = 1.11683 loss)
I0815 09:28:56.680467 11101 sgd_solver.cpp:136] Iteration 312100, lr = 0.00246875, m = 0.9
I0815 09:29:11.739178 11101 solver.cpp:312] Iteration 312200 (6.64082 iter/s, 15.0584s/100 iter), loss = 1.57631
I0815 09:29:11.739387 11101 solver.cpp:334]     Train net output #0: loss = 1.81657 (* 1 = 1.81657 loss)
I0815 09:29:11.739496 11101 sgd_solver.cpp:136] Iteration 312200, lr = 0.0024375, m = 0.9
I0815 09:29:26.822531 11101 solver.cpp:312] Iteration 312300 (6.63001 iter/s, 15.0829s/100 iter), loss = 1.26469
I0815 09:29:26.822778 11101 solver.cpp:334]     Train net output #0: loss = 1.1863 (* 1 = 1.1863 loss)
I0815 09:29:26.822790 11101 sgd_solver.cpp:136] Iteration 312300, lr = 0.00240625, m = 0.9
I0815 09:29:41.949815 11101 solver.cpp:312] Iteration 312400 (6.61076 iter/s, 15.1269s/100 iter), loss = 1.17428
I0815 09:29:41.949894 11101 solver.cpp:334]     Train net output #0: loss = 1.2554 (* 1 = 1.2554 loss)
I0815 09:29:41.949920 11101 sgd_solver.cpp:136] Iteration 312400, lr = 0.002375, m = 0.9
I0815 09:29:56.977078 11101 solver.cpp:312] Iteration 312500 (6.65476 iter/s, 15.0268s/100 iter), loss = 1.03779
I0815 09:29:56.977870 11101 solver.cpp:334]     Train net output #0: loss = 1.02566 (* 1 = 1.02566 loss)
I0815 09:29:56.977957 11101 sgd_solver.cpp:136] Iteration 312500, lr = 0.00234375, m = 0.9
I0815 09:30:11.914759 11101 solver.cpp:312] Iteration 312600 (6.69466 iter/s, 14.9373s/100 iter), loss = 0.946924
I0815 09:30:11.914822 11101 solver.cpp:334]     Train net output #0: loss = 0.919472 (* 1 = 0.919472 loss)
I0815 09:30:11.914842 11101 sgd_solver.cpp:136] Iteration 312600, lr = 0.0023125, m = 0.9
I0815 09:30:26.820346 11101 solver.cpp:312] Iteration 312700 (6.70908 iter/s, 14.9052s/100 iter), loss = 1.08642
I0815 09:30:26.820569 11101 solver.cpp:334]     Train net output #0: loss = 0.702956 (* 1 = 0.702956 loss)
I0815 09:30:26.820683 11101 sgd_solver.cpp:136] Iteration 312700, lr = 0.00228125, m = 0.9
I0815 09:30:41.709347 11101 solver.cpp:312] Iteration 312800 (6.71655 iter/s, 14.8886s/100 iter), loss = 1.32334
I0815 09:30:41.709564 11101 solver.cpp:334]     Train net output #0: loss = 1.40281 (* 1 = 1.40281 loss)
I0815 09:30:41.709652 11101 sgd_solver.cpp:136] Iteration 312800, lr = 0.00225, m = 0.9
I0815 09:30:56.570595 11101 solver.cpp:312] Iteration 312900 (6.7291 iter/s, 14.8608s/100 iter), loss = 1.30165
I0815 09:30:56.570660 11101 solver.cpp:334]     Train net output #0: loss = 1.10965 (* 1 = 1.10965 loss)
I0815 09:30:56.570678 11101 sgd_solver.cpp:136] Iteration 312900, lr = 0.00221875, m = 0.9
I0815 09:31:11.666724 11101 solver.cpp:312] Iteration 313000 (6.6244 iter/s, 15.0957s/100 iter), loss = 1.11673
I0815 09:31:11.666752 11101 solver.cpp:334]     Train net output #0: loss = 0.836278 (* 1 = 0.836278 loss)
I0815 09:31:11.666759 11101 sgd_solver.cpp:136] Iteration 313000, lr = 0.0021875, m = 0.9
I0815 09:31:26.579716 11101 solver.cpp:312] Iteration 313100 (6.70575 iter/s, 14.9126s/100 iter), loss = 1.34332
I0815 09:31:26.579776 11101 solver.cpp:334]     Train net output #0: loss = 1.48054 (* 1 = 1.48054 loss)
I0815 09:31:26.579782 11101 sgd_solver.cpp:136] Iteration 313100, lr = 0.00215625, m = 0.9
I0815 09:31:41.606637 11101 solver.cpp:312] Iteration 313200 (6.65491 iter/s, 15.0265s/100 iter), loss = 1.20071
I0815 09:31:41.606664 11101 solver.cpp:334]     Train net output #0: loss = 1.18938 (* 1 = 1.18938 loss)
I0815 09:31:41.606672 11101 sgd_solver.cpp:136] Iteration 313200, lr = 0.002125, m = 0.9
I0815 09:31:56.314493 11101 solver.cpp:312] Iteration 313300 (6.79927 iter/s, 14.7075s/100 iter), loss = 1.21906
I0815 09:31:56.314518 11101 solver.cpp:334]     Train net output #0: loss = 1.28719 (* 1 = 1.28719 loss)
I0815 09:31:56.314523 11101 sgd_solver.cpp:136] Iteration 313300, lr = 0.00209375, m = 0.9
I0815 09:32:11.279368 11101 solver.cpp:312] Iteration 313400 (6.6825 iter/s, 14.9645s/100 iter), loss = 1.61895
I0815 09:32:11.279479 11101 solver.cpp:334]     Train net output #0: loss = 1.70775 (* 1 = 1.70775 loss)
I0815 09:32:11.279495 11101 sgd_solver.cpp:136] Iteration 313400, lr = 0.0020625, m = 0.9
I0815 09:32:26.391372 11101 solver.cpp:312] Iteration 313500 (6.61744 iter/s, 15.1116s/100 iter), loss = 1.24264
I0815 09:32:26.391397 11101 solver.cpp:334]     Train net output #0: loss = 1.17119 (* 1 = 1.17119 loss)
I0815 09:32:26.391402 11101 sgd_solver.cpp:136] Iteration 313500, lr = 0.00203125, m = 0.9
I0815 09:32:41.525282 11101 solver.cpp:312] Iteration 313600 (6.60786 iter/s, 15.1335s/100 iter), loss = 0.965891
I0815 09:32:41.525331 11101 solver.cpp:334]     Train net output #0: loss = 0.91251 (* 1 = 0.91251 loss)
I0815 09:32:41.525337 11101 sgd_solver.cpp:136] Iteration 313600, lr = 0.002, m = 0.9
I0815 09:32:56.596099 11101 solver.cpp:312] Iteration 313700 (6.63552 iter/s, 15.0704s/100 iter), loss = 1.42261
I0815 09:32:56.596127 11101 solver.cpp:334]     Train net output #0: loss = 0.940158 (* 1 = 0.940158 loss)
I0815 09:32:56.596143 11101 sgd_solver.cpp:136] Iteration 313700, lr = 0.00196875, m = 0.9
I0815 09:33:11.463591 11101 solver.cpp:312] Iteration 313800 (6.72627 iter/s, 14.8671s/100 iter), loss = 1.36852
I0815 09:33:11.463814 11101 solver.cpp:334]     Train net output #0: loss = 1.27639 (* 1 = 1.27639 loss)
I0815 09:33:11.463925 11101 sgd_solver.cpp:136] Iteration 313800, lr = 0.0019375, m = 0.9
I0815 09:33:26.454895 11101 solver.cpp:312] Iteration 313900 (6.67072 iter/s, 14.9909s/100 iter), loss = 1.60798
I0815 09:33:26.454994 11101 solver.cpp:334]     Train net output #0: loss = 1.73423 (* 1 = 1.73423 loss)
I0815 09:33:26.455013 11101 sgd_solver.cpp:136] Iteration 313900, lr = 0.00190625, m = 0.9
I0815 09:33:41.043376 11101 solver.cpp:509] Iteration 314000, Testing net (#0)
I0815 09:34:05.071964 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.599706
I0815 09:34:05.072046 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.823761
I0815 09:34:05.072054 11101 solver.cpp:594]     Test net output #2: loss = 1.72683 (* 1 = 1.72683 loss)
I0815 09:34:05.080726 11101 solver.cpp:264] [MultiGPU] Tests completed in 24.0367s
I0815 09:34:05.230818 11101 solver.cpp:312] Iteration 314000 (2.57899 iter/s, 38.7749s/100 iter), loss = 1.48027
I0815 09:34:05.230901 11101 solver.cpp:334]     Train net output #0: loss = 1.39601 (* 1 = 1.39601 loss)
I0815 09:34:05.230929 11101 sgd_solver.cpp:136] Iteration 314000, lr = 0.001875, m = 0.9
I0815 09:34:20.356685 11101 solver.cpp:312] Iteration 314100 (6.61138 iter/s, 15.1254s/100 iter), loss = 1.29989
I0815 09:34:20.356776 11101 solver.cpp:334]     Train net output #0: loss = 1.23684 (* 1 = 1.23684 loss)
I0815 09:34:20.356801 11101 sgd_solver.cpp:136] Iteration 314100, lr = 0.00184375, m = 0.9
I0815 09:34:35.838851 11101 solver.cpp:312] Iteration 314200 (6.45922 iter/s, 15.4817s/100 iter), loss = 1.39431
I0815 09:34:35.838901 11101 solver.cpp:334]     Train net output #0: loss = 1.3456 (* 1 = 1.3456 loss)
I0815 09:34:35.838907 11101 sgd_solver.cpp:136] Iteration 314200, lr = 0.0018125, m = 0.9
I0815 09:34:50.853997 11101 solver.cpp:312] Iteration 314300 (6.66013 iter/s, 15.0147s/100 iter), loss = 1.32702
I0815 09:34:50.854022 11101 solver.cpp:334]     Train net output #0: loss = 1.3443 (* 1 = 1.3443 loss)
I0815 09:34:50.854027 11101 sgd_solver.cpp:136] Iteration 314300, lr = 0.00178125, m = 0.9
I0815 09:35:06.161309 11101 solver.cpp:312] Iteration 314400 (6.53301 iter/s, 15.3069s/100 iter), loss = 1.28159
I0815 09:35:06.161370 11101 solver.cpp:334]     Train net output #0: loss = 1.41619 (* 1 = 1.41619 loss)
I0815 09:35:06.161378 11101 sgd_solver.cpp:136] Iteration 314400, lr = 0.00175, m = 0.9
I0815 09:35:21.364400 11101 solver.cpp:312] Iteration 314500 (6.57779 iter/s, 15.2027s/100 iter), loss = 1.35287
I0815 09:35:21.364436 11101 solver.cpp:334]     Train net output #0: loss = 1.382 (* 1 = 1.382 loss)
I0815 09:35:21.364444 11101 sgd_solver.cpp:136] Iteration 314500, lr = 0.00171875, m = 0.9
I0815 09:35:36.527608 11101 solver.cpp:312] Iteration 314600 (6.59509 iter/s, 15.1628s/100 iter), loss = 1.23059
I0815 09:35:36.527664 11101 solver.cpp:334]     Train net output #0: loss = 1.2967 (* 1 = 1.2967 loss)
I0815 09:35:36.527670 11101 sgd_solver.cpp:136] Iteration 314600, lr = 0.0016875, m = 0.9
I0815 09:35:51.328822 11101 solver.cpp:312] Iteration 314700 (6.75639 iter/s, 14.8008s/100 iter), loss = 1.55941
I0815 09:35:51.328850 11101 solver.cpp:334]     Train net output #0: loss = 1.51762 (* 1 = 1.51762 loss)
I0815 09:35:51.328856 11101 sgd_solver.cpp:136] Iteration 314700, lr = 0.00165625, m = 0.9
I0815 09:36:06.268254 11101 solver.cpp:312] Iteration 314800 (6.6939 iter/s, 14.939s/100 iter), loss = 1.28413
I0815 09:36:06.268355 11101 solver.cpp:334]     Train net output #0: loss = 1.26802 (* 1 = 1.26802 loss)
I0815 09:36:06.268381 11101 sgd_solver.cpp:136] Iteration 314800, lr = 0.001625, m = 0.9
I0815 09:36:21.403620 11101 solver.cpp:312] Iteration 314900 (6.60722 iter/s, 15.135s/100 iter), loss = 1.3134
I0815 09:36:21.403892 11101 solver.cpp:334]     Train net output #0: loss = 1.32284 (* 1 = 1.32284 loss)
I0815 09:36:21.404003 11101 sgd_solver.cpp:136] Iteration 314900, lr = 0.00159375, m = 0.9
I0815 09:36:36.783357 11101 solver.cpp:312] Iteration 315000 (6.50224 iter/s, 15.3793s/100 iter), loss = 1.21673
I0815 09:36:36.783385 11101 solver.cpp:334]     Train net output #0: loss = 1.20629 (* 1 = 1.20629 loss)
I0815 09:36:36.783391 11101 sgd_solver.cpp:136] Iteration 315000, lr = 0.0015625, m = 0.9
I0815 09:36:51.899484 11101 solver.cpp:312] Iteration 315100 (6.61563 iter/s, 15.1157s/100 iter), loss = 1.30016
I0815 09:36:51.899539 11101 solver.cpp:334]     Train net output #0: loss = 1.45447 (* 1 = 1.45447 loss)
I0815 09:36:51.899546 11101 sgd_solver.cpp:136] Iteration 315100, lr = 0.00153125, m = 0.9
I0815 09:37:07.672650 11101 solver.cpp:312] Iteration 315200 (6.34006 iter/s, 15.7727s/100 iter), loss = 1.58721
I0815 09:37:07.672727 11101 solver.cpp:334]     Train net output #0: loss = 1.74031 (* 1 = 1.74031 loss)
I0815 09:37:07.672747 11101 sgd_solver.cpp:136] Iteration 315200, lr = 0.0015, m = 0.9
I0815 09:37:22.923914 11101 solver.cpp:312] Iteration 315300 (6.55702 iter/s, 15.2508s/100 iter), loss = 1.5659
I0815 09:37:22.923995 11101 solver.cpp:334]     Train net output #0: loss = 1.54128 (* 1 = 1.54128 loss)
I0815 09:37:22.924003 11101 sgd_solver.cpp:136] Iteration 315300, lr = 0.00146875, m = 0.9
I0815 09:37:29.321707 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:37:38.115767 11101 solver.cpp:312] Iteration 315400 (6.58266 iter/s, 15.1914s/100 iter), loss = 1.1798
I0815 09:37:38.115831 11101 solver.cpp:334]     Train net output #0: loss = 1.20233 (* 1 = 1.20233 loss)
I0815 09:37:38.115849 11101 sgd_solver.cpp:136] Iteration 315400, lr = 0.0014375, m = 0.9
I0815 09:37:53.631217 11101 solver.cpp:312] Iteration 315500 (6.44537 iter/s, 15.515s/100 iter), loss = 1.16851
I0815 09:37:53.631276 11101 solver.cpp:334]     Train net output #0: loss = 0.799839 (* 1 = 0.799839 loss)
I0815 09:37:53.631283 11101 sgd_solver.cpp:136] Iteration 315500, lr = 0.00140625, m = 0.9
I0815 09:38:08.770581 11101 solver.cpp:312] Iteration 315600 (6.60548 iter/s, 15.1389s/100 iter), loss = 0.784727
I0815 09:38:08.770637 11101 solver.cpp:334]     Train net output #0: loss = 0.631933 (* 1 = 0.631933 loss)
I0815 09:38:08.770651 11101 sgd_solver.cpp:136] Iteration 315600, lr = 0.001375, m = 0.9
I0815 09:38:24.703217 11101 solver.cpp:312] Iteration 315700 (6.2766 iter/s, 15.9322s/100 iter), loss = 1.21103
I0815 09:38:24.704288 11101 solver.cpp:334]     Train net output #0: loss = 1.04597 (* 1 = 1.04597 loss)
I0815 09:38:24.704309 11101 sgd_solver.cpp:136] Iteration 315700, lr = 0.00134375, m = 0.9
I0815 09:38:40.535897 11101 solver.cpp:312] Iteration 315800 (6.31623 iter/s, 15.8322s/100 iter), loss = 1.2785
I0815 09:38:40.536164 11101 solver.cpp:334]     Train net output #0: loss = 1.43339 (* 1 = 1.43339 loss)
I0815 09:38:40.536278 11101 sgd_solver.cpp:136] Iteration 315800, lr = 0.0013125, m = 0.9
I0815 09:38:55.770931 11101 solver.cpp:312] Iteration 315900 (6.564 iter/s, 15.2346s/100 iter), loss = 1.24362
I0815 09:38:55.770994 11101 solver.cpp:334]     Train net output #0: loss = 1.36137 (* 1 = 1.36137 loss)
I0815 09:38:55.771000 11101 sgd_solver.cpp:136] Iteration 315900, lr = 0.00128125, m = 0.9
I0815 09:39:10.824229 11101 solver.cpp:509] Iteration 316000, Testing net (#0)
I0815 09:39:34.043494 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.613764
I0815 09:39:34.043555 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.834467
I0815 09:39:34.043566 11101 solver.cpp:594]     Test net output #2: loss = 1.67697 (* 1 = 1.67697 loss)
I0815 09:39:34.043592 11101 solver.cpp:264] [MultiGPU] Tests completed in 23.2243s
I0815 09:39:34.207490 11101 solver.cpp:312] Iteration 316000 (2.60176 iter/s, 38.4355s/100 iter), loss = 1.05947
I0815 09:39:34.207665 11101 solver.cpp:334]     Train net output #0: loss = 1.07224 (* 1 = 1.07224 loss)
I0815 09:39:34.207752 11101 sgd_solver.cpp:136] Iteration 316000, lr = 0.00125, m = 0.9
I0815 09:39:49.492197 11101 solver.cpp:312] Iteration 316100 (6.54267 iter/s, 15.2843s/100 iter), loss = 1.35665
I0815 09:39:49.492226 11101 solver.cpp:334]     Train net output #0: loss = 1.40805 (* 1 = 1.40805 loss)
I0815 09:39:49.492234 11101 sgd_solver.cpp:136] Iteration 316100, lr = 0.00121875, m = 0.9
I0815 09:40:04.309968 11101 solver.cpp:312] Iteration 316200 (6.74884 iter/s, 14.8174s/100 iter), loss = 1.45715
I0815 09:40:04.310029 11101 solver.cpp:334]     Train net output #0: loss = 1.55754 (* 1 = 1.55754 loss)
I0815 09:40:04.310036 11101 sgd_solver.cpp:136] Iteration 316200, lr = 0.0011875, m = 0.9
I0815 09:40:19.697549 11101 solver.cpp:312] Iteration 316300 (6.49893 iter/s, 15.3872s/100 iter), loss = 1.31896
I0815 09:40:19.697576 11101 solver.cpp:334]     Train net output #0: loss = 1.25876 (* 1 = 1.25876 loss)
I0815 09:40:19.697582 11101 sgd_solver.cpp:136] Iteration 316300, lr = 0.00115625, m = 0.9
I0815 09:40:35.326465 11101 solver.cpp:312] Iteration 316400 (6.39857 iter/s, 15.6285s/100 iter), loss = 1.19916
I0815 09:40:35.326529 11101 solver.cpp:334]     Train net output #0: loss = 1.20813 (* 1 = 1.20813 loss)
I0815 09:40:35.326534 11101 sgd_solver.cpp:136] Iteration 316400, lr = 0.001125, m = 0.9
I0815 09:40:50.021422 11101 solver.cpp:312] Iteration 316500 (6.80524 iter/s, 14.6945s/100 iter), loss = 0.958299
I0815 09:40:50.021488 11101 solver.cpp:334]     Train net output #0: loss = 0.879187 (* 1 = 0.879187 loss)
I0815 09:40:50.021505 11101 sgd_solver.cpp:136] Iteration 316500, lr = 0.00109375, m = 0.9
I0815 09:41:05.101212 11101 solver.cpp:312] Iteration 316600 (6.63158 iter/s, 15.0794s/100 iter), loss = 1.45561
I0815 09:41:05.101275 11101 solver.cpp:334]     Train net output #0: loss = 1.36316 (* 1 = 1.36316 loss)
I0815 09:41:05.101292 11101 sgd_solver.cpp:136] Iteration 316600, lr = 0.0010625, m = 0.9
I0815 09:41:19.924970 11101 solver.cpp:312] Iteration 316700 (6.74611 iter/s, 14.8234s/100 iter), loss = 1.02555
I0815 09:41:19.925070 11101 solver.cpp:334]     Train net output #0: loss = 1.06883 (* 1 = 1.06883 loss)
I0815 09:41:19.925088 11101 sgd_solver.cpp:136] Iteration 316700, lr = 0.00103125, m = 0.9
I0815 09:41:35.311698 11101 solver.cpp:312] Iteration 316800 (6.49929 iter/s, 15.3863s/100 iter), loss = 1.09266
I0815 09:41:35.311724 11101 solver.cpp:334]     Train net output #0: loss = 1.2201 (* 1 = 1.2201 loss)
I0815 09:41:35.311729 11101 sgd_solver.cpp:136] Iteration 316800, lr = 0.000999999, m = 0.9
I0815 09:41:50.974476 11101 solver.cpp:312] Iteration 316900 (6.38474 iter/s, 15.6623s/100 iter), loss = 1.28629
I0815 09:41:50.974530 11101 solver.cpp:334]     Train net output #0: loss = 1.01885 (* 1 = 1.01885 loss)
I0815 09:41:50.974537 11101 sgd_solver.cpp:136] Iteration 316900, lr = 0.000968748, m = 0.9
I0815 09:42:06.704288 11101 solver.cpp:312] Iteration 317000 (6.35753 iter/s, 15.7294s/100 iter), loss = 1.16436
I0815 09:42:06.704324 11101 solver.cpp:334]     Train net output #0: loss = 1.29391 (* 1 = 1.29391 loss)
I0815 09:42:06.704329 11101 sgd_solver.cpp:136] Iteration 317000, lr = 0.000937498, m = 0.9
I0815 09:42:22.274835 11101 solver.cpp:312] Iteration 317100 (6.42256 iter/s, 15.5701s/100 iter), loss = 1.29842
I0815 09:42:22.275610 11101 solver.cpp:334]     Train net output #0: loss = 1.24188 (* 1 = 1.24188 loss)
I0815 09:42:22.275619 11101 sgd_solver.cpp:136] Iteration 317100, lr = 0.000906253, m = 0.9
I0815 09:42:37.618010 11101 solver.cpp:312] Iteration 317200 (6.51774 iter/s, 15.3428s/100 iter), loss = 1.39494
I0815 09:42:37.618043 11101 solver.cpp:334]     Train net output #0: loss = 1.52953 (* 1 = 1.52953 loss)
I0815 09:42:37.618050 11101 sgd_solver.cpp:136] Iteration 317200, lr = 0.000875002, m = 0.9
I0815 09:42:53.207703 11101 solver.cpp:312] Iteration 317300 (6.41467 iter/s, 15.5893s/100 iter), loss = 1.57847
I0815 09:42:53.207913 11101 solver.cpp:334]     Train net output #0: loss = 1.57547 (* 1 = 1.57547 loss)
I0815 09:42:53.207922 11101 sgd_solver.cpp:136] Iteration 317300, lr = 0.000843751, m = 0.9
I0815 09:43:08.496580 11101 solver.cpp:312] Iteration 317400 (6.54088 iter/s, 15.2885s/100 iter), loss = 1.05905
I0815 09:43:08.496634 11101 solver.cpp:334]     Train net output #0: loss = 0.955867 (* 1 = 0.955867 loss)
I0815 09:43:08.496647 11101 sgd_solver.cpp:136] Iteration 317400, lr = 0.000812501, m = 0.9
I0815 09:43:23.599369 11101 solver.cpp:312] Iteration 317500 (6.62148 iter/s, 15.1024s/100 iter), loss = 1.3906
I0815 09:43:23.599567 11101 solver.cpp:334]     Train net output #0: loss = 1.43888 (* 1 = 1.43888 loss)
I0815 09:43:23.599586 11101 sgd_solver.cpp:136] Iteration 317500, lr = 0.00078125, m = 0.9
I0815 09:43:38.625135 11101 solver.cpp:312] Iteration 317600 (6.65542 iter/s, 15.0254s/100 iter), loss = 1.55659
I0815 09:43:38.625159 11101 solver.cpp:334]     Train net output #0: loss = 1.15761 (* 1 = 1.15761 loss)
I0815 09:43:38.625164 11101 sgd_solver.cpp:136] Iteration 317600, lr = 0.000749999, m = 0.9
I0815 09:43:53.644245 11101 solver.cpp:312] Iteration 317700 (6.65837 iter/s, 15.0187s/100 iter), loss = 1.09124
I0815 09:43:53.648164 11101 solver.cpp:334]     Train net output #0: loss = 1.44189 (* 1 = 1.44189 loss)
I0815 09:43:53.648174 11101 sgd_solver.cpp:136] Iteration 317700, lr = 0.000718749, m = 0.9
I0815 09:44:08.531875 11101 solver.cpp:312] Iteration 317800 (6.71717 iter/s, 14.8872s/100 iter), loss = 1.17835
I0815 09:44:08.531929 11101 solver.cpp:334]     Train net output #0: loss = 1.32128 (* 1 = 1.32128 loss)
I0815 09:44:08.531942 11101 sgd_solver.cpp:136] Iteration 317800, lr = 0.000687498, m = 0.9
I0815 09:44:23.481401 11101 solver.cpp:312] Iteration 317900 (6.68936 iter/s, 14.9491s/100 iter), loss = 1.23046
I0815 09:44:23.481428 11101 solver.cpp:334]     Train net output #0: loss = 1.44974 (* 1 = 1.44974 loss)
I0815 09:44:23.481434 11101 sgd_solver.cpp:136] Iteration 317900, lr = 0.000656247, m = 0.9
I0815 09:44:38.419945 11101 solver.cpp:509] Iteration 318000, Testing net (#0)
I0815 09:45:01.247553 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.610588
I0815 09:45:01.247578 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.830173
I0815 09:45:01.247586 11101 solver.cpp:594]     Test net output #2: loss = 1.67911 (* 1 = 1.67911 loss)
I0815 09:45:01.247611 11101 solver.cpp:264] [MultiGPU] Tests completed in 22.827s
I0815 09:45:01.409473 11101 solver.cpp:312] Iteration 318000 (2.63664 iter/s, 37.927s/100 iter), loss = 1.63093
I0815 09:45:01.409524 11101 solver.cpp:334]     Train net output #0: loss = 1.55441 (* 1 = 1.55441 loss)
I0815 09:45:01.409538 11101 sgd_solver.cpp:136] Iteration 318000, lr = 0.000625002, m = 0.9
I0815 09:45:16.459115 11101 solver.cpp:312] Iteration 318100 (6.64486 iter/s, 15.0492s/100 iter), loss = 1.56777
I0815 09:45:16.459179 11101 solver.cpp:334]     Train net output #0: loss = 1.56694 (* 1 = 1.56694 loss)
I0815 09:45:16.459187 11101 sgd_solver.cpp:136] Iteration 318100, lr = 0.000593752, m = 0.9
I0815 09:45:31.489039 11101 solver.cpp:312] Iteration 318200 (6.65358 iter/s, 15.0295s/100 iter), loss = 1.39685
I0815 09:45:31.489066 11101 solver.cpp:334]     Train net output #0: loss = 1.34206 (* 1 = 1.34206 loss)
I0815 09:45:31.489069 11101 sgd_solver.cpp:136] Iteration 318200, lr = 0.000562501, m = 0.9
I0815 09:45:46.254619 11101 solver.cpp:312] Iteration 318300 (6.7727 iter/s, 14.7652s/100 iter), loss = 1.49941
I0815 09:45:46.254676 11101 solver.cpp:334]     Train net output #0: loss = 1.43875 (* 1 = 1.43875 loss)
I0815 09:45:46.254688 11101 sgd_solver.cpp:136] Iteration 318300, lr = 0.00053125, m = 0.9
I0815 09:46:00.933631 11101 solver.cpp:312] Iteration 318400 (6.81264 iter/s, 14.6786s/100 iter), loss = 1.26501
I0815 09:46:00.933694 11101 solver.cpp:334]     Train net output #0: loss = 1.74566 (* 1 = 1.74566 loss)
I0815 09:46:00.933701 11101 sgd_solver.cpp:136] Iteration 318400, lr = 0.0005, m = 0.9
I0815 09:46:15.827392 11101 solver.cpp:312] Iteration 318500 (6.71441 iter/s, 14.8933s/100 iter), loss = 1.3534
I0815 09:46:15.827425 11101 solver.cpp:334]     Train net output #0: loss = 1.20911 (* 1 = 1.20911 loss)
I0815 09:46:15.827430 11101 sgd_solver.cpp:136] Iteration 318500, lr = 0.000468749, m = 0.9
I0815 09:46:31.089170 11101 solver.cpp:312] Iteration 318600 (6.5525 iter/s, 15.2614s/100 iter), loss = 1.40716
I0815 09:46:31.089226 11101 solver.cpp:334]     Train net output #0: loss = 1.40917 (* 1 = 1.40917 loss)
I0815 09:46:31.089232 11101 sgd_solver.cpp:136] Iteration 318600, lr = 0.000437498, m = 0.9
I0815 09:46:45.876526 11101 solver.cpp:312] Iteration 318700 (6.76272 iter/s, 14.7869s/100 iter), loss = 1.56722
I0815 09:46:45.876552 11101 solver.cpp:334]     Train net output #0: loss = 1.18235 (* 1 = 1.18235 loss)
I0815 09:46:45.876559 11101 sgd_solver.cpp:136] Iteration 318700, lr = 0.000406247, m = 0.9
I0815 09:47:00.748738 11101 solver.cpp:312] Iteration 318800 (6.72413 iter/s, 14.8718s/100 iter), loss = 1.47396
I0815 09:47:00.748766 11101 solver.cpp:334]     Train net output #0: loss = 1.85844 (* 1 = 1.85844 loss)
I0815 09:47:00.748772 11101 sgd_solver.cpp:136] Iteration 318800, lr = 0.000375003, m = 0.9
I0815 09:47:15.522974 11101 solver.cpp:312] Iteration 318900 (6.76873 iter/s, 14.7738s/100 iter), loss = 1.33339
I0815 09:47:15.523190 11101 solver.cpp:334]     Train net output #0: loss = 1.25468 (* 1 = 1.25468 loss)
I0815 09:47:15.523277 11101 sgd_solver.cpp:136] Iteration 318900, lr = 0.000343752, m = 0.9
I0815 09:47:30.357697 11101 solver.cpp:312] Iteration 319000 (6.74113 iter/s, 14.8343s/100 iter), loss = 1.38099
I0815 09:47:30.357727 11101 solver.cpp:334]     Train net output #0: loss = 1.26202 (* 1 = 1.26202 loss)
I0815 09:47:30.357733 11101 sgd_solver.cpp:136] Iteration 319000, lr = 0.000312501, m = 0.9
I0815 09:47:45.679234 11101 solver.cpp:312] Iteration 319100 (6.52694 iter/s, 15.3211s/100 iter), loss = 1.26107
I0815 09:47:45.679296 11101 solver.cpp:334]     Train net output #0: loss = 1.02326 (* 1 = 1.02326 loss)
I0815 09:47:45.679301 11101 sgd_solver.cpp:136] Iteration 319100, lr = 0.00028125, m = 0.9
I0815 09:48:00.556907 11101 solver.cpp:312] Iteration 319200 (6.72167 iter/s, 14.8773s/100 iter), loss = 1.33771
I0815 09:48:00.556972 11101 solver.cpp:334]     Train net output #0: loss = 1.28429 (* 1 = 1.28429 loss)
I0815 09:48:00.556988 11101 sgd_solver.cpp:136] Iteration 319200, lr = 0.00025, m = 0.9
I0815 09:48:15.462369 11101 solver.cpp:312] Iteration 319300 (6.70914 iter/s, 14.9051s/100 iter), loss = 1.04638
I0815 09:48:15.462421 11101 solver.cpp:334]     Train net output #0: loss = 0.926517 (* 1 = 0.926517 loss)
I0815 09:48:15.462435 11101 sgd_solver.cpp:136] Iteration 319300, lr = 0.000218749, m = 0.9
I0815 09:48:30.094465 11101 solver.cpp:312] Iteration 319400 (6.83448 iter/s, 14.6317s/100 iter), loss = 1.5646
I0815 09:48:30.094548 11101 solver.cpp:334]     Train net output #0: loss = 1.40435 (* 1 = 1.40435 loss)
I0815 09:48:30.094565 11101 sgd_solver.cpp:136] Iteration 319400, lr = 0.000187498, m = 0.9
I0815 09:48:44.832119 11101 solver.cpp:312] Iteration 319500 (6.78553 iter/s, 14.7372s/100 iter), loss = 1.04417
I0815 09:48:44.832340 11101 solver.cpp:334]     Train net output #0: loss = 1.02845 (* 1 = 1.02845 loss)
I0815 09:48:44.832449 11101 sgd_solver.cpp:136] Iteration 319500, lr = 0.000156248, m = 0.9
I0815 09:48:59.731045 11101 solver.cpp:312] Iteration 319600 (6.71208 iter/s, 14.8985s/100 iter), loss = 1.18535
I0815 09:48:59.731115 11101 solver.cpp:334]     Train net output #0: loss = 0.970599 (* 1 = 0.970599 loss)
I0815 09:48:59.731137 11101 sgd_solver.cpp:136] Iteration 319600, lr = 0.000125003, m = 0.9
I0815 09:49:14.586259 11101 solver.cpp:312] Iteration 319700 (6.73183 iter/s, 14.8548s/100 iter), loss = 1.44016
I0815 09:49:14.586524 11101 solver.cpp:334]     Train net output #0: loss = 1.36532 (* 1 = 1.36532 loss)
I0815 09:49:14.586645 11101 sgd_solver.cpp:136] Iteration 319700, lr = 9.37521e-05, m = 0.9
I0815 09:49:29.912104 11101 solver.cpp:312] Iteration 319800 (6.52511 iter/s, 15.3254s/100 iter), loss = 1.04441
I0815 09:49:29.912140 11101 solver.cpp:334]     Train net output #0: loss = 0.777412 (* 1 = 0.777412 loss)
I0815 09:49:29.912147 11101 sgd_solver.cpp:136] Iteration 319800, lr = 6.25014e-05, m = 0.9
I0815 09:49:45.577682 11101 solver.cpp:312] Iteration 319900 (6.3836 iter/s, 15.6651s/100 iter), loss = 1.16259
I0815 09:49:45.577776 11101 solver.cpp:334]     Train net output #0: loss = 1.22815 (* 1 = 1.22815 loss)
I0815 09:49:45.577790 11101 sgd_solver.cpp:136] Iteration 319900, lr = 3.12507e-05, m = 0.9
I0815 09:50:00.898922 11101 solver.cpp:312] Iteration 319999 (6.46179 iter/s, 15.3208s/99 iter), loss = 1.26237
I0815 09:50:00.898947 11101 solver.cpp:334]     Train net output #0: loss = 1.07701 (* 1 = 1.07701 loss)
I0815 09:50:00.898952 11101 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_320000.caffemodel
I0815 09:50:00.961211 11101 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_320000.solverstate
I0815 09:50:00.990260 11101 solver.cpp:486] Iteration 320000, loss = 1.22737
I0815 09:50:00.990285 11101 solver.cpp:509] Iteration 320000, Testing net (#0)
I0815 09:50:18.825428 11101 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:50:24.277006 11101 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.608705
I0815 09:50:24.277030 11101 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.833526
I0815 09:50:24.277037 11101 solver.cpp:594]     Test net output #2: loss = 1.67076 (* 1 = 1.67076 loss)
I0815 09:50:24.326033 11027 parallel.cpp:71] Root Solver performance on device 0: 6.354 * 43 = 273.2 img/sec (320000 itr in 5.036e+04 sec)
I0815 09:50:24.326052 11027 parallel.cpp:76]      Solver performance on device 1: 6.354 * 43 = 273.2 img/sec (320000 itr in 5.036e+04 sec)
I0815 09:50:24.326057 11027 parallel.cpp:76]      Solver performance on device 2: 6.354 * 43 = 273.2 img/sec (320000 itr in 5.036e+04 sec)
I0815 09:50:24.326059 11027 parallel.cpp:79] Overall multi-GPU performance: 819.695 img/sec
I0815 09:50:25.007881 11027 caffe.cpp:247] Optimization Done in 13h 59m 49s
I0815 09:50:27.824497  8671 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 09:50:27 2017
I0815 09:50:27.824926  8671 caffe.cpp:611] CuDNN version: 6021
I0815 09:50:27.824932  8671 caffe.cpp:612] CuBLAS version: 8000
I0815 09:50:27.824934  8671 caffe.cpp:613] CUDA version: 8000
I0815 09:50:27.824935  8671 caffe.cpp:614] CUDA driver version: 8000
I0815 09:50:28.095011  8671 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 09:50:28.095633  8671 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 09:50:28.096201  8671 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 09:50:28.096774  8671 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 09:50:28.096791  8671 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 09:50:28.097122  8671 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 09:50:28.097453  8671 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 09:50:28.097784  8671 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 09:50:28.098271  8671 solver.cpp:42] Solver data type: FLOAT
I0815 09:50:28.098320  8671 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/train.prototxt"
test_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt"
test_iter: 1000
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 160000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 2
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 20000
sparsity_start_factor: 0
I0815 09:50:28.117274  8671 solver.cpp:77] Creating training net from train_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/train.prototxt
I0815 09:50:28.117895  8671 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 09:50:28.117902  8671 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 09:50:28.117930  8671 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:28.118649  8671 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_train_lmdb"
    batch_size: 43
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0815 09:50:28.118782  8671 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:28.118788  8671 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:28.118791  8671 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0815 09:50:28.118796  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.134377  8671 net.cpp:184] Created Layer data (0)
I0815 09:50:28.134393  8671 net.cpp:530] data -> data
I0815 09:50:28.134407  8671 net.cpp:530] data -> label
I0815 09:50:28.134431  8671 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:28.134454  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.175618  8726 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:28.177450  8671 data_layer.cpp:185] [0] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:28.177513  8671 data_layer.cpp:209] [0] Output data size: 43, 3, 224, 224
I0815 09:50:28.177520  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.177542  8671 net.cpp:245] Setting up data
I0815 09:50:28.177551  8671 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 3 224 224 (6472704)
I0815 09:50:28.177556  8671 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 (43)
I0815 09:50:28.177564  8671 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 09:50:28.177570  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.177588  8671 net.cpp:184] Created Layer data/bias (1)
I0815 09:50:28.177593  8671 net.cpp:561] data/bias <- data
I0815 09:50:28.177603  8671 net.cpp:530] data/bias -> data/bias
I0815 09:50:28.179690  8671 net.cpp:245] Setting up data/bias
I0815 09:50:28.179702  8671 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 43 3 224 224 (6472704)
I0815 09:50:28.179713  8671 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 09:50:28.179718  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.179733  8671 net.cpp:184] Created Layer conv1a (2)
I0815 09:50:28.179736  8671 net.cpp:561] conv1a <- data/bias
I0815 09:50:28.179741  8671 net.cpp:530] conv1a -> conv1a
I0815 09:50:28.764436  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.97G, req 0G)
I0815 09:50:28.764458  8671 net.cpp:245] Setting up conv1a
I0815 09:50:28.764467  8671 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 43 32 112 112 (17260544)
I0815 09:50:28.764477  8671 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 09:50:28.764482  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.764495  8671 net.cpp:184] Created Layer conv1a/bn (3)
I0815 09:50:28.764500  8671 net.cpp:561] conv1a/bn <- conv1a
I0815 09:50:28.764506  8671 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 09:50:28.765156  8671 net.cpp:245] Setting up conv1a/bn
I0815 09:50:28.765166  8671 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 43 32 112 112 (17260544)
I0815 09:50:28.765175  8671 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 09:50:28.765179  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.765187  8671 net.cpp:184] Created Layer conv1a/relu (4)
I0815 09:50:28.765190  8671 net.cpp:561] conv1a/relu <- conv1a
I0815 09:50:28.765194  8671 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 09:50:28.765209  8671 net.cpp:245] Setting up conv1a/relu
I0815 09:50:28.765214  8671 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 43 32 112 112 (17260544)
I0815 09:50:28.765218  8671 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 09:50:28.765223  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.765233  8671 net.cpp:184] Created Layer conv1b (5)
I0815 09:50:28.765235  8671 net.cpp:561] conv1b <- conv1a
I0815 09:50:28.765240  8671 net.cpp:530] conv1b -> conv1b
I0815 09:50:28.790783  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 09:50:28.790796  8671 net.cpp:245] Setting up conv1b
I0815 09:50:28.790807  8671 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 43 32 112 112 (17260544)
I0815 09:50:28.790814  8671 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 09:50:28.790827  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.790835  8671 net.cpp:184] Created Layer conv1b/bn (6)
I0815 09:50:28.790839  8671 net.cpp:561] conv1b/bn <- conv1b
I0815 09:50:28.790843  8671 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 09:50:28.791451  8671 net.cpp:245] Setting up conv1b/bn
I0815 09:50:28.791460  8671 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 43 32 112 112 (17260544)
I0815 09:50:28.791468  8671 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 09:50:28.791472  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791477  8671 net.cpp:184] Created Layer conv1b/relu (7)
I0815 09:50:28.791481  8671 net.cpp:561] conv1b/relu <- conv1b
I0815 09:50:28.791486  8671 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 09:50:28.791491  8671 net.cpp:245] Setting up conv1b/relu
I0815 09:50:28.791496  8671 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 43 32 112 112 (17260544)
I0815 09:50:28.791501  8671 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 09:50:28.791504  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791512  8671 net.cpp:184] Created Layer pool1 (8)
I0815 09:50:28.791517  8671 net.cpp:561] pool1 <- conv1b
I0815 09:50:28.791520  8671 net.cpp:530] pool1 -> pool1
I0815 09:50:28.791592  8671 net.cpp:245] Setting up pool1
I0815 09:50:28.791599  8671 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 43 32 56 56 (4315136)
I0815 09:50:28.791602  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 09:50:28.791607  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791617  8671 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 09:50:28.791620  8671 net.cpp:561] res2a_branch2a <- pool1
I0815 09:50:28.791625  8671 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 09:50:28.816090  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.71G, req 0G)
I0815 09:50:28.816108  8671 net.cpp:245] Setting up res2a_branch2a
I0815 09:50:28.816115  8671 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 43 64 56 56 (8630272)
I0815 09:50:28.816126  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.816138  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816148  8671 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 09:50:28.816153  8671 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 09:50:28.816157  8671 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 09:50:28.816810  8671 net.cpp:245] Setting up res2a_branch2a/bn
I0815 09:50:28.816818  8671 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 43 64 56 56 (8630272)
I0815 09:50:28.816828  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.816831  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816838  8671 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 09:50:28.816840  8671 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 09:50:28.816844  8671 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 09:50:28.816851  8671 net.cpp:245] Setting up res2a_branch2a/relu
I0815 09:50:28.816856  8671 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 43 64 56 56 (8630272)
I0815 09:50:28.816860  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 09:50:28.816865  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816875  8671 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 09:50:28.816879  8671 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 09:50:28.816882  8671 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 09:50:28.828797  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.64G, req 0G)
I0815 09:50:28.828821  8671 net.cpp:245] Setting up res2a_branch2b
I0815 09:50:28.828829  8671 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 43 64 56 56 (8630272)
I0815 09:50:28.828837  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.828842  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.828855  8671 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 09:50:28.828860  8671 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 09:50:28.828866  8671 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 09:50:28.830344  8671 net.cpp:245] Setting up res2a_branch2b/bn
I0815 09:50:28.830355  8671 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 43 64 56 56 (8630272)
I0815 09:50:28.830364  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.830368  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830374  8671 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 09:50:28.830377  8671 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 09:50:28.830381  8671 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 09:50:28.830387  8671 net.cpp:245] Setting up res2a_branch2b/relu
I0815 09:50:28.830394  8671 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 43 64 56 56 (8630272)
I0815 09:50:28.830397  8671 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 09:50:28.830401  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830410  8671 net.cpp:184] Created Layer pool2 (15)
I0815 09:50:28.830415  8671 net.cpp:561] pool2 <- res2a_branch2b
I0815 09:50:28.830418  8671 net.cpp:530] pool2 -> pool2
I0815 09:50:28.830500  8671 net.cpp:245] Setting up pool2
I0815 09:50:28.830507  8671 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 43 64 28 28 (2157568)
I0815 09:50:28.830512  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 09:50:28.830516  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830528  8671 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 09:50:28.830531  8671 net.cpp:561] res3a_branch2a <- pool2
I0815 09:50:28.830535  8671 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 09:50:28.852579  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.58G, req 0G)
I0815 09:50:28.852594  8671 net.cpp:245] Setting up res3a_branch2a
I0815 09:50:28.852599  8671 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 43 128 28 28 (4315136)
I0815 09:50:28.852604  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.852607  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.852612  8671 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 09:50:28.852615  8671 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 09:50:28.852618  8671 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 09:50:28.853263  8671 net.cpp:245] Setting up res3a_branch2a/bn
I0815 09:50:28.853271  8671 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 43 128 28 28 (4315136)
I0815 09:50:28.853279  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.853283  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.853286  8671 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 09:50:28.853289  8671 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 09:50:28.853291  8671 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 09:50:28.853307  8671 net.cpp:245] Setting up res3a_branch2a/relu
I0815 09:50:28.853310  8671 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 43 128 28 28 (4315136)
I0815 09:50:28.853312  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 09:50:28.853314  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.853338  8671 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 09:50:28.853340  8671 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 09:50:28.853343  8671 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 09:50:28.862071  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.54G, req 0G)
I0815 09:50:28.862093  8671 net.cpp:245] Setting up res3a_branch2b
I0815 09:50:28.862102  8671 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 43 128 28 28 (4315136)
I0815 09:50:28.862112  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.862118  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.862128  8671 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 09:50:28.862131  8671 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 09:50:28.862136  8671 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 09:50:28.863054  8671 net.cpp:245] Setting up res3a_branch2b/bn
I0815 09:50:28.863066  8671 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 43 128 28 28 (4315136)
I0815 09:50:28.863078  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.863085  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863090  8671 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 09:50:28.863095  8671 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 09:50:28.863099  8671 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 09:50:28.863106  8671 net.cpp:245] Setting up res3a_branch2b/relu
I0815 09:50:28.863111  8671 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 43 128 28 28 (4315136)
I0815 09:50:28.863114  8671 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 09:50:28.863118  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863126  8671 net.cpp:184] Created Layer pool3 (22)
I0815 09:50:28.863131  8671 net.cpp:561] pool3 <- res3a_branch2b
I0815 09:50:28.863135  8671 net.cpp:530] pool3 -> pool3
I0815 09:50:28.863234  8671 net.cpp:245] Setting up pool3
I0815 09:50:28.863241  8671 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 43 128 14 14 (1078784)
I0815 09:50:28.863246  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 09:50:28.863252  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863268  8671 net.cpp:184] Created Layer res4a_branch2a (23)
I0815 09:50:28.863275  8671 net.cpp:561] res4a_branch2a <- pool3
I0815 09:50:28.863279  8671 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 09:50:28.889317  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.51G, req 0G)
I0815 09:50:28.889338  8671 net.cpp:245] Setting up res4a_branch2a
I0815 09:50:28.889344  8671 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 43 256 14 14 (2157568)
I0815 09:50:28.889351  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.889355  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.889364  8671 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0815 09:50:28.889367  8671 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 09:50:28.889371  8671 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 09:50:28.890071  8671 net.cpp:245] Setting up res4a_branch2a/bn
I0815 09:50:28.890089  8671 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 43 256 14 14 (2157568)
I0815 09:50:28.890095  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.890099  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.890102  8671 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0815 09:50:28.890105  8671 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 09:50:28.890107  8671 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 09:50:28.890111  8671 net.cpp:245] Setting up res4a_branch2a/relu
I0815 09:50:28.890113  8671 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 43 256 14 14 (2157568)
I0815 09:50:28.890115  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 09:50:28.890117  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.890125  8671 net.cpp:184] Created Layer res4a_branch2b (26)
I0815 09:50:28.890132  8671 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 09:50:28.890135  8671 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 09:50:28.899150  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0G)
I0815 09:50:28.899163  8671 net.cpp:245] Setting up res4a_branch2b
I0815 09:50:28.899168  8671 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 43 256 14 14 (2157568)
I0815 09:50:28.899173  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.899176  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899180  8671 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0815 09:50:28.899183  8671 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 09:50:28.899186  8671 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 09:50:28.899842  8671 net.cpp:245] Setting up res4a_branch2b/bn
I0815 09:50:28.899852  8671 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 43 256 14 14 (2157568)
I0815 09:50:28.899857  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.899860  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899869  8671 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0815 09:50:28.899873  8671 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 09:50:28.899874  8671 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 09:50:28.899879  8671 net.cpp:245] Setting up res4a_branch2b/relu
I0815 09:50:28.899883  8671 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 43 256 14 14 (2157568)
I0815 09:50:28.899884  8671 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 09:50:28.899886  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899890  8671 net.cpp:184] Created Layer pool4 (29)
I0815 09:50:28.899893  8671 net.cpp:561] pool4 <- res4a_branch2b
I0815 09:50:28.899895  8671 net.cpp:530] pool4 -> pool4
I0815 09:50:28.899958  8671 net.cpp:245] Setting up pool4
I0815 09:50:28.899963  8671 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 43 256 7 7 (539392)
I0815 09:50:28.899966  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 09:50:28.899968  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899974  8671 net.cpp:184] Created Layer res5a_branch2a (30)
I0815 09:50:28.899977  8671 net.cpp:561] res5a_branch2a <- pool4
I0815 09:50:28.899979  8671 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 09:50:28.948348  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 7.45G, req 0G)
I0815 09:50:28.948365  8671 net.cpp:245] Setting up res5a_branch2a
I0815 09:50:28.948370  8671 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 43 512 7 7 (1078784)
I0815 09:50:28.948388  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.948392  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.948405  8671 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0815 09:50:28.948410  8671 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 09:50:28.948413  8671 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 09:50:28.949066  8671 net.cpp:245] Setting up res5a_branch2a/bn
I0815 09:50:28.949074  8671 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 43 512 7 7 (1078784)
I0815 09:50:28.949080  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.949084  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.949087  8671 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0815 09:50:28.949090  8671 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 09:50:28.949092  8671 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 09:50:28.949096  8671 net.cpp:245] Setting up res5a_branch2a/relu
I0815 09:50:28.949098  8671 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 43 512 7 7 (1078784)
I0815 09:50:28.949100  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 09:50:28.949103  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.949113  8671 net.cpp:184] Created Layer res5a_branch2b (33)
I0815 09:50:28.949116  8671 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 09:50:28.949120  8671 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 09:50:28.967604  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.44G, req 0G)
I0815 09:50:28.967617  8671 net.cpp:245] Setting up res5a_branch2b
I0815 09:50:28.967622  8671 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 43 512 7 7 (1078784)
I0815 09:50:28.967629  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.967633  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.967639  8671 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0815 09:50:28.967643  8671 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 09:50:28.967644  8671 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 09:50:28.968312  8671 net.cpp:245] Setting up res5a_branch2b/bn
I0815 09:50:28.968322  8671 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 43 512 7 7 (1078784)
I0815 09:50:28.968327  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.968329  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968333  8671 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0815 09:50:28.968335  8671 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 09:50:28.968338  8671 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 09:50:28.968343  8671 net.cpp:245] Setting up res5a_branch2b/relu
I0815 09:50:28.968345  8671 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 43 512 7 7 (1078784)
I0815 09:50:28.968348  8671 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0815 09:50:28.968349  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968354  8671 net.cpp:184] Created Layer pool5 (36)
I0815 09:50:28.968356  8671 net.cpp:561] pool5 <- res5a_branch2b
I0815 09:50:28.968359  8671 net.cpp:530] pool5 -> pool5
I0815 09:50:28.968384  8671 net.cpp:245] Setting up pool5
I0815 09:50:28.968389  8671 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 43 512 1 1 (22016)
I0815 09:50:28.968390  8671 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0815 09:50:28.968401  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968406  8671 net.cpp:184] Created Layer fc1000 (37)
I0815 09:50:28.968410  8671 net.cpp:561] fc1000 <- pool5
I0815 09:50:28.968411  8671 net.cpp:530] fc1000 -> fc1000
I0815 09:50:28.979735  8671 net.cpp:245] Setting up fc1000
I0815 09:50:28.979745  8671 net.cpp:252] TRAIN Top shape for layer 37 'fc1000' 43 1000 (43000)
I0815 09:50:28.979751  8671 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 09:50:28.979755  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.979765  8671 net.cpp:184] Created Layer loss (38)
I0815 09:50:28.979768  8671 net.cpp:561] loss <- fc1000
I0815 09:50:28.979771  8671 net.cpp:561] loss <- label
I0815 09:50:28.979775  8671 net.cpp:530] loss -> loss
I0815 09:50:28.980206  8671 net.cpp:245] Setting up loss
I0815 09:50:28.980214  8671 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0815 09:50:28.980216  8671 net.cpp:256]     with loss weight 1
I0815 09:50:28.980222  8671 net.cpp:323] loss needs backward computation.
I0815 09:50:28.980224  8671 net.cpp:323] fc1000 needs backward computation.
I0815 09:50:28.980227  8671 net.cpp:323] pool5 needs backward computation.
I0815 09:50:28.980229  8671 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 09:50:28.980232  8671 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 09:50:28.980233  8671 net.cpp:323] res5a_branch2b needs backward computation.
I0815 09:50:28.980235  8671 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 09:50:28.980237  8671 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 09:50:28.980239  8671 net.cpp:323] res5a_branch2a needs backward computation.
I0815 09:50:28.980242  8671 net.cpp:323] pool4 needs backward computation.
I0815 09:50:28.980245  8671 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 09:50:28.980247  8671 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 09:50:28.980248  8671 net.cpp:323] res4a_branch2b needs backward computation.
I0815 09:50:28.980250  8671 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 09:50:28.980253  8671 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 09:50:28.980255  8671 net.cpp:323] res4a_branch2a needs backward computation.
I0815 09:50:28.980257  8671 net.cpp:323] pool3 needs backward computation.
I0815 09:50:28.980259  8671 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 09:50:28.980262  8671 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 09:50:28.980264  8671 net.cpp:323] res3a_branch2b needs backward computation.
I0815 09:50:28.980267  8671 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 09:50:28.980268  8671 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 09:50:28.980270  8671 net.cpp:323] res3a_branch2a needs backward computation.
I0815 09:50:28.980273  8671 net.cpp:323] pool2 needs backward computation.
I0815 09:50:28.980275  8671 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 09:50:28.980278  8671 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 09:50:28.980280  8671 net.cpp:323] res2a_branch2b needs backward computation.
I0815 09:50:28.980281  8671 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 09:50:28.980283  8671 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 09:50:28.980286  8671 net.cpp:323] res2a_branch2a needs backward computation.
I0815 09:50:28.980288  8671 net.cpp:323] pool1 needs backward computation.
I0815 09:50:28.980290  8671 net.cpp:323] conv1b/relu needs backward computation.
I0815 09:50:28.980293  8671 net.cpp:323] conv1b/bn needs backward computation.
I0815 09:50:28.980294  8671 net.cpp:323] conv1b needs backward computation.
I0815 09:50:28.980296  8671 net.cpp:323] conv1a/relu needs backward computation.
I0815 09:50:28.980299  8671 net.cpp:323] conv1a/bn needs backward computation.
I0815 09:50:28.980311  8671 net.cpp:323] conv1a needs backward computation.
I0815 09:50:28.980314  8671 net.cpp:325] data/bias does not need backward computation.
I0815 09:50:28.980317  8671 net.cpp:325] data does not need backward computation.
I0815 09:50:28.980319  8671 net.cpp:367] This network produces output loss
I0815 09:50:28.980355  8671 net.cpp:389] Top memory (TRAIN) required for data: 802615296 diff: 802615304
I0815 09:50:28.980358  8671 net.cpp:392] Bottom memory (TRAIN) required for data: 802615296 diff: 802615296
I0815 09:50:28.980360  8671 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 535076864 diff: 535076864
I0815 09:50:28.980362  8671 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0815 09:50:28.980363  8671 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 09:50:28.980366  8671 net.cpp:407] Network initialization done.
I0815 09:50:28.980885  8671 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:28.980942  8671 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:28.981070  8671 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0815 09:50:28.981165  8671 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:28.981170  8671 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:28.981173  8671 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0815 09:50:28.981175  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.981187  8671 net.cpp:184] Created Layer data (0)
I0815 09:50:28.981191  8671 net.cpp:530] data -> data
I0815 09:50:28.981196  8671 net.cpp:530] data -> label
I0815 09:50:28.981205  8671 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:28.981221  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.982070  8747 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:28.982638  8671 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:28.982722  8671 data_layer.cpp:209] (0) Output data size: 17, 3, 224, 224
I0815 09:50:28.982728  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.982748  8671 net.cpp:245] Setting up data
I0815 09:50:28.982755  8671 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 224 224 (2558976)
I0815 09:50:28.982762  8671 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0815 09:50:28.982767  8671 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 09:50:28.982779  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.982787  8671 net.cpp:184] Created Layer label_data_1_split (1)
I0815 09:50:28.982792  8671 net.cpp:561] label_data_1_split <- label
I0815 09:50:28.982797  8671 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 09:50:28.982803  8671 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 09:50:28.982808  8671 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 09:50:28.982875  8671 net.cpp:245] Setting up label_data_1_split
I0815 09:50:28.982882  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982885  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982890  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982894  8671 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 09:50:28.982899  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.982906  8671 net.cpp:184] Created Layer data/bias (2)
I0815 09:50:28.982911  8671 net.cpp:561] data/bias <- data
I0815 09:50:28.982916  8671 net.cpp:530] data/bias -> data/bias
I0815 09:50:28.983103  8671 net.cpp:245] Setting up data/bias
I0815 09:50:28.983109  8671 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 224 224 (2558976)
I0815 09:50:28.983117  8671 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 09:50:28.983122  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.983134  8671 net.cpp:184] Created Layer conv1a (3)
I0815 09:50:28.983139  8671 net.cpp:561] conv1a <- data/bias
I0815 09:50:28.983142  8671 net.cpp:530] conv1a -> conv1a
I0815 09:50:28.984009  8748 data_layer.cpp:97] (0) Parser threads: 1
I0815 09:50:28.984017  8748 data_layer.cpp:99] (0) Transformer threads: 1
I0815 09:50:28.989540  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.4G, req 0G)
I0815 09:50:28.989560  8671 net.cpp:245] Setting up conv1a
I0815 09:50:28.989567  8671 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 112 112 (6823936)
I0815 09:50:28.989579  8671 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 09:50:28.989584  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.989598  8671 net.cpp:184] Created Layer conv1a/bn (4)
I0815 09:50:28.989604  8671 net.cpp:561] conv1a/bn <- conv1a
I0815 09:50:28.989609  8671 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 09:50:28.990360  8671 net.cpp:245] Setting up conv1a/bn
I0815 09:50:28.990368  8671 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 112 112 (6823936)
I0815 09:50:28.990377  8671 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 09:50:28.990381  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.990388  8671 net.cpp:184] Created Layer conv1a/relu (5)
I0815 09:50:28.990392  8671 net.cpp:561] conv1a/relu <- conv1a
I0815 09:50:28.990396  8671 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 09:50:28.990406  8671 net.cpp:245] Setting up conv1a/relu
I0815 09:50:28.990411  8671 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 112 112 (6823936)
I0815 09:50:28.990416  8671 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 09:50:28.990420  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.990432  8671 net.cpp:184] Created Layer conv1b (6)
I0815 09:50:28.990435  8671 net.cpp:561] conv1b <- conv1a
I0815 09:50:28.990439  8671 net.cpp:530] conv1b -> conv1b
I0815 09:50:28.995615  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:28.995625  8671 net.cpp:245] Setting up conv1b
I0815 09:50:28.995631  8671 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 112 112 (6823936)
I0815 09:50:28.995648  8671 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 09:50:28.995652  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.995661  8671 net.cpp:184] Created Layer conv1b/bn (7)
I0815 09:50:28.995666  8671 net.cpp:561] conv1b/bn <- conv1b
I0815 09:50:28.995671  8671 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 09:50:28.996361  8671 net.cpp:245] Setting up conv1b/bn
I0815 09:50:28.996368  8671 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 112 112 (6823936)
I0815 09:50:28.996377  8671 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 09:50:28.996381  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996387  8671 net.cpp:184] Created Layer conv1b/relu (8)
I0815 09:50:28.996392  8671 net.cpp:561] conv1b/relu <- conv1b
I0815 09:50:28.996395  8671 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 09:50:28.996402  8671 net.cpp:245] Setting up conv1b/relu
I0815 09:50:28.996407  8671 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 112 112 (6823936)
I0815 09:50:28.996410  8671 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 09:50:28.996414  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996420  8671 net.cpp:184] Created Layer pool1 (9)
I0815 09:50:28.996424  8671 net.cpp:561] pool1 <- conv1b
I0815 09:50:28.996428  8671 net.cpp:530] pool1 -> pool1
I0815 09:50:28.996492  8671 net.cpp:245] Setting up pool1
I0815 09:50:28.996498  8671 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 56 56 (1705984)
I0815 09:50:28.996502  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 09:50:28.996506  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996515  8671 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 09:50:28.996518  8671 net.cpp:561] res2a_branch2a <- pool1
I0815 09:50:28.996523  8671 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 09:50:29.001720  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.34G, req 0G)
I0815 09:50:29.001732  8671 net.cpp:245] Setting up res2a_branch2a
I0815 09:50:29.001739  8671 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 56 56 (3411968)
I0815 09:50:29.001747  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.001752  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.001760  8671 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 09:50:29.001765  8671 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 09:50:29.001770  8671 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 09:50:29.002534  8671 net.cpp:245] Setting up res2a_branch2a/bn
I0815 09:50:29.002543  8671 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 56 56 (3411968)
I0815 09:50:29.002552  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.002557  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.002562  8671 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 09:50:29.002565  8671 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 09:50:29.002569  8671 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 09:50:29.002578  8671 net.cpp:245] Setting up res2a_branch2a/relu
I0815 09:50:29.002583  8671 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 56 56 (3411968)
I0815 09:50:29.002588  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 09:50:29.002593  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.002604  8671 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 09:50:29.002614  8671 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 09:50:29.002619  8671 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 09:50:29.007417  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0815 09:50:29.007429  8671 net.cpp:245] Setting up res2a_branch2b
I0815 09:50:29.007436  8671 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 56 56 (3411968)
I0815 09:50:29.007443  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.007447  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.007454  8671 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 09:50:29.007459  8671 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 09:50:29.007462  8671 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 09:50:29.008152  8671 net.cpp:245] Setting up res2a_branch2b/bn
I0815 09:50:29.008172  8671 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 56 56 (3411968)
I0815 09:50:29.008183  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.008186  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008190  8671 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 09:50:29.008194  8671 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 09:50:29.008198  8671 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 09:50:29.008203  8671 net.cpp:245] Setting up res2a_branch2b/relu
I0815 09:50:29.008206  8671 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 56 56 (3411968)
I0815 09:50:29.008210  8671 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 09:50:29.008213  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008219  8671 net.cpp:184] Created Layer pool2 (16)
I0815 09:50:29.008224  8671 net.cpp:561] pool2 <- res2a_branch2b
I0815 09:50:29.008229  8671 net.cpp:530] pool2 -> pool2
I0815 09:50:29.008298  8671 net.cpp:245] Setting up pool2
I0815 09:50:29.008304  8671 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 28 28 (852992)
I0815 09:50:29.008306  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 09:50:29.008311  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008317  8671 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 09:50:29.008321  8671 net.cpp:561] res3a_branch2a <- pool2
I0815 09:50:29.008322  8671 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 09:50:29.015259  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.31G, req 0G)
I0815 09:50:29.015271  8671 net.cpp:245] Setting up res3a_branch2a
I0815 09:50:29.015275  8671 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 28 28 (1705984)
I0815 09:50:29.015280  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.015283  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.015288  8671 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 09:50:29.015291  8671 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 09:50:29.015295  8671 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 09:50:29.015980  8671 net.cpp:245] Setting up res3a_branch2a/bn
I0815 09:50:29.015987  8671 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 28 28 (1705984)
I0815 09:50:29.015995  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.015997  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.016000  8671 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 09:50:29.016002  8671 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 09:50:29.016005  8671 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 09:50:29.016017  8671 net.cpp:245] Setting up res3a_branch2a/relu
I0815 09:50:29.016021  8671 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 28 28 (1705984)
I0815 09:50:29.016023  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 09:50:29.016028  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.016038  8671 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 09:50:29.016042  8671 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 09:50:29.016044  8671 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 09:50:29.019454  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.3G, req 0G)
I0815 09:50:29.019464  8671 net.cpp:245] Setting up res3a_branch2b
I0815 09:50:29.019469  8671 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 28 28 (1705984)
I0815 09:50:29.019474  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.019480  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.019484  8671 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 09:50:29.019487  8671 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 09:50:29.019490  8671 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 09:50:29.020169  8671 net.cpp:245] Setting up res3a_branch2b/bn
I0815 09:50:29.020177  8671 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 28 28 (1705984)
I0815 09:50:29.020184  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.020186  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020190  8671 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 09:50:29.020191  8671 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 09:50:29.020193  8671 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 09:50:29.020196  8671 net.cpp:245] Setting up res3a_branch2b/relu
I0815 09:50:29.020200  8671 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 28 28 (1705984)
I0815 09:50:29.020201  8671 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 09:50:29.020203  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020206  8671 net.cpp:184] Created Layer pool3 (23)
I0815 09:50:29.020208  8671 net.cpp:561] pool3 <- res3a_branch2b
I0815 09:50:29.020211  8671 net.cpp:530] pool3 -> pool3
I0815 09:50:29.020273  8671 net.cpp:245] Setting up pool3
I0815 09:50:29.020278  8671 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 14 14 (426496)
I0815 09:50:29.020280  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 09:50:29.020282  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020287  8671 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 09:50:29.020290  8671 net.cpp:561] res4a_branch2a <- pool3
I0815 09:50:29.020292  8671 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 09:50:29.031576  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.29G, req 0G)
I0815 09:50:29.031586  8671 net.cpp:245] Setting up res4a_branch2a
I0815 09:50:29.031590  8671 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 14 14 (852992)
I0815 09:50:29.031595  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.031597  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.031602  8671 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 09:50:29.031605  8671 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 09:50:29.031607  8671 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 09:50:29.032284  8671 net.cpp:245] Setting up res4a_branch2a/bn
I0815 09:50:29.032299  8671 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 14 14 (852992)
I0815 09:50:29.032305  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.032308  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.032311  8671 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 09:50:29.032313  8671 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 09:50:29.032316  8671 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 09:50:29.032320  8671 net.cpp:245] Setting up res4a_branch2a/relu
I0815 09:50:29.032322  8671 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 14 14 (852992)
I0815 09:50:29.032325  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 09:50:29.032326  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.032336  8671 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 09:50:29.032341  8671 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 09:50:29.032342  8671 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 09:50:29.038236  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.28G, req 0G)
I0815 09:50:29.038246  8671 net.cpp:245] Setting up res4a_branch2b
I0815 09:50:29.038250  8671 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 14 14 (852992)
I0815 09:50:29.038254  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.038257  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038266  8671 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 09:50:29.038269  8671 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 09:50:29.038272  8671 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 09:50:29.038949  8671 net.cpp:245] Setting up res4a_branch2b/bn
I0815 09:50:29.038956  8671 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 14 14 (852992)
I0815 09:50:29.038962  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.038964  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038967  8671 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 09:50:29.038970  8671 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 09:50:29.038972  8671 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 09:50:29.038975  8671 net.cpp:245] Setting up res4a_branch2b/relu
I0815 09:50:29.038978  8671 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 14 14 (852992)
I0815 09:50:29.038980  8671 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 09:50:29.038982  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038985  8671 net.cpp:184] Created Layer pool4 (30)
I0815 09:50:29.038987  8671 net.cpp:561] pool4 <- res4a_branch2b
I0815 09:50:29.038990  8671 net.cpp:530] pool4 -> pool4
I0815 09:50:29.039057  8671 net.cpp:245] Setting up pool4
I0815 09:50:29.039062  8671 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 7 7 (213248)
I0815 09:50:29.039064  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 09:50:29.039067  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.039075  8671 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 09:50:29.039079  8671 net.cpp:561] res5a_branch2a <- pool4
I0815 09:50:29.039083  8671 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 09:50:29.069139  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.28G, req 0G)
I0815 09:50:29.069155  8671 net.cpp:245] Setting up res5a_branch2a
I0815 09:50:29.069160  8671 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 7 7 (426496)
I0815 09:50:29.069177  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.069181  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069188  8671 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 09:50:29.069191  8671 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 09:50:29.069196  8671 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 09:50:29.069898  8671 net.cpp:245] Setting up res5a_branch2a/bn
I0815 09:50:29.069906  8671 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 7 7 (426496)
I0815 09:50:29.069912  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.069914  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069921  8671 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 09:50:29.069923  8671 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 09:50:29.069926  8671 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 09:50:29.069931  8671 net.cpp:245] Setting up res5a_branch2a/relu
I0815 09:50:29.069932  8671 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 7 7 (426496)
I0815 09:50:29.069934  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 09:50:29.069937  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069947  8671 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 09:50:29.069949  8671 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 09:50:29.069952  8671 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 09:50:29.086246  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.27G, req 0G)
I0815 09:50:29.086258  8671 net.cpp:245] Setting up res5a_branch2b
I0815 09:50:29.086262  8671 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 7 7 (426496)
I0815 09:50:29.086271  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.086273  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.086277  8671 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 09:50:29.086280  8671 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 09:50:29.086282  8671 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 09:50:29.086974  8671 net.cpp:245] Setting up res5a_branch2b/bn
I0815 09:50:29.086982  8671 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 7 7 (426496)
I0815 09:50:29.086987  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.086990  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.086993  8671 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 09:50:29.086995  8671 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 09:50:29.086998  8671 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 09:50:29.087002  8671 net.cpp:245] Setting up res5a_branch2b/relu
I0815 09:50:29.087004  8671 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 7 7 (426496)
I0815 09:50:29.087007  8671 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0815 09:50:29.087008  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.087011  8671 net.cpp:184] Created Layer pool5 (37)
I0815 09:50:29.087014  8671 net.cpp:561] pool5 <- res5a_branch2b
I0815 09:50:29.087018  8671 net.cpp:530] pool5 -> pool5
I0815 09:50:29.087044  8671 net.cpp:245] Setting up pool5
I0815 09:50:29.087049  8671 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0815 09:50:29.087050  8671 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0815 09:50:29.087052  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.087064  8671 net.cpp:184] Created Layer fc1000 (38)
I0815 09:50:29.087066  8671 net.cpp:561] fc1000 <- pool5
I0815 09:50:29.087069  8671 net.cpp:530] fc1000 -> fc1000
I0815 09:50:29.098291  8671 net.cpp:245] Setting up fc1000
I0815 09:50:29.098315  8671 net.cpp:252] TEST Top shape for layer 38 'fc1000' 17 1000 (17000)
I0815 09:50:29.098323  8671 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0815 09:50:29.098327  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098333  8671 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0815 09:50:29.098348  8671 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0815 09:50:29.098354  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0815 09:50:29.098359  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0815 09:50:29.098364  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0815 09:50:29.098457  8671 net.cpp:245] Setting up fc1000_fc1000_0_split
I0815 09:50:29.098464  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098469  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098475  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098480  8671 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 09:50:29.098484  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098491  8671 net.cpp:184] Created Layer loss (40)
I0815 09:50:29.098495  8671 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0815 09:50:29.098500  8671 net.cpp:561] loss <- label_data_1_split_0
I0815 09:50:29.098505  8671 net.cpp:530] loss -> loss
I0815 09:50:29.098696  8671 net.cpp:245] Setting up loss
I0815 09:50:29.098704  8671 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0815 09:50:29.098707  8671 net.cpp:256]     with loss weight 1
I0815 09:50:29.098716  8671 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 09:50:29.098721  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098734  8671 net.cpp:184] Created Layer accuracy/top1 (41)
I0815 09:50:29.098737  8671 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0815 09:50:29.098742  8671 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 09:50:29.098747  8671 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 09:50:29.098754  8671 net.cpp:245] Setting up accuracy/top1
I0815 09:50:29.098758  8671 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0815 09:50:29.098763  8671 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 09:50:29.098767  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098773  8671 net.cpp:184] Created Layer accuracy/top5 (42)
I0815 09:50:29.098776  8671 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0815 09:50:29.098780  8671 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 09:50:29.098785  8671 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 09:50:29.098793  8671 net.cpp:245] Setting up accuracy/top5
I0815 09:50:29.098798  8671 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0815 09:50:29.098801  8671 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 09:50:29.098805  8671 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 09:50:29.098809  8671 net.cpp:323] loss needs backward computation.
I0815 09:50:29.098814  8671 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0815 09:50:29.098819  8671 net.cpp:323] fc1000 needs backward computation.
I0815 09:50:29.098822  8671 net.cpp:323] pool5 needs backward computation.
I0815 09:50:29.098826  8671 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 09:50:29.098839  8671 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 09:50:29.098842  8671 net.cpp:323] res5a_branch2b needs backward computation.
I0815 09:50:29.098846  8671 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 09:50:29.098850  8671 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 09:50:29.098853  8671 net.cpp:323] res5a_branch2a needs backward computation.
I0815 09:50:29.098858  8671 net.cpp:323] pool4 needs backward computation.
I0815 09:50:29.098862  8671 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 09:50:29.098865  8671 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 09:50:29.098870  8671 net.cpp:323] res4a_branch2b needs backward computation.
I0815 09:50:29.098873  8671 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 09:50:29.098877  8671 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 09:50:29.098881  8671 net.cpp:323] res4a_branch2a needs backward computation.
I0815 09:50:29.098886  8671 net.cpp:323] pool3 needs backward computation.
I0815 09:50:29.098889  8671 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 09:50:29.098893  8671 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 09:50:29.098897  8671 net.cpp:323] res3a_branch2b needs backward computation.
I0815 09:50:29.098901  8671 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 09:50:29.098904  8671 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 09:50:29.098907  8671 net.cpp:323] res3a_branch2a needs backward computation.
I0815 09:50:29.098912  8671 net.cpp:323] pool2 needs backward computation.
I0815 09:50:29.098915  8671 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 09:50:29.098919  8671 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 09:50:29.098923  8671 net.cpp:323] res2a_branch2b needs backward computation.
I0815 09:50:29.098927  8671 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 09:50:29.098932  8671 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 09:50:29.098934  8671 net.cpp:323] res2a_branch2a needs backward computation.
I0815 09:50:29.098938  8671 net.cpp:323] pool1 needs backward computation.
I0815 09:50:29.098942  8671 net.cpp:323] conv1b/relu needs backward computation.
I0815 09:50:29.098947  8671 net.cpp:323] conv1b/bn needs backward computation.
I0815 09:50:29.098950  8671 net.cpp:323] conv1b needs backward computation.
I0815 09:50:29.098954  8671 net.cpp:323] conv1a/relu needs backward computation.
I0815 09:50:29.098958  8671 net.cpp:323] conv1a/bn needs backward computation.
I0815 09:50:29.098963  8671 net.cpp:323] conv1a needs backward computation.
I0815 09:50:29.098966  8671 net.cpp:325] data/bias does not need backward computation.
I0815 09:50:29.098971  8671 net.cpp:325] label_data_1_split does not need backward computation.
I0815 09:50:29.098975  8671 net.cpp:325] data does not need backward computation.
I0815 09:50:29.098978  8671 net.cpp:367] This network produces output accuracy/top1
I0815 09:50:29.098983  8671 net.cpp:367] This network produces output accuracy/top5
I0815 09:50:29.098986  8671 net.cpp:367] This network produces output loss
I0815 09:50:29.099017  8671 net.cpp:389] Top memory (TEST) required for data: 317313024 diff: 8
I0815 09:50:29.099021  8671 net.cpp:392] Bottom memory (TEST) required for data: 317313024 diff: 317313024
I0815 09:50:29.099025  8671 net.cpp:395] Shared (in-place) memory (TEST) by data: 211542016 diff: 211542016
I0815 09:50:29.099028  8671 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0815 09:50:29.099031  8671 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 09:50:29.099035  8671 net.cpp:407] Network initialization done.
I0815 09:50:29.099092  8671 solver.cpp:56] Solver scaffolding done.
I0815 09:50:29.103351  8671 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_320000.caffemodel
I0815 09:50:29.109087  8671 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 09:50:29.109117  8671 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 09:50:29.109182  8671 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 09:50:29.109206  8671 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.109616  8671 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 09:50:29.109625  8671 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 09:50:29.109637  8671 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.109848  8671 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 09:50:29.109858  8671 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 09:50:29.109860  8671 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.109884  8671 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110081  8671 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110088  8671 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110106  8671 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110291  8671 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.110298  8671 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 09:50:29.110301  8671 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.110340  8671 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110471  8671 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110477  8671 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110499  8671 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110621  8671 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.110626  8671 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 09:50:29.110628  8671 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.110749  8671 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110877  8671 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110882  8671 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110949  8671 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111070  8671 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.111075  8671 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 09:50:29.111078  8671 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.111495  8671 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111631  8671 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.111637  8671 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.111840  8671 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111963  8671 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.111968  8671 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0815 09:50:29.111971  8671 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0815 09:50:29.112162  8671 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 09:50:29.116477  8671 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 09:50:29.116492  8671 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 09:50:29.116524  8671 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 09:50:29.116546  8671 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.116806  8671 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 09:50:29.116812  8671 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 09:50:29.116821  8671 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.116969  8671 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 09:50:29.116974  8671 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 09:50:29.116976  8671 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.116991  8671 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117146  8671 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117151  8671 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117163  8671 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117307  8671 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.117312  8671 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 09:50:29.117314  8671 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.117370  8671 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117509  8671 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117514  8671 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117535  8671 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117656  8671 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.117660  8671 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 09:50:29.117663  8671 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.117791  8671 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117920  8671 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117925  8671 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117990  8671 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.118111  8671 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.118116  8671 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 09:50:29.118119  8671 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.118551  8671 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.118677  8671 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.118682  8671 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.118887  8671 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.119009  8671 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.119014  8671 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0815 09:50:29.119019  8671 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0815 09:50:29.119189  8671 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 09:50:29.119256  8671 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 09:50:29.119259  8671 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 09:50:29.119262  8671 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 09:50:29.119263  8671 parallel.cpp:59] Starting Optimization
I0815 09:50:29.119266  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:29.119284  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.119310  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.119946  8764 device_alternate.hpp:116] NVML initialized on thread 140247644952320
I0815 09:50:29.137429  8764 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 09:50:29.137454  8765 device_alternate.hpp:116] NVML initialized on thread 140247636559616
I0815 09:50:29.138136  8765 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 09:50:29.138190  8766 device_alternate.hpp:116] NVML initialized on thread 140247628166912
I0815 09:50:29.139082  8766 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 09:50:29.142688  8765 solver.cpp:42] Solver data type: FLOAT
W0815 09:50:29.143046  8765 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:29.143148  8765 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.143155  8765 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.143187  8765 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:29.143200  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.146636  8766 solver.cpp:42] Solver data type: FLOAT
W0815 09:50:29.146980  8766 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:29.147043  8766 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.147047  8766 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.147068  8766 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:29.147078  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.147315  8767 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:29.148075  8768 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:29.149785  8765 data_layer.cpp:185] [1] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:29.150135  8766 data_layer.cpp:185] [2] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:29.150398  8765 data_layer.cpp:209] [1] Output data size: 43, 3, 224, 224
I0815 09:50:29.150408  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.150449  8766 data_layer.cpp:209] [2] Output data size: 43, 3, 224, 224
I0815 09:50:29.150457  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.642834  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0815 09:50:29.659576  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0815 09:50:29.670234  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0815 09:50:29.686518  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0815 09:50:29.695930  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0815 09:50:29.709318  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0815 09:50:29.714108  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0815 09:50:29.728184  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0815 09:50:29.734447  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.68G, req 0G)
I0815 09:50:29.743526  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0815 09:50:29.751754  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.68G, req 0G)
I0815 09:50:29.761013  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0815 09:50:29.771639  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0815 09:50:29.782881  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0815 09:50:29.789307  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0815 09:50:29.799487  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0815 09:50:29.834940  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0815 09:50:29.852519  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0815 09:50:29.855957  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.53G, req 0G)
I0815 09:50:29.868577  8765 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:29.868638  8765 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:29.868731  8765 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.868736  8765 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.868752  8765 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:29.868760  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.869946  8771 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:29.871270  8765 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:29.871418  8765 data_layer.cpp:209] (1) Output data size: 17, 3, 224, 224
I0815 09:50:29.871428  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.872181  8772 data_layer.cpp:97] (1) Parser threads: 1
I0815 09:50:29.872190  8772 data_layer.cpp:99] (1) Transformer threads: 1
I0815 09:50:29.874037  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 1 1 3  (limit 7.53G, req 0G)
I0815 09:50:29.878528  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0815 09:50:29.885160  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0815 09:50:29.890936  8766 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:29.890998  8766 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:29.891126  8766 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.891134  8766 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.891154  8766 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:29.891163  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.891903  8773 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:29.892587  8766 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:29.892735  8766 data_layer.cpp:209] (2) Output data size: 17, 3, 224, 224
I0815 09:50:29.892741  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.894080  8774 data_layer.cpp:97] (2) Parser threads: 1
I0815 09:50:29.894090  8774 data_layer.cpp:99] (2) Transformer threads: 1
I0815 09:50:29.894641  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.43G, req 0G)
I0815 09:50:29.901062  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0815 09:50:29.904008  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0815 09:50:29.908491  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0815 09:50:29.912778  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0815 09:50:29.916293  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.43G, req 0G)
I0815 09:50:29.919464  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0815 09:50:29.922703  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0815 09:50:29.931401  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0815 09:50:29.935210  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0815 09:50:29.935860  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0815 09:50:29.942101  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0815 09:50:29.950568  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0815 09:50:29.957403  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0815 09:50:29.978293  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0815 09:50:29.988998  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0815 09:50:29.998667  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:30.006583  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:30.013453  8765 solver.cpp:56] Solver scaffolding done.
I0815 09:50:30.019537  8766 solver.cpp:56] Solver scaffolding done.
I0815 09:50:30.062592  8766 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 09:50:30.062592  8765 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 09:50:30.062618  8764 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 09:50:30.255599  8764 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 09:50:30.263182  8765 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.263200  8765 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.263219  8766 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.263229  8766 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.269048  8764 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.269058  8764 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.278180  8766 solver.cpp:227] Starting Optimization on GPU 2
I0815 09:50:30.278182  8764 solver.cpp:227] Starting Optimization on GPU 0
I0815 09:50:30.278180  8765 solver.cpp:227] Starting Optimization on GPU 1
I0815 09:50:30.278375  8764 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 09:50:30.278388  8792 device_alternate.hpp:116] NVML initialized on thread 140105801520896
I0815 09:50:30.278409  8792 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 09:50:30.278421  8794 device_alternate.hpp:116] NVML initialized on thread 140105793128192
I0815 09:50:30.278435  8794 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 09:50:30.278548  8793 device_alternate.hpp:116] NVML initialized on thread 140105809913600
I0815 09:50:30.278563  8793 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 09:50:30.286167  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.3G, req 0G)
I0815 09:50:30.295697  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.24G, req 0G)
I0815 09:50:30.296815  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.3G, req 0G)
I0815 09:50:30.307793  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.24G, req 0G)
I0815 09:50:30.309789  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.17G, req 0G)
I0815 09:50:30.311377  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.24G, req 0G)
I0815 09:50:30.317014  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.14G, req 0G)
I0815 09:50:30.319846  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.17G, req 0G)
I0815 09:50:30.323022  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.17G, req 0G)
I0815 09:50:30.324574  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.11G, req 0G)
I0815 09:50:30.326530  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.14G, req 0G)
I0815 09:50:30.330193  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 09:50:30.333767  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 09:50:30.334270  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.11G, req 0G)
I0815 09:50:30.337064  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0815 09:50:30.339977  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 09:50:30.340641  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.341563  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.347712  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0815 09:50:30.349736  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.04G, req 0G)
I0815 09:50:30.350428  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.06G, req 0G)
I0815 09:50:30.353463  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.356401  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 09:50:30.357349  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 09:50:30.360445  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.06G, req 0G)
I0815 09:50:30.364109  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 09:50:30.364617  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 09:50:30.368089  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 09:50:30.373881  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 6.99G, req 0G)
I0815 09:50:30.377813  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 6.99G, req 0G)
I0815 09:50:30.380987  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.411765
I0815 09:50:30.380998  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.647059
I0815 09:50:30.381002  8764 solver.cpp:594]     Test net output #2: loss = 2.59704 (* 1 = 2.59704 loss)
I0815 09:50:30.381017  8764 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 09:50:30.381031  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:50:30.472728  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 1  (limit 7G, req 0G)
I0815 09:50:30.479776  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.93G, req 0G)
I0815 09:50:30.480700  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7G, req 0G)
I0815 09:50:30.503252  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.86G, req 0G)
I0815 09:50:30.509383  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.79G, req 0G)
I0815 09:50:30.515101  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.86G, req 0G)
I0815 09:50:30.536636  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 6.7G, req 0G)
I0815 09:50:30.541236  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 6.63G, req 0G)
I0815 09:50:30.547163  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 6.7G, req 0G)
I0815 09:50:30.551407  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.63G, req 0G)
I0815 09:50:30.560731  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.56G, req 0G)
I0815 09:50:30.563598  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.63G, req 0G)
I0815 09:50:30.581929  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 6.55G, req 0G)
I0815 09:50:30.583058  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.48G, req 0G)
I0815 09:50:30.587162  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.55G, req 0G)
I0815 09:50:30.592979  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.51G, req 0G)
I0815 09:50:30.593278  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.44G, req 0G)
I0815 09:50:30.597041  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.51G, req 0G)
I0815 09:50:30.614681  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.47G, req 0G)
I0815 09:50:30.615087  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 6.39G, req 0G)
I0815 09:50:30.618422  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.47G, req 0G)
I0815 09:50:30.623517  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.45G, req 0G)
I0815 09:50:30.623908  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.37G, req 0G)
I0815 09:50:30.627024  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.45G, req 0G)
I0815 09:50:30.649219  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 6.42G, req 0G)
I0815 09:50:30.650141  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 6.34G, req 0G)
I0815 09:50:30.653628  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 6.42G, req 0G)
I0815 09:50:30.658583  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.41G, req 0G)
I0815 09:50:30.658764  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.33G, req 0G)
I0815 09:50:30.661267  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.41G, req 0G)
I0815 09:50:30.718473  8729 data_layer.cpp:97] [0] Parser threads: 1
I0815 09:50:30.718488  8729 data_layer.cpp:99] [0] Transformer threads: 1
I0815 09:50:30.720818  8769 data_layer.cpp:97] [1] Parser threads: 1
I0815 09:50:30.720831  8769 data_layer.cpp:99] [1] Transformer threads: 1
I0815 09:50:30.722805  8770 data_layer.cpp:97] [2] Parser threads: 1
I0815 09:50:30.722815  8770 data_layer.cpp:99] [2] Transformer threads: 1
I0815 09:50:30.881630  8764 solver.cpp:317] Iteration 0 (0.500577 s), loss = 1.12189
I0815 09:50:30.881651  8764 solver.cpp:334]     Train net output #0: loss = 1.07472 (* 1 = 1.07472 loss)
I0815 09:50:30.881655  8764 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0815 09:50:30.909463  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.4G, req 0G)
I0815 09:50:30.909730  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.4G, req 0G)
I0815 09:50:30.917467  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.31G, req 0G)
I0815 09:50:30.962762  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:30.963400  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:30.965306  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.006168  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.010534  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.012485  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.023758  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.029104  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.69G, req 0G)
I0815 09:50:31.031196  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.69G, req 0G)
I0815 09:50:31.050061  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 1  (limit 4.6G, req 0G)
I0815 09:50:31.058642  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.062106  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.062432  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.070116  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.073978  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.088513  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.096323  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0.04G)
I0815 09:50:31.099578  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.103124  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.108814  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.112413  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.144206  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.155078  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 1 1 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.158326  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.162051  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.169030  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 7 1 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.172641  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 7 1 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.196440  8764 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.43G -> 0.09G
I0815 09:50:31.212311  8766 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0815 09:50:31.217344  8765 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0815 09:50:31.308323  8764 solver.cpp:317] Iteration 1 (0.426673 s), loss = 0.943738
I0815 09:50:31.308393  8764 solver.cpp:334]     Train net output #0: loss = 0.965158 (* 1 = 0.965158 loss)
I0815 09:50:31.463210  8764 solver.cpp:317] Iteration 2 (0.154872 s), loss = 1.35809
I0815 09:50:31.463241  8764 solver.cpp:334]     Train net output #0: loss = 1.45321 (* 1 = 1.45321 loss)
I0815 09:50:45.508155  8764 solver.cpp:312] Iteration 100 (6.97779 iter/s, 14.0446s/98 iter), loss = 1.45671
I0815 09:50:45.508206  8764 solver.cpp:334]     Train net output #0: loss = 1.44152 (* 1 = 1.44152 loss)
I0815 09:50:45.508219  8764 sgd_solver.cpp:136] Iteration 100, lr = 0.00999375, m = 0.9
I0815 09:51:00.027153  8764 solver.cpp:312] Iteration 200 (6.88772 iter/s, 14.5186s/100 iter), loss = 1.53172
I0815 09:51:00.027304  8764 solver.cpp:334]     Train net output #0: loss = 1.39721 (* 1 = 1.39721 loss)
I0815 09:51:00.027323  8764 sgd_solver.cpp:136] Iteration 200, lr = 0.0099875, m = 0.9
I0815 09:51:14.657567  8764 solver.cpp:312] Iteration 300 (6.83527 iter/s, 14.63s/100 iter), loss = 1.73236
I0815 09:51:14.657625  8764 solver.cpp:334]     Train net output #0: loss = 1.48686 (* 1 = 1.48686 loss)
I0815 09:51:14.657639  8764 sgd_solver.cpp:136] Iteration 300, lr = 0.00998125, m = 0.9
I0815 09:51:29.081573  8764 solver.cpp:312] Iteration 400 (6.93308 iter/s, 14.4236s/100 iter), loss = 1.49868
I0815 09:51:29.081600  8764 solver.cpp:334]     Train net output #0: loss = 1.79943 (* 1 = 1.79943 loss)
I0815 09:51:29.081606  8764 sgd_solver.cpp:136] Iteration 400, lr = 0.009975, m = 0.9
I0815 09:51:43.539085  8764 solver.cpp:312] Iteration 500 (6.91701 iter/s, 14.4571s/100 iter), loss = 1.23975
I0815 09:51:43.539156  8764 solver.cpp:334]     Train net output #0: loss = 1.15057 (* 1 = 1.15057 loss)
I0815 09:51:43.539163  8764 sgd_solver.cpp:136] Iteration 500, lr = 0.00996875, m = 0.9
I0815 09:51:57.878718  8764 solver.cpp:312] Iteration 600 (6.97387 iter/s, 14.3392s/100 iter), loss = 1.24526
I0815 09:51:57.878739  8764 solver.cpp:334]     Train net output #0: loss = 1.32154 (* 1 = 1.32154 loss)
I0815 09:51:57.878829  8764 sgd_solver.cpp:136] Iteration 600, lr = 0.0099625, m = 0.9
I0815 09:52:12.227082  8764 solver.cpp:312] Iteration 700 (6.96963 iter/s, 14.348s/100 iter), loss = 1.25718
I0815 09:52:12.227110  8764 solver.cpp:334]     Train net output #0: loss = 1.42179 (* 1 = 1.42179 loss)
I0815 09:52:12.227118  8764 sgd_solver.cpp:136] Iteration 700, lr = 0.00995625, m = 0.9
I0815 09:52:26.558117  8764 solver.cpp:312] Iteration 800 (6.97806 iter/s, 14.3306s/100 iter), loss = 1.49093
I0815 09:52:26.558198  8764 solver.cpp:334]     Train net output #0: loss = 1.46195 (* 1 = 1.46195 loss)
I0815 09:52:26.558205  8764 sgd_solver.cpp:136] Iteration 800, lr = 0.00995, m = 0.9
I0815 09:52:41.116451  8764 solver.cpp:312] Iteration 900 (6.86911 iter/s, 14.5579s/100 iter), loss = 1.35589
I0815 09:52:41.116514  8764 solver.cpp:334]     Train net output #0: loss = 1.64769 (* 1 = 1.64769 loss)
I0815 09:52:41.116531  8764 sgd_solver.cpp:136] Iteration 900, lr = 0.00994375, m = 0.9
I0815 09:52:55.512455  8764 solver.cpp:363] Sparsity after update:
I0815 09:52:55.527741  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:52:55.527756  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:52:55.527770  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:52:55.527776  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:52:55.527778  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:52:55.527781  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:52:55.527784  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:52:55.527788  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:52:55.527792  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:52:55.527796  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:52:55.527798  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:52:55.527802  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:52:55.527806  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:52:55.663664  8764 solver.cpp:312] Iteration 1000 (6.87436 iter/s, 14.5468s/100 iter), loss = 1.42743
I0815 09:52:55.663724  8764 solver.cpp:334]     Train net output #0: loss = 1.28583 (* 1 = 1.28583 loss)
I0815 09:52:55.663741  8764 sgd_solver.cpp:136] Iteration 1000, lr = 0.0099375, m = 0.9
I0815 09:53:10.044531  8764 solver.cpp:312] Iteration 1100 (6.95388 iter/s, 14.3805s/100 iter), loss = 1.4919
I0815 09:53:10.044610  8764 solver.cpp:334]     Train net output #0: loss = 1.48686 (* 1 = 1.48686 loss)
I0815 09:53:10.044620  8764 sgd_solver.cpp:136] Iteration 1100, lr = 0.00993125, m = 0.9
I0815 09:53:24.620254  8764 solver.cpp:312] Iteration 1200 (6.86091 iter/s, 14.5753s/100 iter), loss = 1.38441
I0815 09:53:24.620278  8764 solver.cpp:334]     Train net output #0: loss = 1.45225 (* 1 = 1.45225 loss)
I0815 09:53:24.620283  8764 sgd_solver.cpp:136] Iteration 1200, lr = 0.009925, m = 0.9
I0815 09:53:39.148172  8764 solver.cpp:312] Iteration 1300 (6.88349 iter/s, 14.5275s/100 iter), loss = 1.19057
I0815 09:53:39.148201  8764 solver.cpp:334]     Train net output #0: loss = 0.978157 (* 1 = 0.978157 loss)
I0815 09:53:39.148208  8764 sgd_solver.cpp:136] Iteration 1300, lr = 0.00991875, m = 0.9
I0815 09:53:53.726241  8764 solver.cpp:312] Iteration 1400 (6.85981 iter/s, 14.5777s/100 iter), loss = 1.3075
I0815 09:53:53.726337  8764 solver.cpp:334]     Train net output #0: loss = 1.17399 (* 1 = 1.17399 loss)
I0815 09:53:53.726357  8764 sgd_solver.cpp:136] Iteration 1400, lr = 0.0099125, m = 0.9
I0815 09:54:08.560456  8764 solver.cpp:312] Iteration 1500 (6.74136 iter/s, 14.8338s/100 iter), loss = 1.5068
I0815 09:54:08.560479  8764 solver.cpp:334]     Train net output #0: loss = 1.54028 (* 1 = 1.54028 loss)
I0815 09:54:08.560483  8764 sgd_solver.cpp:136] Iteration 1500, lr = 0.00990625, m = 0.9
I0815 09:54:23.292912  8764 solver.cpp:312] Iteration 1600 (6.78792 iter/s, 14.7321s/100 iter), loss = 1.69717
I0815 09:54:23.292937  8764 solver.cpp:334]     Train net output #0: loss = 1.74796 (* 1 = 1.74796 loss)
I0815 09:54:23.292940  8764 sgd_solver.cpp:136] Iteration 1600, lr = 0.0099, m = 0.9
I0815 09:54:37.766944  8764 solver.cpp:312] Iteration 1700 (6.90912 iter/s, 14.4736s/100 iter), loss = 1.3926
I0815 09:54:37.767074  8764 solver.cpp:334]     Train net output #0: loss = 1.51627 (* 1 = 1.51627 loss)
I0815 09:54:37.767093  8764 sgd_solver.cpp:136] Iteration 1700, lr = 0.00989375, m = 0.9
I0815 09:54:52.323420  8764 solver.cpp:312] Iteration 1800 (6.86998 iter/s, 14.5561s/100 iter), loss = 1.56322
I0815 09:54:52.323560  8764 solver.cpp:334]     Train net output #0: loss = 1.73 (* 1 = 1.73 loss)
I0815 09:54:52.323575  8764 sgd_solver.cpp:136] Iteration 1800, lr = 0.0098875, m = 0.9
I0815 09:55:07.145094  8764 solver.cpp:312] Iteration 1900 (6.74706 iter/s, 14.8213s/100 iter), loss = 1.43671
I0815 09:55:07.145241  8764 solver.cpp:334]     Train net output #0: loss = 1.74979 (* 1 = 1.74979 loss)
I0815 09:55:07.145309  8764 sgd_solver.cpp:136] Iteration 1900, lr = 0.00988125, m = 0.9
I0815 09:55:21.824743  8764 solver.cpp:363] Sparsity after update:
I0815 09:55:21.835436  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:55:21.835455  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:55:21.835461  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:55:21.835464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:55:21.835466  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:55:21.835469  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:55:21.835472  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:55:21.835475  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:55:21.835479  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:55:21.835490  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:55:21.835494  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:55:21.835499  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:55:21.835501  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:55:21.835513  8764 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 09:55:41.159482  8747 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 09:55:42.320375  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.532588
I0815 09:55:42.320402  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.771704
I0815 09:55:42.320410  8764 solver.cpp:594]     Test net output #2: loss = 2.08463 (* 1 = 2.08463 loss)
I0815 09:55:42.320430  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.4844s
I0815 09:55:42.489234  8764 solver.cpp:312] Iteration 2000 (2.8294 iter/s, 35.3432s/100 iter), loss = 1.57945
I0815 09:55:42.489411  8764 solver.cpp:334]     Train net output #0: loss = 1.76152 (* 1 = 1.76152 loss)
I0815 09:55:42.489500  8764 sgd_solver.cpp:136] Iteration 2000, lr = 0.009875, m = 0.9
I0815 09:55:56.977505  8764 solver.cpp:312] Iteration 2100 (6.90233 iter/s, 14.4879s/100 iter), loss = 1.34038
I0815 09:55:56.977557  8764 solver.cpp:334]     Train net output #0: loss = 1.43644 (* 1 = 1.43644 loss)
I0815 09:55:56.977563  8764 sgd_solver.cpp:136] Iteration 2100, lr = 0.00986875, m = 0.9
I0815 09:56:11.428660  8764 solver.cpp:312] Iteration 2200 (6.92005 iter/s, 14.4508s/100 iter), loss = 1.61807
I0815 09:56:11.428686  8764 solver.cpp:334]     Train net output #0: loss = 1.85548 (* 1 = 1.85548 loss)
I0815 09:56:11.428691  8764 sgd_solver.cpp:136] Iteration 2200, lr = 0.0098625, m = 0.9
I0815 09:56:26.226137  8764 solver.cpp:312] Iteration 2300 (6.7581 iter/s, 14.7971s/100 iter), loss = 1.40816
I0815 09:56:26.226209  8764 solver.cpp:334]     Train net output #0: loss = 1.26492 (* 1 = 1.26492 loss)
I0815 09:56:26.226225  8764 sgd_solver.cpp:136] Iteration 2300, lr = 0.00985625, m = 0.9
I0815 09:56:40.952759  8764 solver.cpp:312] Iteration 2400 (6.79061 iter/s, 14.7262s/100 iter), loss = 1.54934
I0815 09:56:40.952890  8764 solver.cpp:334]     Train net output #0: loss = 1.50371 (* 1 = 1.50371 loss)
I0815 09:56:40.952913  8764 sgd_solver.cpp:136] Iteration 2400, lr = 0.00985, m = 0.9
I0815 09:56:55.548861  8764 solver.cpp:312] Iteration 2500 (6.85133 iter/s, 14.5957s/100 iter), loss = 1.19602
I0815 09:56:55.548889  8764 solver.cpp:334]     Train net output #0: loss = 1.01414 (* 1 = 1.01414 loss)
I0815 09:56:55.548895  8764 sgd_solver.cpp:136] Iteration 2500, lr = 0.00984375, m = 0.9
I0815 09:57:10.292160  8764 solver.cpp:312] Iteration 2600 (6.78293 iter/s, 14.7429s/100 iter), loss = 1.76389
I0815 09:57:10.292227  8764 solver.cpp:334]     Train net output #0: loss = 1.90808 (* 1 = 1.90808 loss)
I0815 09:57:10.292249  8764 sgd_solver.cpp:136] Iteration 2600, lr = 0.0098375, m = 0.9
I0815 09:57:24.831439  8764 solver.cpp:312] Iteration 2700 (6.87811 iter/s, 14.5389s/100 iter), loss = 1.26027
I0815 09:57:24.831634  8764 solver.cpp:334]     Train net output #0: loss = 1.29005 (* 1 = 1.29005 loss)
I0815 09:57:24.831653  8764 sgd_solver.cpp:136] Iteration 2700, lr = 0.00983125, m = 0.9
I0815 09:57:39.398829  8764 solver.cpp:312] Iteration 2800 (6.86484 iter/s, 14.567s/100 iter), loss = 1.44491
I0815 09:57:39.399049  8764 solver.cpp:334]     Train net output #0: loss = 1.67055 (* 1 = 1.67055 loss)
I0815 09:57:39.399157  8764 sgd_solver.cpp:136] Iteration 2800, lr = 0.009825, m = 0.9
I0815 09:57:54.184928  8764 solver.cpp:312] Iteration 2900 (6.7633 iter/s, 14.7857s/100 iter), loss = 1.4977
I0815 09:57:54.184957  8764 solver.cpp:334]     Train net output #0: loss = 1.46464 (* 1 = 1.46464 loss)
I0815 09:57:54.184963  8764 sgd_solver.cpp:136] Iteration 2900, lr = 0.00981875, m = 0.9
I0815 09:58:08.600488  8764 solver.cpp:363] Sparsity after update:
I0815 09:58:08.613029  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:58:08.613046  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:58:08.613054  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:58:08.613057  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:58:08.613061  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:58:08.613065  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:58:08.613067  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:58:08.613070  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:58:08.613073  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:58:08.613076  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:58:08.613080  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:58:08.613082  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:58:08.613085  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:58:08.747040  8764 solver.cpp:312] Iteration 3000 (6.86732 iter/s, 14.5617s/100 iter), loss = 1.48742
I0815 09:58:08.747104  8764 solver.cpp:334]     Train net output #0: loss = 1.4992 (* 1 = 1.4992 loss)
I0815 09:58:08.747122  8764 sgd_solver.cpp:136] Iteration 3000, lr = 0.0098125, m = 0.9
I0815 09:58:23.285490  8764 solver.cpp:312] Iteration 3100 (6.8785 iter/s, 14.5381s/100 iter), loss = 1.64661
I0815 09:58:23.285518  8764 solver.cpp:334]     Train net output #0: loss = 1.38105 (* 1 = 1.38105 loss)
I0815 09:58:23.285523  8764 sgd_solver.cpp:136] Iteration 3100, lr = 0.00980625, m = 0.9
I0815 09:58:37.973593  8764 solver.cpp:312] Iteration 3200 (6.80842 iter/s, 14.6877s/100 iter), loss = 1.45851
I0815 09:58:37.973623  8764 solver.cpp:334]     Train net output #0: loss = 1.53664 (* 1 = 1.53664 loss)
I0815 09:58:37.973630  8764 sgd_solver.cpp:136] Iteration 3200, lr = 0.0098, m = 0.9
I0815 09:58:52.581856  8764 solver.cpp:312] Iteration 3300 (6.84563 iter/s, 14.6079s/100 iter), loss = 1.24967
I0815 09:58:52.581924  8764 solver.cpp:334]     Train net output #0: loss = 1.31604 (* 1 = 1.31604 loss)
I0815 09:58:52.581931  8764 sgd_solver.cpp:136] Iteration 3300, lr = 0.00979375, m = 0.9
I0815 09:59:07.334108  8764 solver.cpp:312] Iteration 3400 (6.77881 iter/s, 14.7518s/100 iter), loss = 1.58056
I0815 09:59:07.334136  8764 solver.cpp:334]     Train net output #0: loss = 1.60852 (* 1 = 1.60852 loss)
I0815 09:59:07.334142  8764 sgd_solver.cpp:136] Iteration 3400, lr = 0.0097875, m = 0.9
I0815 09:59:21.718279  8764 solver.cpp:312] Iteration 3500 (6.95228 iter/s, 14.3838s/100 iter), loss = 1.92395
I0815 09:59:21.718350  8764 solver.cpp:334]     Train net output #0: loss = 1.60401 (* 1 = 1.60401 loss)
I0815 09:59:21.718369  8764 sgd_solver.cpp:136] Iteration 3500, lr = 0.00978125, m = 0.9
I0815 09:59:35.966784  8764 solver.cpp:312] Iteration 3600 (7.01847 iter/s, 14.2481s/100 iter), loss = 1.61741
I0815 09:59:35.966866  8764 solver.cpp:334]     Train net output #0: loss = 1.76983 (* 1 = 1.76983 loss)
I0815 09:59:35.966873  8764 sgd_solver.cpp:136] Iteration 3600, lr = 0.009775, m = 0.9
I0815 09:59:50.886339  8764 solver.cpp:312] Iteration 3700 (6.7028 iter/s, 14.9191s/100 iter), loss = 1.64132
I0815 09:59:50.886409  8764 solver.cpp:334]     Train net output #0: loss = 2.37452 (* 1 = 2.37452 loss)
I0815 09:59:50.886428  8764 sgd_solver.cpp:136] Iteration 3700, lr = 0.00976875, m = 0.9
I0815 10:00:05.344192  8764 solver.cpp:312] Iteration 3800 (6.91685 iter/s, 14.4575s/100 iter), loss = 1.88628
I0815 10:00:05.344220  8764 solver.cpp:334]     Train net output #0: loss = 2.00306 (* 1 = 2.00306 loss)
I0815 10:00:05.344226  8764 sgd_solver.cpp:136] Iteration 3800, lr = 0.0097625, m = 0.9
I0815 10:00:19.785884  8764 solver.cpp:312] Iteration 3900 (6.92459 iter/s, 14.4413s/100 iter), loss = 1.61876
I0815 10:00:19.788166  8764 solver.cpp:334]     Train net output #0: loss = 1.84803 (* 1 = 1.84803 loss)
I0815 10:00:19.788179  8764 sgd_solver.cpp:136] Iteration 3900, lr = 0.00975625, m = 0.9
I0815 10:00:34.244745  8764 solver.cpp:363] Sparsity after update:
I0815 10:00:34.248956  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:00:34.248980  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:00:34.248993  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:00:34.249001  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:00:34.249009  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:00:34.249017  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:00:34.249025  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:00:34.249033  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:00:34.249042  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:00:34.249048  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:00:34.249056  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:00:34.249064  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:00:34.249073  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:00:34.249088  8764 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 10:00:55.193785  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.552471
I0815 10:00:55.193835  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.78788
I0815 10:00:55.193840  8764 solver.cpp:594]     Test net output #2: loss = 1.98977 (* 1 = 1.98977 loss)
I0815 10:00:55.193859  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.9442s
I0815 10:00:55.342311  8764 solver.cpp:312] Iteration 4000 (2.81251 iter/s, 35.5555s/100 iter), loss = 1.44905
I0815 10:00:55.342339  8764 solver.cpp:334]     Train net output #0: loss = 1.55423 (* 1 = 1.55423 loss)
I0815 10:00:55.342345  8764 sgd_solver.cpp:136] Iteration 4000, lr = 0.00975, m = 0.9
I0815 10:01:09.781297  8764 solver.cpp:312] Iteration 4100 (6.92589 iter/s, 14.4386s/100 iter), loss = 1.39839
I0815 10:01:09.781325  8764 solver.cpp:334]     Train net output #0: loss = 1.49331 (* 1 = 1.49331 loss)
I0815 10:01:09.781330  8764 sgd_solver.cpp:136] Iteration 4100, lr = 0.00974375, m = 0.9
I0815 10:01:24.324801  8764 solver.cpp:312] Iteration 4200 (6.87611 iter/s, 14.5431s/100 iter), loss = 1.31043
I0815 10:01:24.324873  8764 solver.cpp:334]     Train net output #0: loss = 1.32433 (* 1 = 1.32433 loss)
I0815 10:01:24.324893  8764 sgd_solver.cpp:136] Iteration 4200, lr = 0.0097375, m = 0.9
I0815 10:01:39.014941  8764 solver.cpp:312] Iteration 4300 (6.80748 iter/s, 14.6897s/100 iter), loss = 1.7333
I0815 10:01:39.015000  8764 solver.cpp:334]     Train net output #0: loss = 1.90661 (* 1 = 1.90661 loss)
I0815 10:01:39.015007  8764 sgd_solver.cpp:136] Iteration 4300, lr = 0.00973125, m = 0.9
I0815 10:01:53.663494  8764 solver.cpp:312] Iteration 4400 (6.8268 iter/s, 14.6481s/100 iter), loss = 1.60305
I0815 10:01:53.663570  8764 solver.cpp:334]     Train net output #0: loss = 1.74804 (* 1 = 1.74804 loss)
I0815 10:01:53.663590  8764 sgd_solver.cpp:136] Iteration 4400, lr = 0.009725, m = 0.9
I0815 10:02:08.045459  8764 solver.cpp:312] Iteration 4500 (6.95334 iter/s, 14.3816s/100 iter), loss = 1.75029
I0815 10:02:08.045526  8764 solver.cpp:334]     Train net output #0: loss = 1.97829 (* 1 = 1.97829 loss)
I0815 10:02:08.045545  8764 sgd_solver.cpp:136] Iteration 4500, lr = 0.00971875, m = 0.9
I0815 10:02:22.421499  8764 solver.cpp:312] Iteration 4600 (6.95621 iter/s, 14.3756s/100 iter), loss = 1.55973
I0815 10:02:22.421572  8764 solver.cpp:334]     Train net output #0: loss = 1.50551 (* 1 = 1.50551 loss)
I0815 10:02:22.421579  8764 sgd_solver.cpp:136] Iteration 4600, lr = 0.0097125, m = 0.9
I0815 10:02:37.009093  8764 solver.cpp:312] Iteration 4700 (6.85533 iter/s, 14.5872s/100 iter), loss = 1.67851
I0815 10:02:37.009117  8764 solver.cpp:334]     Train net output #0: loss = 1.56117 (* 1 = 1.56117 loss)
I0815 10:02:37.009124  8764 sgd_solver.cpp:136] Iteration 4700, lr = 0.00970625, m = 0.9
I0815 10:02:51.685168  8764 solver.cpp:312] Iteration 4800 (6.814 iter/s, 14.6757s/100 iter), loss = 1.47144
I0815 10:02:51.685197  8764 solver.cpp:334]     Train net output #0: loss = 2.09591 (* 1 = 2.09591 loss)
I0815 10:02:51.685204  8764 sgd_solver.cpp:136] Iteration 4800, lr = 0.0097, m = 0.9
I0815 10:03:06.424181  8764 solver.cpp:312] Iteration 4900 (6.7849 iter/s, 14.7386s/100 iter), loss = 1.56856
I0815 10:03:06.424263  8764 solver.cpp:334]     Train net output #0: loss = 1.57836 (* 1 = 1.57836 loss)
I0815 10:03:06.424273  8764 sgd_solver.cpp:136] Iteration 4900, lr = 0.00969375, m = 0.9
I0815 10:03:20.692587  8764 solver.cpp:363] Sparsity after update:
I0815 10:03:20.704169  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:03:20.704185  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:03:20.704192  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:03:20.704195  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:03:20.704200  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:03:20.704215  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:03:20.704226  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:03:20.704236  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:03:20.704244  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:03:20.704253  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:03:20.704262  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:03:20.704272  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:03:20.704279  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:03:20.842365  8764 solver.cpp:312] Iteration 5000 (6.93588 iter/s, 14.4178s/100 iter), loss = 1.38993
I0815 10:03:20.842388  8764 solver.cpp:334]     Train net output #0: loss = 1.76336 (* 1 = 1.76336 loss)
I0815 10:03:20.842393  8764 sgd_solver.cpp:136] Iteration 5000, lr = 0.0096875, m = 0.9
I0815 10:03:35.227777  8764 solver.cpp:312] Iteration 5100 (6.95168 iter/s, 14.385s/100 iter), loss = 1.41983
I0815 10:03:35.227847  8764 solver.cpp:334]     Train net output #0: loss = 1.44808 (* 1 = 1.44808 loss)
I0815 10:03:35.227866  8764 sgd_solver.cpp:136] Iteration 5100, lr = 0.00968125, m = 0.9
I0815 10:03:49.668843  8764 solver.cpp:312] Iteration 5200 (6.92489 iter/s, 14.4407s/100 iter), loss = 1.40807
I0815 10:03:49.674190  8764 solver.cpp:334]     Train net output #0: loss = 1.40904 (* 1 = 1.40904 loss)
I0815 10:03:49.674201  8764 sgd_solver.cpp:136] Iteration 5200, lr = 0.009675, m = 0.9
I0815 10:04:04.184106  8764 solver.cpp:312] Iteration 5300 (6.88949 iter/s, 14.5149s/100 iter), loss = 1.64604
I0815 10:04:04.184134  8764 solver.cpp:334]     Train net output #0: loss = 1.57986 (* 1 = 1.57986 loss)
I0815 10:04:04.184140  8764 sgd_solver.cpp:136] Iteration 5300, lr = 0.00966875, m = 0.9
I0815 10:04:18.856452  8764 solver.cpp:312] Iteration 5400 (6.81573 iter/s, 14.6719s/100 iter), loss = 1.47968
I0815 10:04:18.856609  8764 solver.cpp:334]     Train net output #0: loss = 1.59342 (* 1 = 1.59342 loss)
I0815 10:04:18.856631  8764 sgd_solver.cpp:136] Iteration 5400, lr = 0.0096625, m = 0.9
I0815 10:04:33.564191  8764 solver.cpp:312] Iteration 5500 (6.79933 iter/s, 14.7073s/100 iter), loss = 1.34736
I0815 10:04:33.568195  8764 solver.cpp:334]     Train net output #0: loss = 1.30237 (* 1 = 1.30237 loss)
I0815 10:04:33.568217  8764 sgd_solver.cpp:136] Iteration 5500, lr = 0.00965625, m = 0.9
I0815 10:04:48.212049  8764 solver.cpp:312] Iteration 5600 (6.82713 iter/s, 14.6475s/100 iter), loss = 1.51493
I0815 10:04:48.212077  8764 solver.cpp:334]     Train net output #0: loss = 1.55066 (* 1 = 1.55066 loss)
I0815 10:04:48.212083  8764 sgd_solver.cpp:136] Iteration 5600, lr = 0.00965, m = 0.9
I0815 10:05:02.726135  8764 solver.cpp:312] Iteration 5700 (6.89005 iter/s, 14.5137s/100 iter), loss = 1.66847
I0815 10:05:02.726162  8764 solver.cpp:334]     Train net output #0: loss = 1.77811 (* 1 = 1.77811 loss)
I0815 10:05:02.726168  8764 sgd_solver.cpp:136] Iteration 5700, lr = 0.00964375, m = 0.9
I0815 10:05:17.023833  8764 solver.cpp:312] Iteration 5800 (6.99433 iter/s, 14.2973s/100 iter), loss = 1.42647
I0815 10:05:17.023937  8764 solver.cpp:334]     Train net output #0: loss = 1.56034 (* 1 = 1.56034 loss)
I0815 10:05:17.023954  8764 sgd_solver.cpp:136] Iteration 5800, lr = 0.0096375, m = 0.9
I0815 10:05:31.520084  8764 solver.cpp:312] Iteration 5900 (6.89852 iter/s, 14.4959s/100 iter), loss = 1.28274
I0815 10:05:31.520110  8764 solver.cpp:334]     Train net output #0: loss = 1.21825 (* 1 = 1.21825 loss)
I0815 10:05:31.520117  8764 sgd_solver.cpp:136] Iteration 5900, lr = 0.00963125, m = 0.9
I0815 10:05:45.834892  8764 solver.cpp:363] Sparsity after update:
I0815 10:05:45.839905  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:05:45.839943  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:05:45.839962  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:05:45.839974  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:05:45.839987  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:05:45.839999  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:05:45.840013  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:05:45.840026  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:05:45.840039  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:05:45.840051  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:05:45.840064  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:05:45.840075  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:05:45.840086  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:05:45.840106  8764 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 10:06:04.378435  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:06:06.680872  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.555765
I0815 10:06:06.680894  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790644
I0815 10:06:06.680901  8764 solver.cpp:594]     Test net output #2: loss = 1.99858 (* 1 = 1.99858 loss)
I0815 10:06:06.680917  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8402s
I0815 10:06:06.828153  8764 solver.cpp:312] Iteration 6000 (2.83229 iter/s, 35.3071s/100 iter), loss = 1.44797
I0815 10:06:06.828181  8764 solver.cpp:334]     Train net output #0: loss = 1.55907 (* 1 = 1.55907 loss)
I0815 10:06:06.828187  8764 sgd_solver.cpp:136] Iteration 6000, lr = 0.009625, m = 0.9
I0815 10:06:21.621286  8764 solver.cpp:312] Iteration 6100 (6.76008 iter/s, 14.7927s/100 iter), loss = 1.54992
I0815 10:06:21.621314  8764 solver.cpp:334]     Train net output #0: loss = 1.49461 (* 1 = 1.49461 loss)
I0815 10:06:21.621318  8764 sgd_solver.cpp:136] Iteration 6100, lr = 0.00961875, m = 0.9
I0815 10:06:36.717360  8764 solver.cpp:312] Iteration 6200 (6.62442 iter/s, 15.0957s/100 iter), loss = 1.8655
I0815 10:06:36.717408  8764 solver.cpp:334]     Train net output #0: loss = 1.47909 (* 1 = 1.47909 loss)
I0815 10:06:36.717416  8764 sgd_solver.cpp:136] Iteration 6200, lr = 0.0096125, m = 0.9
I0815 10:06:51.205816  8764 solver.cpp:312] Iteration 6300 (6.90224 iter/s, 14.4881s/100 iter), loss = 1.33149
I0815 10:06:51.205843  8764 solver.cpp:334]     Train net output #0: loss = 1.20779 (* 1 = 1.20779 loss)
I0815 10:06:51.205849  8764 sgd_solver.cpp:136] Iteration 6300, lr = 0.00960625, m = 0.9
I0815 10:07:05.798765  8764 solver.cpp:312] Iteration 6400 (6.85281 iter/s, 14.5925s/100 iter), loss = 1.28445
I0815 10:07:05.798792  8764 solver.cpp:334]     Train net output #0: loss = 1.50755 (* 1 = 1.50755 loss)
I0815 10:07:05.798797  8764 sgd_solver.cpp:136] Iteration 6400, lr = 0.0096, m = 0.9
I0815 10:07:20.356020  8764 solver.cpp:312] Iteration 6500 (6.86962 iter/s, 14.5569s/100 iter), loss = 1.51377
I0815 10:07:20.356122  8764 solver.cpp:334]     Train net output #0: loss = 1.51061 (* 1 = 1.51061 loss)
I0815 10:07:20.356143  8764 sgd_solver.cpp:136] Iteration 6500, lr = 0.00959375, m = 0.9
I0815 10:07:34.969367  8764 solver.cpp:312] Iteration 6600 (6.84325 iter/s, 14.6129s/100 iter), loss = 1.25744
I0815 10:07:34.969394  8764 solver.cpp:334]     Train net output #0: loss = 0.886921 (* 1 = 0.886921 loss)
I0815 10:07:34.969399  8764 sgd_solver.cpp:136] Iteration 6600, lr = 0.0095875, m = 0.9
I0815 10:07:49.511700  8764 solver.cpp:312] Iteration 6700 (6.87667 iter/s, 14.5419s/100 iter), loss = 1.59163
I0815 10:07:49.511729  8764 solver.cpp:334]     Train net output #0: loss = 1.86037 (* 1 = 1.86037 loss)
I0815 10:07:49.511735  8764 sgd_solver.cpp:136] Iteration 6700, lr = 0.00958125, m = 0.9
I0815 10:08:03.758765  8764 solver.cpp:312] Iteration 6800 (7.01918 iter/s, 14.2467s/100 iter), loss = 1.26957
I0815 10:08:03.759009  8764 solver.cpp:334]     Train net output #0: loss = 1.62755 (* 1 = 1.62755 loss)
I0815 10:08:03.759120  8764 sgd_solver.cpp:136] Iteration 6800, lr = 0.009575, m = 0.9
I0815 10:08:18.052237  8764 solver.cpp:312] Iteration 6900 (6.99639 iter/s, 14.2931s/100 iter), loss = 1.4879
I0815 10:08:18.052443  8764 solver.cpp:334]     Train net output #0: loss = 1.23992 (* 1 = 1.23992 loss)
I0815 10:08:18.052551  8764 sgd_solver.cpp:136] Iteration 6900, lr = 0.00956875, m = 0.9
I0815 10:08:32.328367  8764 solver.cpp:363] Sparsity after update:
I0815 10:08:32.340636  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:08:32.340678  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:08:32.340692  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:08:32.340700  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:08:32.340708  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:08:32.340716  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:08:32.340724  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:08:32.340731  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:08:32.340739  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:08:32.340747  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:08:32.340755  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:08:32.340762  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:08:32.340770  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:08:32.481984  8764 solver.cpp:312] Iteration 7000 (6.93032 iter/s, 14.4293s/100 iter), loss = 1.48696
I0815 10:08:32.482012  8764 solver.cpp:334]     Train net output #0: loss = 1.2886 (* 1 = 1.2886 loss)
I0815 10:08:32.482018  8764 sgd_solver.cpp:136] Iteration 7000, lr = 0.0095625, m = 0.9
I0815 10:08:46.978690  8764 solver.cpp:312] Iteration 7100 (6.89831 iter/s, 14.4963s/100 iter), loss = 1.78903
I0815 10:08:46.978775  8764 solver.cpp:334]     Train net output #0: loss = 1.85591 (* 1 = 1.85591 loss)
I0815 10:08:46.978788  8764 sgd_solver.cpp:136] Iteration 7100, lr = 0.00955625, m = 0.9
I0815 10:09:01.387362  8764 solver.cpp:312] Iteration 7200 (6.94046 iter/s, 14.4083s/100 iter), loss = 1.9599
I0815 10:09:01.387392  8764 solver.cpp:334]     Train net output #0: loss = 1.90156 (* 1 = 1.90156 loss)
I0815 10:09:01.387399  8764 sgd_solver.cpp:136] Iteration 7200, lr = 0.00955, m = 0.9
I0815 10:09:15.900842  8764 solver.cpp:312] Iteration 7300 (6.89033 iter/s, 14.5131s/100 iter), loss = 1.26201
I0815 10:09:15.900877  8764 solver.cpp:334]     Train net output #0: loss = 1.13923 (* 1 = 1.13923 loss)
I0815 10:09:15.900883  8764 sgd_solver.cpp:136] Iteration 7300, lr = 0.00954375, m = 0.9
I0815 10:09:30.452217  8764 solver.cpp:312] Iteration 7400 (6.87239 iter/s, 14.551s/100 iter), loss = 1.56688
I0815 10:09:30.452335  8764 solver.cpp:334]     Train net output #0: loss = 1.65461 (* 1 = 1.65461 loss)
I0815 10:09:30.452353  8764 sgd_solver.cpp:136] Iteration 7400, lr = 0.0095375, m = 0.9
I0815 10:09:45.231451  8764 solver.cpp:312] Iteration 7500 (6.76644 iter/s, 14.7788s/100 iter), loss = 1.65908
I0815 10:09:45.231475  8764 solver.cpp:334]     Train net output #0: loss = 1.94755 (* 1 = 1.94755 loss)
I0815 10:09:45.231479  8764 sgd_solver.cpp:136] Iteration 7500, lr = 0.00953125, m = 0.9
I0815 10:09:59.848242  8764 solver.cpp:312] Iteration 7600 (6.84163 iter/s, 14.6164s/100 iter), loss = 1.23027
I0815 10:09:59.848317  8764 solver.cpp:334]     Train net output #0: loss = 1.13527 (* 1 = 1.13527 loss)
I0815 10:09:59.848336  8764 sgd_solver.cpp:136] Iteration 7600, lr = 0.009525, m = 0.9
I0815 10:10:14.572152  8764 solver.cpp:312] Iteration 7700 (6.79186 iter/s, 14.7235s/100 iter), loss = 1.23974
I0815 10:10:14.572391  8764 solver.cpp:334]     Train net output #0: loss = 1.25319 (* 1 = 1.25319 loss)
I0815 10:10:14.572412  8764 sgd_solver.cpp:136] Iteration 7700, lr = 0.00951875, m = 0.9
I0815 10:10:28.965420  8764 solver.cpp:312] Iteration 7800 (6.94788 iter/s, 14.3929s/100 iter), loss = 1.4441
I0815 10:10:28.965446  8764 solver.cpp:334]     Train net output #0: loss = 1.59873 (* 1 = 1.59873 loss)
I0815 10:10:28.965452  8764 sgd_solver.cpp:136] Iteration 7800, lr = 0.0095125, m = 0.9
I0815 10:10:43.292306  8764 solver.cpp:312] Iteration 7900 (6.98008 iter/s, 14.3265s/100 iter), loss = 1.12198
I0815 10:10:43.292335  8764 solver.cpp:334]     Train net output #0: loss = 1.30991 (* 1 = 1.30991 loss)
I0815 10:10:43.292340  8764 sgd_solver.cpp:136] Iteration 7900, lr = 0.00950625, m = 0.9
I0815 10:10:57.863756  8764 solver.cpp:363] Sparsity after update:
I0815 10:10:57.867698  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:10:57.867735  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:10:57.867753  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:10:57.867765  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:10:57.867777  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:10:57.867789  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:10:57.867801  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:10:57.867812  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:10:57.867825  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:10:57.867835  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:10:57.867852  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:10:57.867864  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:10:57.867877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:10:57.867897  8764 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 10:11:00.857771  8748 blocking_queue.cpp:40] Waiting for datum
I0815 10:11:18.963187  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.539059
I0815 10:11:18.963217  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.782704
I0815 10:11:18.963224  8764 solver.cpp:594]     Test net output #2: loss = 2.05523 (* 1 = 2.05523 loss)
I0815 10:11:18.963301  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0948s
I0815 10:11:19.112325  8764 solver.cpp:312] Iteration 8000 (2.79181 iter/s, 35.819s/100 iter), loss = 1.34738
I0815 10:11:19.112349  8764 solver.cpp:334]     Train net output #0: loss = 1.4008 (* 1 = 1.4008 loss)
I0815 10:11:19.112354  8764 sgd_solver.cpp:136] Iteration 8000, lr = 0.0095, m = 0.9
I0815 10:11:33.472110  8764 solver.cpp:312] Iteration 8100 (6.96409 iter/s, 14.3594s/100 iter), loss = 1.68998
I0815 10:11:33.472194  8764 solver.cpp:334]     Train net output #0: loss = 1.89889 (* 1 = 1.89889 loss)
I0815 10:11:33.472208  8764 sgd_solver.cpp:136] Iteration 8100, lr = 0.00949375, m = 0.9
I0815 10:11:47.854619  8764 solver.cpp:312] Iteration 8200 (6.95308 iter/s, 14.3821s/100 iter), loss = 1.45251
I0815 10:11:47.854645  8764 solver.cpp:334]     Train net output #0: loss = 1.55172 (* 1 = 1.55172 loss)
I0815 10:11:47.854648  8764 sgd_solver.cpp:136] Iteration 8200, lr = 0.0094875, m = 0.9
I0815 10:12:02.431308  8764 solver.cpp:312] Iteration 8300 (6.86046 iter/s, 14.5763s/100 iter), loss = 1.16528
I0815 10:12:02.431372  8764 solver.cpp:334]     Train net output #0: loss = 1.31213 (* 1 = 1.31213 loss)
I0815 10:12:02.431390  8764 sgd_solver.cpp:136] Iteration 8300, lr = 0.00948125, m = 0.9
I0815 10:12:17.024359  8764 solver.cpp:312] Iteration 8400 (6.85277 iter/s, 14.5926s/100 iter), loss = 1.32743
I0815 10:12:17.024468  8764 solver.cpp:334]     Train net output #0: loss = 1.56869 (* 1 = 1.56869 loss)
I0815 10:12:17.024487  8764 sgd_solver.cpp:136] Iteration 8400, lr = 0.009475, m = 0.9
I0815 10:12:31.603361  8764 solver.cpp:312] Iteration 8500 (6.85937 iter/s, 14.5786s/100 iter), loss = 1.61987
I0815 10:12:31.603428  8764 solver.cpp:334]     Train net output #0: loss = 1.6365 (* 1 = 1.6365 loss)
I0815 10:12:31.603437  8764 sgd_solver.cpp:136] Iteration 8500, lr = 0.00946875, m = 0.9
I0815 10:12:46.108028  8764 solver.cpp:312] Iteration 8600 (6.89452 iter/s, 14.5043s/100 iter), loss = 1.66105
I0815 10:12:46.108057  8764 solver.cpp:334]     Train net output #0: loss = 1.62928 (* 1 = 1.62928 loss)
I0815 10:12:46.108062  8764 sgd_solver.cpp:136] Iteration 8600, lr = 0.0094625, m = 0.9
I0815 10:13:00.783509  8764 solver.cpp:312] Iteration 8700 (6.81428 iter/s, 14.6751s/100 iter), loss = 1.109
I0815 10:13:00.783748  8764 solver.cpp:334]     Train net output #0: loss = 1.08699 (* 1 = 1.08699 loss)
I0815 10:13:00.783856  8764 sgd_solver.cpp:136] Iteration 8700, lr = 0.00945625, m = 0.9
I0815 10:13:15.143515  8764 solver.cpp:312] Iteration 8800 (6.96398 iter/s, 14.3596s/100 iter), loss = 1.33345
I0815 10:13:15.143576  8764 solver.cpp:334]     Train net output #0: loss = 1.6936 (* 1 = 1.6936 loss)
I0815 10:13:15.143594  8764 sgd_solver.cpp:136] Iteration 8800, lr = 0.00945, m = 0.9
I0815 10:13:29.932106  8764 solver.cpp:312] Iteration 8900 (6.76216 iter/s, 14.7882s/100 iter), loss = 1.36181
I0815 10:13:29.932142  8764 solver.cpp:334]     Train net output #0: loss = 1.24685 (* 1 = 1.24685 loss)
I0815 10:13:29.932148  8764 sgd_solver.cpp:136] Iteration 8900, lr = 0.00944375, m = 0.9
I0815 10:13:44.070381  8764 solver.cpp:363] Sparsity after update:
I0815 10:13:44.080818  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:13:44.080857  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:13:44.080875  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:13:44.080888  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:13:44.080904  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:13:44.080916  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:13:44.080929  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:13:44.080940  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:13:44.080951  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:13:44.080962  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:13:44.080973  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:13:44.080984  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:13:44.080996  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:13:44.230854  8764 solver.cpp:312] Iteration 9000 (6.99381 iter/s, 14.2984s/100 iter), loss = 1.46253
I0815 10:13:44.230880  8764 solver.cpp:334]     Train net output #0: loss = 1.52149 (* 1 = 1.52149 loss)
I0815 10:13:44.230885  8764 sgd_solver.cpp:136] Iteration 9000, lr = 0.0094375, m = 0.9
I0815 10:13:58.656586  8764 solver.cpp:312] Iteration 9100 (6.93225 iter/s, 14.4253s/100 iter), loss = 1.40323
I0815 10:13:58.656637  8764 solver.cpp:334]     Train net output #0: loss = 1.06087 (* 1 = 1.06087 loss)
I0815 10:13:58.656651  8764 sgd_solver.cpp:136] Iteration 9100, lr = 0.00943125, m = 0.9
I0815 10:14:13.218739  8764 solver.cpp:312] Iteration 9200 (6.86731 iter/s, 14.5617s/100 iter), loss = 1.55577
I0815 10:14:13.218770  8764 solver.cpp:334]     Train net output #0: loss = 1.69355 (* 1 = 1.69355 loss)
I0815 10:14:13.218775  8764 sgd_solver.cpp:136] Iteration 9200, lr = 0.009425, m = 0.9
I0815 10:14:27.995007  8764 solver.cpp:312] Iteration 9300 (6.7678 iter/s, 14.7759s/100 iter), loss = 1.74798
I0815 10:14:27.995268  8764 solver.cpp:334]     Train net output #0: loss = 2.06613 (* 1 = 2.06613 loss)
I0815 10:14:27.995383  8764 sgd_solver.cpp:136] Iteration 9300, lr = 0.00941875, m = 0.9
I0815 10:14:42.596657  8764 solver.cpp:312] Iteration 9400 (6.84873 iter/s, 14.6012s/100 iter), loss = 1.24374
I0815 10:14:42.596689  8764 solver.cpp:334]     Train net output #0: loss = 0.800043 (* 1 = 0.800043 loss)
I0815 10:14:42.596694  8764 sgd_solver.cpp:136] Iteration 9400, lr = 0.0094125, m = 0.9
I0815 10:14:57.065856  8764 solver.cpp:312] Iteration 9500 (6.91142 iter/s, 14.4688s/100 iter), loss = 1.72969
I0815 10:14:57.065883  8764 solver.cpp:334]     Train net output #0: loss = 1.68036 (* 1 = 1.68036 loss)
I0815 10:14:57.065891  8764 sgd_solver.cpp:136] Iteration 9500, lr = 0.00940625, m = 0.9
I0815 10:15:11.534775  8764 solver.cpp:312] Iteration 9600 (6.91156 iter/s, 14.4685s/100 iter), loss = 1.70527
I0815 10:15:11.535009  8764 solver.cpp:334]     Train net output #0: loss = 1.89232 (* 1 = 1.89232 loss)
I0815 10:15:11.535092  8764 sgd_solver.cpp:136] Iteration 9600, lr = 0.0094, m = 0.9
I0815 10:15:25.859203  8764 solver.cpp:312] Iteration 9700 (6.98127 iter/s, 14.324s/100 iter), loss = 1.5291
I0815 10:15:25.859232  8764 solver.cpp:334]     Train net output #0: loss = 1.56542 (* 1 = 1.56542 loss)
I0815 10:15:25.859239  8764 sgd_solver.cpp:136] Iteration 9700, lr = 0.00939375, m = 0.9
I0815 10:15:40.385224  8764 solver.cpp:312] Iteration 9800 (6.88438 iter/s, 14.5256s/100 iter), loss = 1.23557
I0815 10:15:40.385251  8764 solver.cpp:334]     Train net output #0: loss = 1.42344 (* 1 = 1.42344 loss)
I0815 10:15:40.385257  8764 sgd_solver.cpp:136] Iteration 9800, lr = 0.0093875, m = 0.9
I0815 10:15:54.808027  8764 solver.cpp:312] Iteration 9900 (6.93366 iter/s, 14.4224s/100 iter), loss = 1.09155
I0815 10:15:54.808845  8764 solver.cpp:334]     Train net output #0: loss = 1.02268 (* 1 = 1.02268 loss)
I0815 10:15:54.808866  8764 sgd_solver.cpp:136] Iteration 9900, lr = 0.00938125, m = 0.9
I0815 10:16:09.261303  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_10000.caffemodel
I0815 10:16:09.311563  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_10000.solverstate
I0815 10:16:09.321283  8764 solver.cpp:363] Sparsity after update:
I0815 10:16:09.335827  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:16:09.335850  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:16:09.335855  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:16:09.335860  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:16:09.335862  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:16:09.335865  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:16:09.335868  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:16:09.335871  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:16:09.335875  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:16:09.335880  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:16:09.335883  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:16:09.335887  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:16:09.335891  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:16:09.335903  8764 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 10:16:29.889214  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.537
I0815 10:16:29.889273  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.776292
I0815 10:16:29.889282  8764 solver.cpp:594]     Test net output #2: loss = 2.05574 (* 1 = 2.05574 loss)
I0815 10:16:29.889303  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.5528s
I0815 10:16:30.062683  8764 solver.cpp:312] Iteration 10000 (2.83658 iter/s, 35.2537s/100 iter), loss = 1.57708
I0815 10:16:30.062749  8764 solver.cpp:334]     Train net output #0: loss = 1.36222 (* 1 = 1.36222 loss)
I0815 10:16:30.062769  8764 sgd_solver.cpp:136] Iteration 10000, lr = 0.009375, m = 0.9
I0815 10:16:44.423110  8764 solver.cpp:312] Iteration 10100 (6.96377 iter/s, 14.36s/100 iter), loss = 1.08106
I0815 10:16:44.423136  8764 solver.cpp:334]     Train net output #0: loss = 1.28649 (* 1 = 1.28649 loss)
I0815 10:16:44.423142  8764 sgd_solver.cpp:136] Iteration 10100, lr = 0.00936875, m = 0.9
I0815 10:16:58.842099  8764 solver.cpp:312] Iteration 10200 (6.93549 iter/s, 14.4186s/100 iter), loss = 1.46343
I0815 10:16:58.842173  8764 solver.cpp:334]     Train net output #0: loss = 1.43318 (* 1 = 1.43318 loss)
I0815 10:16:58.842192  8764 sgd_solver.cpp:136] Iteration 10200, lr = 0.0093625, m = 0.9
I0815 10:17:13.284518  8764 solver.cpp:312] Iteration 10300 (6.92424 iter/s, 14.442s/100 iter), loss = 1.35174
I0815 10:17:13.284637  8764 solver.cpp:334]     Train net output #0: loss = 1.64232 (* 1 = 1.64232 loss)
I0815 10:17:13.284657  8764 sgd_solver.cpp:136] Iteration 10300, lr = 0.00935625, m = 0.9
I0815 10:17:27.795475  8764 solver.cpp:312] Iteration 10400 (6.89153 iter/s, 14.5106s/100 iter), loss = 1.17509
I0815 10:17:27.795497  8764 solver.cpp:334]     Train net output #0: loss = 0.878668 (* 1 = 0.878668 loss)
I0815 10:17:27.795501  8764 sgd_solver.cpp:136] Iteration 10400, lr = 0.00935, m = 0.9
I0815 10:17:42.286389  8764 solver.cpp:312] Iteration 10500 (6.90107 iter/s, 14.4905s/100 iter), loss = 1.08584
I0815 10:17:42.286440  8764 solver.cpp:334]     Train net output #0: loss = 0.902722 (* 1 = 0.902722 loss)
I0815 10:17:42.286453  8764 sgd_solver.cpp:136] Iteration 10500, lr = 0.00934375, m = 0.9
I0815 10:17:56.922474  8764 solver.cpp:312] Iteration 10600 (6.83262 iter/s, 14.6357s/100 iter), loss = 1.47749
I0815 10:17:56.922554  8764 solver.cpp:334]     Train net output #0: loss = 1.4628 (* 1 = 1.4628 loss)
I0815 10:17:56.922562  8764 sgd_solver.cpp:136] Iteration 10600, lr = 0.0093375, m = 0.9
I0815 10:18:11.526206  8764 solver.cpp:312] Iteration 10700 (6.84775 iter/s, 14.6033s/100 iter), loss = 1.72734
I0815 10:18:11.526232  8764 solver.cpp:334]     Train net output #0: loss = 1.36186 (* 1 = 1.36186 loss)
I0815 10:18:11.526239  8764 sgd_solver.cpp:136] Iteration 10700, lr = 0.00933125, m = 0.9
I0815 10:18:25.887877  8764 solver.cpp:312] Iteration 10800 (6.96317 iter/s, 14.3613s/100 iter), loss = 1.27609
I0815 10:18:25.887903  8764 solver.cpp:334]     Train net output #0: loss = 1.13368 (* 1 = 1.13368 loss)
I0815 10:18:25.887907  8764 sgd_solver.cpp:136] Iteration 10800, lr = 0.009325, m = 0.9
I0815 10:18:40.281808  8764 solver.cpp:312] Iteration 10900 (6.94756 iter/s, 14.3935s/100 iter), loss = 1.36548
I0815 10:18:40.281898  8764 solver.cpp:334]     Train net output #0: loss = 1.3521 (* 1 = 1.3521 loss)
I0815 10:18:40.281915  8764 sgd_solver.cpp:136] Iteration 10900, lr = 0.00931875, m = 0.9
I0815 10:18:54.623863  8764 solver.cpp:363] Sparsity after update:
I0815 10:18:54.636503  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:18:54.636543  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:18:54.636562  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:18:54.636574  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:18:54.636585  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:18:54.636597  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:18:54.636610  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:18:54.636620  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:18:54.636632  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:18:54.636643  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:18:54.636654  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:18:54.636667  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:18:54.636677  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:18:54.771378  8764 solver.cpp:312] Iteration 11000 (6.90171 iter/s, 14.4892s/100 iter), loss = 1.12952
I0815 10:18:54.771445  8764 solver.cpp:334]     Train net output #0: loss = 1.15626 (* 1 = 1.15626 loss)
I0815 10:18:54.771462  8764 sgd_solver.cpp:136] Iteration 11000, lr = 0.0093125, m = 0.9
I0815 10:19:09.100778  8764 solver.cpp:312] Iteration 11100 (6.97885 iter/s, 14.329s/100 iter), loss = 1.46508
I0815 10:19:09.100800  8764 solver.cpp:334]     Train net output #0: loss = 1.29209 (* 1 = 1.29209 loss)
I0815 10:19:09.100805  8764 sgd_solver.cpp:136] Iteration 11100, lr = 0.00930625, m = 0.9
I0815 10:19:23.618122  8764 solver.cpp:312] Iteration 11200 (6.8885 iter/s, 14.5169s/100 iter), loss = 1.26725
I0815 10:19:23.618191  8764 solver.cpp:334]     Train net output #0: loss = 1.16406 (* 1 = 1.16406 loss)
I0815 10:19:23.618197  8764 sgd_solver.cpp:136] Iteration 11200, lr = 0.0093, m = 0.9
I0815 10:19:38.304913  8764 solver.cpp:312] Iteration 11300 (6.80903 iter/s, 14.6864s/100 iter), loss = 1.43913
I0815 10:19:38.304977  8764 solver.cpp:334]     Train net output #0: loss = 1.46051 (* 1 = 1.46051 loss)
I0815 10:19:38.304996  8764 sgd_solver.cpp:136] Iteration 11300, lr = 0.00929375, m = 0.9
I0815 10:19:52.677225  8764 solver.cpp:312] Iteration 11400 (6.95801 iter/s, 14.3719s/100 iter), loss = 1.7589
I0815 10:19:52.677253  8764 solver.cpp:334]     Train net output #0: loss = 1.6674 (* 1 = 1.6674 loss)
I0815 10:19:52.677259  8764 sgd_solver.cpp:136] Iteration 11400, lr = 0.0092875, m = 0.9
I0815 10:20:07.105329  8764 solver.cpp:312] Iteration 11500 (6.93111 iter/s, 14.4277s/100 iter), loss = 2.04712
I0815 10:20:07.105429  8764 solver.cpp:334]     Train net output #0: loss = 1.78589 (* 1 = 1.78589 loss)
I0815 10:20:07.105448  8764 sgd_solver.cpp:136] Iteration 11500, lr = 0.00928125, m = 0.9
I0815 10:20:21.500174  8764 solver.cpp:312] Iteration 11600 (6.94712 iter/s, 14.3944s/100 iter), loss = 1.10706
I0815 10:20:21.500201  8764 solver.cpp:334]     Train net output #0: loss = 1.12198 (* 1 = 1.12198 loss)
I0815 10:20:21.500206  8764 sgd_solver.cpp:136] Iteration 11600, lr = 0.009275, m = 0.9
I0815 10:20:35.868270  8764 solver.cpp:312] Iteration 11700 (6.96006 iter/s, 14.3677s/100 iter), loss = 1.30817
I0815 10:20:35.868297  8764 solver.cpp:334]     Train net output #0: loss = 1.3164 (* 1 = 1.3164 loss)
I0815 10:20:35.868304  8764 sgd_solver.cpp:136] Iteration 11700, lr = 0.00926875, m = 0.9
I0815 10:20:50.505903  8764 solver.cpp:312] Iteration 11800 (6.83189 iter/s, 14.6372s/100 iter), loss = 1.44771
I0815 10:20:50.506688  8764 solver.cpp:334]     Train net output #0: loss = 1.05362 (* 1 = 1.05362 loss)
I0815 10:20:50.506697  8764 sgd_solver.cpp:136] Iteration 11800, lr = 0.0092625, m = 0.9
I0815 10:21:04.811462  8764 solver.cpp:312] Iteration 11900 (6.99048 iter/s, 14.3052s/100 iter), loss = 1.71835
I0815 10:21:04.811486  8764 solver.cpp:334]     Train net output #0: loss = 2.11933 (* 1 = 2.11933 loss)
I0815 10:21:04.811491  8764 sgd_solver.cpp:136] Iteration 11900, lr = 0.00925625, m = 0.9
I0815 10:21:19.192898  8764 solver.cpp:363] Sparsity after update:
I0815 10:21:19.197028  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:21:19.197149  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:21:19.197227  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:21:19.197309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:21:19.197382  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:21:19.197459  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:21:19.197525  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:21:19.197592  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:21:19.197674  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:21:19.197751  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:21:19.197824  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:21:19.197891  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:21:19.197969  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:21:19.198057  8764 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 10:21:35.636730  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:21:40.306510  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.541177
I0815 10:21:40.306530  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.777645
I0815 10:21:40.306535  8764 solver.cpp:594]     Test net output #2: loss = 2.05769 (* 1 = 2.05769 loss)
I0815 10:21:40.306557  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1079s
I0815 10:21:40.494673  8764 solver.cpp:312] Iteration 12000 (2.80251 iter/s, 35.6822s/100 iter), loss = 1.54142
I0815 10:21:40.494698  8764 solver.cpp:334]     Train net output #0: loss = 1.42048 (* 1 = 1.42048 loss)
I0815 10:21:40.494701  8764 sgd_solver.cpp:136] Iteration 12000, lr = 0.00925, m = 0.9
I0815 10:21:55.053822  8764 solver.cpp:312] Iteration 12100 (6.86872 iter/s, 14.5587s/100 iter), loss = 1.41261
I0815 10:21:55.053892  8764 solver.cpp:334]     Train net output #0: loss = 1.28694 (* 1 = 1.28694 loss)
I0815 10:21:55.053911  8764 sgd_solver.cpp:136] Iteration 12100, lr = 0.00924375, m = 0.9
I0815 10:22:09.424939  8764 solver.cpp:312] Iteration 12200 (6.95859 iter/s, 14.3707s/100 iter), loss = 1.26473
I0815 10:22:09.428192  8764 solver.cpp:334]     Train net output #0: loss = 0.998983 (* 1 = 0.998983 loss)
I0815 10:22:09.428211  8764 sgd_solver.cpp:136] Iteration 12200, lr = 0.0092375, m = 0.9
I0815 10:22:23.701004  8764 solver.cpp:312] Iteration 12300 (7.00493 iter/s, 14.2757s/100 iter), loss = 1.0436
I0815 10:22:23.701026  8764 solver.cpp:334]     Train net output #0: loss = 0.823622 (* 1 = 0.823622 loss)
I0815 10:22:23.701030  8764 sgd_solver.cpp:136] Iteration 12300, lr = 0.00923125, m = 0.9
I0815 10:22:38.187963  8764 solver.cpp:312] Iteration 12400 (6.90295 iter/s, 14.4866s/100 iter), loss = 1.31849
I0815 10:22:38.188026  8764 solver.cpp:334]     Train net output #0: loss = 1.09411 (* 1 = 1.09411 loss)
I0815 10:22:38.188045  8764 sgd_solver.cpp:136] Iteration 12400, lr = 0.009225, m = 0.9
I0815 10:22:52.550243  8764 solver.cpp:312] Iteration 12500 (6.96287 iter/s, 14.3619s/100 iter), loss = 1.49624
I0815 10:22:52.550307  8764 solver.cpp:334]     Train net output #0: loss = 1.57458 (* 1 = 1.57458 loss)
I0815 10:22:52.550314  8764 sgd_solver.cpp:136] Iteration 12500, lr = 0.00921875, m = 0.9
I0815 10:23:07.005527  8764 solver.cpp:312] Iteration 12600 (6.91808 iter/s, 14.4549s/100 iter), loss = 1.82941
I0815 10:23:07.005554  8764 solver.cpp:334]     Train net output #0: loss = 1.6125 (* 1 = 1.6125 loss)
I0815 10:23:07.005560  8764 sgd_solver.cpp:136] Iteration 12600, lr = 0.0092125, m = 0.9
I0815 10:23:21.522992  8764 solver.cpp:312] Iteration 12700 (6.88844 iter/s, 14.5171s/100 iter), loss = 1.35146
I0815 10:23:21.523025  8764 solver.cpp:334]     Train net output #0: loss = 1.38287 (* 1 = 1.38287 loss)
I0815 10:23:21.523030  8764 sgd_solver.cpp:136] Iteration 12700, lr = 0.00920625, m = 0.9
I0815 10:23:35.972972  8764 solver.cpp:312] Iteration 12800 (6.92062 iter/s, 14.4496s/100 iter), loss = 1.93914
I0815 10:23:35.973076  8764 solver.cpp:334]     Train net output #0: loss = 2.23588 (* 1 = 2.23588 loss)
I0815 10:23:35.973094  8764 sgd_solver.cpp:136] Iteration 12800, lr = 0.0092, m = 0.9
I0815 10:23:50.555377  8764 solver.cpp:312] Iteration 12900 (6.85777 iter/s, 14.582s/100 iter), loss = 1.25388
I0815 10:23:50.555404  8764 solver.cpp:334]     Train net output #0: loss = 1.27162 (* 1 = 1.27162 loss)
I0815 10:23:50.555410  8764 sgd_solver.cpp:136] Iteration 12900, lr = 0.00919375, m = 0.9
I0815 10:24:05.218369  8764 solver.cpp:363] Sparsity after update:
I0815 10:24:05.228261  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:24:05.228286  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:24:05.228301  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:24:05.228309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:24:05.228317  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:24:05.228325  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:24:05.228332  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:24:05.228340  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:24:05.228354  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:24:05.228363  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:24:05.228370  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:24:05.228379  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:24:05.228386  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:24:05.371446  8764 solver.cpp:312] Iteration 13000 (6.74961 iter/s, 14.8157s/100 iter), loss = 1.4975
I0815 10:24:05.371474  8764 solver.cpp:334]     Train net output #0: loss = 1.5766 (* 1 = 1.5766 loss)
I0815 10:24:05.371479  8764 sgd_solver.cpp:136] Iteration 13000, lr = 0.0091875, m = 0.9
I0815 10:24:19.705018  8764 solver.cpp:312] Iteration 13100 (6.97682 iter/s, 14.3332s/100 iter), loss = 1.5332
I0815 10:24:19.705121  8764 solver.cpp:334]     Train net output #0: loss = 1.43757 (* 1 = 1.43757 loss)
I0815 10:24:19.705139  8764 sgd_solver.cpp:136] Iteration 13100, lr = 0.00918125, m = 0.9
I0815 10:24:34.122544  8764 solver.cpp:312] Iteration 13200 (6.93619 iter/s, 14.4171s/100 iter), loss = 1.72298
I0815 10:24:34.122570  8764 solver.cpp:334]     Train net output #0: loss = 1.63294 (* 1 = 1.63294 loss)
I0815 10:24:34.122575  8764 sgd_solver.cpp:136] Iteration 13200, lr = 0.009175, m = 0.9
I0815 10:24:48.621631  8764 solver.cpp:312] Iteration 13300 (6.89718 iter/s, 14.4987s/100 iter), loss = 1.14293
I0815 10:24:48.621665  8764 solver.cpp:334]     Train net output #0: loss = 1.0525 (* 1 = 1.0525 loss)
I0815 10:24:48.621671  8764 sgd_solver.cpp:136] Iteration 13300, lr = 0.00916875, m = 0.9
I0815 10:25:03.396958  8764 solver.cpp:312] Iteration 13400 (6.76822 iter/s, 14.7749s/100 iter), loss = 1.57674
I0815 10:25:03.398630  8764 solver.cpp:334]     Train net output #0: loss = 1.63493 (* 1 = 1.63493 loss)
I0815 10:25:03.398648  8764 sgd_solver.cpp:136] Iteration 13400, lr = 0.0091625, m = 0.9
I0815 10:25:17.840298  8764 solver.cpp:312] Iteration 13500 (6.9238 iter/s, 14.4429s/100 iter), loss = 1.26778
I0815 10:25:17.840324  8764 solver.cpp:334]     Train net output #0: loss = 1.28192 (* 1 = 1.28192 loss)
I0815 10:25:17.840328  8764 sgd_solver.cpp:136] Iteration 13500, lr = 0.00915625, m = 0.9
I0815 10:25:32.309550  8764 solver.cpp:312] Iteration 13600 (6.9114 iter/s, 14.4689s/100 iter), loss = 1.26609
I0815 10:25:32.309577  8764 solver.cpp:334]     Train net output #0: loss = 1.33708 (* 1 = 1.33708 loss)
I0815 10:25:32.309583  8764 sgd_solver.cpp:136] Iteration 13600, lr = 0.00915, m = 0.9
I0815 10:25:47.076370  8764 solver.cpp:312] Iteration 13700 (6.77213 iter/s, 14.7664s/100 iter), loss = 1.25149
I0815 10:25:47.076469  8764 solver.cpp:334]     Train net output #0: loss = 1.21438 (* 1 = 1.21438 loss)
I0815 10:25:47.076488  8764 sgd_solver.cpp:136] Iteration 13700, lr = 0.00914375, m = 0.9
I0815 10:26:01.888300  8764 solver.cpp:312] Iteration 13800 (6.7515 iter/s, 14.8115s/100 iter), loss = 1.25259
I0815 10:26:01.888355  8764 solver.cpp:334]     Train net output #0: loss = 0.99163 (* 1 = 0.99163 loss)
I0815 10:26:01.888368  8764 sgd_solver.cpp:136] Iteration 13800, lr = 0.0091375, m = 0.9
I0815 10:26:16.768690  8764 solver.cpp:312] Iteration 13900 (6.72044 iter/s, 14.88s/100 iter), loss = 1.43786
I0815 10:26:16.768717  8764 solver.cpp:334]     Train net output #0: loss = 1.8209 (* 1 = 1.8209 loss)
I0815 10:26:16.768723  8764 sgd_solver.cpp:136] Iteration 13900, lr = 0.00913125, m = 0.9
I0815 10:26:31.630465  8764 solver.cpp:363] Sparsity after update:
I0815 10:26:31.636924  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:26:31.636940  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:26:31.636948  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:26:31.636951  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:26:31.636955  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:26:31.636960  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:26:31.636962  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:26:31.636965  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:26:31.636968  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:26:31.636971  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:26:31.636976  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:26:31.636978  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:26:31.636981  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:26:31.636992  8764 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 10:26:52.686982  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.537294
I0815 10:26:52.687005  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.778703
I0815 10:26:52.687011  8764 solver.cpp:594]     Test net output #2: loss = 2.07471 (* 1 = 2.07471 loss)
I0815 10:26:52.687036  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0495s
I0815 10:26:52.847307  8764 solver.cpp:312] Iteration 14000 (2.7718 iter/s, 36.0776s/100 iter), loss = 1.46188
I0815 10:26:52.847334  8764 solver.cpp:334]     Train net output #0: loss = 1.80588 (* 1 = 1.80588 loss)
I0815 10:26:52.847340  8764 sgd_solver.cpp:136] Iteration 14000, lr = 0.009125, m = 0.9
I0815 10:27:07.156455  8764 solver.cpp:312] Iteration 14100 (6.98873 iter/s, 14.3088s/100 iter), loss = 1.65773
I0815 10:27:07.156558  8764 solver.cpp:334]     Train net output #0: loss = 1.90813 (* 1 = 1.90813 loss)
I0815 10:27:07.156575  8764 sgd_solver.cpp:136] Iteration 14100, lr = 0.00911875, m = 0.9
I0815 10:27:21.398648  8764 solver.cpp:312] Iteration 14200 (7.02159 iter/s, 14.2418s/100 iter), loss = 1.56751
I0815 10:27:21.398718  8764 solver.cpp:334]     Train net output #0: loss = 1.78187 (* 1 = 1.78187 loss)
I0815 10:27:21.398738  8764 sgd_solver.cpp:136] Iteration 14200, lr = 0.0091125, m = 0.9
I0815 10:27:35.849916  8764 solver.cpp:312] Iteration 14300 (6.92 iter/s, 14.4509s/100 iter), loss = 1.32475
I0815 10:27:35.849982  8764 solver.cpp:334]     Train net output #0: loss = 1.28208 (* 1 = 1.28208 loss)
I0815 10:27:35.849999  8764 sgd_solver.cpp:136] Iteration 14300, lr = 0.00910625, m = 0.9
I0815 10:27:50.475085  8764 solver.cpp:312] Iteration 14400 (6.83772 iter/s, 14.6248s/100 iter), loss = 1.70034
I0815 10:27:50.475147  8764 solver.cpp:334]     Train net output #0: loss = 1.84253 (* 1 = 1.84253 loss)
I0815 10:27:50.475153  8764 sgd_solver.cpp:136] Iteration 14400, lr = 0.0091, m = 0.9
I0815 10:28:04.959123  8764 solver.cpp:312] Iteration 14500 (6.90435 iter/s, 14.4836s/100 iter), loss = 1.52315
I0815 10:28:04.959146  8764 solver.cpp:334]     Train net output #0: loss = 1.57122 (* 1 = 1.57122 loss)
I0815 10:28:04.959151  8764 sgd_solver.cpp:136] Iteration 14500, lr = 0.00909375, m = 0.9
I0815 10:28:19.381454  8764 solver.cpp:312] Iteration 14600 (6.93389 iter/s, 14.4219s/100 iter), loss = 1.55892
I0815 10:28:19.381479  8764 solver.cpp:334]     Train net output #0: loss = 1.20012 (* 1 = 1.20012 loss)
I0815 10:28:19.381484  8764 sgd_solver.cpp:136] Iteration 14600, lr = 0.0090875, m = 0.9
I0815 10:28:33.770452  8764 solver.cpp:312] Iteration 14700 (6.94995 iter/s, 14.3886s/100 iter), loss = 1.74884
I0815 10:28:33.772538  8764 solver.cpp:334]     Train net output #0: loss = 1.69962 (* 1 = 1.69962 loss)
I0815 10:28:33.772553  8764 sgd_solver.cpp:136] Iteration 14700, lr = 0.00908125, m = 0.9
I0815 10:28:48.161681  8764 solver.cpp:312] Iteration 14800 (6.94887 iter/s, 14.3908s/100 iter), loss = 1.31492
I0815 10:28:48.161708  8764 solver.cpp:334]     Train net output #0: loss = 1.31315 (* 1 = 1.31315 loss)
I0815 10:28:48.161715  8764 sgd_solver.cpp:136] Iteration 14800, lr = 0.009075, m = 0.9
I0815 10:29:02.629848  8764 solver.cpp:312] Iteration 14900 (6.91192 iter/s, 14.4678s/100 iter), loss = 1.46861
I0815 10:29:02.629910  8764 solver.cpp:334]     Train net output #0: loss = 1.56518 (* 1 = 1.56518 loss)
I0815 10:29:02.629930  8764 sgd_solver.cpp:136] Iteration 14900, lr = 0.00906875, m = 0.9
I0815 10:29:16.729456  8764 solver.cpp:363] Sparsity after update:
I0815 10:29:16.744709  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:29:16.744747  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:29:16.744765  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:29:16.744784  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:29:16.744796  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:29:16.744807  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:29:16.744819  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:29:16.744830  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:29:16.744841  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:29:16.744853  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:29:16.744864  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:29:16.744875  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:29:16.744886  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:29:16.884160  8764 solver.cpp:312] Iteration 15000 (7.01562 iter/s, 14.2539s/100 iter), loss = 1.55413
I0815 10:29:16.884182  8764 solver.cpp:334]     Train net output #0: loss = 1.5552 (* 1 = 1.5552 loss)
I0815 10:29:16.884187  8764 sgd_solver.cpp:136] Iteration 15000, lr = 0.0090625, m = 0.9
I0815 10:29:31.199621  8764 solver.cpp:312] Iteration 15100 (6.98565 iter/s, 14.3151s/100 iter), loss = 1.24791
I0815 10:29:31.199652  8764 solver.cpp:334]     Train net output #0: loss = 1.1021 (* 1 = 1.1021 loss)
I0815 10:29:31.199657  8764 sgd_solver.cpp:136] Iteration 15100, lr = 0.00905625, m = 0.9
I0815 10:29:45.591223  8764 solver.cpp:312] Iteration 15200 (6.94869 iter/s, 14.3912s/100 iter), loss = 1.80903
I0815 10:29:45.591291  8764 solver.cpp:334]     Train net output #0: loss = 1.83317 (* 1 = 1.83317 loss)
I0815 10:29:45.591308  8764 sgd_solver.cpp:136] Iteration 15200, lr = 0.00905, m = 0.9
I0815 10:29:59.970324  8764 solver.cpp:312] Iteration 15300 (6.95473 iter/s, 14.3787s/100 iter), loss = 1.58119
I0815 10:29:59.970432  8764 solver.cpp:334]     Train net output #0: loss = 1.80936 (* 1 = 1.80936 loss)
I0815 10:29:59.970451  8764 sgd_solver.cpp:136] Iteration 15300, lr = 0.00904375, m = 0.9
I0815 10:30:14.311681  8764 solver.cpp:312] Iteration 15400 (6.97304 iter/s, 14.341s/100 iter), loss = 1.62458
I0815 10:30:14.311749  8764 solver.cpp:334]     Train net output #0: loss = 1.72081 (* 1 = 1.72081 loss)
I0815 10:30:14.311769  8764 sgd_solver.cpp:136] Iteration 15400, lr = 0.0090375, m = 0.9
I0815 10:30:28.667563  8764 solver.cpp:312] Iteration 15500 (6.96598 iter/s, 14.3555s/100 iter), loss = 1.69418
I0815 10:30:28.667589  8764 solver.cpp:334]     Train net output #0: loss = 1.72651 (* 1 = 1.72651 loss)
I0815 10:30:28.667593  8764 sgd_solver.cpp:136] Iteration 15500, lr = 0.00903125, m = 0.9
I0815 10:30:43.277721  8764 solver.cpp:312] Iteration 15600 (6.84474 iter/s, 14.6098s/100 iter), loss = 1.73383
I0815 10:30:43.277819  8764 solver.cpp:334]     Train net output #0: loss = 1.79138 (* 1 = 1.79138 loss)
I0815 10:30:43.277837  8764 sgd_solver.cpp:136] Iteration 15600, lr = 0.009025, m = 0.9
I0815 10:30:57.750053  8764 solver.cpp:312] Iteration 15700 (6.90993 iter/s, 14.4719s/100 iter), loss = 1.58815
I0815 10:30:57.750123  8764 solver.cpp:334]     Train net output #0: loss = 1.33573 (* 1 = 1.33573 loss)
I0815 10:30:57.750144  8764 sgd_solver.cpp:136] Iteration 15700, lr = 0.00901875, m = 0.9
I0815 10:31:12.043956  8764 solver.cpp:312] Iteration 15800 (6.99619 iter/s, 14.2935s/100 iter), loss = 1.35074
I0815 10:31:12.044023  8764 solver.cpp:334]     Train net output #0: loss = 1.36072 (* 1 = 1.36072 loss)
I0815 10:31:12.044042  8764 sgd_solver.cpp:136] Iteration 15800, lr = 0.0090125, m = 0.9
I0815 10:31:26.731420  8764 solver.cpp:312] Iteration 15900 (6.80872 iter/s, 14.6871s/100 iter), loss = 1.51744
I0815 10:31:26.736160  8764 solver.cpp:334]     Train net output #0: loss = 1.39367 (* 1 = 1.39367 loss)
I0815 10:31:26.736171  8764 sgd_solver.cpp:136] Iteration 15900, lr = 0.00900625, m = 0.9
I0815 10:31:40.925514  8764 solver.cpp:363] Sparsity after update:
I0815 10:31:40.931443  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:31:40.931457  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:31:40.931463  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:31:40.931464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:31:40.931466  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:31:40.931468  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:31:40.931470  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:31:40.931471  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:31:40.931473  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:31:40.931475  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:31:40.931478  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:31:40.931479  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:31:40.931483  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:31:40.931493  8764 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 10:31:57.976362  8747 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 10:32:02.031059  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548471
I0815 10:32:02.031080  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.784527
I0815 10:32:02.031086  8764 solver.cpp:594]     Test net output #2: loss = 2.03109 (* 1 = 2.03109 loss)
I0815 10:32:02.031112  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.099s
I0815 10:32:02.195773  8764 solver.cpp:312] Iteration 16000 (2.81981 iter/s, 35.4634s/100 iter), loss = 1.20465
I0815 10:32:02.195801  8764 solver.cpp:334]     Train net output #0: loss = 1.09559 (* 1 = 1.09559 loss)
I0815 10:32:02.195807  8764 sgd_solver.cpp:136] Iteration 16000, lr = 0.009, m = 0.9
I0815 10:32:16.579713  8764 solver.cpp:312] Iteration 16100 (6.95239 iter/s, 14.3835s/100 iter), loss = 1.44112
I0815 10:32:16.579777  8764 solver.cpp:334]     Train net output #0: loss = 1.24157 (* 1 = 1.24157 loss)
I0815 10:32:16.579793  8764 sgd_solver.cpp:136] Iteration 16100, lr = 0.00899375, m = 0.9
I0815 10:32:30.942487  8764 solver.cpp:312] Iteration 16200 (6.96264 iter/s, 14.3624s/100 iter), loss = 1.31002
I0815 10:32:30.942548  8764 solver.cpp:334]     Train net output #0: loss = 1.18965 (* 1 = 1.18965 loss)
I0815 10:32:30.942555  8764 sgd_solver.cpp:136] Iteration 16200, lr = 0.0089875, m = 0.9
I0815 10:32:45.259392  8764 solver.cpp:312] Iteration 16300 (6.98495 iter/s, 14.3165s/100 iter), loss = 1.19198
I0815 10:32:45.259419  8764 solver.cpp:334]     Train net output #0: loss = 1.1025 (* 1 = 1.1025 loss)
I0815 10:32:45.259424  8764 sgd_solver.cpp:136] Iteration 16300, lr = 0.00898125, m = 0.9
I0815 10:32:59.728626  8764 solver.cpp:312] Iteration 16400 (6.91141 iter/s, 14.4688s/100 iter), loss = 1.46634
I0815 10:32:59.728654  8764 solver.cpp:334]     Train net output #0: loss = 1.0785 (* 1 = 1.0785 loss)
I0815 10:32:59.728660  8764 sgd_solver.cpp:136] Iteration 16400, lr = 0.008975, m = 0.9
I0815 10:33:14.081918  8764 solver.cpp:312] Iteration 16500 (6.96724 iter/s, 14.3529s/100 iter), loss = 1.54892
I0815 10:33:14.081982  8764 solver.cpp:334]     Train net output #0: loss = 1.94599 (* 1 = 1.94599 loss)
I0815 10:33:14.081991  8764 sgd_solver.cpp:136] Iteration 16500, lr = 0.00896875, m = 0.9
I0815 10:33:28.505082  8764 solver.cpp:312] Iteration 16600 (6.93348 iter/s, 14.4228s/100 iter), loss = 1.60668
I0815 10:33:28.505103  8764 solver.cpp:334]     Train net output #0: loss = 1.76218 (* 1 = 1.76218 loss)
I0815 10:33:28.505107  8764 sgd_solver.cpp:136] Iteration 16600, lr = 0.0089625, m = 0.9
I0815 10:33:42.984658  8764 solver.cpp:312] Iteration 16700 (6.90647 iter/s, 14.4792s/100 iter), loss = 1.43564
I0815 10:33:42.984690  8764 solver.cpp:334]     Train net output #0: loss = 1.53533 (* 1 = 1.53533 loss)
I0815 10:33:42.984696  8764 sgd_solver.cpp:136] Iteration 16700, lr = 0.00895625, m = 0.9
I0815 10:33:57.689990  8764 solver.cpp:312] Iteration 16800 (6.80044 iter/s, 14.7049s/100 iter), loss = 1.10436
I0815 10:33:57.690083  8764 solver.cpp:334]     Train net output #0: loss = 1.05811 (* 1 = 1.05811 loss)
I0815 10:33:57.690104  8764 sgd_solver.cpp:136] Iteration 16800, lr = 0.00895, m = 0.9
I0815 10:34:12.127557  8764 solver.cpp:312] Iteration 16900 (6.92657 iter/s, 14.4372s/100 iter), loss = 1.47177
I0815 10:34:12.127588  8764 solver.cpp:334]     Train net output #0: loss = 1.53715 (* 1 = 1.53715 loss)
I0815 10:34:12.127593  8764 sgd_solver.cpp:136] Iteration 16900, lr = 0.00894375, m = 0.9
I0815 10:34:26.619760  8764 solver.cpp:363] Sparsity after update:
I0815 10:34:26.632753  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:34:26.632802  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:34:26.632819  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:34:26.632832  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:34:26.632843  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:34:26.632854  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:34:26.632865  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:34:26.632877  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:34:26.632889  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:34:26.632900  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:34:26.632912  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:34:26.632923  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:34:26.632935  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:34:26.775313  8764 solver.cpp:312] Iteration 17000 (6.82718 iter/s, 14.6473s/100 iter), loss = 1.6301
I0815 10:34:26.775380  8764 solver.cpp:334]     Train net output #0: loss = 1.75171 (* 1 = 1.75171 loss)
I0815 10:34:26.775401  8764 sgd_solver.cpp:136] Iteration 17000, lr = 0.0089375, m = 0.9
I0815 10:34:41.195123  8764 solver.cpp:312] Iteration 17100 (6.9351 iter/s, 14.4194s/100 iter), loss = 1.38845
I0815 10:34:41.196164  8764 solver.cpp:334]     Train net output #0: loss = 1.4746 (* 1 = 1.4746 loss)
I0815 10:34:41.196174  8764 sgd_solver.cpp:136] Iteration 17100, lr = 0.00893125, m = 0.9
I0815 10:34:55.639494  8764 solver.cpp:312] Iteration 17200 (6.9233 iter/s, 14.444s/100 iter), loss = 1.94359
I0815 10:34:55.639521  8764 solver.cpp:334]     Train net output #0: loss = 2.11574 (* 1 = 2.11574 loss)
I0815 10:34:55.639528  8764 sgd_solver.cpp:136] Iteration 17200, lr = 0.008925, m = 0.9
I0815 10:35:09.988303  8764 solver.cpp:312] Iteration 17300 (6.96941 iter/s, 14.3484s/100 iter), loss = 1.31806
I0815 10:35:09.988332  8764 solver.cpp:334]     Train net output #0: loss = 0.945189 (* 1 = 0.945189 loss)
I0815 10:35:09.988338  8764 sgd_solver.cpp:136] Iteration 17300, lr = 0.00891875, m = 0.9
I0815 10:35:24.355796  8764 solver.cpp:312] Iteration 17400 (6.96035 iter/s, 14.3671s/100 iter), loss = 1.4887
I0815 10:35:24.355860  8764 solver.cpp:334]     Train net output #0: loss = 1.87435 (* 1 = 1.87435 loss)
I0815 10:35:24.355867  8764 sgd_solver.cpp:136] Iteration 17400, lr = 0.0089125, m = 0.9
I0815 10:35:38.926161  8764 solver.cpp:312] Iteration 17500 (6.86344 iter/s, 14.57s/100 iter), loss = 1.51671
I0815 10:35:38.926187  8764 solver.cpp:334]     Train net output #0: loss = 1.37766 (* 1 = 1.37766 loss)
I0815 10:35:38.926193  8764 sgd_solver.cpp:136] Iteration 17500, lr = 0.00890625, m = 0.9
I0815 10:35:53.224124  8764 solver.cpp:312] Iteration 17600 (6.9942 iter/s, 14.2976s/100 iter), loss = 1.28457
I0815 10:35:53.224161  8764 solver.cpp:334]     Train net output #0: loss = 1.407 (* 1 = 1.407 loss)
I0815 10:35:53.224167  8764 sgd_solver.cpp:136] Iteration 17600, lr = 0.0089, m = 0.9
I0815 10:36:07.869613  8764 solver.cpp:312] Iteration 17700 (6.82823 iter/s, 14.6451s/100 iter), loss = 1.65691
I0815 10:36:07.869701  8764 solver.cpp:334]     Train net output #0: loss = 1.86985 (* 1 = 1.86985 loss)
I0815 10:36:07.869711  8764 sgd_solver.cpp:136] Iteration 17700, lr = 0.00889375, m = 0.9
I0815 10:36:22.580610  8764 solver.cpp:312] Iteration 17800 (6.79782 iter/s, 14.7106s/100 iter), loss = 1.6942
I0815 10:36:22.580672  8764 solver.cpp:334]     Train net output #0: loss = 1.49174 (* 1 = 1.49174 loss)
I0815 10:36:22.580689  8764 sgd_solver.cpp:136] Iteration 17800, lr = 0.0088875, m = 0.9
I0815 10:36:37.274873  8764 solver.cpp:312] Iteration 17900 (6.80557 iter/s, 14.6939s/100 iter), loss = 1.08556
I0815 10:36:37.274902  8764 solver.cpp:334]     Train net output #0: loss = 0.612818 (* 1 = 0.612818 loss)
I0815 10:36:37.274909  8764 sgd_solver.cpp:136] Iteration 17900, lr = 0.00888125, m = 0.9
I0815 10:36:51.753264  8764 solver.cpp:363] Sparsity after update:
I0815 10:36:51.758139  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:36:51.758170  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:36:51.758188  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:36:51.758199  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:36:51.758210  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:36:51.758221  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:36:51.758234  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:36:51.758245  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:36:51.758256  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:36:51.758267  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:36:51.758278  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:36:51.758289  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:36:51.758301  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:36:51.758321  8764 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 10:37:06.472256  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:37:12.789474  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547353
I0815 10:37:12.789499  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.788056
I0815 10:37:12.789505  8764 solver.cpp:594]     Test net output #2: loss = 2.02225 (* 1 = 2.02225 loss)
I0815 10:37:12.789526  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0306s
I0815 10:37:12.930544  8764 solver.cpp:312] Iteration 18000 (2.80468 iter/s, 35.6547s/100 iter), loss = 1.52627
I0815 10:37:12.930567  8764 solver.cpp:334]     Train net output #0: loss = 1.38381 (* 1 = 1.38381 loss)
I0815 10:37:12.930572  8764 sgd_solver.cpp:136] Iteration 18000, lr = 0.008875, m = 0.9
I0815 10:37:27.439707  8764 solver.cpp:312] Iteration 18100 (6.89239 iter/s, 14.5088s/100 iter), loss = 1.73037
I0815 10:37:27.439795  8764 solver.cpp:334]     Train net output #0: loss = 1.9933 (* 1 = 1.9933 loss)
I0815 10:37:27.439811  8764 sgd_solver.cpp:136] Iteration 18100, lr = 0.00886875, m = 0.9
I0815 10:37:41.986382  8764 solver.cpp:312] Iteration 18200 (6.87462 iter/s, 14.5463s/100 iter), loss = 1.75411
I0815 10:37:41.986459  8764 solver.cpp:334]     Train net output #0: loss = 1.4956 (* 1 = 1.4956 loss)
I0815 10:37:41.986479  8764 sgd_solver.cpp:136] Iteration 18200, lr = 0.0088625, m = 0.9
I0815 10:37:56.513049  8764 solver.cpp:312] Iteration 18300 (6.88409 iter/s, 14.5263s/100 iter), loss = 1.52293
I0815 10:37:56.513101  8764 solver.cpp:334]     Train net output #0: loss = 1.42972 (* 1 = 1.42972 loss)
I0815 10:37:56.513113  8764 sgd_solver.cpp:136] Iteration 18300, lr = 0.00885625, m = 0.9
I0815 10:38:10.970475  8764 solver.cpp:312] Iteration 18400 (6.91706 iter/s, 14.457s/100 iter), loss = 1.17809
I0815 10:38:10.970535  8764 solver.cpp:334]     Train net output #0: loss = 1.40938 (* 1 = 1.40938 loss)
I0815 10:38:10.970541  8764 sgd_solver.cpp:136] Iteration 18400, lr = 0.00885, m = 0.9
I0815 10:38:25.490820  8764 solver.cpp:312] Iteration 18500 (6.88708 iter/s, 14.5199s/100 iter), loss = 1.31463
I0815 10:38:25.490845  8764 solver.cpp:334]     Train net output #0: loss = 1.6196 (* 1 = 1.6196 loss)
I0815 10:38:25.490851  8764 sgd_solver.cpp:136] Iteration 18500, lr = 0.00884375, m = 0.9
I0815 10:38:39.900907  8764 solver.cpp:312] Iteration 18600 (6.93978 iter/s, 14.4097s/100 iter), loss = 1.52249
I0815 10:38:39.900935  8764 solver.cpp:334]     Train net output #0: loss = 1.31894 (* 1 = 1.31894 loss)
I0815 10:38:39.900941  8764 sgd_solver.cpp:136] Iteration 18600, lr = 0.0088375, m = 0.9
I0815 10:38:54.243531  8764 solver.cpp:312] Iteration 18700 (6.97242 iter/s, 14.3422s/100 iter), loss = 1.4283
I0815 10:38:54.243628  8764 solver.cpp:334]     Train net output #0: loss = 1.59023 (* 1 = 1.59023 loss)
I0815 10:38:54.243645  8764 sgd_solver.cpp:136] Iteration 18700, lr = 0.00883125, m = 0.9
I0815 10:39:08.565724  8764 solver.cpp:312] Iteration 18800 (6.98236 iter/s, 14.3218s/100 iter), loss = 1.49904
I0815 10:39:08.565747  8764 solver.cpp:334]     Train net output #0: loss = 1.77952 (* 1 = 1.77952 loss)
I0815 10:39:08.565752  8764 sgd_solver.cpp:136] Iteration 18800, lr = 0.008825, m = 0.9
I0815 10:39:22.891083  8764 solver.cpp:312] Iteration 18900 (6.98082 iter/s, 14.325s/100 iter), loss = 1.89046
I0815 10:39:22.891134  8764 solver.cpp:334]     Train net output #0: loss = 1.83585 (* 1 = 1.83585 loss)
I0815 10:39:22.891146  8764 sgd_solver.cpp:136] Iteration 18900, lr = 0.00881875, m = 0.9
I0815 10:39:37.478921  8764 solver.cpp:363] Sparsity after update:
I0815 10:39:37.489289  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:39:37.489300  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:39:37.489308  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:39:37.489312  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:39:37.489317  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:39:37.489320  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:39:37.489325  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:39:37.489327  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:39:37.489329  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:39:37.489332  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:39:37.489334  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:39:37.489337  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:39:37.489341  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:39:37.639734  8764 solver.cpp:312] Iteration 19000 (6.78047 iter/s, 14.7482s/100 iter), loss = 1.88524
I0815 10:39:37.639758  8764 solver.cpp:334]     Train net output #0: loss = 1.87795 (* 1 = 1.87795 loss)
I0815 10:39:37.639763  8764 sgd_solver.cpp:136] Iteration 19000, lr = 0.0088125, m = 0.9
I0815 10:39:52.123793  8764 solver.cpp:312] Iteration 19100 (6.90433 iter/s, 14.4837s/100 iter), loss = 1.21713
I0815 10:39:52.123858  8764 solver.cpp:334]     Train net output #0: loss = 0.896025 (* 1 = 0.896025 loss)
I0815 10:39:52.123875  8764 sgd_solver.cpp:136] Iteration 19100, lr = 0.00880625, m = 0.9
I0815 10:40:06.750233  8764 solver.cpp:312] Iteration 19200 (6.83713 iter/s, 14.626s/100 iter), loss = 1.45944
I0815 10:40:06.750258  8764 solver.cpp:334]     Train net output #0: loss = 1.8879 (* 1 = 1.8879 loss)
I0815 10:40:06.750262  8764 sgd_solver.cpp:136] Iteration 19200, lr = 0.0088, m = 0.9
I0815 10:40:21.113793  8764 solver.cpp:312] Iteration 19300 (6.96226 iter/s, 14.3632s/100 iter), loss = 1.70222
I0815 10:40:21.113893  8764 solver.cpp:334]     Train net output #0: loss = 1.78798 (* 1 = 1.78798 loss)
I0815 10:40:21.113917  8764 sgd_solver.cpp:136] Iteration 19300, lr = 0.00879375, m = 0.9
I0815 10:40:35.346855  8764 solver.cpp:312] Iteration 19400 (7.02609 iter/s, 14.2327s/100 iter), loss = 1.61483
I0815 10:40:35.346927  8764 solver.cpp:334]     Train net output #0: loss = 1.53809 (* 1 = 1.53809 loss)
I0815 10:40:35.346947  8764 sgd_solver.cpp:136] Iteration 19400, lr = 0.0087875, m = 0.9
I0815 10:40:50.025851  8764 solver.cpp:312] Iteration 19500 (6.81264 iter/s, 14.6786s/100 iter), loss = 1.68126
I0815 10:40:50.025872  8764 solver.cpp:334]     Train net output #0: loss = 1.62817 (* 1 = 1.62817 loss)
I0815 10:40:50.025876  8764 sgd_solver.cpp:136] Iteration 19500, lr = 0.00878125, m = 0.9
I0815 10:41:04.491503  8764 solver.cpp:312] Iteration 19600 (6.91312 iter/s, 14.4652s/100 iter), loss = 1.18645
I0815 10:41:04.491574  8764 solver.cpp:334]     Train net output #0: loss = 1.15002 (* 1 = 1.15002 loss)
I0815 10:41:04.491580  8764 sgd_solver.cpp:136] Iteration 19600, lr = 0.008775, m = 0.9
I0815 10:41:19.184689  8764 solver.cpp:312] Iteration 19700 (6.80606 iter/s, 14.6928s/100 iter), loss = 1.40411
I0815 10:41:19.184713  8764 solver.cpp:334]     Train net output #0: loss = 1.49794 (* 1 = 1.49794 loss)
I0815 10:41:19.184718  8764 sgd_solver.cpp:136] Iteration 19700, lr = 0.00876875, m = 0.9
I0815 10:41:33.767304  8764 solver.cpp:312] Iteration 19800 (6.85767 iter/s, 14.5822s/100 iter), loss = 1.9287
I0815 10:41:33.767330  8764 solver.cpp:334]     Train net output #0: loss = 1.53796 (* 1 = 1.53796 loss)
I0815 10:41:33.767338  8764 sgd_solver.cpp:136] Iteration 19800, lr = 0.0087625, m = 0.9
I0815 10:41:48.149811  8764 solver.cpp:312] Iteration 19900 (6.95308 iter/s, 14.3821s/100 iter), loss = 1.53357
I0815 10:41:48.149880  8764 solver.cpp:334]     Train net output #0: loss = 1.29975 (* 1 = 1.29975 loss)
I0815 10:41:48.149888  8764 sgd_solver.cpp:136] Iteration 19900, lr = 0.00875625, m = 0.9
I0815 10:42:02.653903  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_20000.caffemodel
I0815 10:42:02.666936  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_20000.solverstate
I0815 10:42:02.673233  8764 solver.cpp:363] Sparsity after update:
I0815 10:42:02.676901  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:42:02.676919  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:42:02.676928  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:42:02.676931  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:42:02.676935  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:42:02.676939  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:42:02.676949  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:42:02.676954  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:42:02.676959  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:42:02.676965  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:42:02.676970  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:42:02.676975  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:42:02.676978  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:42:02.676992  8764 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 10:42:23.819118  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548824
I0815 10:42:23.819169  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.785468
I0815 10:42:23.819175  8764 solver.cpp:594]     Test net output #2: loss = 2.0085 (* 1 = 2.0085 loss)
I0815 10:42:23.819193  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1416s
I0815 10:42:23.971807  8794 solver.cpp:409] Finding and applying sparsity: 0.01
I0815 10:42:42.599304  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:42:42.601398  8764 solver.cpp:312] Iteration 20000 (1.83654 iter/s, 54.4501s/100 iter), loss = 1.59951
I0815 10:42:42.601418  8764 solver.cpp:334]     Train net output #0: loss = 1.54208 (* 1 = 1.54208 loss)
I0815 10:42:42.601430  8764 sgd_solver.cpp:136] Iteration 20000, lr = 0.00875, m = 0.9
I0815 10:42:57.491704  8764 solver.cpp:312] Iteration 20100 (6.71597 iter/s, 14.8899s/100 iter), loss = 1.5301
I0815 10:42:57.491760  8764 solver.cpp:334]     Train net output #0: loss = 1.81347 (* 1 = 1.81347 loss)
I0815 10:42:57.491766  8764 sgd_solver.cpp:136] Iteration 20100, lr = 0.00874375, m = 0.9
I0815 10:43:12.357650  8764 solver.cpp:312] Iteration 20200 (6.72697 iter/s, 14.8655s/100 iter), loss = 1.74039
I0815 10:43:12.357681  8764 solver.cpp:334]     Train net output #0: loss = 1.58709 (* 1 = 1.58709 loss)
I0815 10:43:12.357686  8764 sgd_solver.cpp:136] Iteration 20200, lr = 0.0087375, m = 0.9
I0815 10:43:27.331311  8764 solver.cpp:312] Iteration 20300 (6.67858 iter/s, 14.9732s/100 iter), loss = 1.37512
I0815 10:43:27.331456  8764 solver.cpp:334]     Train net output #0: loss = 1.60547 (* 1 = 1.60547 loss)
I0815 10:43:27.331478  8764 sgd_solver.cpp:136] Iteration 20300, lr = 0.00873125, m = 0.9
I0815 10:43:42.289146  8764 solver.cpp:312] Iteration 20400 (6.68565 iter/s, 14.9574s/100 iter), loss = 1.40209
I0815 10:43:42.289197  8764 solver.cpp:334]     Train net output #0: loss = 1.40282 (* 1 = 1.40282 loss)
I0815 10:43:42.289204  8764 sgd_solver.cpp:136] Iteration 20400, lr = 0.008725, m = 0.9
I0815 10:43:57.213552  8764 solver.cpp:312] Iteration 20500 (6.70062 iter/s, 14.924s/100 iter), loss = 1.45482
I0815 10:43:57.213578  8764 solver.cpp:334]     Train net output #0: loss = 1.2293 (* 1 = 1.2293 loss)
I0815 10:43:57.213583  8764 sgd_solver.cpp:136] Iteration 20500, lr = 0.00871875, m = 0.9
I0815 10:44:11.939987  8764 solver.cpp:312] Iteration 20600 (6.7907 iter/s, 14.726s/100 iter), loss = 1.39389
I0815 10:44:11.940014  8764 solver.cpp:334]     Train net output #0: loss = 0.971006 (* 1 = 0.971006 loss)
I0815 10:44:11.940018  8764 sgd_solver.cpp:136] Iteration 20600, lr = 0.0087125, m = 0.9
I0815 10:44:26.356204  8764 solver.cpp:312] Iteration 20700 (6.93683 iter/s, 14.4158s/100 iter), loss = 1.77832
I0815 10:44:26.356287  8764 solver.cpp:334]     Train net output #0: loss = 1.51747 (* 1 = 1.51747 loss)
I0815 10:44:26.356293  8764 sgd_solver.cpp:136] Iteration 20700, lr = 0.00870625, m = 0.9
I0815 10:44:41.048907  8764 solver.cpp:312] Iteration 20800 (6.80629 iter/s, 14.6923s/100 iter), loss = 1.31137
I0815 10:44:41.048931  8764 solver.cpp:334]     Train net output #0: loss = 1.5965 (* 1 = 1.5965 loss)
I0815 10:44:41.048935  8764 sgd_solver.cpp:136] Iteration 20800, lr = 0.0087, m = 0.9
I0815 10:44:55.628990  8764 solver.cpp:312] Iteration 20900 (6.85886 iter/s, 14.5797s/100 iter), loss = 1.28849
I0815 10:44:55.629019  8764 solver.cpp:334]     Train net output #0: loss = 1.28038 (* 1 = 1.28038 loss)
I0815 10:44:55.629024  8764 sgd_solver.cpp:136] Iteration 20900, lr = 0.00869375, m = 0.9
I0815 10:45:10.153106  8764 solver.cpp:363] Sparsity after update:
I0815 10:45:10.168390  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:45:10.168432  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:45:10.168452  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:45:10.168463  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:45:10.168474  8764 net.cpp:2192] res2a_branch2a_param_0(0.00694) 
I0815 10:45:10.168488  8764 net.cpp:2192] res2a_branch2b_param_0(0.00694) 
I0815 10:45:10.168500  8764 net.cpp:2192] res3a_branch2a_param_0(0.00868) 
I0815 10:45:10.168512  8764 net.cpp:2192] res3a_branch2b_param_0(0.00694) 
I0815 10:45:10.168524  8764 net.cpp:2192] res4a_branch2a_param_0(0.00954) 
I0815 10:45:10.168536  8764 net.cpp:2192] res4a_branch2b_param_0(0.00867) 
I0815 10:45:10.168548  8764 net.cpp:2192] res5a_branch2a_param_0(0.00997) 
I0815 10:45:10.168560  8764 net.cpp:2192] res5a_branch2b_param_0(0.00952) 
I0815 10:45:10.168571  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (22558/2.86678e+06) 0.00787
I0815 10:45:10.306285  8794 solver.cpp:409] Finding and applying sparsity: 0.02
I0815 10:45:29.175987  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:45:29.177980  8764 solver.cpp:312] Iteration 21000 (2.9808 iter/s, 33.5481s/100 iter), loss = 1.59313
I0815 10:45:29.177999  8764 solver.cpp:334]     Train net output #0: loss = 1.34017 (* 1 = 1.34017 loss)
I0815 10:45:29.178006  8764 sgd_solver.cpp:136] Iteration 21000, lr = 0.0086875, m = 0.9
I0815 10:45:44.513276  8764 solver.cpp:312] Iteration 21100 (6.52109 iter/s, 15.3349s/100 iter), loss = 1.80665
I0815 10:45:44.513381  8764 solver.cpp:334]     Train net output #0: loss = 1.3402 (* 1 = 1.3402 loss)
I0815 10:45:44.513403  8764 sgd_solver.cpp:136] Iteration 21100, lr = 0.00868125, m = 0.9
I0815 10:45:59.201431  8764 solver.cpp:312] Iteration 21200 (6.8084 iter/s, 14.6877s/100 iter), loss = 1.67857
I0815 10:45:59.201455  8764 solver.cpp:334]     Train net output #0: loss = 2.1459 (* 1 = 2.1459 loss)
I0815 10:45:59.201460  8764 sgd_solver.cpp:136] Iteration 21200, lr = 0.008675, m = 0.9
I0815 10:46:13.636041  8764 solver.cpp:312] Iteration 21300 (6.92799 iter/s, 14.4342s/100 iter), loss = 1.47882
I0815 10:46:13.636106  8764 solver.cpp:334]     Train net output #0: loss = 1.98458 (* 1 = 1.98458 loss)
I0815 10:46:13.636123  8764 sgd_solver.cpp:136] Iteration 21300, lr = 0.00866875, m = 0.9
I0815 10:46:28.163369  8764 solver.cpp:312] Iteration 21400 (6.88377 iter/s, 14.5269s/100 iter), loss = 1.43872
I0815 10:46:28.163446  8764 solver.cpp:334]     Train net output #0: loss = 1.23638 (* 1 = 1.23638 loss)
I0815 10:46:28.163455  8764 sgd_solver.cpp:136] Iteration 21400, lr = 0.0086625, m = 0.9
I0815 10:46:42.565021  8764 solver.cpp:312] Iteration 21500 (6.94384 iter/s, 14.4012s/100 iter), loss = 1.53981
I0815 10:46:42.565048  8764 solver.cpp:334]     Train net output #0: loss = 1.59704 (* 1 = 1.59704 loss)
I0815 10:46:42.565053  8764 sgd_solver.cpp:136] Iteration 21500, lr = 0.00865625, m = 0.9
I0815 10:46:57.042495  8764 solver.cpp:312] Iteration 21600 (6.90747 iter/s, 14.4771s/100 iter), loss = 1.87825
I0815 10:46:57.042695  8764 solver.cpp:334]     Train net output #0: loss = 1.69784 (* 1 = 1.69784 loss)
I0815 10:46:57.042809  8764 sgd_solver.cpp:136] Iteration 21600, lr = 0.00865, m = 0.9
I0815 10:47:11.419450  8764 solver.cpp:312] Iteration 21700 (6.95577 iter/s, 14.3766s/100 iter), loss = 1.68366
I0815 10:47:11.419523  8764 solver.cpp:334]     Train net output #0: loss = 1.72389 (* 1 = 1.72389 loss)
I0815 10:47:11.419535  8764 sgd_solver.cpp:136] Iteration 21700, lr = 0.00864375, m = 0.9
I0815 10:47:25.902451  8764 solver.cpp:312] Iteration 21800 (6.90484 iter/s, 14.4826s/100 iter), loss = 1.25637
I0815 10:47:25.902482  8764 solver.cpp:334]     Train net output #0: loss = 1.20337 (* 1 = 1.20337 loss)
I0815 10:47:25.902490  8764 sgd_solver.cpp:136] Iteration 21800, lr = 0.0086375, m = 0.9
I0815 10:47:40.603759  8764 solver.cpp:312] Iteration 21900 (6.80231 iter/s, 14.7009s/100 iter), loss = 1.32153
I0815 10:47:40.603786  8764 solver.cpp:334]     Train net output #0: loss = 1.65279 (* 1 = 1.65279 loss)
I0815 10:47:40.603791  8764 sgd_solver.cpp:136] Iteration 21900, lr = 0.00863125, m = 0.9
I0815 10:47:54.824087  8764 solver.cpp:363] Sparsity after update:
I0815 10:47:54.828191  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:47:54.828200  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:47:54.828208  8764 net.cpp:2192] conv1b_param_0(0.00694) 
I0815 10:47:54.828212  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:47:54.828223  8764 net.cpp:2192] res2a_branch2a_param_0(0.0174) 
I0815 10:47:54.828233  8764 net.cpp:2192] res2a_branch2b_param_0(0.0139) 
I0815 10:47:54.828241  8764 net.cpp:2192] res3a_branch2a_param_0(0.0191) 
I0815 10:47:54.828250  8764 net.cpp:2192] res3a_branch2b_param_0(0.0174) 
I0815 10:47:54.828259  8764 net.cpp:2192] res4a_branch2a_param_0(0.02) 
I0815 10:47:54.828268  8764 net.cpp:2192] res4a_branch2b_param_0(0.0191) 
I0815 10:47:54.828275  8764 net.cpp:2192] res5a_branch2a_param_0(0.02) 
I0815 10:47:54.828285  8764 net.cpp:2192] res5a_branch2b_param_0(0.0199) 
I0815 10:47:54.828294  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (46520/2.86678e+06) 0.0162
I0815 10:47:54.828311  8764 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 10:48:16.009193  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.552236
I0815 10:48:16.009217  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.79335
I0815 10:48:16.009222  8764 solver.cpp:594]     Test net output #2: loss = 1.98977 (* 1 = 1.98977 loss)
I0815 10:48:16.009241  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1803s
I0815 10:48:16.168021  8794 solver.cpp:409] Finding and applying sparsity: 0.03
I0815 10:48:34.965389  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:48:34.967432  8764 solver.cpp:312] Iteration 22000 (1.83951 iter/s, 54.3622s/100 iter), loss = 1.91454
I0815 10:48:34.967455  8764 solver.cpp:334]     Train net output #0: loss = 1.64977 (* 1 = 1.64977 loss)
I0815 10:48:34.967465  8764 sgd_solver.cpp:136] Iteration 22000, lr = 0.008625, m = 0.9
I0815 10:48:49.813133  8764 solver.cpp:312] Iteration 22100 (6.73615 iter/s, 14.8453s/100 iter), loss = 1.60787
I0815 10:48:49.813159  8764 solver.cpp:334]     Train net output #0: loss = 1.62387 (* 1 = 1.62387 loss)
I0815 10:48:49.813164  8764 sgd_solver.cpp:136] Iteration 22100, lr = 0.00861875, m = 0.9
I0815 10:49:04.275087  8764 solver.cpp:312] Iteration 22200 (6.91489 iter/s, 14.4615s/100 iter), loss = 1.15288
I0815 10:49:04.275144  8764 solver.cpp:334]     Train net output #0: loss = 0.914332 (* 1 = 0.914332 loss)
I0815 10:49:04.275156  8764 sgd_solver.cpp:136] Iteration 22200, lr = 0.0086125, m = 0.9
I0815 10:49:18.648142  8764 solver.cpp:312] Iteration 22300 (6.95766 iter/s, 14.3726s/100 iter), loss = 1.14478
I0815 10:49:18.648213  8764 solver.cpp:334]     Train net output #0: loss = 1.12705 (* 1 = 1.12705 loss)
I0815 10:49:18.648221  8764 sgd_solver.cpp:136] Iteration 22300, lr = 0.00860625, m = 0.9
I0815 10:49:33.046385  8764 solver.cpp:312] Iteration 22400 (6.94549 iter/s, 14.3978s/100 iter), loss = 1.61693
I0815 10:49:33.046407  8764 solver.cpp:334]     Train net output #0: loss = 1.56965 (* 1 = 1.56965 loss)
I0815 10:49:33.046412  8764 sgd_solver.cpp:136] Iteration 22400, lr = 0.0086, m = 0.9
I0815 10:49:47.557471  8764 solver.cpp:312] Iteration 22500 (6.89148 iter/s, 14.5107s/100 iter), loss = 1.28368
I0815 10:49:47.557497  8764 solver.cpp:334]     Train net output #0: loss = 1.29777 (* 1 = 1.29777 loss)
I0815 10:49:47.557503  8764 sgd_solver.cpp:136] Iteration 22500, lr = 0.00859375, m = 0.9
I0815 10:50:01.948277  8764 solver.cpp:312] Iteration 22600 (6.94908 iter/s, 14.3904s/100 iter), loss = 1.45606
I0815 10:50:01.948354  8764 solver.cpp:334]     Train net output #0: loss = 0.793708 (* 1 = 0.793708 loss)
I0815 10:50:01.948361  8764 sgd_solver.cpp:136] Iteration 22600, lr = 0.0085875, m = 0.9
I0815 10:50:16.412655  8764 solver.cpp:312] Iteration 22700 (6.91373 iter/s, 14.464s/100 iter), loss = 1.37543
I0815 10:50:16.412708  8764 solver.cpp:334]     Train net output #0: loss = 1.70279 (* 1 = 1.70279 loss)
I0815 10:50:16.412720  8764 sgd_solver.cpp:136] Iteration 22700, lr = 0.00858125, m = 0.9
I0815 10:50:31.364444  8764 solver.cpp:312] Iteration 22800 (6.68835 iter/s, 14.9514s/100 iter), loss = 1.49565
I0815 10:50:31.364472  8764 solver.cpp:334]     Train net output #0: loss = 1.71238 (* 1 = 1.71238 loss)
I0815 10:50:31.364478  8764 sgd_solver.cpp:136] Iteration 22800, lr = 0.008575, m = 0.9
I0815 10:50:45.899433  8764 solver.cpp:312] Iteration 22900 (6.88014 iter/s, 14.5346s/100 iter), loss = 1.57657
I0815 10:50:45.899657  8764 solver.cpp:334]     Train net output #0: loss = 1.47793 (* 1 = 1.47793 loss)
I0815 10:50:45.899670  8764 sgd_solver.cpp:136] Iteration 22900, lr = 0.00856875, m = 0.9
I0815 10:51:00.159179  8764 solver.cpp:363] Sparsity after update:
I0815 10:51:00.170892  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:51:00.170928  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:51:00.170948  8764 net.cpp:2192] conv1b_param_0(0.0139) 
I0815 10:51:00.170960  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:51:00.170979  8764 net.cpp:2192] res2a_branch2a_param_0(0.0278) 
I0815 10:51:00.170991  8764 net.cpp:2192] res2a_branch2b_param_0(0.0278) 
I0815 10:51:00.171002  8764 net.cpp:2192] res3a_branch2a_param_0(0.0295) 
I0815 10:51:00.171015  8764 net.cpp:2192] res3a_branch2b_param_0(0.0278) 
I0815 10:51:00.171025  8764 net.cpp:2192] res4a_branch2a_param_0(0.0295) 
I0815 10:51:00.171037  8764 net.cpp:2192] res4a_branch2b_param_0(0.0295) 
I0815 10:51:00.171048  8764 net.cpp:2192] res5a_branch2a_param_0(0.0299) 
I0815 10:51:00.171061  8764 net.cpp:2192] res5a_branch2b_param_0(0.0295) 
I0815 10:51:00.171072  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (69782/2.86678e+06) 0.0243
I0815 10:51:00.305153  8794 solver.cpp:409] Finding and applying sparsity: 0.04
I0815 10:51:19.085427  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:51:19.087574  8764 solver.cpp:312] Iteration 23000 (3.01321 iter/s, 33.1872s/100 iter), loss = 1.58943
I0815 10:51:19.087594  8764 solver.cpp:334]     Train net output #0: loss = 1.69372 (* 1 = 1.69372 loss)
I0815 10:51:19.087600  8764 sgd_solver.cpp:136] Iteration 23000, lr = 0.0085625, m = 0.9
I0815 10:51:33.884841  8764 solver.cpp:312] Iteration 23100 (6.7582 iter/s, 14.7968s/100 iter), loss = 1.42651
I0815 10:51:33.884912  8764 solver.cpp:334]     Train net output #0: loss = 1.34256 (* 1 = 1.34256 loss)
I0815 10:51:33.884930  8764 sgd_solver.cpp:136] Iteration 23100, lr = 0.00855625, m = 0.9
I0815 10:51:48.259126  8764 solver.cpp:312] Iteration 23200 (6.95706 iter/s, 14.3739s/100 iter), loss = 1.42097
I0815 10:51:48.259150  8764 solver.cpp:334]     Train net output #0: loss = 1.15643 (* 1 = 1.15643 loss)
I0815 10:51:48.259153  8764 sgd_solver.cpp:136] Iteration 23200, lr = 0.00855, m = 0.9
I0815 10:52:02.593327  8764 solver.cpp:312] Iteration 23300 (6.97652 iter/s, 14.3338s/100 iter), loss = 1.47046
I0815 10:52:02.593406  8764 solver.cpp:334]     Train net output #0: loss = 1.39842 (* 1 = 1.39842 loss)
I0815 10:52:02.593412  8764 sgd_solver.cpp:136] Iteration 23300, lr = 0.00854375, m = 0.9
I0815 10:52:16.977816  8764 solver.cpp:312] Iteration 23400 (6.95213 iter/s, 14.3841s/100 iter), loss = 1.37197
I0815 10:52:16.977843  8764 solver.cpp:334]     Train net output #0: loss = 1.44074 (* 1 = 1.44074 loss)
I0815 10:52:16.977849  8764 sgd_solver.cpp:136] Iteration 23400, lr = 0.0085375, m = 0.9
I0815 10:52:31.329288  8764 solver.cpp:312] Iteration 23500 (6.96812 iter/s, 14.3511s/100 iter), loss = 1.3692
I0815 10:52:31.329316  8764 solver.cpp:334]     Train net output #0: loss = 1.40021 (* 1 = 1.40021 loss)
I0815 10:52:31.329321  8764 sgd_solver.cpp:136] Iteration 23500, lr = 0.00853125, m = 0.9
I0815 10:52:45.866128  8764 solver.cpp:312] Iteration 23600 (6.87926 iter/s, 14.5364s/100 iter), loss = 1.69229
I0815 10:52:45.866183  8764 solver.cpp:334]     Train net output #0: loss = 1.79838 (* 1 = 1.79838 loss)
I0815 10:52:45.866189  8764 sgd_solver.cpp:136] Iteration 23600, lr = 0.008525, m = 0.9
I0815 10:53:00.178336  8764 solver.cpp:312] Iteration 23700 (6.98724 iter/s, 14.3118s/100 iter), loss = 1.28587
I0815 10:53:00.178364  8764 solver.cpp:334]     Train net output #0: loss = 1.26745 (* 1 = 1.26745 loss)
I0815 10:53:00.178369  8764 sgd_solver.cpp:136] Iteration 23700, lr = 0.00851875, m = 0.9
I0815 10:53:15.022892  8764 solver.cpp:312] Iteration 23800 (6.73666 iter/s, 14.8441s/100 iter), loss = 1.5245
I0815 10:53:15.022914  8764 solver.cpp:334]     Train net output #0: loss = 1.86026 (* 1 = 1.86026 loss)
I0815 10:53:15.022919  8764 sgd_solver.cpp:136] Iteration 23800, lr = 0.0085125, m = 0.9
I0815 10:53:29.333326  8764 solver.cpp:312] Iteration 23900 (6.9881 iter/s, 14.31s/100 iter), loss = 1.37899
I0815 10:53:29.333431  8764 solver.cpp:334]     Train net output #0: loss = 1.11413 (* 1 = 1.11413 loss)
I0815 10:53:29.333451  8764 sgd_solver.cpp:136] Iteration 23900, lr = 0.00850625, m = 0.9
I0815 10:53:43.467234  8764 solver.cpp:363] Sparsity after update:
I0815 10:53:43.472162  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:53:43.472174  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:53:43.472183  8764 net.cpp:2192] conv1b_param_0(0.0139) 
I0815 10:53:43.472187  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:53:43.472190  8764 net.cpp:2192] res2a_branch2a_param_0(0.0382) 
I0815 10:53:43.472204  8764 net.cpp:2192] res2a_branch2b_param_0(0.0347) 
I0815 10:53:43.472213  8764 net.cpp:2192] res3a_branch2a_param_0(0.0399) 
I0815 10:53:43.472221  8764 net.cpp:2192] res3a_branch2b_param_0(0.0382) 
I0815 10:53:43.472229  8764 net.cpp:2192] res4a_branch2a_param_0(0.0399) 
I0815 10:53:43.472237  8764 net.cpp:2192] res4a_branch2b_param_0(0.0399) 
I0815 10:53:43.472245  8764 net.cpp:2192] res5a_branch2a_param_0(0.0399) 
I0815 10:53:43.472252  8764 net.cpp:2192] res5a_branch2b_param_0(0.0399) 
I0815 10:53:43.472260  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (93730/2.86678e+06) 0.0327
I0815 10:53:43.472276  8764 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 10:53:56.082759  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:54:04.336007  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548529
I0815 10:54:04.336067  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.786939
I0815 10:54:04.336076  8764 solver.cpp:594]     Test net output #2: loss = 1.98828 (* 1 = 1.98828 loss)
I0815 10:54:04.336098  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8632s
I0815 10:54:04.485455  8794 solver.cpp:409] Finding and applying sparsity: 0.05
I0815 10:54:23.374076  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:54:23.376071  8764 solver.cpp:312] Iteration 24000 (1.85044 iter/s, 54.0413s/100 iter), loss = 1.41579
I0815 10:54:23.376091  8764 solver.cpp:334]     Train net output #0: loss = 1.24893 (* 1 = 1.24893 loss)
I0815 10:54:23.376097  8764 sgd_solver.cpp:136] Iteration 24000, lr = 0.0085, m = 0.9
I0815 10:54:38.314175  8764 solver.cpp:312] Iteration 24100 (6.69448 iter/s, 14.9377s/100 iter), loss = 1.31769
I0815 10:54:38.314244  8764 solver.cpp:334]     Train net output #0: loss = 1.08207 (* 1 = 1.08207 loss)
I0815 10:54:38.314249  8764 sgd_solver.cpp:136] Iteration 24100, lr = 0.00849375, m = 0.9
I0815 10:54:53.318390  8764 solver.cpp:312] Iteration 24200 (6.66498 iter/s, 15.0038s/100 iter), loss = 1.31259
I0815 10:54:53.318418  8764 solver.cpp:334]     Train net output #0: loss = 0.99761 (* 1 = 0.99761 loss)
I0815 10:54:53.318423  8764 sgd_solver.cpp:136] Iteration 24200, lr = 0.0084875, m = 0.9
I0815 10:55:08.461946  8764 solver.cpp:312] Iteration 24300 (6.60365 iter/s, 15.1431s/100 iter), loss = 1.09978
I0815 10:55:08.462036  8764 solver.cpp:334]     Train net output #0: loss = 1.16197 (* 1 = 1.16197 loss)
I0815 10:55:08.462054  8764 sgd_solver.cpp:136] Iteration 24300, lr = 0.00848125, m = 0.9
I0815 10:55:22.966084  8764 solver.cpp:312] Iteration 24400 (6.89478 iter/s, 14.5037s/100 iter), loss = 1.36673
I0815 10:55:22.966110  8764 solver.cpp:334]     Train net output #0: loss = 1.41052 (* 1 = 1.41052 loss)
I0815 10:55:22.966115  8764 sgd_solver.cpp:136] Iteration 24400, lr = 0.008475, m = 0.9
I0815 10:55:37.594830  8764 solver.cpp:312] Iteration 24500 (6.83605 iter/s, 14.6283s/100 iter), loss = 1.45214
I0815 10:55:37.594882  8764 solver.cpp:334]     Train net output #0: loss = 1.78618 (* 1 = 1.78618 loss)
I0815 10:55:37.594895  8764 sgd_solver.cpp:136] Iteration 24500, lr = 0.00846875, m = 0.9
I0815 10:55:52.225976  8764 solver.cpp:312] Iteration 24600 (6.83493 iter/s, 14.6307s/100 iter), loss = 1.60348
I0815 10:55:52.226032  8764 solver.cpp:334]     Train net output #0: loss = 1.33022 (* 1 = 1.33022 loss)
I0815 10:55:52.226074  8764 sgd_solver.cpp:136] Iteration 24600, lr = 0.0084625, m = 0.9
I0815 10:56:06.634874  8764 solver.cpp:312] Iteration 24700 (6.94035 iter/s, 14.4085s/100 iter), loss = 1.36035
I0815 10:56:06.634902  8764 solver.cpp:334]     Train net output #0: loss = 1.55896 (* 1 = 1.55896 loss)
I0815 10:56:06.634905  8764 sgd_solver.cpp:136] Iteration 24700, lr = 0.00845625, m = 0.9
I0815 10:56:21.255949  8764 solver.cpp:312] Iteration 24800 (6.83963 iter/s, 14.6207s/100 iter), loss = 1.26099
I0815 10:56:21.255976  8764 solver.cpp:334]     Train net output #0: loss = 1.11093 (* 1 = 1.11093 loss)
I0815 10:56:21.255982  8764 sgd_solver.cpp:136] Iteration 24800, lr = 0.00845, m = 0.9
I0815 10:56:35.643162  8764 solver.cpp:312] Iteration 24900 (6.95081 iter/s, 14.3868s/100 iter), loss = 1.26161
I0815 10:56:35.643267  8764 solver.cpp:334]     Train net output #0: loss = 1.19712 (* 1 = 1.19712 loss)
I0815 10:56:35.643286  8764 sgd_solver.cpp:136] Iteration 24900, lr = 0.00844375, m = 0.9
I0815 10:56:50.086872  8764 solver.cpp:363] Sparsity after update:
I0815 10:56:50.100687  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:56:50.100702  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:56:50.100710  8764 net.cpp:2192] conv1b_param_0(0.0208) 
I0815 10:56:50.100713  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:56:50.100726  8764 net.cpp:2192] res2a_branch2a_param_0(0.0486) 
I0815 10:56:50.100736  8764 net.cpp:2192] res2a_branch2b_param_0(0.0486) 
I0815 10:56:50.100744  8764 net.cpp:2192] res3a_branch2a_param_0(0.0486) 
I0815 10:56:50.100752  8764 net.cpp:2192] res3a_branch2b_param_0(0.0486) 
I0815 10:56:50.100760  8764 net.cpp:2192] res4a_branch2a_param_0(0.0495) 
I0815 10:56:50.100769  8764 net.cpp:2192] res4a_branch2b_param_0(0.0486) 
I0815 10:56:50.100776  8764 net.cpp:2192] res5a_branch2a_param_0(0.0499) 
I0815 10:56:50.100790  8764 net.cpp:2192] res5a_branch2b_param_0(0.0494) 
I0815 10:56:50.100798  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (116588/2.86678e+06) 0.0407
I0815 10:56:50.229231  8794 solver.cpp:409] Finding and applying sparsity: 0.06
I0815 10:57:09.534209  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:57:09.536237  8764 solver.cpp:312] Iteration 25000 (2.95054 iter/s, 33.8921s/100 iter), loss = 1.55955
I0815 10:57:09.536262  8764 solver.cpp:334]     Train net output #0: loss = 1.4423 (* 1 = 1.4423 loss)
I0815 10:57:09.536272  8764 sgd_solver.cpp:136] Iteration 25000, lr = 0.0084375, m = 0.9
I0815 10:57:24.376633  8764 solver.cpp:312] Iteration 25100 (6.73856 iter/s, 14.84s/100 iter), loss = 1.63713
I0815 10:57:24.376660  8764 solver.cpp:334]     Train net output #0: loss = 1.84988 (* 1 = 1.84988 loss)
I0815 10:57:24.376664  8764 sgd_solver.cpp:136] Iteration 25100, lr = 0.00843125, m = 0.9
I0815 10:57:38.670490  8764 solver.cpp:312] Iteration 25200 (6.99621 iter/s, 14.2935s/100 iter), loss = 1.41189
I0815 10:57:38.670642  8764 solver.cpp:334]     Train net output #0: loss = 1.50523 (* 1 = 1.50523 loss)
I0815 10:57:38.670661  8764 sgd_solver.cpp:136] Iteration 25200, lr = 0.008425, m = 0.9
I0815 10:57:53.120515  8764 solver.cpp:312] Iteration 25300 (6.9206 iter/s, 14.4496s/100 iter), loss = 1.90378
I0815 10:57:53.120581  8764 solver.cpp:334]     Train net output #0: loss = 1.95791 (* 1 = 1.95791 loss)
I0815 10:57:53.120587  8764 sgd_solver.cpp:136] Iteration 25300, lr = 0.00841875, m = 0.9
I0815 10:58:07.585923  8764 solver.cpp:312] Iteration 25400 (6.91324 iter/s, 14.465s/100 iter), loss = 1.58621
I0815 10:58:07.585988  8764 solver.cpp:334]     Train net output #0: loss = 1.55782 (* 1 = 1.55782 loss)
I0815 10:58:07.586007  8764 sgd_solver.cpp:136] Iteration 25400, lr = 0.0084125, m = 0.9
I0815 10:58:22.094128  8764 solver.cpp:312] Iteration 25500 (6.89284 iter/s, 14.5078s/100 iter), loss = 1.72325
I0815 10:58:22.094158  8764 solver.cpp:334]     Train net output #0: loss = 1.57287 (* 1 = 1.57287 loss)
I0815 10:58:22.094163  8764 sgd_solver.cpp:136] Iteration 25500, lr = 0.00840625, m = 0.9
I0815 10:58:36.995476  8764 solver.cpp:312] Iteration 25600 (6.71099 iter/s, 14.9009s/100 iter), loss = 1.63977
I0815 10:58:36.995568  8764 solver.cpp:334]     Train net output #0: loss = 1.48799 (* 1 = 1.48799 loss)
I0815 10:58:36.995586  8764 sgd_solver.cpp:136] Iteration 25600, lr = 0.0084, m = 0.9
I0815 10:58:51.653141  8764 solver.cpp:312] Iteration 25700 (6.82256 iter/s, 14.6573s/100 iter), loss = 1.50664
I0815 10:58:51.653188  8764 solver.cpp:334]     Train net output #0: loss = 1.16109 (* 1 = 1.16109 loss)
I0815 10:58:51.653201  8764 sgd_solver.cpp:136] Iteration 25700, lr = 0.00839375, m = 0.9
I0815 10:59:06.007891  8764 solver.cpp:312] Iteration 25800 (6.96653 iter/s, 14.3544s/100 iter), loss = 1.38817
I0815 10:59:06.008035  8764 solver.cpp:334]     Train net output #0: loss = 1.17633 (* 1 = 1.17633 loss)
I0815 10:59:06.008052  8764 sgd_solver.cpp:136] Iteration 25800, lr = 0.0083875, m = 0.9
I0815 10:59:20.672155  8764 solver.cpp:312] Iteration 25900 (6.81949 iter/s, 14.6639s/100 iter), loss = 1.67179
I0815 10:59:20.672209  8764 solver.cpp:334]     Train net output #0: loss = 2.23788 (* 1 = 2.23788 loss)
I0815 10:59:20.672214  8764 sgd_solver.cpp:136] Iteration 25900, lr = 0.00838125, m = 0.9
I0815 10:59:35.450229  8764 solver.cpp:363] Sparsity after update:
I0815 10:59:35.454587  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:59:35.460379  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 10:59:35.460500  8764 net.cpp:2192] conv1b_param_0(0.0278) 
I0815 10:59:35.460590  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:59:35.460677  8764 net.cpp:2192] res2a_branch2a_param_0(0.059) 
I0815 10:59:35.460770  8764 net.cpp:2192] res2a_branch2b_param_0(0.0556) 
I0815 10:59:35.461071  8764 net.cpp:2192] res3a_branch2a_param_0(0.059) 
I0815 10:59:35.461170  8764 net.cpp:2192] res3a_branch2b_param_0(0.059) 
I0815 10:59:35.461263  8764 net.cpp:2192] res4a_branch2a_param_0(0.0599) 
I0815 10:59:35.461364  8764 net.cpp:2192] res4a_branch2b_param_0(0.059) 
I0815 10:59:35.461454  8764 net.cpp:2192] res5a_branch2a_param_0(0.0599) 
I0815 10:59:35.461545  8764 net.cpp:2192] res5a_branch2b_param_0(0.0599) 
I0815 10:59:35.461634  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (140569/2.86678e+06) 0.049
I0815 10:59:35.461747  8764 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 10:59:56.352982  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547941
I0815 10:59:56.353091  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.786115
I0815 10:59:56.353102  8764 solver.cpp:594]     Test net output #2: loss = 2.0518 (* 1 = 2.0518 loss)
I0815 10:59:56.353121  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8908s
I0815 10:59:56.528174  8794 solver.cpp:409] Finding and applying sparsity: 0.07
I0815 11:00:15.796306  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:00:15.798300  8764 solver.cpp:312] Iteration 26000 (1.81407 iter/s, 55.1246s/100 iter), loss = 1.20655
I0815 11:00:15.798321  8764 solver.cpp:334]     Train net output #0: loss = 1.09252 (* 1 = 1.09252 loss)
I0815 11:00:15.798329  8764 sgd_solver.cpp:136] Iteration 26000, lr = 0.008375, m = 0.9
I0815 11:00:30.950533  8764 solver.cpp:312] Iteration 26100 (6.59987 iter/s, 15.1518s/100 iter), loss = 1.55173
I0815 11:00:30.950588  8764 solver.cpp:334]     Train net output #0: loss = 1.63927 (* 1 = 1.63927 loss)
I0815 11:00:30.950594  8764 sgd_solver.cpp:136] Iteration 26100, lr = 0.00836875, m = 0.9
I0815 11:00:45.405405  8764 solver.cpp:312] Iteration 26200 (6.91828 iter/s, 14.4545s/100 iter), loss = 1.48566
I0815 11:00:45.405627  8764 solver.cpp:334]     Train net output #0: loss = 1.84958 (* 1 = 1.84958 loss)
I0815 11:00:45.405737  8764 sgd_solver.cpp:136] Iteration 26200, lr = 0.0083625, m = 0.9
I0815 11:01:00.221204  8764 solver.cpp:312] Iteration 26300 (6.74974 iter/s, 14.8154s/100 iter), loss = 1.81562
I0815 11:01:00.221230  8764 solver.cpp:334]     Train net output #0: loss = 2.20279 (* 1 = 2.20279 loss)
I0815 11:01:00.221235  8764 sgd_solver.cpp:136] Iteration 26300, lr = 0.00835625, m = 0.9
I0815 11:01:14.674392  8764 solver.cpp:312] Iteration 26400 (6.91908 iter/s, 14.4528s/100 iter), loss = 1.33417
I0815 11:01:14.674453  8764 solver.cpp:334]     Train net output #0: loss = 1.06237 (* 1 = 1.06237 loss)
I0815 11:01:14.674458  8764 sgd_solver.cpp:136] Iteration 26400, lr = 0.00835, m = 0.9
I0815 11:01:29.357813  8764 solver.cpp:312] Iteration 26500 (6.81059 iter/s, 14.683s/100 iter), loss = 1.41424
I0815 11:01:29.357880  8764 solver.cpp:334]     Train net output #0: loss = 1.55952 (* 1 = 1.55952 loss)
I0815 11:01:29.357905  8764 sgd_solver.cpp:136] Iteration 26500, lr = 0.00834375, m = 0.9
I0815 11:01:43.896121  8764 solver.cpp:312] Iteration 26600 (6.87857 iter/s, 14.5379s/100 iter), loss = 1.37624
I0815 11:01:43.896193  8764 solver.cpp:334]     Train net output #0: loss = 1.20808 (* 1 = 1.20808 loss)
I0815 11:01:43.896209  8764 sgd_solver.cpp:136] Iteration 26600, lr = 0.0083375, m = 0.9
I0815 11:01:58.587353  8764 solver.cpp:312] Iteration 26700 (6.80697 iter/s, 14.6908s/100 iter), loss = 1.32699
I0815 11:01:58.587558  8764 solver.cpp:334]     Train net output #0: loss = 1.26393 (* 1 = 1.26393 loss)
I0815 11:01:58.587565  8764 sgd_solver.cpp:136] Iteration 26700, lr = 0.00833125, m = 0.9
I0815 11:02:13.227515  8764 solver.cpp:312] Iteration 26800 (6.83071 iter/s, 14.6398s/100 iter), loss = 1.55041
I0815 11:02:13.227572  8764 solver.cpp:334]     Train net output #0: loss = 1.44544 (* 1 = 1.44544 loss)
I0815 11:02:13.227584  8764 sgd_solver.cpp:136] Iteration 26800, lr = 0.008325, m = 0.9
I0815 11:02:27.887164  8764 solver.cpp:312] Iteration 26900 (6.82164 iter/s, 14.6592s/100 iter), loss = 1.54581
I0815 11:02:27.887189  8764 solver.cpp:334]     Train net output #0: loss = 1.52541 (* 1 = 1.52541 loss)
I0815 11:02:27.887193  8764 sgd_solver.cpp:136] Iteration 26900, lr = 0.00831875, m = 0.9
I0815 11:02:42.205195  8764 solver.cpp:363] Sparsity after update:
I0815 11:02:42.220921  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:02:42.220935  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 11:02:42.220944  8764 net.cpp:2192] conv1b_param_0(0.0347) 
I0815 11:02:42.220947  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:02:42.220953  8764 net.cpp:2192] res2a_branch2a_param_0(0.0694) 
I0815 11:02:42.220957  8764 net.cpp:2192] res2a_branch2b_param_0(0.0694) 
I0815 11:02:42.220959  8764 net.cpp:2192] res3a_branch2a_param_0(0.0694) 
I0815 11:02:42.220963  8764 net.cpp:2192] res3a_branch2b_param_0(0.0694) 
I0815 11:02:42.220965  8764 net.cpp:2192] res4a_branch2a_param_0(0.0694) 
I0815 11:02:42.220969  8764 net.cpp:2192] res4a_branch2b_param_0(0.0694) 
I0815 11:02:42.220973  8764 net.cpp:2192] res5a_branch2a_param_0(0.0699) 
I0815 11:02:42.220975  8764 net.cpp:2192] res5a_branch2b_param_0(0.0694) 
I0815 11:02:42.220978  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (163810/2.86678e+06) 0.0571
I0815 11:02:42.360739  8794 solver.cpp:409] Finding and applying sparsity: 0.08
I0815 11:03:01.619261  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:03:01.621296  8764 solver.cpp:312] Iteration 27000 (2.96444 iter/s, 33.7332s/100 iter), loss = 1.27561
I0815 11:03:01.621320  8764 solver.cpp:334]     Train net output #0: loss = 1.21934 (* 1 = 1.21934 loss)
I0815 11:03:01.621326  8764 sgd_solver.cpp:136] Iteration 27000, lr = 0.0083125, m = 0.9
I0815 11:03:16.509910  8764 solver.cpp:312] Iteration 27100 (6.71673 iter/s, 14.8882s/100 iter), loss = 1.41786
I0815 11:03:16.512210  8764 solver.cpp:334]     Train net output #0: loss = 1.6917 (* 1 = 1.6917 loss)
I0815 11:03:16.512225  8764 sgd_solver.cpp:136] Iteration 27100, lr = 0.00830625, m = 0.9
I0815 11:03:30.781731  8764 solver.cpp:312] Iteration 27200 (7.00701 iter/s, 14.2714s/100 iter), loss = 1.44426
I0815 11:03:30.781802  8764 solver.cpp:334]     Train net output #0: loss = 1.45973 (* 1 = 1.45973 loss)
I0815 11:03:30.781822  8764 sgd_solver.cpp:136] Iteration 27200, lr = 0.0083, m = 0.9
I0815 11:03:45.766923  8764 solver.cpp:312] Iteration 27300 (6.67344 iter/s, 14.9848s/100 iter), loss = 1.68348
I0815 11:03:45.766997  8764 solver.cpp:334]     Train net output #0: loss = 1.87922 (* 1 = 1.87922 loss)
I0815 11:03:45.767016  8764 sgd_solver.cpp:136] Iteration 27300, lr = 0.00829375, m = 0.9
I0815 11:03:59.921541  8764 solver.cpp:312] Iteration 27400 (7.06503 iter/s, 14.1542s/100 iter), loss = 1.0589
I0815 11:03:59.921638  8764 solver.cpp:334]     Train net output #0: loss = 0.918475 (* 1 = 0.918475 loss)
I0815 11:03:59.921654  8764 sgd_solver.cpp:136] Iteration 27400, lr = 0.0082875, m = 0.9
I0815 11:04:14.227741  8764 solver.cpp:312] Iteration 27500 (6.99017 iter/s, 14.3058s/100 iter), loss = 1.22823
I0815 11:04:14.227767  8764 solver.cpp:334]     Train net output #0: loss = 1.34658 (* 1 = 1.34658 loss)
I0815 11:04:14.227774  8764 sgd_solver.cpp:136] Iteration 27500, lr = 0.00828125, m = 0.9
I0815 11:04:28.500653  8764 solver.cpp:312] Iteration 27600 (7.00648 iter/s, 14.2725s/100 iter), loss = 1.57171
I0815 11:04:28.500679  8764 solver.cpp:334]     Train net output #0: loss = 1.78909 (* 1 = 1.78909 loss)
I0815 11:04:28.500684  8764 sgd_solver.cpp:136] Iteration 27600, lr = 0.008275, m = 0.9
I0815 11:04:43.009814  8764 solver.cpp:312] Iteration 27700 (6.89239 iter/s, 14.5088s/100 iter), loss = 1.37735
I0815 11:04:43.009912  8764 solver.cpp:334]     Train net output #0: loss = 1.13237 (* 1 = 1.13237 loss)
I0815 11:04:43.009932  8764 sgd_solver.cpp:136] Iteration 27700, lr = 0.00826875, m = 0.9
I0815 11:04:57.670859  8764 solver.cpp:312] Iteration 27800 (6.82099 iter/s, 14.6606s/100 iter), loss = 1.35543
I0815 11:04:57.670887  8764 solver.cpp:334]     Train net output #0: loss = 1.58264 (* 1 = 1.58264 loss)
I0815 11:04:57.670893  8764 sgd_solver.cpp:136] Iteration 27800, lr = 0.0082625, m = 0.9
I0815 11:05:12.274839  8764 solver.cpp:312] Iteration 27900 (6.84764 iter/s, 14.6036s/100 iter), loss = 1.18147
I0815 11:05:12.274914  8764 solver.cpp:334]     Train net output #0: loss = 0.997273 (* 1 = 0.997273 loss)
I0815 11:05:12.274942  8764 sgd_solver.cpp:136] Iteration 27900, lr = 0.00825625, m = 0.9
I0815 11:05:26.634584  8764 solver.cpp:363] Sparsity after update:
I0815 11:05:26.640009  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:05:26.640020  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 11:05:26.640028  8764 net.cpp:2192] conv1b_param_0(0.0347) 
I0815 11:05:26.640041  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:05:26.640053  8764 net.cpp:2192] res2a_branch2a_param_0(0.0799) 
I0815 11:05:26.640058  8764 net.cpp:2192] res2a_branch2b_param_0(0.0764) 
I0815 11:05:26.640063  8764 net.cpp:2192] res3a_branch2a_param_0(0.0799) 
I0815 11:05:26.640070  8764 net.cpp:2192] res3a_branch2b_param_0(0.0798) 
I0815 11:05:26.640075  8764 net.cpp:2192] res4a_branch2a_param_0(0.0799) 
I0815 11:05:26.640079  8764 net.cpp:2192] res4a_branch2b_param_0(0.0799) 
I0815 11:05:26.640086  8764 net.cpp:2192] res5a_branch2a_param_0(0.0799) 
I0815 11:05:26.640090  8764 net.cpp:2192] res5a_branch2b_param_0(0.0798) 
I0815 11:05:26.640094  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (187748/2.86678e+06) 0.0655
I0815 11:05:26.640106  8764 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 11:05:47.695675  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549177
I0815 11:05:47.695699  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791644
I0815 11:05:47.695704  8764 solver.cpp:594]     Test net output #2: loss = 1.99878 (* 1 = 1.99878 loss)
I0815 11:05:47.695772  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0551s
I0815 11:05:47.867705  8794 solver.cpp:409] Finding and applying sparsity: 0.09
I0815 11:06:07.140550  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:06:07.142609  8764 solver.cpp:312] Iteration 28000 (1.82261 iter/s, 54.8663s/100 iter), loss = 1.45269
I0815 11:06:07.142632  8764 solver.cpp:334]     Train net output #0: loss = 1.3289 (* 1 = 1.3289 loss)
I0815 11:06:07.142642  8764 sgd_solver.cpp:136] Iteration 28000, lr = 0.00825, m = 0.9
I0815 11:06:21.876559  8764 solver.cpp:312] Iteration 28100 (6.78724 iter/s, 14.7335s/100 iter), loss = 1.51348
I0815 11:06:21.876622  8764 solver.cpp:334]     Train net output #0: loss = 1.25018 (* 1 = 1.25018 loss)
I0815 11:06:21.876641  8764 sgd_solver.cpp:136] Iteration 28100, lr = 0.00824375, m = 0.9
I0815 11:06:36.373471  8764 solver.cpp:312] Iteration 28200 (6.89821 iter/s, 14.4965s/100 iter), loss = 1.40581
I0815 11:06:36.373536  8764 solver.cpp:334]     Train net output #0: loss = 1.70909 (* 1 = 1.70909 loss)
I0815 11:06:36.373561  8764 sgd_solver.cpp:136] Iteration 28200, lr = 0.0082375, m = 0.9
I0815 11:06:50.755679  8764 solver.cpp:312] Iteration 28300 (6.95323 iter/s, 14.3818s/100 iter), loss = 1.24752
I0815 11:06:50.755779  8764 solver.cpp:334]     Train net output #0: loss = 0.946841 (* 1 = 0.946841 loss)
I0815 11:06:50.755796  8764 sgd_solver.cpp:136] Iteration 28300, lr = 0.00823125, m = 0.9
I0815 11:07:05.520364  8764 solver.cpp:312] Iteration 28400 (6.77311 iter/s, 14.7643s/100 iter), loss = 1.32987
I0815 11:07:05.520437  8764 solver.cpp:334]     Train net output #0: loss = 1.1723 (* 1 = 1.1723 loss)
I0815 11:07:05.520455  8764 sgd_solver.cpp:136] Iteration 28400, lr = 0.008225, m = 0.9
I0815 11:07:20.037627  8764 solver.cpp:312] Iteration 28500 (6.88854 iter/s, 14.5169s/100 iter), loss = 1.53403
I0815 11:07:20.037695  8764 solver.cpp:334]     Train net output #0: loss = 1.18039 (* 1 = 1.18039 loss)
I0815 11:07:20.037714  8764 sgd_solver.cpp:136] Iteration 28500, lr = 0.00821875, m = 0.9
I0815 11:07:34.903842  8764 solver.cpp:312] Iteration 28600 (6.72685 iter/s, 14.8658s/100 iter), loss = 1.46091
I0815 11:07:34.903904  8764 solver.cpp:334]     Train net output #0: loss = 1.26492 (* 1 = 1.26492 loss)
I0815 11:07:34.903911  8764 sgd_solver.cpp:136] Iteration 28600, lr = 0.0082125, m = 0.9
I0815 11:07:49.626127  8764 solver.cpp:312] Iteration 28700 (6.79261 iter/s, 14.7219s/100 iter), loss = 1.62631
I0815 11:07:49.626157  8764 solver.cpp:334]     Train net output #0: loss = 1.60162 (* 1 = 1.60162 loss)
I0815 11:07:49.626163  8764 sgd_solver.cpp:136] Iteration 28700, lr = 0.00820625, m = 0.9
I0815 11:08:05.696491  8764 solver.cpp:312] Iteration 28800 (6.22282 iter/s, 16.0699s/100 iter), loss = 1.48598
I0815 11:08:05.696847  8764 solver.cpp:334]     Train net output #0: loss = 1.60172 (* 1 = 1.60172 loss)
I0815 11:08:05.696970  8764 sgd_solver.cpp:136] Iteration 28800, lr = 0.0082, m = 0.9
I0815 11:08:20.970950  8764 solver.cpp:312] Iteration 28900 (6.54706 iter/s, 15.274s/100 iter), loss = 1.16453
I0815 11:08:20.971004  8764 solver.cpp:334]     Train net output #0: loss = 1.09054 (* 1 = 1.09054 loss)
I0815 11:08:20.971016  8764 sgd_solver.cpp:136] Iteration 28900, lr = 0.00819375, m = 0.9
I0815 11:08:35.569126  8764 solver.cpp:363] Sparsity after update:
I0815 11:08:35.582509  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:08:35.582551  8764 net.cpp:2192] conv1a_param_0(0.04) 
I0815 11:08:35.582571  8764 net.cpp:2192] conv1b_param_0(0.0417) 
I0815 11:08:35.582584  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:08:35.582597  8764 net.cpp:2192] res2a_branch2a_param_0(0.0868) 
I0815 11:08:35.582610  8764 net.cpp:2192] res2a_branch2b_param_0(0.0833) 
I0815 11:08:35.582623  8764 net.cpp:2192] res3a_branch2a_param_0(0.0885) 
I0815 11:08:35.582635  8764 net.cpp:2192] res3a_branch2b_param_0(0.0868) 
I0815 11:08:35.582648  8764 net.cpp:2192] res4a_branch2a_param_0(0.0894) 
I0815 11:08:35.582660  8764 net.cpp:2192] res4a_branch2b_param_0(0.0885) 
I0815 11:08:35.582672  8764 net.cpp:2192] res5a_branch2a_param_0(0.0898) 
I0815 11:08:35.582684  8764 net.cpp:2192] res5a_branch2b_param_0(0.0893) 
I0815 11:08:35.582697  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (210376/2.86678e+06) 0.0734
I0815 11:08:35.712347  8794 solver.cpp:409] Finding and applying sparsity: 0.1
I0815 11:08:55.406098  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:08:55.408097  8764 solver.cpp:312] Iteration 29000 (2.90392 iter/s, 34.4362s/100 iter), loss = 1.08967
I0815 11:08:55.408119  8764 solver.cpp:334]     Train net output #0: loss = 0.927995 (* 1 = 0.927995 loss)
I0815 11:08:55.408128  8764 sgd_solver.cpp:136] Iteration 29000, lr = 0.0081875, m = 0.9
I0815 11:09:10.263370  8764 solver.cpp:312] Iteration 29100 (6.73181 iter/s, 14.8549s/100 iter), loss = 1.32076
I0815 11:09:10.263464  8764 solver.cpp:334]     Train net output #0: loss = 1.10945 (* 1 = 1.10945 loss)
I0815 11:09:10.263481  8764 sgd_solver.cpp:136] Iteration 29100, lr = 0.00818125, m = 0.9
I0815 11:09:25.449616  8764 solver.cpp:312] Iteration 29200 (6.58509 iter/s, 15.1858s/100 iter), loss = 1.41317
I0815 11:09:25.449643  8764 solver.cpp:334]     Train net output #0: loss = 0.977146 (* 1 = 0.977146 loss)
I0815 11:09:25.449650  8764 sgd_solver.cpp:136] Iteration 29200, lr = 0.008175, m = 0.9
I0815 11:09:40.511469  8764 solver.cpp:312] Iteration 29300 (6.63947 iter/s, 15.0614s/100 iter), loss = 1.9192
I0815 11:09:40.511687  8764 solver.cpp:334]     Train net output #0: loss = 1.51714 (* 1 = 1.51714 loss)
I0815 11:09:40.511693  8764 sgd_solver.cpp:136] Iteration 29300, lr = 0.00816875, m = 0.9
I0815 11:09:55.044852  8764 solver.cpp:312] Iteration 29400 (6.8809 iter/s, 14.533s/100 iter), loss = 1.45987
I0815 11:09:55.044884  8764 solver.cpp:334]     Train net output #0: loss = 1.27953 (* 1 = 1.27953 loss)
I0815 11:09:55.044890  8764 sgd_solver.cpp:136] Iteration 29400, lr = 0.0081625, m = 0.9
I0815 11:10:09.582815  8764 solver.cpp:312] Iteration 29500 (6.87873 iter/s, 14.5376s/100 iter), loss = 1.07128
I0815 11:10:09.583035  8764 solver.cpp:334]     Train net output #0: loss = 1.04673 (* 1 = 1.04673 loss)
I0815 11:10:09.583145  8764 sgd_solver.cpp:136] Iteration 29500, lr = 0.00815625, m = 0.9
I0815 11:10:24.397931  8764 solver.cpp:312] Iteration 29600 (6.75005 iter/s, 14.8147s/100 iter), loss = 1.28777
I0815 11:10:24.398026  8764 solver.cpp:334]     Train net output #0: loss = 1.36537 (* 1 = 1.36537 loss)
I0815 11:10:24.398041  8764 sgd_solver.cpp:136] Iteration 29600, lr = 0.00815, m = 0.9
I0815 11:10:40.151679  8764 solver.cpp:312] Iteration 29700 (6.34787 iter/s, 15.7533s/100 iter), loss = 1.31191
I0815 11:10:40.151752  8764 solver.cpp:334]     Train net output #0: loss = 0.798737 (* 1 = 0.798737 loss)
I0815 11:10:40.151772  8764 sgd_solver.cpp:136] Iteration 29700, lr = 0.00814375, m = 0.9
I0815 11:10:54.784852  8764 solver.cpp:312] Iteration 29800 (6.83398 iter/s, 14.6328s/100 iter), loss = 1.22954
I0815 11:10:54.785019  8764 solver.cpp:334]     Train net output #0: loss = 1.32441 (* 1 = 1.32441 loss)
I0815 11:10:54.785037  8764 sgd_solver.cpp:136] Iteration 29800, lr = 0.0081375, m = 0.9
I0815 11:11:09.551268  8764 solver.cpp:312] Iteration 29900 (6.77231 iter/s, 14.766s/100 iter), loss = 1.20236
I0815 11:11:09.551293  8764 solver.cpp:334]     Train net output #0: loss = 1.31346 (* 1 = 1.31346 loss)
I0815 11:11:09.551300  8764 sgd_solver.cpp:136] Iteration 29900, lr = 0.00813125, m = 0.9
I0815 11:11:23.944622  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_30000.caffemodel
I0815 11:11:23.971768  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_30000.solverstate
I0815 11:11:23.977174  8764 solver.cpp:363] Sparsity after update:
I0815 11:11:23.978258  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:11:23.978267  8764 net.cpp:2192] conv1a_param_0(0.04) 
I0815 11:11:23.978276  8764 net.cpp:2192] conv1b_param_0(0.0486) 
I0815 11:11:23.978281  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:11:23.978284  8764 net.cpp:2192] res2a_branch2a_param_0(0.0972) 
I0815 11:11:23.978291  8764 net.cpp:2192] res2a_branch2b_param_0(0.0972) 
I0815 11:11:23.978296  8764 net.cpp:2192] res3a_branch2a_param_0(0.099) 
I0815 11:11:23.978302  8764 net.cpp:2192] res3a_branch2b_param_0(0.0972) 
I0815 11:11:23.978307  8764 net.cpp:2192] res4a_branch2a_param_0(0.0998) 
I0815 11:11:23.978309  8764 net.cpp:2192] res4a_branch2b_param_0(0.099) 
I0815 11:11:23.978314  8764 net.cpp:2192] res5a_branch2a_param_0(0.0998) 
I0815 11:11:23.978317  8764 net.cpp:2192] res5a_branch2b_param_0(0.0998) 
I0815 11:11:23.978322  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (234405/2.86678e+06) 0.0818
I0815 11:11:23.978332  8764 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 11:11:33.009713  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:11:38.409981  8747 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 11:11:45.811498  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.545176
I0815 11:11:45.811520  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.785115
I0815 11:11:45.811528  8764 solver.cpp:594]     Test net output #2: loss = 2.02438 (* 1 = 2.02438 loss)
I0815 11:11:45.811550  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.8326s
I0815 11:11:45.975399  8794 solver.cpp:409] Finding and applying sparsity: 0.11
I0815 11:12:05.725106  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:12:05.727202  8764 solver.cpp:312] Iteration 30000 (1.78017 iter/s, 56.1744s/100 iter), loss = 1.5899
I0815 11:12:05.727221  8764 solver.cpp:334]     Train net output #0: loss = 1.76908 (* 1 = 1.76908 loss)
I0815 11:12:05.727229  8764 sgd_solver.cpp:136] Iteration 30000, lr = 0.008125, m = 0.9
I0815 11:12:20.711761  8764 solver.cpp:312] Iteration 30100 (6.67373 iter/s, 14.9841s/100 iter), loss = 1.2876
I0815 11:12:20.711788  8764 solver.cpp:334]     Train net output #0: loss = 1.54309 (* 1 = 1.54309 loss)
I0815 11:12:20.711794  8764 sgd_solver.cpp:136] Iteration 30100, lr = 0.00811875, m = 0.9
I0815 11:12:35.447245  8764 solver.cpp:312] Iteration 30200 (6.78653 iter/s, 14.7351s/100 iter), loss = 1.61887
I0815 11:12:35.447312  8764 solver.cpp:334]     Train net output #0: loss = 1.47352 (* 1 = 1.47352 loss)
I0815 11:12:35.447329  8764 sgd_solver.cpp:136] Iteration 30200, lr = 0.0081125, m = 0.9
I0815 11:12:51.923940  8764 solver.cpp:312] Iteration 30300 (6.06936 iter/s, 16.4762s/100 iter), loss = 1.4051
I0815 11:12:51.924093  8764 solver.cpp:334]     Train net output #0: loss = 1.42913 (* 1 = 1.42913 loss)
I0815 11:12:51.924120  8764 sgd_solver.cpp:136] Iteration 30300, lr = 0.00810625, m = 0.9
I0815 11:13:09.554170  8764 solver.cpp:312] Iteration 30400 (5.67223 iter/s, 17.6297s/100 iter), loss = 1.70585
I0815 11:13:09.554196  8764 solver.cpp:334]     Train net output #0: loss = 1.48485 (* 1 = 1.48485 loss)
I0815 11:13:09.554200  8764 sgd_solver.cpp:136] Iteration 30400, lr = 0.0081, m = 0.9
I0815 11:13:25.444195  8764 solver.cpp:312] Iteration 30500 (6.29343 iter/s, 15.8896s/100 iter), loss = 1.36114
I0815 11:13:25.444272  8764 solver.cpp:334]     Train net output #0: loss = 1.19018 (* 1 = 1.19018 loss)
I0815 11:13:25.444284  8764 sgd_solver.cpp:136] Iteration 30500, lr = 0.00809375, m = 0.9
I0815 11:13:40.242146  8764 solver.cpp:312] Iteration 30600 (6.75788 iter/s, 14.7975s/100 iter), loss = 1.82426
I0815 11:13:40.242172  8764 solver.cpp:334]     Train net output #0: loss = 1.71371 (* 1 = 1.71371 loss)
I0815 11:13:40.242177  8764 sgd_solver.cpp:136] Iteration 30600, lr = 0.0080875, m = 0.9
I0815 11:13:54.712312  8764 solver.cpp:312] Iteration 30700 (6.91096 iter/s, 14.4698s/100 iter), loss = 1.48349
I0815 11:13:54.712339  8764 solver.cpp:334]     Train net output #0: loss = 1.59649 (* 1 = 1.59649 loss)
I0815 11:13:54.712343  8764 sgd_solver.cpp:136] Iteration 30700, lr = 0.00808125, m = 0.9
I0815 11:14:09.810112  8764 solver.cpp:312] Iteration 30800 (6.62367 iter/s, 15.0974s/100 iter), loss = 1.77905
I0815 11:14:09.810174  8764 solver.cpp:334]     Train net output #0: loss = 1.88324 (* 1 = 1.88324 loss)
I0815 11:14:09.810180  8764 sgd_solver.cpp:136] Iteration 30800, lr = 0.008075, m = 0.9
I0815 11:14:25.199748  8764 solver.cpp:312] Iteration 30900 (6.49806 iter/s, 15.3892s/100 iter), loss = 1.54822
I0815 11:14:25.199931  8764 solver.cpp:334]     Train net output #0: loss = 1.51528 (* 1 = 1.51528 loss)
I0815 11:14:25.200021  8764 sgd_solver.cpp:136] Iteration 30900, lr = 0.00806875, m = 0.9
I0815 11:14:41.291348  8764 solver.cpp:363] Sparsity after update:
I0815 11:14:41.302917  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:14:41.303059  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:14:41.303153  8764 net.cpp:2192] conv1b_param_0(0.0486) 
I0815 11:14:41.303241  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:14:41.303330  8764 net.cpp:2192] res2a_branch2a_param_0(0.108) 
I0815 11:14:41.303422  8764 net.cpp:2192] res2a_branch2b_param_0(0.104) 
I0815 11:14:41.303516  8764 net.cpp:2192] res3a_branch2a_param_0(0.109) 
I0815 11:14:41.303611  8764 net.cpp:2192] res3a_branch2b_param_0(0.108) 
I0815 11:14:41.303700  8764 net.cpp:2192] res4a_branch2a_param_0(0.109) 
I0815 11:14:41.303791  8764 net.cpp:2192] res4a_branch2b_param_0(0.109) 
I0815 11:14:41.303879  8764 net.cpp:2192] res5a_branch2a_param_0(0.11) 
I0815 11:14:41.303967  8764 net.cpp:2192] res5a_branch2b_param_0(0.109) 
I0815 11:14:41.304055  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (257610/2.86678e+06) 0.0899
I0815 11:14:41.462749  8794 solver.cpp:409] Finding and applying sparsity: 0.12
I0815 11:15:02.577852  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:15:02.579936  8764 solver.cpp:312] Iteration 31000 (2.67529 iter/s, 37.3792s/100 iter), loss = 1.56124
I0815 11:15:02.579953  8764 solver.cpp:334]     Train net output #0: loss = 1.43548 (* 1 = 1.43548 loss)
I0815 11:15:02.579958  8764 sgd_solver.cpp:136] Iteration 31000, lr = 0.0080625, m = 0.9
I0815 11:15:18.166894  8764 solver.cpp:312] Iteration 31100 (6.4158 iter/s, 15.5865s/100 iter), loss = 1.17394
I0815 11:15:18.166965  8764 solver.cpp:334]     Train net output #0: loss = 1.12161 (* 1 = 1.12161 loss)
I0815 11:15:18.166972  8764 sgd_solver.cpp:136] Iteration 31100, lr = 0.00805625, m = 0.9
I0815 11:15:34.653153  8764 solver.cpp:312] Iteration 31200 (6.06583 iter/s, 16.4858s/100 iter), loss = 1.52235
I0815 11:15:34.653329  8764 solver.cpp:334]     Train net output #0: loss = 1.31434 (* 1 = 1.31434 loss)
I0815 11:15:34.653417  8764 sgd_solver.cpp:136] Iteration 31200, lr = 0.00805, m = 0.9
I0815 11:15:50.398641  8764 solver.cpp:312] Iteration 31300 (6.3512 iter/s, 15.745s/100 iter), loss = 1.76115
I0815 11:15:50.398721  8764 solver.cpp:334]     Train net output #0: loss = 1.87971 (* 1 = 1.87971 loss)
I0815 11:15:50.398728  8764 sgd_solver.cpp:136] Iteration 31300, lr = 0.00804375, m = 0.9
I0815 11:16:05.529326  8764 solver.cpp:312] Iteration 31400 (6.60927 iter/s, 15.1303s/100 iter), loss = 1.21216
I0815 11:16:05.529353  8764 solver.cpp:334]     Train net output #0: loss = 1.0918 (* 1 = 1.0918 loss)
I0815 11:16:05.529359  8764 sgd_solver.cpp:136] Iteration 31400, lr = 0.0080375, m = 0.9
I0815 11:16:22.730139  8764 solver.cpp:312] Iteration 31500 (5.81384 iter/s, 17.2003s/100 iter), loss = 1.62102
I0815 11:16:22.730232  8764 solver.cpp:334]     Train net output #0: loss = 1.52572 (* 1 = 1.52572 loss)
I0815 11:16:22.730250  8764 sgd_solver.cpp:136] Iteration 31500, lr = 0.00803125, m = 0.9
I0815 11:16:37.267717  8764 solver.cpp:312] Iteration 31600 (6.87892 iter/s, 14.5372s/100 iter), loss = 0.999727
I0815 11:16:37.267791  8764 solver.cpp:334]     Train net output #0: loss = 1.12834 (* 1 = 1.12834 loss)
I0815 11:16:37.267810  8764 sgd_solver.cpp:136] Iteration 31600, lr = 0.008025, m = 0.9
I0815 11:16:51.829358  8764 solver.cpp:312] Iteration 31700 (6.86755 iter/s, 14.5612s/100 iter), loss = 1.52216
I0815 11:16:51.829385  8764 solver.cpp:334]     Train net output #0: loss = 1.38804 (* 1 = 1.38804 loss)
I0815 11:16:51.829390  8764 sgd_solver.cpp:136] Iteration 31700, lr = 0.00801875, m = 0.9
I0815 11:17:07.508738  8764 solver.cpp:312] Iteration 31800 (6.37798 iter/s, 15.6789s/100 iter), loss = 1.3094
I0815 11:17:07.508941  8764 solver.cpp:334]     Train net output #0: loss = 1.31021 (* 1 = 1.31021 loss)
I0815 11:17:07.508958  8764 sgd_solver.cpp:136] Iteration 31800, lr = 0.0080125, m = 0.9
I0815 11:17:23.549547  8764 solver.cpp:312] Iteration 31900 (6.23427 iter/s, 16.0404s/100 iter), loss = 1.54923
I0815 11:17:23.549619  8764 solver.cpp:334]     Train net output #0: loss = 1.49676 (* 1 = 1.49676 loss)
I0815 11:17:23.549638  8764 sgd_solver.cpp:136] Iteration 31900, lr = 0.00800625, m = 0.9
I0815 11:17:40.074251  8764 solver.cpp:363] Sparsity after update:
I0815 11:17:40.078821  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:17:40.078832  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:17:40.078840  8764 net.cpp:2192] conv1b_param_0(0.0556) 
I0815 11:17:40.078843  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:17:40.078850  8764 net.cpp:2192] res2a_branch2a_param_0(0.118) 
I0815 11:17:40.078853  8764 net.cpp:2192] res2a_branch2b_param_0(0.118) 
I0815 11:17:40.078856  8764 net.cpp:2192] res3a_branch2a_param_0(0.12) 
I0815 11:17:40.078860  8764 net.cpp:2192] res3a_branch2b_param_0(0.118) 
I0815 11:17:40.078863  8764 net.cpp:2192] res4a_branch2a_param_0(0.12) 
I0815 11:17:40.078867  8764 net.cpp:2192] res4a_branch2b_param_0(0.12) 
I0815 11:17:40.078871  8764 net.cpp:2192] res5a_branch2a_param_0(0.12) 
I0815 11:17:40.078874  8764 net.cpp:2192] res5a_branch2b_param_0(0.12) 
I0815 11:17:40.078877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (281617/2.86678e+06) 0.0982
I0815 11:17:40.078889  8764 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 11:18:05.524494  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547882
I0815 11:18:05.524520  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.789822
I0815 11:18:05.524528  8764 solver.cpp:594]     Test net output #2: loss = 2.00044 (* 1 = 2.00044 loss)
I0815 11:18:05.524549  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.445s
I0815 11:18:05.672992  8794 solver.cpp:409] Finding and applying sparsity: 0.13
I0815 11:18:25.010236  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:18:25.012337  8764 solver.cpp:312] Iteration 32000 (1.62705 iter/s, 61.4611s/100 iter), loss = 1.4308
I0815 11:18:25.012356  8764 solver.cpp:334]     Train net output #0: loss = 1.13015 (* 1 = 1.13015 loss)
I0815 11:18:25.012362  8764 sgd_solver.cpp:136] Iteration 32000, lr = 0.008, m = 0.9
I0815 11:18:41.392751  8764 solver.cpp:312] Iteration 32100 (6.10503 iter/s, 16.3799s/100 iter), loss = 1.32199
I0815 11:18:41.392779  8764 solver.cpp:334]     Train net output #0: loss = 1.40445 (* 1 = 1.40445 loss)
I0815 11:18:41.392786  8764 sgd_solver.cpp:136] Iteration 32100, lr = 0.00799375, m = 0.9
I0815 11:18:55.968163  8764 solver.cpp:312] Iteration 32200 (6.86106 iter/s, 14.575s/100 iter), loss = 1.65492
I0815 11:18:55.968243  8764 solver.cpp:334]     Train net output #0: loss = 1.71042 (* 1 = 1.71042 loss)
I0815 11:18:55.968250  8764 sgd_solver.cpp:136] Iteration 32200, lr = 0.0079875, m = 0.9
I0815 11:19:11.882033  8764 solver.cpp:312] Iteration 32300 (6.28401 iter/s, 15.9134s/100 iter), loss = 1.4625
I0815 11:19:11.882092  8764 solver.cpp:334]     Train net output #0: loss = 1.22742 (* 1 = 1.22742 loss)
I0815 11:19:11.882107  8764 sgd_solver.cpp:136] Iteration 32300, lr = 0.00798125, m = 0.9
I0815 11:19:26.753446  8764 solver.cpp:312] Iteration 32400 (6.7245 iter/s, 14.871s/100 iter), loss = 1.4643
I0815 11:19:26.753516  8764 solver.cpp:334]     Train net output #0: loss = 1.35998 (* 1 = 1.35998 loss)
I0815 11:19:26.753525  8764 sgd_solver.cpp:136] Iteration 32400, lr = 0.007975, m = 0.9
I0815 11:19:41.259615  8764 solver.cpp:312] Iteration 32500 (6.89381 iter/s, 14.5058s/100 iter), loss = 1.36085
I0815 11:19:41.259645  8764 solver.cpp:334]     Train net output #0: loss = 1.44033 (* 1 = 1.44033 loss)
I0815 11:19:41.259654  8764 sgd_solver.cpp:136] Iteration 32500, lr = 0.00796875, m = 0.9
I0815 11:19:56.219529  8764 solver.cpp:312] Iteration 32600 (6.68472 iter/s, 14.9595s/100 iter), loss = 1.66874
I0815 11:19:56.219552  8764 solver.cpp:334]     Train net output #0: loss = 2.0117 (* 1 = 2.0117 loss)
I0815 11:19:56.219555  8764 sgd_solver.cpp:136] Iteration 32600, lr = 0.0079625, m = 0.9
I0815 11:20:13.282013  8764 solver.cpp:312] Iteration 32700 (5.86097 iter/s, 17.062s/100 iter), loss = 1.20602
I0815 11:20:13.282099  8764 solver.cpp:334]     Train net output #0: loss = 1.33977 (* 1 = 1.33977 loss)
I0815 11:20:13.282112  8764 sgd_solver.cpp:136] Iteration 32700, lr = 0.00795625, m = 0.9
I0815 11:20:30.988759  8764 solver.cpp:312] Iteration 32800 (5.64772 iter/s, 17.7063s/100 iter), loss = 1.21403
I0815 11:20:30.988903  8764 solver.cpp:334]     Train net output #0: loss = 1.16877 (* 1 = 1.16877 loss)
I0815 11:20:30.988920  8764 sgd_solver.cpp:136] Iteration 32800, lr = 0.00795, m = 0.9
I0815 11:20:46.554407  8764 solver.cpp:312] Iteration 32900 (6.42458 iter/s, 15.5652s/100 iter), loss = 1.80834
I0815 11:20:46.554471  8764 solver.cpp:334]     Train net output #0: loss = 1.93065 (* 1 = 1.93065 loss)
I0815 11:20:46.554478  8764 sgd_solver.cpp:136] Iteration 32900, lr = 0.00794375, m = 0.9
I0815 11:21:01.552922  8764 solver.cpp:363] Sparsity after update:
I0815 11:21:01.566452  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:21:01.566483  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:21:01.566496  8764 net.cpp:2192] conv1b_param_0(0.0938) 
I0815 11:21:01.566506  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:21:01.566515  8764 net.cpp:2192] res2a_branch2a_param_0(0.128) 
I0815 11:21:01.566524  8764 net.cpp:2192] res2a_branch2b_param_0(0.125) 
I0815 11:21:01.566532  8764 net.cpp:2192] res3a_branch2a_param_0(0.128) 
I0815 11:21:01.566540  8764 net.cpp:2192] res3a_branch2b_param_0(0.128) 
I0815 11:21:01.566550  8764 net.cpp:2192] res4a_branch2a_param_0(0.129) 
I0815 11:21:01.566560  8764 net.cpp:2192] res4a_branch2b_param_0(0.128) 
I0815 11:21:01.566568  8764 net.cpp:2192] res5a_branch2a_param_0(0.13) 
I0815 11:21:01.566577  8764 net.cpp:2192] res5a_branch2b_param_0(0.129) 
I0815 11:21:01.566587  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (304492/2.86678e+06) 0.106
I0815 11:21:01.705672  8794 solver.cpp:409] Finding and applying sparsity: 0.14
I0815 11:21:21.776078  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:21:21.778131  8764 solver.cpp:312] Iteration 33000 (2.83907 iter/s, 35.2228s/100 iter), loss = 1.53488
I0815 11:21:21.778153  8764 solver.cpp:334]     Train net output #0: loss = 1.46084 (* 1 = 1.46084 loss)
I0815 11:21:21.778161  8764 sgd_solver.cpp:136] Iteration 33000, lr = 0.0079375, m = 0.9
I0815 11:21:37.674592  8764 solver.cpp:312] Iteration 33100 (6.29089 iter/s, 15.896s/100 iter), loss = 1.68687
I0815 11:21:37.674662  8764 solver.cpp:334]     Train net output #0: loss = 1.8109 (* 1 = 1.8109 loss)
I0815 11:21:37.674682  8764 sgd_solver.cpp:136] Iteration 33100, lr = 0.00793125, m = 0.9
I0815 11:21:53.345787  8764 solver.cpp:312] Iteration 33200 (6.38131 iter/s, 15.6708s/100 iter), loss = 1.4617
I0815 11:21:53.345860  8764 solver.cpp:334]     Train net output #0: loss = 1.21956 (* 1 = 1.21956 loss)
I0815 11:21:53.345866  8764 sgd_solver.cpp:136] Iteration 33200, lr = 0.007925, m = 0.9
I0815 11:22:08.656699  8764 solver.cpp:312] Iteration 33300 (6.53147 iter/s, 15.3105s/100 iter), loss = 1.59192
I0815 11:22:08.656728  8764 solver.cpp:334]     Train net output #0: loss = 1.31211 (* 1 = 1.31211 loss)
I0815 11:22:08.656734  8764 sgd_solver.cpp:136] Iteration 33300, lr = 0.00791875, m = 0.9
I0815 11:22:24.034198  8764 solver.cpp:312] Iteration 33400 (6.50319 iter/s, 15.3771s/100 iter), loss = 1.54802
I0815 11:22:24.034251  8764 solver.cpp:334]     Train net output #0: loss = 1.33934 (* 1 = 1.33934 loss)
I0815 11:22:24.034258  8764 sgd_solver.cpp:136] Iteration 33400, lr = 0.0079125, m = 0.9
I0815 11:22:40.046442  8764 solver.cpp:312] Iteration 33500 (6.2454 iter/s, 16.0118s/100 iter), loss = 1.67432
I0815 11:22:40.046510  8764 solver.cpp:334]     Train net output #0: loss = 1.51729 (* 1 = 1.51729 loss)
I0815 11:22:40.046530  8764 sgd_solver.cpp:136] Iteration 33500, lr = 0.00790625, m = 0.9
I0815 11:22:57.076661  8764 solver.cpp:312] Iteration 33600 (5.87208 iter/s, 17.0297s/100 iter), loss = 1.59763
I0815 11:22:57.076769  8764 solver.cpp:334]     Train net output #0: loss = 1.81875 (* 1 = 1.81875 loss)
I0815 11:22:57.076792  8764 sgd_solver.cpp:136] Iteration 33600, lr = 0.0079, m = 0.9
I0815 11:23:12.250653  8764 solver.cpp:312] Iteration 33700 (6.5904 iter/s, 15.1736s/100 iter), loss = 1.34758
I0815 11:23:12.250675  8764 solver.cpp:334]     Train net output #0: loss = 1.60743 (* 1 = 1.60743 loss)
I0815 11:23:12.250681  8764 sgd_solver.cpp:136] Iteration 33700, lr = 0.00789375, m = 0.9
I0815 11:23:27.083969  8764 solver.cpp:312] Iteration 33800 (6.74177 iter/s, 14.8329s/100 iter), loss = 1.55141
I0815 11:23:27.084023  8764 solver.cpp:334]     Train net output #0: loss = 1.90215 (* 1 = 1.90215 loss)
I0815 11:23:27.084030  8764 sgd_solver.cpp:136] Iteration 33800, lr = 0.0078875, m = 0.9
I0815 11:23:43.764436  8764 solver.cpp:312] Iteration 33900 (5.9952 iter/s, 16.68s/100 iter), loss = 1.4318
I0815 11:23:43.764462  8764 solver.cpp:334]     Train net output #0: loss = 1.52724 (* 1 = 1.52724 loss)
I0815 11:23:43.764466  8764 sgd_solver.cpp:136] Iteration 33900, lr = 0.00788125, m = 0.9
I0815 11:23:59.943383  8764 solver.cpp:363] Sparsity after update:
I0815 11:23:59.947299  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:23:59.947329  8764 net.cpp:2192] conv1a_param_0(0.0667) 
I0815 11:23:59.947345  8764 net.cpp:2192] conv1b_param_0(0.104) 
I0815 11:23:59.947355  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:23:59.947365  8764 net.cpp:2192] res2a_branch2a_param_0(0.139) 
I0815 11:23:59.947374  8764 net.cpp:2192] res2a_branch2b_param_0(0.139) 
I0815 11:23:59.947384  8764 net.cpp:2192] res3a_branch2a_param_0(0.139) 
I0815 11:23:59.947393  8764 net.cpp:2192] res3a_branch2b_param_0(0.139) 
I0815 11:23:59.947402  8764 net.cpp:2192] res4a_branch2a_param_0(0.14) 
I0815 11:23:59.947412  8764 net.cpp:2192] res4a_branch2b_param_0(0.139) 
I0815 11:23:59.947420  8764 net.cpp:2192] res5a_branch2a_param_0(0.14) 
I0815 11:23:59.947429  8764 net.cpp:2192] res5a_branch2b_param_0(0.14) 
I0815 11:23:59.947439  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (328550/2.86678e+06) 0.115
I0815 11:23:59.947463  8764 solver.cpp:509] Iteration 34000, Testing net (#0)
I0815 11:24:24.419677  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:24:24.785504  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.557
I0815 11:24:24.785531  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791939
I0815 11:24:24.785537  8764 solver.cpp:594]     Test net output #2: loss = 1.95703 (* 1 = 1.95703 loss)
I0815 11:24:24.785559  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.8374s
I0815 11:24:24.922410  8794 solver.cpp:409] Finding and applying sparsity: 0.15
I0815 11:24:44.920830  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:24:44.922870  8764 solver.cpp:312] Iteration 34000 (1.63514 iter/s, 61.1568s/100 iter), loss = 1.80419
I0815 11:24:44.922891  8764 solver.cpp:334]     Train net output #0: loss = 1.36294 (* 1 = 1.36294 loss)
I0815 11:24:44.922899  8764 sgd_solver.cpp:136] Iteration 34000, lr = 0.007875, m = 0.9
I0815 11:25:00.186173  8764 solver.cpp:312] Iteration 34100 (6.55185 iter/s, 15.2629s/100 iter), loss = 1.54083
I0815 11:25:00.186200  8764 solver.cpp:334]     Train net output #0: loss = 1.98953 (* 1 = 1.98953 loss)
I0815 11:25:00.186206  8764 sgd_solver.cpp:136] Iteration 34100, lr = 0.00786875, m = 0.9
I0815 11:25:15.190654  8764 solver.cpp:312] Iteration 34200 (6.66486 iter/s, 15.0041s/100 iter), loss = 1.47853
I0815 11:25:15.190752  8764 solver.cpp:334]     Train net output #0: loss = 1.33805 (* 1 = 1.33805 loss)
I0815 11:25:15.190760  8764 sgd_solver.cpp:136] Iteration 34200, lr = 0.0078625, m = 0.9
I0815 11:25:31.300882  8764 solver.cpp:312] Iteration 34300 (6.20741 iter/s, 16.1098s/100 iter), loss = 1.49382
I0815 11:25:31.300921  8764 solver.cpp:334]     Train net output #0: loss = 1.41662 (* 1 = 1.41662 loss)
I0815 11:25:31.300930  8764 sgd_solver.cpp:136] Iteration 34300, lr = 0.00785625, m = 0.9
I0815 11:25:47.740947  8764 solver.cpp:312] Iteration 34400 (6.08287 iter/s, 16.4396s/100 iter), loss = 1.41608
I0815 11:25:47.741032  8764 solver.cpp:334]     Train net output #0: loss = 1.23243 (* 1 = 1.23243 loss)
I0815 11:25:47.741045  8764 sgd_solver.cpp:136] Iteration 34400, lr = 0.00785, m = 0.9
I0815 11:26:05.341651  8764 solver.cpp:312] Iteration 34500 (5.68175 iter/s, 17.6002s/100 iter), loss = 1.09213
I0815 11:26:05.341673  8764 solver.cpp:334]     Train net output #0: loss = 1.24418 (* 1 = 1.24418 loss)
I0815 11:26:05.341677  8764 sgd_solver.cpp:136] Iteration 34500, lr = 0.00784375, m = 0.9
I0815 11:26:20.984057  8764 solver.cpp:312] Iteration 34600 (6.39306 iter/s, 15.642s/100 iter), loss = 1.17766
I0815 11:26:20.984140  8764 solver.cpp:334]     Train net output #0: loss = 1.11194 (* 1 = 1.11194 loss)
I0815 11:26:20.984148  8764 sgd_solver.cpp:136] Iteration 34600, lr = 0.0078375, m = 0.9
I0815 11:26:36.007835  8764 solver.cpp:312] Iteration 34700 (6.6563 iter/s, 15.0234s/100 iter), loss = 1.21683
I0815 11:26:36.007915  8764 solver.cpp:334]     Train net output #0: loss = 1.35786 (* 1 = 1.35786 loss)
I0815 11:26:36.007935  8764 sgd_solver.cpp:136] Iteration 34700, lr = 0.00783125, m = 0.9
I0815 11:26:50.811862  8764 solver.cpp:312] Iteration 34800 (6.75511 iter/s, 14.8036s/100 iter), loss = 1.47487
I0815 11:26:50.811887  8764 solver.cpp:334]     Train net output #0: loss = 1.82908 (* 1 = 1.82908 loss)
I0815 11:26:50.811892  8764 sgd_solver.cpp:136] Iteration 34800, lr = 0.007825, m = 0.9
I0815 11:27:07.234835  8764 solver.cpp:312] Iteration 34900 (6.0892 iter/s, 16.4225s/100 iter), loss = 1.36259
I0815 11:27:07.234895  8764 solver.cpp:334]     Train net output #0: loss = 1.49973 (* 1 = 1.49973 loss)
I0815 11:27:07.234901  8764 sgd_solver.cpp:136] Iteration 34900, lr = 0.00781875, m = 0.9
I0815 11:27:21.564394  8764 solver.cpp:363] Sparsity after update:
I0815 11:27:21.574854  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:27:21.574867  8764 net.cpp:2192] conv1a_param_0(0.0662) 
I0815 11:27:21.574875  8764 net.cpp:2192] conv1b_param_0(0.104) 
I0815 11:27:21.574880  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:27:21.575011  8764 net.cpp:2192] res2a_branch2a_param_0(0.149) 
I0815 11:27:21.575016  8764 net.cpp:2192] res2a_branch2b_param_0(0.146) 
I0815 11:27:21.575018  8764 net.cpp:2192] res3a_branch2a_param_0(0.149) 
I0815 11:27:21.575021  8764 net.cpp:2192] res3a_branch2b_param_0(0.149) 
I0815 11:27:21.575139  8764 net.cpp:2192] res4a_branch2a_param_0(0.149) 
I0815 11:27:21.575142  8764 net.cpp:2192] res4a_branch2b_param_0(0.149) 
I0815 11:27:21.575146  8764 net.cpp:2192] res5a_branch2a_param_0(0.15) 
I0815 11:27:21.575150  8764 net.cpp:2192] res5a_branch2b_param_0(0.149) 
I0815 11:27:21.575268  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (351719/2.86678e+06) 0.123
I0815 11:27:21.707368  8794 solver.cpp:409] Finding and applying sparsity: 0.16
I0815 11:27:42.163173  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:27:42.165204  8764 solver.cpp:312] Iteration 35000 (2.86292 iter/s, 34.9294s/100 iter), loss = 1.19298
I0815 11:27:42.165222  8764 solver.cpp:334]     Train net output #0: loss = 0.96306 (* 1 = 0.96306 loss)
I0815 11:27:42.165227  8764 sgd_solver.cpp:136] Iteration 35000, lr = 0.0078125, m = 0.9
I0815 11:27:57.014886  8764 solver.cpp:312] Iteration 35100 (6.73434 iter/s, 14.8493s/100 iter), loss = 1.25358
I0815 11:27:57.015107  8764 solver.cpp:334]     Train net output #0: loss = 1.65726 (* 1 = 1.65726 loss)
I0815 11:27:57.015218  8764 sgd_solver.cpp:136] Iteration 35100, lr = 0.00780625, m = 0.9
I0815 11:28:11.910329  8764 solver.cpp:312] Iteration 35200 (6.71365 iter/s, 14.895s/100 iter), loss = 1.64793
I0815 11:28:11.910353  8764 solver.cpp:334]     Train net output #0: loss = 1.838 (* 1 = 1.838 loss)
I0815 11:28:11.910359  8764 sgd_solver.cpp:136] Iteration 35200, lr = 0.0078, m = 0.9
I0815 11:28:29.028208  8764 solver.cpp:312] Iteration 35300 (5.84201 iter/s, 17.1174s/100 iter), loss = 1.52537
I0815 11:28:29.028307  8764 solver.cpp:334]     Train net output #0: loss = 0.974179 (* 1 = 0.974179 loss)
I0815 11:28:29.028319  8764 sgd_solver.cpp:136] Iteration 35300, lr = 0.00779375, m = 0.9
I0815 11:28:44.439420  8764 solver.cpp:312] Iteration 35400 (6.48896 iter/s, 15.4108s/100 iter), loss = 1.42437
I0815 11:28:44.439448  8764 solver.cpp:334]     Train net output #0: loss = 1.31148 (* 1 = 1.31148 loss)
I0815 11:28:44.439455  8764 sgd_solver.cpp:136] Iteration 35400, lr = 0.0077875, m = 0.9
I0815 11:29:01.591969  8764 solver.cpp:312] Iteration 35500 (5.8302 iter/s, 17.1521s/100 iter), loss = 1.59699
I0815 11:29:01.592020  8764 solver.cpp:334]     Train net output #0: loss = 1.45553 (* 1 = 1.45553 loss)
I0815 11:29:01.592026  8764 sgd_solver.cpp:136] Iteration 35500, lr = 0.00778125, m = 0.9
I0815 11:29:19.469418  8764 solver.cpp:312] Iteration 35600 (5.59379 iter/s, 17.877s/100 iter), loss = 1.89033
I0815 11:29:19.469596  8764 solver.cpp:334]     Train net output #0: loss = 1.87403 (* 1 = 1.87403 loss)
I0815 11:29:19.469686  8764 sgd_solver.cpp:136] Iteration 35600, lr = 0.007775, m = 0.9
I0815 11:29:36.423794  8764 solver.cpp:312] Iteration 35700 (5.89834 iter/s, 16.9539s/100 iter), loss = 1.39617
I0815 11:29:36.423857  8764 solver.cpp:334]     Train net output #0: loss = 0.967375 (* 1 = 0.967375 loss)
I0815 11:29:36.423864  8764 sgd_solver.cpp:136] Iteration 35700, lr = 0.00776875, m = 0.9
I0815 11:29:53.480577  8764 solver.cpp:312] Iteration 35800 (5.86294 iter/s, 17.0563s/100 iter), loss = 1.39828
I0815 11:29:53.480634  8764 solver.cpp:334]     Train net output #0: loss = 1.41542 (* 1 = 1.41542 loss)
I0815 11:29:53.480650  8764 sgd_solver.cpp:136] Iteration 35800, lr = 0.0077625, m = 0.9
I0815 11:30:10.723016  8764 solver.cpp:312] Iteration 35900 (5.7998 iter/s, 17.242s/100 iter), loss = 1.47997
I0815 11:30:10.723117  8764 solver.cpp:334]     Train net output #0: loss = 1.29137 (* 1 = 1.29137 loss)
I0815 11:30:10.723136  8764 sgd_solver.cpp:136] Iteration 35900, lr = 0.00775625, m = 0.9
I0815 11:30:27.144430  8764 solver.cpp:363] Sparsity after update:
I0815 11:30:27.157223  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:30:27.157285  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:30:27.157315  8764 net.cpp:2192] conv1b_param_0(0.115) 
I0815 11:30:27.157335  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:30:27.157387  8764 net.cpp:2192] res2a_branch2a_param_0(0.16) 
I0815 11:30:27.157424  8764 net.cpp:2192] res2a_branch2b_param_0(0.16) 
I0815 11:30:27.157444  8764 net.cpp:2192] res3a_branch2a_param_0(0.16) 
I0815 11:30:27.157461  8764 net.cpp:2192] res3a_branch2b_param_0(0.16) 
I0815 11:30:27.157495  8764 net.cpp:2192] res4a_branch2a_param_0(0.16) 
I0815 11:30:27.157512  8764 net.cpp:2192] res4a_branch2b_param_0(0.16) 
I0815 11:30:27.157553  8764 net.cpp:2192] res5a_branch2a_param_0(0.16) 
I0815 11:30:27.157570  8764 net.cpp:2192] res5a_branch2b_param_0(0.16) 
I0815 11:30:27.157589  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (375789/2.86678e+06) 0.131
I0815 11:30:27.157622  8764 solver.cpp:509] Iteration 36000, Testing net (#0)
I0815 11:30:51.102052  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.556824
I0815 11:30:51.102121  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791586
I0815 11:30:51.102128  8764 solver.cpp:594]     Test net output #2: loss = 1.97146 (* 1 = 1.97146 loss)
I0815 11:30:51.102147  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.9439s
I0815 11:30:51.260190  8794 solver.cpp:409] Finding and applying sparsity: 0.17
I0815 11:31:11.242786  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:31:11.244763  8764 solver.cpp:312] Iteration 36000 (1.65234 iter/s, 60.5201s/100 iter), loss = 1.55046
I0815 11:31:11.244786  8764 solver.cpp:334]     Train net output #0: loss = 1.4654 (* 1 = 1.4654 loss)
I0815 11:31:11.244794  8764 sgd_solver.cpp:136] Iteration 36000, lr = 0.00775, m = 0.9
I0815 11:31:28.195852  8764 solver.cpp:312] Iteration 36100 (5.89949 iter/s, 16.9506s/100 iter), loss = 1.37475
I0815 11:31:28.196061  8764 solver.cpp:334]     Train net output #0: loss = 1.34243 (* 1 = 1.34243 loss)
I0815 11:31:28.196152  8764 sgd_solver.cpp:136] Iteration 36100, lr = 0.00774375, m = 0.9
I0815 11:31:44.183331  8764 solver.cpp:312] Iteration 36200 (6.25507 iter/s, 15.987s/100 iter), loss = 1.87017
I0815 11:31:44.183357  8764 solver.cpp:334]     Train net output #0: loss = 1.61033 (* 1 = 1.61033 loss)
I0815 11:31:44.183363  8764 sgd_solver.cpp:136] Iteration 36200, lr = 0.0077375, m = 0.9
I0815 11:31:59.514802  8764 solver.cpp:312] Iteration 36300 (6.52271 iter/s, 15.331s/100 iter), loss = 1.33007
I0815 11:31:59.514858  8764 solver.cpp:334]     Train net output #0: loss = 1.4809 (* 1 = 1.4809 loss)
I0815 11:31:59.514864  8764 sgd_solver.cpp:136] Iteration 36300, lr = 0.00773125, m = 0.9
I0815 11:32:15.762256  8764 solver.cpp:312] Iteration 36400 (6.15498 iter/s, 16.247s/100 iter), loss = 1.25349
I0815 11:32:15.762284  8764 solver.cpp:334]     Train net output #0: loss = 1.64154 (* 1 = 1.64154 loss)
I0815 11:32:15.762287  8764 sgd_solver.cpp:136] Iteration 36400, lr = 0.007725, m = 0.9
I0815 11:32:32.052091  8764 solver.cpp:312] Iteration 36500 (6.13897 iter/s, 16.2894s/100 iter), loss = 1.45727
I0815 11:32:32.052160  8764 solver.cpp:334]     Train net output #0: loss = 1.44405 (* 1 = 1.44405 loss)
I0815 11:32:32.052168  8764 sgd_solver.cpp:136] Iteration 36500, lr = 0.00771875, m = 0.9
I0815 11:32:47.590975  8764 solver.cpp:312] Iteration 36600 (6.43565 iter/s, 15.5385s/100 iter), loss = 1.46538
I0815 11:32:47.591006  8764 solver.cpp:334]     Train net output #0: loss = 1.22474 (* 1 = 1.22474 loss)
I0815 11:32:47.591011  8764 sgd_solver.cpp:136] Iteration 36600, lr = 0.0077125, m = 0.9
I0815 11:33:02.649451  8764 solver.cpp:312] Iteration 36700 (6.64096 iter/s, 15.0581s/100 iter), loss = 1.34385
I0815 11:33:02.649695  8764 solver.cpp:334]     Train net output #0: loss = 1.06285 (* 1 = 1.06285 loss)
I0815 11:33:02.649719  8764 sgd_solver.cpp:136] Iteration 36700, lr = 0.00770625, m = 0.9
I0815 11:33:17.599063  8764 solver.cpp:312] Iteration 36800 (6.68932 iter/s, 14.9492s/100 iter), loss = 1.14324
I0815 11:33:17.599088  8764 solver.cpp:334]     Train net output #0: loss = 1.43592 (* 1 = 1.43592 loss)
I0815 11:33:17.599094  8764 sgd_solver.cpp:136] Iteration 36800, lr = 0.0077, m = 0.9
I0815 11:33:35.190912  8764 solver.cpp:312] Iteration 36900 (5.68461 iter/s, 17.5914s/100 iter), loss = 1.47882
I0815 11:33:35.191040  8764 solver.cpp:334]     Train net output #0: loss = 1.55108 (* 1 = 1.55108 loss)
I0815 11:33:35.191058  8764 sgd_solver.cpp:136] Iteration 36900, lr = 0.00769375, m = 0.9
I0815 11:33:51.420459  8764 solver.cpp:363] Sparsity after update:
I0815 11:33:51.432106  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:33:51.432258  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:33:51.432353  8764 net.cpp:2192] conv1b_param_0(0.125) 
I0815 11:33:51.432441  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:33:51.432526  8764 net.cpp:2192] res2a_branch2a_param_0(0.167) 
I0815 11:33:51.432615  8764 net.cpp:2192] res2a_branch2b_param_0(0.167) 
I0815 11:33:51.432703  8764 net.cpp:2192] res3a_branch2a_param_0(0.168) 
I0815 11:33:51.432792  8764 net.cpp:2192] res3a_branch2b_param_0(0.167) 
I0815 11:33:51.432883  8764 net.cpp:2192] res4a_branch2a_param_0(0.169) 
I0815 11:33:51.432974  8764 net.cpp:2192] res4a_branch2b_param_0(0.168) 
I0815 11:33:51.433063  8764 net.cpp:2192] res5a_branch2a_param_0(0.17) 
I0815 11:33:51.433151  8764 net.cpp:2192] res5a_branch2b_param_0(0.169) 
I0815 11:33:51.433245  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (398388/2.86678e+06) 0.139
I0815 11:33:51.567394  8794 solver.cpp:409] Finding and applying sparsity: 0.18
I0815 11:34:12.200837  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:34:12.204712  8764 solver.cpp:312] Iteration 37000 (2.70177 iter/s, 37.0128s/100 iter), loss = 1.34163
I0815 11:34:12.204741  8764 solver.cpp:334]     Train net output #0: loss = 1.03597 (* 1 = 1.03597 loss)
I0815 11:34:12.204748  8764 sgd_solver.cpp:136] Iteration 37000, lr = 0.0076875, m = 0.9
I0815 11:34:29.559839  8764 solver.cpp:312] Iteration 37100 (5.76215 iter/s, 17.3546s/100 iter), loss = 1.10127
I0815 11:34:29.559867  8764 solver.cpp:334]     Train net output #0: loss = 0.903144 (* 1 = 0.903144 loss)
I0815 11:34:29.559872  8764 sgd_solver.cpp:136] Iteration 37100, lr = 0.00768125, m = 0.9
I0815 11:34:44.139724  8764 solver.cpp:312] Iteration 37200 (6.85896 iter/s, 14.5795s/100 iter), loss = 1.6586
I0815 11:34:44.139827  8764 solver.cpp:334]     Train net output #0: loss = 1.73729 (* 1 = 1.73729 loss)
I0815 11:34:44.139848  8764 sgd_solver.cpp:136] Iteration 37200, lr = 0.007675, m = 0.9
I0815 11:34:58.815135  8764 solver.cpp:312] Iteration 37300 (6.81431 iter/s, 14.675s/100 iter), loss = 1.84164
I0815 11:34:58.815189  8764 solver.cpp:334]     Train net output #0: loss = 1.8634 (* 1 = 1.8634 loss)
I0815 11:34:58.815198  8764 sgd_solver.cpp:136] Iteration 37300, lr = 0.00766875, m = 0.9
I0815 11:35:15.880841  8764 solver.cpp:312] Iteration 37400 (5.85987 iter/s, 17.0652s/100 iter), loss = 1.53415
I0815 11:35:15.880905  8764 solver.cpp:334]     Train net output #0: loss = 1.55228 (* 1 = 1.55228 loss)
I0815 11:35:15.880913  8764 sgd_solver.cpp:136] Iteration 37400, lr = 0.0076625, m = 0.9
I0815 11:35:33.582965  8764 solver.cpp:312] Iteration 37500 (5.6492 iter/s, 17.7016s/100 iter), loss = 1.60261
I0815 11:35:33.583029  8764 solver.cpp:334]     Train net output #0: loss = 1.87063 (* 1 = 1.87063 loss)
I0815 11:35:33.583047  8764 sgd_solver.cpp:136] Iteration 37500, lr = 0.00765625, m = 0.9
I0815 11:35:51.310339  8764 solver.cpp:312] Iteration 37600 (5.64115 iter/s, 17.7269s/100 iter), loss = 1.25786
I0815 11:35:51.310598  8764 solver.cpp:334]     Train net output #0: loss = 1.4747 (* 1 = 1.4747 loss)
I0815 11:35:51.310627  8764 sgd_solver.cpp:136] Iteration 37600, lr = 0.00765, m = 0.9
I0815 11:36:09.663689  8764 solver.cpp:312] Iteration 37700 (5.44875 iter/s, 18.3528s/100 iter), loss = 1.58454
I0815 11:36:09.663743  8764 solver.cpp:334]     Train net output #0: loss = 1.38431 (* 1 = 1.38431 loss)
I0815 11:36:09.663754  8764 sgd_solver.cpp:136] Iteration 37700, lr = 0.00764375, m = 0.9
I0815 11:36:27.896723  8764 solver.cpp:312] Iteration 37800 (5.4847 iter/s, 18.2325s/100 iter), loss = 1.58842
I0815 11:36:27.896790  8764 solver.cpp:334]     Train net output #0: loss = 1.40674 (* 1 = 1.40674 loss)
I0815 11:36:27.896795  8764 sgd_solver.cpp:136] Iteration 37800, lr = 0.0076375, m = 0.9
I0815 11:36:44.533901  8764 solver.cpp:312] Iteration 37900 (6.0108 iter/s, 16.6367s/100 iter), loss = 1.62509
I0815 11:36:44.533924  8764 solver.cpp:334]     Train net output #0: loss = 2.20072 (* 1 = 2.20072 loss)
I0815 11:36:44.533928  8764 sgd_solver.cpp:136] Iteration 37900, lr = 0.00763125, m = 0.9
I0815 11:36:59.702090  8764 solver.cpp:363] Sparsity after update:
I0815 11:36:59.706921  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:36:59.706933  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:36:59.706941  8764 net.cpp:2192] conv1b_param_0(0.125) 
I0815 11:36:59.706945  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:36:59.707118  8764 net.cpp:2192] res2a_branch2a_param_0(0.177) 
I0815 11:36:59.707130  8764 net.cpp:2192] res2a_branch2b_param_0(0.174) 
I0815 11:36:59.707249  8764 net.cpp:2192] res3a_branch2a_param_0(0.179) 
I0815 11:36:59.707314  8764 net.cpp:2192] res3a_branch2b_param_0(0.177) 
I0815 11:36:59.707378  8764 net.cpp:2192] res4a_branch2a_param_0(0.18) 
I0815 11:36:59.707440  8764 net.cpp:2192] res4a_branch2b_param_0(0.179) 
I0815 11:36:59.707504  8764 net.cpp:2192] res5a_branch2a_param_0(0.18) 
I0815 11:36:59.707566  8764 net.cpp:2192] res5a_branch2b_param_0(0.18) 
I0815 11:36:59.707628  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (422333/2.86678e+06) 0.147
I0815 11:36:59.707711  8764 solver.cpp:509] Iteration 38000, Testing net (#0)
I0815 11:37:20.342571  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549118
I0815 11:37:20.342598  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.793233
I0815 11:37:20.342607  8764 solver.cpp:594]     Test net output #2: loss = 1.97336 (* 1 = 1.97336 loss)
I0815 11:37:20.342628  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.6343s
I0815 11:37:20.512686  8794 solver.cpp:409] Finding and applying sparsity: 0.19
I0815 11:37:40.956804  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:37:40.958834  8764 solver.cpp:312] Iteration 38000 (1.77232 iter/s, 56.4234s/100 iter), loss = 1.49895
I0815 11:37:40.958855  8764 solver.cpp:334]     Train net output #0: loss = 1.65869 (* 1 = 1.65869 loss)
I0815 11:37:40.958864  8764 sgd_solver.cpp:136] Iteration 38000, lr = 0.007625, m = 0.9
I0815 11:38:00.798552  8764 solver.cpp:312] Iteration 38100 (5.04054 iter/s, 19.8392s/100 iter), loss = 1.76343
I0815 11:38:00.798580  8764 solver.cpp:334]     Train net output #0: loss = 2.04413 (* 1 = 2.04413 loss)
I0815 11:38:00.798584  8764 sgd_solver.cpp:136] Iteration 38100, lr = 0.00761875, m = 0.9
I0815 11:38:15.247417  8764 solver.cpp:312] Iteration 38200 (6.92115 iter/s, 14.4485s/100 iter), loss = 1.21291
I0815 11:38:15.247563  8764 solver.cpp:334]     Train net output #0: loss = 1.48143 (* 1 = 1.48143 loss)
I0815 11:38:15.247582  8764 sgd_solver.cpp:136] Iteration 38200, lr = 0.0076125, m = 0.9
I0815 11:38:29.973095  8764 solver.cpp:312] Iteration 38300 (6.79105 iter/s, 14.7253s/100 iter), loss = 1.411
I0815 11:38:29.973163  8764 solver.cpp:334]     Train net output #0: loss = 1.4162 (* 1 = 1.4162 loss)
I0815 11:38:29.973181  8764 sgd_solver.cpp:136] Iteration 38300, lr = 0.00760625, m = 0.9
I0815 11:38:45.308043  8764 solver.cpp:312] Iteration 38400 (6.52123 iter/s, 15.3345s/100 iter), loss = 1.26071
I0815 11:38:45.312181  8764 solver.cpp:334]     Train net output #0: loss = 1.44127 (* 1 = 1.44127 loss)
I0815 11:38:45.312201  8764 sgd_solver.cpp:136] Iteration 38400, lr = 0.0076, m = 0.9
I0815 11:39:02.956161  8764 solver.cpp:312] Iteration 38500 (5.66648 iter/s, 17.6476s/100 iter), loss = 1.44182
I0815 11:39:02.956192  8764 solver.cpp:334]     Train net output #0: loss = 1.27267 (* 1 = 1.27267 loss)
I0815 11:39:02.956197  8764 sgd_solver.cpp:136] Iteration 38500, lr = 0.00759375, m = 0.9
I0815 11:39:19.250962  8764 solver.cpp:312] Iteration 38600 (6.1371 iter/s, 16.2943s/100 iter), loss = 1.45062
I0815 11:39:19.251072  8764 solver.cpp:334]     Train net output #0: loss = 1.38586 (* 1 = 1.38586 loss)
I0815 11:39:19.251092  8764 sgd_solver.cpp:136] Iteration 38600, lr = 0.0075875, m = 0.9
I0815 11:39:33.939545  8764 solver.cpp:312] Iteration 38700 (6.8082 iter/s, 14.6882s/100 iter), loss = 1.32119
I0815 11:39:33.939574  8764 solver.cpp:334]     Train net output #0: loss = 1.23376 (* 1 = 1.23376 loss)
I0815 11:39:33.939579  8764 sgd_solver.cpp:136] Iteration 38700, lr = 0.00758125, m = 0.9
I0815 11:39:49.186115  8764 solver.cpp:312] Iteration 38800 (6.55904 iter/s, 15.2461s/100 iter), loss = 1.07166
I0815 11:39:49.186143  8764 solver.cpp:334]     Train net output #0: loss = 1.33458 (* 1 = 1.33458 loss)
I0815 11:39:49.186151  8764 sgd_solver.cpp:136] Iteration 38800, lr = 0.007575, m = 0.9
I0815 11:40:06.023881  8764 solver.cpp:312] Iteration 38900 (5.9392 iter/s, 16.8373s/100 iter), loss = 1.64579
I0815 11:40:06.023985  8764 solver.cpp:334]     Train net output #0: loss = 1.47681 (* 1 = 1.47681 loss)
I0815 11:40:06.024003  8764 sgd_solver.cpp:136] Iteration 38900, lr = 0.00756875, m = 0.9
I0815 11:40:24.416576  8764 solver.cpp:363] Sparsity after update:
I0815 11:40:24.422780  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:40:24.424140  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:40:24.424165  8764 net.cpp:2192] conv1b_param_0(0.135) 
I0815 11:40:24.424180  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:40:24.424192  8764 net.cpp:2192] res2a_branch2a_param_0(0.188) 
I0815 11:40:24.424203  8764 net.cpp:2192] res2a_branch2b_param_0(0.187) 
I0815 11:40:24.424216  8764 net.cpp:2192] res3a_branch2a_param_0(0.189) 
I0815 11:40:24.424226  8764 net.cpp:2192] res3a_branch2b_param_0(0.188) 
I0815 11:40:24.424237  8764 net.cpp:2192] res4a_branch2a_param_0(0.189) 
I0815 11:40:24.424248  8764 net.cpp:2192] res4a_branch2b_param_0(0.189) 
I0815 11:40:24.424262  8764 net.cpp:2192] res5a_branch2a_param_0(0.19) 
I0815 11:40:24.424271  8764 net.cpp:2192] res5a_branch2b_param_0(0.189) 
I0815 11:40:24.424283  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (445609/2.86678e+06) 0.155
I0815 11:40:24.715215  8794 solver.cpp:409] Finding and applying sparsity: 0.2
I0815 11:40:45.977237  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:40:45.979388  8764 solver.cpp:312] Iteration 39000 (2.50285 iter/s, 39.9544s/100 iter), loss = 1.38248
I0815 11:40:45.979413  8764 solver.cpp:334]     Train net output #0: loss = 1.44988 (* 1 = 1.44988 loss)
I0815 11:40:45.979420  8764 sgd_solver.cpp:136] Iteration 39000, lr = 0.0075625, m = 0.9
I0815 11:41:03.810811  8764 solver.cpp:312] Iteration 39100 (5.60824 iter/s, 17.8309s/100 iter), loss = 1.51247
I0815 11:41:03.811035  8764 solver.cpp:334]     Train net output #0: loss = 1.42328 (* 1 = 1.42328 loss)
I0815 11:41:03.811146  8764 sgd_solver.cpp:136] Iteration 39100, lr = 0.00755625, m = 0.9
I0815 11:41:20.162564  8764 solver.cpp:312] Iteration 39200 (6.11572 iter/s, 16.3513s/100 iter), loss = 1.32689
I0815 11:41:20.162648  8764 solver.cpp:334]     Train net output #0: loss = 1.35685 (* 1 = 1.35685 loss)
I0815 11:41:20.162658  8764 sgd_solver.cpp:136] Iteration 39200, lr = 0.00755, m = 0.9
I0815 11:41:35.761705  8764 solver.cpp:312] Iteration 39300 (6.41079 iter/s, 15.5987s/100 iter), loss = 1.64366
I0815 11:41:35.761731  8764 solver.cpp:334]     Train net output #0: loss = 1.68788 (* 1 = 1.68788 loss)
I0815 11:41:35.761737  8764 sgd_solver.cpp:136] Iteration 39300, lr = 0.00754375, m = 0.9
I0815 11:41:52.704222  8764 solver.cpp:312] Iteration 39400 (5.90248 iter/s, 16.942s/100 iter), loss = 1.37474
I0815 11:41:52.706936  8764 solver.cpp:334]     Train net output #0: loss = 1.34989 (* 1 = 1.34989 loss)
I0815 11:41:52.706959  8764 sgd_solver.cpp:136] Iteration 39400, lr = 0.0075375, m = 0.9
I0815 11:42:09.371809  8764 solver.cpp:312] Iteration 39500 (5.99984 iter/s, 16.6671s/100 iter), loss = 1.14548
I0815 11:42:09.371836  8764 solver.cpp:334]     Train net output #0: loss = 1.116 (* 1 = 1.116 loss)
I0815 11:42:09.371842  8764 sgd_solver.cpp:136] Iteration 39500, lr = 0.00753125, m = 0.9
I0815 11:42:25.625625  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:42:26.828435  8764 solver.cpp:312] Iteration 39600 (5.72864 iter/s, 17.4561s/100 iter), loss = 1.21043
I0815 11:42:26.828498  8764 solver.cpp:334]     Train net output #0: loss = 1.23909 (* 1 = 1.23909 loss)
I0815 11:42:26.828516  8764 sgd_solver.cpp:136] Iteration 39600, lr = 0.007525, m = 0.9
I0815 11:42:41.835108  8764 solver.cpp:312] Iteration 39700 (6.66389 iter/s, 15.0063s/100 iter), loss = 1.74436
I0815 11:42:41.835132  8764 solver.cpp:334]     Train net output #0: loss = 1.63773 (* 1 = 1.63773 loss)
I0815 11:42:41.835137  8764 sgd_solver.cpp:136] Iteration 39700, lr = 0.00751875, m = 0.9
I0815 11:42:56.206392  8764 solver.cpp:312] Iteration 39800 (6.95851 iter/s, 14.3709s/100 iter), loss = 1.44734
I0815 11:42:56.206467  8764 solver.cpp:334]     Train net output #0: loss = 1.3769 (* 1 = 1.3769 loss)
I0815 11:42:56.206480  8764 sgd_solver.cpp:136] Iteration 39800, lr = 0.0075125, m = 0.9
I0815 11:43:10.735148  8764 solver.cpp:312] Iteration 39900 (6.88309 iter/s, 14.5284s/100 iter), loss = 1.47683
I0815 11:43:10.735199  8764 solver.cpp:334]     Train net output #0: loss = 1.70602 (* 1 = 1.70602 loss)
I0815 11:43:10.735213  8764 sgd_solver.cpp:136] Iteration 39900, lr = 0.00750625, m = 0.9
I0815 11:43:25.970715  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_40000.caffemodel
I0815 11:43:25.985014  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_40000.solverstate
I0815 11:43:25.991058  8764 solver.cpp:363] Sparsity after update:
I0815 11:43:25.993029  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:43:25.993041  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:43:25.993049  8764 net.cpp:2192] conv1b_param_0(0.146) 
I0815 11:43:25.993053  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:43:25.993057  8764 net.cpp:2192] res2a_branch2a_param_0(0.198) 
I0815 11:43:25.993060  8764 net.cpp:2192] res2a_branch2b_param_0(0.194) 
I0815 11:43:25.993063  8764 net.cpp:2192] res3a_branch2a_param_0(0.2) 
I0815 11:43:25.993067  8764 net.cpp:2192] res3a_branch2b_param_0(0.198) 
I0815 11:43:25.993070  8764 net.cpp:2192] res4a_branch2a_param_0(0.2) 
I0815 11:43:25.993074  8764 net.cpp:2192] res4a_branch2b_param_0(0.2) 
I0815 11:43:25.993077  8764 net.cpp:2192] res5a_branch2a_param_0(0.2) 
I0815 11:43:25.993098  8764 net.cpp:2192] res5a_branch2b_param_0(0.2) 
I0815 11:43:25.993113  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (469572/2.86678e+06) 0.164
I0815 11:43:25.993132  8764 solver.cpp:509] Iteration 40000, Testing net (#0)
I0815 11:43:46.576879  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549118
I0815 11:43:46.576926  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.787704
I0815 11:43:46.576931  8764 solver.cpp:594]     Test net output #2: loss = 2.00016 (* 1 = 2.00016 loss)
I0815 11:43:46.576951  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.5833s
I0815 11:43:46.724166  8794 solver.cpp:409] Finding and applying sparsity: 0.21
I0815 11:44:07.559698  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:44:07.561921  8764 solver.cpp:312] Iteration 40000 (1.75978 iter/s, 56.8252s/100 iter), loss = 1.33954
I0815 11:44:07.561945  8764 solver.cpp:334]     Train net output #0: loss = 1.25362 (* 1 = 1.25362 loss)
I0815 11:44:07.561951  8764 sgd_solver.cpp:136] Iteration 40000, lr = 0.0075, m = 0.9
I0815 11:44:22.564882  8764 solver.cpp:312] Iteration 40100 (6.66554 iter/s, 15.0025s/100 iter), loss = 1.58491
I0815 11:44:22.564996  8764 solver.cpp:334]     Train net output #0: loss = 1.26917 (* 1 = 1.26917 loss)
I0815 11:44:22.565004  8764 sgd_solver.cpp:136] Iteration 40100, lr = 0.00749375, m = 0.9
I0815 11:44:37.559574  8764 solver.cpp:312] Iteration 40200 (6.66921 iter/s, 14.9943s/100 iter), loss = 1.27357
I0815 11:44:37.559638  8764 solver.cpp:334]     Train net output #0: loss = 1.39762 (* 1 = 1.39762 loss)
I0815 11:44:37.559655  8764 sgd_solver.cpp:136] Iteration 40200, lr = 0.0074875, m = 0.9
I0815 11:44:54.653398  8764 solver.cpp:312] Iteration 40300 (5.85023 iter/s, 17.0933s/100 iter), loss = 1.4494
I0815 11:44:54.653604  8764 solver.cpp:334]     Train net output #0: loss = 1.4241 (* 1 = 1.4241 loss)
I0815 11:44:54.653688  8764 sgd_solver.cpp:136] Iteration 40300, lr = 0.00748125, m = 0.9
I0815 11:45:10.723640  8764 solver.cpp:312] Iteration 40400 (6.22286 iter/s, 16.0698s/100 iter), loss = 1.17721
I0815 11:45:10.723667  8764 solver.cpp:334]     Train net output #0: loss = 1.13213 (* 1 = 1.13213 loss)
I0815 11:45:10.723671  8764 sgd_solver.cpp:136] Iteration 40400, lr = 0.007475, m = 0.9
I0815 11:45:26.757740  8764 solver.cpp:312] Iteration 40500 (6.23688 iter/s, 16.0337s/100 iter), loss = 1.39355
I0815 11:45:26.764202  8764 solver.cpp:334]     Train net output #0: loss = 1.34223 (* 1 = 1.34223 loss)
I0815 11:45:26.764226  8764 sgd_solver.cpp:136] Iteration 40500, lr = 0.00746875, m = 0.9
I0815 11:45:44.359653  8764 solver.cpp:312] Iteration 40600 (5.68136 iter/s, 17.6014s/100 iter), loss = 1.64247
I0815 11:45:44.359696  8764 solver.cpp:334]     Train net output #0: loss = 1.77967 (* 1 = 1.77967 loss)
I0815 11:45:44.359705  8764 sgd_solver.cpp:136] Iteration 40600, lr = 0.0074625, m = 0.9
I0815 11:46:01.303287  8764 solver.cpp:312] Iteration 40700 (5.90209 iter/s, 16.9432s/100 iter), loss = 1.07761
I0815 11:46:01.303391  8764 solver.cpp:334]     Train net output #0: loss = 1.14018 (* 1 = 1.14018 loss)
I0815 11:46:01.303406  8764 sgd_solver.cpp:136] Iteration 40700, lr = 0.00745625, m = 0.9
I0815 11:46:19.363677  8764 solver.cpp:312] Iteration 40800 (5.53713 iter/s, 18.0599s/100 iter), loss = 1.55574
I0815 11:46:19.363756  8764 solver.cpp:334]     Train net output #0: loss = 1.55386 (* 1 = 1.55386 loss)
I0815 11:46:19.363776  8764 sgd_solver.cpp:136] Iteration 40800, lr = 0.00745, m = 0.9
I0815 11:46:36.668980  8764 solver.cpp:312] Iteration 40900 (5.77875 iter/s, 17.3048s/100 iter), loss = 1.55835
I0815 11:46:36.669160  8764 solver.cpp:334]     Train net output #0: loss = 1.52459 (* 1 = 1.52459 loss)
I0815 11:46:36.669201  8764 sgd_solver.cpp:136] Iteration 40900, lr = 0.00744375, m = 0.9
I0815 11:46:52.674320  8764 solver.cpp:363] Sparsity after update:
I0815 11:46:52.688282  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:46:52.688297  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:46:52.688305  8764 net.cpp:2192] conv1b_param_0(0.156) 
I0815 11:46:52.688309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:46:52.688325  8764 net.cpp:2192] res2a_branch2a_param_0(0.208) 
I0815 11:46:52.688336  8764 net.cpp:2192] res2a_branch2b_param_0(0.208) 
I0815 11:46:52.688344  8764 net.cpp:2192] res3a_branch2a_param_0(0.208) 
I0815 11:46:52.688354  8764 net.cpp:2192] res3a_branch2b_param_0(0.208) 
I0815 11:46:52.688361  8764 net.cpp:2192] res4a_branch2a_param_0(0.209) 
I0815 11:46:52.688369  8764 net.cpp:2192] res4a_branch2b_param_0(0.208) 
I0815 11:46:52.688376  8764 net.cpp:2192] res5a_branch2a_param_0(0.21) 
I0815 11:46:52.688385  8764 net.cpp:2192] res5a_branch2b_param_0(0.209) 
I0815 11:46:52.688392  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (492445/2.86678e+06) 0.172
I0815 11:46:52.815456  8794 solver.cpp:409] Finding and applying sparsity: 0.22
I0815 11:47:13.745668  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:47:13.747722  8764 solver.cpp:312] Iteration 41000 (2.69704 iter/s, 37.0777s/100 iter), loss = 0.992317
I0815 11:47:13.747745  8764 solver.cpp:334]     Train net output #0: loss = 1.01455 (* 1 = 1.01455 loss)
I0815 11:47:13.747753  8764 sgd_solver.cpp:136] Iteration 41000, lr = 0.0074375, m = 0.9
I0815 11:47:30.309731  8764 solver.cpp:312] Iteration 41100 (6.03809 iter/s, 16.5615s/100 iter), loss = 1.89137
I0815 11:47:30.309757  8764 solver.cpp:334]     Train net output #0: loss = 1.83254 (* 1 = 1.83254 loss)
I0815 11:47:30.309762  8764 sgd_solver.cpp:136] Iteration 41100, lr = 0.00743125, m = 0.9
I0815 11:47:47.053587  8764 solver.cpp:312] Iteration 41200 (5.97251 iter/s, 16.7434s/100 iter), loss = 1.5478
I0815 11:47:47.053683  8764 solver.cpp:334]     Train net output #0: loss = 1.43836 (* 1 = 1.43836 loss)
I0815 11:47:47.053700  8764 sgd_solver.cpp:136] Iteration 41200, lr = 0.007425, m = 0.9
I0815 11:48:04.258460  8764 solver.cpp:312] Iteration 41300 (5.81249 iter/s, 17.2043s/100 iter), loss = 1.29145
I0815 11:48:04.258544  8764 solver.cpp:334]     Train net output #0: loss = 1.4247 (* 1 = 1.4247 loss)
I0815 11:48:04.258563  8764 sgd_solver.cpp:136] Iteration 41300, lr = 0.00741875, m = 0.9
I0815 11:48:22.533788  8764 solver.cpp:312] Iteration 41400 (5.472 iter/s, 18.2748s/100 iter), loss = 1.51453
I0815 11:48:22.533840  8764 solver.cpp:334]     Train net output #0: loss = 1.44247 (* 1 = 1.44247 loss)
I0815 11:48:22.533848  8764 sgd_solver.cpp:136] Iteration 41400, lr = 0.0074125, m = 0.9
I0815 11:48:41.749469  8764 solver.cpp:312] Iteration 41500 (5.20423 iter/s, 19.2151s/100 iter), loss = 1.3972
I0815 11:48:41.749532  8764 solver.cpp:334]     Train net output #0: loss = 1.1877 (* 1 = 1.1877 loss)
I0815 11:48:41.749547  8764 sgd_solver.cpp:136] Iteration 41500, lr = 0.00740625, m = 0.9
I0815 11:49:03.237642  8764 solver.cpp:312] Iteration 41600 (4.65386 iter/s, 21.4875s/100 iter), loss = 1.49586
I0815 11:49:03.237778  8764 solver.cpp:334]     Train net output #0: loss = 1.11073 (* 1 = 1.11073 loss)
I0815 11:49:03.237793  8764 sgd_solver.cpp:136] Iteration 41600, lr = 0.0074, m = 0.9
I0815 11:49:19.571549  8764 solver.cpp:312] Iteration 41700 (6.1224 iter/s, 16.3335s/100 iter), loss = 1.2191
I0815 11:49:19.571614  8764 solver.cpp:334]     Train net output #0: loss = 1.11921 (* 1 = 1.11921 loss)
I0815 11:49:19.571635  8764 sgd_solver.cpp:136] Iteration 41700, lr = 0.00739375, m = 0.9
I0815 11:49:36.373278  8764 solver.cpp:312] Iteration 41800 (5.95193 iter/s, 16.8013s/100 iter), loss = 1.35527
I0815 11:49:36.373332  8764 solver.cpp:334]     Train net output #0: loss = 1.04712 (* 1 = 1.04712 loss)
I0815 11:49:36.373338  8764 sgd_solver.cpp:136] Iteration 41800, lr = 0.0073875, m = 0.9
I0815 11:49:52.429368  8764 solver.cpp:312] Iteration 41900 (6.22834 iter/s, 16.0556s/100 iter), loss = 1.98607
I0815 11:49:52.429395  8764 solver.cpp:334]     Train net output #0: loss = 2.54703 (* 1 = 2.54703 loss)
I0815 11:49:52.429400  8764 sgd_solver.cpp:136] Iteration 41900, lr = 0.00738125, m = 0.9
I0815 11:50:07.508283  8764 solver.cpp:363] Sparsity after update:
I0815 11:50:07.522851  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:50:07.522902  8764 net.cpp:2192] conv1a_param_0(0.107) 
I0815 11:50:07.522933  8764 net.cpp:2192] conv1b_param_0(0.156) 
I0815 11:50:07.522946  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:50:07.522958  8764 net.cpp:2192] res2a_branch2a_param_0(0.219) 
I0815 11:50:07.522970  8764 net.cpp:2192] res2a_branch2b_param_0(0.215) 
I0815 11:50:07.522984  8764 net.cpp:2192] res3a_branch2a_param_0(0.219) 
I0815 11:50:07.522996  8764 net.cpp:2192] res3a_branch2b_param_0(0.219) 
I0815 11:50:07.523005  8764 net.cpp:2192] res4a_branch2a_param_0(0.22) 
I0815 11:50:07.523017  8764 net.cpp:2192] res4a_branch2b_param_0(0.219) 
I0815 11:50:07.523026  8764 net.cpp:2192] res5a_branch2a_param_0(0.22) 
I0815 11:50:07.523041  8764 net.cpp:2192] res5a_branch2b_param_0(0.22) 
I0815 11:50:07.523048  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (516422/2.86678e+06) 0.18
I0815 11:50:07.523088  8764 solver.cpp:509] Iteration 42000, Testing net (#0)
I0815 11:50:31.705016  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571
I0815 11:50:31.705042  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805703
I0815 11:50:31.705049  8764 solver.cpp:594]     Test net output #2: loss = 1.8802 (* 1 = 1.8802 loss)
I0815 11:50:31.705096  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.1813s
I0815 11:50:31.853641  8794 solver.cpp:409] Finding and applying sparsity: 0.23
I0815 11:50:52.637635  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:50:52.639650  8764 solver.cpp:312] Iteration 42000 (1.66089 iter/s, 60.2086s/100 iter), loss = 1.27541
I0815 11:50:52.639673  8764 solver.cpp:334]     Train net output #0: loss = 1.28959 (* 1 = 1.28959 loss)
I0815 11:50:52.639683  8764 sgd_solver.cpp:136] Iteration 42000, lr = 0.007375, m = 0.9
I0815 11:51:07.841305  8764 solver.cpp:312] Iteration 42100 (6.57842 iter/s, 15.2012s/100 iter), loss = 1.00871
I0815 11:51:07.841332  8764 solver.cpp:334]     Train net output #0: loss = 1.25629 (* 1 = 1.25629 loss)
I0815 11:51:07.841338  8764 sgd_solver.cpp:136] Iteration 42100, lr = 0.00736875, m = 0.9
I0815 11:51:23.127192  8764 solver.cpp:312] Iteration 42200 (6.54217 iter/s, 15.2855s/100 iter), loss = 1.60824
I0815 11:51:23.127244  8764 solver.cpp:334]     Train net output #0: loss = 1.32156 (* 1 = 1.32156 loss)
I0815 11:51:23.127250  8764 sgd_solver.cpp:136] Iteration 42200, lr = 0.0073625, m = 0.9
I0815 11:51:39.580008  8764 solver.cpp:312] Iteration 42300 (6.07816 iter/s, 16.4524s/100 iter), loss = 1.12307
I0815 11:51:39.580041  8764 solver.cpp:334]     Train net output #0: loss = 1.27288 (* 1 = 1.27288 loss)
I0815 11:51:39.580047  8764 sgd_solver.cpp:136] Iteration 42300, lr = 0.00735625, m = 0.9
I0815 11:51:57.247968  8764 solver.cpp:312] Iteration 42400 (5.66012 iter/s, 17.6675s/100 iter), loss = 1.77322
I0815 11:51:57.248028  8764 solver.cpp:334]     Train net output #0: loss = 1.85288 (* 1 = 1.85288 loss)
I0815 11:51:57.248036  8764 sgd_solver.cpp:136] Iteration 42400, lr = 0.00735, m = 0.9
I0815 11:52:13.072741  8764 solver.cpp:312] Iteration 42500 (6.31938 iter/s, 15.8243s/100 iter), loss = 1.26093
I0815 11:52:13.072818  8764 solver.cpp:334]     Train net output #0: loss = 1.0852 (* 1 = 1.0852 loss)
I0815 11:52:13.072837  8764 sgd_solver.cpp:136] Iteration 42500, lr = 0.00734375, m = 0.9
I0815 11:52:27.530305  8764 solver.cpp:312] Iteration 42600 (6.91699 iter/s, 14.4572s/100 iter), loss = 1.31173
I0815 11:52:27.530365  8764 solver.cpp:334]     Train net output #0: loss = 1.06282 (* 1 = 1.06282 loss)
I0815 11:52:27.530371  8764 sgd_solver.cpp:136] Iteration 42600, lr = 0.0073375, m = 0.9
I0815 11:52:42.235016  8764 solver.cpp:312] Iteration 42700 (6.80073 iter/s, 14.7043s/100 iter), loss = 1.56785
I0815 11:52:42.235041  8764 solver.cpp:334]     Train net output #0: loss = 1.66233 (* 1 = 1.66233 loss)
I0815 11:52:42.235047  8764 sgd_solver.cpp:136] Iteration 42700, lr = 0.00733125, m = 0.9
I0815 11:52:59.723587  8764 solver.cpp:312] Iteration 42800 (5.71818 iter/s, 17.4881s/100 iter), loss = 1.04003
I0815 11:52:59.723685  8764 solver.cpp:334]     Train net output #0: loss = 0.698877 (* 1 = 0.698877 loss)
I0815 11:52:59.723704  8764 sgd_solver.cpp:136] Iteration 42800, lr = 0.007325, m = 0.9
I0815 11:53:15.506523  8764 solver.cpp:312] Iteration 42900 (6.33613 iter/s, 15.7825s/100 iter), loss = 1.32275
I0815 11:53:15.506585  8764 solver.cpp:334]     Train net output #0: loss = 1.43373 (* 1 = 1.43373 loss)
I0815 11:53:15.506608  8764 sgd_solver.cpp:136] Iteration 42900, lr = 0.00731875, m = 0.9
I0815 11:53:31.004928  8764 solver.cpp:363] Sparsity after update:
I0815 11:53:31.018941  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:53:31.018988  8764 net.cpp:2192] conv1a_param_0(0.107) 
I0815 11:53:31.019006  8764 net.cpp:2192] conv1b_param_0(0.167) 
I0815 11:53:31.019019  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:53:31.019032  8764 net.cpp:2192] res2a_branch2a_param_0(0.229) 
I0815 11:53:31.019044  8764 net.cpp:2192] res2a_branch2b_param_0(0.229) 
I0815 11:53:31.019057  8764 net.cpp:2192] res3a_branch2a_param_0(0.229) 
I0815 11:53:31.019068  8764 net.cpp:2192] res3a_branch2b_param_0(0.229) 
I0815 11:53:31.019080  8764 net.cpp:2192] res4a_branch2a_param_0(0.229) 
I0815 11:53:31.019091  8764 net.cpp:2192] res4a_branch2b_param_0(0.229) 
I0815 11:53:31.019109  8764 net.cpp:2192] res5a_branch2a_param_0(0.23) 
I0815 11:53:31.019122  8764 net.cpp:2192] res5a_branch2b_param_0(0.229) 
I0815 11:53:31.019134  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (539669/2.86678e+06) 0.188
I0815 11:53:31.152777  8794 solver.cpp:409] Finding and applying sparsity: 0.24
I0815 11:53:53.399744  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:53:53.401758  8764 solver.cpp:312] Iteration 43000 (2.63893 iter/s, 37.8942s/100 iter), loss = 1.28148
I0815 11:53:53.401777  8764 solver.cpp:334]     Train net output #0: loss = 1.01363 (* 1 = 1.01363 loss)
I0815 11:53:53.401783  8764 sgd_solver.cpp:136] Iteration 43000, lr = 0.0073125, m = 0.9
I0815 11:54:09.859926  8764 solver.cpp:312] Iteration 43100 (6.07618 iter/s, 16.4577s/100 iter), loss = 1.09728
I0815 11:54:09.859997  8764 solver.cpp:334]     Train net output #0: loss = 1.28057 (* 1 = 1.28057 loss)
I0815 11:54:09.860007  8764 sgd_solver.cpp:136] Iteration 43100, lr = 0.00730625, m = 0.9
I0815 11:54:26.898460  8764 solver.cpp:312] Iteration 43200 (5.86922 iter/s, 17.0381s/100 iter), loss = 1.57557
I0815 11:54:26.898504  8764 solver.cpp:334]     Train net output #0: loss = 1.90334 (* 1 = 1.90334 loss)
I0815 11:54:26.898512  8764 sgd_solver.cpp:136] Iteration 43200, lr = 0.0073, m = 0.9
I0815 11:54:44.432389  8764 solver.cpp:312] Iteration 43300 (5.70339 iter/s, 17.5334s/100 iter), loss = 1.02392
I0815 11:54:44.432456  8764 solver.cpp:334]     Train net output #0: loss = 1.20143 (* 1 = 1.20143 loss)
I0815 11:54:44.432462  8764 sgd_solver.cpp:136] Iteration 43300, lr = 0.00729375, m = 0.9
I0815 11:54:59.163769  8764 solver.cpp:312] Iteration 43400 (6.78842 iter/s, 14.731s/100 iter), loss = 1.28839
I0815 11:54:59.163803  8764 solver.cpp:334]     Train net output #0: loss = 1.4176 (* 1 = 1.4176 loss)
I0815 11:54:59.163810  8764 sgd_solver.cpp:136] Iteration 43400, lr = 0.0072875, m = 0.9
I0815 11:55:16.026682  8764 solver.cpp:312] Iteration 43500 (5.93034 iter/s, 16.8624s/100 iter), loss = 1.52607
I0815 11:55:16.026780  8764 solver.cpp:334]     Train net output #0: loss = 1.57361 (* 1 = 1.57361 loss)
I0815 11:55:16.026799  8764 sgd_solver.cpp:136] Iteration 43500, lr = 0.00728125, m = 0.9
I0815 11:55:34.197878  8764 solver.cpp:312] Iteration 43600 (5.50337 iter/s, 18.1707s/100 iter), loss = 1.59554
I0815 11:55:34.197971  8764 solver.cpp:334]     Train net output #0: loss = 1.33383 (* 1 = 1.33383 loss)
I0815 11:55:34.198009  8764 sgd_solver.cpp:136] Iteration 43600, lr = 0.007275, m = 0.9
I0815 11:55:50.861923  8764 solver.cpp:312] Iteration 43700 (6.00111 iter/s, 16.6636s/100 iter), loss = 1.92091
I0815 11:55:50.862023  8764 solver.cpp:334]     Train net output #0: loss = 1.59046 (* 1 = 1.59046 loss)
I0815 11:55:50.862041  8764 sgd_solver.cpp:136] Iteration 43700, lr = 0.00726875, m = 0.9
I0815 11:56:07.861461  8764 solver.cpp:312] Iteration 43800 (5.88268 iter/s, 16.9991s/100 iter), loss = 1.42534
I0815 11:56:07.861505  8764 solver.cpp:334]     Train net output #0: loss = 1.89759 (* 1 = 1.89759 loss)
I0815 11:56:07.861519  8764 sgd_solver.cpp:136] Iteration 43800, lr = 0.0072625, m = 0.9
I0815 11:56:24.440541  8764 solver.cpp:312] Iteration 43900 (6.03186 iter/s, 16.5786s/100 iter), loss = 1.72971
I0815 11:56:24.440632  8764 solver.cpp:334]     Train net output #0: loss = 2.16681 (* 1 = 2.16681 loss)
I0815 11:56:24.440652  8764 sgd_solver.cpp:136] Iteration 43900, lr = 0.00725625, m = 0.9
I0815 11:56:39.350481  8764 solver.cpp:363] Sparsity after update:
I0815 11:56:39.354569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:56:39.354600  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 11:56:39.354620  8764 net.cpp:2192] conv1b_param_0(0.177) 
I0815 11:56:39.354632  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:56:39.354645  8764 net.cpp:2192] res2a_branch2a_param_0(0.24) 
I0815 11:56:39.354657  8764 net.cpp:2192] res2a_branch2b_param_0(0.236) 
I0815 11:56:39.354668  8764 net.cpp:2192] res3a_branch2a_param_0(0.24) 
I0815 11:56:39.354681  8764 net.cpp:2192] res3a_branch2b_param_0(0.24) 
I0815 11:56:39.354697  8764 net.cpp:2192] res4a_branch2a_param_0(0.24) 
I0815 11:56:39.354709  8764 net.cpp:2192] res4a_branch2b_param_0(0.24) 
I0815 11:56:39.354720  8764 net.cpp:2192] res5a_branch2a_param_0(0.24) 
I0815 11:56:39.354732  8764 net.cpp:2192] res5a_branch2b_param_0(0.24) 
I0815 11:56:39.354744  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (563662/2.86678e+06) 0.197
I0815 11:56:39.354763  8764 solver.cpp:509] Iteration 44000, Testing net (#0)
I0815 11:56:44.103063  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:57:00.994954  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.555882
I0815 11:57:00.995079  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790762
I0815 11:57:00.995087  8764 solver.cpp:594]     Test net output #2: loss = 1.99448 (* 1 = 1.99448 loss)
I0815 11:57:00.995108  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.6397s
I0815 11:57:01.143477  8794 solver.cpp:409] Finding and applying sparsity: 0.25
I0815 11:57:22.724108  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:57:22.726094  8764 solver.cpp:312] Iteration 44000 (1.71574 iter/s, 58.284s/100 iter), loss = 1.45532
I0815 11:57:22.726109  8764 solver.cpp:334]     Train net output #0: loss = 1.69752 (* 1 = 1.69752 loss)
I0815 11:57:22.726114  8764 sgd_solver.cpp:136] Iteration 44000, lr = 0.00725, m = 0.9
I0815 11:57:38.666327  8764 solver.cpp:312] Iteration 44100 (6.27361 iter/s, 15.9398s/100 iter), loss = 1.26984
I0815 11:57:38.666404  8764 solver.cpp:334]     Train net output #0: loss = 1.48498 (* 1 = 1.48498 loss)
I0815 11:57:38.666422  8764 sgd_solver.cpp:136] Iteration 44100, lr = 0.00724375, m = 0.9
I0815 11:57:53.387658  8764 solver.cpp:312] Iteration 44200 (6.79305 iter/s, 14.7209s/100 iter), loss = 1.3449
I0815 11:57:53.387686  8764 solver.cpp:334]     Train net output #0: loss = 1.54304 (* 1 = 1.54304 loss)
I0815 11:57:53.387692  8764 sgd_solver.cpp:136] Iteration 44200, lr = 0.0072375, m = 0.9
I0815 11:58:07.885869  8764 solver.cpp:312] Iteration 44300 (6.8976 iter/s, 14.4978s/100 iter), loss = 1.61303
I0815 11:58:07.885939  8764 solver.cpp:334]     Train net output #0: loss = 1.28602 (* 1 = 1.28602 loss)
I0815 11:58:07.885957  8764 sgd_solver.cpp:136] Iteration 44300, lr = 0.00723125, m = 0.9
I0815 11:58:25.124315  8764 solver.cpp:312] Iteration 44400 (5.80115 iter/s, 17.238s/100 iter), loss = 1.42898
I0815 11:58:25.124402  8764 solver.cpp:334]     Train net output #0: loss = 1.55798 (* 1 = 1.55798 loss)
I0815 11:58:25.124415  8764 sgd_solver.cpp:136] Iteration 44400, lr = 0.007225, m = 0.9
I0815 11:58:42.120970  8764 solver.cpp:312] Iteration 44500 (5.88367 iter/s, 16.9962s/100 iter), loss = 1.67365
I0815 11:58:42.120999  8764 solver.cpp:334]     Train net output #0: loss = 1.4465 (* 1 = 1.4465 loss)
I0815 11:58:42.121006  8764 sgd_solver.cpp:136] Iteration 44500, lr = 0.00721875, m = 0.9
I0815 11:58:59.736941  8764 solver.cpp:312] Iteration 44600 (5.67682 iter/s, 17.6155s/100 iter), loss = 1.72893
I0815 11:58:59.737001  8764 solver.cpp:334]     Train net output #0: loss = 1.53836 (* 1 = 1.53836 loss)
I0815 11:58:59.737007  8764 sgd_solver.cpp:136] Iteration 44600, lr = 0.0072125, m = 0.9
I0815 11:59:14.823428  8726 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 11:59:16.414969  8764 solver.cpp:312] Iteration 44700 (5.99608 iter/s, 16.6776s/100 iter), loss = 1.29547
I0815 11:59:16.414995  8764 solver.cpp:334]     Train net output #0: loss = 1.35022 (* 1 = 1.35022 loss)
I0815 11:59:16.414999  8764 sgd_solver.cpp:136] Iteration 44700, lr = 0.00720625, m = 0.9
I0815 11:59:31.003136  8764 solver.cpp:312] Iteration 44800 (6.85506 iter/s, 14.5878s/100 iter), loss = 1.78236
I0815 11:59:31.003309  8764 solver.cpp:334]     Train net output #0: loss = 1.87248 (* 1 = 1.87248 loss)
I0815 11:59:31.003330  8764 sgd_solver.cpp:136] Iteration 44800, lr = 0.0072, m = 0.9
I0815 11:59:46.616648  8764 solver.cpp:312] Iteration 44900 (6.40489 iter/s, 15.6131s/100 iter), loss = 1.4337
I0815 11:59:46.616677  8764 solver.cpp:334]     Train net output #0: loss = 1.78207 (* 1 = 1.78207 loss)
I0815 11:59:46.616683  8764 sgd_solver.cpp:136] Iteration 44900, lr = 0.00719375, m = 0.9
I0815 12:00:01.080303  8764 solver.cpp:363] Sparsity after update:
I0815 12:00:01.094779  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:00:01.094938  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 12:00:01.095036  8764 net.cpp:2192] conv1b_param_0(0.188) 
I0815 12:00:01.095130  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:00:01.095221  8764 net.cpp:2192] res2a_branch2a_param_0(0.25) 
I0815 12:00:01.095310  8764 net.cpp:2192] res2a_branch2b_param_0(0.25) 
I0815 12:00:01.095401  8764 net.cpp:2192] res3a_branch2a_param_0(0.25) 
I0815 12:00:01.095489  8764 net.cpp:2192] res3a_branch2b_param_0(0.25) 
I0815 12:00:01.095577  8764 net.cpp:2192] res4a_branch2a_param_0(0.25) 
I0815 12:00:01.095665  8764 net.cpp:2192] res4a_branch2b_param_0(0.25) 
I0815 12:00:01.095755  8764 net.cpp:2192] res5a_branch2a_param_0(0.25) 
I0815 12:00:01.095844  8764 net.cpp:2192] res5a_branch2b_param_0(0.25) 
I0815 12:00:01.095934  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (588214/2.86678e+06) 0.205
I0815 12:00:01.251045  8794 solver.cpp:409] Finding and applying sparsity: 0.26
I0815 12:00:22.645915  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:00:22.647920  8764 solver.cpp:312] Iteration 45000 (2.77544 iter/s, 36.0303s/100 iter), loss = 1.56589
I0815 12:00:22.647946  8764 solver.cpp:334]     Train net output #0: loss = 1.8113 (* 1 = 1.8113 loss)
I0815 12:00:22.647956  8764 sgd_solver.cpp:136] Iteration 45000, lr = 0.0071875, m = 0.9
I0815 12:00:37.819586  8764 solver.cpp:312] Iteration 45100 (6.59142 iter/s, 15.1712s/100 iter), loss = 1.5246
I0815 12:00:37.819845  8764 solver.cpp:334]     Train net output #0: loss = 1.83433 (* 1 = 1.83433 loss)
I0815 12:00:37.819953  8764 sgd_solver.cpp:136] Iteration 45100, lr = 0.00718125, m = 0.9
I0815 12:00:53.750051  8764 solver.cpp:312] Iteration 45200 (6.27746 iter/s, 15.93s/100 iter), loss = 1.55649
I0815 12:00:53.750102  8764 solver.cpp:334]     Train net output #0: loss = 1.27667 (* 1 = 1.27667 loss)
I0815 12:00:53.750115  8764 sgd_solver.cpp:136] Iteration 45200, lr = 0.007175, m = 0.9
I0815 12:01:09.099642  8764 solver.cpp:312] Iteration 45300 (6.51501 iter/s, 15.3492s/100 iter), loss = 1.46779
I0815 12:01:09.099716  8764 solver.cpp:334]     Train net output #0: loss = 1.38662 (* 1 = 1.38662 loss)
I0815 12:01:09.099728  8764 sgd_solver.cpp:136] Iteration 45300, lr = 0.00716875, m = 0.9
I0815 12:01:24.613761  8764 solver.cpp:312] Iteration 45400 (6.44592 iter/s, 15.5137s/100 iter), loss = 1.49828
I0815 12:01:24.613790  8764 solver.cpp:334]     Train net output #0: loss = 1.49396 (* 1 = 1.49396 loss)
I0815 12:01:24.613793  8764 sgd_solver.cpp:136] Iteration 45400, lr = 0.0071625, m = 0.9
I0815 12:01:40.074525  8764 solver.cpp:312] Iteration 45500 (6.46816 iter/s, 15.4603s/100 iter), loss = 1.60593
I0815 12:01:40.074600  8764 solver.cpp:334]     Train net output #0: loss = 1.9962 (* 1 = 1.9962 loss)
I0815 12:01:40.074614  8764 sgd_solver.cpp:136] Iteration 45500, lr = 0.00715625, m = 0.9
I0815 12:01:55.151181  8764 solver.cpp:312] Iteration 45600 (6.63295 iter/s, 15.0762s/100 iter), loss = 1.1566
I0815 12:01:55.151209  8764 solver.cpp:334]     Train net output #0: loss = 1.07997 (* 1 = 1.07997 loss)
I0815 12:01:55.151214  8764 sgd_solver.cpp:136] Iteration 45600, lr = 0.00715, m = 0.9
I0815 12:02:10.114506  8764 solver.cpp:312] Iteration 45700 (6.68319 iter/s, 14.9629s/100 iter), loss = 1.15881
I0815 12:02:10.116245  8764 solver.cpp:334]     Train net output #0: loss = 1.461 (* 1 = 1.461 loss)
I0815 12:02:10.116261  8764 sgd_solver.cpp:136] Iteration 45700, lr = 0.00714375, m = 0.9
I0815 12:02:27.248343  8764 solver.cpp:312] Iteration 45800 (5.83657 iter/s, 17.1334s/100 iter), loss = 1.54467
I0815 12:02:27.248591  8764 solver.cpp:334]     Train net output #0: loss = 1.6908 (* 1 = 1.6908 loss)
I0815 12:02:27.248716  8764 sgd_solver.cpp:136] Iteration 45800, lr = 0.0071375, m = 0.9
I0815 12:02:44.970896  8764 solver.cpp:312] Iteration 45900 (5.64268 iter/s, 17.7221s/100 iter), loss = 1.25274
I0815 12:02:44.971011  8764 solver.cpp:334]     Train net output #0: loss = 1.21828 (* 1 = 1.21828 loss)
I0815 12:02:44.971031  8764 sgd_solver.cpp:136] Iteration 45900, lr = 0.00713125, m = 0.9
I0815 12:03:03.341490  8764 solver.cpp:363] Sparsity after update:
I0815 12:03:03.348592  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:03:03.348608  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 12:03:03.348615  8764 net.cpp:2192] conv1b_param_0(0.188) 
I0815 12:03:03.348619  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:03:03.348623  8764 net.cpp:2192] res2a_branch2a_param_0(0.257) 
I0815 12:03:03.348625  8764 net.cpp:2192] res2a_branch2b_param_0(0.257) 
I0815 12:03:03.348628  8764 net.cpp:2192] res3a_branch2a_param_0(0.259) 
I0815 12:03:03.348634  8764 net.cpp:2192] res3a_branch2b_param_0(0.257) 
I0815 12:03:03.348636  8764 net.cpp:2192] res4a_branch2a_param_0(0.26) 
I0815 12:03:03.348639  8764 net.cpp:2192] res4a_branch2b_param_0(0.259) 
I0815 12:03:03.348642  8764 net.cpp:2192] res5a_branch2a_param_0(0.26) 
I0815 12:03:03.348646  8764 net.cpp:2192] res5a_branch2b_param_0(0.26) 
I0815 12:03:03.348649  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (610795/2.86678e+06) 0.213
I0815 12:03:03.348686  8764 solver.cpp:509] Iteration 46000, Testing net (#0)
I0815 12:03:25.667194  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.563118
I0815 12:03:25.667282  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.797527
I0815 12:03:25.667291  8764 solver.cpp:594]     Test net output #2: loss = 1.92302 (* 1 = 1.92302 loss)
I0815 12:03:25.667312  8764 solver.cpp:264] [MultiGPU] Tests completed in 22.318s
I0815 12:03:25.824658  8794 solver.cpp:409] Finding and applying sparsity: 0.27
I0815 12:03:47.631114  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:03:47.633178  8764 solver.cpp:312] Iteration 46000 (1.5959 iter/s, 62.6606s/100 iter), loss = 1.62186
I0815 12:03:47.633196  8764 solver.cpp:334]     Train net output #0: loss = 1.45711 (* 1 = 1.45711 loss)
I0815 12:03:47.633205  8764 sgd_solver.cpp:136] Iteration 46000, lr = 0.007125, m = 0.9
I0815 12:04:05.362973  8764 solver.cpp:312] Iteration 46100 (5.64038 iter/s, 17.7293s/100 iter), loss = 1.52139
I0815 12:04:05.363056  8764 solver.cpp:334]     Train net output #0: loss = 1.42839 (* 1 = 1.42839 loss)
I0815 12:04:05.363071  8764 sgd_solver.cpp:136] Iteration 46100, lr = 0.00711875, m = 0.9
I0815 12:04:22.494807  8764 solver.cpp:312] Iteration 46200 (5.83725 iter/s, 17.1313s/100 iter), loss = 0.994209
I0815 12:04:22.494833  8764 solver.cpp:334]     Train net output #0: loss = 0.684631 (* 1 = 0.684631 loss)
I0815 12:04:22.494839  8764 sgd_solver.cpp:136] Iteration 46200, lr = 0.0071125, m = 0.9
I0815 12:04:37.270679  8764 solver.cpp:312] Iteration 46300 (6.76798 iter/s, 14.7755s/100 iter), loss = 1.18022
I0815 12:04:37.270730  8764 solver.cpp:334]     Train net output #0: loss = 1.06926 (* 1 = 1.06926 loss)
I0815 12:04:37.270736  8764 sgd_solver.cpp:136] Iteration 46300, lr = 0.00710625, m = 0.9
I0815 12:04:52.719944  8764 solver.cpp:312] Iteration 46400 (6.47298 iter/s, 15.4488s/100 iter), loss = 1.37574
I0815 12:04:52.719996  8764 solver.cpp:334]     Train net output #0: loss = 1.06927 (* 1 = 1.06927 loss)
I0815 12:04:52.720016  8764 sgd_solver.cpp:136] Iteration 46400, lr = 0.0071, m = 0.9
I0815 12:05:08.542891  8764 solver.cpp:312] Iteration 46500 (6.32011 iter/s, 15.8225s/100 iter), loss = 1.55071
I0815 12:05:08.542992  8764 solver.cpp:334]     Train net output #0: loss = 1.73957 (* 1 = 1.73957 loss)
I0815 12:05:08.543007  8764 sgd_solver.cpp:136] Iteration 46500, lr = 0.00709375, m = 0.9
I0815 12:05:26.504575  8764 solver.cpp:312] Iteration 46600 (5.56756 iter/s, 17.9612s/100 iter), loss = 1.24983
I0815 12:05:26.504602  8764 solver.cpp:334]     Train net output #0: loss = 0.944877 (* 1 = 0.944877 loss)
I0815 12:05:26.504609  8764 sgd_solver.cpp:136] Iteration 46600, lr = 0.0070875, m = 0.9
I0815 12:05:41.608597  8764 solver.cpp:312] Iteration 46700 (6.62094 iter/s, 15.1036s/100 iter), loss = 1.85067
I0815 12:05:41.608657  8764 solver.cpp:334]     Train net output #0: loss = 1.5561 (* 1 = 1.5561 loss)
I0815 12:05:41.608664  8764 sgd_solver.cpp:136] Iteration 46700, lr = 0.00708125, m = 0.9
I0815 12:05:56.106071  8764 solver.cpp:312] Iteration 46800 (6.89794 iter/s, 14.4971s/100 iter), loss = 1.5578
I0815 12:05:56.106099  8764 solver.cpp:334]     Train net output #0: loss = 1.83319 (* 1 = 1.83319 loss)
I0815 12:05:56.106106  8764 sgd_solver.cpp:136] Iteration 46800, lr = 0.007075, m = 0.9
I0815 12:06:10.689889  8764 solver.cpp:312] Iteration 46900 (6.85711 iter/s, 14.5834s/100 iter), loss = 1.61098
I0815 12:06:10.689914  8764 solver.cpp:334]     Train net output #0: loss = 1.77921 (* 1 = 1.77921 loss)
I0815 12:06:10.689919  8764 sgd_solver.cpp:136] Iteration 46900, lr = 0.00706875, m = 0.9
I0815 12:06:26.741771  8764 solver.cpp:363] Sparsity after update:
I0815 12:06:26.754218  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:06:26.754250  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:06:26.754266  8764 net.cpp:2192] conv1b_param_0(0.198) 
I0815 12:06:26.754274  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:06:26.754284  8764 net.cpp:2192] res2a_branch2a_param_0(0.267) 
I0815 12:06:26.754293  8764 net.cpp:2192] res2a_branch2b_param_0(0.264) 
I0815 12:06:26.754302  8764 net.cpp:2192] res3a_branch2a_param_0(0.269) 
I0815 12:06:26.754312  8764 net.cpp:2192] res3a_branch2b_param_0(0.267) 
I0815 12:06:26.754320  8764 net.cpp:2192] res4a_branch2a_param_0(0.27) 
I0815 12:06:26.754329  8764 net.cpp:2192] res4a_branch2b_param_0(0.269) 
I0815 12:06:26.754338  8764 net.cpp:2192] res5a_branch2a_param_0(0.27) 
I0815 12:06:26.754346  8764 net.cpp:2192] res5a_branch2b_param_0(0.27) 
I0815 12:06:26.754356  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (634803/2.86678e+06) 0.221
I0815 12:06:26.899572  8794 solver.cpp:409] Finding and applying sparsity: 0.28
I0815 12:06:48.919200  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:06:48.921195  8764 solver.cpp:312] Iteration 47000 (2.61573 iter/s, 38.2303s/100 iter), loss = 1.54641
I0815 12:06:48.921217  8764 solver.cpp:334]     Train net output #0: loss = 1.16724 (* 1 = 1.16724 loss)
I0815 12:06:48.921226  8764 sgd_solver.cpp:136] Iteration 47000, lr = 0.0070625, m = 0.9
I0815 12:07:03.850061  8764 solver.cpp:312] Iteration 47100 (6.69862 iter/s, 14.9284s/100 iter), loss = 1.71577
I0815 12:07:03.852161  8764 solver.cpp:334]     Train net output #0: loss = 1.92475 (* 1 = 1.92475 loss)
I0815 12:07:03.852169  8764 sgd_solver.cpp:136] Iteration 47100, lr = 0.00705625, m = 0.9
I0815 12:07:20.646836  8764 solver.cpp:312] Iteration 47200 (5.95369 iter/s, 16.7963s/100 iter), loss = 1.53251
I0815 12:07:20.646862  8764 solver.cpp:334]     Train net output #0: loss = 1.37507 (* 1 = 1.37507 loss)
I0815 12:07:20.646867  8764 sgd_solver.cpp:136] Iteration 47200, lr = 0.00705, m = 0.9
I0815 12:07:37.345934  8764 solver.cpp:312] Iteration 47300 (5.98851 iter/s, 16.6986s/100 iter), loss = 1.16392
I0815 12:07:37.346020  8764 solver.cpp:334]     Train net output #0: loss = 1.206 (* 1 = 1.206 loss)
I0815 12:07:37.346035  8764 sgd_solver.cpp:136] Iteration 47300, lr = 0.00704375, m = 0.9
I0815 12:07:55.026607  8764 solver.cpp:312] Iteration 47400 (5.65605 iter/s, 17.6802s/100 iter), loss = 1.21349
I0815 12:07:55.026630  8764 solver.cpp:334]     Train net output #0: loss = 1.22781 (* 1 = 1.22781 loss)
I0815 12:07:55.026636  8764 sgd_solver.cpp:136] Iteration 47400, lr = 0.0070375, m = 0.9
I0815 12:08:11.181463  8764 solver.cpp:312] Iteration 47500 (6.19026 iter/s, 16.1544s/100 iter), loss = 1.30624
I0815 12:08:11.181545  8764 solver.cpp:334]     Train net output #0: loss = 1.41948 (* 1 = 1.41948 loss)
I0815 12:08:11.181550  8764 sgd_solver.cpp:136] Iteration 47500, lr = 0.00703125, m = 0.9
I0815 12:08:27.219866  8764 solver.cpp:312] Iteration 47600 (6.23521 iter/s, 16.038s/100 iter), loss = 1.82261
I0815 12:08:27.219894  8764 solver.cpp:334]     Train net output #0: loss = 1.90741 (* 1 = 1.90741 loss)
I0815 12:08:27.219898  8764 sgd_solver.cpp:136] Iteration 47600, lr = 0.007025, m = 0.9
I0815 12:08:44.710924  8764 solver.cpp:312] Iteration 47700 (5.71737 iter/s, 17.4906s/100 iter), loss = 1.38151
I0815 12:08:44.711052  8764 solver.cpp:334]     Train net output #0: loss = 1.37571 (* 1 = 1.37571 loss)
I0815 12:08:44.711067  8764 sgd_solver.cpp:136] Iteration 47700, lr = 0.00701875, m = 0.9
I0815 12:09:00.969638  8764 solver.cpp:312] Iteration 47800 (6.15072 iter/s, 16.2583s/100 iter), loss = 1.02217
I0815 12:09:00.969704  8764 solver.cpp:334]     Train net output #0: loss = 0.909984 (* 1 = 0.909984 loss)
I0815 12:09:00.969722  8764 sgd_solver.cpp:136] Iteration 47800, lr = 0.0070125, m = 0.9
I0815 12:09:17.128612  8764 solver.cpp:312] Iteration 47900 (6.18868 iter/s, 16.1585s/100 iter), loss = 1.53592
I0815 12:09:17.128672  8764 solver.cpp:334]     Train net output #0: loss = 2.12751 (* 1 = 2.12751 loss)
I0815 12:09:17.128679  8764 sgd_solver.cpp:136] Iteration 47900, lr = 0.00700625, m = 0.9
I0815 12:09:34.932883  8764 solver.cpp:363] Sparsity after update:
I0815 12:09:34.936831  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:09:34.936841  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:09:34.936849  8764 net.cpp:2192] conv1b_param_0(0.208) 
I0815 12:09:34.936853  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:09:34.936858  8764 net.cpp:2192] res2a_branch2a_param_0(0.278) 
I0815 12:09:34.936862  8764 net.cpp:2192] res2a_branch2b_param_0(0.278) 
I0815 12:09:34.936866  8764 net.cpp:2192] res3a_branch2a_param_0(0.28) 
I0815 12:09:34.936872  8764 net.cpp:2192] res3a_branch2b_param_0(0.278) 
I0815 12:09:34.936875  8764 net.cpp:2192] res4a_branch2a_param_0(0.28) 
I0815 12:09:34.936880  8764 net.cpp:2192] res4a_branch2b_param_0(0.28) 
I0815 12:09:34.936884  8764 net.cpp:2192] res5a_branch2a_param_0(0.28) 
I0815 12:09:34.936888  8764 net.cpp:2192] res5a_branch2b_param_0(0.279) 
I0815 12:09:34.936892  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (658049/2.86678e+06) 0.23
I0815 12:09:34.936903  8764 solver.cpp:509] Iteration 48000, Testing net (#0)
I0815 12:09:51.351804  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:09:58.606039  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.564765
I0815 12:09:58.606058  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.799233
I0815 12:09:58.606063  8764 solver.cpp:594]     Test net output #2: loss = 1.92158 (* 1 = 1.92158 loss)
I0815 12:09:58.606128  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.6686s
I0815 12:09:58.741977  8794 solver.cpp:409] Finding and applying sparsity: 0.29
I0815 12:10:20.740388  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:10:20.742483  8764 solver.cpp:312] Iteration 48000 (1.57203 iter/s, 63.6121s/100 iter), loss = 1.49832
I0815 12:10:20.742501  8764 solver.cpp:334]     Train net output #0: loss = 1.65855 (* 1 = 1.65855 loss)
I0815 12:10:20.742506  8764 sgd_solver.cpp:136] Iteration 48000, lr = 0.007, m = 0.9
I0815 12:10:39.106744  8764 solver.cpp:312] Iteration 48100 (5.44551 iter/s, 18.3637s/100 iter), loss = 1.57247
I0815 12:10:39.106819  8764 solver.cpp:334]     Train net output #0: loss = 2.05396 (* 1 = 2.05396 loss)
I0815 12:10:39.106832  8764 sgd_solver.cpp:136] Iteration 48100, lr = 0.00699375, m = 0.9
I0815 12:10:54.151089  8764 solver.cpp:312] Iteration 48200 (6.6472 iter/s, 15.0439s/100 iter), loss = 1.5505
I0815 12:10:54.151141  8764 solver.cpp:334]     Train net output #0: loss = 1.82996 (* 1 = 1.82996 loss)
I0815 12:10:54.151154  8764 sgd_solver.cpp:136] Iteration 48200, lr = 0.0069875, m = 0.9
I0815 12:11:10.475033  8764 solver.cpp:312] Iteration 48300 (6.12614 iter/s, 16.3235s/100 iter), loss = 1.365
I0815 12:11:10.475114  8764 solver.cpp:334]     Train net output #0: loss = 1.4657 (* 1 = 1.4657 loss)
I0815 12:11:10.475131  8764 sgd_solver.cpp:136] Iteration 48300, lr = 0.00698125, m = 0.9
I0815 12:11:26.591536  8764 solver.cpp:312] Iteration 48400 (6.205 iter/s, 16.116s/100 iter), loss = 1.64312
I0815 12:11:26.591594  8764 solver.cpp:334]     Train net output #0: loss = 1.29514 (* 1 = 1.29514 loss)
I0815 12:11:26.591708  8764 sgd_solver.cpp:136] Iteration 48400, lr = 0.006975, m = 0.9
I0815 12:11:41.945718  8764 solver.cpp:312] Iteration 48500 (6.51306 iter/s, 15.3538s/100 iter), loss = 1.38458
I0815 12:11:41.945825  8764 solver.cpp:334]     Train net output #0: loss = 1.26828 (* 1 = 1.26828 loss)
I0815 12:11:41.945844  8764 sgd_solver.cpp:136] Iteration 48500, lr = 0.00696875, m = 0.9
I0815 12:11:56.485824  8764 solver.cpp:312] Iteration 48600 (6.87772 iter/s, 14.5397s/100 iter), loss = 1.25453
I0815 12:11:56.485852  8764 solver.cpp:334]     Train net output #0: loss = 1.40803 (* 1 = 1.40803 loss)
I0815 12:11:56.485855  8764 sgd_solver.cpp:136] Iteration 48600, lr = 0.0069625, m = 0.9
I0815 12:12:11.100008  8764 solver.cpp:312] Iteration 48700 (6.84286 iter/s, 14.6138s/100 iter), loss = 1.53765
I0815 12:12:11.100039  8764 solver.cpp:334]     Train net output #0: loss = 1.34266 (* 1 = 1.34266 loss)
I0815 12:12:11.100078  8764 sgd_solver.cpp:136] Iteration 48700, lr = 0.00695625, m = 0.9
I0815 12:12:26.160913  8764 solver.cpp:312] Iteration 48800 (6.63989 iter/s, 15.0605s/100 iter), loss = 1.53279
I0815 12:12:26.160969  8764 solver.cpp:334]     Train net output #0: loss = 1.73425 (* 1 = 1.73425 loss)
I0815 12:12:26.160975  8764 sgd_solver.cpp:136] Iteration 48800, lr = 0.00695, m = 0.9
I0815 12:12:42.156818  8764 solver.cpp:312] Iteration 48900 (6.25177 iter/s, 15.9955s/100 iter), loss = 1.48683
I0815 12:12:42.156844  8764 solver.cpp:334]     Train net output #0: loss = 1.38383 (* 1 = 1.38383 loss)
I0815 12:12:42.156850  8764 sgd_solver.cpp:136] Iteration 48900, lr = 0.00694375, m = 0.9
I0815 12:12:58.637769  8764 solver.cpp:363] Sparsity after update:
I0815 12:12:58.649293  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:12:58.649441  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:12:58.649540  8764 net.cpp:2192] conv1b_param_0(0.208) 
I0815 12:12:58.649611  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:12:58.649682  8764 net.cpp:2192] res2a_branch2a_param_0(0.288) 
I0815 12:12:58.649752  8764 net.cpp:2192] res2a_branch2b_param_0(0.285) 
I0815 12:12:58.649821  8764 net.cpp:2192] res3a_branch2a_param_0(0.29) 
I0815 12:12:58.650117  8764 net.cpp:2192] res3a_branch2b_param_0(0.288) 
I0815 12:12:58.650198  8764 net.cpp:2192] res4a_branch2a_param_0(0.29) 
I0815 12:12:58.650274  8764 net.cpp:2192] res4a_branch2b_param_0(0.29) 
I0815 12:12:58.650342  8764 net.cpp:2192] res5a_branch2a_param_0(0.29) 
I0815 12:12:58.650418  8764 net.cpp:2192] res5a_branch2b_param_0(0.29) 
I0815 12:12:58.650486  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (681993/2.86678e+06) 0.238
I0815 12:12:58.802037  8794 solver.cpp:409] Finding and applying sparsity: 0.3
I0815 12:13:23.737083  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:13:23.739070  8764 solver.cpp:312] Iteration 49000 (2.40494 iter/s, 41.5811s/100 iter), loss = 1.5265
I0815 12:13:23.739092  8764 solver.cpp:334]     Train net output #0: loss = 1.39039 (* 1 = 1.39039 loss)
I0815 12:13:23.739104  8764 sgd_solver.cpp:136] Iteration 49000, lr = 0.0069375, m = 0.9
I0815 12:13:39.736353  8764 solver.cpp:312] Iteration 49100 (6.25124 iter/s, 15.9968s/100 iter), loss = 1.4934
I0815 12:13:39.736440  8764 solver.cpp:334]     Train net output #0: loss = 1.75301 (* 1 = 1.75301 loss)
I0815 12:13:39.736469  8764 sgd_solver.cpp:136] Iteration 49100, lr = 0.00693125, m = 0.9
I0815 12:13:56.096521  8764 solver.cpp:312] Iteration 49200 (6.11258 iter/s, 16.3597s/100 iter), loss = 1.66964
I0815 12:13:56.096547  8764 solver.cpp:334]     Train net output #0: loss = 2.03676 (* 1 = 2.03676 loss)
I0815 12:13:56.096552  8764 sgd_solver.cpp:136] Iteration 49200, lr = 0.006925, m = 0.9
I0815 12:14:11.917856  8764 solver.cpp:312] Iteration 49300 (6.32076 iter/s, 15.8209s/100 iter), loss = 1.41552
I0815 12:14:11.917951  8764 solver.cpp:334]     Train net output #0: loss = 1.56721 (* 1 = 1.56721 loss)
I0815 12:14:11.917976  8764 sgd_solver.cpp:136] Iteration 49300, lr = 0.00691875, m = 0.9
I0815 12:14:28.976078  8764 solver.cpp:312] Iteration 49400 (5.86244 iter/s, 17.0577s/100 iter), loss = 1.13411
I0815 12:14:28.976106  8764 solver.cpp:334]     Train net output #0: loss = 0.897654 (* 1 = 0.897654 loss)
I0815 12:14:28.976114  8764 sgd_solver.cpp:136] Iteration 49400, lr = 0.0069125, m = 0.9
I0815 12:14:44.171216  8764 solver.cpp:312] Iteration 49500 (6.58124 iter/s, 15.1947s/100 iter), loss = 1.23731
I0815 12:14:44.171285  8764 solver.cpp:334]     Train net output #0: loss = 1.09128 (* 1 = 1.09128 loss)
I0815 12:14:44.171293  8764 sgd_solver.cpp:136] Iteration 49500, lr = 0.00690625, m = 0.9
I0815 12:15:02.557997  8764 solver.cpp:312] Iteration 49600 (5.43884 iter/s, 18.3863s/100 iter), loss = 1.61531
I0815 12:15:02.558024  8764 solver.cpp:334]     Train net output #0: loss = 1.66046 (* 1 = 1.66046 loss)
I0815 12:15:02.558029  8764 sgd_solver.cpp:136] Iteration 49600, lr = 0.0069, m = 0.9
I0815 12:15:19.412220  8764 solver.cpp:312] Iteration 49700 (5.9334 iter/s, 16.8538s/100 iter), loss = 1.66179
I0815 12:15:19.412327  8764 solver.cpp:334]     Train net output #0: loss = 1.62524 (* 1 = 1.62524 loss)
I0815 12:15:19.412338  8764 sgd_solver.cpp:136] Iteration 49700, lr = 0.00689375, m = 0.9
I0815 12:15:35.781685  8764 solver.cpp:312] Iteration 49800 (6.1091 iter/s, 16.369s/100 iter), loss = 1.70197
I0815 12:15:35.781708  8764 solver.cpp:334]     Train net output #0: loss = 2.06953 (* 1 = 2.06953 loss)
I0815 12:15:35.781713  8764 sgd_solver.cpp:136] Iteration 49800, lr = 0.0068875, m = 0.9
I0815 12:15:51.608513  8764 solver.cpp:312] Iteration 49900 (6.31856 iter/s, 15.8264s/100 iter), loss = 1.24262
I0815 12:15:51.608564  8764 solver.cpp:334]     Train net output #0: loss = 1.27901 (* 1 = 1.27901 loss)
I0815 12:15:51.608570  8764 sgd_solver.cpp:136] Iteration 49900, lr = 0.00688125, m = 0.9
I0815 12:16:08.060145  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_50000.caffemodel
I0815 12:16:08.105482  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_50000.solverstate
I0815 12:16:08.111462  8764 solver.cpp:363] Sparsity after update:
I0815 12:16:08.115473  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:16:08.115516  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:16:08.115535  8764 net.cpp:2192] conv1b_param_0(0.219) 
I0815 12:16:08.115551  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:16:08.115566  8764 net.cpp:2192] res2a_branch2a_param_0(0.299) 
I0815 12:16:08.115577  8764 net.cpp:2192] res2a_branch2b_param_0(0.299) 
I0815 12:16:08.115589  8764 net.cpp:2192] res3a_branch2a_param_0(0.299) 
I0815 12:16:08.115600  8764 net.cpp:2192] res3a_branch2b_param_0(0.299) 
I0815 12:16:08.115612  8764 net.cpp:2192] res4a_branch2a_param_0(0.299) 
I0815 12:16:08.115624  8764 net.cpp:2192] res4a_branch2b_param_0(0.299) 
I0815 12:16:08.115635  8764 net.cpp:2192] res5a_branch2a_param_0(0.3) 
I0815 12:16:08.115648  8764 net.cpp:2192] res5a_branch2b_param_0(0.299) 
I0815 12:16:08.115659  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (704892/2.86678e+06) 0.246
I0815 12:16:08.115682  8764 solver.cpp:509] Iteration 50000, Testing net (#0)
I0815 12:16:33.626886  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.564
I0815 12:16:33.626967  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.798351
I0815 12:16:33.626976  8764 solver.cpp:594]     Test net output #2: loss = 1.93075 (* 1 = 1.93075 loss)
I0815 12:16:33.626996  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.5106s
I0815 12:16:33.792814  8794 solver.cpp:409] Finding and applying sparsity: 0.31
I0815 12:16:56.202391  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:16:56.204380  8764 solver.cpp:312] Iteration 50000 (1.54813 iter/s, 64.5941s/100 iter), loss = 1.69561
I0815 12:16:56.204399  8764 solver.cpp:334]     Train net output #0: loss = 1.61734 (* 1 = 1.61734 loss)
I0815 12:16:56.204406  8764 sgd_solver.cpp:136] Iteration 50000, lr = 0.006875, m = 0.9
I0815 12:17:12.486609  8764 solver.cpp:312] Iteration 50100 (6.14184 iter/s, 16.2818s/100 iter), loss = 1.32587
I0815 12:17:12.486711  8764 solver.cpp:334]     Train net output #0: loss = 1.17205 (* 1 = 1.17205 loss)
I0815 12:17:12.486726  8764 sgd_solver.cpp:136] Iteration 50100, lr = 0.00686875, m = 0.9
I0815 12:17:30.793222  8764 solver.cpp:312] Iteration 50200 (5.46266 iter/s, 18.3061s/100 iter), loss = 1.60456
I0815 12:17:30.793315  8764 solver.cpp:334]     Train net output #0: loss = 1.48049 (* 1 = 1.48049 loss)
I0815 12:17:30.793344  8764 sgd_solver.cpp:136] Iteration 50200, lr = 0.0068625, m = 0.9
I0815 12:17:47.591727  8764 solver.cpp:312] Iteration 50300 (5.95308 iter/s, 16.798s/100 iter), loss = 1.29112
I0815 12:17:47.592340  8764 solver.cpp:334]     Train net output #0: loss = 1.20653 (* 1 = 1.20653 loss)
I0815 12:17:47.592347  8764 sgd_solver.cpp:136] Iteration 50300, lr = 0.00685625, m = 0.9
I0815 12:18:06.452807  8764 solver.cpp:312] Iteration 50400 (5.30207 iter/s, 18.8606s/100 iter), loss = 1.59051
I0815 12:18:06.452836  8764 solver.cpp:334]     Train net output #0: loss = 1.94822 (* 1 = 1.94822 loss)
I0815 12:18:06.452842  8764 sgd_solver.cpp:136] Iteration 50400, lr = 0.00685, m = 0.9
I0815 12:18:21.024885  8764 solver.cpp:312] Iteration 50500 (6.86263 iter/s, 14.5717s/100 iter), loss = 1.42025
I0815 12:18:21.024940  8764 solver.cpp:334]     Train net output #0: loss = 1.37511 (* 1 = 1.37511 loss)
I0815 12:18:21.024946  8764 sgd_solver.cpp:136] Iteration 50500, lr = 0.00684375, m = 0.9
I0815 12:18:35.660806  8764 solver.cpp:312] Iteration 50600 (6.8327 iter/s, 14.6355s/100 iter), loss = 1.26979
I0815 12:18:35.660856  8764 solver.cpp:334]     Train net output #0: loss = 1.59741 (* 1 = 1.59741 loss)
I0815 12:18:35.660869  8764 sgd_solver.cpp:136] Iteration 50600, lr = 0.0068375, m = 0.9
I0815 12:18:50.164551  8764 solver.cpp:312] Iteration 50700 (6.89497 iter/s, 14.5033s/100 iter), loss = 1.48423
I0815 12:18:50.164613  8764 solver.cpp:334]     Train net output #0: loss = 1.75006 (* 1 = 1.75006 loss)
I0815 12:18:50.164630  8764 sgd_solver.cpp:136] Iteration 50700, lr = 0.00683125, m = 0.9
I0815 12:19:07.139281  8764 solver.cpp:312] Iteration 50800 (5.89128 iter/s, 16.9742s/100 iter), loss = 1.65512
I0815 12:19:07.139431  8764 solver.cpp:334]     Train net output #0: loss = 1.61348 (* 1 = 1.61348 loss)
I0815 12:19:07.139456  8764 sgd_solver.cpp:136] Iteration 50800, lr = 0.006825, m = 0.9
I0815 12:19:22.577517  8764 solver.cpp:312] Iteration 50900 (6.4776 iter/s, 15.4378s/100 iter), loss = 1.26324
I0815 12:19:22.577567  8764 solver.cpp:334]     Train net output #0: loss = 1.17853 (* 1 = 1.17853 loss)
I0815 12:19:22.577580  8764 sgd_solver.cpp:136] Iteration 50900, lr = 0.00681875, m = 0.9
I0815 12:19:36.982797  8764 solver.cpp:363] Sparsity after update:
I0815 12:19:36.995421  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:19:36.995455  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:19:36.995470  8764 net.cpp:2192] conv1b_param_0(0.229) 
I0815 12:19:36.995479  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:19:36.995488  8764 net.cpp:2192] res2a_branch2a_param_0(0.309) 
I0815 12:19:36.995498  8764 net.cpp:2192] res2a_branch2b_param_0(0.306) 
I0815 12:19:36.995507  8764 net.cpp:2192] res3a_branch2a_param_0(0.309) 
I0815 12:19:36.995515  8764 net.cpp:2192] res3a_branch2b_param_0(0.309) 
I0815 12:19:36.995524  8764 net.cpp:2192] res4a_branch2a_param_0(0.31) 
I0815 12:19:36.995533  8764 net.cpp:2192] res4a_branch2b_param_0(0.309) 
I0815 12:19:36.995542  8764 net.cpp:2192] res5a_branch2a_param_0(0.31) 
I0815 12:19:36.995550  8764 net.cpp:2192] res5a_branch2b_param_0(0.31) 
I0815 12:19:36.995559  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (728858/2.86678e+06) 0.254
I0815 12:19:37.140291  8794 solver.cpp:409] Finding and applying sparsity: 0.32
I0815 12:20:00.293084  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:20:00.295086  8764 solver.cpp:312] Iteration 51000 (2.65136 iter/s, 37.7165s/100 iter), loss = 1.34622
I0815 12:20:00.295109  8764 solver.cpp:334]     Train net output #0: loss = 1.54464 (* 1 = 1.54464 loss)
I0815 12:20:00.295119  8764 sgd_solver.cpp:136] Iteration 51000, lr = 0.0068125, m = 0.9
I0815 12:20:15.524338  8764 solver.cpp:312] Iteration 51100 (6.5665 iter/s, 15.2288s/100 iter), loss = 1.54826
I0815 12:20:15.524415  8764 solver.cpp:334]     Train net output #0: loss = 1.56856 (* 1 = 1.56856 loss)
I0815 12:20:15.524421  8764 sgd_solver.cpp:136] Iteration 51100, lr = 0.00680625, m = 0.9
I0815 12:20:32.963016  8764 solver.cpp:312] Iteration 51200 (5.73454 iter/s, 17.4382s/100 iter), loss = 1.13849
I0815 12:20:32.963160  8764 solver.cpp:334]     Train net output #0: loss = 1.0593 (* 1 = 1.0593 loss)
I0815 12:20:32.963176  8764 sgd_solver.cpp:136] Iteration 51200, lr = 0.0068, m = 0.9
I0815 12:20:48.011862  8764 solver.cpp:312] Iteration 51300 (6.64521 iter/s, 15.0484s/100 iter), loss = 1.29535
I0815 12:20:48.011955  8764 solver.cpp:334]     Train net output #0: loss = 1.50254 (* 1 = 1.50254 loss)
I0815 12:20:48.011972  8764 sgd_solver.cpp:136] Iteration 51300, lr = 0.00679375, m = 0.9
I0815 12:21:04.133414  8764 solver.cpp:312] Iteration 51400 (6.20305 iter/s, 16.1211s/100 iter), loss = 1.69135
I0815 12:21:04.133440  8764 solver.cpp:334]     Train net output #0: loss = 1.70859 (* 1 = 1.70859 loss)
I0815 12:21:04.133445  8764 sgd_solver.cpp:136] Iteration 51400, lr = 0.0067875, m = 0.9
I0815 12:21:18.824151  8764 solver.cpp:312] Iteration 51500 (6.8072 iter/s, 14.6903s/100 iter), loss = 1.4026
I0815 12:21:18.824210  8764 solver.cpp:334]     Train net output #0: loss = 1.30629 (* 1 = 1.30629 loss)
I0815 12:21:18.824218  8764 sgd_solver.cpp:136] Iteration 51500, lr = 0.00678125, m = 0.9
I0815 12:21:34.819317  8764 solver.cpp:312] Iteration 51600 (6.25206 iter/s, 15.9947s/100 iter), loss = 1.31866
I0815 12:21:34.819341  8764 solver.cpp:334]     Train net output #0: loss = 1.2085 (* 1 = 1.2085 loss)
I0815 12:21:34.819345  8764 sgd_solver.cpp:136] Iteration 51600, lr = 0.006775, m = 0.9
I0815 12:21:51.974074  8764 solver.cpp:312] Iteration 51700 (5.82945 iter/s, 17.1543s/100 iter), loss = 1.35794
I0815 12:21:51.974141  8764 solver.cpp:334]     Train net output #0: loss = 1.40449 (* 1 = 1.40449 loss)
I0815 12:21:51.974148  8764 sgd_solver.cpp:136] Iteration 51700, lr = 0.00676875, m = 0.9
I0815 12:22:09.210952  8764 solver.cpp:312] Iteration 51800 (5.80167 iter/s, 17.2364s/100 iter), loss = 1.1651
I0815 12:22:09.210978  8764 solver.cpp:334]     Train net output #0: loss = 0.786118 (* 1 = 0.786118 loss)
I0815 12:22:09.210981  8764 sgd_solver.cpp:136] Iteration 51800, lr = 0.0067625, m = 0.9
I0815 12:22:26.383611  8764 solver.cpp:312] Iteration 51900 (5.82337 iter/s, 17.1722s/100 iter), loss = 1.74761
I0815 12:22:26.383716  8764 solver.cpp:334]     Train net output #0: loss = 1.54096 (* 1 = 1.54096 loss)
I0815 12:22:26.383735  8764 sgd_solver.cpp:136] Iteration 51900, lr = 0.00675625, m = 0.9
I0815 12:22:43.319113  8764 solver.cpp:363] Sparsity after update:
I0815 12:22:43.323026  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:22:43.323143  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:22:43.323225  8764 net.cpp:2192] conv1b_param_0(0.24) 
I0815 12:22:43.323295  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:22:43.323362  8764 net.cpp:2192] res2a_branch2a_param_0(0.319) 
I0815 12:22:43.323432  8764 net.cpp:2192] res2a_branch2b_param_0(0.319) 
I0815 12:22:43.323501  8764 net.cpp:2192] res3a_branch2a_param_0(0.319) 
I0815 12:22:43.323570  8764 net.cpp:2192] res3a_branch2b_param_0(0.319) 
I0815 12:22:43.323635  8764 net.cpp:2192] res4a_branch2a_param_0(0.319) 
I0815 12:22:43.323704  8764 net.cpp:2192] res4a_branch2b_param_0(0.319) 
I0815 12:22:43.323770  8764 net.cpp:2192] res5a_branch2a_param_0(0.32) 
I0815 12:22:43.323839  8764 net.cpp:2192] res5a_branch2b_param_0(0.319) 
I0815 12:22:43.323906  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (752114/2.86678e+06) 0.262
I0815 12:22:43.324003  8764 solver.cpp:509] Iteration 52000, Testing net (#0)
I0815 12:23:05.993937  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.57053
I0815 12:23:05.994060  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805998
I0815 12:23:05.994068  8764 solver.cpp:594]     Test net output #2: loss = 1.87023 (* 1 = 1.87023 loss)
I0815 12:23:05.994086  8764 solver.cpp:264] [MultiGPU] Tests completed in 22.6695s
I0815 12:23:06.140754  8794 solver.cpp:409] Finding and applying sparsity: 0.33
I0815 12:23:30.798746  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:23:30.800775  8764 solver.cpp:312] Iteration 52000 (1.55242 iter/s, 64.4154s/100 iter), loss = 1.43277
I0815 12:23:30.800792  8764 solver.cpp:334]     Train net output #0: loss = 0.852598 (* 1 = 0.852598 loss)
I0815 12:23:30.800798  8764 sgd_solver.cpp:136] Iteration 52000, lr = 0.00675, m = 0.9
I0815 12:23:46.086879  8764 solver.cpp:312] Iteration 52100 (6.54207 iter/s, 15.2857s/100 iter), loss = 1.37018
I0815 12:23:46.086930  8764 solver.cpp:334]     Train net output #0: loss = 1.23538 (* 1 = 1.23538 loss)
I0815 12:23:46.086937  8764 sgd_solver.cpp:136] Iteration 52100, lr = 0.00674375, m = 0.9
I0815 12:24:00.021019  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:24:04.037585  8764 solver.cpp:312] Iteration 52200 (5.57097 iter/s, 17.9502s/100 iter), loss = 1.14367
I0815 12:24:04.037609  8764 solver.cpp:334]     Train net output #0: loss = 1.29614 (* 1 = 1.29614 loss)
I0815 12:24:04.037616  8764 sgd_solver.cpp:136] Iteration 52200, lr = 0.0067375, m = 0.9
I0815 12:24:21.742593  8764 solver.cpp:312] Iteration 52300 (5.64828 iter/s, 17.7045s/100 iter), loss = 1.54924
I0815 12:24:21.742883  8764 solver.cpp:334]     Train net output #0: loss = 1.55209 (* 1 = 1.55209 loss)
I0815 12:24:21.743000  8764 sgd_solver.cpp:136] Iteration 52300, lr = 0.00673125, m = 0.9
I0815 12:24:37.832157  8764 solver.cpp:312] Iteration 52400 (6.21538 iter/s, 16.0891s/100 iter), loss = 1.28388
I0815 12:24:37.832221  8764 solver.cpp:334]     Train net output #0: loss = 1.2624 (* 1 = 1.2624 loss)
I0815 12:24:37.832237  8764 sgd_solver.cpp:136] Iteration 52400, lr = 0.006725, m = 0.9
I0815 12:24:52.658485  8764 solver.cpp:312] Iteration 52500 (6.74495 iter/s, 14.8259s/100 iter), loss = 1.44685
I0815 12:24:52.658565  8764 solver.cpp:334]     Train net output #0: loss = 1.2415 (* 1 = 1.2415 loss)
I0815 12:24:52.658579  8764 sgd_solver.cpp:136] Iteration 52500, lr = 0.00671875, m = 0.9
I0815 12:25:07.287089  8764 solver.cpp:312] Iteration 52600 (6.83611 iter/s, 14.6282s/100 iter), loss = 1.31348
I0815 12:25:07.287142  8764 solver.cpp:334]     Train net output #0: loss = 1.47019 (* 1 = 1.47019 loss)
I0815 12:25:07.287153  8764 sgd_solver.cpp:136] Iteration 52600, lr = 0.0067125, m = 0.9
I0815 12:25:24.308967  8764 solver.cpp:312] Iteration 52700 (5.87496 iter/s, 17.0214s/100 iter), loss = 1.29145
I0815 12:25:24.309082  8764 solver.cpp:334]     Train net output #0: loss = 1.02401 (* 1 = 1.02401 loss)
I0815 12:25:24.309108  8764 sgd_solver.cpp:136] Iteration 52700, lr = 0.00670625, m = 0.9
I0815 12:25:42.467923  8764 solver.cpp:312] Iteration 52800 (5.50708 iter/s, 18.1585s/100 iter), loss = 1.5578
I0815 12:25:42.467950  8764 solver.cpp:334]     Train net output #0: loss = 1.19745 (* 1 = 1.19745 loss)
I0815 12:25:42.467954  8764 sgd_solver.cpp:136] Iteration 52800, lr = 0.0067, m = 0.9
I0815 12:25:58.958534  8764 solver.cpp:312] Iteration 52900 (6.06423 iter/s, 16.4902s/100 iter), loss = 1.40384
I0815 12:25:58.958631  8764 solver.cpp:334]     Train net output #0: loss = 1.3108 (* 1 = 1.3108 loss)
I0815 12:25:58.958648  8764 sgd_solver.cpp:136] Iteration 52900, lr = 0.00669375, m = 0.9
I0815 12:26:15.865139  8764 solver.cpp:363] Sparsity after update:
I0815 12:26:15.880775  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:26:15.880791  8764 net.cpp:2192] conv1a_param_0(0.16) 
I0815 12:26:15.880801  8764 net.cpp:2192] conv1b_param_0(0.24) 
I0815 12:26:15.880805  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:26:15.880808  8764 net.cpp:2192] res2a_branch2a_param_0(0.33) 
I0815 12:26:15.880812  8764 net.cpp:2192] res2a_branch2b_param_0(0.326) 
I0815 12:26:15.880827  8764 net.cpp:2192] res3a_branch2a_param_0(0.33) 
I0815 12:26:15.880837  8764 net.cpp:2192] res3a_branch2b_param_0(0.33) 
I0815 12:26:15.880846  8764 net.cpp:2192] res4a_branch2a_param_0(0.33) 
I0815 12:26:15.880853  8764 net.cpp:2192] res4a_branch2b_param_0(0.33) 
I0815 12:26:15.880861  8764 net.cpp:2192] res5a_branch2a_param_0(0.33) 
I0815 12:26:15.880872  8764 net.cpp:2192] res5a_branch2b_param_0(0.33) 
I0815 12:26:15.880880  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (776078/2.86678e+06) 0.271
I0815 12:26:16.031514  8794 solver.cpp:409] Finding and applying sparsity: 0.34
I0815 12:26:39.381769  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:26:39.383831  8764 solver.cpp:312] Iteration 53000 (2.47377 iter/s, 40.4242s/100 iter), loss = 1.32734
I0815 12:26:39.383854  8764 solver.cpp:334]     Train net output #0: loss = 1.19055 (* 1 = 1.19055 loss)
I0815 12:26:39.383868  8764 sgd_solver.cpp:136] Iteration 53000, lr = 0.0066875, m = 0.9
I0815 12:26:54.614543  8764 solver.cpp:312] Iteration 53100 (6.56587 iter/s, 15.2303s/100 iter), loss = 1.34362
I0815 12:26:54.614568  8764 solver.cpp:334]     Train net output #0: loss = 1.36786 (* 1 = 1.36786 loss)
I0815 12:26:54.614574  8764 sgd_solver.cpp:136] Iteration 53100, lr = 0.00668125, m = 0.9
I0815 12:27:11.461181  8764 solver.cpp:312] Iteration 53200 (5.93607 iter/s, 16.8462s/100 iter), loss = 1.24478
I0815 12:27:11.461241  8764 solver.cpp:334]     Train net output #0: loss = 1.33558 (* 1 = 1.33558 loss)
I0815 12:27:11.461246  8764 sgd_solver.cpp:136] Iteration 53200, lr = 0.006675, m = 0.9
I0815 12:27:29.170125  8764 solver.cpp:312] Iteration 53300 (5.64702 iter/s, 17.7084s/100 iter), loss = 1.09232
I0815 12:27:29.170151  8764 solver.cpp:334]     Train net output #0: loss = 1.01427 (* 1 = 1.01427 loss)
I0815 12:27:29.170156  8764 sgd_solver.cpp:136] Iteration 53300, lr = 0.00666875, m = 0.9
I0815 12:27:43.754289  8764 solver.cpp:312] Iteration 53400 (6.85694 iter/s, 14.5838s/100 iter), loss = 1.57764
I0815 12:27:43.754380  8764 solver.cpp:334]     Train net output #0: loss = 1.74569 (* 1 = 1.74569 loss)
I0815 12:27:43.754398  8764 sgd_solver.cpp:136] Iteration 53400, lr = 0.0066625, m = 0.9
I0815 12:28:02.056634  8764 solver.cpp:312] Iteration 53500 (5.46393 iter/s, 18.3018s/100 iter), loss = 1.25891
I0815 12:28:02.058202  8764 solver.cpp:334]     Train net output #0: loss = 1.44068 (* 1 = 1.44068 loss)
I0815 12:28:02.058231  8764 sgd_solver.cpp:136] Iteration 53500, lr = 0.00665625, m = 0.9
I0815 12:28:17.404484  8764 solver.cpp:312] Iteration 53600 (6.51575 iter/s, 15.3474s/100 iter), loss = 1.46637
I0815 12:28:17.405243  8764 solver.cpp:334]     Train net output #0: loss = 1.46788 (* 1 = 1.46788 loss)
I0815 12:28:17.405261  8764 sgd_solver.cpp:136] Iteration 53600, lr = 0.00665, m = 0.9
I0815 12:28:34.735841  8764 solver.cpp:312] Iteration 53700 (5.77005 iter/s, 17.3309s/100 iter), loss = 1.31344
I0815 12:28:34.735868  8764 solver.cpp:334]     Train net output #0: loss = 1.39106 (* 1 = 1.39106 loss)
I0815 12:28:34.735874  8764 sgd_solver.cpp:136] Iteration 53700, lr = 0.00664375, m = 0.9
I0815 12:28:51.607003  8764 solver.cpp:312] Iteration 53800 (5.92744 iter/s, 16.8707s/100 iter), loss = 1.55178
I0815 12:28:51.607219  8764 solver.cpp:334]     Train net output #0: loss = 1.94969 (* 1 = 1.94969 loss)
I0815 12:28:51.607234  8764 sgd_solver.cpp:136] Iteration 53800, lr = 0.0066375, m = 0.9
I0815 12:29:07.852169  8764 solver.cpp:312] Iteration 53900 (6.15585 iter/s, 16.2447s/100 iter), loss = 1.37435
I0815 12:29:07.852222  8764 solver.cpp:334]     Train net output #0: loss = 1.30394 (* 1 = 1.30394 loss)
I0815 12:29:07.852236  8764 sgd_solver.cpp:136] Iteration 53900, lr = 0.00663125, m = 0.9
I0815 12:29:23.992152  8764 solver.cpp:363] Sparsity after update:
I0815 12:29:23.994640  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:29:23.994660  8764 net.cpp:2192] conv1a_param_0(0.16) 
I0815 12:29:23.994678  8764 net.cpp:2192] conv1b_param_0(0.25) 
I0815 12:29:23.994688  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:29:23.994694  8764 net.cpp:2192] res2a_branch2a_param_0(0.337) 
I0815 12:29:23.994700  8764 net.cpp:2192] res2a_branch2b_param_0(0.333) 
I0815 12:29:23.994707  8764 net.cpp:2192] res3a_branch2a_param_0(0.339) 
I0815 12:29:23.994714  8764 net.cpp:2192] res3a_branch2b_param_0(0.337) 
I0815 12:29:23.994720  8764 net.cpp:2192] res4a_branch2a_param_0(0.339) 
I0815 12:29:23.994729  8764 net.cpp:2192] res4a_branch2b_param_0(0.339) 
I0815 12:29:23.994735  8764 net.cpp:2192] res5a_branch2a_param_0(0.34) 
I0815 12:29:23.994741  8764 net.cpp:2192] res5a_branch2b_param_0(0.339) 
I0815 12:29:23.994748  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (798708/2.86678e+06) 0.279
I0815 12:29:23.994774  8764 solver.cpp:509] Iteration 54000, Testing net (#0)
I0815 12:29:47.303164  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.558471
I0815 12:29:47.303184  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.799644
I0815 12:29:47.303191  8764 solver.cpp:594]     Test net output #2: loss = 1.9315 (* 1 = 1.9315 loss)
I0815 12:29:47.303211  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.3078s
I0815 12:29:47.457026  8794 solver.cpp:409] Finding and applying sparsity: 0.35
I0815 12:30:11.527894  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:30:11.529909  8764 solver.cpp:312] Iteration 54000 (1.57045 iter/s, 63.676s/100 iter), loss = 1.30581
I0815 12:30:11.529930  8764 solver.cpp:334]     Train net output #0: loss = 1.56092 (* 1 = 1.56092 loss)
I0815 12:30:11.529940  8764 sgd_solver.cpp:136] Iteration 54000, lr = 0.006625, m = 0.9
I0815 12:30:27.924468  8764 solver.cpp:312] Iteration 54100 (6.09976 iter/s, 16.3941s/100 iter), loss = 1.20821
I0815 12:30:27.924495  8764 solver.cpp:334]     Train net output #0: loss = 1.53579 (* 1 = 1.53579 loss)
I0815 12:30:27.924499  8764 sgd_solver.cpp:136] Iteration 54100, lr = 0.00661875, m = 0.9
I0815 12:30:44.457183  8764 solver.cpp:312] Iteration 54200 (6.04878 iter/s, 16.5323s/100 iter), loss = 1.76486
I0815 12:30:44.457247  8764 solver.cpp:334]     Train net output #0: loss = 1.33795 (* 1 = 1.33795 loss)
I0815 12:30:44.457254  8764 sgd_solver.cpp:136] Iteration 54200, lr = 0.0066125, m = 0.9
I0815 12:31:04.651316  8764 solver.cpp:312] Iteration 54300 (4.95207 iter/s, 20.1936s/100 iter), loss = 1.38692
I0815 12:31:04.651376  8764 solver.cpp:334]     Train net output #0: loss = 1.07039 (* 1 = 1.07039 loss)
I0815 12:31:04.651398  8764 sgd_solver.cpp:136] Iteration 54300, lr = 0.00660625, m = 0.9
I0815 12:31:20.684638  8764 solver.cpp:312] Iteration 54400 (6.23719 iter/s, 16.0329s/100 iter), loss = 1.35246
I0815 12:31:20.684692  8764 solver.cpp:334]     Train net output #0: loss = 1.41178 (* 1 = 1.41178 loss)
I0815 12:31:20.684700  8764 sgd_solver.cpp:136] Iteration 54400, lr = 0.0066, m = 0.9
I0815 12:31:37.337643  8764 solver.cpp:312] Iteration 54500 (6.00509 iter/s, 16.6525s/100 iter), loss = 1.63038
I0815 12:31:37.337668  8764 solver.cpp:334]     Train net output #0: loss = 1.91332 (* 1 = 1.91332 loss)
I0815 12:31:37.337673  8764 sgd_solver.cpp:136] Iteration 54500, lr = 0.00659375, m = 0.9
I0815 12:31:53.990952  8764 solver.cpp:312] Iteration 54600 (6.00498 iter/s, 16.6528s/100 iter), loss = 1.28982
I0815 12:31:53.991034  8764 solver.cpp:334]     Train net output #0: loss = 1.4631 (* 1 = 1.4631 loss)
I0815 12:31:53.991123  8764 sgd_solver.cpp:136] Iteration 54600, lr = 0.0065875, m = 0.9
I0815 12:32:10.561812  8764 solver.cpp:312] Iteration 54700 (6.03485 iter/s, 16.5704s/100 iter), loss = 1.6286
I0815 12:32:10.561841  8764 solver.cpp:334]     Train net output #0: loss = 1.84617 (* 1 = 1.84617 loss)
I0815 12:32:10.561846  8764 sgd_solver.cpp:136] Iteration 54700, lr = 0.00658125, m = 0.9
I0815 12:32:27.624892  8764 solver.cpp:312] Iteration 54800 (5.86077 iter/s, 17.0626s/100 iter), loss = 1.11796
I0815 12:32:27.624963  8764 solver.cpp:334]     Train net output #0: loss = 1.20446 (* 1 = 1.20446 loss)
I0815 12:32:27.624971  8764 sgd_solver.cpp:136] Iteration 54800, lr = 0.006575, m = 0.9
I0815 12:32:44.277544  8764 solver.cpp:312] Iteration 54900 (6.00521 iter/s, 16.6522s/100 iter), loss = 1.67474
I0815 12:32:44.277572  8764 solver.cpp:334]     Train net output #0: loss = 1.60701 (* 1 = 1.60701 loss)
I0815 12:32:44.277578  8764 sgd_solver.cpp:136] Iteration 54900, lr = 0.00656875, m = 0.9
I0815 12:32:59.088043  8764 solver.cpp:363] Sparsity after update:
I0815 12:32:59.098551  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:32:59.098589  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:32:59.098603  8764 net.cpp:2192] conv1b_param_0(0.26) 
I0815 12:32:59.098613  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:32:59.098620  8764 net.cpp:2192] res2a_branch2a_param_0(0.347) 
I0815 12:32:59.098628  8764 net.cpp:2192] res2a_branch2b_param_0(0.347) 
I0815 12:32:59.098636  8764 net.cpp:2192] res3a_branch2a_param_0(0.349) 
I0815 12:32:59.098644  8764 net.cpp:2192] res3a_branch2b_param_0(0.347) 
I0815 12:32:59.098651  8764 net.cpp:2192] res4a_branch2a_param_0(0.35) 
I0815 12:32:59.098659  8764 net.cpp:2192] res4a_branch2b_param_0(0.349) 
I0815 12:32:59.098667  8764 net.cpp:2192] res5a_branch2a_param_0(0.35) 
I0815 12:32:59.098675  8764 net.cpp:2192] res5a_branch2b_param_0(0.35) 
I0815 12:32:59.098682  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (822758/2.86678e+06) 0.287
I0815 12:32:59.252229  8794 solver.cpp:409] Finding and applying sparsity: 0.36
I0815 12:33:23.255916  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:33:23.258029  8764 solver.cpp:312] Iteration 55000 (2.56546 iter/s, 38.9794s/100 iter), loss = 1.17539
I0815 12:33:23.258049  8764 solver.cpp:334]     Train net output #0: loss = 1.35282 (* 1 = 1.35282 loss)
I0815 12:33:23.258057  8764 sgd_solver.cpp:136] Iteration 55000, lr = 0.0065625, m = 0.9
I0815 12:33:38.066442  8764 solver.cpp:312] Iteration 55100 (6.75311 iter/s, 14.808s/100 iter), loss = 1.30555
I0815 12:33:38.066493  8764 solver.cpp:334]     Train net output #0: loss = 1.05288 (* 1 = 1.05288 loss)
I0815 12:33:38.066499  8764 sgd_solver.cpp:136] Iteration 55100, lr = 0.00655625, m = 0.9
I0815 12:33:56.071745  8764 solver.cpp:312] Iteration 55200 (5.55407 iter/s, 18.0048s/100 iter), loss = 1.22635
I0815 12:33:56.071784  8764 solver.cpp:334]     Train net output #0: loss = 1.52938 (* 1 = 1.52938 loss)
I0815 12:33:56.071792  8764 sgd_solver.cpp:136] Iteration 55200, lr = 0.00655, m = 0.9
I0815 12:34:12.673099  8764 solver.cpp:312] Iteration 55300 (6.02377 iter/s, 16.6009s/100 iter), loss = 1.30569
I0815 12:34:12.673158  8764 solver.cpp:334]     Train net output #0: loss = 1.39365 (* 1 = 1.39365 loss)
I0815 12:34:12.673166  8764 sgd_solver.cpp:136] Iteration 55300, lr = 0.00654375, m = 0.9
I0815 12:34:27.010190  8764 solver.cpp:312] Iteration 55400 (6.97511 iter/s, 14.3367s/100 iter), loss = 1.49972
I0815 12:34:27.010215  8764 solver.cpp:334]     Train net output #0: loss = 1.42432 (* 1 = 1.42432 loss)
I0815 12:34:27.010241  8764 sgd_solver.cpp:136] Iteration 55400, lr = 0.0065375, m = 0.9
I0815 12:34:44.191239  8764 solver.cpp:312] Iteration 55500 (5.82053 iter/s, 17.1806s/100 iter), loss = 1.29205
I0815 12:34:44.191388  8764 solver.cpp:334]     Train net output #0: loss = 1.10008 (* 1 = 1.10008 loss)
I0815 12:34:44.191411  8764 sgd_solver.cpp:136] Iteration 55500, lr = 0.00653125, m = 0.9
I0815 12:35:02.660953  8764 solver.cpp:312] Iteration 55600 (5.41442 iter/s, 18.4692s/100 iter), loss = 1.60754
I0815 12:35:02.660979  8764 solver.cpp:334]     Train net output #0: loss = 1.74925 (* 1 = 1.74925 loss)
I0815 12:35:02.660984  8764 sgd_solver.cpp:136] Iteration 55600, lr = 0.006525, m = 0.9
I0815 12:35:21.712406  8764 solver.cpp:312] Iteration 55700 (5.24909 iter/s, 19.0509s/100 iter), loss = 1.47047
I0815 12:35:21.712528  8764 solver.cpp:334]     Train net output #0: loss = 1.54156 (* 1 = 1.54156 loss)
I0815 12:35:21.712543  8764 sgd_solver.cpp:136] Iteration 55700, lr = 0.00651875, m = 0.9
I0815 12:35:40.867506  8764 solver.cpp:312] Iteration 55800 (5.22069 iter/s, 19.1546s/100 iter), loss = 1.47894
I0815 12:35:40.869690  8764 solver.cpp:334]     Train net output #0: loss = 1.85913 (* 1 = 1.85913 loss)
I0815 12:35:40.869712  8764 sgd_solver.cpp:136] Iteration 55800, lr = 0.0065125, m = 0.9
I0815 12:35:55.736387  8764 solver.cpp:312] Iteration 55900 (6.72564 iter/s, 14.8685s/100 iter), loss = 1.33017
I0815 12:35:55.736481  8764 solver.cpp:334]     Train net output #0: loss = 1.42967 (* 1 = 1.42967 loss)
I0815 12:35:55.736498  8764 sgd_solver.cpp:136] Iteration 55900, lr = 0.00650625, m = 0.9
I0815 12:36:10.227483  8764 solver.cpp:363] Sparsity after update:
I0815 12:36:10.231364  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:36:10.231374  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:36:10.231382  8764 net.cpp:2192] conv1b_param_0(0.26) 
I0815 12:36:10.231385  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:36:10.231398  8764 net.cpp:2192] res2a_branch2a_param_0(0.358) 
I0815 12:36:10.231412  8764 net.cpp:2192] res2a_branch2b_param_0(0.354) 
I0815 12:36:10.231418  8764 net.cpp:2192] res3a_branch2a_param_0(0.359) 
I0815 12:36:10.231421  8764 net.cpp:2192] res3a_branch2b_param_0(0.358) 
I0815 12:36:10.231429  8764 net.cpp:2192] res4a_branch2a_param_0(0.359) 
I0815 12:36:10.231434  8764 net.cpp:2192] res4a_branch2b_param_0(0.359) 
I0815 12:36:10.231443  8764 net.cpp:2192] res5a_branch2a_param_0(0.36) 
I0815 12:36:10.231448  8764 net.cpp:2192] res5a_branch2b_param_0(0.359) 
I0815 12:36:10.231451  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (845930/2.86678e+06) 0.295
I0815 12:36:10.231470  8764 solver.cpp:509] Iteration 56000, Testing net (#0)
I0815 12:36:31.056681  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571294
I0815 12:36:31.056728  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804939
I0815 12:36:31.056735  8764 solver.cpp:594]     Test net output #2: loss = 1.8944 (* 1 = 1.8944 loss)
I0815 12:36:31.059449  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8274s
I0815 12:36:31.206226  8794 solver.cpp:409] Finding and applying sparsity: 0.37
I0815 12:36:55.348986  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:36:55.351032  8764 solver.cpp:312] Iteration 56000 (1.67749 iter/s, 59.613s/100 iter), loss = 1.28572
I0815 12:36:55.351054  8764 solver.cpp:334]     Train net output #0: loss = 1.41576 (* 1 = 1.41576 loss)
I0815 12:36:55.351061  8764 sgd_solver.cpp:136] Iteration 56000, lr = 0.0065, m = 0.9
I0815 12:37:12.334408  8764 solver.cpp:312] Iteration 56100 (5.88828 iter/s, 16.9829s/100 iter), loss = 1.43881
I0815 12:37:12.334498  8764 solver.cpp:334]     Train net output #0: loss = 1.6393 (* 1 = 1.6393 loss)
I0815 12:37:12.334512  8764 sgd_solver.cpp:136] Iteration 56100, lr = 0.00649375, m = 0.9
I0815 12:37:28.215925  8764 solver.cpp:312] Iteration 56200 (6.2968 iter/s, 15.8811s/100 iter), loss = 1.37647
I0815 12:37:28.216150  8764 solver.cpp:334]     Train net output #0: loss = 1.62677 (* 1 = 1.62677 loss)
I0815 12:37:28.216264  8764 sgd_solver.cpp:136] Iteration 56200, lr = 0.0064875, m = 0.9
I0815 12:37:45.912179  8764 solver.cpp:312] Iteration 56300 (5.65107 iter/s, 17.6958s/100 iter), loss = 1.59454
I0815 12:37:45.912227  8764 solver.cpp:334]     Train net output #0: loss = 1.70409 (* 1 = 1.70409 loss)
I0815 12:37:45.912232  8764 sgd_solver.cpp:136] Iteration 56300, lr = 0.00648125, m = 0.9
I0815 12:38:01.295022  8764 solver.cpp:312] Iteration 56400 (6.50093 iter/s, 15.3824s/100 iter), loss = 1.72164
I0815 12:38:01.295047  8764 solver.cpp:334]     Train net output #0: loss = 1.33969 (* 1 = 1.33969 loss)
I0815 12:38:01.295053  8764 sgd_solver.cpp:136] Iteration 56400, lr = 0.006475, m = 0.9
I0815 12:38:17.987663  8764 solver.cpp:312] Iteration 56500 (5.99083 iter/s, 16.6922s/100 iter), loss = 1.46486
I0815 12:38:17.987721  8764 solver.cpp:334]     Train net output #0: loss = 1.67182 (* 1 = 1.67182 loss)
I0815 12:38:17.987726  8764 sgd_solver.cpp:136] Iteration 56500, lr = 0.00646875, m = 0.9
I0815 12:38:34.220371  8764 solver.cpp:312] Iteration 56600 (6.16057 iter/s, 16.2323s/100 iter), loss = 1.10914
I0815 12:38:34.220448  8764 solver.cpp:334]     Train net output #0: loss = 0.975896 (* 1 = 0.975896 loss)
I0815 12:38:34.220468  8764 sgd_solver.cpp:136] Iteration 56600, lr = 0.0064625, m = 0.9
I0815 12:38:50.128127  8764 solver.cpp:312] Iteration 56700 (6.28642 iter/s, 15.9073s/100 iter), loss = 1.48183
I0815 12:38:50.128204  8764 solver.cpp:334]     Train net output #0: loss = 2.02349 (* 1 = 2.02349 loss)
I0815 12:38:50.128209  8764 sgd_solver.cpp:136] Iteration 56700, lr = 0.00645625, m = 0.9
I0815 12:39:06.162526  8764 solver.cpp:312] Iteration 56800 (6.23676 iter/s, 16.034s/100 iter), loss = 1.30046
I0815 12:39:06.162557  8764 solver.cpp:334]     Train net output #0: loss = 1.50873 (* 1 = 1.50873 loss)
I0815 12:39:06.162564  8764 sgd_solver.cpp:136] Iteration 56800, lr = 0.00645, m = 0.9
I0815 12:39:21.399505  8764 solver.cpp:312] Iteration 56900 (6.56316 iter/s, 15.2366s/100 iter), loss = 1.11307
I0815 12:39:21.399598  8764 solver.cpp:334]     Train net output #0: loss = 1.10894 (* 1 = 1.10894 loss)
I0815 12:39:21.399616  8764 sgd_solver.cpp:136] Iteration 56900, lr = 0.00644375, m = 0.9
I0815 12:39:37.939755  8764 solver.cpp:363] Sparsity after update:
I0815 12:39:37.952587  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:39:37.952602  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:39:37.952611  8764 net.cpp:2192] conv1b_param_0(0.271) 
I0815 12:39:37.952615  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:39:37.952621  8764 net.cpp:2192] res2a_branch2a_param_0(0.368) 
I0815 12:39:37.952625  8764 net.cpp:2192] res2a_branch2b_param_0(0.368) 
I0815 12:39:37.952627  8764 net.cpp:2192] res3a_branch2a_param_0(0.37) 
I0815 12:39:37.952630  8764 net.cpp:2192] res3a_branch2b_param_0(0.368) 
I0815 12:39:37.952635  8764 net.cpp:2192] res4a_branch2a_param_0(0.37) 
I0815 12:39:37.952637  8764 net.cpp:2192] res4a_branch2b_param_0(0.37) 
I0815 12:39:37.952641  8764 net.cpp:2192] res5a_branch2a_param_0(0.37) 
I0815 12:39:37.952643  8764 net.cpp:2192] res5a_branch2b_param_0(0.37) 
I0815 12:39:37.952646  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (869947/2.86678e+06) 0.303
I0815 12:39:38.092272  8794 solver.cpp:409] Finding and applying sparsity: 0.38
I0815 12:40:07.145928  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:40:07.148059  8764 solver.cpp:312] Iteration 57000 (2.18592 iter/s, 45.7473s/100 iter), loss = 1.39197
I0815 12:40:07.148078  8764 solver.cpp:334]     Train net output #0: loss = 1.45955 (* 1 = 1.45955 loss)
I0815 12:40:07.148087  8764 sgd_solver.cpp:136] Iteration 57000, lr = 0.0064375, m = 0.9
I0815 12:40:23.704613  8764 solver.cpp:312] Iteration 57100 (6.04007 iter/s, 16.5561s/100 iter), loss = 1.42926
I0815 12:40:23.704640  8764 solver.cpp:334]     Train net output #0: loss = 1.3874 (* 1 = 1.3874 loss)
I0815 12:40:23.704645  8764 sgd_solver.cpp:136] Iteration 57100, lr = 0.00643125, m = 0.9
I0815 12:40:41.731842  8764 solver.cpp:312] Iteration 57200 (5.54732 iter/s, 18.0267s/100 iter), loss = 1.35756
I0815 12:40:41.731945  8764 solver.cpp:334]     Train net output #0: loss = 1.25896 (* 1 = 1.25896 loss)
I0815 12:40:41.731958  8764 sgd_solver.cpp:136] Iteration 57200, lr = 0.006425, m = 0.9
I0815 12:40:58.606662  8764 solver.cpp:312] Iteration 57300 (5.92615 iter/s, 16.8743s/100 iter), loss = 1.24083
I0815 12:40:58.606709  8764 solver.cpp:334]     Train net output #0: loss = 1.33497 (* 1 = 1.33497 loss)
I0815 12:40:58.606720  8764 sgd_solver.cpp:136] Iteration 57300, lr = 0.00641875, m = 0.9
I0815 12:40:59.534304  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:41:15.535609  8764 solver.cpp:312] Iteration 57400 (5.9072 iter/s, 16.9285s/100 iter), loss = 1.65923
I0815 12:41:15.535691  8764 solver.cpp:334]     Train net output #0: loss = 1.61933 (* 1 = 1.61933 loss)
I0815 12:41:15.535697  8764 sgd_solver.cpp:136] Iteration 57400, lr = 0.0064125, m = 0.9
I0815 12:41:31.025511  8764 solver.cpp:312] Iteration 57500 (6.456 iter/s, 15.4895s/100 iter), loss = 0.931446
I0815 12:41:31.025534  8764 solver.cpp:334]     Train net output #0: loss = 0.634437 (* 1 = 0.634437 loss)
I0815 12:41:31.025538  8764 sgd_solver.cpp:136] Iteration 57500, lr = 0.00640625, m = 0.9
I0815 12:41:46.862406  8764 solver.cpp:312] Iteration 57600 (6.31454 iter/s, 15.8365s/100 iter), loss = 1.4606
I0815 12:41:46.864157  8764 solver.cpp:334]     Train net output #0: loss = 1.70401 (* 1 = 1.70401 loss)
I0815 12:41:46.864166  8764 sgd_solver.cpp:136] Iteration 57600, lr = 0.0064, m = 0.9
I0815 12:42:03.397094  8764 solver.cpp:312] Iteration 57700 (6.04806 iter/s, 16.5342s/100 iter), loss = 1.4333
I0815 12:42:03.397120  8764 solver.cpp:334]     Train net output #0: loss = 1.41328 (* 1 = 1.41328 loss)
I0815 12:42:03.397125  8764 sgd_solver.cpp:136] Iteration 57700, lr = 0.00639375, m = 0.9
I0815 12:42:20.062961  8764 solver.cpp:312] Iteration 57800 (6.00045 iter/s, 16.6654s/100 iter), loss = 1.80786
I0815 12:42:20.063201  8764 solver.cpp:334]     Train net output #0: loss = 1.81814 (* 1 = 1.81814 loss)
I0815 12:42:20.063313  8764 sgd_solver.cpp:136] Iteration 57800, lr = 0.0063875, m = 0.9
I0815 12:42:36.194933  8764 solver.cpp:312] Iteration 57900 (6.19904 iter/s, 16.1315s/100 iter), loss = 1.58255
I0815 12:42:36.194962  8764 solver.cpp:334]     Train net output #0: loss = 1.51307 (* 1 = 1.51307 loss)
I0815 12:42:36.194967  8764 sgd_solver.cpp:136] Iteration 57900, lr = 0.00638125, m = 0.9
I0815 12:42:51.517217  8764 solver.cpp:363] Sparsity after update:
I0815 12:42:51.521083  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:42:51.521093  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:42:51.521103  8764 net.cpp:2192] conv1b_param_0(0.375) 
I0815 12:42:51.521117  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:42:51.521127  8764 net.cpp:2192] res2a_branch2a_param_0(0.378) 
I0815 12:42:51.521139  8764 net.cpp:2192] res2a_branch2b_param_0(0.375) 
I0815 12:42:51.521148  8764 net.cpp:2192] res3a_branch2a_param_0(0.378) 
I0815 12:42:51.521157  8764 net.cpp:2192] res3a_branch2b_param_0(0.378) 
I0815 12:42:51.521165  8764 net.cpp:2192] res4a_branch2a_param_0(0.379) 
I0815 12:42:51.521174  8764 net.cpp:2192] res4a_branch2b_param_0(0.378) 
I0815 12:42:51.521183  8764 net.cpp:2192] res5a_branch2a_param_0(0.38) 
I0815 12:42:51.521193  8764 net.cpp:2192] res5a_branch2b_param_0(0.379) 
I0815 12:42:51.521200  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (893005/2.86678e+06) 0.312
I0815 12:42:51.521217  8764 solver.cpp:509] Iteration 58000, Testing net (#0)
I0815 12:43:14.737154  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.574824
I0815 12:43:14.737175  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804174
I0815 12:43:14.737182  8764 solver.cpp:594]     Test net output #2: loss = 1.86589 (* 1 = 1.86589 loss)
I0815 12:43:14.737258  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.2154s
I0815 12:43:14.894122  8794 solver.cpp:409] Finding and applying sparsity: 0.39
I0815 12:43:39.942683  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:43:39.944768  8764 solver.cpp:312] Iteration 58000 (1.56867 iter/s, 63.7481s/100 iter), loss = 1.83487
I0815 12:43:39.944790  8764 solver.cpp:334]     Train net output #0: loss = 1.5769 (* 1 = 1.5769 loss)
I0815 12:43:39.944798  8764 sgd_solver.cpp:136] Iteration 58000, lr = 0.006375, m = 0.9
I0815 12:43:56.628553  8764 solver.cpp:312] Iteration 58100 (5.99401 iter/s, 16.6833s/100 iter), loss = 1.54222
I0815 12:43:56.628578  8764 solver.cpp:334]     Train net output #0: loss = 1.99252 (* 1 = 1.99252 loss)
I0815 12:43:56.628582  8764 sgd_solver.cpp:136] Iteration 58100, lr = 0.00636875, m = 0.9
I0815 12:44:11.225312  8764 solver.cpp:312] Iteration 58200 (6.85103 iter/s, 14.5964s/100 iter), loss = 1.51371
I0815 12:44:11.225375  8764 solver.cpp:334]     Train net output #0: loss = 1.38955 (* 1 = 1.38955 loss)
I0815 12:44:11.225380  8764 sgd_solver.cpp:136] Iteration 58200, lr = 0.0063625, m = 0.9
I0815 12:44:26.733448  8764 solver.cpp:312] Iteration 58300 (6.44841 iter/s, 15.5077s/100 iter), loss = 1.78242
I0815 12:44:26.733517  8764 solver.cpp:334]     Train net output #0: loss = 1.77128 (* 1 = 1.77128 loss)
I0815 12:44:26.733536  8764 sgd_solver.cpp:136] Iteration 58300, lr = 0.00635625, m = 0.9
I0815 12:44:43.791076  8764 solver.cpp:312] Iteration 58400 (5.86264 iter/s, 17.0572s/100 iter), loss = 1.63353
I0815 12:44:43.791136  8764 solver.cpp:334]     Train net output #0: loss = 1.73331 (* 1 = 1.73331 loss)
I0815 12:44:43.791141  8764 sgd_solver.cpp:136] Iteration 58400, lr = 0.00635, m = 0.9
I0815 12:45:00.440737  8764 solver.cpp:312] Iteration 58500 (6.0063 iter/s, 16.6492s/100 iter), loss = 1.66732
I0815 12:45:00.440794  8764 solver.cpp:334]     Train net output #0: loss = 1.35997 (* 1 = 1.35997 loss)
I0815 12:45:00.440805  8764 sgd_solver.cpp:136] Iteration 58500, lr = 0.00634375, m = 0.9
I0815 12:45:16.168184  8764 solver.cpp:312] Iteration 58600 (6.35848 iter/s, 15.727s/100 iter), loss = 1.5193
I0815 12:45:16.168231  8764 solver.cpp:334]     Train net output #0: loss = 1.2942 (* 1 = 1.2942 loss)
I0815 12:45:16.168236  8764 sgd_solver.cpp:136] Iteration 58600, lr = 0.0063375, m = 0.9
I0815 12:45:32.196089  8764 solver.cpp:312] Iteration 58700 (6.23929 iter/s, 16.0275s/100 iter), loss = 1.25862
I0815 12:45:32.196117  8764 solver.cpp:334]     Train net output #0: loss = 1.23083 (* 1 = 1.23083 loss)
I0815 12:45:32.196123  8764 sgd_solver.cpp:136] Iteration 58700, lr = 0.00633125, m = 0.9
I0815 12:45:50.465425  8764 solver.cpp:312] Iteration 58800 (5.4738 iter/s, 18.2688s/100 iter), loss = 1.91853
I0815 12:45:50.468240  8764 solver.cpp:334]     Train net output #0: loss = 1.98158 (* 1 = 1.98158 loss)
I0815 12:45:50.468271  8764 sgd_solver.cpp:136] Iteration 58800, lr = 0.006325, m = 0.9
I0815 12:46:07.476353  8764 solver.cpp:312] Iteration 58900 (5.87873 iter/s, 17.0105s/100 iter), loss = 1.29631
I0815 12:46:07.476382  8764 solver.cpp:334]     Train net output #0: loss = 1.33704 (* 1 = 1.33704 loss)
I0815 12:46:07.476388  8764 sgd_solver.cpp:136] Iteration 58900, lr = 0.00631875, m = 0.9
I0815 12:46:23.682009  8764 solver.cpp:363] Sparsity after update:
I0815 12:46:23.695515  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:46:23.695530  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:46:23.695539  8764 net.cpp:2192] conv1b_param_0(0.389) 
I0815 12:46:23.695543  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:46:23.695547  8764 net.cpp:2192] res2a_branch2a_param_0(0.389) 
I0815 12:46:23.695551  8764 net.cpp:2192] res2a_branch2b_param_0(0.389) 
I0815 12:46:23.695554  8764 net.cpp:2192] res3a_branch2a_param_0(0.389) 
I0815 12:46:23.695557  8764 net.cpp:2192] res3a_branch2b_param_0(0.389) 
I0815 12:46:23.695560  8764 net.cpp:2192] res4a_branch2a_param_0(0.39) 
I0815 12:46:23.695564  8764 net.cpp:2192] res4a_branch2b_param_0(0.389) 
I0815 12:46:23.695566  8764 net.cpp:2192] res5a_branch2a_param_0(0.39) 
I0815 12:46:23.695570  8764 net.cpp:2192] res5a_branch2b_param_0(0.39) 
I0815 12:46:23.695574  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (917042/2.86678e+06) 0.32
I0815 12:46:23.843201  8794 solver.cpp:409] Finding and applying sparsity: 0.4
I0815 12:46:51.063246  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:46:51.065251  8764 solver.cpp:312] Iteration 59000 (2.29422 iter/s, 43.5877s/100 iter), loss = 1.61565
I0815 12:46:51.065270  8764 solver.cpp:334]     Train net output #0: loss = 2.1244 (* 1 = 2.1244 loss)
I0815 12:46:51.065275  8764 sgd_solver.cpp:136] Iteration 59000, lr = 0.0063125, m = 0.9
I0815 12:47:08.078419  8764 solver.cpp:312] Iteration 59100 (5.87796 iter/s, 17.0127s/100 iter), loss = 1.72105
I0815 12:47:08.078474  8764 solver.cpp:334]     Train net output #0: loss = 1.20753 (* 1 = 1.20753 loss)
I0815 12:47:08.078477  8764 sgd_solver.cpp:136] Iteration 59100, lr = 0.00630625, m = 0.9
I0815 12:47:25.024884  8764 solver.cpp:312] Iteration 59200 (5.9011 iter/s, 16.946s/100 iter), loss = 1.49584
I0815 12:47:25.024952  8764 solver.cpp:334]     Train net output #0: loss = 1.56826 (* 1 = 1.56826 loss)
I0815 12:47:25.024969  8764 sgd_solver.cpp:136] Iteration 59200, lr = 0.0063, m = 0.9
I0815 12:47:41.528509  8764 solver.cpp:312] Iteration 59300 (6.05944 iter/s, 16.5032s/100 iter), loss = 1.45367
I0815 12:47:41.528568  8764 solver.cpp:334]     Train net output #0: loss = 1.20018 (* 1 = 1.20018 loss)
I0815 12:47:41.528576  8764 sgd_solver.cpp:136] Iteration 59300, lr = 0.00629375, m = 0.9
I0815 12:47:57.735184  8764 solver.cpp:312] Iteration 59400 (6.17047 iter/s, 16.2062s/100 iter), loss = 1.72684
I0815 12:47:57.735211  8764 solver.cpp:334]     Train net output #0: loss = 1.84737 (* 1 = 1.84737 loss)
I0815 12:47:57.735219  8764 sgd_solver.cpp:136] Iteration 59400, lr = 0.0062875, m = 0.9
I0815 12:48:14.983206  8764 solver.cpp:312] Iteration 59500 (5.79793 iter/s, 17.2475s/100 iter), loss = 1.42758
I0815 12:48:14.984213  8764 solver.cpp:334]     Train net output #0: loss = 1.35373 (* 1 = 1.35373 loss)
I0815 12:48:14.984221  8764 sgd_solver.cpp:136] Iteration 59500, lr = 0.00628125, m = 0.9
I0815 12:48:34.005475  8726 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 12:48:35.896302  8764 solver.cpp:312] Iteration 59600 (4.78183 iter/s, 20.9125s/100 iter), loss = 1.50134
I0815 12:48:35.896332  8764 solver.cpp:334]     Train net output #0: loss = 1.35289 (* 1 = 1.35289 loss)
I0815 12:48:35.896338  8764 sgd_solver.cpp:136] Iteration 59600, lr = 0.006275, m = 0.9
I0815 12:48:53.346122  8764 solver.cpp:312] Iteration 59700 (5.73088 iter/s, 17.4493s/100 iter), loss = 1.59109
I0815 12:48:53.346215  8764 solver.cpp:334]     Train net output #0: loss = 1.75092 (* 1 = 1.75092 loss)
I0815 12:48:53.346232  8764 sgd_solver.cpp:136] Iteration 59700, lr = 0.00626875, m = 0.9
I0815 12:49:08.829792  8764 solver.cpp:312] Iteration 59800 (6.4586 iter/s, 15.4832s/100 iter), loss = 1.44818
I0815 12:49:08.829815  8764 solver.cpp:334]     Train net output #0: loss = 1.35366 (* 1 = 1.35366 loss)
I0815 12:49:08.829819  8764 sgd_solver.cpp:136] Iteration 59800, lr = 0.0062625, m = 0.9
I0815 12:49:27.431583  8764 solver.cpp:312] Iteration 59900 (5.37597 iter/s, 18.6013s/100 iter), loss = 1.26812
I0815 12:49:27.431674  8764 solver.cpp:334]     Train net output #0: loss = 1.33322 (* 1 = 1.33322 loss)
I0815 12:49:27.431694  8764 sgd_solver.cpp:136] Iteration 59900, lr = 0.00625625, m = 0.9
I0815 12:49:48.257220  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_60000.caffemodel
I0815 12:49:48.282263  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_60000.solverstate
I0815 12:49:48.286793  8764 solver.cpp:363] Sparsity after update:
I0815 12:49:48.289248  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:49:48.289258  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:49:48.289268  8764 net.cpp:2192] conv1b_param_0(0.389) 
I0815 12:49:48.289278  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:49:48.289288  8764 net.cpp:2192] res2a_branch2a_param_0(0.399) 
I0815 12:49:48.289300  8764 net.cpp:2192] res2a_branch2b_param_0(0.396) 
I0815 12:49:48.289306  8764 net.cpp:2192] res3a_branch2a_param_0(0.399) 
I0815 12:49:48.289310  8764 net.cpp:2192] res3a_branch2b_param_0(0.399) 
I0815 12:49:48.289319  8764 net.cpp:2192] res4a_branch2a_param_0(0.399) 
I0815 12:49:48.289327  8764 net.cpp:2192] res4a_branch2b_param_0(0.399) 
I0815 12:49:48.289336  8764 net.cpp:2192] res5a_branch2a_param_0(0.4) 
I0815 12:49:48.289342  8764 net.cpp:2192] res5a_branch2b_param_0(0.399) 
I0815 12:49:48.289345  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (940215/2.86678e+06) 0.328
I0815 12:49:48.289362  8764 solver.cpp:509] Iteration 60000, Testing net (#0)
I0815 12:50:09.246868  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.565647
I0815 12:50:09.246948  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804528
I0815 12:50:09.246956  8764 solver.cpp:594]     Test net output #2: loss = 1.90823 (* 1 = 1.90823 loss)
I0815 12:50:09.246978  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.957s
I0815 12:50:09.393518  8794 solver.cpp:409] Finding and applying sparsity: 0.41
I0815 12:50:35.388639  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:50:35.390632  8764 solver.cpp:312] Iteration 60000 (1.47151 iter/s, 67.9572s/100 iter), loss = 2.00603
I0815 12:50:35.390655  8764 solver.cpp:334]     Train net output #0: loss = 2.71763 (* 1 = 2.71763 loss)
I0815 12:50:35.390664  8764 sgd_solver.cpp:136] Iteration 60000, lr = 0.00625, m = 0.9
I0815 12:50:50.319118  8764 solver.cpp:312] Iteration 60100 (6.69879 iter/s, 14.9281s/100 iter), loss = 1.4814
I0815 12:50:50.319197  8764 solver.cpp:334]     Train net output #0: loss = 1.32399 (* 1 = 1.32399 loss)
I0815 12:50:50.319205  8764 sgd_solver.cpp:136] Iteration 60100, lr = 0.00624375, m = 0.9
I0815 12:51:04.589813  8764 solver.cpp:312] Iteration 60200 (7.00756 iter/s, 14.2703s/100 iter), loss = 1.34191
I0815 12:51:04.589839  8764 solver.cpp:334]     Train net output #0: loss = 1.08615 (* 1 = 1.08615 loss)
I0815 12:51:04.589845  8764 sgd_solver.cpp:136] Iteration 60200, lr = 0.0062375, m = 0.9
I0815 12:51:20.933516  8764 solver.cpp:312] Iteration 60300 (6.11873 iter/s, 16.3433s/100 iter), loss = 1.36182
I0815 12:51:20.933568  8764 solver.cpp:334]     Train net output #0: loss = 1.24709 (* 1 = 1.24709 loss)
I0815 12:51:20.933574  8764 sgd_solver.cpp:136] Iteration 60300, lr = 0.00623125, m = 0.9
I0815 12:51:39.237521  8764 solver.cpp:312] Iteration 60400 (5.46344 iter/s, 18.3035s/100 iter), loss = 1.36529
I0815 12:51:39.237593  8764 solver.cpp:334]     Train net output #0: loss = 1.41465 (* 1 = 1.41465 loss)
I0815 12:51:39.237614  8764 sgd_solver.cpp:136] Iteration 60400, lr = 0.006225, m = 0.9
I0815 12:51:57.480775  8764 solver.cpp:312] Iteration 60500 (5.48163 iter/s, 18.2428s/100 iter), loss = 1.17489
I0815 12:51:57.480880  8764 solver.cpp:334]     Train net output #0: loss = 0.977305 (* 1 = 0.977305 loss)
I0815 12:51:57.480897  8764 sgd_solver.cpp:136] Iteration 60500, lr = 0.00621875, m = 0.9
I0815 12:52:13.574512  8764 solver.cpp:312] Iteration 60600 (6.21377 iter/s, 16.0933s/100 iter), loss = 1.57528
I0815 12:52:13.574535  8764 solver.cpp:334]     Train net output #0: loss = 1.82204 (* 1 = 1.82204 loss)
I0815 12:52:13.574539  8764 sgd_solver.cpp:136] Iteration 60600, lr = 0.0062125, m = 0.9
I0815 12:52:31.176517  8764 solver.cpp:312] Iteration 60700 (5.68133 iter/s, 17.6015s/100 iter), loss = 1.6933
I0815 12:52:31.176568  8764 solver.cpp:334]     Train net output #0: loss = 1.5932 (* 1 = 1.5932 loss)
I0815 12:52:31.176574  8764 sgd_solver.cpp:136] Iteration 60700, lr = 0.00620625, m = 0.9
I0815 12:52:47.179056  8764 solver.cpp:312] Iteration 60800 (6.24918 iter/s, 16.0021s/100 iter), loss = 1.47919
I0815 12:52:47.179110  8764 solver.cpp:334]     Train net output #0: loss = 1.57729 (* 1 = 1.57729 loss)
I0815 12:52:47.179122  8764 sgd_solver.cpp:136] Iteration 60800, lr = 0.0062, m = 0.9
I0815 12:53:04.719851  8764 solver.cpp:312] Iteration 60900 (5.70116 iter/s, 17.5403s/100 iter), loss = 1.77734
I0815 12:53:04.719944  8764 solver.cpp:334]     Train net output #0: loss = 1.49476 (* 1 = 1.49476 loss)
I0815 12:53:04.719960  8764 sgd_solver.cpp:136] Iteration 60900, lr = 0.00619375, m = 0.9
I0815 12:53:24.871227  8764 solver.cpp:363] Sparsity after update:
I0815 12:53:24.881078  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:53:24.881089  8764 net.cpp:2192] conv1a_param_0(0.2) 
I0815 12:53:24.881098  8764 net.cpp:2192] conv1b_param_0(0.403) 
I0815 12:53:24.881101  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:53:24.881116  8764 net.cpp:2192] res2a_branch2a_param_0(0.41) 
I0815 12:53:24.881125  8764 net.cpp:2192] res2a_branch2b_param_0(0.41) 
I0815 12:53:24.881134  8764 net.cpp:2192] res3a_branch2a_param_0(0.41) 
I0815 12:53:24.881141  8764 net.cpp:2192] res3a_branch2b_param_0(0.41) 
I0815 12:53:24.881155  8764 net.cpp:2192] res4a_branch2a_param_0(0.41) 
I0815 12:53:24.881163  8764 net.cpp:2192] res4a_branch2b_param_0(0.41) 
I0815 12:53:24.881171  8764 net.cpp:2192] res5a_branch2a_param_0(0.41) 
I0815 12:53:24.881178  8764 net.cpp:2192] res5a_branch2b_param_0(0.41) 
I0815 12:53:24.881186  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (964276/2.86678e+06) 0.336
I0815 12:53:25.011606  8794 solver.cpp:409] Finding and applying sparsity: 0.42
I0815 12:54:04.278414  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:54:04.280534  8764 solver.cpp:312] Iteration 61000 (1.67901 iter/s, 59.5591s/100 iter), loss = 1.64483
I0815 12:54:04.280553  8764 solver.cpp:334]     Train net output #0: loss = 1.63877 (* 1 = 1.63877 loss)
I0815 12:54:04.280561  8764 sgd_solver.cpp:136] Iteration 61000, lr = 0.0061875, m = 0.9
I0815 12:54:19.228817  8764 solver.cpp:312] Iteration 61100 (6.68992 iter/s, 14.9479s/100 iter), loss = 1.374
I0815 12:54:19.228870  8764 solver.cpp:334]     Train net output #0: loss = 1.26007 (* 1 = 1.26007 loss)
I0815 12:54:19.228894  8764 sgd_solver.cpp:136] Iteration 61100, lr = 0.00618125, m = 0.9
I0815 12:54:35.333278  8764 solver.cpp:312] Iteration 61200 (6.20963 iter/s, 16.104s/100 iter), loss = 1.53721
I0815 12:54:35.333328  8764 solver.cpp:334]     Train net output #0: loss = 1.97372 (* 1 = 1.97372 loss)
I0815 12:54:35.333333  8764 sgd_solver.cpp:136] Iteration 61200, lr = 0.006175, m = 0.9
I0815 12:54:52.515410  8764 solver.cpp:312] Iteration 61300 (5.82016 iter/s, 17.1817s/100 iter), loss = 1.85532
I0815 12:54:52.515471  8764 solver.cpp:334]     Train net output #0: loss = 1.62121 (* 1 = 1.62121 loss)
I0815 12:54:52.515488  8764 sgd_solver.cpp:136] Iteration 61300, lr = 0.00616875, m = 0.9
I0815 12:55:09.773262  8764 solver.cpp:312] Iteration 61400 (5.79463 iter/s, 17.2574s/100 iter), loss = 1.39158
I0815 12:55:09.773346  8764 solver.cpp:334]     Train net output #0: loss = 1.40008 (* 1 = 1.40008 loss)
I0815 12:55:09.773353  8764 sgd_solver.cpp:136] Iteration 61400, lr = 0.0061625, m = 0.9
I0815 12:55:24.842167  8764 solver.cpp:312] Iteration 61500 (6.63637 iter/s, 15.0685s/100 iter), loss = 1.1842
I0815 12:55:24.842222  8764 solver.cpp:334]     Train net output #0: loss = 1.09671 (* 1 = 1.09671 loss)
I0815 12:55:24.842236  8764 sgd_solver.cpp:136] Iteration 61500, lr = 0.00615625, m = 0.9
I0815 12:55:43.032631  8764 solver.cpp:312] Iteration 61600 (5.49754 iter/s, 18.19s/100 iter), loss = 1.15654
I0815 12:55:43.032721  8764 solver.cpp:334]     Train net output #0: loss = 1.08556 (* 1 = 1.08556 loss)
I0815 12:55:43.032739  8764 sgd_solver.cpp:136] Iteration 61600, lr = 0.00615, m = 0.9
I0815 12:55:59.008615  8764 solver.cpp:312] Iteration 61700 (6.25957 iter/s, 15.9755s/100 iter), loss = 1.33313
I0815 12:55:59.008680  8764 solver.cpp:334]     Train net output #0: loss = 1.40643 (* 1 = 1.40643 loss)
I0815 12:55:59.008697  8764 sgd_solver.cpp:136] Iteration 61700, lr = 0.00614375, m = 0.9
I0815 12:56:16.761963  8764 solver.cpp:312] Iteration 61800 (5.6329 iter/s, 17.7529s/100 iter), loss = 1.2476
I0815 12:56:16.762058  8764 solver.cpp:334]     Train net output #0: loss = 1.24123 (* 1 = 1.24123 loss)
I0815 12:56:16.762075  8764 sgd_solver.cpp:136] Iteration 61800, lr = 0.0061375, m = 0.9
I0815 12:56:31.726631  8764 solver.cpp:312] Iteration 61900 (6.68259 iter/s, 14.9643s/100 iter), loss = 1.70402
I0815 12:56:31.726699  8764 solver.cpp:334]     Train net output #0: loss = 1.52929 (* 1 = 1.52929 loss)
I0815 12:56:31.726716  8764 sgd_solver.cpp:136] Iteration 61900, lr = 0.00613125, m = 0.9
I0815 12:56:33.307108  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:56:46.225857  8764 solver.cpp:363] Sparsity after update:
I0815 12:56:46.234266  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:56:46.234277  8764 net.cpp:2192] conv1a_param_0(0.2) 
I0815 12:56:46.234284  8764 net.cpp:2192] conv1b_param_0(0.417) 
I0815 12:56:46.234288  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:56:46.234293  8764 net.cpp:2192] res2a_branch2a_param_0(0.417) 
I0815 12:56:46.234297  8764 net.cpp:2192] res2a_branch2b_param_0(0.417) 
I0815 12:56:46.234300  8764 net.cpp:2192] res3a_branch2a_param_0(0.418) 
I0815 12:56:46.234302  8764 net.cpp:2192] res3a_branch2b_param_0(0.417) 
I0815 12:56:46.234305  8764 net.cpp:2192] res4a_branch2a_param_0(0.419) 
I0815 12:56:46.234308  8764 net.cpp:2192] res4a_branch2b_param_0(0.418) 
I0815 12:56:46.234311  8764 net.cpp:2192] res5a_branch2a_param_0(0.42) 
I0815 12:56:46.234314  8764 net.cpp:2192] res5a_branch2b_param_0(0.419) 
I0815 12:56:46.234318  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (986901/2.86678e+06) 0.344
I0815 12:56:46.234329  8764 solver.cpp:509] Iteration 62000, Testing net (#0)
I0815 12:57:07.583370  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.578294
I0815 12:57:07.583442  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804409
I0815 12:57:07.583449  8764 solver.cpp:594]     Test net output #2: loss = 1.87143 (* 1 = 1.87143 loss)
I0815 12:57:07.583472  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.3486s
I0815 12:57:07.732343  8794 solver.cpp:409] Finding and applying sparsity: 0.43
I0815 12:57:34.147594  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:57:34.149576  8764 solver.cpp:312] Iteration 62000 (1.60202 iter/s, 62.4212s/100 iter), loss = 1.35888
I0815 12:57:34.149598  8764 solver.cpp:334]     Train net output #0: loss = 1.30671 (* 1 = 1.30671 loss)
I0815 12:57:34.149606  8764 sgd_solver.cpp:136] Iteration 62000, lr = 0.006125, m = 0.9
I0815 12:57:49.134184  8764 solver.cpp:312] Iteration 62100 (6.6737 iter/s, 14.9842s/100 iter), loss = 1.68422
I0815 12:57:49.134286  8764 solver.cpp:334]     Train net output #0: loss = 1.87793 (* 1 = 1.87793 loss)
I0815 12:57:49.134305  8764 sgd_solver.cpp:136] Iteration 62100, lr = 0.00611875, m = 0.9
I0815 12:58:05.782207  8764 solver.cpp:312] Iteration 62200 (6.00689 iter/s, 16.6476s/100 iter), loss = 1.27317
I0815 12:58:05.782246  8764 solver.cpp:334]     Train net output #0: loss = 1.30325 (* 1 = 1.30325 loss)
I0815 12:58:05.782255  8764 sgd_solver.cpp:136] Iteration 62200, lr = 0.0061125, m = 0.9
I0815 12:58:23.413094  8764 solver.cpp:312] Iteration 62300 (5.67202 iter/s, 17.6304s/100 iter), loss = 1.72359
I0815 12:58:23.413146  8764 solver.cpp:334]     Train net output #0: loss = 2.19357 (* 1 = 2.19357 loss)
I0815 12:58:23.413153  8764 sgd_solver.cpp:136] Iteration 62300, lr = 0.00610625, m = 0.9
I0815 12:58:39.460616  8764 solver.cpp:312] Iteration 62400 (6.23166 iter/s, 16.0471s/100 iter), loss = 1.65933
I0815 12:58:39.460680  8764 solver.cpp:334]     Train net output #0: loss = 1.5394 (* 1 = 1.5394 loss)
I0815 12:58:39.460703  8764 sgd_solver.cpp:136] Iteration 62400, lr = 0.0061, m = 0.9
I0815 12:58:58.322710  8764 solver.cpp:312] Iteration 62500 (5.30179 iter/s, 18.8616s/100 iter), loss = 1.3287
I0815 12:58:58.322789  8764 solver.cpp:334]     Train net output #0: loss = 1.06271 (* 1 = 1.06271 loss)
I0815 12:58:58.322801  8764 sgd_solver.cpp:136] Iteration 62500, lr = 0.00609375, m = 0.9
I0815 12:59:17.239631  8764 solver.cpp:312] Iteration 62600 (5.28642 iter/s, 18.9164s/100 iter), loss = 1.2398
I0815 12:59:17.239677  8764 solver.cpp:334]     Train net output #0: loss = 1.03079 (* 1 = 1.03079 loss)
I0815 12:59:17.239686  8764 sgd_solver.cpp:136] Iteration 62600, lr = 0.0060875, m = 0.9
I0815 12:59:34.666725  8764 solver.cpp:312] Iteration 62700 (5.73835 iter/s, 17.4266s/100 iter), loss = 1.37029
I0815 12:59:34.666805  8764 solver.cpp:334]     Train net output #0: loss = 1.68098 (* 1 = 1.68098 loss)
I0815 12:59:34.666811  8764 sgd_solver.cpp:136] Iteration 62700, lr = 0.00608125, m = 0.9
I0815 12:59:50.721254  8764 solver.cpp:312] Iteration 62800 (6.22894 iter/s, 16.0541s/100 iter), loss = 1.42268
I0815 12:59:50.721282  8764 solver.cpp:334]     Train net output #0: loss = 1.46988 (* 1 = 1.46988 loss)
I0815 12:59:50.721287  8764 sgd_solver.cpp:136] Iteration 62800, lr = 0.006075, m = 0.9
I0815 13:00:07.722420  8764 solver.cpp:312] Iteration 62900 (5.88211 iter/s, 17.0007s/100 iter), loss = 1.57324
I0815 13:00:07.736161  8764 solver.cpp:334]     Train net output #0: loss = 1.46042 (* 1 = 1.46042 loss)
I0815 13:00:07.736181  8764 sgd_solver.cpp:136] Iteration 62900, lr = 0.00606875, m = 0.9
I0815 13:00:25.893345  8764 solver.cpp:363] Sparsity after update:
I0815 13:00:25.901809  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:00:25.901832  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:00:25.901849  8764 net.cpp:2192] conv1b_param_0(0.417) 
I0815 13:00:25.901859  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:00:25.901868  8764 net.cpp:2192] res2a_branch2a_param_0(0.427) 
I0815 13:00:25.901877  8764 net.cpp:2192] res2a_branch2b_param_0(0.424) 
I0815 13:00:25.901887  8764 net.cpp:2192] res3a_branch2a_param_0(0.429) 
I0815 13:00:25.901895  8764 net.cpp:2192] res3a_branch2b_param_0(0.427) 
I0815 13:00:25.901904  8764 net.cpp:2192] res4a_branch2a_param_0(0.43) 
I0815 13:00:25.901913  8764 net.cpp:2192] res4a_branch2b_param_0(0.429) 
I0815 13:00:25.901922  8764 net.cpp:2192] res5a_branch2a_param_0(0.43) 
I0815 13:00:25.901932  8764 net.cpp:2192] res5a_branch2b_param_0(0.43) 
I0815 13:00:25.901939  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.01087e+06/2.86678e+06) 0.353
I0815 13:00:26.065733  8794 solver.cpp:409] Finding and applying sparsity: 0.44
I0815 13:01:09.699960  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:01:09.702064  8764 solver.cpp:312] Iteration 63000 (1.61348 iter/s, 61.9779s/100 iter), loss = 1.26998
I0815 13:01:09.702080  8764 solver.cpp:334]     Train net output #0: loss = 1.04654 (* 1 = 1.04654 loss)
I0815 13:01:09.702086  8764 sgd_solver.cpp:136] Iteration 63000, lr = 0.0060625, m = 0.9
I0815 13:01:27.374088  8764 solver.cpp:312] Iteration 63100 (5.65882 iter/s, 17.6715s/100 iter), loss = 1.80403
I0815 13:01:27.374135  8764 solver.cpp:334]     Train net output #0: loss = 1.5742 (* 1 = 1.5742 loss)
I0815 13:01:27.374143  8764 sgd_solver.cpp:136] Iteration 63100, lr = 0.00605625, m = 0.9
I0815 13:01:43.115913  8764 solver.cpp:312] Iteration 63200 (6.35268 iter/s, 15.7414s/100 iter), loss = 1.6208
I0815 13:01:43.120846  8764 solver.cpp:334]     Train net output #0: loss = 1.52821 (* 1 = 1.52821 loss)
I0815 13:01:43.120852  8764 sgd_solver.cpp:136] Iteration 63200, lr = 0.00605, m = 0.9
I0815 13:01:59.462496  8764 solver.cpp:312] Iteration 63300 (6.11766 iter/s, 16.3461s/100 iter), loss = 1.13974
I0815 13:01:59.462538  8764 solver.cpp:334]     Train net output #0: loss = 1.06235 (* 1 = 1.06235 loss)
I0815 13:01:59.462554  8764 sgd_solver.cpp:136] Iteration 63300, lr = 0.00604375, m = 0.9
I0815 13:02:17.056931  8764 solver.cpp:312] Iteration 63400 (5.68377 iter/s, 17.594s/100 iter), loss = 1.20662
I0815 13:02:17.056995  8764 solver.cpp:334]     Train net output #0: loss = 1.47672 (* 1 = 1.47672 loss)
I0815 13:02:17.057000  8764 sgd_solver.cpp:136] Iteration 63400, lr = 0.0060375, m = 0.9
I0815 13:02:33.684144  8764 solver.cpp:312] Iteration 63500 (6.01441 iter/s, 16.6267s/100 iter), loss = 1.25218
I0815 13:02:33.684171  8764 solver.cpp:334]     Train net output #0: loss = 1.31544 (* 1 = 1.31544 loss)
I0815 13:02:33.684176  8764 sgd_solver.cpp:136] Iteration 63500, lr = 0.00603125, m = 0.9
I0815 13:02:52.544231  8764 solver.cpp:312] Iteration 63600 (5.30235 iter/s, 18.8596s/100 iter), loss = 1.59148
I0815 13:02:52.544293  8764 solver.cpp:334]     Train net output #0: loss = 1.51833 (* 1 = 1.51833 loss)
I0815 13:02:52.544299  8764 sgd_solver.cpp:136] Iteration 63600, lr = 0.006025, m = 0.9
I0815 13:03:09.138936  8764 solver.cpp:312] Iteration 63700 (6.02619 iter/s, 16.5942s/100 iter), loss = 1.23551
I0815 13:03:09.138964  8764 solver.cpp:334]     Train net output #0: loss = 1.29027 (* 1 = 1.29027 loss)
I0815 13:03:09.138969  8764 sgd_solver.cpp:136] Iteration 63700, lr = 0.00601875, m = 0.9
I0815 13:03:27.566093  8764 solver.cpp:312] Iteration 63800 (5.42692 iter/s, 18.4267s/100 iter), loss = 1.33243
I0815 13:03:27.566195  8764 solver.cpp:334]     Train net output #0: loss = 1.22377 (* 1 = 1.22377 loss)
I0815 13:03:27.566217  8764 sgd_solver.cpp:136] Iteration 63800, lr = 0.0060125, m = 0.9
I0815 13:03:43.835019  8764 solver.cpp:312] Iteration 63900 (6.14686 iter/s, 16.2685s/100 iter), loss = 1.70912
I0815 13:03:43.835047  8764 solver.cpp:334]     Train net output #0: loss = 2.01047 (* 1 = 2.01047 loss)
I0815 13:03:43.835054  8764 sgd_solver.cpp:136] Iteration 63900, lr = 0.00600625, m = 0.9
I0815 13:03:59.483263  8764 solver.cpp:363] Sparsity after update:
I0815 13:03:59.488739  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:03:59.488752  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:03:59.488761  8764 net.cpp:2192] conv1b_param_0(0.431) 
I0815 13:03:59.488765  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:03:59.488772  8764 net.cpp:2192] res2a_branch2a_param_0(0.438) 
I0815 13:03:59.488775  8764 net.cpp:2192] res2a_branch2b_param_0(0.438) 
I0815 13:03:59.488780  8764 net.cpp:2192] res3a_branch2a_param_0(0.439) 
I0815 13:03:59.488785  8764 net.cpp:2192] res3a_branch2b_param_0(0.438) 
I0815 13:03:59.488790  8764 net.cpp:2192] res4a_branch2a_param_0(0.439) 
I0815 13:03:59.488793  8764 net.cpp:2192] res4a_branch2b_param_0(0.439) 
I0815 13:03:59.488796  8764 net.cpp:2192] res5a_branch2a_param_0(0.44) 
I0815 13:03:59.488801  8764 net.cpp:2192] res5a_branch2b_param_0(0.439) 
I0815 13:03:59.488803  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.03413e+06/2.86678e+06) 0.361
I0815 13:03:59.488814  8764 solver.cpp:509] Iteration 64000, Testing net (#0)
I0815 13:04:26.445896  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.573353
I0815 13:04:26.445919  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805527
I0815 13:04:26.445924  8764 solver.cpp:594]     Test net output #2: loss = 1.87058 (* 1 = 1.87058 loss)
I0815 13:04:26.445947  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9564s
I0815 13:04:26.606147  8794 solver.cpp:409] Finding and applying sparsity: 0.45
I0815 13:04:54.008700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:04:54.010720  8764 solver.cpp:312] Iteration 64000 (1.42503 iter/s, 70.1738s/100 iter), loss = 1.5287
I0815 13:04:54.010745  8764 solver.cpp:334]     Train net output #0: loss = 2.04011 (* 1 = 2.04011 loss)
I0815 13:04:54.010752  8764 sgd_solver.cpp:136] Iteration 64000, lr = 0.006, m = 0.9
I0815 13:05:10.551008  8764 solver.cpp:312] Iteration 64100 (6.04601 iter/s, 16.5398s/100 iter), loss = 1.55201
I0815 13:05:10.551059  8764 solver.cpp:334]     Train net output #0: loss = 1.23312 (* 1 = 1.23312 loss)
I0815 13:05:10.551071  8764 sgd_solver.cpp:136] Iteration 64100, lr = 0.00599375, m = 0.9
I0815 13:05:27.030252  8764 solver.cpp:312] Iteration 64200 (6.06841 iter/s, 16.4788s/100 iter), loss = 1.12914
I0815 13:05:27.030345  8764 solver.cpp:334]     Train net output #0: loss = 1.41669 (* 1 = 1.41669 loss)
I0815 13:05:27.030369  8764 sgd_solver.cpp:136] Iteration 64200, lr = 0.0059875, m = 0.9
I0815 13:05:45.112574  8764 solver.cpp:312] Iteration 64300 (5.53042 iter/s, 18.0818s/100 iter), loss = 1.76796
I0815 13:05:45.112645  8764 solver.cpp:334]     Train net output #0: loss = 1.83705 (* 1 = 1.83705 loss)
I0815 13:05:45.112665  8764 sgd_solver.cpp:136] Iteration 64300, lr = 0.00598125, m = 0.9
I0815 13:06:02.738343  8764 solver.cpp:312] Iteration 64400 (5.67367 iter/s, 17.6253s/100 iter), loss = 1.58658
I0815 13:06:02.738524  8764 solver.cpp:334]     Train net output #0: loss = 1.5808 (* 1 = 1.5808 loss)
I0815 13:06:02.738544  8764 sgd_solver.cpp:136] Iteration 64400, lr = 0.005975, m = 0.9
I0815 13:06:20.789386  8764 solver.cpp:312] Iteration 64500 (5.54 iter/s, 18.0505s/100 iter), loss = 1.18877
I0815 13:06:20.789425  8764 solver.cpp:334]     Train net output #0: loss = 1.639 (* 1 = 1.639 loss)
I0815 13:06:20.789435  8764 sgd_solver.cpp:136] Iteration 64500, lr = 0.00596875, m = 0.9
I0815 13:06:38.402802  8764 solver.cpp:312] Iteration 64600 (5.67765 iter/s, 17.6129s/100 iter), loss = 1.7929
I0815 13:06:38.403008  8764 solver.cpp:334]     Train net output #0: loss = 1.73411 (* 1 = 1.73411 loss)
I0815 13:06:38.403086  8764 sgd_solver.cpp:136] Iteration 64600, lr = 0.0059625, m = 0.9
I0815 13:06:54.630553  8764 solver.cpp:312] Iteration 64700 (6.16245 iter/s, 16.2273s/100 iter), loss = 1.7429
I0815 13:06:54.630580  8764 solver.cpp:334]     Train net output #0: loss = 1.84382 (* 1 = 1.84382 loss)
I0815 13:06:54.630585  8764 sgd_solver.cpp:136] Iteration 64700, lr = 0.00595625, m = 0.9
I0815 13:07:10.981813  8764 solver.cpp:312] Iteration 64800 (6.11591 iter/s, 16.3508s/100 iter), loss = 1.68711
I0815 13:07:10.981878  8764 solver.cpp:334]     Train net output #0: loss = 1.77009 (* 1 = 1.77009 loss)
I0815 13:07:10.981885  8764 sgd_solver.cpp:136] Iteration 64800, lr = 0.00595, m = 0.9
I0815 13:07:25.416648  8764 solver.cpp:312] Iteration 64900 (6.92787 iter/s, 14.4344s/100 iter), loss = 1.4653
I0815 13:07:25.416713  8764 solver.cpp:334]     Train net output #0: loss = 1.47136 (* 1 = 1.47136 loss)
I0815 13:07:25.416730  8764 sgd_solver.cpp:136] Iteration 64900, lr = 0.00594375, m = 0.9
I0815 13:07:40.959288  8764 solver.cpp:363] Sparsity after update:
I0815 13:07:40.977955  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:07:40.978016  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:07:40.978042  8764 net.cpp:2192] conv1b_param_0(0.444) 
I0815 13:07:40.978056  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:07:40.978070  8764 net.cpp:2192] res2a_branch2a_param_0(0.448) 
I0815 13:07:40.978083  8764 net.cpp:2192] res2a_branch2b_param_0(0.444) 
I0815 13:07:40.978096  8764 net.cpp:2192] res3a_branch2a_param_0(0.45) 
I0815 13:07:40.978109  8764 net.cpp:2192] res3a_branch2b_param_0(0.448) 
I0815 13:07:40.978121  8764 net.cpp:2192] res4a_branch2a_param_0(0.45) 
I0815 13:07:40.978133  8764 net.cpp:2192] res4a_branch2b_param_0(0.45) 
I0815 13:07:40.978147  8764 net.cpp:2192] res5a_branch2a_param_0(0.45) 
I0815 13:07:40.978159  8764 net.cpp:2192] res5a_branch2b_param_0(0.45) 
I0815 13:07:40.978173  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.0581e+06/2.86678e+06) 0.369
I0815 13:07:41.145735  8794 solver.cpp:409] Finding and applying sparsity: 0.46
I0815 13:08:17.089399  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:08:17.091718  8764 solver.cpp:312] Iteration 65000 (1.93522 iter/s, 51.6737s/100 iter), loss = 1.17349
I0815 13:08:17.091755  8764 solver.cpp:334]     Train net output #0: loss = 1.41889 (* 1 = 1.41889 loss)
I0815 13:08:17.091773  8764 sgd_solver.cpp:136] Iteration 65000, lr = 0.0059375, m = 0.9
I0815 13:08:37.717114  8764 solver.cpp:312] Iteration 65100 (4.84853 iter/s, 20.6248s/100 iter), loss = 1.3322
I0815 13:08:37.717187  8764 solver.cpp:334]     Train net output #0: loss = 1.46155 (* 1 = 1.46155 loss)
I0815 13:08:37.717213  8764 sgd_solver.cpp:136] Iteration 65100, lr = 0.00593125, m = 0.9
I0815 13:08:58.872285  8764 solver.cpp:312] Iteration 65200 (4.72711 iter/s, 21.1546s/100 iter), loss = 1.58034
I0815 13:08:58.872376  8764 solver.cpp:334]     Train net output #0: loss = 1.53174 (* 1 = 1.53174 loss)
I0815 13:08:58.872390  8764 sgd_solver.cpp:136] Iteration 65200, lr = 0.005925, m = 0.9
I0815 13:09:16.897444  8764 solver.cpp:312] Iteration 65300 (5.54795 iter/s, 18.0247s/100 iter), loss = 1.49843
I0815 13:09:16.897585  8764 solver.cpp:334]     Train net output #0: loss = 1.68334 (* 1 = 1.68334 loss)
I0815 13:09:16.897601  8764 sgd_solver.cpp:136] Iteration 65300, lr = 0.00591875, m = 0.9
I0815 13:09:36.164700  8764 solver.cpp:312] Iteration 65400 (5.1903 iter/s, 19.2667s/100 iter), loss = 1.37127
I0815 13:09:36.164757  8764 solver.cpp:334]     Train net output #0: loss = 1.62407 (* 1 = 1.62407 loss)
I0815 13:09:36.164763  8764 sgd_solver.cpp:136] Iteration 65400, lr = 0.0059125, m = 0.9
I0815 13:09:57.621767  8764 solver.cpp:312] Iteration 65500 (4.66061 iter/s, 21.4564s/100 iter), loss = 1.28434
I0815 13:09:57.622195  8764 solver.cpp:334]     Train net output #0: loss = 1.13793 (* 1 = 1.13793 loss)
I0815 13:09:57.622387  8764 sgd_solver.cpp:136] Iteration 65500, lr = 0.00590625, m = 0.9
I0815 13:10:15.129700  8764 solver.cpp:312] Iteration 65600 (5.71185 iter/s, 17.5075s/100 iter), loss = 1.33897
I0815 13:10:15.129811  8764 solver.cpp:334]     Train net output #0: loss = 1.06932 (* 1 = 1.06932 loss)
I0815 13:10:15.129828  8764 sgd_solver.cpp:136] Iteration 65600, lr = 0.0059, m = 0.9
I0815 13:10:32.135716  8764 solver.cpp:312] Iteration 65700 (5.88044 iter/s, 17.0055s/100 iter), loss = 1.16395
I0815 13:10:32.135764  8764 solver.cpp:334]     Train net output #0: loss = 1.08777 (* 1 = 1.08777 loss)
I0815 13:10:32.135776  8764 sgd_solver.cpp:136] Iteration 65700, lr = 0.00589375, m = 0.9
I0815 13:10:40.084488  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:10:56.500113  8764 solver.cpp:312] Iteration 65800 (4.10447 iter/s, 24.3637s/100 iter), loss = 1.42356
I0815 13:10:56.500423  8764 solver.cpp:334]     Train net output #0: loss = 1.32615 (* 1 = 1.32615 loss)
I0815 13:10:56.500437  8764 sgd_solver.cpp:136] Iteration 65800, lr = 0.0058875, m = 0.9
I0815 13:11:17.671625  8764 solver.cpp:312] Iteration 65900 (4.72346 iter/s, 21.1709s/100 iter), loss = 1.60577
I0815 13:11:17.671679  8764 solver.cpp:334]     Train net output #0: loss = 2.11977 (* 1 = 2.11977 loss)
I0815 13:11:17.671689  8764 sgd_solver.cpp:136] Iteration 65900, lr = 0.00588125, m = 0.9
I0815 13:11:35.522290  8764 solver.cpp:363] Sparsity after update:
I0815 13:11:35.525642  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:11:35.525657  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:11:35.525662  8764 net.cpp:2192] conv1b_param_0(0.458) 
I0815 13:11:35.525665  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:11:35.525667  8764 net.cpp:2192] res2a_branch2a_param_0(0.458) 
I0815 13:11:35.525669  8764 net.cpp:2192] res2a_branch2b_param_0(0.458) 
I0815 13:11:35.525671  8764 net.cpp:2192] res3a_branch2a_param_0(0.458) 
I0815 13:11:35.525673  8764 net.cpp:2192] res3a_branch2b_param_0(0.458) 
I0815 13:11:35.525676  8764 net.cpp:2192] res4a_branch2a_param_0(0.459) 
I0815 13:11:35.525678  8764 net.cpp:2192] res4a_branch2b_param_0(0.458) 
I0815 13:11:35.525681  8764 net.cpp:2192] res5a_branch2a_param_0(0.46) 
I0815 13:11:35.525682  8764 net.cpp:2192] res5a_branch2b_param_0(0.459) 
I0815 13:11:35.525684  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.08101e+06/2.86678e+06) 0.377
I0815 13:11:35.525693  8764 solver.cpp:509] Iteration 66000, Testing net (#0)
I0815 13:12:06.323820  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571529
I0815 13:12:06.323870  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.80588
I0815 13:12:06.323876  8764 solver.cpp:594]     Test net output #2: loss = 1.86551 (* 1 = 1.86551 loss)
I0815 13:12:06.336163  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.8096s
I0815 13:12:06.471952  8794 solver.cpp:409] Finding and applying sparsity: 0.47
I0815 13:12:52.640643  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:12:52.643153  8764 solver.cpp:312] Iteration 66000 (1.05298 iter/s, 94.9689s/100 iter), loss = 1.38213
I0815 13:12:52.643193  8764 solver.cpp:334]     Train net output #0: loss = 1.51744 (* 1 = 1.51744 loss)
I0815 13:12:52.643211  8764 sgd_solver.cpp:136] Iteration 66000, lr = 0.005875, m = 0.9
I0815 13:13:13.029465  8764 solver.cpp:312] Iteration 66100 (4.90539 iter/s, 20.3857s/100 iter), loss = 1.11908
I0815 13:13:13.029531  8764 solver.cpp:334]     Train net output #0: loss = 0.744224 (* 1 = 0.744224 loss)
I0815 13:13:13.029552  8764 sgd_solver.cpp:136] Iteration 66100, lr = 0.00586875, m = 0.9
I0815 13:13:30.236754  8764 solver.cpp:312] Iteration 66200 (5.81165 iter/s, 17.2068s/100 iter), loss = 1.29443
I0815 13:13:30.236821  8764 solver.cpp:334]     Train net output #0: loss = 1.67036 (* 1 = 1.67036 loss)
I0815 13:13:30.236827  8764 sgd_solver.cpp:136] Iteration 66200, lr = 0.0058625, m = 0.9
I0815 13:13:47.692559  8764 solver.cpp:312] Iteration 66300 (5.72891 iter/s, 17.4553s/100 iter), loss = 1.4839
I0815 13:13:47.692612  8764 solver.cpp:334]     Train net output #0: loss = 1.37712 (* 1 = 1.37712 loss)
I0815 13:13:47.692627  8764 sgd_solver.cpp:136] Iteration 66300, lr = 0.00585625, m = 0.9
I0815 13:14:05.624788  8764 solver.cpp:312] Iteration 66400 (5.57671 iter/s, 17.9317s/100 iter), loss = 1.1872
I0815 13:14:05.624923  8764 solver.cpp:334]     Train net output #0: loss = 1.14941 (* 1 = 1.14941 loss)
I0815 13:14:05.624938  8764 sgd_solver.cpp:136] Iteration 66400, lr = 0.00585, m = 0.9
I0815 13:14:27.483467  8764 solver.cpp:312] Iteration 66500 (4.57497 iter/s, 21.8581s/100 iter), loss = 1.39026
I0815 13:14:27.483690  8764 solver.cpp:334]     Train net output #0: loss = 1.47278 (* 1 = 1.47278 loss)
I0815 13:14:27.483800  8764 sgd_solver.cpp:136] Iteration 66500, lr = 0.00584375, m = 0.9
I0815 13:14:47.518101  8764 solver.cpp:312] Iteration 66600 (4.9915 iter/s, 20.0341s/100 iter), loss = 1.17274
I0815 13:14:47.518221  8764 solver.cpp:334]     Train net output #0: loss = 1.20311 (* 1 = 1.20311 loss)
I0815 13:14:47.518239  8764 sgd_solver.cpp:136] Iteration 66600, lr = 0.0058375, m = 0.9
I0815 13:15:03.512670  8764 solver.cpp:312] Iteration 66700 (6.25229 iter/s, 15.9941s/100 iter), loss = 1.51075
I0815 13:15:03.512734  8764 solver.cpp:334]     Train net output #0: loss = 1.62757 (* 1 = 1.62757 loss)
I0815 13:15:03.512758  8764 sgd_solver.cpp:136] Iteration 66700, lr = 0.00583125, m = 0.9
I0815 13:15:21.602460  8764 solver.cpp:312] Iteration 66800 (5.52813 iter/s, 18.0893s/100 iter), loss = 1.21686
I0815 13:15:21.602588  8764 solver.cpp:334]     Train net output #0: loss = 1.0644 (* 1 = 1.0644 loss)
I0815 13:15:21.602614  8764 sgd_solver.cpp:136] Iteration 66800, lr = 0.005825, m = 0.9
I0815 13:15:38.718693  8764 solver.cpp:312] Iteration 66900 (5.84257 iter/s, 17.1158s/100 iter), loss = 1.37457
I0815 13:15:38.718729  8764 solver.cpp:334]     Train net output #0: loss = 1.30464 (* 1 = 1.30464 loss)
I0815 13:15:38.718735  8764 sgd_solver.cpp:136] Iteration 66900, lr = 0.00581875, m = 0.9
I0815 13:16:00.345567  8764 solver.cpp:363] Sparsity after update:
I0815 13:16:00.356160  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:16:00.356178  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:16:00.356204  8764 net.cpp:2192] conv1b_param_0(0.458) 
I0815 13:16:00.356214  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:16:00.356230  8764 net.cpp:2192] res2a_branch2a_param_0(0.469) 
I0815 13:16:00.356236  8764 net.cpp:2192] res2a_branch2b_param_0(0.465) 
I0815 13:16:00.356250  8764 net.cpp:2192] res3a_branch2a_param_0(0.469) 
I0815 13:16:00.356257  8764 net.cpp:2192] res3a_branch2b_param_0(0.469) 
I0815 13:16:00.356271  8764 net.cpp:2192] res4a_branch2a_param_0(0.47) 
I0815 13:16:00.356277  8764 net.cpp:2192] res4a_branch2b_param_0(0.469) 
I0815 13:16:00.356286  8764 net.cpp:2192] res5a_branch2a_param_0(0.47) 
I0815 13:16:00.356298  8764 net.cpp:2192] res5a_branch2b_param_0(0.47) 
I0815 13:16:00.356305  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.10495e+06/2.86678e+06) 0.385
I0815 13:16:00.571846  8794 solver.cpp:409] Finding and applying sparsity: 0.48
I0815 13:16:48.113127  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:16:48.115517  8764 solver.cpp:312] Iteration 67000 (1.44103 iter/s, 69.3949s/100 iter), loss = 1.52261
I0815 13:16:48.115556  8764 solver.cpp:334]     Train net output #0: loss = 1.69908 (* 1 = 1.69908 loss)
I0815 13:16:48.115569  8764 sgd_solver.cpp:136] Iteration 67000, lr = 0.0058125, m = 0.9
I0815 13:17:06.513216  8764 solver.cpp:312] Iteration 67100 (5.43561 iter/s, 18.3972s/100 iter), loss = 1.20898
I0815 13:17:06.513243  8764 solver.cpp:334]     Train net output #0: loss = 1.11067 (* 1 = 1.11067 loss)
I0815 13:17:06.513247  8764 sgd_solver.cpp:136] Iteration 67100, lr = 0.00580625, m = 0.9
I0815 13:17:23.922999  8764 solver.cpp:312] Iteration 67200 (5.74406 iter/s, 17.4093s/100 iter), loss = 1.42368
I0815 13:17:23.923099  8764 solver.cpp:334]     Train net output #0: loss = 1.52245 (* 1 = 1.52245 loss)
I0815 13:17:23.923115  8764 sgd_solver.cpp:136] Iteration 67200, lr = 0.0058, m = 0.9
I0815 13:17:40.079193  8764 solver.cpp:312] Iteration 67300 (6.18975 iter/s, 16.1557s/100 iter), loss = 1.31919
I0815 13:17:40.079252  8764 solver.cpp:334]     Train net output #0: loss = 1.21525 (* 1 = 1.21525 loss)
I0815 13:17:40.079262  8764 sgd_solver.cpp:136] Iteration 67300, lr = 0.00579375, m = 0.9
I0815 13:17:57.210067  8764 solver.cpp:312] Iteration 67400 (5.83758 iter/s, 17.1304s/100 iter), loss = 1.18411
I0815 13:17:57.210155  8764 solver.cpp:334]     Train net output #0: loss = 1.04356 (* 1 = 1.04356 loss)
I0815 13:17:57.210176  8764 sgd_solver.cpp:136] Iteration 67400, lr = 0.0057875, m = 0.9
I0815 13:18:14.100121  8764 solver.cpp:312] Iteration 67500 (5.92081 iter/s, 16.8896s/100 iter), loss = 1.30163
I0815 13:18:14.100157  8764 solver.cpp:334]     Train net output #0: loss = 1.12928 (* 1 = 1.12928 loss)
I0815 13:18:14.100162  8764 sgd_solver.cpp:136] Iteration 67500, lr = 0.00578125, m = 0.9
I0815 13:18:31.593940  8764 solver.cpp:312] Iteration 67600 (5.71646 iter/s, 17.4933s/100 iter), loss = 1.19478
I0815 13:18:31.594002  8764 solver.cpp:334]     Train net output #0: loss = 1.10683 (* 1 = 1.10683 loss)
I0815 13:18:31.594008  8764 sgd_solver.cpp:136] Iteration 67600, lr = 0.005775, m = 0.9
I0815 13:18:46.327811  8764 solver.cpp:312] Iteration 67700 (6.78727 iter/s, 14.7335s/100 iter), loss = 1.16353
I0815 13:18:46.327875  8764 solver.cpp:334]     Train net output #0: loss = 1.07391 (* 1 = 1.07391 loss)
I0815 13:18:46.327893  8764 sgd_solver.cpp:136] Iteration 67700, lr = 0.00576875, m = 0.9
I0815 13:19:03.612557  8764 solver.cpp:312] Iteration 67800 (5.78561 iter/s, 17.2843s/100 iter), loss = 1.3854
I0815 13:19:03.612644  8764 solver.cpp:334]     Train net output #0: loss = 1.33172 (* 1 = 1.33172 loss)
I0815 13:19:03.612653  8764 sgd_solver.cpp:136] Iteration 67800, lr = 0.0057625, m = 0.9
I0815 13:19:22.164736  8764 solver.cpp:312] Iteration 67900 (5.39035 iter/s, 18.5517s/100 iter), loss = 1.54934
I0815 13:19:22.164779  8764 solver.cpp:334]     Train net output #0: loss = 1.60223 (* 1 = 1.60223 loss)
I0815 13:19:22.164786  8764 sgd_solver.cpp:136] Iteration 67900, lr = 0.00575625, m = 0.9
I0815 13:19:40.792330  8764 solver.cpp:363] Sparsity after update:
I0815 13:19:40.797329  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:19:40.797341  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:19:40.797349  8764 net.cpp:2192] conv1b_param_0(0.472) 
I0815 13:19:40.797353  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:19:40.797358  8764 net.cpp:2192] res2a_branch2a_param_0(0.479) 
I0815 13:19:40.797361  8764 net.cpp:2192] res2a_branch2b_param_0(0.479) 
I0815 13:19:40.797364  8764 net.cpp:2192] res3a_branch2a_param_0(0.479) 
I0815 13:19:40.797368  8764 net.cpp:2192] res3a_branch2b_param_0(0.479) 
I0815 13:19:40.797370  8764 net.cpp:2192] res4a_branch2a_param_0(0.479) 
I0815 13:19:40.797374  8764 net.cpp:2192] res4a_branch2b_param_0(0.479) 
I0815 13:19:40.797375  8764 net.cpp:2192] res5a_branch2a_param_0(0.48) 
I0815 13:19:40.797379  8764 net.cpp:2192] res5a_branch2b_param_0(0.479) 
I0815 13:19:40.797382  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.12821e+06/2.86678e+06) 0.394
I0815 13:19:40.797392  8764 solver.cpp:509] Iteration 68000, Testing net (#0)
I0815 13:20:05.065351  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575059
I0815 13:20:05.065399  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.806057
I0815 13:20:05.065418  8764 solver.cpp:594]     Test net output #2: loss = 1.88469 (* 1 = 1.88469 loss)
I0815 13:20:05.065474  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.2674s
I0815 13:20:05.208216  8794 solver.cpp:409] Finding and applying sparsity: 0.49
I0815 13:20:56.183220  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:20:56.185338  8764 solver.cpp:312] Iteration 68000 (1.06363 iter/s, 94.018s/100 iter), loss = 1.6056
I0815 13:20:56.185355  8764 solver.cpp:334]     Train net output #0: loss = 1.29496 (* 1 = 1.29496 loss)
I0815 13:20:56.185360  8764 sgd_solver.cpp:136] Iteration 68000, lr = 0.00575, m = 0.9
I0815 13:21:12.376251  8764 solver.cpp:312] Iteration 68100 (6.17649 iter/s, 16.1904s/100 iter), loss = 1.3345
I0815 13:21:12.376441  8764 solver.cpp:334]     Train net output #0: loss = 0.970617 (* 1 = 0.970617 loss)
I0815 13:21:12.376514  8764 sgd_solver.cpp:136] Iteration 68100, lr = 0.00574375, m = 0.9
I0815 13:21:32.906891  8764 solver.cpp:312] Iteration 68200 (4.8709 iter/s, 20.5301s/100 iter), loss = 1.40848
I0815 13:21:32.906962  8764 solver.cpp:334]     Train net output #0: loss = 1.40567 (* 1 = 1.40567 loss)
I0815 13:21:32.906970  8764 sgd_solver.cpp:136] Iteration 68200, lr = 0.0057375, m = 0.9
I0815 13:21:58.998153  8764 solver.cpp:312] Iteration 68300 (3.83281 iter/s, 26.0905s/100 iter), loss = 1.91848
I0815 13:21:58.998200  8764 solver.cpp:334]     Train net output #0: loss = 1.72673 (* 1 = 1.72673 loss)
I0815 13:21:58.998211  8764 sgd_solver.cpp:136] Iteration 68300, lr = 0.00573125, m = 0.9
I0815 13:22:18.028627  8764 solver.cpp:312] Iteration 68400 (5.25488 iter/s, 19.0299s/100 iter), loss = 1.71025
I0815 13:22:18.028690  8764 solver.cpp:334]     Train net output #0: loss = 1.89361 (* 1 = 1.89361 loss)
I0815 13:22:18.028699  8764 sgd_solver.cpp:136] Iteration 68400, lr = 0.005725, m = 0.9
I0815 13:22:34.064322  8764 solver.cpp:312] Iteration 68500 (6.23626 iter/s, 16.0352s/100 iter), loss = 1.30958
I0815 13:22:34.064391  8764 solver.cpp:334]     Train net output #0: loss = 1.53588 (* 1 = 1.53588 loss)
I0815 13:22:34.064410  8764 sgd_solver.cpp:136] Iteration 68500, lr = 0.00571875, m = 0.9
I0815 13:22:55.090487  8764 solver.cpp:312] Iteration 68600 (4.75611 iter/s, 21.0256s/100 iter), loss = 1.16677
I0815 13:22:55.090581  8764 solver.cpp:334]     Train net output #0: loss = 1.44158 (* 1 = 1.44158 loss)
I0815 13:22:55.090600  8764 sgd_solver.cpp:136] Iteration 68600, lr = 0.0057125, m = 0.9
I0815 13:23:03.740913  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:23:10.270617  8764 solver.cpp:312] Iteration 68700 (6.58774 iter/s, 15.1797s/100 iter), loss = 1.34469
I0815 13:23:10.270690  8764 solver.cpp:334]     Train net output #0: loss = 1.2625 (* 1 = 1.2625 loss)
I0815 13:23:10.270709  8764 sgd_solver.cpp:136] Iteration 68700, lr = 0.00570625, m = 0.9
I0815 13:23:25.967900  8764 solver.cpp:312] Iteration 68800 (6.3707 iter/s, 15.6969s/100 iter), loss = 1.2449
I0815 13:23:25.967957  8764 solver.cpp:334]     Train net output #0: loss = 1.40576 (* 1 = 1.40576 loss)
I0815 13:23:25.967964  8764 sgd_solver.cpp:136] Iteration 68800, lr = 0.0057, m = 0.9
I0815 13:23:42.088192  8764 solver.cpp:312] Iteration 68900 (6.20353 iter/s, 16.1198s/100 iter), loss = 0.934832
I0815 13:23:42.088259  8764 solver.cpp:334]     Train net output #0: loss = 1.09363 (* 1 = 1.09363 loss)
I0815 13:23:42.088284  8764 sgd_solver.cpp:136] Iteration 68900, lr = 0.00569375, m = 0.9
I0815 13:24:00.063720  8764 solver.cpp:363] Sparsity after update:
I0815 13:24:00.077392  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:24:00.077435  8764 net.cpp:2192] conv1a_param_0(0.239) 
I0815 13:24:00.077448  8764 net.cpp:2192] conv1b_param_0(0.486) 
I0815 13:24:00.077457  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:24:00.077466  8764 net.cpp:2192] res2a_branch2a_param_0(0.49) 
I0815 13:24:00.077476  8764 net.cpp:2192] res2a_branch2b_param_0(0.486) 
I0815 13:24:00.077483  8764 net.cpp:2192] res3a_branch2a_param_0(0.49) 
I0815 13:24:00.077491  8764 net.cpp:2192] res3a_branch2b_param_0(0.49) 
I0815 13:24:00.077499  8764 net.cpp:2192] res4a_branch2a_param_0(0.49) 
I0815 13:24:00.077508  8764 net.cpp:2192] res4a_branch2b_param_0(0.49) 
I0815 13:24:00.077517  8764 net.cpp:2192] res5a_branch2a_param_0(0.49) 
I0815 13:24:00.077527  8764 net.cpp:2192] res5a_branch2b_param_0(0.49) 
I0815 13:24:00.077535  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.15221e+06/2.86678e+06) 0.402
I0815 13:24:00.269779  8794 solver.cpp:409] Finding and applying sparsity: 0.5
I0815 13:24:32.477629  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:24:32.479812  8764 solver.cpp:312] Iteration 69000 (1.98451 iter/s, 50.3902s/100 iter), loss = 1.68543
I0815 13:24:32.479835  8764 solver.cpp:334]     Train net output #0: loss = 1.56553 (* 1 = 1.56553 loss)
I0815 13:24:32.479842  8764 sgd_solver.cpp:136] Iteration 69000, lr = 0.0056875, m = 0.9
I0815 13:24:49.495286  8764 solver.cpp:312] Iteration 69100 (5.87717 iter/s, 17.015s/100 iter), loss = 1.28258
I0815 13:24:49.495528  8764 solver.cpp:334]     Train net output #0: loss = 1.45268 (* 1 = 1.45268 loss)
I0815 13:24:49.495647  8764 sgd_solver.cpp:136] Iteration 69100, lr = 0.00568125, m = 0.9
I0815 13:25:06.594962  8764 solver.cpp:312] Iteration 69200 (5.84823 iter/s, 17.0992s/100 iter), loss = 1.47579
I0815 13:25:06.595021  8764 solver.cpp:334]     Train net output #0: loss = 1.24272 (* 1 = 1.24272 loss)
I0815 13:25:06.595026  8764 sgd_solver.cpp:136] Iteration 69200, lr = 0.005675, m = 0.9
I0815 13:25:23.462337  8764 solver.cpp:312] Iteration 69300 (5.92878 iter/s, 16.8669s/100 iter), loss = 1.49861
I0815 13:25:23.462399  8764 solver.cpp:334]     Train net output #0: loss = 1.01725 (* 1 = 1.01725 loss)
I0815 13:25:23.462412  8764 sgd_solver.cpp:136] Iteration 69300, lr = 0.00566875, m = 0.9
I0815 13:25:39.891876  8764 solver.cpp:312] Iteration 69400 (6.08676 iter/s, 16.4291s/100 iter), loss = 1.55036
I0815 13:25:39.891960  8764 solver.cpp:334]     Train net output #0: loss = 1.14888 (* 1 = 1.14888 loss)
I0815 13:25:39.891973  8764 sgd_solver.cpp:136] Iteration 69400, lr = 0.0056625, m = 0.9
I0815 13:25:55.466994  8764 solver.cpp:312] Iteration 69500 (6.42067 iter/s, 15.5747s/100 iter), loss = 1.39307
I0815 13:25:55.467022  8764 solver.cpp:334]     Train net output #0: loss = 1.41923 (* 1 = 1.41923 loss)
I0815 13:25:55.467028  8764 sgd_solver.cpp:136] Iteration 69500, lr = 0.00565625, m = 0.9
I0815 13:26:10.572741  8764 solver.cpp:312] Iteration 69600 (6.62018 iter/s, 15.1053s/100 iter), loss = 1.2397
I0815 13:26:10.572796  8764 solver.cpp:334]     Train net output #0: loss = 1.42685 (* 1 = 1.42685 loss)
I0815 13:26:10.572803  8764 sgd_solver.cpp:136] Iteration 69600, lr = 0.00565, m = 0.9
I0815 13:26:26.268477  8764 solver.cpp:312] Iteration 69700 (6.37133 iter/s, 15.6953s/100 iter), loss = 1.53504
I0815 13:26:26.268551  8764 solver.cpp:334]     Train net output #0: loss = 1.40105 (* 1 = 1.40105 loss)
I0815 13:26:26.268570  8764 sgd_solver.cpp:136] Iteration 69700, lr = 0.00564375, m = 0.9
I0815 13:26:42.499006  8764 solver.cpp:312] Iteration 69800 (6.1614 iter/s, 16.2301s/100 iter), loss = 1.57601
I0815 13:26:42.499073  8764 solver.cpp:334]     Train net output #0: loss = 1.7385 (* 1 = 1.7385 loss)
I0815 13:26:42.499081  8764 sgd_solver.cpp:136] Iteration 69800, lr = 0.0056375, m = 0.9
I0815 13:27:00.303617  8764 solver.cpp:312] Iteration 69900 (5.61667 iter/s, 17.8041s/100 iter), loss = 1.18314
I0815 13:27:00.303639  8764 solver.cpp:334]     Train net output #0: loss = 1.41551 (* 1 = 1.41551 loss)
I0815 13:27:00.303642  8764 sgd_solver.cpp:136] Iteration 69900, lr = 0.00563125, m = 0.9
I0815 13:27:18.280887  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_70000.caffemodel
I0815 13:27:18.460563  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_70000.solverstate
I0815 13:27:18.468539  8764 solver.cpp:363] Sparsity after update:
I0815 13:27:18.471418  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:27:18.472347  8764 net.cpp:2192] conv1a_param_0(0.239) 
I0815 13:27:18.472383  8764 net.cpp:2192] conv1b_param_0(0.486) 
I0815 13:27:18.472400  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:27:18.472417  8764 net.cpp:2192] res2a_branch2a_param_0(0.497) 
I0815 13:27:18.472432  8764 net.cpp:2192] res2a_branch2b_param_0(0.493) 
I0815 13:27:18.472446  8764 net.cpp:2192] res3a_branch2a_param_0(0.498) 
I0815 13:27:18.472460  8764 net.cpp:2192] res3a_branch2b_param_0(0.497) 
I0815 13:27:18.472472  8764 net.cpp:2192] res4a_branch2a_param_0(0.499) 
I0815 13:27:18.472486  8764 net.cpp:2192] res4a_branch2b_param_0(0.498) 
I0815 13:27:18.472507  8764 net.cpp:2192] res5a_branch2a_param_0(0.5) 
I0815 13:27:18.472519  8764 net.cpp:2192] res5a_branch2b_param_0(0.499) 
I0815 13:27:18.472530  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.1748e+06/2.86678e+06) 0.41
I0815 13:27:18.472554  8764 solver.cpp:509] Iteration 70000, Testing net (#0)
I0815 13:27:43.539031  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58347
I0815 13:27:43.539049  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815115
I0815 13:27:43.539057  8764 solver.cpp:594]     Test net output #2: loss = 1.83643 (* 1 = 1.83643 loss)
I0815 13:27:43.539078  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.0658s
I0815 13:27:43.697067  8794 solver.cpp:409] Finding and applying sparsity: 0.51
I0815 13:28:13.921357  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:28:13.923425  8764 solver.cpp:312] Iteration 70000 (1.35837 iter/s, 73.6178s/100 iter), loss = 1.4378
I0815 13:28:13.923449  8764 solver.cpp:334]     Train net output #0: loss = 1.44726 (* 1 = 1.44726 loss)
I0815 13:28:13.923460  8764 sgd_solver.cpp:136] Iteration 70000, lr = 0.005625, m = 0.9
I0815 13:28:30.126468  8764 solver.cpp:312] Iteration 70100 (6.17185 iter/s, 16.2026s/100 iter), loss = 1.2762
I0815 13:28:30.126518  8764 solver.cpp:334]     Train net output #0: loss = 1.21528 (* 1 = 1.21528 loss)
I0815 13:28:30.126530  8764 sgd_solver.cpp:136] Iteration 70100, lr = 0.00561875, m = 0.9
I0815 13:28:47.920068  8764 solver.cpp:312] Iteration 70200 (5.62015 iter/s, 17.7931s/100 iter), loss = 1.20405
I0815 13:28:47.920132  8764 solver.cpp:334]     Train net output #0: loss = 1.3407 (* 1 = 1.3407 loss)
I0815 13:28:47.920140  8764 sgd_solver.cpp:136] Iteration 70200, lr = 0.0056125, m = 0.9
I0815 13:29:03.682466  8764 solver.cpp:312] Iteration 70300 (6.34439 iter/s, 15.762s/100 iter), loss = 1.63355
I0815 13:29:03.682519  8764 solver.cpp:334]     Train net output #0: loss = 1.10683 (* 1 = 1.10683 loss)
I0815 13:29:03.682533  8764 sgd_solver.cpp:136] Iteration 70300, lr = 0.00560625, m = 0.9
I0815 13:29:21.226047  8764 solver.cpp:312] Iteration 70400 (5.70025 iter/s, 17.5431s/100 iter), loss = 1.32081
I0815 13:29:21.226124  8764 solver.cpp:334]     Train net output #0: loss = 1.13253 (* 1 = 1.13253 loss)
I0815 13:29:21.226136  8764 sgd_solver.cpp:136] Iteration 70400, lr = 0.0056, m = 0.9
I0815 13:29:38.273231  8764 solver.cpp:312] Iteration 70500 (5.86623 iter/s, 17.0467s/100 iter), loss = 1.186
I0815 13:29:38.273260  8764 solver.cpp:334]     Train net output #0: loss = 1.30868 (* 1 = 1.30868 loss)
I0815 13:29:38.273267  8764 sgd_solver.cpp:136] Iteration 70500, lr = 0.00559375, m = 0.9
I0815 13:29:56.238643  8764 solver.cpp:312] Iteration 70600 (5.5664 iter/s, 17.9649s/100 iter), loss = 1.66847
I0815 13:29:56.238699  8764 solver.cpp:334]     Train net output #0: loss = 1.51678 (* 1 = 1.51678 loss)
I0815 13:29:56.238706  8764 sgd_solver.cpp:136] Iteration 70600, lr = 0.0055875, m = 0.9
I0815 13:30:12.549562  8764 solver.cpp:312] Iteration 70700 (6.13103 iter/s, 16.3105s/100 iter), loss = 1.59683
I0815 13:30:12.549592  8764 solver.cpp:334]     Train net output #0: loss = 1.52847 (* 1 = 1.52847 loss)
I0815 13:30:12.549598  8764 sgd_solver.cpp:136] Iteration 70700, lr = 0.00558125, m = 0.9
I0815 13:30:31.078505  8764 solver.cpp:312] Iteration 70800 (5.39711 iter/s, 18.5284s/100 iter), loss = 1.34794
I0815 13:30:31.078598  8764 solver.cpp:334]     Train net output #0: loss = 1.03813 (* 1 = 1.03813 loss)
I0815 13:30:31.078626  8764 sgd_solver.cpp:136] Iteration 70800, lr = 0.005575, m = 0.9
I0815 13:30:48.912989  8764 solver.cpp:312] Iteration 70900 (5.60727 iter/s, 17.834s/100 iter), loss = 1.06901
I0815 13:30:48.913013  8764 solver.cpp:334]     Train net output #0: loss = 0.934914 (* 1 = 0.934914 loss)
I0815 13:30:48.913019  8764 sgd_solver.cpp:136] Iteration 70900, lr = 0.00556875, m = 0.9
I0815 13:31:04.770673  8764 solver.cpp:363] Sparsity after update:
I0815 13:31:04.788504  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:31:04.788539  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:31:04.788558  8764 net.cpp:2192] conv1b_param_0(0.499) 
I0815 13:31:04.788568  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:31:04.788575  8764 net.cpp:2192] res2a_branch2a_param_0(0.507) 
I0815 13:31:04.788583  8764 net.cpp:2192] res2a_branch2b_param_0(0.506) 
I0815 13:31:04.788589  8764 net.cpp:2192] res3a_branch2a_param_0(0.509) 
I0815 13:31:04.788596  8764 net.cpp:2192] res3a_branch2b_param_0(0.507) 
I0815 13:31:04.788602  8764 net.cpp:2192] res4a_branch2a_param_0(0.51) 
I0815 13:31:04.788609  8764 net.cpp:2192] res4a_branch2b_param_0(0.509) 
I0815 13:31:04.788616  8764 net.cpp:2192] res5a_branch2a_param_0(0.51) 
I0815 13:31:04.788624  8764 net.cpp:2192] res5a_branch2b_param_0(0.51) 
I0815 13:31:04.788630  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.19937e+06/2.86678e+06) 0.418
I0815 13:31:04.963099  8794 solver.cpp:409] Finding and applying sparsity: 0.52
I0815 13:31:36.401479  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:31:36.403611  8764 solver.cpp:312] Iteration 71000 (2.10574 iter/s, 47.4893s/100 iter), loss = 1.11723
I0815 13:31:36.403635  8764 solver.cpp:334]     Train net output #0: loss = 1.19103 (* 1 = 1.19103 loss)
I0815 13:31:36.403641  8764 sgd_solver.cpp:136] Iteration 71000, lr = 0.0055625, m = 0.9
I0815 13:31:52.622756  8764 solver.cpp:312] Iteration 71100 (6.16573 iter/s, 16.2187s/100 iter), loss = 1.00766
I0815 13:31:52.622781  8764 solver.cpp:334]     Train net output #0: loss = 1.16084 (* 1 = 1.16084 loss)
I0815 13:31:52.622786  8764 sgd_solver.cpp:136] Iteration 71100, lr = 0.00555625, m = 0.9
I0815 13:32:10.270056  8764 solver.cpp:312] Iteration 71200 (5.66675 iter/s, 17.6468s/100 iter), loss = 1.27051
I0815 13:32:10.270120  8764 solver.cpp:334]     Train net output #0: loss = 1.14007 (* 1 = 1.14007 loss)
I0815 13:32:10.270128  8764 sgd_solver.cpp:136] Iteration 71200, lr = 0.00555, m = 0.9
I0815 13:32:28.771471  8764 solver.cpp:312] Iteration 71300 (5.40514 iter/s, 18.5009s/100 iter), loss = 1.4299
I0815 13:32:28.771495  8764 solver.cpp:334]     Train net output #0: loss = 1.30661 (* 1 = 1.30661 loss)
I0815 13:32:28.771499  8764 sgd_solver.cpp:136] Iteration 71300, lr = 0.00554375, m = 0.9
I0815 13:32:44.302573  8764 solver.cpp:312] Iteration 71400 (6.43887 iter/s, 15.5307s/100 iter), loss = 1.26389
I0815 13:32:44.302633  8764 solver.cpp:334]     Train net output #0: loss = 1.72833 (* 1 = 1.72833 loss)
I0815 13:32:44.302639  8764 sgd_solver.cpp:136] Iteration 71400, lr = 0.0055375, m = 0.9
I0815 13:33:00.330325  8764 solver.cpp:312] Iteration 71500 (6.23935 iter/s, 16.0273s/100 iter), loss = 1.08291
I0815 13:33:00.330395  8764 solver.cpp:334]     Train net output #0: loss = 1.22494 (* 1 = 1.22494 loss)
I0815 13:33:00.330413  8764 sgd_solver.cpp:136] Iteration 71500, lr = 0.00553125, m = 0.9
I0815 13:33:16.644997  8764 solver.cpp:312] Iteration 71600 (6.12962 iter/s, 16.3142s/100 iter), loss = 1.13526
I0815 13:33:16.648159  8764 solver.cpp:334]     Train net output #0: loss = 0.955377 (* 1 = 0.955377 loss)
I0815 13:33:16.648167  8764 sgd_solver.cpp:136] Iteration 71600, lr = 0.005525, m = 0.9
I0815 13:33:33.573279  8764 solver.cpp:312] Iteration 71700 (5.90744 iter/s, 16.9278s/100 iter), loss = 1.49086
I0815 13:33:33.573314  8764 solver.cpp:334]     Train net output #0: loss = 1.47299 (* 1 = 1.47299 loss)
I0815 13:33:33.573320  8764 sgd_solver.cpp:136] Iteration 71700, lr = 0.00551875, m = 0.9
I0815 13:33:52.425997  8764 solver.cpp:312] Iteration 71800 (5.30442 iter/s, 18.8522s/100 iter), loss = 1.61794
I0815 13:33:52.426082  8764 solver.cpp:334]     Train net output #0: loss = 1.56014 (* 1 = 1.56014 loss)
I0815 13:33:52.426100  8764 sgd_solver.cpp:136] Iteration 71800, lr = 0.0055125, m = 0.9
I0815 13:34:07.371035  8764 solver.cpp:312] Iteration 71900 (6.69137 iter/s, 14.9446s/100 iter), loss = 1.47572
I0815 13:34:07.371062  8764 solver.cpp:334]     Train net output #0: loss = 1.3653 (* 1 = 1.3653 loss)
I0815 13:34:07.371068  8764 sgd_solver.cpp:136] Iteration 71900, lr = 0.00550625, m = 0.9
I0815 13:34:25.176157  8764 solver.cpp:363] Sparsity after update:
I0815 13:34:25.194059  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:34:25.194228  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:34:25.194325  8764 net.cpp:2192] conv1b_param_0(0.513) 
I0815 13:34:25.194411  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:34:25.194499  8764 net.cpp:2192] res2a_branch2a_param_0(0.517) 
I0815 13:34:25.194586  8764 net.cpp:2192] res2a_branch2b_param_0(0.513) 
I0815 13:34:25.194672  8764 net.cpp:2192] res3a_branch2a_param_0(0.519) 
I0815 13:34:25.194759  8764 net.cpp:2192] res3a_branch2b_param_0(0.517) 
I0815 13:34:25.194845  8764 net.cpp:2192] res4a_branch2a_param_0(0.52) 
I0815 13:34:25.194932  8764 net.cpp:2192] res4a_branch2b_param_0(0.519) 
I0815 13:34:25.195020  8764 net.cpp:2192] res5a_branch2a_param_0(0.52) 
I0815 13:34:25.195106  8764 net.cpp:2192] res5a_branch2b_param_0(0.52) 
I0815 13:34:25.195194  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.22334e+06/2.86678e+06) 0.427
I0815 13:34:25.195307  8764 solver.cpp:509] Iteration 72000, Testing net (#0)
I0815 13:34:58.880463  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.577647
I0815 13:34:58.880555  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807705
I0815 13:34:58.880564  8764 solver.cpp:594]     Test net output #2: loss = 1.868 (* 1 = 1.868 loss)
I0815 13:34:58.880584  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.6844s
I0815 13:34:59.074308  8794 solver.cpp:409] Finding and applying sparsity: 0.53
I0815 13:35:43.485175  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:35:43.487687  8764 solver.cpp:312] Iteration 72000 (1.04043 iter/s, 96.114s/100 iter), loss = 1.31492
I0815 13:35:43.487726  8764 solver.cpp:334]     Train net output #0: loss = 1.27071 (* 1 = 1.27071 loss)
I0815 13:35:43.487740  8764 sgd_solver.cpp:136] Iteration 72000, lr = 0.0055, m = 0.9
I0815 13:36:02.486325  8764 solver.cpp:312] Iteration 72100 (5.26368 iter/s, 18.9981s/100 iter), loss = 1.45444
I0815 13:36:02.486377  8764 solver.cpp:334]     Train net output #0: loss = 1.23593 (* 1 = 1.23593 loss)
I0815 13:36:02.486480  8764 sgd_solver.cpp:136] Iteration 72100, lr = 0.00549375, m = 0.9
I0815 13:36:18.491150  8764 solver.cpp:312] Iteration 72200 (6.24829 iter/s, 16.0044s/100 iter), loss = 1.26378
I0815 13:36:18.491243  8764 solver.cpp:334]     Train net output #0: loss = 1.4246 (* 1 = 1.4246 loss)
I0815 13:36:18.491261  8764 sgd_solver.cpp:136] Iteration 72200, lr = 0.0054875, m = 0.9
I0815 13:36:32.940659  8764 solver.cpp:312] Iteration 72300 (6.92084 iter/s, 14.4491s/100 iter), loss = 1.3406
I0815 13:36:32.940682  8764 solver.cpp:334]     Train net output #0: loss = 1.26069 (* 1 = 1.26069 loss)
I0815 13:36:32.940688  8764 sgd_solver.cpp:136] Iteration 72300, lr = 0.00548125, m = 0.9
I0815 13:36:49.161640  8764 solver.cpp:312] Iteration 72400 (6.16502 iter/s, 16.2205s/100 iter), loss = 1.47459
I0815 13:36:49.161700  8764 solver.cpp:334]     Train net output #0: loss = 1.54757 (* 1 = 1.54757 loss)
I0815 13:36:49.161707  8764 sgd_solver.cpp:136] Iteration 72400, lr = 0.005475, m = 0.9
I0815 13:37:08.864179  8764 solver.cpp:312] Iteration 72500 (5.07563 iter/s, 19.702s/100 iter), loss = 1.20936
I0815 13:37:08.864260  8764 solver.cpp:334]     Train net output #0: loss = 1.39287 (* 1 = 1.39287 loss)
I0815 13:37:08.864284  8764 sgd_solver.cpp:136] Iteration 72500, lr = 0.00546875, m = 0.9
I0815 13:37:28.907801  8764 solver.cpp:312] Iteration 72600 (4.98926 iter/s, 20.0431s/100 iter), loss = 1.16458
I0815 13:37:28.907871  8764 solver.cpp:334]     Train net output #0: loss = 1.34948 (* 1 = 1.34948 loss)
I0815 13:37:28.907878  8764 sgd_solver.cpp:136] Iteration 72600, lr = 0.0054625, m = 0.9
I0815 13:37:47.116658  8764 solver.cpp:312] Iteration 72700 (5.492 iter/s, 18.2083s/100 iter), loss = 1.38161
I0815 13:37:47.116791  8764 solver.cpp:334]     Train net output #0: loss = 1.34025 (* 1 = 1.34025 loss)
I0815 13:37:47.116824  8764 sgd_solver.cpp:136] Iteration 72700, lr = 0.00545625, m = 0.9
I0815 13:38:07.187736  8764 solver.cpp:312] Iteration 72800 (4.98243 iter/s, 20.0705s/100 iter), loss = 1.38824
I0815 13:38:07.187865  8764 solver.cpp:334]     Train net output #0: loss = 1.62624 (* 1 = 1.62624 loss)
I0815 13:38:07.187891  8764 sgd_solver.cpp:136] Iteration 72800, lr = 0.00545, m = 0.9
I0815 13:38:14.876085  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:38:30.647259  8764 solver.cpp:312] Iteration 72900 (4.26278 iter/s, 23.4589s/100 iter), loss = 1.38717
I0815 13:38:30.647286  8764 solver.cpp:334]     Train net output #0: loss = 1.28371 (* 1 = 1.28371 loss)
I0815 13:38:30.647292  8764 sgd_solver.cpp:136] Iteration 72900, lr = 0.00544375, m = 0.9
I0815 13:38:47.435343  8764 solver.cpp:363] Sparsity after update:
I0815 13:38:47.448081  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:38:47.448141  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:38:47.448160  8764 net.cpp:2192] conv1b_param_0(0.526) 
I0815 13:38:47.448173  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:38:47.448184  8764 net.cpp:2192] res2a_branch2a_param_0(0.528) 
I0815 13:38:47.448195  8764 net.cpp:2192] res2a_branch2b_param_0(0.526) 
I0815 13:38:47.448226  8764 net.cpp:2192] res3a_branch2a_param_0(0.53) 
I0815 13:38:47.448242  8764 net.cpp:2192] res3a_branch2b_param_0(0.528) 
I0815 13:38:47.448256  8764 net.cpp:2192] res4a_branch2a_param_0(0.53) 
I0815 13:38:47.448269  8764 net.cpp:2192] res4a_branch2b_param_0(0.53) 
I0815 13:38:47.448283  8764 net.cpp:2192] res5a_branch2a_param_0(0.53) 
I0815 13:38:47.448297  8764 net.cpp:2192] res5a_branch2b_param_0(0.53) 
I0815 13:38:47.448310  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.24659e+06/2.86678e+06) 0.435
I0815 13:38:47.605795  8794 solver.cpp:409] Finding and applying sparsity: 0.54
I0815 13:39:52.968119  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:39:52.970613  8764 solver.cpp:312] Iteration 73000 (1.21476 iter/s, 82.3211s/100 iter), loss = 1.40478
I0815 13:39:52.970661  8764 solver.cpp:334]     Train net output #0: loss = 1.60809 (* 1 = 1.60809 loss)
I0815 13:39:52.970679  8764 sgd_solver.cpp:136] Iteration 73000, lr = 0.0054375, m = 0.9
I0815 13:40:11.232157  8764 solver.cpp:312] Iteration 73100 (5.47614 iter/s, 18.261s/100 iter), loss = 1.46007
I0815 13:40:11.232183  8764 solver.cpp:334]     Train net output #0: loss = 1.29588 (* 1 = 1.29588 loss)
I0815 13:40:11.232189  8764 sgd_solver.cpp:136] Iteration 73100, lr = 0.00543125, m = 0.9
I0815 13:40:32.977454  8764 solver.cpp:312] Iteration 73200 (4.59882 iter/s, 21.7447s/100 iter), loss = 1.3195
I0815 13:40:32.977519  8764 solver.cpp:334]     Train net output #0: loss = 1.48863 (* 1 = 1.48863 loss)
I0815 13:40:32.977525  8764 sgd_solver.cpp:136] Iteration 73200, lr = 0.005425, m = 0.9
I0815 13:40:53.051101  8764 solver.cpp:312] Iteration 73300 (4.98181 iter/s, 20.073s/100 iter), loss = 1.25838
I0815 13:40:53.051271  8764 solver.cpp:334]     Train net output #0: loss = 1.2955 (* 1 = 1.2955 loss)
I0815 13:40:53.051360  8764 sgd_solver.cpp:136] Iteration 73300, lr = 0.00541875, m = 0.9
I0815 13:41:11.478224  8764 solver.cpp:312] Iteration 73400 (5.42693 iter/s, 18.4266s/100 iter), loss = 1.17954
I0815 13:41:11.478469  8764 solver.cpp:334]     Train net output #0: loss = 0.989542 (* 1 = 0.989542 loss)
I0815 13:41:11.478582  8764 sgd_solver.cpp:136] Iteration 73400, lr = 0.0054125, m = 0.9
I0815 13:41:29.673141  8764 solver.cpp:312] Iteration 73500 (5.49619 iter/s, 18.1944s/100 iter), loss = 1.27165
I0815 13:41:29.673238  8764 solver.cpp:334]     Train net output #0: loss = 0.93066 (* 1 = 0.93066 loss)
I0815 13:41:29.673269  8764 sgd_solver.cpp:136] Iteration 73500, lr = 0.00540625, m = 0.9
I0815 13:41:56.620193  8764 solver.cpp:312] Iteration 73600 (3.71109 iter/s, 26.9463s/100 iter), loss = 1.13744
I0815 13:41:56.620327  8764 solver.cpp:334]     Train net output #0: loss = 0.891401 (* 1 = 0.891401 loss)
I0815 13:41:56.620368  8764 sgd_solver.cpp:136] Iteration 73600, lr = 0.0054, m = 0.9
I0815 13:42:17.149361  8764 solver.cpp:312] Iteration 73700 (4.87125 iter/s, 20.5286s/100 iter), loss = 1.46148
I0815 13:42:17.149574  8764 solver.cpp:334]     Train net output #0: loss = 1.8041 (* 1 = 1.8041 loss)
I0815 13:42:17.149657  8764 sgd_solver.cpp:136] Iteration 73700, lr = 0.00539375, m = 0.9
I0815 13:42:34.899082  8764 solver.cpp:312] Iteration 73800 (5.63405 iter/s, 17.7492s/100 iter), loss = 1.46362
I0815 13:42:34.899204  8764 solver.cpp:334]     Train net output #0: loss = 1.72779 (* 1 = 1.72779 loss)
I0815 13:42:34.899222  8764 sgd_solver.cpp:136] Iteration 73800, lr = 0.0053875, m = 0.9
I0815 13:42:56.789317  8764 solver.cpp:312] Iteration 73900 (4.56837 iter/s, 21.8896s/100 iter), loss = 1.47106
I0815 13:42:56.789347  8764 solver.cpp:334]     Train net output #0: loss = 1.50359 (* 1 = 1.50359 loss)
I0815 13:42:56.789353  8764 sgd_solver.cpp:136] Iteration 73900, lr = 0.00538125, m = 0.9
I0815 13:43:15.124948  8764 solver.cpp:363] Sparsity after update:
I0815 13:43:15.132913  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:43:15.132927  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:43:15.132936  8764 net.cpp:2192] conv1b_param_0(0.526) 
I0815 13:43:15.132939  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:43:15.132942  8764 net.cpp:2192] res2a_branch2a_param_0(0.538) 
I0815 13:43:15.132946  8764 net.cpp:2192] res2a_branch2b_param_0(0.533) 
I0815 13:43:15.132949  8764 net.cpp:2192] res3a_branch2a_param_0(0.54) 
I0815 13:43:15.132952  8764 net.cpp:2192] res3a_branch2b_param_0(0.538) 
I0815 13:43:15.132956  8764 net.cpp:2192] res4a_branch2a_param_0(0.54) 
I0815 13:43:15.132958  8764 net.cpp:2192] res4a_branch2b_param_0(0.54) 
I0815 13:43:15.132961  8764 net.cpp:2192] res5a_branch2a_param_0(0.54) 
I0815 13:43:15.132964  8764 net.cpp:2192] res5a_branch2b_param_0(0.54) 
I0815 13:43:15.132967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.27056e+06/2.86678e+06) 0.443
I0815 13:43:15.132977  8764 solver.cpp:509] Iteration 74000, Testing net (#0)
I0815 13:43:23.207036  8747 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 13:43:48.081229  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583176
I0815 13:43:48.081272  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815409
I0815 13:43:48.081277  8764 solver.cpp:594]     Test net output #2: loss = 1.79888 (* 1 = 1.79888 loss)
I0815 13:43:48.081298  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.9474s
I0815 13:43:48.249557  8794 solver.cpp:409] Finding and applying sparsity: 0.55
I0815 13:44:45.648016  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:44:45.650044  8764 solver.cpp:312] Iteration 74000 (0.91863 iter/s, 108.858s/100 iter), loss = 1.1336
I0815 13:44:45.650068  8764 solver.cpp:334]     Train net output #0: loss = 1.01767 (* 1 = 1.01767 loss)
I0815 13:44:45.650141  8764 sgd_solver.cpp:136] Iteration 74000, lr = 0.005375, m = 0.9
I0815 13:45:05.060369  8764 solver.cpp:312] Iteration 74100 (5.15204 iter/s, 19.4098s/100 iter), loss = 1.44655
I0815 13:45:05.060433  8764 solver.cpp:334]     Train net output #0: loss = 1.4243 (* 1 = 1.4243 loss)
I0815 13:45:05.060451  8764 sgd_solver.cpp:136] Iteration 74100, lr = 0.00536875, m = 0.9
I0815 13:45:27.321580  8764 solver.cpp:312] Iteration 74200 (4.49224 iter/s, 22.2606s/100 iter), loss = 1.65701
I0815 13:45:27.321665  8764 solver.cpp:334]     Train net output #0: loss = 2.02465 (* 1 = 2.02465 loss)
I0815 13:45:27.321679  8764 sgd_solver.cpp:136] Iteration 74200, lr = 0.0053625, m = 0.9
I0815 13:45:44.048034  8764 solver.cpp:312] Iteration 74300 (5.97872 iter/s, 16.726s/100 iter), loss = 1.21866
I0815 13:45:44.048096  8764 solver.cpp:334]     Train net output #0: loss = 1.51586 (* 1 = 1.51586 loss)
I0815 13:45:44.048113  8764 sgd_solver.cpp:136] Iteration 74300, lr = 0.00535625, m = 0.9
I0815 13:46:02.544136  8764 solver.cpp:312] Iteration 74400 (5.4067 iter/s, 18.4956s/100 iter), loss = 1.03548
I0815 13:46:02.544245  8764 solver.cpp:334]     Train net output #0: loss = 0.978426 (* 1 = 0.978426 loss)
I0815 13:46:02.544268  8764 sgd_solver.cpp:136] Iteration 74400, lr = 0.00535, m = 0.9
I0815 13:46:23.794894  8764 solver.cpp:312] Iteration 74500 (4.70584 iter/s, 21.2502s/100 iter), loss = 1.66779
I0815 13:46:23.794982  8764 solver.cpp:334]     Train net output #0: loss = 1.81796 (* 1 = 1.81796 loss)
I0815 13:46:23.795003  8764 sgd_solver.cpp:136] Iteration 74500, lr = 0.00534375, m = 0.9
I0815 13:46:41.537704  8764 solver.cpp:312] Iteration 74600 (5.63624 iter/s, 17.7423s/100 iter), loss = 1.33991
I0815 13:46:41.537811  8764 solver.cpp:334]     Train net output #0: loss = 1.07503 (* 1 = 1.07503 loss)
I0815 13:46:41.537829  8764 sgd_solver.cpp:136] Iteration 74600, lr = 0.0053375, m = 0.9
I0815 13:46:57.841248  8764 solver.cpp:312] Iteration 74700 (6.13381 iter/s, 16.3031s/100 iter), loss = 1.30291
I0815 13:46:57.841295  8764 solver.cpp:334]     Train net output #0: loss = 1.30166 (* 1 = 1.30166 loss)
I0815 13:46:57.841305  8764 sgd_solver.cpp:136] Iteration 74700, lr = 0.00533125, m = 0.9
I0815 13:47:14.239966  8764 solver.cpp:312] Iteration 74800 (6.0982 iter/s, 16.3983s/100 iter), loss = 1.66095
I0815 13:47:14.240020  8764 solver.cpp:334]     Train net output #0: loss = 1.90864 (* 1 = 1.90864 loss)
I0815 13:47:14.240025  8764 sgd_solver.cpp:136] Iteration 74800, lr = 0.005325, m = 0.9
I0815 13:47:32.650331  8764 solver.cpp:312] Iteration 74900 (5.43188 iter/s, 18.4098s/100 iter), loss = 1.50802
I0815 13:47:32.650413  8764 solver.cpp:334]     Train net output #0: loss = 1.94347 (* 1 = 1.94347 loss)
I0815 13:47:32.650430  8764 sgd_solver.cpp:136] Iteration 74900, lr = 0.00531875, m = 0.9
I0815 13:47:52.884798  8764 solver.cpp:363] Sparsity after update:
I0815 13:47:52.897260  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:47:52.897285  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:47:52.897310  8764 net.cpp:2192] conv1b_param_0(0.539) 
I0815 13:47:52.897323  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:47:52.897334  8764 net.cpp:2192] res2a_branch2a_param_0(0.549) 
I0815 13:47:52.897346  8764 net.cpp:2192] res2a_branch2b_param_0(0.546) 
I0815 13:47:52.897356  8764 net.cpp:2192] res3a_branch2a_param_0(0.549) 
I0815 13:47:52.897367  8764 net.cpp:2192] res3a_branch2b_param_0(0.549) 
I0815 13:47:52.897377  8764 net.cpp:2192] res4a_branch2a_param_0(0.549) 
I0815 13:47:52.897388  8764 net.cpp:2192] res4a_branch2b_param_0(0.549) 
I0815 13:47:52.897398  8764 net.cpp:2192] res5a_branch2a_param_0(0.55) 
I0815 13:47:52.897409  8764 net.cpp:2192] res5a_branch2b_param_0(0.549) 
I0815 13:47:52.897419  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.29342e+06/2.86678e+06) 0.451
I0815 13:47:53.084859  8794 solver.cpp:409] Finding and applying sparsity: 0.56
I0815 13:48:41.573949  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:48:41.576445  8764 solver.cpp:312] Iteration 75000 (1.45087 iter/s, 68.9242s/100 iter), loss = 1.00258
I0815 13:48:41.576495  8764 solver.cpp:334]     Train net output #0: loss = 1.11989 (* 1 = 1.11989 loss)
I0815 13:48:41.576512  8764 sgd_solver.cpp:136] Iteration 75000, lr = 0.0053125, m = 0.9
I0815 13:48:58.256510  8764 solver.cpp:312] Iteration 75100 (5.99535 iter/s, 16.6796s/100 iter), loss = 0.913202
I0815 13:48:58.256561  8764 solver.cpp:334]     Train net output #0: loss = 1.00748 (* 1 = 1.00748 loss)
I0815 13:48:58.256573  8764 sgd_solver.cpp:136] Iteration 75100, lr = 0.00530625, m = 0.9
I0815 13:49:14.516646  8764 solver.cpp:312] Iteration 75200 (6.15018 iter/s, 16.2597s/100 iter), loss = 1.26299
I0815 13:49:14.516728  8764 solver.cpp:334]     Train net output #0: loss = 1.2991 (* 1 = 1.2991 loss)
I0815 13:49:14.516741  8764 sgd_solver.cpp:136] Iteration 75200, lr = 0.0053, m = 0.9
I0815 13:49:31.712338  8764 solver.cpp:312] Iteration 75300 (5.81557 iter/s, 17.1952s/100 iter), loss = 1.50106
I0815 13:49:31.712365  8764 solver.cpp:334]     Train net output #0: loss = 1.93389 (* 1 = 1.93389 loss)
I0815 13:49:31.712370  8764 sgd_solver.cpp:136] Iteration 75300, lr = 0.00529375, m = 0.9
I0815 13:49:52.210034  8764 solver.cpp:312] Iteration 75400 (4.87873 iter/s, 20.4971s/100 iter), loss = 1.70027
I0815 13:49:52.210175  8764 solver.cpp:334]     Train net output #0: loss = 1.67273 (* 1 = 1.67273 loss)
I0815 13:49:52.210183  8764 sgd_solver.cpp:136] Iteration 75400, lr = 0.0052875, m = 0.9
I0815 13:50:10.703825  8764 solver.cpp:312] Iteration 75500 (5.40737 iter/s, 18.4933s/100 iter), loss = 1.38705
I0815 13:50:10.712151  8764 solver.cpp:334]     Train net output #0: loss = 1.46105 (* 1 = 1.46105 loss)
I0815 13:50:10.712185  8764 sgd_solver.cpp:136] Iteration 75500, lr = 0.00528125, m = 0.9
I0815 13:50:27.676693  8764 solver.cpp:312] Iteration 75600 (5.89192 iter/s, 16.9724s/100 iter), loss = 1.32964
I0815 13:50:27.676785  8764 solver.cpp:334]     Train net output #0: loss = 1.50343 (* 1 = 1.50343 loss)
I0815 13:50:27.676802  8764 sgd_solver.cpp:136] Iteration 75600, lr = 0.005275, m = 0.9
I0815 13:50:43.767388  8764 solver.cpp:312] Iteration 75700 (6.21494 iter/s, 16.0903s/100 iter), loss = 1.30666
I0815 13:50:43.767417  8764 solver.cpp:334]     Train net output #0: loss = 1.48023 (* 1 = 1.48023 loss)
I0815 13:50:43.767423  8764 sgd_solver.cpp:136] Iteration 75700, lr = 0.00526875, m = 0.9
I0815 13:51:01.914274  8764 solver.cpp:312] Iteration 75800 (5.51074 iter/s, 18.1464s/100 iter), loss = 1.19163
I0815 13:51:01.916179  8764 solver.cpp:334]     Train net output #0: loss = 1.11732 (* 1 = 1.11732 loss)
I0815 13:51:01.916216  8764 sgd_solver.cpp:136] Iteration 75800, lr = 0.0052625, m = 0.9
I0815 13:51:21.665263  8764 solver.cpp:312] Iteration 75900 (5.06317 iter/s, 19.7505s/100 iter), loss = 1.14395
I0815 13:51:21.665290  8764 solver.cpp:334]     Train net output #0: loss = 1.02167 (* 1 = 1.02167 loss)
I0815 13:51:21.665297  8764 sgd_solver.cpp:136] Iteration 75900, lr = 0.00525625, m = 0.9
I0815 13:51:38.824400  8764 solver.cpp:363] Sparsity after update:
I0815 13:51:38.830765  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:51:38.830788  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:51:38.830803  8764 net.cpp:2192] conv1b_param_0(0.552) 
I0815 13:51:38.830812  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:51:38.830821  8764 net.cpp:2192] res2a_branch2a_param_0(0.559) 
I0815 13:51:38.830831  8764 net.cpp:2192] res2a_branch2b_param_0(0.553) 
I0815 13:51:38.830839  8764 net.cpp:2192] res3a_branch2a_param_0(0.559) 
I0815 13:51:38.830848  8764 net.cpp:2192] res3a_branch2b_param_0(0.559) 
I0815 13:51:38.830857  8764 net.cpp:2192] res4a_branch2a_param_0(0.56) 
I0815 13:51:38.830865  8764 net.cpp:2192] res4a_branch2b_param_0(0.559) 
I0815 13:51:38.830873  8764 net.cpp:2192] res5a_branch2a_param_0(0.56) 
I0815 13:51:38.830883  8764 net.cpp:2192] res5a_branch2b_param_0(0.56) 
I0815 13:51:38.830890  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.31739e+06/2.86678e+06) 0.46
I0815 13:51:38.830907  8764 solver.cpp:509] Iteration 76000, Testing net (#0)
I0815 13:52:01.261704  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:52:06.131361  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.580706
I0815 13:52:06.131381  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.808821
I0815 13:52:06.131386  8764 solver.cpp:594]     Test net output #2: loss = 1.85827 (* 1 = 1.85827 loss)
I0815 13:52:06.131409  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.2998s
I0815 13:52:06.304857  8794 solver.cpp:409] Finding and applying sparsity: 0.57
I0815 13:52:41.283316  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:52:41.285377  8764 solver.cpp:312] Iteration 76000 (1.256 iter/s, 79.618s/100 iter), loss = 1.43638
I0815 13:52:41.285398  8764 solver.cpp:334]     Train net output #0: loss = 1.43316 (* 1 = 1.43316 loss)
I0815 13:52:41.285405  8764 sgd_solver.cpp:136] Iteration 76000, lr = 0.00525, m = 0.9
I0815 13:52:59.660406  8764 solver.cpp:312] Iteration 76100 (5.44232 iter/s, 18.3745s/100 iter), loss = 1.19768
I0815 13:52:59.660447  8764 solver.cpp:334]     Train net output #0: loss = 1.0534 (* 1 = 1.0534 loss)
I0815 13:52:59.660455  8764 sgd_solver.cpp:136] Iteration 76100, lr = 0.00524375, m = 0.9
I0815 13:53:16.070694  8764 solver.cpp:312] Iteration 76200 (6.09391 iter/s, 16.4098s/100 iter), loss = 1.11379
I0815 13:53:16.070762  8764 solver.cpp:334]     Train net output #0: loss = 1.41549 (* 1 = 1.41549 loss)
I0815 13:53:16.070770  8764 sgd_solver.cpp:136] Iteration 76200, lr = 0.0052375, m = 0.9
I0815 13:53:35.267165  8764 solver.cpp:312] Iteration 76300 (5.20944 iter/s, 19.1959s/100 iter), loss = 1.24614
I0815 13:53:35.267266  8764 solver.cpp:334]     Train net output #0: loss = 1.33311 (* 1 = 1.33311 loss)
I0815 13:53:35.267293  8764 sgd_solver.cpp:136] Iteration 76300, lr = 0.00523125, m = 0.9
I0815 13:53:51.545001  8764 solver.cpp:312] Iteration 76400 (6.14349 iter/s, 16.2774s/100 iter), loss = 1.05348
I0815 13:53:51.545701  8764 solver.cpp:334]     Train net output #0: loss = 0.957156 (* 1 = 0.957156 loss)
I0815 13:53:51.545809  8764 sgd_solver.cpp:136] Iteration 76400, lr = 0.005225, m = 0.9
I0815 13:54:08.844421  8764 solver.cpp:312] Iteration 76500 (5.7807 iter/s, 17.2989s/100 iter), loss = 1.42427
I0815 13:54:08.844477  8764 solver.cpp:334]     Train net output #0: loss = 1.4424 (* 1 = 1.4424 loss)
I0815 13:54:08.844491  8764 sgd_solver.cpp:136] Iteration 76500, lr = 0.00521875, m = 0.9
I0815 13:54:25.813961  8764 solver.cpp:312] Iteration 76600 (5.89307 iter/s, 16.9691s/100 iter), loss = 1.16146
I0815 13:54:25.814167  8764 solver.cpp:334]     Train net output #0: loss = 1.12659 (* 1 = 1.12659 loss)
I0815 13:54:25.814252  8764 sgd_solver.cpp:136] Iteration 76600, lr = 0.0052125, m = 0.9
I0815 13:54:41.397620  8764 solver.cpp:312] Iteration 76700 (6.41715 iter/s, 15.5832s/100 iter), loss = 1.17507
I0815 13:54:41.397644  8764 solver.cpp:334]     Train net output #0: loss = 1.19324 (* 1 = 1.19324 loss)
I0815 13:54:41.397650  8764 sgd_solver.cpp:136] Iteration 76700, lr = 0.00520625, m = 0.9
I0815 13:54:55.938654  8764 solver.cpp:312] Iteration 76800 (6.87728 iter/s, 14.5406s/100 iter), loss = 1.22019
I0815 13:54:55.938714  8764 solver.cpp:334]     Train net output #0: loss = 1.19016 (* 1 = 1.19016 loss)
I0815 13:54:55.938719  8764 sgd_solver.cpp:136] Iteration 76800, lr = 0.0052, m = 0.9
I0815 13:55:12.773144  8764 solver.cpp:312] Iteration 76900 (5.94035 iter/s, 16.834s/100 iter), loss = 1.47172
I0815 13:55:12.773171  8764 solver.cpp:334]     Train net output #0: loss = 1.53934 (* 1 = 1.53934 loss)
I0815 13:55:12.773176  8764 sgd_solver.cpp:136] Iteration 76900, lr = 0.00519375, m = 0.9
I0815 13:55:28.463145  8764 solver.cpp:363] Sparsity after update:
I0815 13:55:28.473598  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:55:28.473610  8764 net.cpp:2192] conv1a_param_0(0.278) 
I0815 13:55:28.473620  8764 net.cpp:2192] conv1b_param_0(0.563) 
I0815 13:55:28.473631  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:55:28.473636  8764 net.cpp:2192] res2a_branch2a_param_0(0.569) 
I0815 13:55:28.473640  8764 net.cpp:2192] res2a_branch2b_param_0(0.564) 
I0815 13:55:28.473644  8764 net.cpp:2192] res3a_branch2a_param_0(0.569) 
I0815 13:55:28.473646  8764 net.cpp:2192] res3a_branch2b_param_0(0.569) 
I0815 13:55:28.473650  8764 net.cpp:2192] res4a_branch2a_param_0(0.569) 
I0815 13:55:28.473654  8764 net.cpp:2192] res4a_branch2b_param_0(0.569) 
I0815 13:55:28.473657  8764 net.cpp:2192] res5a_branch2a_param_0(0.57) 
I0815 13:55:28.473662  8764 net.cpp:2192] res5a_branch2b_param_0(0.569) 
I0815 13:55:28.473666  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.34065e+06/2.86678e+06) 0.468
I0815 13:55:28.632561  8794 solver.cpp:409] Finding and applying sparsity: 0.58
I0815 13:56:03.837258  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:56:03.839395  8764 solver.cpp:312] Iteration 77000 (1.95829 iter/s, 51.0648s/100 iter), loss = 1.31031
I0815 13:56:03.839413  8764 solver.cpp:334]     Train net output #0: loss = 1.42772 (* 1 = 1.42772 loss)
I0815 13:56:03.839423  8764 sgd_solver.cpp:136] Iteration 77000, lr = 0.0051875, m = 0.9
I0815 13:56:21.304471  8764 solver.cpp:312] Iteration 77100 (5.72588 iter/s, 17.4646s/100 iter), loss = 1.19945
I0815 13:56:21.304595  8764 solver.cpp:334]     Train net output #0: loss = 1.11596 (* 1 = 1.11596 loss)
I0815 13:56:21.304643  8764 sgd_solver.cpp:136] Iteration 77100, lr = 0.00518125, m = 0.9
I0815 13:56:38.923822  8764 solver.cpp:312] Iteration 77200 (5.67573 iter/s, 17.6189s/100 iter), loss = 1.25527
I0815 13:56:38.923893  8764 solver.cpp:334]     Train net output #0: loss = 1.03412 (* 1 = 1.03412 loss)
I0815 13:56:38.923900  8764 sgd_solver.cpp:136] Iteration 77200, lr = 0.005175, m = 0.9
I0815 13:56:56.083168  8764 solver.cpp:312] Iteration 77300 (5.82789 iter/s, 17.1589s/100 iter), loss = 1.40372
I0815 13:56:56.083341  8764 solver.cpp:334]     Train net output #0: loss = 1.16774 (* 1 = 1.16774 loss)
I0815 13:56:56.083428  8764 sgd_solver.cpp:136] Iteration 77300, lr = 0.00516875, m = 0.9
I0815 13:57:12.802129  8764 solver.cpp:312] Iteration 77400 (5.9814 iter/s, 16.7185s/100 iter), loss = 1.33528
I0815 13:57:12.802189  8764 solver.cpp:334]     Train net output #0: loss = 0.996885 (* 1 = 0.996885 loss)
I0815 13:57:12.802196  8764 sgd_solver.cpp:136] Iteration 77400, lr = 0.0051625, m = 0.9
I0815 13:57:29.360671  8764 solver.cpp:312] Iteration 77500 (6.03934 iter/s, 16.5581s/100 iter), loss = 1.65802
I0815 13:57:29.360694  8764 solver.cpp:334]     Train net output #0: loss = 1.73316 (* 1 = 1.73316 loss)
I0815 13:57:29.360699  8764 sgd_solver.cpp:136] Iteration 77500, lr = 0.00515625, m = 0.9
I0815 13:57:47.074259  8764 solver.cpp:312] Iteration 77600 (5.64554 iter/s, 17.7131s/100 iter), loss = 1.17913
I0815 13:57:47.074358  8764 solver.cpp:334]     Train net output #0: loss = 1.21175 (* 1 = 1.21175 loss)
I0815 13:57:47.074378  8764 sgd_solver.cpp:136] Iteration 77600, lr = 0.00515, m = 0.9
I0815 13:58:06.216951  8764 solver.cpp:312] Iteration 77700 (5.22407 iter/s, 19.1422s/100 iter), loss = 1.50315
I0815 13:58:06.216979  8764 solver.cpp:334]     Train net output #0: loss = 1.62973 (* 1 = 1.62973 loss)
I0815 13:58:06.216984  8764 sgd_solver.cpp:136] Iteration 77700, lr = 0.00514375, m = 0.9
I0815 13:58:23.965375  8764 solver.cpp:312] Iteration 77800 (5.63446 iter/s, 17.7479s/100 iter), loss = 1.09306
I0815 13:58:23.965610  8764 solver.cpp:334]     Train net output #0: loss = 1.28899 (* 1 = 1.28899 loss)
I0815 13:58:23.965623  8764 sgd_solver.cpp:136] Iteration 77800, lr = 0.0051375, m = 0.9
I0815 13:58:41.033462  8764 solver.cpp:312] Iteration 77900 (5.85905 iter/s, 17.0676s/100 iter), loss = 1.12937
I0815 13:58:41.033483  8764 solver.cpp:334]     Train net output #0: loss = 1.00155 (* 1 = 1.00155 loss)
I0815 13:58:41.033486  8764 sgd_solver.cpp:136] Iteration 77900, lr = 0.00513125, m = 0.9
I0815 13:58:59.328941  8764 solver.cpp:363] Sparsity after update:
I0815 13:58:59.332854  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:58:59.332866  8764 net.cpp:2192] conv1a_param_0(0.279) 
I0815 13:58:59.332875  8764 net.cpp:2192] conv1b_param_0(0.564) 
I0815 13:58:59.332878  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:58:59.332892  8764 net.cpp:2192] res2a_branch2a_param_0(0.579) 
I0815 13:58:59.332902  8764 net.cpp:2192] res2a_branch2b_param_0(0.571) 
I0815 13:58:59.332911  8764 net.cpp:2192] res3a_branch2a_param_0(0.58) 
I0815 13:58:59.332918  8764 net.cpp:2192] res3a_branch2b_param_0(0.58) 
I0815 13:58:59.332926  8764 net.cpp:2192] res4a_branch2a_param_0(0.58) 
I0815 13:58:59.332934  8764 net.cpp:2192] res4a_branch2b_param_0(0.58) 
I0815 13:58:59.332942  8764 net.cpp:2192] res5a_branch2a_param_0(0.58) 
I0815 13:58:59.332950  8764 net.cpp:2192] res5a_branch2b_param_0(0.58) 
I0815 13:58:59.332958  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.36458e+06/2.86678e+06) 0.476
I0815 13:58:59.332973  8764 solver.cpp:509] Iteration 78000, Testing net (#0)
I0815 13:59:29.671066  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583529
I0815 13:59:29.671135  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810881
I0815 13:59:29.671159  8764 solver.cpp:594]     Test net output #2: loss = 1.81208 (* 1 = 1.81208 loss)
I0815 13:59:29.671195  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.3374s
I0815 13:59:29.894726  8794 solver.cpp:409] Finding and applying sparsity: 0.59
I0815 14:00:08.257700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:00:08.259752  8764 solver.cpp:312] Iteration 78000 (1.14647 iter/s, 87.2239s/100 iter), loss = 1.21672
I0815 14:00:08.259768  8764 solver.cpp:334]     Train net output #0: loss = 1.19087 (* 1 = 1.19087 loss)
I0815 14:00:08.259774  8764 sgd_solver.cpp:136] Iteration 78000, lr = 0.005125, m = 0.9
I0815 14:00:24.420838  8764 solver.cpp:312] Iteration 78100 (6.18788 iter/s, 16.1606s/100 iter), loss = 1.32997
I0815 14:00:24.420866  8764 solver.cpp:334]     Train net output #0: loss = 1.43266 (* 1 = 1.43266 loss)
I0815 14:00:24.420872  8764 sgd_solver.cpp:136] Iteration 78100, lr = 0.00511875, m = 0.9
I0815 14:00:44.617434  8764 solver.cpp:312] Iteration 78200 (4.95147 iter/s, 20.196s/100 iter), loss = 1.42135
I0815 14:00:44.617532  8764 solver.cpp:334]     Train net output #0: loss = 1.52396 (* 1 = 1.52396 loss)
I0815 14:00:44.617550  8764 sgd_solver.cpp:136] Iteration 78200, lr = 0.0051125, m = 0.9
I0815 14:00:59.295253  8764 solver.cpp:312] Iteration 78300 (6.81319 iter/s, 14.6774s/100 iter), loss = 1.47947
I0815 14:00:59.295280  8764 solver.cpp:334]     Train net output #0: loss = 1.26338 (* 1 = 1.26338 loss)
I0815 14:00:59.295286  8764 sgd_solver.cpp:136] Iteration 78300, lr = 0.00510625, m = 0.9
I0815 14:01:17.996472  8764 solver.cpp:312] Iteration 78400 (5.34739 iter/s, 18.7007s/100 iter), loss = 1.52452
I0815 14:01:17.996527  8764 solver.cpp:334]     Train net output #0: loss = 1.53519 (* 1 = 1.53519 loss)
I0815 14:01:17.996533  8764 sgd_solver.cpp:136] Iteration 78400, lr = 0.0051, m = 0.9
I0815 14:01:32.643646  8764 solver.cpp:312] Iteration 78500 (6.82744 iter/s, 14.6468s/100 iter), loss = 1.0836
I0815 14:01:32.643715  8764 solver.cpp:334]     Train net output #0: loss = 1.12205 (* 1 = 1.12205 loss)
I0815 14:01:32.643735  8764 sgd_solver.cpp:136] Iteration 78500, lr = 0.00509375, m = 0.9
I0815 14:01:50.075871  8764 solver.cpp:312] Iteration 78600 (5.73666 iter/s, 17.4317s/100 iter), loss = 1.33488
I0815 14:01:50.075930  8764 solver.cpp:334]     Train net output #0: loss = 1.63349 (* 1 = 1.63349 loss)
I0815 14:01:50.075937  8764 sgd_solver.cpp:136] Iteration 78600, lr = 0.0050875, m = 0.9
I0815 14:02:08.149205  8764 solver.cpp:312] Iteration 78700 (5.53316 iter/s, 18.0728s/100 iter), loss = 1.46113
I0815 14:02:08.149233  8764 solver.cpp:334]     Train net output #0: loss = 1.38657 (* 1 = 1.38657 loss)
I0815 14:02:08.149237  8764 sgd_solver.cpp:136] Iteration 78700, lr = 0.00508125, m = 0.9
I0815 14:02:25.777597  8764 solver.cpp:312] Iteration 78800 (5.67282 iter/s, 17.6279s/100 iter), loss = 1.37533
I0815 14:02:25.780159  8764 solver.cpp:334]     Train net output #0: loss = 1.14315 (* 1 = 1.14315 loss)
I0815 14:02:25.780169  8764 sgd_solver.cpp:136] Iteration 78800, lr = 0.005075, m = 0.9
I0815 14:02:43.186267  8764 solver.cpp:312] Iteration 78900 (5.74442 iter/s, 17.4082s/100 iter), loss = 1.08301
I0815 14:02:43.186295  8764 solver.cpp:334]     Train net output #0: loss = 1.1251 (* 1 = 1.1251 loss)
I0815 14:02:43.186300  8764 sgd_solver.cpp:136] Iteration 78900, lr = 0.00506875, m = 0.9
I0815 14:02:57.866828  8764 solver.cpp:363] Sparsity after update:
I0815 14:02:57.877261  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:02:57.877298  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:02:57.877317  8764 net.cpp:2192] conv1b_param_0(0.575) 
I0815 14:02:57.877329  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:02:57.877341  8764 net.cpp:2192] res2a_branch2a_param_0(0.586) 
I0815 14:02:57.877353  8764 net.cpp:2192] res2a_branch2b_param_0(0.576) 
I0815 14:02:57.877365  8764 net.cpp:2192] res3a_branch2a_param_0(0.589) 
I0815 14:02:57.877377  8764 net.cpp:2192] res3a_branch2b_param_0(0.587) 
I0815 14:02:57.877388  8764 net.cpp:2192] res4a_branch2a_param_0(0.589) 
I0815 14:02:57.877400  8764 net.cpp:2192] res4a_branch2b_param_0(0.589) 
I0815 14:02:57.877418  8764 net.cpp:2192] res5a_branch2a_param_0(0.59) 
I0815 14:02:57.877429  8764 net.cpp:2192] res5a_branch2b_param_0(0.589) 
I0815 14:02:57.877440  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.38722e+06/2.86678e+06) 0.484
I0815 14:02:58.015166  8794 solver.cpp:409] Finding and applying sparsity: 0.6
I0815 14:03:34.802429  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:03:34.804463  8764 solver.cpp:312] Iteration 79000 (1.93735 iter/s, 51.6168s/100 iter), loss = 1.22992
I0815 14:03:34.804486  8764 solver.cpp:334]     Train net output #0: loss = 1.26349 (* 1 = 1.26349 loss)
I0815 14:03:34.804494  8764 sgd_solver.cpp:136] Iteration 79000, lr = 0.0050625, m = 0.9
I0815 14:03:51.758153  8764 solver.cpp:312] Iteration 79100 (5.89859 iter/s, 16.9532s/100 iter), loss = 1.29853
I0815 14:03:51.758178  8764 solver.cpp:334]     Train net output #0: loss = 1.04669 (* 1 = 1.04669 loss)
I0815 14:03:51.758183  8764 sgd_solver.cpp:136] Iteration 79100, lr = 0.00505625, m = 0.9
I0815 14:04:09.609027  8764 solver.cpp:312] Iteration 79200 (5.60212 iter/s, 17.8504s/100 iter), loss = 1.42978
I0815 14:04:09.609091  8764 solver.cpp:334]     Train net output #0: loss = 1.30263 (* 1 = 1.30263 loss)
I0815 14:04:09.609098  8764 sgd_solver.cpp:136] Iteration 79200, lr = 0.00505, m = 0.9
I0815 14:04:27.152170  8764 solver.cpp:312] Iteration 79300 (5.70039 iter/s, 17.5427s/100 iter), loss = 0.961555
I0815 14:04:27.152209  8764 solver.cpp:334]     Train net output #0: loss = 1.09247 (* 1 = 1.09247 loss)
I0815 14:04:27.152216  8764 sgd_solver.cpp:136] Iteration 79300, lr = 0.00504375, m = 0.9
I0815 14:04:44.642822  8764 solver.cpp:312] Iteration 79400 (5.7175 iter/s, 17.4902s/100 iter), loss = 1.43284
I0815 14:04:44.642922  8764 solver.cpp:334]     Train net output #0: loss = 1.51546 (* 1 = 1.51546 loss)
I0815 14:04:44.642936  8764 sgd_solver.cpp:136] Iteration 79400, lr = 0.0050375, m = 0.9
I0815 14:05:02.518226  8764 solver.cpp:312] Iteration 79500 (5.59443 iter/s, 17.8749s/100 iter), loss = 1.41846
I0815 14:05:02.518266  8764 solver.cpp:334]     Train net output #0: loss = 1.48044 (* 1 = 1.48044 loss)
I0815 14:05:02.518275  8764 sgd_solver.cpp:136] Iteration 79500, lr = 0.00503125, m = 0.9
I0815 14:05:23.172166  8764 solver.cpp:312] Iteration 79600 (4.84183 iter/s, 20.6534s/100 iter), loss = 0.87254
I0815 14:05:23.172266  8764 solver.cpp:334]     Train net output #0: loss = 0.940715 (* 1 = 0.940715 loss)
I0815 14:05:23.172282  8764 sgd_solver.cpp:136] Iteration 79600, lr = 0.005025, m = 0.9
I0815 14:05:40.602203  8764 solver.cpp:312] Iteration 79700 (5.73738 iter/s, 17.4296s/100 iter), loss = 1.41946
I0815 14:05:40.602233  8764 solver.cpp:334]     Train net output #0: loss = 1.68366 (* 1 = 1.68366 loss)
I0815 14:05:40.602241  8764 sgd_solver.cpp:136] Iteration 79700, lr = 0.00501875, m = 0.9
I0815 14:05:57.411633  8764 solver.cpp:312] Iteration 79800 (5.94921 iter/s, 16.809s/100 iter), loss = 1.32533
I0815 14:05:57.411728  8764 solver.cpp:334]     Train net output #0: loss = 1.01095 (* 1 = 1.01095 loss)
I0815 14:05:57.411742  8764 sgd_solver.cpp:136] Iteration 79800, lr = 0.0050125, m = 0.9
I0815 14:06:15.473908  8764 solver.cpp:312] Iteration 79900 (5.53655 iter/s, 18.0618s/100 iter), loss = 1.6585
I0815 14:06:15.473968  8764 solver.cpp:334]     Train net output #0: loss = 1.63224 (* 1 = 1.63224 loss)
I0815 14:06:15.473978  8764 sgd_solver.cpp:136] Iteration 79900, lr = 0.00500625, m = 0.9
I0815 14:06:32.495507  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_80000.caffemodel
I0815 14:06:32.525611  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_80000.solverstate
I0815 14:06:32.531903  8764 solver.cpp:363] Sparsity after update:
I0815 14:06:32.533181  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:06:32.533192  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:06:32.533200  8764 net.cpp:2192] conv1b_param_0(0.586) 
I0815 14:06:32.533205  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:06:32.533207  8764 net.cpp:2192] res2a_branch2a_param_0(0.597) 
I0815 14:06:32.533210  8764 net.cpp:2192] res2a_branch2b_param_0(0.588) 
I0815 14:06:32.533215  8764 net.cpp:2192] res3a_branch2a_param_0(0.599) 
I0815 14:06:32.533218  8764 net.cpp:2192] res3a_branch2b_param_0(0.597) 
I0815 14:06:32.533221  8764 net.cpp:2192] res4a_branch2a_param_0(0.6) 
I0815 14:06:32.533224  8764 net.cpp:2192] res4a_branch2b_param_0(0.599) 
I0815 14:06:32.533228  8764 net.cpp:2192] res5a_branch2a_param_0(0.6) 
I0815 14:06:32.533231  8764 net.cpp:2192] res5a_branch2b_param_0(0.6) 
I0815 14:06:32.533234  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.41122e+06/2.86678e+06) 0.492
I0815 14:06:32.533244  8764 solver.cpp:509] Iteration 80000, Testing net (#0)
I0815 14:07:00.605337  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:07:05.642781  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583941
I0815 14:07:05.642839  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814232
I0815 14:07:05.642846  8764 solver.cpp:594]     Test net output #2: loss = 1.82412 (* 1 = 1.82412 loss)
I0815 14:07:05.642864  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.1087s
I0815 14:07:05.793496  8794 solver.cpp:409] Finding and applying sparsity: 0.61
I0815 14:07:44.492436  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:07:44.494474  8764 solver.cpp:312] Iteration 80000 (1.12337 iter/s, 89.0181s/100 iter), loss = 1.22296
I0815 14:07:44.494496  8764 solver.cpp:334]     Train net output #0: loss = 1.18691 (* 1 = 1.18691 loss)
I0815 14:07:44.494504  8764 sgd_solver.cpp:136] Iteration 80000, lr = 0.005, m = 0.9
I0815 14:07:59.976236  8764 solver.cpp:312] Iteration 80100 (6.45939 iter/s, 15.4813s/100 iter), loss = 1.41535
I0815 14:07:59.976260  8764 solver.cpp:334]     Train net output #0: loss = 1.46901 (* 1 = 1.46901 loss)
I0815 14:07:59.976264  8764 sgd_solver.cpp:136] Iteration 80100, lr = 0.00499375, m = 0.9
I0815 14:08:16.844087  8764 solver.cpp:312] Iteration 80200 (5.9286 iter/s, 16.8674s/100 iter), loss = 1.30288
I0815 14:08:16.844185  8764 solver.cpp:334]     Train net output #0: loss = 1.45557 (* 1 = 1.45557 loss)
I0815 14:08:16.844203  8764 sgd_solver.cpp:136] Iteration 80200, lr = 0.0049875, m = 0.9
I0815 14:08:33.728485  8764 solver.cpp:312] Iteration 80300 (5.92279 iter/s, 16.8839s/100 iter), loss = 1.06484
I0815 14:08:33.728555  8764 solver.cpp:334]     Train net output #0: loss = 1.05947 (* 1 = 1.05947 loss)
I0815 14:08:33.728581  8764 sgd_solver.cpp:136] Iteration 80300, lr = 0.00498125, m = 0.9
I0815 14:08:51.143457  8764 solver.cpp:312] Iteration 80400 (5.74234 iter/s, 17.4145s/100 iter), loss = 1.23376
I0815 14:08:51.143512  8764 solver.cpp:334]     Train net output #0: loss = 1.28657 (* 1 = 1.28657 loss)
I0815 14:08:51.143517  8764 sgd_solver.cpp:136] Iteration 80400, lr = 0.004975, m = 0.9
I0815 14:09:07.059029  8764 solver.cpp:312] Iteration 80500 (6.28333 iter/s, 15.9151s/100 iter), loss = 1.43968
I0815 14:09:07.059053  8764 solver.cpp:334]     Train net output #0: loss = 1.13297 (* 1 = 1.13297 loss)
I0815 14:09:07.059059  8764 sgd_solver.cpp:136] Iteration 80500, lr = 0.00496875, m = 0.9
I0815 14:09:22.680541  8764 solver.cpp:312] Iteration 80600 (6.40161 iter/s, 15.6211s/100 iter), loss = 1.65056
I0815 14:09:22.680824  8764 solver.cpp:334]     Train net output #0: loss = 2.00595 (* 1 = 2.00595 loss)
I0815 14:09:22.680940  8764 sgd_solver.cpp:136] Iteration 80600, lr = 0.0049625, m = 0.9
I0815 14:09:41.309136  8764 solver.cpp:312] Iteration 80700 (5.36824 iter/s, 18.6281s/100 iter), loss = 1.35408
I0815 14:09:41.309214  8764 solver.cpp:334]     Train net output #0: loss = 1.40364 (* 1 = 1.40364 loss)
I0815 14:09:41.309237  8764 sgd_solver.cpp:136] Iteration 80700, lr = 0.00495625, m = 0.9
I0815 14:09:58.922379  8764 solver.cpp:312] Iteration 80800 (5.6777 iter/s, 17.6128s/100 iter), loss = 1.19097
I0815 14:09:58.930796  8764 solver.cpp:334]     Train net output #0: loss = 1.41116 (* 1 = 1.41116 loss)
I0815 14:09:58.930810  8764 sgd_solver.cpp:136] Iteration 80800, lr = 0.00495, m = 0.9
I0815 14:10:16.094542  8764 solver.cpp:312] Iteration 80900 (5.82354 iter/s, 17.1717s/100 iter), loss = 1.12402
I0815 14:10:16.094573  8764 solver.cpp:334]     Train net output #0: loss = 1.10179 (* 1 = 1.10179 loss)
I0815 14:10:16.094578  8764 sgd_solver.cpp:136] Iteration 80900, lr = 0.00494375, m = 0.9
I0815 14:10:34.417428  8764 solver.cpp:363] Sparsity after update:
I0815 14:10:34.433223  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:10:34.433262  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:10:34.433297  8764 net.cpp:2192] conv1b_param_0(0.587) 
I0815 14:10:34.433315  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:10:34.433331  8764 net.cpp:2192] res2a_branch2a_param_0(0.607) 
I0815 14:10:34.433341  8764 net.cpp:2192] res2a_branch2b_param_0(0.593) 
I0815 14:10:34.433349  8764 net.cpp:2192] res3a_branch2a_param_0(0.609) 
I0815 14:10:34.433362  8764 net.cpp:2192] res3a_branch2b_param_0(0.607) 
I0815 14:10:34.433375  8764 net.cpp:2192] res4a_branch2a_param_0(0.609) 
I0815 14:10:34.433393  8764 net.cpp:2192] res4a_branch2b_param_0(0.609) 
I0815 14:10:34.433410  8764 net.cpp:2192] res5a_branch2a_param_0(0.61) 
I0815 14:10:34.433421  8764 net.cpp:2192] res5a_branch2b_param_0(0.609) 
I0815 14:10:34.433436  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.43438e+06/2.86678e+06) 0.5
I0815 14:10:34.808419  8794 solver.cpp:409] Finding and applying sparsity: 0.62
I0815 14:11:16.458614  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:11:16.460680  8764 solver.cpp:312] Iteration 81000 (1.6566 iter/s, 60.3645s/100 iter), loss = 1.33992
I0815 14:11:16.460703  8764 solver.cpp:334]     Train net output #0: loss = 1.13687 (* 1 = 1.13687 loss)
I0815 14:11:16.460712  8764 sgd_solver.cpp:136] Iteration 81000, lr = 0.0049375, m = 0.9
I0815 14:11:36.149379  8764 solver.cpp:312] Iteration 81100 (5.0792 iter/s, 19.6881s/100 iter), loss = 1.40127
I0815 14:11:36.149405  8764 solver.cpp:334]     Train net output #0: loss = 1.35297 (* 1 = 1.35297 loss)
I0815 14:11:36.149440  8764 sgd_solver.cpp:136] Iteration 81100, lr = 0.00493125, m = 0.9
I0815 14:11:52.610958  8764 solver.cpp:312] Iteration 81200 (6.07492 iter/s, 16.4611s/100 iter), loss = 1.05522
I0815 14:11:52.611045  8764 solver.cpp:334]     Train net output #0: loss = 1.18069 (* 1 = 1.18069 loss)
I0815 14:11:52.611063  8764 sgd_solver.cpp:136] Iteration 81200, lr = 0.004925, m = 0.9
I0815 14:12:10.117558  8764 solver.cpp:312] Iteration 81300 (5.71229 iter/s, 17.5061s/100 iter), loss = 1.45076
I0815 14:12:10.117584  8764 solver.cpp:334]     Train net output #0: loss = 0.944007 (* 1 = 0.944007 loss)
I0815 14:12:10.117591  8764 sgd_solver.cpp:136] Iteration 81300, lr = 0.00491875, m = 0.9
I0815 14:12:29.918900  8764 solver.cpp:312] Iteration 81400 (5.0503 iter/s, 19.8008s/100 iter), loss = 1.34157
I0815 14:12:29.918977  8764 solver.cpp:334]     Train net output #0: loss = 1.32597 (* 1 = 1.32597 loss)
I0815 14:12:29.918989  8764 sgd_solver.cpp:136] Iteration 81400, lr = 0.0049125, m = 0.9
I0815 14:12:45.455142  8764 solver.cpp:312] Iteration 81500 (6.43674 iter/s, 15.5358s/100 iter), loss = 1.62228
I0815 14:12:45.455171  8764 solver.cpp:334]     Train net output #0: loss = 1.61507 (* 1 = 1.61507 loss)
I0815 14:12:45.455178  8764 sgd_solver.cpp:136] Iteration 81500, lr = 0.00490625, m = 0.9
I0815 14:13:04.163959  8764 solver.cpp:312] Iteration 81600 (5.34522 iter/s, 18.7083s/100 iter), loss = 1.26786
I0815 14:13:04.164036  8764 solver.cpp:334]     Train net output #0: loss = 1.18083 (* 1 = 1.18083 loss)
I0815 14:13:04.164049  8764 sgd_solver.cpp:136] Iteration 81600, lr = 0.0049, m = 0.9
I0815 14:13:20.646679  8764 solver.cpp:312] Iteration 81700 (6.06713 iter/s, 16.4823s/100 iter), loss = 1.14376
I0815 14:13:20.646708  8764 solver.cpp:334]     Train net output #0: loss = 1.09733 (* 1 = 1.09733 loss)
I0815 14:13:20.646714  8764 sgd_solver.cpp:136] Iteration 81700, lr = 0.00489375, m = 0.9
I0815 14:13:35.895846  8764 solver.cpp:312] Iteration 81800 (6.55791 iter/s, 15.2487s/100 iter), loss = 1.60374
I0815 14:13:35.895923  8764 solver.cpp:334]     Train net output #0: loss = 1.30075 (* 1 = 1.30075 loss)
I0815 14:13:35.895932  8764 sgd_solver.cpp:136] Iteration 81800, lr = 0.0048875, m = 0.9
I0815 14:13:50.791602  8764 solver.cpp:312] Iteration 81900 (6.7135 iter/s, 14.8953s/100 iter), loss = 1.19126
I0815 14:13:50.791672  8764 solver.cpp:334]     Train net output #0: loss = 1.23163 (* 1 = 1.23163 loss)
I0815 14:13:50.791692  8764 sgd_solver.cpp:136] Iteration 81900, lr = 0.00488125, m = 0.9
I0815 14:14:06.921975  8764 solver.cpp:363] Sparsity after update:
I0815 14:14:06.926610  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:14:06.926743  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:14:06.926836  8764 net.cpp:2192] conv1b_param_0(0.597) 
I0815 14:14:06.926923  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:14:06.927006  8764 net.cpp:2192] res2a_branch2a_param_0(0.618) 
I0815 14:14:06.927098  8764 net.cpp:2192] res2a_branch2b_param_0(0.604) 
I0815 14:14:06.927188  8764 net.cpp:2192] res3a_branch2a_param_0(0.62) 
I0815 14:14:06.927278  8764 net.cpp:2192] res3a_branch2b_param_0(0.617) 
I0815 14:14:06.927364  8764 net.cpp:2192] res4a_branch2a_param_0(0.62) 
I0815 14:14:06.927454  8764 net.cpp:2192] res4a_branch2b_param_0(0.62) 
I0815 14:14:06.927542  8764 net.cpp:2192] res5a_branch2a_param_0(0.62) 
I0815 14:14:06.927630  8764 net.cpp:2192] res5a_branch2b_param_0(0.62) 
I0815 14:14:06.927719  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.45839e+06/2.86678e+06) 0.509
I0815 14:14:06.927827  8764 solver.cpp:509] Iteration 82000, Testing net (#0)
I0815 14:14:41.069046  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576882
I0815 14:14:41.069139  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.811586
I0815 14:14:41.069159  8764 solver.cpp:594]     Test net output #2: loss = 1.84846 (* 1 = 1.84846 loss)
I0815 14:14:41.069201  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.1404s
I0815 14:14:41.257091  8794 solver.cpp:409] Finding and applying sparsity: 0.63
I0815 14:15:53.922154  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:15:53.924533  8764 solver.cpp:312] Iteration 82000 (0.812153 iter/s, 123.13s/100 iter), loss = 1.23752
I0815 14:15:53.924582  8764 solver.cpp:334]     Train net output #0: loss = 1.146 (* 1 = 1.146 loss)
I0815 14:15:53.924597  8764 sgd_solver.cpp:136] Iteration 82000, lr = 0.004875, m = 0.9
I0815 14:16:13.142514  8764 solver.cpp:312] Iteration 82100 (5.20361 iter/s, 19.2174s/100 iter), loss = 1.23006
I0815 14:16:13.142604  8764 solver.cpp:334]     Train net output #0: loss = 1.32661 (* 1 = 1.32661 loss)
I0815 14:16:13.142629  8764 sgd_solver.cpp:136] Iteration 82100, lr = 0.00486875, m = 0.9
I0815 14:16:29.358317  8764 solver.cpp:312] Iteration 82200 (6.167 iter/s, 16.2153s/100 iter), loss = 1.39961
I0815 14:16:29.358379  8764 solver.cpp:334]     Train net output #0: loss = 1.10819 (* 1 = 1.10819 loss)
I0815 14:16:29.358386  8764 sgd_solver.cpp:136] Iteration 82200, lr = 0.0048625, m = 0.9
I0815 14:16:45.544492  8764 solver.cpp:312] Iteration 82300 (6.17829 iter/s, 16.1857s/100 iter), loss = 1.35569
I0815 14:16:45.544535  8764 solver.cpp:334]     Train net output #0: loss = 1.03428 (* 1 = 1.03428 loss)
I0815 14:16:45.544548  8764 sgd_solver.cpp:136] Iteration 82300, lr = 0.00485625, m = 0.9
I0815 14:17:04.436385  8764 solver.cpp:312] Iteration 82400 (5.29342 iter/s, 18.8914s/100 iter), loss = 1.21874
I0815 14:17:04.436650  8764 solver.cpp:334]     Train net output #0: loss = 1.36399 (* 1 = 1.36399 loss)
I0815 14:17:04.436755  8764 sgd_solver.cpp:136] Iteration 82400, lr = 0.00485, m = 0.9
I0815 14:17:21.093336  8764 solver.cpp:312] Iteration 82500 (6.00366 iter/s, 16.6565s/100 iter), loss = 1.17384
I0815 14:17:21.093361  8764 solver.cpp:334]     Train net output #0: loss = 0.924864 (* 1 = 0.924864 loss)
I0815 14:17:21.093364  8764 sgd_solver.cpp:136] Iteration 82500, lr = 0.00484375, m = 0.9
I0815 14:17:37.215070  8764 solver.cpp:312] Iteration 82600 (6.20298 iter/s, 16.1213s/100 iter), loss = 1.32248
I0815 14:17:37.215134  8764 solver.cpp:334]     Train net output #0: loss = 1.47368 (* 1 = 1.47368 loss)
I0815 14:17:37.215142  8764 sgd_solver.cpp:136] Iteration 82600, lr = 0.0048375, m = 0.9
I0815 14:17:56.606057  8764 solver.cpp:312] Iteration 82700 (5.15718 iter/s, 19.3904s/100 iter), loss = 1.46028
I0815 14:17:56.613450  8764 solver.cpp:334]     Train net output #0: loss = 1.43146 (* 1 = 1.43146 loss)
I0815 14:17:56.613471  8764 sgd_solver.cpp:136] Iteration 82700, lr = 0.00483125, m = 0.9
I0815 14:18:14.912348  8764 solver.cpp:312] Iteration 82800 (5.46277 iter/s, 18.3057s/100 iter), loss = 1.29521
I0815 14:18:14.912564  8764 solver.cpp:334]     Train net output #0: loss = 1.44741 (* 1 = 1.44741 loss)
I0815 14:18:14.912643  8764 sgd_solver.cpp:136] Iteration 82800, lr = 0.004825, m = 0.9
I0815 14:18:32.626485  8764 solver.cpp:312] Iteration 82900 (5.64536 iter/s, 17.7137s/100 iter), loss = 1.29959
I0815 14:18:32.626543  8764 solver.cpp:334]     Train net output #0: loss = 1.47281 (* 1 = 1.47281 loss)
I0815 14:18:32.626552  8764 sgd_solver.cpp:136] Iteration 82900, lr = 0.00481875, m = 0.9
I0815 14:18:49.641587  8764 solver.cpp:363] Sparsity after update:
I0815 14:18:49.654044  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:18:49.654153  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:18:49.654225  8764 net.cpp:2192] conv1b_param_0(0.606) 
I0815 14:18:49.654291  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:18:49.654353  8764 net.cpp:2192] res2a_branch2a_param_0(0.628) 
I0815 14:18:49.654425  8764 net.cpp:2192] res2a_branch2b_param_0(0.61) 
I0815 14:18:49.654491  8764 net.cpp:2192] res3a_branch2a_param_0(0.628) 
I0815 14:18:49.654557  8764 net.cpp:2192] res3a_branch2b_param_0(0.628) 
I0815 14:18:49.654623  8764 net.cpp:2192] res4a_branch2a_param_0(0.629) 
I0815 14:18:49.654686  8764 net.cpp:2192] res4a_branch2b_param_0(0.628) 
I0815 14:18:49.654752  8764 net.cpp:2192] res5a_branch2a_param_0(0.63) 
I0815 14:18:49.654816  8764 net.cpp:2192] res5a_branch2b_param_0(0.629) 
I0815 14:18:49.654881  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.48118e+06/2.86678e+06) 0.517
I0815 14:18:49.786146  8794 solver.cpp:409] Finding and applying sparsity: 0.64
I0815 14:20:01.378927  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:20:01.380923  8764 solver.cpp:312] Iteration 83000 (1.12673 iter/s, 88.752s/100 iter), loss = 1.29777
I0815 14:20:01.380946  8764 solver.cpp:334]     Train net output #0: loss = 1.32254 (* 1 = 1.32254 loss)
I0815 14:20:01.380954  8764 sgd_solver.cpp:136] Iteration 83000, lr = 0.0048125, m = 0.9
I0815 14:20:19.042560  8764 solver.cpp:312] Iteration 83100 (5.66215 iter/s, 17.6611s/100 iter), loss = 1.58361
I0815 14:20:19.042620  8764 solver.cpp:334]     Train net output #0: loss = 1.62628 (* 1 = 1.62628 loss)
I0815 14:20:19.042637  8764 sgd_solver.cpp:136] Iteration 83100, lr = 0.00480625, m = 0.9
I0815 14:20:37.817981  8764 solver.cpp:312] Iteration 83200 (5.32626 iter/s, 18.7749s/100 iter), loss = 1.2578
I0815 14:20:37.818233  8764 solver.cpp:334]     Train net output #0: loss = 1.5599 (* 1 = 1.5599 loss)
I0815 14:20:37.818325  8764 sgd_solver.cpp:136] Iteration 83200, lr = 0.0048, m = 0.9
I0815 14:20:54.776161  8764 solver.cpp:312] Iteration 83300 (5.89702 iter/s, 16.9577s/100 iter), loss = 1.18968
I0815 14:20:54.776233  8764 solver.cpp:334]     Train net output #0: loss = 1.06591 (* 1 = 1.06591 loss)
I0815 14:20:54.776252  8764 sgd_solver.cpp:136] Iteration 83300, lr = 0.00479375, m = 0.9
I0815 14:21:12.345999  8764 solver.cpp:312] Iteration 83400 (5.69173 iter/s, 17.5694s/100 iter), loss = 1.83297
I0815 14:21:12.346055  8764 solver.cpp:334]     Train net output #0: loss = 2.17298 (* 1 = 2.17298 loss)
I0815 14:21:12.346062  8764 sgd_solver.cpp:136] Iteration 83400, lr = 0.0047875, m = 0.9
I0815 14:21:28.733192  8764 solver.cpp:312] Iteration 83500 (6.1025 iter/s, 16.3867s/100 iter), loss = 1.23815
I0815 14:21:28.733239  8764 solver.cpp:334]     Train net output #0: loss = 1.39546 (* 1 = 1.39546 loss)
I0815 14:21:28.733250  8764 sgd_solver.cpp:136] Iteration 83500, lr = 0.00478125, m = 0.9
I0815 14:21:46.220757  8764 solver.cpp:312] Iteration 83600 (5.71851 iter/s, 17.4871s/100 iter), loss = 1.7465
I0815 14:21:46.220861  8764 solver.cpp:334]     Train net output #0: loss = 1.59385 (* 1 = 1.59385 loss)
I0815 14:21:46.220877  8764 sgd_solver.cpp:136] Iteration 83600, lr = 0.004775, m = 0.9
I0815 14:22:05.888160  8764 solver.cpp:312] Iteration 83700 (5.0847 iter/s, 19.6669s/100 iter), loss = 1.38969
I0815 14:22:05.888204  8764 solver.cpp:334]     Train net output #0: loss = 1.53208 (* 1 = 1.53208 loss)
I0815 14:22:05.888214  8764 sgd_solver.cpp:136] Iteration 83700, lr = 0.00476875, m = 0.9
I0815 14:22:24.355553  8764 solver.cpp:312] Iteration 83800 (5.4151 iter/s, 18.4669s/100 iter), loss = 1.46024
I0815 14:22:24.355629  8764 solver.cpp:334]     Train net output #0: loss = 1.7669 (* 1 = 1.7669 loss)
I0815 14:22:24.355641  8764 sgd_solver.cpp:136] Iteration 83800, lr = 0.0047625, m = 0.9
I0815 14:22:42.120383  8764 solver.cpp:312] Iteration 83900 (5.62925 iter/s, 17.7643s/100 iter), loss = 1.46721
I0815 14:22:42.120410  8764 solver.cpp:334]     Train net output #0: loss = 1.37755 (* 1 = 1.37755 loss)
I0815 14:22:42.120414  8764 sgd_solver.cpp:136] Iteration 83900, lr = 0.00475625, m = 0.9
I0815 14:22:58.909368  8764 solver.cpp:363] Sparsity after update:
I0815 14:22:58.914085  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:22:58.914109  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:22:58.914124  8764 net.cpp:2192] conv1b_param_0(0.614) 
I0815 14:22:58.914134  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:22:58.914144  8764 net.cpp:2192] res2a_branch2a_param_0(0.638) 
I0815 14:22:58.914151  8764 net.cpp:2192] res2a_branch2b_param_0(0.62) 
I0815 14:22:58.914160  8764 net.cpp:2192] res3a_branch2a_param_0(0.639) 
I0815 14:22:58.914170  8764 net.cpp:2192] res3a_branch2b_param_0(0.637) 
I0815 14:22:58.914178  8764 net.cpp:2192] res4a_branch2a_param_0(0.64) 
I0815 14:22:58.914187  8764 net.cpp:2192] res4a_branch2b_param_0(0.639) 
I0815 14:22:58.914196  8764 net.cpp:2192] res5a_branch2a_param_0(0.64) 
I0815 14:22:58.914204  8764 net.cpp:2192] res5a_branch2b_param_0(0.64) 
I0815 14:22:58.914213  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.50512e+06/2.86678e+06) 0.525
I0815 14:22:58.914230  8764 solver.cpp:509] Iteration 84000, Testing net (#0)
I0815 14:23:24.587138  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:23:31.684767  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585294
I0815 14:23:31.684844  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812585
I0815 14:23:31.684854  8764 solver.cpp:594]     Test net output #2: loss = 1.8141 (* 1 = 1.8141 loss)
I0815 14:23:31.684875  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.7697s
I0815 14:23:31.822388  8794 solver.cpp:409] Finding and applying sparsity: 0.65
I0815 14:24:16.233182  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:24:16.235266  8764 solver.cpp:312] Iteration 84000 (1.06256 iter/s, 94.1123s/100 iter), loss = 1.32017
I0815 14:24:16.235286  8764 solver.cpp:334]     Train net output #0: loss = 1.59795 (* 1 = 1.59795 loss)
I0815 14:24:16.235296  8764 sgd_solver.cpp:136] Iteration 84000, lr = 0.00475, m = 0.9
I0815 14:24:33.632000  8764 solver.cpp:312] Iteration 84100 (5.74837 iter/s, 17.3962s/100 iter), loss = 1.49362
I0815 14:24:33.632277  8764 solver.cpp:334]     Train net output #0: loss = 1.42017 (* 1 = 1.42017 loss)
I0815 14:24:33.632416  8764 sgd_solver.cpp:136] Iteration 84100, lr = 0.00474375, m = 0.9
I0815 14:24:49.715581  8764 solver.cpp:312] Iteration 84200 (6.21769 iter/s, 16.0831s/100 iter), loss = 1.60583
I0815 14:24:49.715637  8764 solver.cpp:334]     Train net output #0: loss = 1.85842 (* 1 = 1.85842 loss)
I0815 14:24:49.715644  8764 sgd_solver.cpp:136] Iteration 84200, lr = 0.0047375, m = 0.9
I0815 14:25:06.675746  8764 solver.cpp:312] Iteration 84300 (5.89633 iter/s, 16.9597s/100 iter), loss = 1.50066
I0815 14:25:06.675772  8764 solver.cpp:334]     Train net output #0: loss = 1.44543 (* 1 = 1.44543 loss)
I0815 14:25:06.675778  8764 sgd_solver.cpp:136] Iteration 84300, lr = 0.00473125, m = 0.9
I0815 14:25:23.736083  8764 solver.cpp:312] Iteration 84400 (5.86171 iter/s, 17.0599s/100 iter), loss = 1.22322
I0815 14:25:23.736217  8764 solver.cpp:334]     Train net output #0: loss = 1.22081 (* 1 = 1.22081 loss)
I0815 14:25:23.736233  8764 sgd_solver.cpp:136] Iteration 84400, lr = 0.004725, m = 0.9
I0815 14:25:40.505393  8764 solver.cpp:312] Iteration 84500 (5.96344 iter/s, 16.7688s/100 iter), loss = 1.42346
I0815 14:25:40.505417  8764 solver.cpp:334]     Train net output #0: loss = 1.27846 (* 1 = 1.27846 loss)
I0815 14:25:40.505422  8764 sgd_solver.cpp:136] Iteration 84500, lr = 0.00471875, m = 0.9
I0815 14:25:57.772173  8764 solver.cpp:312] Iteration 84600 (5.79163 iter/s, 17.2663s/100 iter), loss = 1.26689
I0815 14:25:57.772235  8764 solver.cpp:334]     Train net output #0: loss = 1.10734 (* 1 = 1.10734 loss)
I0815 14:25:57.772243  8764 sgd_solver.cpp:136] Iteration 84600, lr = 0.0047125, m = 0.9
I0815 14:26:19.764334  8764 solver.cpp:312] Iteration 84700 (4.5472 iter/s, 21.9916s/100 iter), loss = 1.30703
I0815 14:26:19.764369  8764 solver.cpp:334]     Train net output #0: loss = 1.08755 (* 1 = 1.08755 loss)
I0815 14:26:19.764377  8764 sgd_solver.cpp:136] Iteration 84700, lr = 0.00470625, m = 0.9
I0815 14:26:40.983780  8764 solver.cpp:312] Iteration 84800 (4.71279 iter/s, 21.2189s/100 iter), loss = 1.36915
I0815 14:26:40.983831  8764 solver.cpp:334]     Train net output #0: loss = 0.98797 (* 1 = 0.98797 loss)
I0815 14:26:40.983839  8764 sgd_solver.cpp:136] Iteration 84800, lr = 0.0047, m = 0.9
I0815 14:26:58.317769  8764 solver.cpp:312] Iteration 84900 (5.76917 iter/s, 17.3335s/100 iter), loss = 1.39134
I0815 14:26:58.317793  8764 solver.cpp:334]     Train net output #0: loss = 1.1584 (* 1 = 1.1584 loss)
I0815 14:26:58.317798  8764 sgd_solver.cpp:136] Iteration 84900, lr = 0.00469375, m = 0.9
I0815 14:27:18.091308  8764 solver.cpp:363] Sparsity after update:
I0815 14:27:18.101887  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:27:18.101992  8764 net.cpp:2192] conv1a_param_0(0.318) 
I0815 14:27:18.102059  8764 net.cpp:2192] conv1b_param_0(0.615) 
I0815 14:27:18.102120  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:27:18.102180  8764 net.cpp:2192] res2a_branch2a_param_0(0.648) 
I0815 14:27:18.102247  8764 net.cpp:2192] res2a_branch2b_param_0(0.625) 
I0815 14:27:18.102310  8764 net.cpp:2192] res3a_branch2a_param_0(0.649) 
I0815 14:27:18.102371  8764 net.cpp:2192] res3a_branch2b_param_0(0.647) 
I0815 14:27:18.102435  8764 net.cpp:2192] res4a_branch2a_param_0(0.649) 
I0815 14:27:18.102497  8764 net.cpp:2192] res4a_branch2b_param_0(0.649) 
I0815 14:27:18.102558  8764 net.cpp:2192] res5a_branch2a_param_0(0.65) 
I0815 14:27:18.102618  8764 net.cpp:2192] res5a_branch2b_param_0(0.649) 
I0815 14:27:18.102680  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.52828e+06/2.86678e+06) 0.533
I0815 14:27:18.337580  8794 solver.cpp:409] Finding and applying sparsity: 0.66
I0815 14:28:37.698923  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:28:37.701458  8764 solver.cpp:312] Iteration 85000 (1.00623 iter/s, 99.381s/100 iter), loss = 1.29193
I0815 14:28:37.701498  8764 solver.cpp:334]     Train net output #0: loss = 0.953374 (* 1 = 0.953374 loss)
I0815 14:28:37.701510  8764 sgd_solver.cpp:136] Iteration 85000, lr = 0.0046875, m = 0.9
I0815 14:28:57.880601  8764 solver.cpp:312] Iteration 85100 (4.95576 iter/s, 20.1786s/100 iter), loss = 1.56708
I0815 14:28:57.880874  8764 solver.cpp:334]     Train net output #0: loss = 1.70273 (* 1 = 1.70273 loss)
I0815 14:28:57.881005  8764 sgd_solver.cpp:136] Iteration 85100, lr = 0.00468125, m = 0.9
I0815 14:29:15.844235  8764 solver.cpp:312] Iteration 85200 (5.56696 iter/s, 17.9631s/100 iter), loss = 1.14244
I0815 14:29:15.844297  8764 solver.cpp:334]     Train net output #0: loss = 1.09223 (* 1 = 1.09223 loss)
I0815 14:29:15.844302  8764 sgd_solver.cpp:136] Iteration 85200, lr = 0.004675, m = 0.9
I0815 14:29:35.047286  8764 solver.cpp:312] Iteration 85300 (5.20765 iter/s, 19.2025s/100 iter), loss = 1.60098
I0815 14:29:35.047355  8764 solver.cpp:334]     Train net output #0: loss = 1.41373 (* 1 = 1.41373 loss)
I0815 14:29:35.047371  8764 sgd_solver.cpp:136] Iteration 85300, lr = 0.00466875, m = 0.9
I0815 14:29:54.085996  8764 solver.cpp:312] Iteration 85400 (5.2526 iter/s, 19.0382s/100 iter), loss = 1.4755
I0815 14:29:54.086127  8764 solver.cpp:334]     Train net output #0: loss = 1.72883 (* 1 = 1.72883 loss)
I0815 14:29:54.086149  8764 sgd_solver.cpp:136] Iteration 85400, lr = 0.0046625, m = 0.9
I0815 14:30:13.460446  8764 solver.cpp:312] Iteration 85500 (5.1616 iter/s, 19.3738s/100 iter), loss = 1.224
I0815 14:30:13.460631  8764 solver.cpp:334]     Train net output #0: loss = 1.26732 (* 1 = 1.26732 loss)
I0815 14:30:13.460698  8764 sgd_solver.cpp:136] Iteration 85500, lr = 0.00465625, m = 0.9
I0815 14:30:33.171718  8764 solver.cpp:312] Iteration 85600 (5.07337 iter/s, 19.7108s/100 iter), loss = 1.60804
I0815 14:30:33.171823  8764 solver.cpp:334]     Train net output #0: loss = 1.50811 (* 1 = 1.50811 loss)
I0815 14:30:33.171841  8764 sgd_solver.cpp:136] Iteration 85600, lr = 0.00465, m = 0.9
I0815 14:30:51.553186  8764 solver.cpp:312] Iteration 85700 (5.44041 iter/s, 18.381s/100 iter), loss = 1.12261
I0815 14:30:51.553231  8764 solver.cpp:334]     Train net output #0: loss = 1.42923 (* 1 = 1.42923 loss)
I0815 14:30:51.553241  8764 sgd_solver.cpp:136] Iteration 85700, lr = 0.00464375, m = 0.9
I0815 14:31:11.297709  8764 solver.cpp:312] Iteration 85800 (5.06483 iter/s, 19.744s/100 iter), loss = 1.1753
I0815 14:31:11.297760  8764 solver.cpp:334]     Train net output #0: loss = 1.37679 (* 1 = 1.37679 loss)
I0815 14:31:11.297765  8764 sgd_solver.cpp:136] Iteration 85800, lr = 0.0046375, m = 0.9
I0815 14:31:29.877710  8764 solver.cpp:312] Iteration 85900 (5.38228 iter/s, 18.5795s/100 iter), loss = 1.32522
I0815 14:31:29.877741  8764 solver.cpp:334]     Train net output #0: loss = 1.28554 (* 1 = 1.28554 loss)
I0815 14:31:29.877748  8764 sgd_solver.cpp:136] Iteration 85900, lr = 0.00463125, m = 0.9
I0815 14:31:46.920151  8764 solver.cpp:363] Sparsity after update:
I0815 14:31:46.925767  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:31:46.925869  8764 net.cpp:2192] conv1a_param_0(0.318) 
I0815 14:31:46.925940  8764 net.cpp:2192] conv1b_param_0(0.624) 
I0815 14:31:46.926004  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:31:46.926067  8764 net.cpp:2192] res2a_branch2a_param_0(0.658) 
I0815 14:31:46.926134  8764 net.cpp:2192] res2a_branch2b_param_0(0.635) 
I0815 14:31:46.926198  8764 net.cpp:2192] res3a_branch2a_param_0(0.66) 
I0815 14:31:46.926261  8764 net.cpp:2192] res3a_branch2b_param_0(0.657) 
I0815 14:31:46.926326  8764 net.cpp:2192] res4a_branch2a_param_0(0.66) 
I0815 14:31:46.926388  8764 net.cpp:2192] res4a_branch2b_param_0(0.66) 
I0815 14:31:46.926452  8764 net.cpp:2192] res5a_branch2a_param_0(0.66) 
I0815 14:31:46.926517  8764 net.cpp:2192] res5a_branch2b_param_0(0.66) 
I0815 14:31:46.926580  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.55222e+06/2.86678e+06) 0.541
I0815 14:31:46.926666  8764 solver.cpp:509] Iteration 86000, Testing net (#0)
I0815 14:32:18.983095  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576588
I0815 14:32:18.983142  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810351
I0815 14:32:18.983148  8764 solver.cpp:594]     Test net output #2: loss = 1.83458 (* 1 = 1.83458 loss)
I0815 14:32:18.983170  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.0556s
I0815 14:32:19.126997  8794 solver.cpp:409] Finding and applying sparsity: 0.67
I0815 14:33:03.220955  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:33:03.223006  8764 solver.cpp:312] Iteration 86000 (1.07132 iter/s, 93.3427s/100 iter), loss = 1.36528
I0815 14:33:03.223026  8764 solver.cpp:334]     Train net output #0: loss = 1.73774 (* 1 = 1.73774 loss)
I0815 14:33:03.223033  8764 sgd_solver.cpp:136] Iteration 86000, lr = 0.004625, m = 0.9
I0815 14:33:19.546802  8764 solver.cpp:312] Iteration 86100 (6.1262 iter/s, 16.3233s/100 iter), loss = 1.38526
I0815 14:33:19.546854  8764 solver.cpp:334]     Train net output #0: loss = 1.22924 (* 1 = 1.22924 loss)
I0815 14:33:19.546864  8764 sgd_solver.cpp:136] Iteration 86100, lr = 0.00461875, m = 0.9
I0815 14:33:38.928611  8764 solver.cpp:312] Iteration 86200 (5.15962 iter/s, 19.3813s/100 iter), loss = 1.38776
I0815 14:33:38.928694  8764 solver.cpp:334]     Train net output #0: loss = 1.16959 (* 1 = 1.16959 loss)
I0815 14:33:38.928707  8764 sgd_solver.cpp:136] Iteration 86200, lr = 0.0046125, m = 0.9
I0815 14:33:58.141645  8764 solver.cpp:312] Iteration 86300 (5.20494 iter/s, 19.2125s/100 iter), loss = 1.4739
I0815 14:33:58.141674  8764 solver.cpp:334]     Train net output #0: loss = 1.41462 (* 1 = 1.41462 loss)
I0815 14:33:58.141681  8764 sgd_solver.cpp:136] Iteration 86300, lr = 0.00460625, m = 0.9
I0815 14:34:15.887079  8764 solver.cpp:312] Iteration 86400 (5.63541 iter/s, 17.7449s/100 iter), loss = 1.20137
I0815 14:34:15.887176  8764 solver.cpp:334]     Train net output #0: loss = 1.12167 (* 1 = 1.12167 loss)
I0815 14:34:15.887195  8764 sgd_solver.cpp:136] Iteration 86400, lr = 0.0046, m = 0.9
I0815 14:34:34.841568  8764 solver.cpp:312] Iteration 86500 (5.27594 iter/s, 18.954s/100 iter), loss = 1.16531
I0815 14:34:34.841593  8764 solver.cpp:334]     Train net output #0: loss = 1.22759 (* 1 = 1.22759 loss)
I0815 14:34:34.841599  8764 sgd_solver.cpp:136] Iteration 86500, lr = 0.00459375, m = 0.9
I0815 14:34:53.444231  8764 solver.cpp:312] Iteration 86600 (5.37572 iter/s, 18.6022s/100 iter), loss = 1.62414
I0815 14:34:53.444283  8764 solver.cpp:334]     Train net output #0: loss = 1.58769 (* 1 = 1.58769 loss)
I0815 14:34:53.444289  8764 sgd_solver.cpp:136] Iteration 86600, lr = 0.0045875, m = 0.9
I0815 14:35:11.453657  8764 solver.cpp:312] Iteration 86700 (5.5528 iter/s, 18.0089s/100 iter), loss = 1.29274
I0815 14:35:11.453728  8764 solver.cpp:334]     Train net output #0: loss = 1.08006 (* 1 = 1.08006 loss)
I0815 14:35:11.453747  8764 sgd_solver.cpp:136] Iteration 86700, lr = 0.00458125, m = 0.9
I0815 14:35:27.215175  8764 solver.cpp:312] Iteration 86800 (6.34474 iter/s, 15.7611s/100 iter), loss = 1.50709
I0815 14:35:27.215240  8764 solver.cpp:334]     Train net output #0: loss = 1.5891 (* 1 = 1.5891 loss)
I0815 14:35:27.215247  8764 sgd_solver.cpp:136] Iteration 86800, lr = 0.004575, m = 0.9
I0815 14:35:44.578204  8764 solver.cpp:312] Iteration 86900 (5.75952 iter/s, 17.3626s/100 iter), loss = 1.11304
I0815 14:35:44.578228  8764 solver.cpp:334]     Train net output #0: loss = 1.05889 (* 1 = 1.05889 loss)
I0815 14:35:44.578233  8764 sgd_solver.cpp:136] Iteration 86900, lr = 0.00456875, m = 0.9
I0815 14:36:05.213459  8764 solver.cpp:363] Sparsity after update:
I0815 14:36:05.225980  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:36:05.226024  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:36:05.226044  8764 net.cpp:2192] conv1b_param_0(0.632) 
I0815 14:36:05.226056  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:36:05.226068  8764 net.cpp:2192] res2a_branch2a_param_0(0.665) 
I0815 14:36:05.226079  8764 net.cpp:2192] res2a_branch2b_param_0(0.64) 
I0815 14:36:05.226092  8764 net.cpp:2192] res3a_branch2a_param_0(0.668) 
I0815 14:36:05.226104  8764 net.cpp:2192] res3a_branch2b_param_0(0.664) 
I0815 14:36:05.226115  8764 net.cpp:2192] res4a_branch2a_param_0(0.669) 
I0815 14:36:05.226127  8764 net.cpp:2192] res4a_branch2b_param_0(0.668) 
I0815 14:36:05.226138  8764 net.cpp:2192] res5a_branch2a_param_0(0.67) 
I0815 14:36:05.226150  8764 net.cpp:2192] res5a_branch2b_param_0(0.669) 
I0815 14:36:05.226161  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.57482e+06/2.86678e+06) 0.549
I0815 14:36:05.356005  8794 solver.cpp:409] Finding and applying sparsity: 0.68
I0815 14:37:17.611243  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:37:17.613293  8764 solver.cpp:312] Iteration 87000 (1.07489 iter/s, 93.0326s/100 iter), loss = 1.1714
I0815 14:37:17.613312  8764 solver.cpp:334]     Train net output #0: loss = 1.27568 (* 1 = 1.27568 loss)
I0815 14:37:17.613317  8764 sgd_solver.cpp:136] Iteration 87000, lr = 0.0045625, m = 0.9
I0815 14:37:36.229511  8764 solver.cpp:312] Iteration 87100 (5.37181 iter/s, 18.6157s/100 iter), loss = 1.28489
I0815 14:37:36.229535  8764 solver.cpp:334]     Train net output #0: loss = 1.58833 (* 1 = 1.58833 loss)
I0815 14:37:36.229539  8764 sgd_solver.cpp:136] Iteration 87100, lr = 0.00455625, m = 0.9
I0815 14:37:52.585364  8764 solver.cpp:312] Iteration 87200 (6.11419 iter/s, 16.3554s/100 iter), loss = 1.45596
I0815 14:37:52.585563  8764 solver.cpp:334]     Train net output #0: loss = 1.38621 (* 1 = 1.38621 loss)
I0815 14:37:52.585571  8764 sgd_solver.cpp:136] Iteration 87200, lr = 0.00455, m = 0.9
I0815 14:38:10.834036  8764 solver.cpp:312] Iteration 87300 (5.48 iter/s, 18.2482s/100 iter), loss = 1.88859
I0815 14:38:10.848152  8764 solver.cpp:334]     Train net output #0: loss = 2.08316 (* 1 = 2.08316 loss)
I0815 14:38:10.848193  8764 sgd_solver.cpp:136] Iteration 87300, lr = 0.00454375, m = 0.9
I0815 14:38:29.648268  8764 solver.cpp:312] Iteration 87400 (5.31527 iter/s, 18.8137s/100 iter), loss = 1.46712
I0815 14:38:29.648385  8764 solver.cpp:334]     Train net output #0: loss = 1.55166 (* 1 = 1.55166 loss)
I0815 14:38:29.648411  8764 sgd_solver.cpp:136] Iteration 87400, lr = 0.0045375, m = 0.9
I0815 14:38:47.247921  8764 solver.cpp:312] Iteration 87500 (5.68209 iter/s, 17.5992s/100 iter), loss = 1.30199
I0815 14:38:47.248009  8764 solver.cpp:334]     Train net output #0: loss = 1.59722 (* 1 = 1.59722 loss)
I0815 14:38:47.248040  8764 sgd_solver.cpp:136] Iteration 87500, lr = 0.00453125, m = 0.9
I0815 14:39:04.814240  8764 solver.cpp:312] Iteration 87600 (5.69287 iter/s, 17.5658s/100 iter), loss = 1.35647
I0815 14:39:04.814344  8764 solver.cpp:334]     Train net output #0: loss = 1.52769 (* 1 = 1.52769 loss)
I0815 14:39:04.814358  8764 sgd_solver.cpp:136] Iteration 87600, lr = 0.004525, m = 0.9
I0815 14:39:26.154489  8764 solver.cpp:312] Iteration 87700 (4.68611 iter/s, 21.3397s/100 iter), loss = 1.37923
I0815 14:39:26.154533  8764 solver.cpp:334]     Train net output #0: loss = 1.38089 (* 1 = 1.38089 loss)
I0815 14:39:26.154546  8764 sgd_solver.cpp:136] Iteration 87700, lr = 0.00451875, m = 0.9
I0815 14:39:45.530897  8764 solver.cpp:312] Iteration 87800 (5.16106 iter/s, 19.3759s/100 iter), loss = 1.60718
I0815 14:39:45.531052  8764 solver.cpp:334]     Train net output #0: loss = 1.86513 (* 1 = 1.86513 loss)
I0815 14:39:45.531091  8764 sgd_solver.cpp:136] Iteration 87800, lr = 0.0045125, m = 0.9
I0815 14:40:01.879732  8764 solver.cpp:312] Iteration 87900 (6.11681 iter/s, 16.3484s/100 iter), loss = 1.47245
I0815 14:40:01.879801  8764 solver.cpp:334]     Train net output #0: loss = 1.43464 (* 1 = 1.43464 loss)
I0815 14:40:01.879815  8764 sgd_solver.cpp:136] Iteration 87900, lr = 0.00450625, m = 0.9
I0815 14:40:19.332657  8764 solver.cpp:363] Sparsity after update:
I0815 14:40:19.337808  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:40:19.337819  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:40:19.337857  8764 net.cpp:2192] conv1b_param_0(0.632) 
I0815 14:40:19.337872  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:40:19.337883  8764 net.cpp:2192] res2a_branch2a_param_0(0.675) 
I0815 14:40:19.337895  8764 net.cpp:2192] res2a_branch2b_param_0(0.645) 
I0815 14:40:19.337909  8764 net.cpp:2192] res3a_branch2a_param_0(0.679) 
I0815 14:40:19.337923  8764 net.cpp:2192] res3a_branch2b_param_0(0.673) 
I0815 14:40:19.337934  8764 net.cpp:2192] res4a_branch2a_param_0(0.68) 
I0815 14:40:19.337946  8764 net.cpp:2192] res4a_branch2b_param_0(0.678) 
I0815 14:40:19.337959  8764 net.cpp:2192] res5a_branch2a_param_0(0.68) 
I0815 14:40:19.337970  8764 net.cpp:2192] res5a_branch2b_param_0(0.68) 
I0815 14:40:19.337981  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.59865e+06/2.86678e+06) 0.558
I0815 14:40:19.338011  8764 solver.cpp:509] Iteration 88000, Testing net (#0)
I0815 14:40:23.270232  8747 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 14:40:31.452127  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:40:48.271651  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585059
I0815 14:40:48.271672  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.813115
I0815 14:40:48.271679  8764 solver.cpp:594]     Test net output #2: loss = 1.81377 (* 1 = 1.81377 loss)
I0815 14:40:48.271698  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.9329s
I0815 14:40:48.424757  8794 solver.cpp:409] Finding and applying sparsity: 0.69
I0815 14:41:34.376721  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:41:34.378772  8764 solver.cpp:312] Iteration 88000 (1.08112 iter/s, 92.4965s/100 iter), loss = 1.16957
I0815 14:41:34.378794  8764 solver.cpp:334]     Train net output #0: loss = 1.0959 (* 1 = 1.0959 loss)
I0815 14:41:34.378801  8764 sgd_solver.cpp:136] Iteration 88000, lr = 0.0045, m = 0.9
I0815 14:41:51.256121  8764 solver.cpp:312] Iteration 88100 (5.92527 iter/s, 16.8769s/100 iter), loss = 1.24707
I0815 14:41:51.256150  8764 solver.cpp:334]     Train net output #0: loss = 1.04758 (* 1 = 1.04758 loss)
I0815 14:41:51.256155  8764 sgd_solver.cpp:136] Iteration 88100, lr = 0.00449375, m = 0.9
I0815 14:42:08.360824  8764 solver.cpp:312] Iteration 88200 (5.84651 iter/s, 17.1042s/100 iter), loss = 1.19267
I0815 14:42:08.360877  8764 solver.cpp:334]     Train net output #0: loss = 1.11566 (* 1 = 1.11566 loss)
I0815 14:42:08.360885  8764 sgd_solver.cpp:136] Iteration 88200, lr = 0.0044875, m = 0.9
I0815 14:42:26.063160  8764 solver.cpp:312] Iteration 88300 (5.64913 iter/s, 17.7018s/100 iter), loss = 1.57109
I0815 14:42:26.063202  8764 solver.cpp:334]     Train net output #0: loss = 1.6236 (* 1 = 1.6236 loss)
I0815 14:42:26.063211  8764 sgd_solver.cpp:136] Iteration 88300, lr = 0.00448125, m = 0.9
I0815 14:42:44.205312  8764 solver.cpp:312] Iteration 88400 (5.51218 iter/s, 18.1417s/100 iter), loss = 1.51424
I0815 14:42:44.205373  8764 solver.cpp:334]     Train net output #0: loss = 1.76304 (* 1 = 1.76304 loss)
I0815 14:42:44.205380  8764 sgd_solver.cpp:136] Iteration 88400, lr = 0.004475, m = 0.9
I0815 14:43:05.165290  8764 solver.cpp:312] Iteration 88500 (4.77113 iter/s, 20.9594s/100 iter), loss = 1.2715
I0815 14:43:05.165356  8764 solver.cpp:334]     Train net output #0: loss = 1.26755 (* 1 = 1.26755 loss)
I0815 14:43:05.165374  8764 sgd_solver.cpp:136] Iteration 88500, lr = 0.00446875, m = 0.9
I0815 14:43:24.708734  8764 solver.cpp:312] Iteration 88600 (5.11695 iter/s, 19.5429s/100 iter), loss = 1.47151
I0815 14:43:24.708827  8764 solver.cpp:334]     Train net output #0: loss = 1.4769 (* 1 = 1.4769 loss)
I0815 14:43:24.708847  8764 sgd_solver.cpp:136] Iteration 88600, lr = 0.0044625, m = 0.9
I0815 14:43:42.052404  8764 solver.cpp:312] Iteration 88700 (5.76596 iter/s, 17.3432s/100 iter), loss = 1.66332
I0815 14:43:42.052469  8764 solver.cpp:334]     Train net output #0: loss = 2.09987 (* 1 = 2.09987 loss)
I0815 14:43:42.052481  8764 sgd_solver.cpp:136] Iteration 88700, lr = 0.00445625, m = 0.9
I0815 14:44:03.166551  8764 solver.cpp:312] Iteration 88800 (4.73629 iter/s, 21.1136s/100 iter), loss = 1.4042
I0815 14:44:03.172232  8764 solver.cpp:334]     Train net output #0: loss = 1.42055 (* 1 = 1.42055 loss)
I0815 14:44:03.172264  8764 sgd_solver.cpp:136] Iteration 88800, lr = 0.00445, m = 0.9
I0815 14:44:20.548537  8764 solver.cpp:312] Iteration 88900 (5.75324 iter/s, 17.3815s/100 iter), loss = 1.42491
I0815 14:44:20.548569  8764 solver.cpp:334]     Train net output #0: loss = 0.877914 (* 1 = 0.877914 loss)
I0815 14:44:20.548575  8764 sgd_solver.cpp:136] Iteration 88900, lr = 0.00444375, m = 0.9
I0815 14:44:37.611142  8764 solver.cpp:363] Sparsity after update:
I0815 14:44:37.623896  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:44:37.623913  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:44:37.623922  8764 net.cpp:2192] conv1b_param_0(0.638) 
I0815 14:44:37.623926  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:44:37.623929  8764 net.cpp:2192] res2a_branch2a_param_0(0.684) 
I0815 14:44:37.623934  8764 net.cpp:2192] res2a_branch2b_param_0(0.653) 
I0815 14:44:37.623940  8764 net.cpp:2192] res3a_branch2a_param_0(0.689) 
I0815 14:44:37.623946  8764 net.cpp:2192] res3a_branch2b_param_0(0.682) 
I0815 14:44:37.623951  8764 net.cpp:2192] res4a_branch2a_param_0(0.689) 
I0815 14:44:37.623956  8764 net.cpp:2192] res4a_branch2b_param_0(0.688) 
I0815 14:44:37.623961  8764 net.cpp:2192] res5a_branch2a_param_0(0.69) 
I0815 14:44:37.623965  8764 net.cpp:2192] res5a_branch2b_param_0(0.689) 
I0815 14:44:37.623967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.62174e+06/2.86678e+06) 0.566
I0815 14:44:37.757796  8794 solver.cpp:409] Finding and applying sparsity: 0.7
I0815 14:45:26.977248  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:45:26.979370  8764 solver.cpp:312] Iteration 89000 (1.50537 iter/s, 66.429s/100 iter), loss = 1.56292
I0815 14:45:26.979393  8764 solver.cpp:334]     Train net output #0: loss = 1.76041 (* 1 = 1.76041 loss)
I0815 14:45:26.979399  8764 sgd_solver.cpp:136] Iteration 89000, lr = 0.0044375, m = 0.9
I0815 14:45:46.550619  8764 solver.cpp:312] Iteration 89100 (5.10968 iter/s, 19.5707s/100 iter), loss = 1.08682
I0815 14:45:46.550649  8764 solver.cpp:334]     Train net output #0: loss = 1.47752 (* 1 = 1.47752 loss)
I0815 14:45:46.550655  8764 sgd_solver.cpp:136] Iteration 89100, lr = 0.00443125, m = 0.9
I0815 14:46:04.225690  8764 solver.cpp:312] Iteration 89200 (5.65784 iter/s, 17.6746s/100 iter), loss = 1.86854
I0815 14:46:04.225783  8764 solver.cpp:334]     Train net output #0: loss = 1.37382 (* 1 = 1.37382 loss)
I0815 14:46:04.225803  8764 sgd_solver.cpp:136] Iteration 89200, lr = 0.004425, m = 0.9
I0815 14:46:22.927530  8764 solver.cpp:312] Iteration 89300 (5.34722 iter/s, 18.7013s/100 iter), loss = 1.12347
I0815 14:46:22.927620  8764 solver.cpp:334]     Train net output #0: loss = 0.965724 (* 1 = 0.965724 loss)
I0815 14:46:22.927644  8764 sgd_solver.cpp:136] Iteration 89300, lr = 0.00441875, m = 0.9
I0815 14:46:41.663413  8764 solver.cpp:312] Iteration 89400 (5.3375 iter/s, 18.7354s/100 iter), loss = 1.121
I0815 14:46:41.663488  8764 solver.cpp:334]     Train net output #0: loss = 1.21622 (* 1 = 1.21622 loss)
I0815 14:46:41.663496  8764 sgd_solver.cpp:136] Iteration 89400, lr = 0.0044125, m = 0.9
I0815 14:47:02.932246  8764 solver.cpp:312] Iteration 89500 (4.70184 iter/s, 21.2682s/100 iter), loss = 1.21942
I0815 14:47:02.932293  8764 solver.cpp:334]     Train net output #0: loss = 0.935765 (* 1 = 0.935765 loss)
I0815 14:47:02.932304  8764 sgd_solver.cpp:136] Iteration 89500, lr = 0.00440625, m = 0.9
I0815 14:47:23.321877  8764 solver.cpp:312] Iteration 89600 (4.90459 iter/s, 20.3891s/100 iter), loss = 1.34851
I0815 14:47:23.321972  8764 solver.cpp:334]     Train net output #0: loss = 1.31478 (* 1 = 1.31478 loss)
I0815 14:47:23.321992  8764 sgd_solver.cpp:136] Iteration 89600, lr = 0.0044, m = 0.9
I0815 14:47:42.123622  8764 solver.cpp:312] Iteration 89700 (5.3188 iter/s, 18.8012s/100 iter), loss = 1.45222
I0815 14:47:42.123675  8764 solver.cpp:334]     Train net output #0: loss = 1.49028 (* 1 = 1.49028 loss)
I0815 14:47:42.123688  8764 sgd_solver.cpp:136] Iteration 89700, lr = 0.00439375, m = 0.9
I0815 14:48:01.423529  8764 solver.cpp:312] Iteration 89800 (5.18152 iter/s, 19.2994s/100 iter), loss = 1.67831
I0815 14:48:01.423624  8764 solver.cpp:334]     Train net output #0: loss = 1.96836 (* 1 = 1.96836 loss)
I0815 14:48:01.423636  8764 sgd_solver.cpp:136] Iteration 89800, lr = 0.0043875, m = 0.9
I0815 14:48:19.164810  8764 solver.cpp:312] Iteration 89900 (5.63672 iter/s, 17.7408s/100 iter), loss = 1.4803
I0815 14:48:19.164837  8764 solver.cpp:334]     Train net output #0: loss = 1.77303 (* 1 = 1.77303 loss)
I0815 14:48:19.164842  8764 sgd_solver.cpp:136] Iteration 89900, lr = 0.00438125, m = 0.9
I0815 14:48:40.358762  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_90000.caffemodel
I0815 14:48:40.416069  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_90000.solverstate
I0815 14:48:40.428184  8764 solver.cpp:363] Sparsity after update:
I0815 14:48:40.430234  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:48:40.430250  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:48:40.430260  8764 net.cpp:2192] conv1b_param_0(0.645) 
I0815 14:48:40.430265  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:48:40.430269  8764 net.cpp:2192] res2a_branch2a_param_0(0.694) 
I0815 14:48:40.430272  8764 net.cpp:2192] res2a_branch2b_param_0(0.657) 
I0815 14:48:40.430275  8764 net.cpp:2192] res3a_branch2a_param_0(0.7) 
I0815 14:48:40.430279  8764 net.cpp:2192] res3a_branch2b_param_0(0.691) 
I0815 14:48:40.430281  8764 net.cpp:2192] res4a_branch2a_param_0(0.7) 
I0815 14:48:40.430284  8764 net.cpp:2192] res4a_branch2b_param_0(0.698) 
I0815 14:48:40.430287  8764 net.cpp:2192] res5a_branch2a_param_0(0.7) 
I0815 14:48:40.430290  8764 net.cpp:2192] res5a_branch2b_param_0(0.7) 
I0815 14:48:40.430294  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.64552e+06/2.86678e+06) 0.574
I0815 14:48:40.430304  8764 solver.cpp:509] Iteration 90000, Testing net (#0)
I0815 14:49:13.643223  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588118
I0815 14:49:13.643260  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817468
I0815 14:49:13.643266  8764 solver.cpp:594]     Test net output #2: loss = 1.80664 (* 1 = 1.80664 loss)
I0815 14:49:13.643285  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.2121s
I0815 14:49:13.783923  8794 solver.cpp:409] Finding and applying sparsity: 0.71
I0815 14:50:02.106982  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:50:02.109035  8764 solver.cpp:312] Iteration 90000 (0.971426 iter/s, 102.941s/100 iter), loss = 1.75389
I0815 14:50:02.109055  8764 solver.cpp:334]     Train net output #0: loss = 1.93343 (* 1 = 1.93343 loss)
I0815 14:50:02.109063  8764 sgd_solver.cpp:136] Iteration 90000, lr = 0.004375, m = 0.9
I0815 14:50:20.406991  8764 solver.cpp:312] Iteration 90100 (5.46524 iter/s, 18.2974s/100 iter), loss = 1.4065
I0815 14:50:20.407016  8764 solver.cpp:334]     Train net output #0: loss = 1.26586 (* 1 = 1.26586 loss)
I0815 14:50:20.407021  8764 sgd_solver.cpp:136] Iteration 90100, lr = 0.00436875, m = 0.9
I0815 14:50:40.117106  8764 solver.cpp:312] Iteration 90200 (5.07368 iter/s, 19.7096s/100 iter), loss = 1.23461
I0815 14:50:40.117197  8764 solver.cpp:334]     Train net output #0: loss = 1.14493 (* 1 = 1.14493 loss)
I0815 14:50:40.117215  8764 sgd_solver.cpp:136] Iteration 90200, lr = 0.0043625, m = 0.9
I0815 14:50:58.961380  8764 solver.cpp:312] Iteration 90300 (5.3068 iter/s, 18.8438s/100 iter), loss = 1.01083
I0815 14:50:58.961405  8764 solver.cpp:334]     Train net output #0: loss = 0.798531 (* 1 = 0.798531 loss)
I0815 14:50:58.961411  8764 sgd_solver.cpp:136] Iteration 90300, lr = 0.00435625, m = 0.9
I0815 14:51:16.247588  8764 solver.cpp:312] Iteration 90400 (5.78512 iter/s, 17.2857s/100 iter), loss = 1.34646
I0815 14:51:16.247680  8764 solver.cpp:334]     Train net output #0: loss = 1.20842 (* 1 = 1.20842 loss)
I0815 14:51:16.247697  8764 sgd_solver.cpp:136] Iteration 90400, lr = 0.00435, m = 0.9
I0815 14:51:32.535013  8764 solver.cpp:312] Iteration 90500 (6.13992 iter/s, 16.2869s/100 iter), loss = 1.11883
I0815 14:51:32.535308  8764 solver.cpp:334]     Train net output #0: loss = 1.34529 (* 1 = 1.34529 loss)
I0815 14:51:32.535362  8764 sgd_solver.cpp:136] Iteration 90500, lr = 0.00434375, m = 0.9
I0815 14:51:47.855242  8764 solver.cpp:312] Iteration 90600 (6.52747 iter/s, 15.3199s/100 iter), loss = 1.29711
I0815 14:51:47.855357  8764 solver.cpp:334]     Train net output #0: loss = 0.983411 (* 1 = 0.983411 loss)
I0815 14:51:47.855381  8764 sgd_solver.cpp:136] Iteration 90600, lr = 0.0043375, m = 0.9
I0815 14:52:03.638200  8764 solver.cpp:312] Iteration 90700 (6.33612 iter/s, 15.7825s/100 iter), loss = 1.0757
I0815 14:52:03.640756  8764 solver.cpp:334]     Train net output #0: loss = 1.22026 (* 1 = 1.22026 loss)
I0815 14:52:03.640768  8764 sgd_solver.cpp:136] Iteration 90700, lr = 0.00433125, m = 0.9
I0815 14:52:21.888161  8764 solver.cpp:312] Iteration 90800 (5.47961 iter/s, 18.2495s/100 iter), loss = 1.45639
I0815 14:52:21.888219  8764 solver.cpp:334]     Train net output #0: loss = 1.0713 (* 1 = 1.0713 loss)
I0815 14:52:21.888226  8764 sgd_solver.cpp:136] Iteration 90800, lr = 0.004325, m = 0.9
I0815 14:52:39.398057  8764 solver.cpp:312] Iteration 90900 (5.71121 iter/s, 17.5094s/100 iter), loss = 1.86629
I0815 14:52:39.398090  8764 solver.cpp:334]     Train net output #0: loss = 2.06372 (* 1 = 2.06372 loss)
I0815 14:52:39.398097  8764 sgd_solver.cpp:136] Iteration 90900, lr = 0.00431875, m = 0.9
I0815 14:52:58.067046  8764 solver.cpp:363] Sparsity after update:
I0815 14:52:58.079773  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:52:58.079813  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:52:58.079834  8764 net.cpp:2192] conv1b_param_0(0.651) 
I0815 14:52:58.079848  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:52:58.079860  8764 net.cpp:2192] res2a_branch2a_param_0(0.704) 
I0815 14:52:58.079874  8764 net.cpp:2192] res2a_branch2b_param_0(0.664) 
I0815 14:52:58.079885  8764 net.cpp:2192] res3a_branch2a_param_0(0.708) 
I0815 14:52:58.079897  8764 net.cpp:2192] res3a_branch2b_param_0(0.7) 
I0815 14:52:58.079910  8764 net.cpp:2192] res4a_branch2a_param_0(0.709) 
I0815 14:52:58.079923  8764 net.cpp:2192] res4a_branch2b_param_0(0.707) 
I0815 14:52:58.079937  8764 net.cpp:2192] res5a_branch2a_param_0(0.71) 
I0815 14:52:58.079953  8764 net.cpp:2192] res5a_branch2b_param_0(0.709) 
I0815 14:52:58.079967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.66818e+06/2.86678e+06) 0.582
I0815 14:52:58.277683  8794 solver.cpp:409] Finding and applying sparsity: 0.72
I0815 14:53:50.910208  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:53:50.912241  8764 solver.cpp:312] Iteration 91000 (1.39836 iter/s, 71.5122s/100 iter), loss = 1.68311
I0815 14:53:50.912264  8764 solver.cpp:334]     Train net output #0: loss = 1.81641 (* 1 = 1.81641 loss)
I0815 14:53:50.912272  8764 sgd_solver.cpp:136] Iteration 91000, lr = 0.0043125, m = 0.9
I0815 14:54:06.602551  8764 solver.cpp:312] Iteration 91100 (6.37354 iter/s, 15.6899s/100 iter), loss = 1.4389
I0815 14:54:06.602597  8764 solver.cpp:334]     Train net output #0: loss = 1.71615 (* 1 = 1.71615 loss)
I0815 14:54:06.602605  8764 sgd_solver.cpp:136] Iteration 91100, lr = 0.00430625, m = 0.9
I0815 14:54:25.640394  8764 solver.cpp:312] Iteration 91200 (5.25284 iter/s, 19.0373s/100 iter), loss = 1.29427
I0815 14:54:25.640455  8764 solver.cpp:334]     Train net output #0: loss = 1.05526 (* 1 = 1.05526 loss)
I0815 14:54:25.640461  8764 sgd_solver.cpp:136] Iteration 91200, lr = 0.0043, m = 0.9
I0815 14:54:42.470305  8764 solver.cpp:312] Iteration 91300 (5.94197 iter/s, 16.8294s/100 iter), loss = 1.02891
I0815 14:54:42.470335  8764 solver.cpp:334]     Train net output #0: loss = 1.1025 (* 1 = 1.1025 loss)
I0815 14:54:42.470340  8764 sgd_solver.cpp:136] Iteration 91300, lr = 0.00429375, m = 0.9
I0815 14:54:59.841661  8764 solver.cpp:312] Iteration 91400 (5.75676 iter/s, 17.3709s/100 iter), loss = 1.28412
I0815 14:54:59.841744  8764 solver.cpp:334]     Train net output #0: loss = 1.45173 (* 1 = 1.45173 loss)
I0815 14:54:59.841760  8764 sgd_solver.cpp:136] Iteration 91400, lr = 0.0042875, m = 0.9
I0815 14:55:16.444577  8764 solver.cpp:312] Iteration 91500 (6.0232 iter/s, 16.6025s/100 iter), loss = 1.2571
I0815 14:55:16.444648  8764 solver.cpp:334]     Train net output #0: loss = 1.07985 (* 1 = 1.07985 loss)
I0815 14:55:16.444669  8764 sgd_solver.cpp:136] Iteration 91500, lr = 0.00428125, m = 0.9
I0815 14:55:32.497277  8764 solver.cpp:312] Iteration 91600 (6.22965 iter/s, 16.0523s/100 iter), loss = 1.26925
I0815 14:55:32.497390  8764 solver.cpp:334]     Train net output #0: loss = 1.1627 (* 1 = 1.1627 loss)
I0815 14:55:32.497406  8764 sgd_solver.cpp:136] Iteration 91600, lr = 0.004275, m = 0.9
I0815 14:55:50.877060  8764 solver.cpp:312] Iteration 91700 (5.44091 iter/s, 18.3793s/100 iter), loss = 1.12325
I0815 14:55:50.877270  8764 solver.cpp:334]     Train net output #0: loss = 1.11918 (* 1 = 1.11918 loss)
I0815 14:55:50.877377  8764 sgd_solver.cpp:136] Iteration 91700, lr = 0.00426875, m = 0.9
I0815 14:56:12.024325  8764 solver.cpp:312] Iteration 91800 (4.72887 iter/s, 21.1467s/100 iter), loss = 1.53254
I0815 14:56:12.024461  8764 solver.cpp:334]     Train net output #0: loss = 1.35674 (* 1 = 1.35674 loss)
I0815 14:56:12.024492  8764 sgd_solver.cpp:136] Iteration 91800, lr = 0.0042625, m = 0.9
I0815 14:56:28.134866  8764 solver.cpp:312] Iteration 91900 (6.20728 iter/s, 16.1101s/100 iter), loss = 1.31805
I0815 14:56:28.134933  8764 solver.cpp:334]     Train net output #0: loss = 1.50366 (* 1 = 1.50366 loss)
I0815 14:56:28.134951  8764 sgd_solver.cpp:136] Iteration 91900, lr = 0.00425625, m = 0.9
I0815 14:56:44.085495  8764 solver.cpp:363] Sparsity after update:
I0815 14:56:44.092715  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:56:44.092726  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:56:44.092733  8764 net.cpp:2192] conv1b_param_0(0.651) 
I0815 14:56:44.092736  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:56:44.092737  8764 net.cpp:2192] res2a_branch2a_param_0(0.714) 
I0815 14:56:44.092739  8764 net.cpp:2192] res2a_branch2b_param_0(0.668) 
I0815 14:56:44.092741  8764 net.cpp:2192] res3a_branch2a_param_0(0.718) 
I0815 14:56:44.092743  8764 net.cpp:2192] res3a_branch2b_param_0(0.708) 
I0815 14:56:44.092746  8764 net.cpp:2192] res4a_branch2a_param_0(0.72) 
I0815 14:56:44.092747  8764 net.cpp:2192] res4a_branch2b_param_0(0.716) 
I0815 14:56:44.092762  8764 net.cpp:2192] res5a_branch2a_param_0(0.72) 
I0815 14:56:44.092769  8764 net.cpp:2192] res5a_branch2b_param_0(0.72) 
I0815 14:56:44.092773  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.69181e+06/2.86678e+06) 0.59
I0815 14:56:44.092787  8764 solver.cpp:509] Iteration 92000, Testing net (#0)
I0815 14:56:46.722347  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:57:13.761045  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575647
I0815 14:57:13.761070  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812232
I0815 14:57:13.761078  8764 solver.cpp:594]     Test net output #2: loss = 1.84922 (* 1 = 1.84922 loss)
I0815 14:57:13.761098  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.6675s
I0815 14:57:13.924527  8794 solver.cpp:409] Finding and applying sparsity: 0.73
I0815 14:58:43.919446  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:58:43.921542  8764 solver.cpp:312] Iteration 92000 (0.736469 iter/s, 135.783s/100 iter), loss = 1.50542
I0815 14:58:43.921571  8764 solver.cpp:334]     Train net output #0: loss = 1.29436 (* 1 = 1.29436 loss)
I0815 14:58:43.921581  8764 sgd_solver.cpp:136] Iteration 92000, lr = 0.00425, m = 0.9
I0815 14:59:00.991418  8764 solver.cpp:312] Iteration 92100 (5.85844 iter/s, 17.0694s/100 iter), loss = 1.61081
I0815 14:59:00.991480  8764 solver.cpp:334]     Train net output #0: loss = 1.3064 (* 1 = 1.3064 loss)
I0815 14:59:00.991492  8764 sgd_solver.cpp:136] Iteration 92100, lr = 0.00424375, m = 0.9
I0815 14:59:22.086182  8764 solver.cpp:312] Iteration 92200 (4.74064 iter/s, 21.0942s/100 iter), loss = 1.36812
I0815 14:59:22.086272  8764 solver.cpp:334]     Train net output #0: loss = 1.31836 (* 1 = 1.31836 loss)
I0815 14:59:22.086285  8764 sgd_solver.cpp:136] Iteration 92200, lr = 0.0042375, m = 0.9
I0815 14:59:42.399063  8764 solver.cpp:312] Iteration 92300 (4.92314 iter/s, 20.3122s/100 iter), loss = 1.78761
I0815 14:59:42.399364  8764 solver.cpp:334]     Train net output #0: loss = 1.79527 (* 1 = 1.79527 loss)
I0815 14:59:42.399493  8764 sgd_solver.cpp:136] Iteration 92300, lr = 0.00423125, m = 0.9
I0815 15:00:02.483204  8764 solver.cpp:312] Iteration 92400 (4.97918 iter/s, 20.0836s/100 iter), loss = 1.17203
I0815 15:00:02.483317  8764 solver.cpp:334]     Train net output #0: loss = 1.04947 (* 1 = 1.04947 loss)
I0815 15:00:02.483335  8764 sgd_solver.cpp:136] Iteration 92400, lr = 0.004225, m = 0.9
I0815 15:00:20.604173  8764 solver.cpp:312] Iteration 92500 (5.51862 iter/s, 18.1205s/100 iter), loss = 1.09313
I0815 15:00:20.604218  8764 solver.cpp:334]     Train net output #0: loss = 1.16514 (* 1 = 1.16514 loss)
I0815 15:00:20.604228  8764 sgd_solver.cpp:136] Iteration 92500, lr = 0.00421875, m = 0.9
I0815 15:00:42.773773  8764 solver.cpp:312] Iteration 92600 (4.51081 iter/s, 22.169s/100 iter), loss = 1.46807
I0815 15:00:42.779340  8764 solver.cpp:334]     Train net output #0: loss = 1.73134 (* 1 = 1.73134 loss)
I0815 15:00:42.779368  8764 sgd_solver.cpp:136] Iteration 92600, lr = 0.0042125, m = 0.9
I0815 15:01:03.994838  8764 solver.cpp:312] Iteration 92700 (4.71243 iter/s, 21.2205s/100 iter), loss = 1.32071
I0815 15:01:03.994880  8764 solver.cpp:334]     Train net output #0: loss = 0.73759 (* 1 = 0.73759 loss)
I0815 15:01:03.994889  8764 sgd_solver.cpp:136] Iteration 92700, lr = 0.00420625, m = 0.9
I0815 15:01:24.794260  8764 solver.cpp:312] Iteration 92800 (4.80796 iter/s, 20.7988s/100 iter), loss = 1.93022
I0815 15:01:24.794371  8764 solver.cpp:334]     Train net output #0: loss = 2.3832 (* 1 = 2.3832 loss)
I0815 15:01:24.794386  8764 sgd_solver.cpp:136] Iteration 92800, lr = 0.0042, m = 0.9
I0815 15:01:44.753849  8764 solver.cpp:312] Iteration 92900 (5.01026 iter/s, 19.959s/100 iter), loss = 1.28271
I0815 15:01:44.753944  8764 solver.cpp:334]     Train net output #0: loss = 1.19581 (* 1 = 1.19581 loss)
I0815 15:01:44.753975  8764 sgd_solver.cpp:136] Iteration 92900, lr = 0.00419375, m = 0.9
I0815 15:02:05.950651  8764 solver.cpp:363] Sparsity after update:
I0815 15:02:05.961494  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:02:05.961511  8764 net.cpp:2192] conv1a_param_0(0.357) 
I0815 15:02:05.961519  8764 net.cpp:2192] conv1b_param_0(0.656) 
I0815 15:02:05.961522  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:02:05.961526  8764 net.cpp:2192] res2a_branch2a_param_0(0.723) 
I0815 15:02:05.961529  8764 net.cpp:2192] res2a_branch2b_param_0(0.673) 
I0815 15:02:05.961534  8764 net.cpp:2192] res3a_branch2a_param_0(0.729) 
I0815 15:02:05.961537  8764 net.cpp:2192] res3a_branch2b_param_0(0.716) 
I0815 15:02:05.961540  8764 net.cpp:2192] res4a_branch2a_param_0(0.729) 
I0815 15:02:05.961546  8764 net.cpp:2192] res4a_branch2b_param_0(0.725) 
I0815 15:02:05.961550  8764 net.cpp:2192] res5a_branch2a_param_0(0.73) 
I0815 15:02:05.961558  8764 net.cpp:2192] res5a_branch2b_param_0(0.729) 
I0815 15:02:05.961562  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.71463e+06/2.86678e+06) 0.598
I0815 15:02:06.098601  8794 solver.cpp:409] Finding and applying sparsity: 0.74
I0815 15:03:40.515523  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:03:40.517537  8764 solver.cpp:312] Iteration 93000 (0.863852 iter/s, 115.761s/100 iter), loss = 1.42549
I0815 15:03:40.517562  8764 solver.cpp:334]     Train net output #0: loss = 1.68914 (* 1 = 1.68914 loss)
I0815 15:03:40.517571  8764 sgd_solver.cpp:136] Iteration 93000, lr = 0.0041875, m = 0.9
I0815 15:03:59.112527  8764 solver.cpp:312] Iteration 93100 (5.37794 iter/s, 18.5945s/100 iter), loss = 1.26989
I0815 15:03:59.112553  8764 solver.cpp:334]     Train net output #0: loss = 1.51034 (* 1 = 1.51034 loss)
I0815 15:03:59.112557  8764 sgd_solver.cpp:136] Iteration 93100, lr = 0.00418125, m = 0.9
I0815 15:04:20.557979  8764 solver.cpp:312] Iteration 93200 (4.66312 iter/s, 21.4449s/100 iter), loss = 1.26564
I0815 15:04:20.558037  8764 solver.cpp:334]     Train net output #0: loss = 1.33966 (* 1 = 1.33966 loss)
I0815 15:04:20.558044  8764 sgd_solver.cpp:136] Iteration 93200, lr = 0.004175, m = 0.9
I0815 15:04:41.064877  8764 solver.cpp:312] Iteration 93300 (4.87654 iter/s, 20.5063s/100 iter), loss = 1.28937
I0815 15:04:41.064905  8764 solver.cpp:334]     Train net output #0: loss = 1.15885 (* 1 = 1.15885 loss)
I0815 15:04:41.064911  8764 sgd_solver.cpp:136] Iteration 93300, lr = 0.00416875, m = 0.9
I0815 15:04:59.800959  8764 solver.cpp:312] Iteration 93400 (5.33745 iter/s, 18.7355s/100 iter), loss = 1.74997
I0815 15:04:59.801064  8764 solver.cpp:334]     Train net output #0: loss = 1.90497 (* 1 = 1.90497 loss)
I0815 15:04:59.801080  8764 sgd_solver.cpp:136] Iteration 93400, lr = 0.0041625, m = 0.9
I0815 15:05:20.215062  8764 solver.cpp:312] Iteration 93500 (4.89871 iter/s, 20.4135s/100 iter), loss = 1.41445
I0815 15:05:20.215155  8764 solver.cpp:334]     Train net output #0: loss = 1.50016 (* 1 = 1.50016 loss)
I0815 15:05:20.215179  8764 sgd_solver.cpp:136] Iteration 93500, lr = 0.00415625, m = 0.9
I0815 15:05:41.187738  8764 solver.cpp:312] Iteration 93600 (4.76824 iter/s, 20.9721s/100 iter), loss = 1.49785
I0815 15:05:41.187825  8764 solver.cpp:334]     Train net output #0: loss = 1.31616 (* 1 = 1.31616 loss)
I0815 15:05:41.187839  8764 sgd_solver.cpp:136] Iteration 93600, lr = 0.00415, m = 0.9
I0815 15:05:59.292770  8764 solver.cpp:312] Iteration 93700 (5.52348 iter/s, 18.1045s/100 iter), loss = 1.20637
I0815 15:05:59.320186  8764 solver.cpp:334]     Train net output #0: loss = 1.4168 (* 1 = 1.4168 loss)
I0815 15:05:59.320312  8764 sgd_solver.cpp:136] Iteration 93700, lr = 0.00414375, m = 0.9
I0815 15:06:15.992934  8764 solver.cpp:312] Iteration 93800 (5.98813 iter/s, 16.6997s/100 iter), loss = 1.20546
I0815 15:06:15.993034  8764 solver.cpp:334]     Train net output #0: loss = 1.23131 (* 1 = 1.23131 loss)
I0815 15:06:15.993050  8764 sgd_solver.cpp:136] Iteration 93800, lr = 0.0041375, m = 0.9
I0815 15:06:38.433292  8764 solver.cpp:312] Iteration 93900 (4.45638 iter/s, 22.4397s/100 iter), loss = 1.28
I0815 15:06:38.433318  8764 solver.cpp:334]     Train net output #0: loss = 1.31703 (* 1 = 1.31703 loss)
I0815 15:06:38.433324  8764 sgd_solver.cpp:136] Iteration 93900, lr = 0.00413125, m = 0.9
I0815 15:06:56.397475  8764 solver.cpp:363] Sparsity after update:
I0815 15:06:56.398926  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:06:56.398938  8764 net.cpp:2192] conv1a_param_0(0.357) 
I0815 15:06:56.398947  8764 net.cpp:2192] conv1b_param_0(0.662) 
I0815 15:06:56.398952  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:06:56.398954  8764 net.cpp:2192] res2a_branch2a_param_0(0.732) 
I0815 15:06:56.398958  8764 net.cpp:2192] res2a_branch2b_param_0(0.677) 
I0815 15:06:56.398962  8764 net.cpp:2192] res3a_branch2a_param_0(0.738) 
I0815 15:06:56.398965  8764 net.cpp:2192] res3a_branch2b_param_0(0.724) 
I0815 15:06:56.398968  8764 net.cpp:2192] res4a_branch2a_param_0(0.74) 
I0815 15:06:56.398972  8764 net.cpp:2192] res4a_branch2b_param_0(0.734) 
I0815 15:06:56.398975  8764 net.cpp:2192] res5a_branch2a_param_0(0.74) 
I0815 15:06:56.398978  8764 net.cpp:2192] res5a_branch2b_param_0(0.74) 
I0815 15:06:56.398983  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.73816e+06/2.86678e+06) 0.606
I0815 15:06:56.398993  8764 solver.cpp:509] Iteration 94000, Testing net (#0)
I0815 15:07:24.161453  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576058
I0815 15:07:24.161487  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.808174
I0815 15:07:24.161497  8764 solver.cpp:594]     Test net output #2: loss = 1.84861 (* 1 = 1.84861 loss)
I0815 15:07:24.161530  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.7618s
I0815 15:07:24.395162  8794 solver.cpp:409] Finding and applying sparsity: 0.75
I0815 15:08:53.455729  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:08:53.457746  8764 solver.cpp:312] Iteration 94000 (0.740627 iter/s, 135.021s/100 iter), loss = 1.50801
I0815 15:08:53.457764  8764 solver.cpp:334]     Train net output #0: loss = 1.85236 (* 1 = 1.85236 loss)
I0815 15:08:53.457772  8764 sgd_solver.cpp:136] Iteration 94000, lr = 0.004125, m = 0.9
I0815 15:09:11.730159  8764 solver.cpp:312] Iteration 94100 (5.47289 iter/s, 18.2719s/100 iter), loss = 1.52659
I0815 15:09:11.730208  8764 solver.cpp:334]     Train net output #0: loss = 1.34791 (* 1 = 1.34791 loss)
I0815 15:09:11.730219  8764 sgd_solver.cpp:136] Iteration 94100, lr = 0.00411875, m = 0.9
I0815 15:09:29.238188  8764 solver.cpp:312] Iteration 94200 (5.71182 iter/s, 17.5075s/100 iter), loss = 1.80044
I0815 15:09:29.238302  8764 solver.cpp:334]     Train net output #0: loss = 1.70249 (* 1 = 1.70249 loss)
I0815 15:09:29.238322  8764 sgd_solver.cpp:136] Iteration 94200, lr = 0.0041125, m = 0.9
I0815 15:09:46.300448  8764 solver.cpp:312] Iteration 94300 (5.86105 iter/s, 17.0618s/100 iter), loss = 1.36339
I0815 15:09:46.300474  8764 solver.cpp:334]     Train net output #0: loss = 1.69927 (* 1 = 1.69927 loss)
I0815 15:09:46.300478  8764 sgd_solver.cpp:136] Iteration 94300, lr = 0.00410625, m = 0.9
I0815 15:10:04.608036  8764 solver.cpp:312] Iteration 94400 (5.46237 iter/s, 18.3071s/100 iter), loss = 1.70226
I0815 15:10:04.608319  8764 solver.cpp:334]     Train net output #0: loss = 1.93784 (* 1 = 1.93784 loss)
I0815 15:10:04.608434  8764 sgd_solver.cpp:136] Iteration 94400, lr = 0.0041, m = 0.9
I0815 15:10:22.194685  8764 solver.cpp:312] Iteration 94500 (5.68629 iter/s, 17.5862s/100 iter), loss = 1.78266
I0815 15:10:22.194731  8764 solver.cpp:334]     Train net output #0: loss = 1.7468 (* 1 = 1.7468 loss)
I0815 15:10:22.194743  8764 sgd_solver.cpp:136] Iteration 94500, lr = 0.00409375, m = 0.9
I0815 15:10:38.786896  8764 solver.cpp:312] Iteration 94600 (6.02709 iter/s, 16.5918s/100 iter), loss = 0.961031
I0815 15:10:38.786954  8764 solver.cpp:334]     Train net output #0: loss = 1.22116 (* 1 = 1.22116 loss)
I0815 15:10:38.786962  8764 sgd_solver.cpp:136] Iteration 94600, lr = 0.0040875, m = 0.9
I0815 15:10:55.313697  8764 solver.cpp:312] Iteration 94700 (6.05094 iter/s, 16.5263s/100 iter), loss = 1.73321
I0815 15:10:55.313724  8764 solver.cpp:334]     Train net output #0: loss = 1.81741 (* 1 = 1.81741 loss)
I0815 15:10:55.313730  8764 sgd_solver.cpp:136] Iteration 94700, lr = 0.00408125, m = 0.9
I0815 15:11:13.353451  8764 solver.cpp:312] Iteration 94800 (5.54347 iter/s, 18.0393s/100 iter), loss = 1.13181
I0815 15:11:13.353514  8764 solver.cpp:334]     Train net output #0: loss = 1.03672 (* 1 = 1.03672 loss)
I0815 15:11:13.353521  8764 sgd_solver.cpp:136] Iteration 94800, lr = 0.004075, m = 0.9
I0815 15:11:14.173437  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:11:32.083815  8764 solver.cpp:312] Iteration 94900 (5.33907 iter/s, 18.7298s/100 iter), loss = 1.27376
I0815 15:11:32.083839  8764 solver.cpp:334]     Train net output #0: loss = 1.29663 (* 1 = 1.29663 loss)
I0815 15:11:32.083845  8764 sgd_solver.cpp:136] Iteration 94900, lr = 0.00406875, m = 0.9
I0815 15:11:51.562678  8764 solver.cpp:363] Sparsity after update:
I0815 15:11:51.575052  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:11:51.575090  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:11:51.575109  8764 net.cpp:2192] conv1b_param_0(0.663) 
I0815 15:11:51.575121  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:11:51.575134  8764 net.cpp:2192] res2a_branch2a_param_0(0.738) 
I0815 15:11:51.575145  8764 net.cpp:2192] res2a_branch2b_param_0(0.68) 
I0815 15:11:51.575157  8764 net.cpp:2192] res3a_branch2a_param_0(0.747) 
I0815 15:11:51.575168  8764 net.cpp:2192] res3a_branch2b_param_0(0.729) 
I0815 15:11:51.575181  8764 net.cpp:2192] res4a_branch2a_param_0(0.749) 
I0815 15:11:51.575191  8764 net.cpp:2192] res4a_branch2b_param_0(0.741) 
I0815 15:11:51.575202  8764 net.cpp:2192] res5a_branch2a_param_0(0.75) 
I0815 15:11:51.575214  8764 net.cpp:2192] res5a_branch2b_param_0(0.749) 
I0815 15:11:51.575225  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.76036e+06/2.86678e+06) 0.614
I0815 15:11:51.705307  8794 solver.cpp:409] Finding and applying sparsity: 0.76
I0815 15:13:20.845085  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:13:20.847239  8764 solver.cpp:312] Iteration 95000 (0.919452 iter/s, 108.76s/100 iter), loss = 1.49047
I0815 15:13:20.847256  8764 solver.cpp:334]     Train net output #0: loss = 1.50446 (* 1 = 1.50446 loss)
I0815 15:13:20.847261  8764 sgd_solver.cpp:136] Iteration 95000, lr = 0.0040625, m = 0.9
I0815 15:13:38.631568  8764 solver.cpp:312] Iteration 95100 (5.62309 iter/s, 17.7838s/100 iter), loss = 1.35874
I0815 15:13:38.631597  8764 solver.cpp:334]     Train net output #0: loss = 1.35765 (* 1 = 1.35765 loss)
I0815 15:13:38.631603  8764 sgd_solver.cpp:136] Iteration 95100, lr = 0.00405625, m = 0.9
I0815 15:13:57.010221  8764 solver.cpp:312] Iteration 95200 (5.44125 iter/s, 18.3781s/100 iter), loss = 1.35214
I0815 15:13:57.010265  8764 solver.cpp:334]     Train net output #0: loss = 1.49137 (* 1 = 1.49137 loss)
I0815 15:13:57.010270  8764 sgd_solver.cpp:136] Iteration 95200, lr = 0.00405, m = 0.9
I0815 15:14:14.239728  8764 solver.cpp:312] Iteration 95300 (5.80416 iter/s, 17.229s/100 iter), loss = 1.24904
I0815 15:14:14.239951  8764 solver.cpp:334]     Train net output #0: loss = 1.28033 (* 1 = 1.28033 loss)
I0815 15:14:14.240058  8764 sgd_solver.cpp:136] Iteration 95300, lr = 0.00404375, m = 0.9
I0815 15:14:31.059710  8764 solver.cpp:312] Iteration 95400 (5.94548 iter/s, 16.8195s/100 iter), loss = 1.39141
I0815 15:14:31.059767  8764 solver.cpp:334]     Train net output #0: loss = 1.35 (* 1 = 1.35 loss)
I0815 15:14:31.059773  8764 sgd_solver.cpp:136] Iteration 95400, lr = 0.0040375, m = 0.9
I0815 15:14:48.572883  8764 solver.cpp:312] Iteration 95500 (5.71014 iter/s, 17.5127s/100 iter), loss = 1.66396
I0815 15:14:48.572906  8764 solver.cpp:334]     Train net output #0: loss = 1.43773 (* 1 = 1.43773 loss)
I0815 15:14:48.572911  8764 sgd_solver.cpp:136] Iteration 95500, lr = 0.00403125, m = 0.9
I0815 15:15:06.080777  8764 solver.cpp:312] Iteration 95600 (5.71187 iter/s, 17.5074s/100 iter), loss = 1.26276
I0815 15:15:06.080858  8764 solver.cpp:334]     Train net output #0: loss = 1.2878 (* 1 = 1.2878 loss)
I0815 15:15:06.080955  8764 sgd_solver.cpp:136] Iteration 95600, lr = 0.004025, m = 0.9
I0815 15:15:24.414449  8764 solver.cpp:312] Iteration 95700 (5.4546 iter/s, 18.3332s/100 iter), loss = 1.54495
I0815 15:15:24.414512  8764 solver.cpp:334]     Train net output #0: loss = 1.51319 (* 1 = 1.51319 loss)
I0815 15:15:24.414531  8764 sgd_solver.cpp:136] Iteration 95700, lr = 0.00401875, m = 0.9
I0815 15:15:42.651855  8764 solver.cpp:312] Iteration 95800 (5.48339 iter/s, 18.2369s/100 iter), loss = 1.36663
I0815 15:15:42.651932  8764 solver.cpp:334]     Train net output #0: loss = 1.41798 (* 1 = 1.41798 loss)
I0815 15:15:42.651950  8764 sgd_solver.cpp:136] Iteration 95800, lr = 0.0040125, m = 0.9
I0815 15:16:05.404757  8764 solver.cpp:312] Iteration 95900 (4.39517 iter/s, 22.7523s/100 iter), loss = 1.32898
I0815 15:16:05.405021  8764 solver.cpp:334]     Train net output #0: loss = 1.21221 (* 1 = 1.21221 loss)
I0815 15:16:05.405154  8764 sgd_solver.cpp:136] Iteration 95900, lr = 0.00400625, m = 0.9
I0815 15:16:25.664582  8764 solver.cpp:363] Sparsity after update:
I0815 15:16:25.668666  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:16:25.668701  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:16:25.668715  8764 net.cpp:2192] conv1b_param_0(0.667) 
I0815 15:16:25.668725  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:16:25.668732  8764 net.cpp:2192] res2a_branch2a_param_0(0.747) 
I0815 15:16:25.668740  8764 net.cpp:2192] res2a_branch2b_param_0(0.685) 
I0815 15:16:25.668747  8764 net.cpp:2192] res3a_branch2a_param_0(0.756) 
I0815 15:16:25.668756  8764 net.cpp:2192] res3a_branch2b_param_0(0.736) 
I0815 15:16:25.668762  8764 net.cpp:2192] res4a_branch2a_param_0(0.759) 
I0815 15:16:25.668771  8764 net.cpp:2192] res4a_branch2b_param_0(0.749) 
I0815 15:16:25.668777  8764 net.cpp:2192] res5a_branch2a_param_0(0.76) 
I0815 15:16:25.668786  8764 net.cpp:2192] res5a_branch2b_param_0(0.759) 
I0815 15:16:25.668792  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.78413e+06/2.86678e+06) 0.622
I0815 15:16:25.668809  8764 solver.cpp:509] Iteration 96000, Testing net (#0)
I0815 15:16:55.149899  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583941
I0815 15:16:55.149991  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.813703
I0815 15:16:55.150035  8764 solver.cpp:594]     Test net output #2: loss = 1.84159 (* 1 = 1.84159 loss)
I0815 15:16:55.150163  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.4805s
I0815 15:16:55.452987  8794 solver.cpp:409] Finding and applying sparsity: 0.77
I0815 15:17:56.326067  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:17:56.328094  8764 solver.cpp:312] Iteration 96000 (0.901548 iter/s, 110.92s/100 iter), loss = 1.5161
I0815 15:17:56.328116  8764 solver.cpp:334]     Train net output #0: loss = 1.57328 (* 1 = 1.57328 loss)
I0815 15:17:56.328124  8764 sgd_solver.cpp:136] Iteration 96000, lr = 0.004, m = 0.9
I0815 15:18:15.286288  8764 solver.cpp:312] Iteration 96100 (5.27491 iter/s, 18.9577s/100 iter), loss = 1.5573
I0815 15:18:15.286319  8764 solver.cpp:334]     Train net output #0: loss = 1.52354 (* 1 = 1.52354 loss)
I0815 15:18:15.286324  8764 sgd_solver.cpp:136] Iteration 96100, lr = 0.00399375, m = 0.9
I0815 15:18:35.821398  8764 solver.cpp:312] Iteration 96200 (4.86985 iter/s, 20.5345s/100 iter), loss = 1.59131
I0815 15:18:35.821538  8764 solver.cpp:334]     Train net output #0: loss = 1.23457 (* 1 = 1.23457 loss)
I0815 15:18:35.821563  8764 sgd_solver.cpp:136] Iteration 96200, lr = 0.0039875, m = 0.9
I0815 15:18:54.011181  8764 solver.cpp:312] Iteration 96300 (5.49775 iter/s, 18.1893s/100 iter), loss = 1.39646
I0815 15:18:54.011423  8764 solver.cpp:334]     Train net output #0: loss = 1.46119 (* 1 = 1.46119 loss)
I0815 15:18:54.011543  8764 sgd_solver.cpp:136] Iteration 96300, lr = 0.00398125, m = 0.9
I0815 15:19:12.656100  8764 solver.cpp:312] Iteration 96400 (5.36354 iter/s, 18.6444s/100 iter), loss = 1.26525
I0815 15:19:12.656162  8764 solver.cpp:334]     Train net output #0: loss = 1.23114 (* 1 = 1.23114 loss)
I0815 15:19:12.656169  8764 sgd_solver.cpp:136] Iteration 96400, lr = 0.003975, m = 0.9
I0815 15:19:30.434979  8764 solver.cpp:312] Iteration 96500 (5.62481 iter/s, 17.7784s/100 iter), loss = 1.41452
I0815 15:19:30.435005  8764 solver.cpp:334]     Train net output #0: loss = 1.61121 (* 1 = 1.61121 loss)
I0815 15:19:30.435009  8764 sgd_solver.cpp:136] Iteration 96500, lr = 0.00396875, m = 0.9
I0815 15:19:47.989297  8764 solver.cpp:312] Iteration 96600 (5.69676 iter/s, 17.5538s/100 iter), loss = 1.00598
I0815 15:19:47.995628  8764 solver.cpp:334]     Train net output #0: loss = 0.976773 (* 1 = 0.976773 loss)
I0815 15:19:47.995649  8764 sgd_solver.cpp:136] Iteration 96600, lr = 0.0039625, m = 0.9
I0815 15:20:06.263543  8764 solver.cpp:312] Iteration 96700 (5.47233 iter/s, 18.2737s/100 iter), loss = 1.38291
I0815 15:20:06.263573  8764 solver.cpp:334]     Train net output #0: loss = 1.37435 (* 1 = 1.37435 loss)
I0815 15:20:06.263579  8764 sgd_solver.cpp:136] Iteration 96700, lr = 0.00395625, m = 0.9
I0815 15:20:23.587369  8764 solver.cpp:312] Iteration 96800 (5.77256 iter/s, 17.3233s/100 iter), loss = 1.42991
I0815 15:20:23.587458  8764 solver.cpp:334]     Train net output #0: loss = 1.04492 (* 1 = 1.04492 loss)
I0815 15:20:23.587467  8764 sgd_solver.cpp:136] Iteration 96800, lr = 0.00395, m = 0.9
I0815 15:20:43.058655  8764 solver.cpp:312] Iteration 96900 (5.13591 iter/s, 19.4708s/100 iter), loss = 1.22658
I0815 15:20:43.058862  8764 solver.cpp:334]     Train net output #0: loss = 1.21035 (* 1 = 1.21035 loss)
I0815 15:20:43.058970  8764 sgd_solver.cpp:136] Iteration 96900, lr = 0.00394375, m = 0.9
I0815 15:21:01.001579  8764 solver.cpp:363] Sparsity after update:
I0815 15:21:01.012076  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:21:01.012090  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:21:01.012099  8764 net.cpp:2192] conv1b_param_0(0.67) 
I0815 15:21:01.012102  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:21:01.012109  8764 net.cpp:2192] res2a_branch2a_param_0(0.756) 
I0815 15:21:01.012114  8764 net.cpp:2192] res2a_branch2b_param_0(0.687) 
I0815 15:21:01.012118  8764 net.cpp:2192] res3a_branch2a_param_0(0.766) 
I0815 15:21:01.012123  8764 net.cpp:2192] res3a_branch2b_param_0(0.742) 
I0815 15:21:01.012126  8764 net.cpp:2192] res4a_branch2a_param_0(0.769) 
I0815 15:21:01.012136  8764 net.cpp:2192] res4a_branch2b_param_0(0.757) 
I0815 15:21:01.012140  8764 net.cpp:2192] res5a_branch2a_param_0(0.77) 
I0815 15:21:01.012145  8764 net.cpp:2192] res5a_branch2b_param_0(0.77) 
I0815 15:21:01.012148  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.80728e+06/2.86678e+06) 0.63
I0815 15:21:01.140736  8794 solver.cpp:409] Finding and applying sparsity: 0.78
I0815 15:22:47.167987  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:22:47.170308  8764 solver.cpp:312] Iteration 97000 (0.805748 iter/s, 124.108s/100 iter), loss = 1.62172
I0815 15:22:47.170352  8764 solver.cpp:334]     Train net output #0: loss = 1.64237 (* 1 = 1.64237 loss)
I0815 15:22:47.170370  8764 sgd_solver.cpp:136] Iteration 97000, lr = 0.0039375, m = 0.9
I0815 15:23:07.572432  8764 solver.cpp:312] Iteration 97100 (4.90159 iter/s, 20.4015s/100 iter), loss = 1.78638
I0815 15:23:07.572520  8764 solver.cpp:334]     Train net output #0: loss = 1.38922 (* 1 = 1.38922 loss)
I0815 15:23:07.572546  8764 sgd_solver.cpp:136] Iteration 97100, lr = 0.00393125, m = 0.9
I0815 15:23:26.112995  8764 solver.cpp:312] Iteration 97200 (5.39373 iter/s, 18.5401s/100 iter), loss = 1.38585
I0815 15:23:26.113054  8764 solver.cpp:334]     Train net output #0: loss = 1.33909 (* 1 = 1.33909 loss)
I0815 15:23:26.113060  8764 sgd_solver.cpp:136] Iteration 97200, lr = 0.003925, m = 0.9
I0815 15:23:44.853570  8764 solver.cpp:312] Iteration 97300 (5.33616 iter/s, 18.7401s/100 iter), loss = 1.5266
I0815 15:23:44.853624  8764 solver.cpp:334]     Train net output #0: loss = 1.29738 (* 1 = 1.29738 loss)
I0815 15:23:44.853637  8764 sgd_solver.cpp:136] Iteration 97300, lr = 0.00391875, m = 0.9
I0815 15:24:01.112861  8764 solver.cpp:312] Iteration 97400 (6.1505 iter/s, 16.2588s/100 iter), loss = 1.65497
I0815 15:24:01.112936  8764 solver.cpp:334]     Train net output #0: loss = 1.01399 (* 1 = 1.01399 loss)
I0815 15:24:01.112951  8764 sgd_solver.cpp:136] Iteration 97400, lr = 0.0039125, m = 0.9
I0815 15:24:23.493810  8764 solver.cpp:312] Iteration 97500 (4.46821 iter/s, 22.3803s/100 iter), loss = 1.14017
I0815 15:24:23.493851  8764 solver.cpp:334]     Train net output #0: loss = 1.0051 (* 1 = 1.0051 loss)
I0815 15:24:23.493860  8764 sgd_solver.cpp:136] Iteration 97500, lr = 0.00390625, m = 0.9
I0815 15:24:42.541182  8764 solver.cpp:312] Iteration 97600 (5.25021 iter/s, 19.0468s/100 iter), loss = 1.23418
I0815 15:24:42.541275  8764 solver.cpp:334]     Train net output #0: loss = 1.24007 (* 1 = 1.24007 loss)
I0815 15:24:42.541296  8764 sgd_solver.cpp:136] Iteration 97600, lr = 0.0039, m = 0.9
I0815 15:24:57.591622  8764 solver.cpp:312] Iteration 97700 (6.64451 iter/s, 15.05s/100 iter), loss = 1.67353
I0815 15:24:57.591648  8764 solver.cpp:334]     Train net output #0: loss = 1.44158 (* 1 = 1.44158 loss)
I0815 15:24:57.591653  8764 sgd_solver.cpp:136] Iteration 97700, lr = 0.00389375, m = 0.9
I0815 15:25:16.789162  8764 solver.cpp:312] Iteration 97800 (5.20914 iter/s, 19.197s/100 iter), loss = 1.30634
I0815 15:25:16.789225  8764 solver.cpp:334]     Train net output #0: loss = 0.688109 (* 1 = 0.688109 loss)
I0815 15:25:16.789232  8764 sgd_solver.cpp:136] Iteration 97800, lr = 0.0038875, m = 0.9
I0815 15:25:34.934655  8764 solver.cpp:312] Iteration 97900 (5.51116 iter/s, 18.145s/100 iter), loss = 1.64101
I0815 15:25:34.934861  8764 solver.cpp:334]     Train net output #0: loss = 1.90743 (* 1 = 1.90743 loss)
I0815 15:25:34.934973  8764 sgd_solver.cpp:136] Iteration 97900, lr = 0.00388125, m = 0.9
I0815 15:25:54.690176  8764 solver.cpp:363] Sparsity after update:
I0815 15:25:54.694886  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:25:54.694957  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:25:54.694993  8764 net.cpp:2192] conv1b_param_0(0.673) 
I0815 15:25:54.695017  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:25:54.695045  8764 net.cpp:2192] res2a_branch2a_param_0(0.764) 
I0815 15:25:54.695071  8764 net.cpp:2192] res2a_branch2b_param_0(0.691) 
I0815 15:25:54.695094  8764 net.cpp:2192] res3a_branch2a_param_0(0.776) 
I0815 15:25:54.695116  8764 net.cpp:2192] res3a_branch2b_param_0(0.748) 
I0815 15:25:54.695137  8764 net.cpp:2192] res4a_branch2a_param_0(0.779) 
I0815 15:25:54.695158  8764 net.cpp:2192] res4a_branch2b_param_0(0.764) 
I0815 15:25:54.695178  8764 net.cpp:2192] res5a_branch2a_param_0(0.78) 
I0815 15:25:54.695197  8764 net.cpp:2192] res5a_branch2b_param_0(0.779) 
I0815 15:25:54.695217  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.82954e+06/2.86678e+06) 0.638
I0815 15:25:54.695261  8764 solver.cpp:509] Iteration 98000, Testing net (#0)
I0815 15:26:23.604164  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:26:29.667541  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.57947
I0815 15:26:29.667651  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810998
I0815 15:26:29.667661  8764 solver.cpp:594]     Test net output #2: loss = 1.82851 (* 1 = 1.82851 loss)
I0815 15:26:29.667681  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.9715s
I0815 15:26:29.809829  8794 solver.cpp:409] Finding and applying sparsity: 0.79
I0815 15:28:13.664402  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:28:13.666893  8764 solver.cpp:312] Iteration 98000 (0.630009 iter/s, 158.728s/100 iter), loss = 1.49494
I0815 15:28:13.666929  8764 solver.cpp:334]     Train net output #0: loss = 1.45985 (* 1 = 1.45985 loss)
I0815 15:28:13.666941  8764 sgd_solver.cpp:136] Iteration 98000, lr = 0.003875, m = 0.9
I0815 15:28:31.035413  8764 solver.cpp:312] Iteration 98100 (5.7577 iter/s, 17.368s/100 iter), loss = 1.22629
I0815 15:28:31.035437  8764 solver.cpp:334]     Train net output #0: loss = 1.03451 (* 1 = 1.03451 loss)
I0815 15:28:31.035444  8764 sgd_solver.cpp:136] Iteration 98100, lr = 0.00386875, m = 0.9
I0815 15:28:49.616538  8764 solver.cpp:312] Iteration 98200 (5.38196 iter/s, 18.5806s/100 iter), loss = 1.4253
I0815 15:28:49.616613  8764 solver.cpp:334]     Train net output #0: loss = 1.45612 (* 1 = 1.45612 loss)
I0815 15:28:49.616626  8764 sgd_solver.cpp:136] Iteration 98200, lr = 0.0038625, m = 0.9
I0815 15:29:08.935221  8764 solver.cpp:312] Iteration 98300 (5.17648 iter/s, 19.3181s/100 iter), loss = 1.46963
I0815 15:29:08.935257  8764 solver.cpp:334]     Train net output #0: loss = 1.37614 (* 1 = 1.37614 loss)
I0815 15:29:08.935266  8764 sgd_solver.cpp:136] Iteration 98300, lr = 0.00385625, m = 0.9
I0815 15:29:29.316766  8764 solver.cpp:312] Iteration 98400 (4.90654 iter/s, 20.381s/100 iter), loss = 1.62255
I0815 15:29:29.316864  8764 solver.cpp:334]     Train net output #0: loss = 1.38108 (* 1 = 1.38108 loss)
I0815 15:29:29.316871  8764 sgd_solver.cpp:136] Iteration 98400, lr = 0.00385, m = 0.9
I0815 15:29:45.441030  8764 solver.cpp:312] Iteration 98500 (6.20203 iter/s, 16.1238s/100 iter), loss = 1.67365
I0815 15:29:45.441120  8764 solver.cpp:334]     Train net output #0: loss = 1.04715 (* 1 = 1.04715 loss)
I0815 15:29:45.441162  8764 sgd_solver.cpp:136] Iteration 98500, lr = 0.00384375, m = 0.9
I0815 15:30:04.826122  8764 solver.cpp:312] Iteration 98600 (5.15874 iter/s, 19.3846s/100 iter), loss = 1.8912
I0815 15:30:04.826179  8764 solver.cpp:334]     Train net output #0: loss = 1.60452 (* 1 = 1.60452 loss)
I0815 15:30:04.826185  8764 sgd_solver.cpp:136] Iteration 98600, lr = 0.0038375, m = 0.9
I0815 15:30:22.385090  8764 solver.cpp:312] Iteration 98700 (5.69526 iter/s, 17.5585s/100 iter), loss = 1.42214
I0815 15:30:22.385308  8764 solver.cpp:334]     Train net output #0: loss = 1.35854 (* 1 = 1.35854 loss)
I0815 15:30:22.385408  8764 sgd_solver.cpp:136] Iteration 98700, lr = 0.00383125, m = 0.9
I0815 15:30:39.798665  8764 solver.cpp:312] Iteration 98800 (5.7428 iter/s, 17.4131s/100 iter), loss = 1.61621
I0815 15:30:39.798743  8764 solver.cpp:334]     Train net output #0: loss = 1.96296 (* 1 = 1.96296 loss)
I0815 15:30:39.798751  8764 sgd_solver.cpp:136] Iteration 98800, lr = 0.003825, m = 0.9
I0815 15:30:57.157068  8764 solver.cpp:312] Iteration 98900 (5.76106 iter/s, 17.3579s/100 iter), loss = 1.41937
I0815 15:30:57.157094  8764 solver.cpp:334]     Train net output #0: loss = 1.34817 (* 1 = 1.34817 loss)
I0815 15:30:57.157099  8764 sgd_solver.cpp:136] Iteration 98900, lr = 0.00381875, m = 0.9
I0815 15:31:15.232698  8764 solver.cpp:363] Sparsity after update:
I0815 15:31:15.244277  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:31:15.244305  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:31:15.244320  8764 net.cpp:2192] conv1b_param_0(0.675) 
I0815 15:31:15.244328  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:31:15.244336  8764 net.cpp:2192] res2a_branch2a_param_0(0.772) 
I0815 15:31:15.244344  8764 net.cpp:2192] res2a_branch2b_param_0(0.693) 
I0815 15:31:15.244352  8764 net.cpp:2192] res3a_branch2a_param_0(0.785) 
I0815 15:31:15.244360  8764 net.cpp:2192] res3a_branch2b_param_0(0.753) 
I0815 15:31:15.244367  8764 net.cpp:2192] res4a_branch2a_param_0(0.788) 
I0815 15:31:15.244375  8764 net.cpp:2192] res4a_branch2b_param_0(0.77) 
I0815 15:31:15.244385  8764 net.cpp:2192] res5a_branch2a_param_0(0.79) 
I0815 15:31:15.244393  8764 net.cpp:2192] res5a_branch2b_param_0(0.789) 
I0815 15:31:15.244402  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.85232e+06/2.86678e+06) 0.646
I0815 15:31:15.398624  8794 solver.cpp:409] Finding and applying sparsity: 0.8
I0815 15:33:02.868700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:33:02.870798  8764 solver.cpp:312] Iteration 99000 (0.79548 iter/s, 125.71s/100 iter), loss = 1.84481
I0815 15:33:02.870818  8764 solver.cpp:334]     Train net output #0: loss = 2.06868 (* 1 = 2.06868 loss)
I0815 15:33:02.870826  8764 sgd_solver.cpp:136] Iteration 99000, lr = 0.0038125, m = 0.9
I0815 15:33:24.284648  8764 solver.cpp:312] Iteration 99100 (4.67001 iter/s, 21.4132s/100 iter), loss = 1.58187
I0815 15:33:24.284700  8764 solver.cpp:334]     Train net output #0: loss = 1.48142 (* 1 = 1.48142 loss)
I0815 15:33:24.284713  8764 sgd_solver.cpp:136] Iteration 99100, lr = 0.00380625, m = 0.9
I0815 15:33:44.096164  8764 solver.cpp:312] Iteration 99200 (5.04771 iter/s, 19.811s/100 iter), loss = 1.75487
I0815 15:33:44.096266  8764 solver.cpp:334]     Train net output #0: loss = 1.71351 (* 1 = 1.71351 loss)
I0815 15:33:44.096287  8764 sgd_solver.cpp:136] Iteration 99200, lr = 0.0038, m = 0.9
I0815 15:34:03.581439  8764 solver.cpp:312] Iteration 99300 (5.13223 iter/s, 19.4847s/100 iter), loss = 1.48841
I0815 15:34:03.581478  8764 solver.cpp:334]     Train net output #0: loss = 1.50102 (* 1 = 1.50102 loss)
I0815 15:34:03.581485  8764 sgd_solver.cpp:136] Iteration 99300, lr = 0.00379375, m = 0.9
I0815 15:34:22.966205  8764 solver.cpp:312] Iteration 99400 (5.15883 iter/s, 19.3842s/100 iter), loss = 1.50794
I0815 15:34:22.966267  8764 solver.cpp:334]     Train net output #0: loss = 1.41021 (* 1 = 1.41021 loss)
I0815 15:34:22.966274  8764 sgd_solver.cpp:136] Iteration 99400, lr = 0.0037875, m = 0.9
I0815 15:34:42.040640  8764 solver.cpp:312] Iteration 99500 (5.24277 iter/s, 19.0739s/100 iter), loss = 1.62552
I0815 15:34:42.040668  8764 solver.cpp:334]     Train net output #0: loss = 1.66402 (* 1 = 1.66402 loss)
I0815 15:34:42.040673  8764 sgd_solver.cpp:136] Iteration 99500, lr = 0.00378125, m = 0.9
I0815 15:35:02.816900  8764 solver.cpp:312] Iteration 99600 (4.81332 iter/s, 20.7757s/100 iter), loss = 1.24257
I0815 15:35:02.817001  8764 solver.cpp:334]     Train net output #0: loss = 1.16879 (* 1 = 1.16879 loss)
I0815 15:35:02.817015  8764 sgd_solver.cpp:136] Iteration 99600, lr = 0.003775, m = 0.9
I0815 15:35:23.869906  8764 solver.cpp:312] Iteration 99700 (4.75005 iter/s, 21.0524s/100 iter), loss = 1.40957
I0815 15:35:23.869971  8764 solver.cpp:334]     Train net output #0: loss = 1.47679 (* 1 = 1.47679 loss)
I0815 15:35:23.869988  8764 sgd_solver.cpp:136] Iteration 99700, lr = 0.00376875, m = 0.9
I0815 15:35:41.284255  8764 solver.cpp:312] Iteration 99800 (5.74256 iter/s, 17.4138s/100 iter), loss = 1.43133
I0815 15:35:41.284421  8764 solver.cpp:334]     Train net output #0: loss = 1.27589 (* 1 = 1.27589 loss)
I0815 15:35:41.284451  8764 sgd_solver.cpp:136] Iteration 99800, lr = 0.0037625, m = 0.9
I0815 15:36:00.169349  8764 solver.cpp:312] Iteration 99900 (5.29533 iter/s, 18.8846s/100 iter), loss = 1.30312
I0815 15:36:00.169397  8764 solver.cpp:334]     Train net output #0: loss = 1.17972 (* 1 = 1.17972 loss)
I0815 15:36:00.169412  8764 sgd_solver.cpp:136] Iteration 99900, lr = 0.00375625, m = 0.9
I0815 15:36:20.801858  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_100000.caffemodel
I0815 15:36:20.877348  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_100000.solverstate
I0815 15:36:20.881842  8764 solver.cpp:363] Sparsity after update:
I0815 15:36:20.885818  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:36:20.885840  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:36:20.885855  8764 net.cpp:2192] conv1b_param_0(0.677) 
I0815 15:36:20.885865  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:36:20.885874  8764 net.cpp:2192] res2a_branch2a_param_0(0.781) 
I0815 15:36:20.885885  8764 net.cpp:2192] res2a_branch2b_param_0(0.697) 
I0815 15:36:20.885895  8764 net.cpp:2192] res3a_branch2a_param_0(0.792) 
I0815 15:36:20.885902  8764 net.cpp:2192] res3a_branch2b_param_0(0.758) 
I0815 15:36:20.885911  8764 net.cpp:2192] res4a_branch2a_param_0(0.797) 
I0815 15:36:20.885921  8764 net.cpp:2192] res4a_branch2b_param_0(0.776) 
I0815 15:36:20.885928  8764 net.cpp:2192] res5a_branch2a_param_0(0.8) 
I0815 15:36:20.885937  8764 net.cpp:2192] res5a_branch2b_param_0(0.799) 
I0815 15:36:20.885946  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.87401e+06/2.86678e+06) 0.654
I0815 15:36:20.885962  8764 solver.cpp:509] Iteration 100000, Testing net (#0)
I0815 15:36:45.656796  8747 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 15:36:46.301048  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.580588
I0815 15:36:46.301070  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.809174
I0815 15:36:46.301077  8764 solver.cpp:594]     Test net output #2: loss = 1.83862 (* 1 = 1.83862 loss)
I0815 15:36:46.301100  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.4144s
I0815 15:36:46.458827  8794 solver.cpp:409] Finding and applying sparsity: 0.81
I0815 15:37:47.070982  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:37:47.073060  8764 solver.cpp:312] Iteration 100000 (0.935447 iter/s, 106.901s/100 iter), loss = 1.14955
I0815 15:37:47.073077  8764 solver.cpp:334]     Train net output #0: loss = 1.28295 (* 1 = 1.28295 loss)
I0815 15:37:47.073082  8764 sgd_solver.cpp:136] Iteration 100000, lr = 0.00375, m = 0.9
I0815 15:38:06.283458  8764 solver.cpp:312] Iteration 100100 (5.20566 iter/s, 19.2099s/100 iter), loss = 2.03431
I0815 15:38:06.283483  8764 solver.cpp:334]     Train net output #0: loss = 1.99049 (* 1 = 1.99049 loss)
I0815 15:38:06.283489  8764 sgd_solver.cpp:136] Iteration 100100, lr = 0.00374375, m = 0.9
I0815 15:38:26.894295  8764 solver.cpp:312] Iteration 100200 (4.85195 iter/s, 20.6103s/100 iter), loss = 1.69256
I0815 15:38:26.894394  8764 solver.cpp:334]     Train net output #0: loss = 1.96718 (* 1 = 1.96718 loss)
I0815 15:38:26.894419  8764 sgd_solver.cpp:136] Iteration 100200, lr = 0.0037375, m = 0.9
I0815 15:38:44.360554  8764 solver.cpp:312] Iteration 100300 (5.72548 iter/s, 17.4658s/100 iter), loss = 1.13287
I0815 15:38:44.360631  8764 solver.cpp:334]     Train net output #0: loss = 1.07133 (* 1 = 1.07133 loss)
I0815 15:38:44.360651  8764 sgd_solver.cpp:136] Iteration 100300, lr = 0.00373125, m = 0.9
I0815 15:39:02.059878  8764 solver.cpp:312] Iteration 100400 (5.6501 iter/s, 17.6988s/100 iter), loss = 1.92705
I0815 15:39:02.060098  8764 solver.cpp:334]     Train net output #0: loss = 2.03329 (* 1 = 2.03329 loss)
I0815 15:39:02.060142  8764 sgd_solver.cpp:136] Iteration 100400, lr = 0.003725, m = 0.9
I0815 15:39:22.593220  8764 solver.cpp:312] Iteration 100500 (4.87028 iter/s, 20.5327s/100 iter), loss = 1.60862
I0815 15:39:22.593334  8764 solver.cpp:334]     Train net output #0: loss = 1.94227 (* 1 = 1.94227 loss)
I0815 15:39:22.593369  8764 sgd_solver.cpp:136] Iteration 100500, lr = 0.00371875, m = 0.9
I0815 15:39:41.181401  8764 solver.cpp:312] Iteration 100600 (5.3799 iter/s, 18.5877s/100 iter), loss = 1.80421
I0815 15:39:41.181493  8764 solver.cpp:334]     Train net output #0: loss = 1.9565 (* 1 = 1.9565 loss)
I0815 15:39:41.181502  8764 sgd_solver.cpp:136] Iteration 100600, lr = 0.0037125, m = 0.9
I0815 15:40:03.465258  8764 solver.cpp:312] Iteration 100700 (4.48768 iter/s, 22.2832s/100 iter), loss = 1.35829
I0815 15:40:03.465299  8764 solver.cpp:334]     Train net output #0: loss = 1.95666 (* 1 = 1.95666 loss)
I0815 15:40:03.465309  8764 sgd_solver.cpp:136] Iteration 100700, lr = 0.00370625, m = 0.9
I0815 15:40:21.480336  8764 solver.cpp:312] Iteration 100800 (5.55106 iter/s, 18.0146s/100 iter), loss = 0.946652
I0815 15:40:21.480396  8764 solver.cpp:334]     Train net output #0: loss = 0.824354 (* 1 = 0.824354 loss)
I0815 15:40:21.480432  8764 sgd_solver.cpp:136] Iteration 100800, lr = 0.0037, m = 0.9
I0815 15:40:39.789916  8764 solver.cpp:312] Iteration 100900 (5.46177 iter/s, 18.3091s/100 iter), loss = 1.42122
I0815 15:40:39.789938  8764 solver.cpp:334]     Train net output #0: loss = 1.62381 (* 1 = 1.62381 loss)
I0815 15:40:39.790043  8764 sgd_solver.cpp:136] Iteration 100900, lr = 0.00369375, m = 0.9
I0815 15:40:59.432695  8764 solver.cpp:363] Sparsity after update:
I0815 15:40:59.441146  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:40:59.441164  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:40:59.441172  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:40:59.441175  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:40:59.441179  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:40:59.441184  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:40:59.441187  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:40:59.441190  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:40:59.441193  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:40:59.441197  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:40:59.441200  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:40:59.441205  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:40:59.441207  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:40:59.774333  8764 solver.cpp:312] Iteration 101000 (5.00404 iter/s, 19.9838s/100 iter), loss = 1.8552
I0815 15:40:59.774413  8764 solver.cpp:334]     Train net output #0: loss = 1.99506 (* 1 = 1.99506 loss)
I0815 15:40:59.774435  8764 sgd_solver.cpp:136] Iteration 101000, lr = 0.0036875, m = 0.9
I0815 15:41:20.365310  8764 solver.cpp:312] Iteration 101100 (4.85663 iter/s, 20.5904s/100 iter), loss = 1.47291
I0815 15:41:20.365336  8764 solver.cpp:334]     Train net output #0: loss = 1.15817 (* 1 = 1.15817 loss)
I0815 15:41:20.365342  8764 sgd_solver.cpp:136] Iteration 101100, lr = 0.00368125, m = 0.9
I0815 15:41:38.547646  8764 solver.cpp:312] Iteration 101200 (5.5 iter/s, 18.1818s/100 iter), loss = 1.26918
I0815 15:41:38.547703  8764 solver.cpp:334]     Train net output #0: loss = 1.26826 (* 1 = 1.26826 loss)
I0815 15:41:38.547708  8764 sgd_solver.cpp:136] Iteration 101200, lr = 0.003675, m = 0.9
I0815 15:41:57.215495  8764 solver.cpp:312] Iteration 101300 (5.35695 iter/s, 18.6673s/100 iter), loss = 1.55624
I0815 15:41:57.215570  8764 solver.cpp:334]     Train net output #0: loss = 1.68862 (* 1 = 1.68862 loss)
I0815 15:41:57.215590  8764 sgd_solver.cpp:136] Iteration 101300, lr = 0.00366875, m = 0.9
I0815 15:42:16.547626  8764 solver.cpp:312] Iteration 101400 (5.17288 iter/s, 19.3316s/100 iter), loss = 1.20204
I0815 15:42:16.547736  8764 solver.cpp:334]     Train net output #0: loss = 1.10286 (* 1 = 1.10286 loss)
I0815 15:42:16.547757  8764 sgd_solver.cpp:136] Iteration 101400, lr = 0.0036625, m = 0.9
I0815 15:42:34.634840  8764 solver.cpp:312] Iteration 101500 (5.52892 iter/s, 18.0867s/100 iter), loss = 1.46288
I0815 15:42:34.635000  8764 solver.cpp:334]     Train net output #0: loss = 1.66568 (* 1 = 1.66568 loss)
I0815 15:42:34.635084  8764 sgd_solver.cpp:136] Iteration 101500, lr = 0.00365625, m = 0.9
I0815 15:42:54.509402  8764 solver.cpp:312] Iteration 101600 (5.03171 iter/s, 19.874s/100 iter), loss = 1.37365
I0815 15:42:54.509676  8764 solver.cpp:334]     Train net output #0: loss = 1.63657 (* 1 = 1.63657 loss)
I0815 15:42:54.509745  8764 sgd_solver.cpp:136] Iteration 101600, lr = 0.00365, m = 0.9
I0815 15:43:13.409581  8764 solver.cpp:312] Iteration 101700 (5.29109 iter/s, 18.8997s/100 iter), loss = 1.35914
I0815 15:43:13.409605  8764 solver.cpp:334]     Train net output #0: loss = 0.893922 (* 1 = 0.893922 loss)
I0815 15:43:13.409610  8764 sgd_solver.cpp:136] Iteration 101700, lr = 0.00364375, m = 0.9
I0815 15:43:31.941485  8764 solver.cpp:312] Iteration 101800 (5.39625 iter/s, 18.5314s/100 iter), loss = 1.64824
I0815 15:43:31.941541  8764 solver.cpp:334]     Train net output #0: loss = 2.23721 (* 1 = 2.23721 loss)
I0815 15:43:31.941550  8764 sgd_solver.cpp:136] Iteration 101800, lr = 0.0036375, m = 0.9
I0815 15:43:49.072962  8764 solver.cpp:312] Iteration 101900 (5.83737 iter/s, 17.131s/100 iter), loss = 1.44591
I0815 15:43:49.072988  8764 solver.cpp:334]     Train net output #0: loss = 1.45907 (* 1 = 1.45907 loss)
I0815 15:43:49.072994  8764 sgd_solver.cpp:136] Iteration 101900, lr = 0.00363125, m = 0.9
I0815 15:44:07.052655  8764 solver.cpp:363] Sparsity after update:
I0815 15:44:07.061389  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:44:07.061413  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:44:07.061419  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:44:07.061424  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:44:07.061427  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:44:07.061434  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:44:07.061437  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:44:07.061441  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:44:07.061444  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:44:07.061449  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:44:07.061451  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:44:07.061455  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:44:07.061458  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:44:07.061470  8764 solver.cpp:509] Iteration 102000, Testing net (#0)
I0815 15:44:16.640926  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:44:42.873944  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571353
I0815 15:44:42.874110  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807998
I0815 15:44:42.874130  8764 solver.cpp:594]     Test net output #2: loss = 1.86307 (* 1 = 1.86307 loss)
I0815 15:44:42.874280  8764 solver.cpp:264] [MultiGPU] Tests completed in 35.8118s
I0815 15:44:43.126494  8764 solver.cpp:312] Iteration 102000 (1.85007 iter/s, 54.0521s/100 iter), loss = 1.93814
I0815 15:44:43.126526  8764 solver.cpp:334]     Train net output #0: loss = 2.47777 (* 1 = 2.47777 loss)
I0815 15:44:43.126533  8764 sgd_solver.cpp:136] Iteration 102000, lr = 0.003625, m = 0.9
I0815 15:44:59.149490  8764 solver.cpp:312] Iteration 102100 (6.2412 iter/s, 16.0226s/100 iter), loss = 1.1357
I0815 15:44:59.149543  8764 solver.cpp:334]     Train net output #0: loss = 1.03683 (* 1 = 1.03683 loss)
I0815 15:44:59.149555  8764 sgd_solver.cpp:136] Iteration 102100, lr = 0.00361875, m = 0.9
I0815 15:45:16.710481  8764 solver.cpp:312] Iteration 102200 (5.6946 iter/s, 17.5605s/100 iter), loss = 1.38303
I0815 15:45:16.710582  8764 solver.cpp:334]     Train net output #0: loss = 1.40306 (* 1 = 1.40306 loss)
I0815 15:45:16.710597  8764 sgd_solver.cpp:136] Iteration 102200, lr = 0.0036125, m = 0.9
I0815 15:45:32.933929  8764 solver.cpp:312] Iteration 102300 (6.16409 iter/s, 16.223s/100 iter), loss = 1.72274
I0815 15:45:32.934106  8764 solver.cpp:334]     Train net output #0: loss = 2.13346 (* 1 = 2.13346 loss)
I0815 15:45:32.934191  8764 sgd_solver.cpp:136] Iteration 102300, lr = 0.00360625, m = 0.9
I0815 15:45:50.844070  8764 solver.cpp:312] Iteration 102400 (5.58358 iter/s, 17.9096s/100 iter), loss = 1.15329
I0815 15:45:50.844136  8764 solver.cpp:334]     Train net output #0: loss = 1.37996 (* 1 = 1.37996 loss)
I0815 15:45:50.844143  8764 sgd_solver.cpp:136] Iteration 102400, lr = 0.0036, m = 0.9
I0815 15:46:07.246655  8764 solver.cpp:312] Iteration 102500 (6.09677 iter/s, 16.4021s/100 iter), loss = 1.10059
I0815 15:46:07.246686  8764 solver.cpp:334]     Train net output #0: loss = 1.04606 (* 1 = 1.04606 loss)
I0815 15:46:07.246693  8764 sgd_solver.cpp:136] Iteration 102500, lr = 0.00359375, m = 0.9
I0815 15:46:23.381911  8764 solver.cpp:312] Iteration 102600 (6.19778 iter/s, 16.1348s/100 iter), loss = 1.18578
I0815 15:46:23.381999  8764 solver.cpp:334]     Train net output #0: loss = 0.967594 (* 1 = 0.967594 loss)
I0815 15:46:23.382011  8764 sgd_solver.cpp:136] Iteration 102600, lr = 0.0035875, m = 0.9
I0815 15:46:42.461120  8764 solver.cpp:312] Iteration 102700 (5.24145 iter/s, 19.0787s/100 iter), loss = 1.50521
I0815 15:46:42.461370  8764 solver.cpp:334]     Train net output #0: loss = 1.54623 (* 1 = 1.54623 loss)
I0815 15:46:42.461486  8764 sgd_solver.cpp:136] Iteration 102700, lr = 0.00358125, m = 0.9
I0815 15:46:59.255754  8764 solver.cpp:312] Iteration 102800 (5.95445 iter/s, 16.7942s/100 iter), loss = 1.71349
I0815 15:46:59.255954  8764 solver.cpp:334]     Train net output #0: loss = 1.30193 (* 1 = 1.30193 loss)
I0815 15:46:59.255967  8764 sgd_solver.cpp:136] Iteration 102800, lr = 0.003575, m = 0.9
I0815 15:47:15.530717  8764 solver.cpp:312] Iteration 102900 (6.14458 iter/s, 16.2745s/100 iter), loss = 1.38444
I0815 15:47:15.530766  8764 solver.cpp:334]     Train net output #0: loss = 1.20229 (* 1 = 1.20229 loss)
I0815 15:47:15.530776  8764 sgd_solver.cpp:136] Iteration 102900, lr = 0.00356875, m = 0.9
I0815 15:47:32.358680  8764 solver.cpp:363] Sparsity after update:
I0815 15:47:32.370265  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:47:32.370307  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:47:32.370327  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:47:32.370339  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:47:32.370352  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:47:32.370368  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:47:32.370381  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:47:32.370393  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:47:32.370404  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:47:32.370416  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:47:32.370430  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:47:32.370443  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:47:32.370455  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:47:32.518007  8764 solver.cpp:312] Iteration 103000 (5.88692 iter/s, 16.9868s/100 iter), loss = 1.47524
I0815 15:47:32.518069  8764 solver.cpp:334]     Train net output #0: loss = 0.976127 (* 1 = 0.976127 loss)
I0815 15:47:32.518086  8764 sgd_solver.cpp:136] Iteration 103000, lr = 0.0035625, m = 0.9
I0815 15:47:49.597251  8764 solver.cpp:312] Iteration 103100 (5.85522 iter/s, 17.0788s/100 iter), loss = 1.19524
I0815 15:47:49.597296  8764 solver.cpp:334]     Train net output #0: loss = 0.979024 (* 1 = 0.979024 loss)
I0815 15:47:49.597309  8764 sgd_solver.cpp:136] Iteration 103100, lr = 0.00355625, m = 0.9
I0815 15:48:06.996870  8764 solver.cpp:312] Iteration 103200 (5.74741 iter/s, 17.3991s/100 iter), loss = 1.41383
I0815 15:48:06.997120  8764 solver.cpp:334]     Train net output #0: loss = 1.42129 (* 1 = 1.42129 loss)
I0815 15:48:06.997231  8764 sgd_solver.cpp:136] Iteration 103200, lr = 0.00355, m = 0.9
I0815 15:48:24.237177  8764 solver.cpp:312] Iteration 103300 (5.80052 iter/s, 17.2398s/100 iter), loss = 1.60673
I0815 15:48:24.237205  8764 solver.cpp:334]     Train net output #0: loss = 1.89872 (* 1 = 1.89872 loss)
I0815 15:48:24.237210  8764 sgd_solver.cpp:136] Iteration 103300, lr = 0.00354375, m = 0.9
I0815 15:48:39.427260  8764 solver.cpp:312] Iteration 103400 (6.58342 iter/s, 15.1897s/100 iter), loss = 1.55347
I0815 15:48:39.427321  8764 solver.cpp:334]     Train net output #0: loss = 1.75798 (* 1 = 1.75798 loss)
I0815 15:48:39.427330  8764 sgd_solver.cpp:136] Iteration 103400, lr = 0.0035375, m = 0.9
I0815 15:48:56.237741  8764 solver.cpp:312] Iteration 103500 (5.94884 iter/s, 16.81s/100 iter), loss = 1.34739
I0815 15:48:56.237828  8764 solver.cpp:334]     Train net output #0: loss = 1.11753 (* 1 = 1.11753 loss)
I0815 15:48:56.237853  8764 sgd_solver.cpp:136] Iteration 103500, lr = 0.00353125, m = 0.9
I0815 15:49:14.273829  8764 solver.cpp:312] Iteration 103600 (5.54459 iter/s, 18.0356s/100 iter), loss = 1.34195
I0815 15:49:14.280164  8764 solver.cpp:334]     Train net output #0: loss = 1.40104 (* 1 = 1.40104 loss)
I0815 15:49:14.280174  8764 sgd_solver.cpp:136] Iteration 103600, lr = 0.003525, m = 0.9
I0815 15:49:32.768304  8764 solver.cpp:312] Iteration 103700 (5.40717 iter/s, 18.494s/100 iter), loss = 1.16567
I0815 15:49:32.768409  8764 solver.cpp:334]     Train net output #0: loss = 1.47474 (* 1 = 1.47474 loss)
I0815 15:49:32.768427  8764 sgd_solver.cpp:136] Iteration 103700, lr = 0.00351875, m = 0.9
I0815 15:49:51.665650  8764 solver.cpp:312] Iteration 103800 (5.29189 iter/s, 18.8968s/100 iter), loss = 1.32617
I0815 15:49:51.665736  8764 solver.cpp:334]     Train net output #0: loss = 1.24121 (* 1 = 1.24121 loss)
I0815 15:49:51.665752  8764 sgd_solver.cpp:136] Iteration 103800, lr = 0.0035125, m = 0.9
I0815 15:50:12.116166  8764 solver.cpp:312] Iteration 103900 (4.88999 iter/s, 20.45s/100 iter), loss = 1.5258
I0815 15:50:12.116195  8764 solver.cpp:334]     Train net output #0: loss = 1.40691 (* 1 = 1.40691 loss)
I0815 15:50:12.116201  8764 sgd_solver.cpp:136] Iteration 103900, lr = 0.00350625, m = 0.9
I0815 15:50:31.972296  8764 solver.cpp:363] Sparsity after update:
I0815 15:50:31.978124  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:50:31.978288  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:50:31.978389  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:50:31.978483  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:50:31.978571  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:50:31.978670  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:50:31.978760  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:50:31.978853  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:50:31.978945  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:50:31.979038  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:50:31.979132  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:50:31.979220  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:50:31.979310  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:50:31.979418  8764 solver.cpp:509] Iteration 104000, Testing net (#0)
I0815 15:51:05.452145  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575647
I0815 15:51:05.452198  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.809645
I0815 15:51:05.452203  8764 solver.cpp:594]     Test net output #2: loss = 1.84575 (* 1 = 1.84575 loss)
I0815 15:51:05.452224  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.4719s
I0815 15:51:05.611678  8764 solver.cpp:312] Iteration 104000 (1.86937 iter/s, 53.494s/100 iter), loss = 1.38233
I0815 15:51:05.611834  8764 solver.cpp:334]     Train net output #0: loss = 1.24265 (* 1 = 1.24265 loss)
I0815 15:51:05.611922  8764 sgd_solver.cpp:136] Iteration 104000, lr = 0.0035, m = 0.9
I0815 15:51:26.564837  8764 solver.cpp:312] Iteration 104100 (4.77268 iter/s, 20.9526s/100 iter), loss = 1.33392
I0815 15:51:26.564865  8764 solver.cpp:334]     Train net output #0: loss = 1.15895 (* 1 = 1.15895 loss)
I0815 15:51:26.564870  8764 sgd_solver.cpp:136] Iteration 104100, lr = 0.00349375, m = 0.9
I0815 15:51:48.548784  8764 solver.cpp:312] Iteration 104200 (4.5489 iter/s, 21.9833s/100 iter), loss = 1.52176
I0815 15:51:48.548912  8764 solver.cpp:334]     Train net output #0: loss = 1.88896 (* 1 = 1.88896 loss)
I0815 15:51:48.548930  8764 sgd_solver.cpp:136] Iteration 104200, lr = 0.0034875, m = 0.9
I0815 15:52:06.783097  8764 solver.cpp:312] Iteration 104300 (5.48432 iter/s, 18.2338s/100 iter), loss = 1.40122
I0815 15:52:06.783139  8764 solver.cpp:334]     Train net output #0: loss = 1.64431 (* 1 = 1.64431 loss)
I0815 15:52:06.783151  8764 sgd_solver.cpp:136] Iteration 104300, lr = 0.00348125, m = 0.9
I0815 15:52:26.731212  8764 solver.cpp:312] Iteration 104400 (5.01314 iter/s, 19.9476s/100 iter), loss = 1.95052
I0815 15:52:26.731315  8764 solver.cpp:334]     Train net output #0: loss = 1.93198 (* 1 = 1.93198 loss)
I0815 15:52:26.731329  8764 sgd_solver.cpp:136] Iteration 104400, lr = 0.003475, m = 0.9
I0815 15:52:50.787714  8764 solver.cpp:312] Iteration 104500 (4.157 iter/s, 24.0558s/100 iter), loss = 1.54016
I0815 15:52:50.801390  8764 solver.cpp:334]     Train net output #0: loss = 1.41076 (* 1 = 1.41076 loss)
I0815 15:52:50.801409  8764 sgd_solver.cpp:136] Iteration 104500, lr = 0.00346875, m = 0.9
I0815 15:53:11.339115  8764 solver.cpp:312] Iteration 104600 (4.86598 iter/s, 20.5508s/100 iter), loss = 1.34923
I0815 15:53:11.339380  8764 solver.cpp:334]     Train net output #0: loss = 1.33608 (* 1 = 1.33608 loss)
I0815 15:53:11.339494  8764 sgd_solver.cpp:136] Iteration 104600, lr = 0.0034625, m = 0.9
I0815 15:53:30.745293  8764 solver.cpp:312] Iteration 104700 (5.15314 iter/s, 19.4056s/100 iter), loss = 1.06672
I0815 15:53:30.745316  8764 solver.cpp:334]     Train net output #0: loss = 1.24763 (* 1 = 1.24763 loss)
I0815 15:53:30.745321  8764 sgd_solver.cpp:136] Iteration 104700, lr = 0.00345625, m = 0.9
I0815 15:53:51.236145  8764 solver.cpp:312] Iteration 104800 (4.88036 iter/s, 20.4903s/100 iter), loss = 1.33759
I0815 15:53:51.236202  8764 solver.cpp:334]     Train net output #0: loss = 1.29361 (* 1 = 1.29361 loss)
I0815 15:53:51.236212  8764 sgd_solver.cpp:136] Iteration 104800, lr = 0.00345, m = 0.9
I0815 15:54:12.701725  8764 solver.cpp:312] Iteration 104900 (4.65875 iter/s, 21.465s/100 iter), loss = 1.72312
I0815 15:54:12.701764  8764 solver.cpp:334]     Train net output #0: loss = 1.57296 (* 1 = 1.57296 loss)
I0815 15:54:12.701773  8764 sgd_solver.cpp:136] Iteration 104900, lr = 0.00344375, m = 0.9
I0815 15:54:30.579212  8764 solver.cpp:363] Sparsity after update:
I0815 15:54:30.591642  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:54:30.591658  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:54:30.591666  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:54:30.591670  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:54:30.591675  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:54:30.591678  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:54:30.591681  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:54:30.591686  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:54:30.591689  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:54:30.591692  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:54:30.591697  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:54:30.591701  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:54:30.591704  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:54:30.728979  8764 solver.cpp:312] Iteration 105000 (5.54731 iter/s, 18.0268s/100 iter), loss = 1.09316
I0815 15:54:30.739414  8764 solver.cpp:334]     Train net output #0: loss = 1.4025 (* 1 = 1.4025 loss)
I0815 15:54:30.739559  8764 sgd_solver.cpp:136] Iteration 105000, lr = 0.0034375, m = 0.9
I0815 15:54:52.243422  8764 solver.cpp:312] Iteration 105100 (4.64817 iter/s, 21.5138s/100 iter), loss = 1.32188
I0815 15:54:52.243508  8764 solver.cpp:334]     Train net output #0: loss = 1.12064 (* 1 = 1.12064 loss)
I0815 15:54:52.243526  8764 sgd_solver.cpp:136] Iteration 105100, lr = 0.00343125, m = 0.9
I0815 15:55:12.399266  8764 solver.cpp:312] Iteration 105200 (4.96148 iter/s, 20.1553s/100 iter), loss = 1.58474
I0815 15:55:12.399531  8764 solver.cpp:334]     Train net output #0: loss = 1.40318 (* 1 = 1.40318 loss)
I0815 15:55:12.399638  8764 sgd_solver.cpp:136] Iteration 105200, lr = 0.003425, m = 0.9
I0815 15:55:29.802278  8764 solver.cpp:312] Iteration 105300 (5.74629 iter/s, 17.4025s/100 iter), loss = 1.60502
I0815 15:55:29.802335  8764 solver.cpp:334]     Train net output #0: loss = 1.50929 (* 1 = 1.50929 loss)
I0815 15:55:29.802351  8764 sgd_solver.cpp:136] Iteration 105300, lr = 0.00341875, m = 0.9
I0815 15:55:49.457494  8764 solver.cpp:312] Iteration 105400 (5.08786 iter/s, 19.6546s/100 iter), loss = 1.56611
I0815 15:55:49.457676  8764 solver.cpp:334]     Train net output #0: loss = 1.39362 (* 1 = 1.39362 loss)
I0815 15:55:49.457747  8764 sgd_solver.cpp:136] Iteration 105400, lr = 0.0034125, m = 0.9
I0815 15:56:02.020067  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:56:10.540179  8764 solver.cpp:312] Iteration 105500 (4.74335 iter/s, 21.0821s/100 iter), loss = 1.20497
I0815 15:56:10.540223  8764 solver.cpp:334]     Train net output #0: loss = 1.01518 (* 1 = 1.01518 loss)
I0815 15:56:10.540236  8764 sgd_solver.cpp:136] Iteration 105500, lr = 0.00340625, m = 0.9
I0815 15:56:29.573487  8764 solver.cpp:312] Iteration 105600 (5.2541 iter/s, 19.0328s/100 iter), loss = 1.5095
I0815 15:56:29.573585  8764 solver.cpp:334]     Train net output #0: loss = 1.09912 (* 1 = 1.09912 loss)
I0815 15:56:29.573596  8764 sgd_solver.cpp:136] Iteration 105600, lr = 0.0034, m = 0.9
I0815 15:56:48.018499  8764 solver.cpp:312] Iteration 105700 (5.42167 iter/s, 18.4445s/100 iter), loss = 1.5605
I0815 15:56:48.018527  8764 solver.cpp:334]     Train net output #0: loss = 1.499 (* 1 = 1.499 loss)
I0815 15:56:48.018532  8764 sgd_solver.cpp:136] Iteration 105700, lr = 0.00339375, m = 0.9
I0815 15:57:05.602295  8764 solver.cpp:312] Iteration 105800 (5.68721 iter/s, 17.5833s/100 iter), loss = 1.5108
I0815 15:57:05.602519  8764 solver.cpp:334]     Train net output #0: loss = 1.79747 (* 1 = 1.79747 loss)
I0815 15:57:05.602532  8764 sgd_solver.cpp:136] Iteration 105800, lr = 0.0033875, m = 0.9
I0815 15:57:23.931872  8764 solver.cpp:312] Iteration 105900 (5.45581 iter/s, 18.3291s/100 iter), loss = 1.45179
I0815 15:57:23.931900  8764 solver.cpp:334]     Train net output #0: loss = 1.87381 (* 1 = 1.87381 loss)
I0815 15:57:23.931906  8764 sgd_solver.cpp:136] Iteration 105900, lr = 0.00338125, m = 0.9
I0815 15:57:41.494163  8764 solver.cpp:363] Sparsity after update:
I0815 15:57:41.500218  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:57:41.500237  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:57:41.500246  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:57:41.500249  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:57:41.500252  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:57:41.500257  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:57:41.500259  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:57:41.500262  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:57:41.500265  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:57:41.500268  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:57:41.500272  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:57:41.500274  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:57:41.500277  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:57:41.500289  8764 solver.cpp:509] Iteration 106000, Testing net (#0)
I0815 15:58:11.369966  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.582058
I0815 15:58:11.369990  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.81282
I0815 15:58:11.369995  8764 solver.cpp:594]     Test net output #2: loss = 1.83708 (* 1 = 1.83708 loss)
I0815 15:58:11.370020  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.8689s
I0815 15:58:11.518431  8764 solver.cpp:312] Iteration 106000 (2.10149 iter/s, 47.5853s/100 iter), loss = 1.41676
I0815 15:58:11.518512  8764 solver.cpp:334]     Train net output #0: loss = 1.43165 (* 1 = 1.43165 loss)
I0815 15:58:11.518518  8764 sgd_solver.cpp:136] Iteration 106000, lr = 0.003375, m = 0.9
I0815 15:58:30.700357  8764 solver.cpp:312] Iteration 106100 (5.21339 iter/s, 19.1814s/100 iter), loss = 1.34735
I0815 15:58:30.700419  8764 solver.cpp:334]     Train net output #0: loss = 1.51439 (* 1 = 1.51439 loss)
I0815 15:58:30.700431  8764 sgd_solver.cpp:136] Iteration 106100, lr = 0.00336875, m = 0.9
I0815 15:58:49.955842  8764 solver.cpp:312] Iteration 106200 (5.19348 iter/s, 19.2549s/100 iter), loss = 1.55182
I0815 15:58:49.956059  8764 solver.cpp:334]     Train net output #0: loss = 1.51968 (* 1 = 1.51968 loss)
I0815 15:58:49.956096  8764 sgd_solver.cpp:136] Iteration 106200, lr = 0.0033625, m = 0.9
I0815 15:59:07.415392  8764 solver.cpp:312] Iteration 106300 (5.72767 iter/s, 17.4591s/100 iter), loss = 1.18725
I0815 15:59:07.415465  8764 solver.cpp:334]     Train net output #0: loss = 0.978001 (* 1 = 0.978001 loss)
I0815 15:59:07.415485  8764 sgd_solver.cpp:136] Iteration 106300, lr = 0.00335625, m = 0.9
I0815 15:59:27.260335  8764 solver.cpp:312] Iteration 106400 (5.03921 iter/s, 19.8444s/100 iter), loss = 1.36138
I0815 15:59:27.260480  8764 solver.cpp:334]     Train net output #0: loss = 1.68374 (* 1 = 1.68374 loss)
I0815 15:59:27.260511  8764 sgd_solver.cpp:136] Iteration 106400, lr = 0.00335, m = 0.9
I0815 15:59:44.420017  8764 solver.cpp:312] Iteration 106500 (5.82778 iter/s, 17.1592s/100 iter), loss = 1.42783
I0815 15:59:44.420275  8764 solver.cpp:334]     Train net output #0: loss = 1.26234 (* 1 = 1.26234 loss)
I0815 15:59:44.420392  8764 sgd_solver.cpp:136] Iteration 106500, lr = 0.00334375, m = 0.9
I0815 16:00:01.718258  8764 solver.cpp:312] Iteration 106600 (5.78109 iter/s, 17.2978s/100 iter), loss = 1.57907
I0815 16:00:01.718341  8764 solver.cpp:334]     Train net output #0: loss = 1.67161 (* 1 = 1.67161 loss)
I0815 16:00:01.718353  8764 sgd_solver.cpp:136] Iteration 106600, lr = 0.0033375, m = 0.9
I0815 16:00:18.916278  8764 solver.cpp:312] Iteration 106700 (5.81478 iter/s, 17.1975s/100 iter), loss = 1.66473
I0815 16:00:18.916303  8764 solver.cpp:334]     Train net output #0: loss = 1.45946 (* 1 = 1.45946 loss)
I0815 16:00:18.916307  8764 sgd_solver.cpp:136] Iteration 106700, lr = 0.00333125, m = 0.9
I0815 16:00:37.306687  8764 solver.cpp:312] Iteration 106800 (5.43777 iter/s, 18.3899s/100 iter), loss = 1.76232
I0815 16:00:37.307668  8764 solver.cpp:334]     Train net output #0: loss = 1.62883 (* 1 = 1.62883 loss)
I0815 16:00:37.307687  8764 sgd_solver.cpp:136] Iteration 106800, lr = 0.003325, m = 0.9
I0815 16:00:55.958892  8764 solver.cpp:312] Iteration 106900 (5.36144 iter/s, 18.6517s/100 iter), loss = 2.12076
I0815 16:00:55.958920  8764 solver.cpp:334]     Train net output #0: loss = 2.03765 (* 1 = 2.03765 loss)
I0815 16:00:55.958925  8764 sgd_solver.cpp:136] Iteration 106900, lr = 0.00331875, m = 0.9
I0815 16:01:13.724467  8764 solver.cpp:363] Sparsity after update:
I0815 16:01:13.735654  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:01:13.735880  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:01:13.735986  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:01:13.736078  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:01:13.736177  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:01:13.736270  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:01:13.736361  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:01:13.736450  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:01:13.736539  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:01:13.736629  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:01:13.736726  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:01:13.736819  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:01:13.736910  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:01:14.012845  8764 solver.cpp:312] Iteration 107000 (5.5391 iter/s, 18.0535s/100 iter), loss = 1.61992
I0815 16:01:14.012874  8764 solver.cpp:334]     Train net output #0: loss = 1.58592 (* 1 = 1.58592 loss)
I0815 16:01:14.012879  8764 sgd_solver.cpp:136] Iteration 107000, lr = 0.0033125, m = 0.9
I0815 16:01:33.771396  8764 solver.cpp:312] Iteration 107100 (5.06124 iter/s, 19.758s/100 iter), loss = 1.75157
I0815 16:01:33.771494  8764 solver.cpp:334]     Train net output #0: loss = 2.00514 (* 1 = 2.00514 loss)
I0815 16:01:33.771524  8764 sgd_solver.cpp:136] Iteration 107100, lr = 0.00330625, m = 0.9
I0815 16:01:52.321934  8764 solver.cpp:312] Iteration 107200 (5.39083 iter/s, 18.55s/100 iter), loss = 1.71748
I0815 16:01:52.322185  8764 solver.cpp:334]     Train net output #0: loss = 1.34009 (* 1 = 1.34009 loss)
I0815 16:01:52.322203  8764 sgd_solver.cpp:136] Iteration 107200, lr = 0.0033, m = 0.9
I0815 16:02:10.882153  8764 solver.cpp:312] Iteration 107300 (5.38802 iter/s, 18.5597s/100 iter), loss = 1.51668
I0815 16:02:10.882181  8764 solver.cpp:334]     Train net output #0: loss = 1.72483 (* 1 = 1.72483 loss)
I0815 16:02:10.882186  8764 sgd_solver.cpp:136] Iteration 107300, lr = 0.00329375, m = 0.9
I0815 16:02:28.280745  8764 solver.cpp:312] Iteration 107400 (5.74775 iter/s, 17.3981s/100 iter), loss = 1.23842
I0815 16:02:28.280809  8764 solver.cpp:334]     Train net output #0: loss = 1.3992 (* 1 = 1.3992 loss)
I0815 16:02:28.280817  8764 sgd_solver.cpp:136] Iteration 107400, lr = 0.0032875, m = 0.9
I0815 16:02:44.756265  8764 solver.cpp:312] Iteration 107500 (6.06978 iter/s, 16.4751s/100 iter), loss = 1.62787
I0815 16:02:44.756338  8764 solver.cpp:334]     Train net output #0: loss = 1.94362 (* 1 = 1.94362 loss)
I0815 16:02:44.756357  8764 sgd_solver.cpp:136] Iteration 107500, lr = 0.00328125, m = 0.9
I0815 16:03:03.645947  8764 solver.cpp:312] Iteration 107600 (5.29404 iter/s, 18.8892s/100 iter), loss = 1.33059
I0815 16:03:03.646023  8764 solver.cpp:334]     Train net output #0: loss = 1.36918 (* 1 = 1.36918 loss)
I0815 16:03:03.646049  8764 sgd_solver.cpp:136] Iteration 107600, lr = 0.003275, m = 0.9
I0815 16:03:23.104223  8764 solver.cpp:312] Iteration 107700 (5.13934 iter/s, 19.4577s/100 iter), loss = 1.22445
I0815 16:03:23.104272  8764 solver.cpp:334]     Train net output #0: loss = 1.28329 (* 1 = 1.28329 loss)
I0815 16:03:23.104281  8764 sgd_solver.cpp:136] Iteration 107700, lr = 0.00326875, m = 0.9
I0815 16:03:40.876061  8764 solver.cpp:312] Iteration 107800 (5.62703 iter/s, 17.7714s/100 iter), loss = 1.36565
I0815 16:03:40.876144  8764 solver.cpp:334]     Train net output #0: loss = 1.31426 (* 1 = 1.31426 loss)
I0815 16:03:40.876155  8764 sgd_solver.cpp:136] Iteration 107800, lr = 0.0032625, m = 0.9
I0815 16:03:57.741446  8764 solver.cpp:312] Iteration 107900 (5.92947 iter/s, 16.8649s/100 iter), loss = 1.48015
I0815 16:03:57.741475  8764 solver.cpp:334]     Train net output #0: loss = 1.34884 (* 1 = 1.34884 loss)
I0815 16:03:57.741480  8764 sgd_solver.cpp:136] Iteration 107900, lr = 0.00325625, m = 0.9
I0815 16:04:15.441298  8764 solver.cpp:363] Sparsity after update:
I0815 16:04:15.446190  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:04:15.446228  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:04:15.446247  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:04:15.446259  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:04:15.446271  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:04:15.446285  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:04:15.446296  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:04:15.446307  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:04:15.446319  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:04:15.446331  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:04:15.446348  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:04:15.446363  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:04:15.446377  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:04:15.446396  8764 solver.cpp:509] Iteration 108000, Testing net (#0)
I0815 16:04:43.529120  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.581588
I0815 16:04:43.529144  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.81335
I0815 16:04:43.529150  8764 solver.cpp:594]     Test net output #2: loss = 1.82613 (* 1 = 1.82613 loss)
I0815 16:04:43.529171  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.082s
I0815 16:04:43.671911  8764 solver.cpp:312] Iteration 108000 (2.17726 iter/s, 45.9292s/100 iter), loss = 1.27125
I0815 16:04:43.671934  8764 solver.cpp:334]     Train net output #0: loss = 1.1074 (* 1 = 1.1074 loss)
I0815 16:04:43.671939  8764 sgd_solver.cpp:136] Iteration 108000, lr = 0.00325, m = 0.9
I0815 16:05:00.810331  8764 solver.cpp:312] Iteration 108100 (5.83501 iter/s, 17.1379s/100 iter), loss = 1.46527
I0815 16:05:00.810395  8764 solver.cpp:334]     Train net output #0: loss = 1.49723 (* 1 = 1.49723 loss)
I0815 16:05:00.810401  8764 sgd_solver.cpp:136] Iteration 108100, lr = 0.00324375, m = 0.9
I0815 16:05:19.436190  8764 solver.cpp:312] Iteration 108200 (5.36903 iter/s, 18.6253s/100 iter), loss = 1.32026
I0815 16:05:19.436215  8764 solver.cpp:334]     Train net output #0: loss = 1.31307 (* 1 = 1.31307 loss)
I0815 16:05:19.436218  8764 sgd_solver.cpp:136] Iteration 108200, lr = 0.0032375, m = 0.9
I0815 16:05:38.938267  8764 solver.cpp:312] Iteration 108300 (5.1278 iter/s, 19.5015s/100 iter), loss = 1.33295
I0815 16:05:38.938346  8764 solver.cpp:334]     Train net output #0: loss = 1.33877 (* 1 = 1.33877 loss)
I0815 16:05:38.938360  8764 sgd_solver.cpp:136] Iteration 108300, lr = 0.00323125, m = 0.9
I0815 16:05:56.558003  8764 solver.cpp:312] Iteration 108400 (5.67561 iter/s, 17.6192s/100 iter), loss = 1.73851
I0815 16:05:56.558256  8764 solver.cpp:334]     Train net output #0: loss = 1.67147 (* 1 = 1.67147 loss)
I0815 16:05:56.558377  8764 sgd_solver.cpp:136] Iteration 108400, lr = 0.003225, m = 0.9
I0815 16:06:15.457666  8764 solver.cpp:312] Iteration 108500 (5.29124 iter/s, 18.8991s/100 iter), loss = 1.38747
I0815 16:06:15.457733  8764 solver.cpp:334]     Train net output #0: loss = 1.56171 (* 1 = 1.56171 loss)
I0815 16:06:15.457741  8764 sgd_solver.cpp:136] Iteration 108500, lr = 0.00321875, m = 0.9
I0815 16:06:34.134361  8764 solver.cpp:312] Iteration 108600 (5.35441 iter/s, 18.6762s/100 iter), loss = 1.37272
I0815 16:06:34.134389  8764 solver.cpp:334]     Train net output #0: loss = 1.56209 (* 1 = 1.56209 loss)
I0815 16:06:34.134395  8764 sgd_solver.cpp:136] Iteration 108600, lr = 0.0032125, m = 0.9
I0815 16:06:37.722013  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:06:57.896026  8764 solver.cpp:312] Iteration 108700 (4.20858 iter/s, 23.761s/100 iter), loss = 1.59915
I0815 16:06:57.896117  8764 solver.cpp:334]     Train net output #0: loss = 1.59753 (* 1 = 1.59753 loss)
I0815 16:06:57.896148  8764 sgd_solver.cpp:136] Iteration 108700, lr = 0.00320625, m = 0.9
I0815 16:07:17.832505  8764 solver.cpp:312] Iteration 108800 (5.01607 iter/s, 19.9359s/100 iter), loss = 1.45992
I0815 16:07:17.832577  8764 solver.cpp:334]     Train net output #0: loss = 1.42627 (* 1 = 1.42627 loss)
I0815 16:07:17.832597  8764 sgd_solver.cpp:136] Iteration 108800, lr = 0.0032, m = 0.9
I0815 16:07:38.434265  8764 solver.cpp:312] Iteration 108900 (4.85409 iter/s, 20.6012s/100 iter), loss = 1.71638
I0815 16:07:38.434355  8764 solver.cpp:334]     Train net output #0: loss = 1.80799 (* 1 = 1.80799 loss)
I0815 16:07:38.434367  8764 sgd_solver.cpp:136] Iteration 108900, lr = 0.00319375, m = 0.9
I0815 16:07:58.615411  8764 solver.cpp:363] Sparsity after update:
I0815 16:07:58.620473  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:07:58.620507  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:07:58.620527  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:07:58.620532  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:07:58.620540  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:07:58.620548  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:07:58.620554  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:07:58.620561  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:07:58.620568  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:07:58.620575  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:07:58.620582  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:07:58.620589  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:07:58.620596  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:07:59.029526  8764 solver.cpp:312] Iteration 109000 (4.85563 iter/s, 20.5947s/100 iter), loss = 1.38637
I0815 16:07:59.029798  8764 solver.cpp:334]     Train net output #0: loss = 1.44425 (* 1 = 1.44425 loss)
I0815 16:07:59.029944  8764 sgd_solver.cpp:136] Iteration 109000, lr = 0.0031875, m = 0.9
I0815 16:08:20.045441  8764 solver.cpp:312] Iteration 109100 (4.75843 iter/s, 21.0153s/100 iter), loss = 1.50472
I0815 16:08:20.070117  8764 solver.cpp:334]     Train net output #0: loss = 1.60986 (* 1 = 1.60986 loss)
I0815 16:08:20.070243  8764 sgd_solver.cpp:136] Iteration 109100, lr = 0.00318125, m = 0.9
I0815 16:08:37.859380  8764 solver.cpp:312] Iteration 109200 (5.61374 iter/s, 17.8134s/100 iter), loss = 1.39216
I0815 16:08:37.859408  8764 solver.cpp:334]     Train net output #0: loss = 1.32406 (* 1 = 1.32406 loss)
I0815 16:08:37.859414  8764 sgd_solver.cpp:136] Iteration 109200, lr = 0.003175, m = 0.9
I0815 16:08:57.700304  8764 solver.cpp:312] Iteration 109300 (5.04023 iter/s, 19.8404s/100 iter), loss = 1.25909
I0815 16:08:57.700388  8764 solver.cpp:334]     Train net output #0: loss = 1.46571 (* 1 = 1.46571 loss)
I0815 16:08:57.700398  8764 sgd_solver.cpp:136] Iteration 109300, lr = 0.00316875, m = 0.9
I0815 16:09:17.407801  8764 solver.cpp:312] Iteration 109400 (5.07435 iter/s, 19.707s/100 iter), loss = 1.11777
I0815 16:09:17.407840  8764 solver.cpp:334]     Train net output #0: loss = 1.06542 (* 1 = 1.06542 loss)
I0815 16:09:17.407850  8764 sgd_solver.cpp:136] Iteration 109400, lr = 0.0031625, m = 0.9
I0815 16:09:36.911404  8764 solver.cpp:312] Iteration 109500 (5.1274 iter/s, 19.5031s/100 iter), loss = 1.28335
I0815 16:09:36.911459  8764 solver.cpp:334]     Train net output #0: loss = 1.11181 (* 1 = 1.11181 loss)
I0815 16:09:36.911465  8764 sgd_solver.cpp:136] Iteration 109500, lr = 0.00315625, m = 0.9
I0815 16:09:54.857429  8764 solver.cpp:312] Iteration 109600 (5.57242 iter/s, 17.9455s/100 iter), loss = 1.04551
I0815 16:09:54.857458  8764 solver.cpp:334]     Train net output #0: loss = 0.91011 (* 1 = 0.91011 loss)
I0815 16:09:54.857465  8764 sgd_solver.cpp:136] Iteration 109600, lr = 0.00315, m = 0.9
I0815 16:10:14.625538  8764 solver.cpp:312] Iteration 109700 (5.05879 iter/s, 19.7676s/100 iter), loss = 1.58956
I0815 16:10:14.625633  8764 solver.cpp:334]     Train net output #0: loss = 1.61332 (* 1 = 1.61332 loss)
I0815 16:10:14.625650  8764 sgd_solver.cpp:136] Iteration 109700, lr = 0.00314375, m = 0.9
I0815 16:10:33.533391  8764 solver.cpp:312] Iteration 109800 (5.28896 iter/s, 18.9073s/100 iter), loss = 1.59597
I0815 16:10:33.533437  8764 solver.cpp:334]     Train net output #0: loss = 1.5603 (* 1 = 1.5603 loss)
I0815 16:10:33.533449  8764 sgd_solver.cpp:136] Iteration 109800, lr = 0.0031375, m = 0.9
I0815 16:10:52.014552  8764 solver.cpp:312] Iteration 109900 (5.41106 iter/s, 18.4807s/100 iter), loss = 1.4372
I0815 16:10:52.024179  8764 solver.cpp:334]     Train net output #0: loss = 1.5296 (* 1 = 1.5296 loss)
I0815 16:10:52.024199  8764 sgd_solver.cpp:136] Iteration 109900, lr = 0.00313125, m = 0.9
I0815 16:11:09.628958  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_110000.caffemodel
I0815 16:11:09.682989  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_110000.solverstate
I0815 16:11:09.689448  8764 solver.cpp:363] Sparsity after update:
I0815 16:11:09.694461  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:11:09.694492  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:11:09.694507  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:11:09.694515  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:11:09.694524  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:11:09.694531  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:11:09.694540  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:11:09.694548  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:11:09.694557  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:11:09.694566  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:11:09.694574  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:11:09.694582  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:11:09.694591  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:11:09.694607  8764 solver.cpp:509] Iteration 110000, Testing net (#0)
I0815 16:11:42.601620  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.584765
I0815 16:11:42.601774  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814879
I0815 16:11:42.601796  8764 solver.cpp:594]     Test net output #2: loss = 1.82971 (* 1 = 1.82971 loss)
I0815 16:11:42.601837  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.9063s
I0815 16:11:42.881901  8764 solver.cpp:312] Iteration 110000 (1.96595 iter/s, 50.8659s/100 iter), loss = 1.49945
I0815 16:11:42.881947  8764 solver.cpp:334]     Train net output #0: loss = 1.69433 (* 1 = 1.69433 loss)
I0815 16:11:42.881956  8764 sgd_solver.cpp:136] Iteration 110000, lr = 0.003125, m = 0.9
I0815 16:12:01.620116  8764 solver.cpp:312] Iteration 110100 (5.33683 iter/s, 18.7377s/100 iter), loss = 1.25771
I0815 16:12:01.620208  8764 solver.cpp:334]     Train net output #0: loss = 0.776489 (* 1 = 0.776489 loss)
I0815 16:12:01.620229  8764 sgd_solver.cpp:136] Iteration 110100, lr = 0.00311875, m = 0.9
I0815 16:12:21.626164  8764 solver.cpp:312] Iteration 110200 (4.99863 iter/s, 20.0055s/100 iter), loss = 1.68591
I0815 16:12:21.626242  8764 solver.cpp:334]     Train net output #0: loss = 1.5258 (* 1 = 1.5258 loss)
I0815 16:12:21.626257  8764 sgd_solver.cpp:136] Iteration 110200, lr = 0.0031125, m = 0.9
I0815 16:12:42.091493  8764 solver.cpp:312] Iteration 110300 (4.88645 iter/s, 20.4648s/100 iter), loss = 1.46002
I0815 16:12:42.091523  8764 solver.cpp:334]     Train net output #0: loss = 1.30471 (* 1 = 1.30471 loss)
I0815 16:12:42.091528  8764 sgd_solver.cpp:136] Iteration 110300, lr = 0.00310625, m = 0.9
I0815 16:13:00.519323  8764 solver.cpp:312] Iteration 110400 (5.42673 iter/s, 18.4273s/100 iter), loss = 1.56342
I0815 16:13:00.520190  8764 solver.cpp:334]     Train net output #0: loss = 1.68352 (* 1 = 1.68352 loss)
I0815 16:13:00.520220  8764 sgd_solver.cpp:136] Iteration 110400, lr = 0.0031, m = 0.9
I0815 16:13:17.919270  8764 solver.cpp:312] Iteration 110500 (5.7473 iter/s, 17.3995s/100 iter), loss = 0.93888
I0815 16:13:17.919293  8764 solver.cpp:334]     Train net output #0: loss = 0.716048 (* 1 = 0.716048 loss)
I0815 16:13:17.919299  8764 sgd_solver.cpp:136] Iteration 110500, lr = 0.00309375, m = 0.9
I0815 16:13:35.675952  8764 solver.cpp:312] Iteration 110600 (5.63184 iter/s, 17.7562s/100 iter), loss = 1.59368
I0815 16:13:35.676033  8764 solver.cpp:334]     Train net output #0: loss = 1.32365 (* 1 = 1.32365 loss)
I0815 16:13:35.676044  8764 sgd_solver.cpp:136] Iteration 110600, lr = 0.0030875, m = 0.9
I0815 16:13:53.464164  8764 solver.cpp:312] Iteration 110700 (5.62185 iter/s, 17.7877s/100 iter), loss = 1.58618
I0815 16:13:53.464233  8764 solver.cpp:334]     Train net output #0: loss = 1.55262 (* 1 = 1.55262 loss)
I0815 16:13:53.464253  8764 sgd_solver.cpp:136] Iteration 110700, lr = 0.00308125, m = 0.9
I0815 16:14:11.639042  8764 solver.cpp:312] Iteration 110800 (5.50225 iter/s, 18.1744s/100 iter), loss = 1.42627
I0815 16:14:11.639135  8764 solver.cpp:334]     Train net output #0: loss = 1.6947 (* 1 = 1.6947 loss)
I0815 16:14:11.639143  8764 sgd_solver.cpp:136] Iteration 110800, lr = 0.003075, m = 0.9
I0815 16:14:30.043834  8764 solver.cpp:312] Iteration 110900 (5.43352 iter/s, 18.4043s/100 iter), loss = 1.72753
I0815 16:14:30.043864  8764 solver.cpp:334]     Train net output #0: loss = 1.65599 (* 1 = 1.65599 loss)
I0815 16:14:30.043870  8764 sgd_solver.cpp:136] Iteration 110900, lr = 0.00306875, m = 0.9
I0815 16:14:46.575855  8764 solver.cpp:363] Sparsity after update:
I0815 16:14:46.585286  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:14:46.597460  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:14:46.597525  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:14:46.597558  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:14:46.597600  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:14:46.597628  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:14:46.597667  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:14:46.597703  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:14:46.597739  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:14:46.597764  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:14:46.597790  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:14:46.597821  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:14:46.597857  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:14:46.789885  8764 solver.cpp:312] Iteration 111000 (5.97172 iter/s, 16.7456s/100 iter), loss = 1.21831
I0815 16:14:46.789912  8764 solver.cpp:334]     Train net output #0: loss = 1.36962 (* 1 = 1.36962 loss)
I0815 16:14:46.789918  8764 sgd_solver.cpp:136] Iteration 111000, lr = 0.0030625, m = 0.9
I0815 16:15:06.472501  8764 solver.cpp:312] Iteration 111100 (5.08076 iter/s, 19.6821s/100 iter), loss = 1.37602
I0815 16:15:06.472650  8764 solver.cpp:334]     Train net output #0: loss = 1.56952 (* 1 = 1.56952 loss)
I0815 16:15:06.472666  8764 sgd_solver.cpp:136] Iteration 111100, lr = 0.00305625, m = 0.9
I0815 16:15:24.894866  8764 solver.cpp:312] Iteration 111200 (5.42834 iter/s, 18.4219s/100 iter), loss = 1.54344
I0815 16:15:24.894950  8764 solver.cpp:334]     Train net output #0: loss = 1.49478 (* 1 = 1.49478 loss)
I0815 16:15:24.894961  8764 sgd_solver.cpp:136] Iteration 111200, lr = 0.00305, m = 0.9
I0815 16:15:43.654660  8764 solver.cpp:312] Iteration 111300 (5.33069 iter/s, 18.7593s/100 iter), loss = 1.81423
I0815 16:15:43.654686  8764 solver.cpp:334]     Train net output #0: loss = 2.02344 (* 1 = 2.02344 loss)
I0815 16:15:43.654691  8764 sgd_solver.cpp:136] Iteration 111300, lr = 0.00304375, m = 0.9
I0815 16:16:00.184144  8764 solver.cpp:312] Iteration 111400 (6.04997 iter/s, 16.529s/100 iter), loss = 1.38263
I0815 16:16:00.184288  8764 solver.cpp:334]     Train net output #0: loss = 1.51345 (* 1 = 1.51345 loss)
I0815 16:16:00.184303  8764 sgd_solver.cpp:136] Iteration 111400, lr = 0.0030375, m = 0.9
I0815 16:16:19.025842  8764 solver.cpp:312] Iteration 111500 (5.30752 iter/s, 18.8412s/100 iter), loss = 1.55067
I0815 16:16:19.025871  8764 solver.cpp:334]     Train net output #0: loss = 1.3168 (* 1 = 1.3168 loss)
I0815 16:16:19.025878  8764 sgd_solver.cpp:136] Iteration 111500, lr = 0.00303125, m = 0.9
I0815 16:16:37.176420  8764 solver.cpp:312] Iteration 111600 (5.50962 iter/s, 18.1501s/100 iter), loss = 1.06307
I0815 16:16:37.176555  8764 solver.cpp:334]     Train net output #0: loss = 1.20734 (* 1 = 1.20734 loss)
I0815 16:16:37.176584  8764 sgd_solver.cpp:136] Iteration 111600, lr = 0.003025, m = 0.9
I0815 16:16:54.951289  8764 solver.cpp:312] Iteration 111700 (5.62607 iter/s, 17.7744s/100 iter), loss = 1.3033
I0815 16:16:54.951318  8764 solver.cpp:334]     Train net output #0: loss = 1.43555 (* 1 = 1.43555 loss)
I0815 16:16:54.951323  8764 sgd_solver.cpp:136] Iteration 111700, lr = 0.00301875, m = 0.9
I0815 16:17:13.699733  8764 solver.cpp:312] Iteration 111800 (5.33392 iter/s, 18.7479s/100 iter), loss = 1.16842
I0815 16:17:13.699992  8764 solver.cpp:334]     Train net output #0: loss = 1.06594 (* 1 = 1.06594 loss)
I0815 16:17:13.700103  8764 sgd_solver.cpp:136] Iteration 111800, lr = 0.0030125, m = 0.9
I0815 16:17:31.835150  8764 solver.cpp:312] Iteration 111900 (5.51422 iter/s, 18.1349s/100 iter), loss = 1.35228
I0815 16:17:31.835211  8764 solver.cpp:334]     Train net output #0: loss = 0.930728 (* 1 = 0.930728 loss)
I0815 16:17:31.835230  8764 sgd_solver.cpp:136] Iteration 111900, lr = 0.00300625, m = 0.9
I0815 16:17:50.053299  8764 solver.cpp:363] Sparsity after update:
I0815 16:17:50.070384  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:17:50.070446  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:17:50.070485  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:17:50.070504  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:17:50.070523  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:17:50.070533  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:17:50.070549  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:17:50.070564  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:17:50.070581  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:17:50.070595  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:17:50.070611  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:17:50.070624  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:17:50.070641  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:17:50.070663  8764 solver.cpp:509] Iteration 112000, Testing net (#0)
I0815 16:18:07.931627  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:18:18.175268  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.579058
I0815 16:18:18.175293  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.811644
I0815 16:18:18.175300  8764 solver.cpp:594]     Test net output #2: loss = 1.83388 (* 1 = 1.83388 loss)
I0815 16:18:18.175334  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.1039s
I0815 16:18:18.330319  8764 solver.cpp:312] Iteration 112000 (2.15082 iter/s, 46.4939s/100 iter), loss = 1.47464
I0815 16:18:18.330341  8764 solver.cpp:334]     Train net output #0: loss = 1.56839 (* 1 = 1.56839 loss)
I0815 16:18:18.330344  8764 sgd_solver.cpp:136] Iteration 112000, lr = 0.003, m = 0.9
I0815 16:18:37.313483  8764 solver.cpp:312] Iteration 112100 (5.26798 iter/s, 18.9826s/100 iter), loss = 1.60382
I0815 16:18:37.313592  8764 solver.cpp:334]     Train net output #0: loss = 1.54944 (* 1 = 1.54944 loss)
I0815 16:18:37.313609  8764 sgd_solver.cpp:136] Iteration 112100, lr = 0.00299375, m = 0.9
I0815 16:18:54.105809  8764 solver.cpp:312] Iteration 112200 (5.95526 iter/s, 16.7919s/100 iter), loss = 1.28287
I0815 16:18:54.105836  8764 solver.cpp:334]     Train net output #0: loss = 1.26569 (* 1 = 1.26569 loss)
I0815 16:18:54.105842  8764 sgd_solver.cpp:136] Iteration 112200, lr = 0.0029875, m = 0.9
I0815 16:19:10.213157  8764 solver.cpp:312] Iteration 112300 (6.20852 iter/s, 16.1069s/100 iter), loss = 1.54892
I0815 16:19:10.213210  8764 solver.cpp:334]     Train net output #0: loss = 1.4449 (* 1 = 1.4449 loss)
I0815 16:19:10.213217  8764 sgd_solver.cpp:136] Iteration 112300, lr = 0.00298125, m = 0.9
I0815 16:19:27.344463  8764 solver.cpp:312] Iteration 112400 (5.83743 iter/s, 17.1308s/100 iter), loss = 1.36692
I0815 16:19:27.344489  8764 solver.cpp:334]     Train net output #0: loss = 1.2299 (* 1 = 1.2299 loss)
I0815 16:19:27.344494  8764 sgd_solver.cpp:136] Iteration 112400, lr = 0.002975, m = 0.9
I0815 16:19:45.671284  8764 solver.cpp:312] Iteration 112500 (5.45664 iter/s, 18.3263s/100 iter), loss = 1.68395
I0815 16:19:45.671452  8764 solver.cpp:334]     Train net output #0: loss = 1.3391 (* 1 = 1.3391 loss)
I0815 16:19:45.671581  8764 sgd_solver.cpp:136] Iteration 112500, lr = 0.00296875, m = 0.9
I0815 16:20:05.182237  8764 solver.cpp:312] Iteration 112600 (5.12546 iter/s, 19.5104s/100 iter), loss = 1.43015
I0815 16:20:05.182262  8764 solver.cpp:334]     Train net output #0: loss = 1.47554 (* 1 = 1.47554 loss)
I0815 16:20:05.182267  8764 sgd_solver.cpp:136] Iteration 112600, lr = 0.0029625, m = 0.9
I0815 16:20:24.093184  8764 solver.cpp:312] Iteration 112700 (5.28809 iter/s, 18.9104s/100 iter), loss = 1.37813
I0815 16:20:24.093257  8764 solver.cpp:334]     Train net output #0: loss = 1.45073 (* 1 = 1.45073 loss)
I0815 16:20:24.093263  8764 sgd_solver.cpp:136] Iteration 112700, lr = 0.00295625, m = 0.9
I0815 16:20:42.641340  8764 solver.cpp:312] Iteration 112800 (5.39152 iter/s, 18.5476s/100 iter), loss = 1.33457
I0815 16:20:42.641367  8764 solver.cpp:334]     Train net output #0: loss = 1.5436 (* 1 = 1.5436 loss)
I0815 16:20:42.641372  8764 sgd_solver.cpp:136] Iteration 112800, lr = 0.00295, m = 0.9
I0815 16:21:00.348939  8764 solver.cpp:312] Iteration 112900 (5.64745 iter/s, 17.7071s/100 iter), loss = 1.57976
I0815 16:21:00.349001  8764 solver.cpp:334]     Train net output #0: loss = 1.40332 (* 1 = 1.40332 loss)
I0815 16:21:00.349009  8764 sgd_solver.cpp:136] Iteration 112900, lr = 0.00294375, m = 0.9
I0815 16:21:19.280153  8764 solver.cpp:363] Sparsity after update:
I0815 16:21:19.301458  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:21:19.301548  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:21:19.301607  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:21:19.301641  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:21:19.301668  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:21:19.301692  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:21:19.301724  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:21:19.301751  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:21:19.301774  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:21:19.301796  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:21:19.301818  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:21:19.301847  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:21:19.301877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:21:19.510453  8764 solver.cpp:312] Iteration 113000 (5.21894 iter/s, 19.161s/100 iter), loss = 1.69799
I0815 16:21:19.510510  8764 solver.cpp:334]     Train net output #0: loss = 1.5059 (* 1 = 1.5059 loss)
I0815 16:21:19.510524  8764 sgd_solver.cpp:136] Iteration 113000, lr = 0.0029375, m = 0.9
I0815 16:21:37.752043  8764 solver.cpp:312] Iteration 113100 (5.48213 iter/s, 18.2411s/100 iter), loss = 1.50452
I0815 16:21:37.752127  8764 solver.cpp:334]     Train net output #0: loss = 1.49685 (* 1 = 1.49685 loss)
I0815 16:21:37.752152  8764 sgd_solver.cpp:136] Iteration 113100, lr = 0.00293125, m = 0.9
I0815 16:21:54.733639  8764 solver.cpp:312] Iteration 113200 (5.88889 iter/s, 16.9811s/100 iter), loss = 1.19931
I0815 16:21:54.733666  8764 solver.cpp:334]     Train net output #0: loss = 0.962372 (* 1 = 0.962372 loss)
I0815 16:21:54.733672  8764 sgd_solver.cpp:136] Iteration 113200, lr = 0.002925, m = 0.9
I0815 16:22:13.984164  8764 solver.cpp:312] Iteration 113300 (5.19481 iter/s, 19.25s/100 iter), loss = 1.54982
I0815 16:22:13.984302  8764 solver.cpp:334]     Train net output #0: loss = 1.21857 (* 1 = 1.21857 loss)
I0815 16:22:13.984323  8764 sgd_solver.cpp:136] Iteration 113300, lr = 0.00291875, m = 0.9
I0815 16:22:32.190157  8764 solver.cpp:312] Iteration 113400 (5.49285 iter/s, 18.2055s/100 iter), loss = 1.34007
I0815 16:22:32.190224  8764 solver.cpp:334]     Train net output #0: loss = 1.36837 (* 1 = 1.36837 loss)
I0815 16:22:32.190240  8764 sgd_solver.cpp:136] Iteration 113400, lr = 0.0029125, m = 0.9
I0815 16:22:49.439285  8764 solver.cpp:312] Iteration 113500 (5.79755 iter/s, 17.2487s/100 iter), loss = 1.34059
I0815 16:22:49.439380  8764 solver.cpp:334]     Train net output #0: loss = 1.4265 (* 1 = 1.4265 loss)
I0815 16:22:49.439388  8764 sgd_solver.cpp:136] Iteration 113500, lr = 0.00290625, m = 0.9
I0815 16:23:09.498170  8764 solver.cpp:312] Iteration 113600 (4.98546 iter/s, 20.0583s/100 iter), loss = 1.25502
I0815 16:23:09.498232  8764 solver.cpp:334]     Train net output #0: loss = 1.35989 (* 1 = 1.35989 loss)
I0815 16:23:09.498277  8764 sgd_solver.cpp:136] Iteration 113600, lr = 0.0029, m = 0.9
I0815 16:23:31.172164  8764 solver.cpp:312] Iteration 113700 (4.61395 iter/s, 21.6734s/100 iter), loss = 1.84899
I0815 16:23:31.172245  8764 solver.cpp:334]     Train net output #0: loss = 1.75144 (* 1 = 1.75144 loss)
I0815 16:23:31.172256  8764 sgd_solver.cpp:136] Iteration 113700, lr = 0.00289375, m = 0.9
I0815 16:23:49.799049  8764 solver.cpp:312] Iteration 113800 (5.36873 iter/s, 18.6264s/100 iter), loss = 1.47792
I0815 16:23:49.799129  8764 solver.cpp:334]     Train net output #0: loss = 1.73262 (* 1 = 1.73262 loss)
I0815 16:23:49.799147  8764 sgd_solver.cpp:136] Iteration 113800, lr = 0.0028875, m = 0.9
I0815 16:24:10.355128  8764 solver.cpp:312] Iteration 113900 (4.86488 iter/s, 20.5555s/100 iter), loss = 1.52759
I0815 16:24:10.355244  8764 solver.cpp:334]     Train net output #0: loss = 1.53869 (* 1 = 1.53869 loss)
I0815 16:24:10.355258  8764 sgd_solver.cpp:136] Iteration 113900, lr = 0.00288125, m = 0.9
I0815 16:24:31.831159  8764 solver.cpp:363] Sparsity after update:
I0815 16:24:31.835824  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:24:31.835834  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:24:31.835840  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:24:31.835844  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:24:31.835845  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:24:31.835847  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:24:31.835850  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:24:31.835851  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:24:31.835855  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:24:31.835857  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:24:31.835860  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:24:31.835862  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:24:31.835866  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:24:31.835877  8764 solver.cpp:509] Iteration 114000, Testing net (#0)
I0815 16:25:09.040432  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.581412
I0815 16:25:09.040478  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812939
I0815 16:25:09.040485  8764 solver.cpp:594]     Test net output #2: loss = 1.82856 (* 1 = 1.82856 loss)
I0815 16:25:09.040504  8764 solver.cpp:264] [MultiGPU] Tests completed in 37.2036s
I0815 16:25:09.192549  8764 solver.cpp:312] Iteration 114000 (1.69965 iter/s, 58.8358s/100 iter), loss = 0.926058
I0815 16:25:09.192659  8764 solver.cpp:334]     Train net output #0: loss = 0.953272 (* 1 = 0.953272 loss)
I0815 16:25:09.192680  8764 sgd_solver.cpp:136] Iteration 114000, lr = 0.002875, m = 0.9
I0815 16:25:28.844578  8764 solver.cpp:312] Iteration 114100 (5.08867 iter/s, 19.6515s/100 iter), loss = 1.36714
I0815 16:25:28.844646  8764 solver.cpp:334]     Train net output #0: loss = 1.46082 (* 1 = 1.46082 loss)
I0815 16:25:28.844663  8764 sgd_solver.cpp:136] Iteration 114100, lr = 0.00286875, m = 0.9
I0815 16:25:50.146183  8764 solver.cpp:312] Iteration 114200 (4.69461 iter/s, 21.301s/100 iter), loss = 1.15622
I0815 16:25:50.146306  8764 solver.cpp:334]     Train net output #0: loss = 1.02446 (* 1 = 1.02446 loss)
I0815 16:25:50.146327  8764 sgd_solver.cpp:136] Iteration 114200, lr = 0.0028625, m = 0.9
I0815 16:25:52.067590  8726 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 16:26:11.413108  8764 solver.cpp:312] Iteration 114300 (4.70227 iter/s, 21.2663s/100 iter), loss = 1.59443
I0815 16:26:11.413137  8764 solver.cpp:334]     Train net output #0: loss = 2.10321 (* 1 = 2.10321 loss)
I0815 16:26:11.413143  8764 sgd_solver.cpp:136] Iteration 114300, lr = 0.00285625, m = 0.9
I0815 16:26:33.141044  8764 solver.cpp:312] Iteration 114400 (4.6025 iter/s, 21.7273s/100 iter), loss = 1.73548
I0815 16:26:33.141300  8764 solver.cpp:334]     Train net output #0: loss = 2.00047 (* 1 = 2.00047 loss)
I0815 16:26:33.141316  8764 sgd_solver.cpp:136] Iteration 114400, lr = 0.00285, m = 0.9
I0815 16:26:55.514225  8764 solver.cpp:312] Iteration 114500 (4.46976 iter/s, 22.3725s/100 iter), loss = 2.00495
I0815 16:26:55.514328  8764 solver.cpp:334]     Train net output #0: loss = 2.16445 (* 1 = 2.16445 loss)
I0815 16:26:55.514564  8764 sgd_solver.cpp:136] Iteration 114500, lr = 0.00284375, m = 0.9
I0815 16:27:16.401558  8764 solver.cpp:312] Iteration 114600 (4.78772 iter/s, 20.8868s/100 iter), loss = 1.5646
I0815 16:27:16.401665  8764 solver.cpp:334]     Train net output #0: loss = 1.43328 (* 1 = 1.43328 loss)
I0815 16:27:16.401679  8764 sgd_solver.cpp:136] Iteration 114600, lr = 0.0028375, m = 0.9
I0815 16:27:37.003939  8764 solver.cpp:312] Iteration 114700 (4.85394 iter/s, 20.6018s/100 iter), loss = 1.26367
I0815 16:27:37.003963  8764 solver.cpp:334]     Train net output #0: loss = 1.26792 (* 1 = 1.26792 loss)
I0815 16:27:37.003967  8764 sgd_solver.cpp:136] Iteration 114700, lr = 0.00283125, m = 0.9
I0815 16:27:56.045708  8764 solver.cpp:312] Iteration 114800 (5.25176 iter/s, 19.0412s/100 iter), loss = 1.38579
I0815 16:27:56.045842  8764 solver.cpp:334]     Train net output #0: loss = 1.53976 (* 1 = 1.53976 loss)
I0815 16:27:56.045868  8764 sgd_solver.cpp:136] Iteration 114800, lr = 0.002825, m = 0.9
I0815 16:28:14.766316  8764 solver.cpp:312] Iteration 114900 (5.34186 iter/s, 18.7201s/100 iter), loss = 1.24201
I0815 16:28:14.766386  8764 solver.cpp:334]     Train net output #0: loss = 1.28948 (* 1 = 1.28948 loss)
I0815 16:28:14.766403  8764 sgd_solver.cpp:136] Iteration 114900, lr = 0.00281875, m = 0.9
I0815 16:28:35.484385  8764 solver.cpp:363] Sparsity after update:
I0815 16:28:35.497906  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:28:35.497923  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:28:35.497930  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:28:35.497932  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:28:35.497936  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:28:35.497938  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:28:35.497942  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:28:35.497946  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:28:35.497949  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:28:35.497952  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:28:35.497956  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:28:35.497958  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:28:35.497961  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:28:35.645674  8764 solver.cpp:312] Iteration 115000 (4.78955 iter/s, 20.8788s/100 iter), loss = 1.56565
I0815 16:28:35.645701  8764 solver.cpp:334]     Train net output #0: loss = 1.22918 (* 1 = 1.22918 loss)
I0815 16:28:35.645707  8764 sgd_solver.cpp:136] Iteration 115000, lr = 0.0028125, m = 0.9
I0815 16:28:53.534525  8764 solver.cpp:312] Iteration 115100 (5.59023 iter/s, 17.8884s/100 iter), loss = 1.50343
I0815 16:28:53.534579  8764 solver.cpp:334]     Train net output #0: loss = 1.12271 (* 1 = 1.12271 loss)
I0815 16:28:53.534590  8764 sgd_solver.cpp:136] Iteration 115100, lr = 0.00280625, m = 0.9
I0815 16:29:16.736421  8764 solver.cpp:312] Iteration 115200 (4.31011 iter/s, 23.2013s/100 iter), loss = 1.33077
I0815 16:29:16.736513  8764 solver.cpp:334]     Train net output #0: loss = 1.33849 (* 1 = 1.33849 loss)
I0815 16:29:16.736521  8764 sgd_solver.cpp:136] Iteration 115200, lr = 0.0028, m = 0.9
I0815 16:29:35.983247  8764 solver.cpp:312] Iteration 115300 (5.1958 iter/s, 19.2463s/100 iter), loss = 1.70338
I0815 16:29:35.983275  8764 solver.cpp:334]     Train net output #0: loss = 2.06592 (* 1 = 2.06592 loss)
I0815 16:29:35.983302  8764 sgd_solver.cpp:136] Iteration 115300, lr = 0.00279375, m = 0.9
I0815 16:29:55.159436  8764 solver.cpp:312] Iteration 115400 (5.21495 iter/s, 19.1757s/100 iter), loss = 0.873009
I0815 16:29:55.159555  8764 solver.cpp:334]     Train net output #0: loss = 0.958324 (* 1 = 0.958324 loss)
I0815 16:29:55.159571  8764 sgd_solver.cpp:136] Iteration 115400, lr = 0.0027875, m = 0.9
I0815 16:30:15.019711  8764 solver.cpp:312] Iteration 115500 (5.03532 iter/s, 19.8597s/100 iter), loss = 1.55157
I0815 16:30:15.019757  8764 solver.cpp:334]     Train net output #0: loss = 1.56662 (* 1 = 1.56662 loss)
I0815 16:30:15.019763  8764 sgd_solver.cpp:136] Iteration 115500, lr = 0.00278125, m = 0.9
I0815 16:30:32.911799  8764 solver.cpp:312] Iteration 115600 (5.58922 iter/s, 17.8916s/100 iter), loss = 1.5506
I0815 16:30:32.911864  8764 solver.cpp:334]     Train net output #0: loss = 1.76794 (* 1 = 1.76794 loss)
I0815 16:30:32.911871  8764 sgd_solver.cpp:136] Iteration 115600, lr = 0.002775, m = 0.9
I0815 16:30:51.004429  8764 solver.cpp:312] Iteration 115700 (5.52727 iter/s, 18.0921s/100 iter), loss = 1.14737
I0815 16:30:51.004462  8764 solver.cpp:334]     Train net output #0: loss = 1.08021 (* 1 = 1.08021 loss)
I0815 16:30:51.008157  8764 sgd_solver.cpp:136] Iteration 115700, lr = 0.00276875, m = 0.9
I0815 16:31:08.021540  8764 solver.cpp:312] Iteration 115800 (5.8766 iter/s, 17.0166s/100 iter), loss = 1.75267
I0815 16:31:08.021667  8764 solver.cpp:334]     Train net output #0: loss = 1.79353 (* 1 = 1.79353 loss)
I0815 16:31:08.021675  8764 sgd_solver.cpp:136] Iteration 115800, lr = 0.0027625, m = 0.9
I0815 16:31:28.980455  8764 solver.cpp:312] Iteration 115900 (4.77137 iter/s, 20.9583s/100 iter), loss = 1.47975
I0815 16:31:28.980515  8764 solver.cpp:334]     Train net output #0: loss = 1.50996 (* 1 = 1.50996 loss)
I0815 16:31:28.980530  8764 sgd_solver.cpp:136] Iteration 115900, lr = 0.00275625, m = 0.9
I0815 16:31:39.472842  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:31:47.401012  8764 solver.cpp:363] Sparsity after update:
I0815 16:31:47.402860  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:31:47.402878  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:31:47.402890  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:31:47.402896  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:31:47.402902  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:31:47.402909  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:31:47.402914  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:31:47.402920  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:31:47.402926  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:31:47.402932  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:31:47.402938  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:31:47.402945  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:31:47.402951  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:31:47.402967  8764 solver.cpp:509] Iteration 116000, Testing net (#0)
I0815 16:32:16.898381  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.59047
I0815 16:32:16.898572  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.816174
I0815 16:32:16.898670  8764 solver.cpp:594]     Test net output #2: loss = 1.78854 (* 1 = 1.78854 loss)
I0815 16:32:16.898763  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.495s
I0815 16:32:17.156985  8764 solver.cpp:312] Iteration 116000 (2.07576 iter/s, 48.1752s/100 iter), loss = 1.12442
I0815 16:32:17.157013  8764 solver.cpp:334]     Train net output #0: loss = 1.18765 (* 1 = 1.18765 loss)
I0815 16:32:17.157019  8764 sgd_solver.cpp:136] Iteration 116000, lr = 0.00275, m = 0.9
I0815 16:32:34.448874  8764 solver.cpp:312] Iteration 116100 (5.78322 iter/s, 17.2914s/100 iter), loss = 1.56479
I0815 16:32:34.448904  8764 solver.cpp:334]     Train net output #0: loss = 1.32537 (* 1 = 1.32537 loss)
I0815 16:32:34.448909  8764 sgd_solver.cpp:136] Iteration 116100, lr = 0.00274375, m = 0.9
I0815 16:32:52.128892  8764 solver.cpp:312] Iteration 116200 (5.65626 iter/s, 17.6795s/100 iter), loss = 1.33268
I0815 16:32:52.128965  8764 solver.cpp:334]     Train net output #0: loss = 1.20308 (* 1 = 1.20308 loss)
I0815 16:32:52.128973  8764 sgd_solver.cpp:136] Iteration 116200, lr = 0.0027375, m = 0.9
I0815 16:33:10.899914  8764 solver.cpp:312] Iteration 116300 (5.32751 iter/s, 18.7705s/100 iter), loss = 1.8364
I0815 16:33:10.899976  8764 solver.cpp:334]     Train net output #0: loss = 1.93223 (* 1 = 1.93223 loss)
I0815 16:33:10.899993  8764 sgd_solver.cpp:136] Iteration 116300, lr = 0.00273125, m = 0.9
I0815 16:33:27.084623  8764 solver.cpp:312] Iteration 116400 (6.17884 iter/s, 16.1843s/100 iter), loss = 1.3644
I0815 16:33:27.084715  8764 solver.cpp:334]     Train net output #0: loss = 1.0715 (* 1 = 1.0715 loss)
I0815 16:33:27.084733  8764 sgd_solver.cpp:136] Iteration 116400, lr = 0.002725, m = 0.9
I0815 16:33:45.003770  8764 solver.cpp:312] Iteration 116500 (5.58078 iter/s, 17.9186s/100 iter), loss = 1.32778
I0815 16:33:45.003805  8764 solver.cpp:334]     Train net output #0: loss = 1.30482 (* 1 = 1.30482 loss)
I0815 16:33:45.003810  8764 sgd_solver.cpp:136] Iteration 116500, lr = 0.00271875, m = 0.9
I0815 16:34:03.292412  8764 solver.cpp:312] Iteration 116600 (5.46803 iter/s, 18.2881s/100 iter), loss = 1.30271
I0815 16:34:03.292629  8764 solver.cpp:334]     Train net output #0: loss = 1.27325 (* 1 = 1.27325 loss)
I0815 16:34:03.292660  8764 sgd_solver.cpp:136] Iteration 116600, lr = 0.0027125, m = 0.9
I0815 16:34:22.776865  8764 solver.cpp:312] Iteration 116700 (5.13244 iter/s, 19.4839s/100 iter), loss = 0.975956
I0815 16:34:22.776918  8764 solver.cpp:334]     Train net output #0: loss = 1.02228 (* 1 = 1.02228 loss)
I0815 16:34:22.776931  8764 sgd_solver.cpp:136] Iteration 116700, lr = 0.00270625, m = 0.9
I0815 16:34:39.706707  8764 solver.cpp:312] Iteration 116800 (5.90689 iter/s, 16.9294s/100 iter), loss = 1.53282
I0815 16:34:39.706804  8764 solver.cpp:334]     Train net output #0: loss = 1.44322 (* 1 = 1.44322 loss)
I0815 16:34:39.706827  8764 sgd_solver.cpp:136] Iteration 116800, lr = 0.0027, m = 0.9
I0815 16:34:57.693024  8764 solver.cpp:312] Iteration 116900 (5.55993 iter/s, 17.9858s/100 iter), loss = 1.38285
I0815 16:34:57.693092  8764 solver.cpp:334]     Train net output #0: loss = 1.36682 (* 1 = 1.36682 loss)
I0815 16:34:57.693111  8764 sgd_solver.cpp:136] Iteration 116900, lr = 0.00269375, m = 0.9
I0815 16:35:14.868151  8764 solver.cpp:363] Sparsity after update:
I0815 16:35:14.875260  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:35:14.875301  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:35:14.875327  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:35:14.875349  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:35:14.875366  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:35:14.875382  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:35:14.875397  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:35:14.875468  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:35:14.875484  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:35:14.875499  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:35:14.875516  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:35:14.875532  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:35:14.875547  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:35:15.125295  8764 solver.cpp:312] Iteration 117000 (5.73665 iter/s, 17.4318s/100 iter), loss = 1.49418
I0815 16:35:15.125362  8764 solver.cpp:334]     Train net output #0: loss = 1.41482 (* 1 = 1.41482 loss)
I0815 16:35:15.125382  8764 sgd_solver.cpp:136] Iteration 117000, lr = 0.0026875, m = 0.9
I0815 16:35:34.640763  8764 solver.cpp:312] Iteration 117100 (5.12428 iter/s, 19.5149s/100 iter), loss = 0.970678
I0815 16:35:34.640812  8764 solver.cpp:334]     Train net output #0: loss = 1.08113 (* 1 = 1.08113 loss)
I0815 16:35:34.640825  8764 sgd_solver.cpp:136] Iteration 117100, lr = 0.00268125, m = 0.9
I0815 16:35:51.578513  8764 solver.cpp:312] Iteration 117200 (5.90413 iter/s, 16.9373s/100 iter), loss = 1.57464
I0815 16:35:51.578601  8764 solver.cpp:334]     Train net output #0: loss = 1.55022 (* 1 = 1.55022 loss)
I0815 16:35:51.578610  8764 sgd_solver.cpp:136] Iteration 117200, lr = 0.002675, m = 0.9
I0815 16:36:07.847534  8764 solver.cpp:312] Iteration 117300 (6.14682 iter/s, 16.2686s/100 iter), loss = 1.44004
I0815 16:36:07.847611  8764 solver.cpp:334]     Train net output #0: loss = 1.73514 (* 1 = 1.73514 loss)
I0815 16:36:07.847633  8764 sgd_solver.cpp:136] Iteration 117300, lr = 0.00266875, m = 0.9
I0815 16:36:26.957701  8764 solver.cpp:312] Iteration 117400 (5.23296 iter/s, 19.1096s/100 iter), loss = 1.51405
I0815 16:36:26.957754  8764 solver.cpp:334]     Train net output #0: loss = 1.56155 (* 1 = 1.56155 loss)
I0815 16:36:26.957762  8764 sgd_solver.cpp:136] Iteration 117400, lr = 0.0026625, m = 0.9
I0815 16:36:44.220196  8764 solver.cpp:312] Iteration 117500 (5.79306 iter/s, 17.262s/100 iter), loss = 1.38498
I0815 16:36:44.220222  8764 solver.cpp:334]     Train net output #0: loss = 0.959721 (* 1 = 0.959721 loss)
I0815 16:36:44.220228  8764 sgd_solver.cpp:136] Iteration 117500, lr = 0.00265625, m = 0.9
I0815 16:37:01.964162  8764 solver.cpp:312] Iteration 117600 (5.63587 iter/s, 17.7435s/100 iter), loss = 1.12726
I0815 16:37:01.968156  8764 solver.cpp:334]     Train net output #0: loss = 1.02013 (* 1 = 1.02013 loss)
I0815 16:37:01.968183  8764 sgd_solver.cpp:136] Iteration 117600, lr = 0.00265, m = 0.9
I0815 16:37:19.740970  8764 solver.cpp:312] Iteration 117700 (5.62546 iter/s, 17.7763s/100 iter), loss = 1.29783
I0815 16:37:19.740994  8764 solver.cpp:334]     Train net output #0: loss = 1.21238 (* 1 = 1.21238 loss)
I0815 16:37:19.740999  8764 sgd_solver.cpp:136] Iteration 117700, lr = 0.00264375, m = 0.9
I0815 16:37:38.017568  8764 solver.cpp:312] Iteration 117800 (5.47165 iter/s, 18.276s/100 iter), loss = 1.29843
I0815 16:37:38.017964  8764 solver.cpp:334]     Train net output #0: loss = 1.09797 (* 1 = 1.09797 loss)
I0815 16:37:38.018024  8764 sgd_solver.cpp:136] Iteration 117800, lr = 0.0026375, m = 0.9
I0815 16:37:56.238483  8764 solver.cpp:312] Iteration 117900 (5.48834 iter/s, 18.2204s/100 iter), loss = 1.26849
I0815 16:37:56.238509  8764 solver.cpp:334]     Train net output #0: loss = 1.23976 (* 1 = 1.23976 loss)
I0815 16:37:56.238513  8764 sgd_solver.cpp:136] Iteration 117900, lr = 0.00263125, m = 0.9
I0815 16:38:13.615406  8764 solver.cpp:363] Sparsity after update:
I0815 16:38:13.621476  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:38:13.621490  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:38:13.621611  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:38:13.621685  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:38:13.621812  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:38:13.621882  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:38:13.621953  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:38:13.622020  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:38:13.622519  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:38:13.622596  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:38:13.622668  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:38:13.622738  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:38:13.622809  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:38:13.622931  8764 solver.cpp:509] Iteration 118000, Testing net (#0)
I0815 16:38:40.525478  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58347
I0815 16:38:40.525518  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815409
I0815 16:38:40.525529  8764 solver.cpp:594]     Test net output #2: loss = 1.80735 (* 1 = 1.80735 loss)
I0815 16:38:40.525563  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9019s
I0815 16:38:40.739624  8764 solver.cpp:312] Iteration 118000 (2.24719 iter/s, 44.4999s/100 iter), loss = 1.47512
I0815 16:38:40.739653  8764 solver.cpp:334]     Train net output #0: loss = 1.38109 (* 1 = 1.38109 loss)
I0815 16:38:40.739658  8764 sgd_solver.cpp:136] Iteration 118000, lr = 0.002625, m = 0.9
I0815 16:38:58.140447  8764 solver.cpp:312] Iteration 118100 (5.74701 iter/s, 17.4003s/100 iter), loss = 1.49844
I0815 16:38:58.140535  8764 solver.cpp:334]     Train net output #0: loss = 1.54564 (* 1 = 1.54564 loss)
I0815 16:38:58.140543  8764 sgd_solver.cpp:136] Iteration 118100, lr = 0.00261875, m = 0.9
I0815 16:39:15.630507  8764 solver.cpp:312] Iteration 118200 (5.71769 iter/s, 17.4896s/100 iter), loss = 1.60109
I0815 16:39:15.630535  8764 solver.cpp:334]     Train net output #0: loss = 1.26314 (* 1 = 1.26314 loss)
I0815 16:39:15.630542  8764 sgd_solver.cpp:136] Iteration 118200, lr = 0.0026125, m = 0.9
I0815 16:39:30.750941  8764 solver.cpp:312] Iteration 118300 (6.61375 iter/s, 15.12s/100 iter), loss = 1.46331
I0815 16:39:30.752180  8764 solver.cpp:334]     Train net output #0: loss = 1.67239 (* 1 = 1.67239 loss)
I0815 16:39:30.752198  8764 sgd_solver.cpp:136] Iteration 118300, lr = 0.00260625, m = 0.9
I0815 16:39:48.820966  8764 solver.cpp:312] Iteration 118400 (5.53418 iter/s, 18.0695s/100 iter), loss = 1.55364
I0815 16:39:48.821012  8764 solver.cpp:334]     Train net output #0: loss = 1.28088 (* 1 = 1.28088 loss)
I0815 16:39:48.821023  8764 sgd_solver.cpp:136] Iteration 118400, lr = 0.0026, m = 0.9
I0815 16:40:11.935317  8764 solver.cpp:312] Iteration 118500 (4.32644 iter/s, 23.1137s/100 iter), loss = 1.68218
I0815 16:40:11.935405  8764 solver.cpp:334]     Train net output #0: loss = 1.78796 (* 1 = 1.78796 loss)
I0815 16:40:11.935415  8764 sgd_solver.cpp:136] Iteration 118500, lr = 0.00259375, m = 0.9
I0815 16:40:33.341030  8764 solver.cpp:312] Iteration 118600 (4.67178 iter/s, 21.4051s/100 iter), loss = 1.19838
I0815 16:40:33.341073  8764 solver.cpp:334]     Train net output #0: loss = 1.37162 (* 1 = 1.37162 loss)
I0815 16:40:33.341084  8764 sgd_solver.cpp:136] Iteration 118600, lr = 0.0025875, m = 0.9
I0815 16:40:53.358410  8764 solver.cpp:312] Iteration 118700 (4.9958 iter/s, 20.0168s/100 iter), loss = 1.68358
I0815 16:40:53.358675  8764 solver.cpp:334]     Train net output #0: loss = 1.39555 (* 1 = 1.39555 loss)
I0815 16:40:53.358695  8764 sgd_solver.cpp:136] Iteration 118700, lr = 0.00258125, m = 0.9
I0815 16:41:11.592865  8764 solver.cpp:312] Iteration 118800 (5.48427 iter/s, 18.234s/100 iter), loss = 1.28401
I0815 16:41:11.592926  8764 solver.cpp:334]     Train net output #0: loss = 1.39824 (* 1 = 1.39824 loss)
I0815 16:41:11.592941  8764 sgd_solver.cpp:136] Iteration 118800, lr = 0.002575, m = 0.9
I0815 16:41:32.026688  8764 solver.cpp:312] Iteration 118900 (4.89398 iter/s, 20.4333s/100 iter), loss = 1.23373
I0815 16:41:32.026741  8764 solver.cpp:334]     Train net output #0: loss = 1.10099 (* 1 = 1.10099 loss)
I0815 16:41:32.026748  8764 sgd_solver.cpp:136] Iteration 118900, lr = 0.00256875, m = 0.9
I0815 16:41:51.404461  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:41:52.735671  8764 solver.cpp:363] Sparsity after update:
I0815 16:41:52.746937  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:41:52.746966  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:41:52.746980  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:41:52.746989  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:41:52.746997  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:41:52.747005  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:41:52.747014  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:41:52.747021  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:41:52.747030  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:41:52.747040  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:41:52.747052  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:41:52.747066  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:41:52.747073  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:41:52.948786  8764 solver.cpp:312] Iteration 119000 (4.77977 iter/s, 20.9215s/100 iter), loss = 1.39004
I0815 16:41:52.957671  8764 solver.cpp:334]     Train net output #0: loss = 1.35477 (* 1 = 1.35477 loss)
I0815 16:41:52.957693  8764 sgd_solver.cpp:136] Iteration 119000, lr = 0.0025625, m = 0.9
I0815 16:42:14.021688  8764 solver.cpp:312] Iteration 119100 (4.74556 iter/s, 21.0723s/100 iter), loss = 1.49827
I0815 16:42:14.021809  8764 solver.cpp:334]     Train net output #0: loss = 1.90254 (* 1 = 1.90254 loss)
I0815 16:42:14.021829  8764 sgd_solver.cpp:136] Iteration 119100, lr = 0.00255625, m = 0.9
I0815 16:42:33.367374  8764 solver.cpp:312] Iteration 119200 (5.16925 iter/s, 19.3452s/100 iter), loss = 0.821491
I0815 16:42:33.367403  8764 solver.cpp:334]     Train net output #0: loss = 0.602199 (* 1 = 0.602199 loss)
I0815 16:42:33.367408  8764 sgd_solver.cpp:136] Iteration 119200, lr = 0.00255, m = 0.9
I0815 16:42:54.635284  8764 solver.cpp:312] Iteration 119300 (4.70205 iter/s, 21.2673s/100 iter), loss = 1.13889
I0815 16:42:54.635397  8764 solver.cpp:334]     Train net output #0: loss = 1.29693 (* 1 = 1.29693 loss)
I0815 16:42:54.635411  8764 sgd_solver.cpp:136] Iteration 119300, lr = 0.00254375, m = 0.9
I0815 16:43:12.934885  8764 solver.cpp:312] Iteration 119400 (5.46475 iter/s, 18.2991s/100 iter), loss = 1.38219
I0815 16:43:12.934911  8764 solver.cpp:334]     Train net output #0: loss = 1.24938 (* 1 = 1.24938 loss)
I0815 16:43:12.934916  8764 sgd_solver.cpp:136] Iteration 119400, lr = 0.0025375, m = 0.9
I0815 16:43:34.931947  8764 solver.cpp:312] Iteration 119500 (4.54619 iter/s, 21.9965s/100 iter), loss = 1.39768
I0815 16:43:34.932035  8764 solver.cpp:334]     Train net output #0: loss = 1.13036 (* 1 = 1.13036 loss)
I0815 16:43:34.932052  8764 sgd_solver.cpp:136] Iteration 119500, lr = 0.00253125, m = 0.9
I0815 16:43:58.376698  8764 solver.cpp:312] Iteration 119600 (4.26547 iter/s, 23.4441s/100 iter), loss = 1.25854
I0815 16:43:58.376796  8764 solver.cpp:334]     Train net output #0: loss = 1.43815 (* 1 = 1.43815 loss)
I0815 16:43:58.376821  8764 sgd_solver.cpp:136] Iteration 119600, lr = 0.002525, m = 0.9
I0815 16:44:19.809144  8764 solver.cpp:312] Iteration 119700 (4.66595 iter/s, 21.4319s/100 iter), loss = 1.24189
I0815 16:44:19.809242  8764 solver.cpp:334]     Train net output #0: loss = 1.05919 (* 1 = 1.05919 loss)
I0815 16:44:19.809257  8764 sgd_solver.cpp:136] Iteration 119700, lr = 0.00251875, m = 0.9
I0815 16:44:40.490239  8764 solver.cpp:312] Iteration 119800 (4.83547 iter/s, 20.6805s/100 iter), loss = 1.59107
I0815 16:44:40.490290  8764 solver.cpp:334]     Train net output #0: loss = 1.26387 (* 1 = 1.26387 loss)
I0815 16:44:40.490303  8764 sgd_solver.cpp:136] Iteration 119800, lr = 0.0025125, m = 0.9
I0815 16:45:00.260637  8764 solver.cpp:312] Iteration 119900 (5.05821 iter/s, 19.7699s/100 iter), loss = 1.30199
I0815 16:45:00.260697  8764 solver.cpp:334]     Train net output #0: loss = 1.17324 (* 1 = 1.17324 loss)
I0815 16:45:00.260704  8764 sgd_solver.cpp:136] Iteration 119900, lr = 0.00250625, m = 0.9
I0815 16:45:18.624758  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_120000.caffemodel
I0815 16:45:18.666470  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_120000.solverstate
I0815 16:45:18.673167  8764 solver.cpp:363] Sparsity after update:
I0815 16:45:18.675505  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:45:18.675693  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:45:18.675791  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:45:18.675879  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:45:18.675966  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:45:18.676065  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:45:18.676167  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:45:18.676255  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:45:18.676343  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:45:18.676434  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:45:18.676519  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:45:18.676635  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:45:18.676736  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:45:18.676862  8764 solver.cpp:509] Iteration 120000, Testing net (#0)
I0815 16:45:51.235754  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.587235
I0815 16:45:51.235857  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815879
I0815 16:45:51.235877  8764 solver.cpp:594]     Test net output #2: loss = 1.80091 (* 1 = 1.80091 loss)
I0815 16:45:51.235921  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.5582s
I0815 16:45:51.473497  8764 solver.cpp:312] Iteration 120000 (1.95269 iter/s, 51.2115s/100 iter), loss = 1.4675
I0815 16:45:51.473536  8764 solver.cpp:334]     Train net output #0: loss = 1.52106 (* 1 = 1.52106 loss)
I0815 16:45:51.473542  8764 sgd_solver.cpp:136] Iteration 120000, lr = 0.0025, m = 0.9
I0815 16:46:10.122344  8764 solver.cpp:312] Iteration 120100 (5.36241 iter/s, 18.6483s/100 iter), loss = 1.46773
I0815 16:46:10.122370  8764 solver.cpp:334]     Train net output #0: loss = 1.62554 (* 1 = 1.62554 loss)
I0815 16:46:10.122376  8764 sgd_solver.cpp:136] Iteration 120100, lr = 0.00249375, m = 0.9
I0815 16:46:29.465642  8764 solver.cpp:312] Iteration 120200 (5.16989 iter/s, 19.3428s/100 iter), loss = 1.52636
I0815 16:46:29.465762  8764 solver.cpp:334]     Train net output #0: loss = 1.38008 (* 1 = 1.38008 loss)
I0815 16:46:29.465770  8764 sgd_solver.cpp:136] Iteration 120200, lr = 0.0024875, m = 0.9
I0815 16:46:46.704790  8764 solver.cpp:312] Iteration 120300 (5.80091 iter/s, 17.2387s/100 iter), loss = 1.77569
I0815 16:46:46.704988  8764 solver.cpp:334]     Train net output #0: loss = 0.898904 (* 1 = 0.898904 loss)
I0815 16:46:46.705129  8764 sgd_solver.cpp:136] Iteration 120300, lr = 0.00248125, m = 0.9
I0815 16:47:07.877898  8764 solver.cpp:312] Iteration 120400 (4.7231 iter/s, 21.1725s/100 iter), loss = 1.25235
I0815 16:47:07.877959  8764 solver.cpp:334]     Train net output #0: loss = 1.00787 (* 1 = 1.00787 loss)
I0815 16:47:07.877966  8764 sgd_solver.cpp:136] Iteration 120400, lr = 0.002475, m = 0.9
I0815 16:47:27.694578  8764 solver.cpp:312] Iteration 120500 (5.04641 iter/s, 19.8161s/100 iter), loss = 1.50518
I0815 16:47:27.694747  8764 solver.cpp:334]     Train net output #0: loss = 1.31917 (* 1 = 1.31917 loss)
I0815 16:47:27.694788  8764 sgd_solver.cpp:136] Iteration 120500, lr = 0.00246875, m = 0.9
I0815 16:47:43.641980  8764 solver.cpp:312] Iteration 120600 (6.27078 iter/s, 15.947s/100 iter), loss = 1.30507
I0815 16:47:43.642060  8764 solver.cpp:334]     Train net output #0: loss = 1.31046 (* 1 = 1.31046 loss)
I0815 16:47:43.642073  8764 sgd_solver.cpp:136] Iteration 120600, lr = 0.0024625, m = 0.9
I0815 16:48:04.690767  8764 solver.cpp:312] Iteration 120700 (4.751 iter/s, 21.0482s/100 iter), loss = 1.19732
I0815 16:48:04.690824  8764 solver.cpp:334]     Train net output #0: loss = 1.01484 (* 1 = 1.01484 loss)
I0815 16:48:04.690830  8764 sgd_solver.cpp:136] Iteration 120700, lr = 0.00245625, m = 0.9
I0815 16:48:24.615062  8764 solver.cpp:312] Iteration 120800 (5.01914 iter/s, 19.9237s/100 iter), loss = 1.59322
I0815 16:48:24.615198  8764 solver.cpp:334]     Train net output #0: loss = 1.38399 (* 1 = 1.38399 loss)
I0815 16:48:24.615227  8764 sgd_solver.cpp:136] Iteration 120800, lr = 0.00245, m = 0.9
I0815 16:48:44.378449  8764 solver.cpp:312] Iteration 120900 (5.06 iter/s, 19.7628s/100 iter), loss = 0.938661
I0815 16:48:44.378515  8764 solver.cpp:334]     Train net output #0: loss = 1.01031 (* 1 = 1.01031 loss)
I0815 16:48:44.378532  8764 sgd_solver.cpp:136] Iteration 120900, lr = 0.00244375, m = 0.9
I0815 16:49:01.199909  8764 solver.cpp:363] Sparsity after update:
I0815 16:49:01.210317  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:49:01.210350  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:49:01.210371  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:49:01.210381  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:49:01.210389  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:49:01.210398  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:49:01.210407  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:49:01.210415  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:49:01.210424  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:49:01.210433  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:49:01.210443  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:49:01.210450  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:49:01.210459  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:49:01.362154  8764 solver.cpp:312] Iteration 121000 (5.88816 iter/s, 16.9832s/100 iter), loss = 1.323
I0815 16:49:01.362182  8764 solver.cpp:334]     Train net output #0: loss = 1.03387 (* 1 = 1.03387 loss)
I0815 16:49:01.362188  8764 sgd_solver.cpp:136] Iteration 121000, lr = 0.0024375, m = 0.9
I0815 16:49:21.489238  8764 solver.cpp:312] Iteration 121100 (4.96857 iter/s, 20.1265s/100 iter), loss = 1.43609
I0815 16:49:21.489274  8764 solver.cpp:334]     Train net output #0: loss = 1.41473 (* 1 = 1.41473 loss)
I0815 16:49:21.489279  8764 sgd_solver.cpp:136] Iteration 121100, lr = 0.00243125, m = 0.9
I0815 16:49:41.729523  8764 solver.cpp:312] Iteration 121200 (4.94078 iter/s, 20.2397s/100 iter), loss = 1.63406
I0815 16:49:41.729578  8764 solver.cpp:334]     Train net output #0: loss = 1.47157 (* 1 = 1.47157 loss)
I0815 16:49:41.729583  8764 sgd_solver.cpp:136] Iteration 121200, lr = 0.002425, m = 0.9
I0815 16:49:58.368386  8764 solver.cpp:312] Iteration 121300 (6.01019 iter/s, 16.6384s/100 iter), loss = 1.43773
I0815 16:49:58.368412  8764 solver.cpp:334]     Train net output #0: loss = 1.02348 (* 1 = 1.02348 loss)
I0815 16:49:58.368418  8764 sgd_solver.cpp:136] Iteration 121300, lr = 0.00241875, m = 0.9
I0815 16:50:16.628170  8764 solver.cpp:312] Iteration 121400 (5.47667 iter/s, 18.2593s/100 iter), loss = 1.36189
I0815 16:50:16.632172  8764 solver.cpp:334]     Train net output #0: loss = 0.84588 (* 1 = 0.84588 loss)
I0815 16:50:16.632189  8764 sgd_solver.cpp:136] Iteration 121400, lr = 0.0024125, m = 0.9
I0815 16:50:36.053176  8764 solver.cpp:312] Iteration 121500 (5.14814 iter/s, 19.4245s/100 iter), loss = 1.53727
I0815 16:50:36.053222  8764 solver.cpp:334]     Train net output #0: loss = 1.56684 (* 1 = 1.56684 loss)
I0815 16:50:36.053236  8764 sgd_solver.cpp:136] Iteration 121500, lr = 0.00240625, m = 0.9
I0815 16:50:53.541620  8764 solver.cpp:312] Iteration 121600 (5.71822 iter/s, 17.488s/100 iter), loss = 1.56711
I0815 16:50:53.541677  8764 solver.cpp:334]     Train net output #0: loss = 1.32909 (* 1 = 1.32909 loss)
I0815 16:50:53.541683  8764 sgd_solver.cpp:136] Iteration 121600, lr = 0.0024, m = 0.9
I0815 16:51:11.825129  8764 solver.cpp:312] Iteration 121700 (5.46956 iter/s, 18.283s/100 iter), loss = 1.41876
I0815 16:51:11.825155  8764 solver.cpp:334]     Train net output #0: loss = 1.41382 (* 1 = 1.41382 loss)
I0815 16:51:11.825160  8764 sgd_solver.cpp:136] Iteration 121700, lr = 0.00239375, m = 0.9
I0815 16:51:29.925391  8764 solver.cpp:312] Iteration 121800 (5.52494 iter/s, 18.0998s/100 iter), loss = 1.31249
I0815 16:51:29.925493  8764 solver.cpp:334]     Train net output #0: loss = 1.24379 (* 1 = 1.24379 loss)
I0815 16:51:29.925513  8764 sgd_solver.cpp:136] Iteration 121800, lr = 0.0023875, m = 0.9
I0815 16:51:49.532654  8764 solver.cpp:312] Iteration 121900 (5.10029 iter/s, 19.6067s/100 iter), loss = 1.86534
I0815 16:51:49.532676  8764 solver.cpp:334]     Train net output #0: loss = 1.74769 (* 1 = 1.74769 loss)
I0815 16:51:49.532680  8764 sgd_solver.cpp:136] Iteration 121900, lr = 0.00238125, m = 0.9
I0815 16:52:09.011860  8764 solver.cpp:363] Sparsity after update:
I0815 16:52:09.016762  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:52:09.016808  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:52:09.016842  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:52:09.016863  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:52:09.016880  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:52:09.016897  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:52:09.016914  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:52:09.016933  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:52:09.016952  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:52:09.016970  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:52:09.016988  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:52:09.017004  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:52:09.017019  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:52:09.017045  8764 solver.cpp:509] Iteration 122000, Testing net (#0)
I0815 16:52:27.728909  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:52:39.974268  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583588
I0815 16:52:39.974483  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815997
I0815 16:52:39.974512  8764 solver.cpp:594]     Test net output #2: loss = 1.80454 (* 1 = 1.80454 loss)
I0815 16:52:39.974576  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.9567s
I0815 16:52:40.234491  8764 solver.cpp:312] Iteration 122000 (1.97237 iter/s, 50.7005s/100 iter), loss = 1.23455
I0815 16:52:40.234550  8764 solver.cpp:334]     Train net output #0: loss = 1.29382 (* 1 = 1.29382 loss)
I0815 16:52:40.234567  8764 sgd_solver.cpp:136] Iteration 122000, lr = 0.002375, m = 0.9
I0815 16:52:58.608296  8764 solver.cpp:312] Iteration 122100 (5.44268 iter/s, 18.3733s/100 iter), loss = 1.44568
I0815 16:52:58.608321  8764 solver.cpp:334]     Train net output #0: loss = 1.35227 (* 1 = 1.35227 loss)
I0815 16:52:58.608327  8764 sgd_solver.cpp:136] Iteration 122100, lr = 0.00236875, m = 0.9
I0815 16:53:16.711833  8764 solver.cpp:312] Iteration 122200 (5.52394 iter/s, 18.103s/100 iter), loss = 1.10274
I0815 16:53:16.711895  8764 solver.cpp:334]     Train net output #0: loss = 1.0669 (* 1 = 1.0669 loss)
I0815 16:53:16.711901  8764 sgd_solver.cpp:136] Iteration 122200, lr = 0.0023625, m = 0.9
I0815 16:53:32.852174  8764 solver.cpp:312] Iteration 122300 (6.19583 iter/s, 16.1399s/100 iter), loss = 1.28606
I0815 16:53:32.852232  8764 solver.cpp:334]     Train net output #0: loss = 1.24172 (* 1 = 1.24172 loss)
I0815 16:53:32.852244  8764 sgd_solver.cpp:136] Iteration 122300, lr = 0.00235625, m = 0.9
I0815 16:53:51.769840  8764 solver.cpp:312] Iteration 122400 (5.28621 iter/s, 18.9171s/100 iter), loss = 1.34715
I0815 16:53:51.770097  8764 solver.cpp:334]     Train net output #0: loss = 1.43305 (* 1 = 1.43305 loss)
I0815 16:53:51.770117  8764 sgd_solver.cpp:136] Iteration 122400, lr = 0.00235, m = 0.9
I0815 16:54:10.961478  8764 solver.cpp:312] Iteration 122500 (5.21075 iter/s, 19.1911s/100 iter), loss = 1.42986
I0815 16:54:10.961510  8764 solver.cpp:334]     Train net output #0: loss = 1.4375 (* 1 = 1.4375 loss)
I0815 16:54:10.961518  8764 sgd_solver.cpp:136] Iteration 122500, lr = 0.00234375, m = 0.9
I0815 16:54:29.465615  8764 solver.cpp:312] Iteration 122600 (5.40435 iter/s, 18.5036s/100 iter), loss = 1.51816
I0815 16:54:29.465689  8764 solver.cpp:334]     Train net output #0: loss = 1.56053 (* 1 = 1.56053 loss)
I0815 16:54:29.465697  8764 sgd_solver.cpp:136] Iteration 122600, lr = 0.0023375, m = 0.9
I0815 16:54:46.873265  8764 solver.cpp:312] Iteration 122700 (5.74476 iter/s, 17.4072s/100 iter), loss = 1.34204
I0815 16:54:46.873288  8764 solver.cpp:334]     Train net output #0: loss = 1.38463 (* 1 = 1.38463 loss)
I0815 16:54:46.873293  8764 sgd_solver.cpp:136] Iteration 122700, lr = 0.00233125, m = 0.9
I0815 16:55:03.527963  8764 solver.cpp:312] Iteration 122800 (6.00448 iter/s, 16.6542s/100 iter), loss = 1.57483
I0815 16:55:03.528023  8764 solver.cpp:334]     Train net output #0: loss = 1.70635 (* 1 = 1.70635 loss)
I0815 16:55:03.528028  8764 sgd_solver.cpp:136] Iteration 122800, lr = 0.002325, m = 0.9
I0815 16:55:21.731648  8764 solver.cpp:312] Iteration 122900 (5.49354 iter/s, 18.2032s/100 iter), loss = 1.53598
I0815 16:55:21.731673  8764 solver.cpp:334]     Train net output #0: loss = 1.65499 (* 1 = 1.65499 loss)
I0815 16:55:21.731676  8764 sgd_solver.cpp:136] Iteration 122900, lr = 0.00231875, m = 0.9
I0815 16:55:39.265154  8764 solver.cpp:363] Sparsity after update:
I0815 16:55:39.275604  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:55:39.275630  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:55:39.275646  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:55:39.275655  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:55:39.275665  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:55:39.275674  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:55:39.275683  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:55:39.275693  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:55:39.275702  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:55:39.275712  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:55:39.275722  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:55:39.275730  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:55:39.275740  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:55:39.408109  8764 solver.cpp:312] Iteration 123000 (5.6574 iter/s, 17.676s/100 iter), loss = 1.31762
I0815 16:55:39.408161  8764 solver.cpp:334]     Train net output #0: loss = 1.07071 (* 1 = 1.07071 loss)
I0815 16:55:39.408181  8764 sgd_solver.cpp:136] Iteration 123000, lr = 0.0023125, m = 0.9
I0815 16:55:56.402187  8764 solver.cpp:312] Iteration 123100 (5.88457 iter/s, 16.9936s/100 iter), loss = 1.37889
I0815 16:55:56.402215  8764 solver.cpp:334]     Train net output #0: loss = 1.65764 (* 1 = 1.65764 loss)
I0815 16:55:56.402221  8764 sgd_solver.cpp:136] Iteration 123100, lr = 0.00230625, m = 0.9
I0815 16:56:12.264235  8764 solver.cpp:312] Iteration 123200 (6.30455 iter/s, 15.8616s/100 iter), loss = 1.69522
I0815 16:56:12.264420  8764 solver.cpp:334]     Train net output #0: loss = 1.76185 (* 1 = 1.76185 loss)
I0815 16:56:12.264468  8764 sgd_solver.cpp:136] Iteration 123200, lr = 0.0023, m = 0.9
I0815 16:56:29.528717  8764 solver.cpp:312] Iteration 123300 (5.79239 iter/s, 17.264s/100 iter), loss = 1.48133
I0815 16:56:29.528740  8764 solver.cpp:334]     Train net output #0: loss = 1.2064 (* 1 = 1.2064 loss)
I0815 16:56:29.528745  8764 sgd_solver.cpp:136] Iteration 123300, lr = 0.00229375, m = 0.9
I0815 16:56:46.258316  8764 solver.cpp:312] Iteration 123400 (5.97759 iter/s, 16.7291s/100 iter), loss = 1.3797
I0815 16:56:46.258374  8764 solver.cpp:334]     Train net output #0: loss = 1.43731 (* 1 = 1.43731 loss)
I0815 16:56:46.258381  8764 sgd_solver.cpp:136] Iteration 123400, lr = 0.0022875, m = 0.9
I0815 16:57:02.679810  8764 solver.cpp:312] Iteration 123500 (6.08975 iter/s, 16.421s/100 iter), loss = 1.36917
I0815 16:57:02.679838  8764 solver.cpp:334]     Train net output #0: loss = 1.00788 (* 1 = 1.00788 loss)
I0815 16:57:02.679843  8764 sgd_solver.cpp:136] Iteration 123500, lr = 0.00228125, m = 0.9
I0815 16:57:21.611925  8764 solver.cpp:312] Iteration 123600 (5.28218 iter/s, 18.9316s/100 iter), loss = 1.35995
I0815 16:57:21.612009  8764 solver.cpp:334]     Train net output #0: loss = 1.51726 (* 1 = 1.51726 loss)
I0815 16:57:21.612022  8764 sgd_solver.cpp:136] Iteration 123600, lr = 0.002275, m = 0.9
I0815 16:57:43.186204  8764 solver.cpp:312] Iteration 123700 (4.63528 iter/s, 21.5737s/100 iter), loss = 1.60536
I0815 16:57:43.186256  8764 solver.cpp:334]     Train net output #0: loss = 1.78078 (* 1 = 1.78078 loss)
I0815 16:57:43.186271  8764 sgd_solver.cpp:136] Iteration 123700, lr = 0.00226875, m = 0.9
I0815 16:58:05.750020  8764 solver.cpp:312] Iteration 123800 (4.43199 iter/s, 22.5632s/100 iter), loss = 1.29464
I0815 16:58:05.750123  8764 solver.cpp:334]     Train net output #0: loss = 1.45853 (* 1 = 1.45853 loss)
I0815 16:58:05.750135  8764 sgd_solver.cpp:136] Iteration 123800, lr = 0.0022625, m = 0.9
I0815 16:58:23.996311  8764 solver.cpp:312] Iteration 123900 (5.48072 iter/s, 18.2458s/100 iter), loss = 1.52835
I0815 16:58:23.996379  8764 solver.cpp:334]     Train net output #0: loss = 1.72224 (* 1 = 1.72224 loss)
I0815 16:58:23.996402  8764 sgd_solver.cpp:136] Iteration 123900, lr = 0.00225625, m = 0.9
I0815 16:58:47.462265  8764 solver.cpp:363] Sparsity after update:
I0815 16:58:47.465039  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:58:47.465060  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:58:47.465077  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:58:47.465085  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:58:47.465092  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:58:47.465101  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:58:47.465106  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:58:47.465113  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:58:47.465121  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:58:47.465127  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:58:47.465134  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:58:47.465140  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:58:47.465147  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:58:47.465168  8764 solver.cpp:509] Iteration 124000, Testing net (#0)
I0815 16:59:17.309409  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585883
I0815 16:59:17.309458  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814879
I0815 16:59:17.309475  8764 solver.cpp:594]     Test net output #2: loss = 1.80197 (* 1 = 1.80197 loss)
I0815 16:59:17.309587  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.8436s
I0815 16:59:17.688177  8764 solver.cpp:312] Iteration 124000 (1.86253 iter/s, 53.6904s/100 iter), loss = 1.60112
I0815 16:59:17.688356  8764 solver.cpp:334]     Train net output #0: loss = 1.61323 (* 1 = 1.61323 loss)
I0815 16:59:17.688393  8764 sgd_solver.cpp:136] Iteration 124000, lr = 0.00225, m = 0.9
I0815 16:59:33.651476  8764 solver.cpp:312] Iteration 124100 (6.26454 iter/s, 15.9629s/100 iter), loss = 1.41488
I0815 16:59:33.651521  8764 solver.cpp:334]     Train net output #0: loss = 1.20451 (* 1 = 1.20451 loss)
I0815 16:59:33.651531  8764 sgd_solver.cpp:136] Iteration 124100, lr = 0.00224375, m = 0.9
I0815 16:59:53.134361  8764 solver.cpp:312] Iteration 124200 (5.13285 iter/s, 19.4823s/100 iter), loss = 1.53534
I0815 16:59:53.134441  8764 solver.cpp:334]     Train net output #0: loss = 1.24198 (* 1 = 1.24198 loss)
I0815 16:59:53.134454  8764 sgd_solver.cpp:136] Iteration 124200, lr = 0.0022375, m = 0.9
I0815 17:00:13.985292  8764 solver.cpp:312] Iteration 124300 (4.79608 iter/s, 20.8504s/100 iter), loss = 1.35472
I0815 17:00:13.985366  8764 solver.cpp:334]     Train net output #0: loss = 1.49106 (* 1 = 1.49106 loss)
I0815 17:00:13.985386  8764 sgd_solver.cpp:136] Iteration 124300, lr = 0.00223125, m = 0.9
I0815 17:00:34.017964  8764 solver.cpp:312] Iteration 124400 (4.99198 iter/s, 20.0321s/100 iter), loss = 1.12927
I0815 17:00:34.018193  8764 solver.cpp:334]     Train net output #0: loss = 1.12176 (* 1 = 1.12176 loss)
I0815 17:00:34.018206  8764 sgd_solver.cpp:136] Iteration 124400, lr = 0.002225, m = 0.9
I0815 17:00:51.444598  8764 solver.cpp:312] Iteration 124500 (5.73852 iter/s, 17.4261s/100 iter), loss = 1.5884
I0815 17:00:51.444795  8764 solver.cpp:334]     Train net output #0: loss = 1.32972 (* 1 = 1.32972 loss)
I0815 17:00:51.444834  8764 sgd_solver.cpp:136] Iteration 124500, lr = 0.00221875, m = 0.9
I0815 17:01:09.807231  8764 solver.cpp:312] Iteration 124600 (5.44599 iter/s, 18.3621s/100 iter), loss = 1.34828
I0815 17:01:09.807293  8764 solver.cpp:334]     Train net output #0: loss = 1.22787 (* 1 = 1.22787 loss)
I0815 17:01:09.807301  8764 sgd_solver.cpp:136] Iteration 124600, lr = 0.0022125, m = 0.9
I0815 17:01:32.710289  8764 solver.cpp:312] Iteration 124700 (4.36635 iter/s, 22.9024s/100 iter), loss = 1.40382
I0815 17:01:32.710312  8764 solver.cpp:334]     Train net output #0: loss = 1.252 (* 1 = 1.252 loss)
I0815 17:01:32.710317  8764 sgd_solver.cpp:136] Iteration 124700, lr = 0.00220625, m = 0.9
I0815 17:01:50.270531  8764 solver.cpp:312] Iteration 124800 (5.69484 iter/s, 17.5598s/100 iter), loss = 1.23158
I0815 17:01:50.270778  8764 solver.cpp:334]     Train net output #0: loss = 1.25457 (* 1 = 1.25457 loss)
I0815 17:01:50.270794  8764 sgd_solver.cpp:136] Iteration 124800, lr = 0.0022, m = 0.9
I0815 17:02:09.040791  8764 solver.cpp:312] Iteration 124900 (5.32772 iter/s, 18.7697s/100 iter), loss = 1.53368
I0815 17:02:09.040814  8764 solver.cpp:334]     Train net output #0: loss = 1.68444 (* 1 = 1.68444 loss)
I0815 17:02:09.040820  8764 sgd_solver.cpp:136] Iteration 124900, lr = 0.00219375, m = 0.9
I0815 17:02:28.300740  8764 solver.cpp:363] Sparsity after update:
I0815 17:02:28.341627  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:02:28.341665  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:02:28.341687  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:02:28.341696  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:02:28.341704  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:02:28.341711  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:02:28.341718  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:02:28.341725  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:02:28.341732  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:02:28.341738  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:02:28.341745  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:02:28.341753  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:02:28.341759  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:02:28.563820  8764 solver.cpp:312] Iteration 125000 (5.1223 iter/s, 19.5225s/100 iter), loss = 1.68212
I0815 17:02:28.563891  8764 solver.cpp:334]     Train net output #0: loss = 1.33714 (* 1 = 1.33714 loss)
I0815 17:02:28.563910  8764 sgd_solver.cpp:136] Iteration 125000, lr = 0.0021875, m = 0.9
I0815 17:02:48.677747  8764 solver.cpp:312] Iteration 125100 (4.97182 iter/s, 20.1134s/100 iter), loss = 1.36924
I0815 17:02:48.677927  8764 solver.cpp:334]     Train net output #0: loss = 1.18669 (* 1 = 1.18669 loss)
I0815 17:02:48.678002  8764 sgd_solver.cpp:136] Iteration 125100, lr = 0.00218125, m = 0.9
I0815 17:03:05.698107  8764 solver.cpp:312] Iteration 125200 (5.87548 iter/s, 17.0199s/100 iter), loss = 1.15267
I0815 17:03:05.698168  8764 solver.cpp:334]     Train net output #0: loss = 1.28245 (* 1 = 1.28245 loss)
I0815 17:03:05.698174  8764 sgd_solver.cpp:136] Iteration 125200, lr = 0.002175, m = 0.9
I0815 17:03:25.089874  8764 solver.cpp:312] Iteration 125300 (5.15697 iter/s, 19.3912s/100 iter), loss = 1.56301
I0815 17:03:25.089932  8764 solver.cpp:334]     Train net output #0: loss = 1.42842 (* 1 = 1.42842 loss)
I0815 17:03:25.089951  8764 sgd_solver.cpp:136] Iteration 125300, lr = 0.00216875, m = 0.9
I0815 17:03:46.719861  8764 solver.cpp:312] Iteration 125400 (4.62334 iter/s, 21.6294s/100 iter), loss = 1.5784
I0815 17:03:46.719938  8764 solver.cpp:334]     Train net output #0: loss = 1.4181 (* 1 = 1.4181 loss)
I0815 17:03:46.719951  8764 sgd_solver.cpp:136] Iteration 125400, lr = 0.0021625, m = 0.9
I0815 17:04:06.789422  8764 solver.cpp:312] Iteration 125500 (4.98281 iter/s, 20.069s/100 iter), loss = 1.50448
I0815 17:04:06.789453  8764 solver.cpp:334]     Train net output #0: loss = 1.45248 (* 1 = 1.45248 loss)
I0815 17:04:06.789458  8764 sgd_solver.cpp:136] Iteration 125500, lr = 0.00215625, m = 0.9
I0815 17:04:26.275662  8764 solver.cpp:312] Iteration 125600 (5.13197 iter/s, 19.4857s/100 iter), loss = 1.70883
I0815 17:04:26.275744  8764 solver.cpp:334]     Train net output #0: loss = 1.77564 (* 1 = 1.77564 loss)
I0815 17:04:26.275759  8764 sgd_solver.cpp:136] Iteration 125600, lr = 0.00215, m = 0.9
I0815 17:04:45.636158  8764 solver.cpp:312] Iteration 125700 (5.1653 iter/s, 19.36s/100 iter), loss = 1.32027
I0815 17:04:45.636229  8764 solver.cpp:334]     Train net output #0: loss = 1.37896 (* 1 = 1.37896 loss)
I0815 17:04:45.636248  8764 sgd_solver.cpp:136] Iteration 125700, lr = 0.00214375, m = 0.9
I0815 17:05:06.461311  8764 solver.cpp:312] Iteration 125800 (4.80202 iter/s, 20.8246s/100 iter), loss = 1.22197
I0815 17:05:06.463407  8764 solver.cpp:334]     Train net output #0: loss = 1.45306 (* 1 = 1.45306 loss)
I0815 17:05:06.463515  8764 sgd_solver.cpp:136] Iteration 125800, lr = 0.0021375, m = 0.9
I0815 17:05:23.392499  8764 solver.cpp:312] Iteration 125900 (5.90642 iter/s, 16.9307s/100 iter), loss = 1.44101
I0815 17:05:23.392561  8764 solver.cpp:334]     Train net output #0: loss = 1.50898 (* 1 = 1.50898 loss)
I0815 17:05:23.392580  8764 sgd_solver.cpp:136] Iteration 125900, lr = 0.00213125, m = 0.9
I0815 17:05:43.426791  8764 solver.cpp:363] Sparsity after update:
I0815 17:05:43.432145  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:05:43.432164  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:05:43.432176  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:05:43.432180  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:05:43.432183  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:05:43.432188  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:05:43.432191  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:05:43.432195  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:05:43.432199  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:05:43.432202  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:05:43.432205  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:05:43.432209  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:05:43.432212  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:05:43.432225  8764 solver.cpp:509] Iteration 126000, Testing net (#0)
I0815 17:05:48.058756  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:06:16.505400  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.589764
I0815 17:06:16.505456  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819526
I0815 17:06:16.505465  8764 solver.cpp:594]     Test net output #2: loss = 1.79839 (* 1 = 1.79839 loss)
I0815 17:06:16.505486  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.0724s
I0815 17:06:16.735556  8764 solver.cpp:312] Iteration 126000 (1.87471 iter/s, 53.3416s/100 iter), loss = 1.82476
I0815 17:06:16.735600  8764 solver.cpp:334]     Train net output #0: loss = 1.64501 (* 1 = 1.64501 loss)
I0815 17:06:16.735615  8764 sgd_solver.cpp:136] Iteration 126000, lr = 0.002125, m = 0.9
I0815 17:06:34.124151  8764 solver.cpp:312] Iteration 126100 (5.75106 iter/s, 17.3881s/100 iter), loss = 1.20154
I0815 17:06:34.124320  8764 solver.cpp:334]     Train net output #0: loss = 1.07767 (* 1 = 1.07767 loss)
I0815 17:06:34.124348  8764 sgd_solver.cpp:136] Iteration 126100, lr = 0.00211875, m = 0.9
I0815 17:06:53.305510  8764 solver.cpp:312] Iteration 126200 (5.21354 iter/s, 19.1808s/100 iter), loss = 1.45512
I0815 17:06:53.305625  8764 solver.cpp:334]     Train net output #0: loss = 1.59632 (* 1 = 1.59632 loss)
I0815 17:06:53.305641  8764 sgd_solver.cpp:136] Iteration 126200, lr = 0.0021125, m = 0.9
I0815 17:07:13.596163  8764 solver.cpp:312] Iteration 126300 (4.92851 iter/s, 20.2901s/100 iter), loss = 1.68595
I0815 17:07:13.596236  8764 solver.cpp:334]     Train net output #0: loss = 2.17678 (* 1 = 2.17678 loss)
I0815 17:07:13.596256  8764 sgd_solver.cpp:136] Iteration 126300, lr = 0.00210625, m = 0.9
I0815 17:07:32.603162  8764 solver.cpp:312] Iteration 126400 (5.26137 iter/s, 19.0065s/100 iter), loss = 1.35398
I0815 17:07:32.616180  8764 solver.cpp:334]     Train net output #0: loss = 1.36872 (* 1 = 1.36872 loss)
I0815 17:07:32.616209  8764 sgd_solver.cpp:136] Iteration 126400, lr = 0.0021, m = 0.9
I0815 17:07:49.243892  8764 solver.cpp:312] Iteration 126500 (6.00952 iter/s, 16.6403s/100 iter), loss = 1.84949
I0815 17:07:49.243984  8764 solver.cpp:334]     Train net output #0: loss = 2.01681 (* 1 = 2.01681 loss)
I0815 17:07:49.244010  8764 sgd_solver.cpp:136] Iteration 126500, lr = 0.00209375, m = 0.9
I0815 17:08:09.406693  8764 solver.cpp:312] Iteration 126600 (4.95976 iter/s, 20.1623s/100 iter), loss = 1.63575
I0815 17:08:09.406761  8764 solver.cpp:334]     Train net output #0: loss = 1.41282 (* 1 = 1.41282 loss)
I0815 17:08:09.406769  8764 sgd_solver.cpp:136] Iteration 126600, lr = 0.0020875, m = 0.9
I0815 17:08:27.970481  8764 solver.cpp:312] Iteration 126700 (5.38698 iter/s, 18.5633s/100 iter), loss = 1.37446
I0815 17:08:27.970527  8764 solver.cpp:334]     Train net output #0: loss = 1.36834 (* 1 = 1.36834 loss)
I0815 17:08:27.970540  8764 sgd_solver.cpp:136] Iteration 126700, lr = 0.00208125, m = 0.9
I0815 17:08:47.746351  8764 solver.cpp:312] Iteration 126800 (5.05681 iter/s, 19.7753s/100 iter), loss = 1.1602
I0815 17:08:47.746448  8764 solver.cpp:334]     Train net output #0: loss = 1.34307 (* 1 = 1.34307 loss)
I0815 17:08:47.746464  8764 sgd_solver.cpp:136] Iteration 126800, lr = 0.002075, m = 0.9
I0815 17:09:06.390178  8764 solver.cpp:312] Iteration 126900 (5.36386 iter/s, 18.6433s/100 iter), loss = 1.20543
I0815 17:09:06.412174  8764 solver.cpp:334]     Train net output #0: loss = 1.10808 (* 1 = 1.10808 loss)
I0815 17:09:06.412200  8764 sgd_solver.cpp:136] Iteration 126900, lr = 0.00206875, m = 0.9
I0815 17:09:24.463171  8764 solver.cpp:363] Sparsity after update:
I0815 17:09:24.478569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:09:24.478587  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:09:24.478596  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:09:24.478600  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:09:24.478605  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:09:24.478624  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:09:24.478636  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:09:24.478644  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:09:24.478653  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:09:24.478672  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:09:24.478682  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:09:24.478691  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:09:24.478700  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:09:24.607796  8764 solver.cpp:312] Iteration 127000 (5.48934 iter/s, 18.2171s/100 iter), loss = 1.40548
I0815 17:09:24.607825  8764 solver.cpp:334]     Train net output #0: loss = 1.52936 (* 1 = 1.52936 loss)
I0815 17:09:24.607831  8764 sgd_solver.cpp:136] Iteration 127000, lr = 0.0020625, m = 0.9
I0815 17:09:43.816354  8764 solver.cpp:312] Iteration 127100 (5.20616 iter/s, 19.208s/100 iter), loss = 1.27825
I0815 17:09:43.816380  8764 solver.cpp:334]     Train net output #0: loss = 1.3444 (* 1 = 1.3444 loss)
I0815 17:09:43.816385  8764 sgd_solver.cpp:136] Iteration 127100, lr = 0.00205625, m = 0.9
I0815 17:10:01.339206  8764 solver.cpp:312] Iteration 127200 (5.70699 iter/s, 17.5224s/100 iter), loss = 1.1807
I0815 17:10:01.339248  8764 solver.cpp:334]     Train net output #0: loss = 1.16003 (* 1 = 1.16003 loss)
I0815 17:10:01.339254  8764 sgd_solver.cpp:136] Iteration 127200, lr = 0.00205, m = 0.9
I0815 17:10:17.944097  8764 solver.cpp:312] Iteration 127300 (6.02249 iter/s, 16.6044s/100 iter), loss = 1.32736
I0815 17:10:17.944149  8764 solver.cpp:334]     Train net output #0: loss = 1.30993 (* 1 = 1.30993 loss)
I0815 17:10:17.944159  8764 sgd_solver.cpp:136] Iteration 127300, lr = 0.00204375, m = 0.9
I0815 17:10:37.823766  8764 solver.cpp:312] Iteration 127400 (5.0304 iter/s, 19.8791s/100 iter), loss = 1.26724
I0815 17:10:37.823860  8764 solver.cpp:334]     Train net output #0: loss = 1.49237 (* 1 = 1.49237 loss)
I0815 17:10:37.823874  8764 sgd_solver.cpp:136] Iteration 127400, lr = 0.0020375, m = 0.9
I0815 17:10:54.629554  8764 solver.cpp:312] Iteration 127500 (5.95049 iter/s, 16.8053s/100 iter), loss = 1.43378
I0815 17:10:54.629582  8764 solver.cpp:334]     Train net output #0: loss = 1.42391 (* 1 = 1.42391 loss)
I0815 17:10:54.629588  8764 sgd_solver.cpp:136] Iteration 127500, lr = 0.00203125, m = 0.9
I0815 17:11:12.209220  8764 solver.cpp:312] Iteration 127600 (5.68855 iter/s, 17.5792s/100 iter), loss = 1.30301
I0815 17:11:12.209355  8764 solver.cpp:334]     Train net output #0: loss = 1.4197 (* 1 = 1.4197 loss)
I0815 17:11:12.209369  8764 sgd_solver.cpp:136] Iteration 127600, lr = 0.002025, m = 0.9
I0815 17:11:28.565254  8764 solver.cpp:312] Iteration 127700 (6.11412 iter/s, 16.3556s/100 iter), loss = 1.57528
I0815 17:11:28.565280  8764 solver.cpp:334]     Train net output #0: loss = 1.79272 (* 1 = 1.79272 loss)
I0815 17:11:28.565287  8764 sgd_solver.cpp:136] Iteration 127700, lr = 0.00201875, m = 0.9
I0815 17:11:47.973165  8764 solver.cpp:312] Iteration 127800 (5.15268 iter/s, 19.4074s/100 iter), loss = 1.55694
I0815 17:11:47.973453  8764 solver.cpp:334]     Train net output #0: loss = 1.39133 (* 1 = 1.39133 loss)
I0815 17:11:47.973574  8764 sgd_solver.cpp:136] Iteration 127800, lr = 0.0020125, m = 0.9
I0815 17:12:04.644223  8764 solver.cpp:312] Iteration 127900 (5.99858 iter/s, 16.6706s/100 iter), loss = 1.21328
I0815 17:12:04.644251  8764 solver.cpp:334]     Train net output #0: loss = 0.999575 (* 1 = 0.999575 loss)
I0815 17:12:04.644256  8764 sgd_solver.cpp:136] Iteration 127900, lr = 0.00200625, m = 0.9
I0815 17:12:23.299715  8764 solver.cpp:363] Sparsity after update:
I0815 17:12:23.311008  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:12:23.311084  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:12:23.311136  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:12:23.311151  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:12:23.311163  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:12:23.311173  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:12:23.311190  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:12:23.311198  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:12:23.311218  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:12:23.311234  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:12:23.311252  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:12:23.311277  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:12:23.311305  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:12:23.311368  8764 solver.cpp:509] Iteration 128000, Testing net (#0)
I0815 17:12:50.298908  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.584823
I0815 17:12:50.298938  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814291
I0815 17:12:50.298943  8764 solver.cpp:594]     Test net output #2: loss = 1.79734 (* 1 = 1.79734 loss)
I0815 17:12:50.298967  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9869s
I0815 17:12:50.518247  8764 solver.cpp:312] Iteration 128000 (2.17994 iter/s, 45.8728s/100 iter), loss = 1.34882
I0815 17:12:50.518301  8764 solver.cpp:334]     Train net output #0: loss = 1.39404 (* 1 = 1.39404 loss)
I0815 17:12:50.518313  8764 sgd_solver.cpp:136] Iteration 128000, lr = 0.002, m = 0.9
I0815 17:13:07.989948  8764 solver.cpp:312] Iteration 128100 (5.7237 iter/s, 17.4712s/100 iter), loss = 1.02687
I0815 17:13:07.998904  8764 solver.cpp:334]     Train net output #0: loss = 1.11087 (* 1 = 1.11087 loss)
I0815 17:13:07.998916  8764 sgd_solver.cpp:136] Iteration 128100, lr = 0.00199375, m = 0.9
I0815 17:13:25.117156  8764 solver.cpp:312] Iteration 128200 (5.83883 iter/s, 17.1267s/100 iter), loss = 1.05284
I0815 17:13:25.117252  8764 solver.cpp:334]     Train net output #0: loss = 0.72964 (* 1 = 0.72964 loss)
I0815 17:13:25.117280  8764 sgd_solver.cpp:136] Iteration 128200, lr = 0.0019875, m = 0.9
I0815 17:13:40.631798  8764 solver.cpp:312] Iteration 128300 (6.4457 iter/s, 15.5142s/100 iter), loss = 1.23448
I0815 17:13:40.631863  8764 solver.cpp:334]     Train net output #0: loss = 1.04904 (* 1 = 1.04904 loss)
I0815 17:13:40.631870  8764 sgd_solver.cpp:136] Iteration 128300, lr = 0.00198125, m = 0.9
I0815 17:13:57.455849  8764 solver.cpp:312] Iteration 128400 (5.94404 iter/s, 16.8236s/100 iter), loss = 1.30775
I0815 17:13:57.455873  8764 solver.cpp:334]     Train net output #0: loss = 1.23768 (* 1 = 1.23768 loss)
I0815 17:13:57.455878  8764 sgd_solver.cpp:136] Iteration 128400, lr = 0.001975, m = 0.9
I0815 17:14:15.677525  8764 solver.cpp:312] Iteration 128500 (5.48812 iter/s, 18.2212s/100 iter), loss = 1.72007
I0815 17:14:15.677577  8764 solver.cpp:334]     Train net output #0: loss = 1.77705 (* 1 = 1.77705 loss)
I0815 17:14:15.677584  8764 sgd_solver.cpp:136] Iteration 128500, lr = 0.00196875, m = 0.9
I0815 17:14:38.199393  8764 solver.cpp:312] Iteration 128600 (4.44025 iter/s, 22.5212s/100 iter), loss = 1.21492
I0815 17:14:38.199425  8764 solver.cpp:334]     Train net output #0: loss = 1.01693 (* 1 = 1.01693 loss)
I0815 17:14:38.199434  8764 sgd_solver.cpp:136] Iteration 128600, lr = 0.0019625, m = 0.9
I0815 17:14:59.981477  8764 solver.cpp:312] Iteration 128700 (4.59106 iter/s, 21.7815s/100 iter), loss = 1.21398
I0815 17:14:59.981567  8764 solver.cpp:334]     Train net output #0: loss = 0.842298 (* 1 = 0.842298 loss)
I0815 17:14:59.981580  8764 sgd_solver.cpp:136] Iteration 128700, lr = 0.00195625, m = 0.9
I0815 17:15:20.903497  8764 solver.cpp:312] Iteration 128800 (4.77979 iter/s, 20.9214s/100 iter), loss = 1.35614
I0815 17:15:20.903543  8764 solver.cpp:334]     Train net output #0: loss = 1.4568 (* 1 = 1.4568 loss)
I0815 17:15:20.903554  8764 sgd_solver.cpp:136] Iteration 128800, lr = 0.00195, m = 0.9
I0815 17:15:44.544180  8764 solver.cpp:312] Iteration 128900 (4.23011 iter/s, 23.64s/100 iter), loss = 1.74407
I0815 17:15:44.544281  8764 solver.cpp:334]     Train net output #0: loss = 1.70053 (* 1 = 1.70053 loss)
I0815 17:15:44.544296  8764 sgd_solver.cpp:136] Iteration 128900, lr = 0.00194375, m = 0.9
I0815 17:16:02.387634  8764 solver.cpp:363] Sparsity after update:
I0815 17:16:02.398764  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:16:02.398782  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:16:02.398792  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:16:02.398794  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:16:02.398797  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:16:02.398803  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:16:02.398808  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:16:02.398811  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:16:02.398814  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:16:02.398818  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:16:02.398820  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:16:02.398823  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:16:02.398828  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:16:02.543727  8764 solver.cpp:312] Iteration 129000 (5.55585 iter/s, 17.9991s/100 iter), loss = 1.45426
I0815 17:16:02.543750  8764 solver.cpp:334]     Train net output #0: loss = 1.43579 (* 1 = 1.43579 loss)
I0815 17:16:02.543753  8764 sgd_solver.cpp:136] Iteration 129000, lr = 0.0019375, m = 0.9
I0815 17:16:21.953409  8764 solver.cpp:312] Iteration 129100 (5.15221 iter/s, 19.4091s/100 iter), loss = 1.73234
I0815 17:16:21.953506  8764 solver.cpp:334]     Train net output #0: loss = 1.63236 (* 1 = 1.63236 loss)
I0815 17:16:21.953526  8764 sgd_solver.cpp:136] Iteration 129100, lr = 0.00193125, m = 0.9
I0815 17:16:24.155280  8726 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 17:16:40.826834  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:16:46.138929  8764 solver.cpp:312] Iteration 129200 (4.13482 iter/s, 24.1848s/100 iter), loss = 1.42239
I0815 17:16:46.138981  8764 solver.cpp:334]     Train net output #0: loss = 1.12906 (* 1 = 1.12906 loss)
I0815 17:16:46.138993  8764 sgd_solver.cpp:136] Iteration 129200, lr = 0.001925, m = 0.9
I0815 17:17:04.383561  8764 solver.cpp:312] Iteration 129300 (5.48122 iter/s, 18.2441s/100 iter), loss = 1.0938
I0815 17:17:04.383759  8764 solver.cpp:334]     Train net output #0: loss = 1.34953 (* 1 = 1.34953 loss)
I0815 17:17:04.383783  8764 sgd_solver.cpp:136] Iteration 129300, lr = 0.00191875, m = 0.9
I0815 17:17:23.883965  8764 solver.cpp:312] Iteration 129400 (5.12824 iter/s, 19.4999s/100 iter), loss = 1.35504
I0815 17:17:23.884012  8764 solver.cpp:334]     Train net output #0: loss = 1.19291 (* 1 = 1.19291 loss)
I0815 17:17:23.884021  8764 sgd_solver.cpp:136] Iteration 129400, lr = 0.0019125, m = 0.9
I0815 17:17:43.936698  8764 solver.cpp:312] Iteration 129500 (4.98699 iter/s, 20.0522s/100 iter), loss = 1.20573
I0815 17:17:43.940174  8764 solver.cpp:334]     Train net output #0: loss = 1.43859 (* 1 = 1.43859 loss)
I0815 17:17:43.940191  8764 sgd_solver.cpp:136] Iteration 129500, lr = 0.00190625, m = 0.9
I0815 17:18:09.395064  8764 solver.cpp:312] Iteration 129600 (3.92809 iter/s, 25.4577s/100 iter), loss = 1.83191
I0815 17:18:09.395138  8764 solver.cpp:334]     Train net output #0: loss = 2.27386 (* 1 = 2.27386 loss)
I0815 17:18:09.395160  8764 sgd_solver.cpp:136] Iteration 129600, lr = 0.0019, m = 0.9
I0815 17:18:28.652477  8764 solver.cpp:312] Iteration 129700 (5.19295 iter/s, 19.2569s/100 iter), loss = 0.960912
I0815 17:18:28.652573  8764 solver.cpp:334]     Train net output #0: loss = 1.26243 (* 1 = 1.26243 loss)
I0815 17:18:28.652580  8764 sgd_solver.cpp:136] Iteration 129700, lr = 0.00189375, m = 0.9
I0815 17:18:48.800379  8764 solver.cpp:312] Iteration 129800 (4.96343 iter/s, 20.1473s/100 iter), loss = 1.82519
I0815 17:18:48.800410  8764 solver.cpp:334]     Train net output #0: loss = 1.84679 (* 1 = 1.84679 loss)
I0815 17:18:48.800415  8764 sgd_solver.cpp:136] Iteration 129800, lr = 0.0018875, m = 0.9
I0815 17:19:08.588856  8764 solver.cpp:312] Iteration 129900 (5.05359 iter/s, 19.7879s/100 iter), loss = 1.56422
I0815 17:19:08.588949  8764 solver.cpp:334]     Train net output #0: loss = 1.3593 (* 1 = 1.3593 loss)
I0815 17:19:08.588961  8764 sgd_solver.cpp:136] Iteration 129900, lr = 0.00188125, m = 0.9
I0815 17:19:29.908202  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_130000.caffemodel
I0815 17:19:30.005930  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_130000.solverstate
I0815 17:19:30.012125  8764 solver.cpp:363] Sparsity after update:
I0815 17:19:30.013435  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:19:30.013447  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:19:30.013454  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:19:30.013458  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:19:30.013460  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:19:30.013464  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:19:30.013468  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:19:30.013470  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:19:30.013473  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:19:30.013476  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:19:30.013480  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:19:30.013484  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:19:30.013486  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:19:30.013509  8764 solver.cpp:509] Iteration 130000, Testing net (#0)
I0815 17:20:02.439919  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.589412
I0815 17:20:02.440052  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.818996
I0815 17:20:02.440099  8764 solver.cpp:594]     Test net output #2: loss = 1.77976 (* 1 = 1.77976 loss)
I0815 17:20:02.440176  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.4258s
I0815 17:20:02.764709  8764 solver.cpp:312] Iteration 130000 (1.84589 iter/s, 54.1743s/100 iter), loss = 1.27043
I0815 17:20:02.764789  8764 solver.cpp:334]     Train net output #0: loss = 1.63499 (* 1 = 1.63499 loss)
I0815 17:20:02.764816  8764 sgd_solver.cpp:136] Iteration 130000, lr = 0.001875, m = 0.9
I0815 17:20:22.427492  8764 solver.cpp:312] Iteration 130100 (5.08589 iter/s, 19.6622s/100 iter), loss = 1.49812
I0815 17:20:22.427541  8764 solver.cpp:334]     Train net output #0: loss = 1.54984 (* 1 = 1.54984 loss)
I0815 17:20:22.427551  8764 sgd_solver.cpp:136] Iteration 130100, lr = 0.00186875, m = 0.9
I0815 17:20:41.068940  8764 solver.cpp:312] Iteration 130200 (5.36454 iter/s, 18.6409s/100 iter), loss = 1.53985
I0815 17:20:41.069016  8764 solver.cpp:334]     Train net output #0: loss = 1.45333 (* 1 = 1.45333 loss)
I0815 17:20:41.069023  8764 sgd_solver.cpp:136] Iteration 130200, lr = 0.0018625, m = 0.9
I0815 17:20:59.703496  8764 solver.cpp:312] Iteration 130300 (5.36653 iter/s, 18.634s/100 iter), loss = 1.3469
I0815 17:20:59.703634  8764 solver.cpp:334]     Train net output #0: loss = 1.25095 (* 1 = 1.25095 loss)
I0815 17:20:59.703668  8764 sgd_solver.cpp:136] Iteration 130300, lr = 0.00185625, m = 0.9
I0815 17:21:21.067569  8764 solver.cpp:312] Iteration 130400 (4.68088 iter/s, 21.3635s/100 iter), loss = 1.05584
I0815 17:21:21.067631  8764 solver.cpp:334]     Train net output #0: loss = 1.18852 (* 1 = 1.18852 loss)
I0815 17:21:21.067637  8764 sgd_solver.cpp:136] Iteration 130400, lr = 0.00185, m = 0.9
I0815 17:21:40.853276  8764 solver.cpp:312] Iteration 130500 (5.05429 iter/s, 19.7852s/100 iter), loss = 1.29286
I0815 17:21:40.853340  8764 solver.cpp:334]     Train net output #0: loss = 1.53981 (* 1 = 1.53981 loss)
I0815 17:21:40.853358  8764 sgd_solver.cpp:136] Iteration 130500, lr = 0.00184375, m = 0.9
I0815 17:21:57.243203  8764 solver.cpp:312] Iteration 130600 (6.10148 iter/s, 16.3895s/100 iter), loss = 1.99654
I0815 17:21:57.245162  8764 solver.cpp:334]     Train net output #0: loss = 2.66144 (* 1 = 2.66144 loss)
I0815 17:21:57.245187  8764 sgd_solver.cpp:136] Iteration 130600, lr = 0.0018375, m = 0.9
I0815 17:22:14.241308  8764 solver.cpp:312] Iteration 130700 (5.88317 iter/s, 16.9976s/100 iter), loss = 1.25891
I0815 17:22:14.241344  8764 solver.cpp:334]     Train net output #0: loss = 1.40489 (* 1 = 1.40489 loss)
I0815 17:22:14.241351  8764 sgd_solver.cpp:136] Iteration 130700, lr = 0.00183125, m = 0.9
I0815 17:22:32.313590  8764 solver.cpp:312] Iteration 130800 (5.53349 iter/s, 18.0718s/100 iter), loss = 1.27486
I0815 17:22:32.313695  8764 solver.cpp:334]     Train net output #0: loss = 1.51742 (* 1 = 1.51742 loss)
I0815 17:22:32.313704  8764 sgd_solver.cpp:136] Iteration 130800, lr = 0.001825, m = 0.9
I0815 17:22:52.209511  8764 solver.cpp:312] Iteration 130900 (5.02629 iter/s, 19.8954s/100 iter), loss = 1.17972
I0815 17:22:52.209533  8764 solver.cpp:334]     Train net output #0: loss = 1.23541 (* 1 = 1.23541 loss)
I0815 17:22:52.209538  8764 sgd_solver.cpp:136] Iteration 130900, lr = 0.00181875, m = 0.9
I0815 17:23:09.027231  8764 solver.cpp:363] Sparsity after update:
I0815 17:23:09.038789  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:23:09.038805  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:23:09.038813  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:23:09.038816  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:23:09.038822  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:23:09.038826  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:23:09.038830  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:23:09.038832  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:23:09.038836  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:23:09.038839  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:23:09.038842  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:23:09.038846  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:23:09.038848  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:23:09.179023  8764 solver.cpp:312] Iteration 131000 (5.89308 iter/s, 16.969s/100 iter), loss = 1.86589
I0815 17:23:09.179076  8764 solver.cpp:334]     Train net output #0: loss = 2.10535 (* 1 = 2.10535 loss)
I0815 17:23:09.179090  8764 sgd_solver.cpp:136] Iteration 131000, lr = 0.0018125, m = 0.9
I0815 17:23:25.728157  8764 solver.cpp:312] Iteration 131100 (6.04278 iter/s, 16.5487s/100 iter), loss = 1.52704
I0815 17:23:25.728207  8764 solver.cpp:334]     Train net output #0: loss = 1.84386 (* 1 = 1.84386 loss)
I0815 17:23:25.728219  8764 sgd_solver.cpp:136] Iteration 131100, lr = 0.00180625, m = 0.9
I0815 17:23:45.408041  8764 solver.cpp:312] Iteration 131200 (5.08147 iter/s, 19.6793s/100 iter), loss = 1.34465
I0815 17:23:45.408107  8764 solver.cpp:334]     Train net output #0: loss = 1.58709 (* 1 = 1.58709 loss)
I0815 17:23:45.408113  8764 sgd_solver.cpp:136] Iteration 131200, lr = 0.0018, m = 0.9
I0815 17:24:07.257922  8764 solver.cpp:312] Iteration 131300 (4.57681 iter/s, 21.8493s/100 iter), loss = 1.56876
I0815 17:24:07.257997  8764 solver.cpp:334]     Train net output #0: loss = 1.55274 (* 1 = 1.55274 loss)
I0815 17:24:07.258016  8764 sgd_solver.cpp:136] Iteration 131300, lr = 0.00179375, m = 0.9
I0815 17:24:25.645272  8764 solver.cpp:312] Iteration 131400 (5.43867 iter/s, 18.3868s/100 iter), loss = 1.31606
I0815 17:24:25.645355  8764 solver.cpp:334]     Train net output #0: loss = 1.48385 (* 1 = 1.48385 loss)
I0815 17:24:25.645375  8764 sgd_solver.cpp:136] Iteration 131400, lr = 0.0017875, m = 0.9
I0815 17:24:44.396997  8764 solver.cpp:312] Iteration 131500 (5.33299 iter/s, 18.7512s/100 iter), loss = 1.93663
I0815 17:24:44.397027  8764 solver.cpp:334]     Train net output #0: loss = 1.8183 (* 1 = 1.8183 loss)
I0815 17:24:44.397032  8764 sgd_solver.cpp:136] Iteration 131500, lr = 0.00178125, m = 0.9
I0815 17:25:04.074728  8764 solver.cpp:312] Iteration 131600 (5.08203 iter/s, 19.6772s/100 iter), loss = 0.959314
I0815 17:25:04.074780  8764 solver.cpp:334]     Train net output #0: loss = 0.76447 (* 1 = 0.76447 loss)
I0815 17:25:04.074785  8764 sgd_solver.cpp:136] Iteration 131600, lr = 0.001775, m = 0.9
I0815 17:25:25.822871  8764 solver.cpp:312] Iteration 131700 (4.59822 iter/s, 21.7475s/100 iter), loss = 1.33738
I0815 17:25:25.822939  8764 solver.cpp:334]     Train net output #0: loss = 1.64302 (* 1 = 1.64302 loss)
I0815 17:25:25.822955  8764 sgd_solver.cpp:136] Iteration 131700, lr = 0.00176875, m = 0.9
I0815 17:25:42.141067  8764 solver.cpp:312] Iteration 131800 (6.1283 iter/s, 16.3177s/100 iter), loss = 1.41934
I0815 17:25:42.141149  8764 solver.cpp:334]     Train net output #0: loss = 1.55354 (* 1 = 1.55354 loss)
I0815 17:25:42.141171  8764 sgd_solver.cpp:136] Iteration 131800, lr = 0.0017625, m = 0.9
I0815 17:26:01.213870  8764 solver.cpp:312] Iteration 131900 (5.24321 iter/s, 19.0723s/100 iter), loss = 1.47877
I0815 17:26:01.213945  8764 solver.cpp:334]     Train net output #0: loss = 1.79184 (* 1 = 1.79184 loss)
I0815 17:26:01.213965  8764 sgd_solver.cpp:136] Iteration 131900, lr = 0.00175625, m = 0.9
I0815 17:26:19.057579  8764 solver.cpp:363] Sparsity after update:
I0815 17:26:19.061997  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:26:19.062036  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:26:19.062068  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:26:19.062093  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:26:19.062116  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:26:19.062140  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:26:19.062161  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:26:19.062180  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:26:19.062201  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:26:19.062218  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:26:19.062237  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:26:19.062255  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:26:19.062273  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:26:19.062314  8764 solver.cpp:509] Iteration 132000, Testing net (#0)
I0815 17:26:41.878682  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:26:44.570834  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.593117
I0815 17:26:44.570858  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.820115
I0815 17:26:44.570863  8764 solver.cpp:594]     Test net output #2: loss = 1.77681 (* 1 = 1.77681 loss)
I0815 17:26:44.570878  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.5079s
I0815 17:26:44.743706  8764 solver.cpp:312] Iteration 132000 (2.29734 iter/s, 43.5286s/100 iter), loss = 1.39118
I0815 17:26:44.743734  8764 solver.cpp:334]     Train net output #0: loss = 1.32367 (* 1 = 1.32367 loss)
I0815 17:26:44.743741  8764 sgd_solver.cpp:136] Iteration 132000, lr = 0.00175, m = 0.9
I0815 17:27:06.078294  8764 solver.cpp:312] Iteration 132100 (4.68735 iter/s, 21.334s/100 iter), loss = 1.74822
I0815 17:27:06.078383  8764 solver.cpp:334]     Train net output #0: loss = 1.44198 (* 1 = 1.44198 loss)
I0815 17:27:06.078392  8764 sgd_solver.cpp:136] Iteration 132100, lr = 0.00174375, m = 0.9
I0815 17:27:25.558334  8764 solver.cpp:312] Iteration 132200 (5.1336 iter/s, 19.4795s/100 iter), loss = 1.17014
I0815 17:27:25.558362  8764 solver.cpp:334]     Train net output #0: loss = 0.899074 (* 1 = 0.899074 loss)
I0815 17:27:25.558368  8764 sgd_solver.cpp:136] Iteration 132200, lr = 0.0017375, m = 0.9
I0815 17:27:43.469065  8764 solver.cpp:312] Iteration 132300 (5.5834 iter/s, 17.9102s/100 iter), loss = 1.56735
I0815 17:27:43.469123  8764 solver.cpp:334]     Train net output #0: loss = 1.94571 (* 1 = 1.94571 loss)
I0815 17:27:43.469127  8764 sgd_solver.cpp:136] Iteration 132300, lr = 0.00173125, m = 0.9
I0815 17:28:00.434290  8764 solver.cpp:312] Iteration 132400 (5.89457 iter/s, 16.9648s/100 iter), loss = 1.32808
I0815 17:28:00.434319  8764 solver.cpp:334]     Train net output #0: loss = 1.36799 (* 1 = 1.36799 loss)
I0815 17:28:00.434325  8764 sgd_solver.cpp:136] Iteration 132400, lr = 0.001725, m = 0.9
I0815 17:28:18.370705  8764 solver.cpp:312] Iteration 132500 (5.5754 iter/s, 17.9359s/100 iter), loss = 1.41387
I0815 17:28:18.370757  8764 solver.cpp:334]     Train net output #0: loss = 1.30757 (* 1 = 1.30757 loss)
I0815 17:28:18.370764  8764 sgd_solver.cpp:136] Iteration 132500, lr = 0.00171875, m = 0.9
I0815 17:28:36.163020  8764 solver.cpp:312] Iteration 132600 (5.62056 iter/s, 17.7918s/100 iter), loss = 1.23818
I0815 17:28:36.163045  8764 solver.cpp:334]     Train net output #0: loss = 1.20743 (* 1 = 1.20743 loss)
I0815 17:28:36.163051  8764 sgd_solver.cpp:136] Iteration 132600, lr = 0.0017125, m = 0.9
I0815 17:28:54.526492  8764 solver.cpp:312] Iteration 132700 (5.44574 iter/s, 18.363s/100 iter), loss = 1.34947
I0815 17:28:54.526553  8764 solver.cpp:334]     Train net output #0: loss = 1.44059 (* 1 = 1.44059 loss)
I0815 17:28:54.526561  8764 sgd_solver.cpp:136] Iteration 132700, lr = 0.00170625, m = 0.9
I0815 17:29:11.706342  8764 solver.cpp:312] Iteration 132800 (5.82093 iter/s, 17.1794s/100 iter), loss = 1.16655
I0815 17:29:11.706406  8764 solver.cpp:334]     Train net output #0: loss = 0.98528 (* 1 = 0.98528 loss)
I0815 17:29:11.706423  8764 sgd_solver.cpp:136] Iteration 132800, lr = 0.0017, m = 0.9
I0815 17:29:30.256767  8764 solver.cpp:312] Iteration 132900 (5.39086 iter/s, 18.5499s/100 iter), loss = 1.48015
I0815 17:29:30.256880  8764 solver.cpp:334]     Train net output #0: loss = 1.34148 (* 1 = 1.34148 loss)
I0815 17:29:30.256903  8764 sgd_solver.cpp:136] Iteration 132900, lr = 0.00169375, m = 0.9
I0815 17:29:47.959995  8764 solver.cpp:363] Sparsity after update:
I0815 17:29:47.968354  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:29:47.968399  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:29:47.968436  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:29:47.968464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:29:47.968488  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:29:47.968513  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:29:47.968536  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:29:47.968566  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:29:47.968593  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:29:47.968618  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:29:47.968642  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:29:47.968667  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:29:47.968693  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:29:48.290923  8764 solver.cpp:312] Iteration 133000 (5.54519 iter/s, 18.0337s/100 iter), loss = 1.10578
I0815 17:29:48.290959  8764 solver.cpp:334]     Train net output #0: loss = 1.06993 (* 1 = 1.06993 loss)
I0815 17:29:48.290966  8764 sgd_solver.cpp:136] Iteration 133000, lr = 0.0016875, m = 0.9
I0815 17:30:05.662282  8764 solver.cpp:312] Iteration 133100 (5.75676 iter/s, 17.3709s/100 iter), loss = 1.42551
I0815 17:30:05.662523  8764 solver.cpp:334]     Train net output #0: loss = 1.8064 (* 1 = 1.8064 loss)
I0815 17:30:05.662541  8764 sgd_solver.cpp:136] Iteration 133100, lr = 0.00168125, m = 0.9
I0815 17:30:22.838812  8764 solver.cpp:312] Iteration 133200 (5.82206 iter/s, 17.1761s/100 iter), loss = 1.12785
I0815 17:30:22.838851  8764 solver.cpp:334]     Train net output #0: loss = 0.991353 (* 1 = 0.991353 loss)
I0815 17:30:22.839015  8764 sgd_solver.cpp:136] Iteration 133200, lr = 0.001675, m = 0.9
I0815 17:30:41.658640  8764 solver.cpp:312] Iteration 133300 (5.31369 iter/s, 18.8193s/100 iter), loss = 1.60237
I0815 17:30:41.658704  8764 solver.cpp:334]     Train net output #0: loss = 1.7237 (* 1 = 1.7237 loss)
I0815 17:30:41.658711  8764 sgd_solver.cpp:136] Iteration 133300, lr = 0.00166875, m = 0.9
I0815 17:31:00.340356  8764 solver.cpp:312] Iteration 133400 (5.35297 iter/s, 18.6812s/100 iter), loss = 1.14864
I0815 17:31:00.340538  8764 solver.cpp:334]     Train net output #0: loss = 1.165 (* 1 = 1.165 loss)
I0815 17:31:00.340617  8764 sgd_solver.cpp:136] Iteration 133400, lr = 0.0016625, m = 0.9
I0815 17:31:17.644100  8764 solver.cpp:312] Iteration 133500 (5.77925 iter/s, 17.3033s/100 iter), loss = 1.17668
I0815 17:31:17.644325  8764 solver.cpp:334]     Train net output #0: loss = 1.07934 (* 1 = 1.07934 loss)
I0815 17:31:17.644413  8764 sgd_solver.cpp:136] Iteration 133500, lr = 0.00165625, m = 0.9
I0815 17:31:35.185799  8764 solver.cpp:312] Iteration 133600 (5.70086 iter/s, 17.5412s/100 iter), loss = 1.52824
I0815 17:31:35.185849  8764 solver.cpp:334]     Train net output #0: loss = 1.68763 (* 1 = 1.68763 loss)
I0815 17:31:35.185860  8764 sgd_solver.cpp:136] Iteration 133600, lr = 0.00165, m = 0.9
I0815 17:31:57.120683  8764 solver.cpp:312] Iteration 133700 (4.55907 iter/s, 21.9343s/100 iter), loss = 1.38269
I0815 17:31:57.120774  8764 solver.cpp:334]     Train net output #0: loss = 0.985292 (* 1 = 0.985292 loss)
I0815 17:31:57.120793  8764 sgd_solver.cpp:136] Iteration 133700, lr = 0.00164375, m = 0.9
I0815 17:32:19.377300  8764 solver.cpp:312] Iteration 133800 (4.49317 iter/s, 22.256s/100 iter), loss = 0.981417
I0815 17:32:19.377373  8764 solver.cpp:334]     Train net output #0: loss = 0.885605 (* 1 = 0.885605 loss)
I0815 17:32:19.377394  8764 sgd_solver.cpp:136] Iteration 133800, lr = 0.0016375, m = 0.9
I0815 17:32:41.567522  8764 solver.cpp:312] Iteration 133900 (4.50661 iter/s, 22.1896s/100 iter), loss = 1.32559
I0815 17:32:41.567605  8764 solver.cpp:334]     Train net output #0: loss = 1.70436 (* 1 = 1.70436 loss)
I0815 17:32:41.567617  8764 sgd_solver.cpp:136] Iteration 133900, lr = 0.00163125, m = 0.9
I0815 17:33:02.465883  8764 solver.cpp:363] Sparsity after update:
I0815 17:33:02.471698  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:33:02.471706  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:33:02.471714  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:33:02.471719  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:33:02.471724  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:33:02.471726  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:33:02.471729  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:33:02.471734  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:33:02.471736  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:33:02.471740  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:33:02.471745  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:33:02.471747  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:33:02.471751  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:33:02.471765  8764 solver.cpp:509] Iteration 134000, Testing net (#0)
I0815 17:33:37.252636  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588411
I0815 17:33:37.252707  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814291
I0815 17:33:37.252712  8764 solver.cpp:594]     Test net output #2: loss = 1.79674 (* 1 = 1.79674 loss)
I0815 17:33:37.252732  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.78s
I0815 17:33:37.418210  8764 solver.cpp:312] Iteration 134000 (1.79054 iter/s, 55.8492s/100 iter), loss = 1.26797
I0815 17:33:37.418395  8764 solver.cpp:334]     Train net output #0: loss = 1.13422 (* 1 = 1.13422 loss)
I0815 17:33:37.418483  8764 sgd_solver.cpp:136] Iteration 134000, lr = 0.001625, m = 0.9
I0815 17:33:55.536168  8764 solver.cpp:312] Iteration 134100 (5.51954 iter/s, 18.1174s/100 iter), loss = 1.5052
I0815 17:33:55.536203  8764 solver.cpp:334]     Train net output #0: loss = 1.64976 (* 1 = 1.64976 loss)
I0815 17:33:55.536209  8764 sgd_solver.cpp:136] Iteration 134100, lr = 0.00161875, m = 0.9
I0815 17:34:17.248491  8764 solver.cpp:312] Iteration 134200 (4.60581 iter/s, 21.7117s/100 iter), loss = 1.23115
I0815 17:34:17.256224  8764 solver.cpp:334]     Train net output #0: loss = 1.54866 (* 1 = 1.54866 loss)
I0815 17:34:17.256263  8764 sgd_solver.cpp:136] Iteration 134200, lr = 0.0016125, m = 0.9
I0815 17:34:39.705312  8764 solver.cpp:312] Iteration 134300 (4.45311 iter/s, 22.4562s/100 iter), loss = 1.38126
I0815 17:34:39.705359  8764 solver.cpp:334]     Train net output #0: loss = 1.30305 (* 1 = 1.30305 loss)
I0815 17:34:39.705369  8764 sgd_solver.cpp:136] Iteration 134300, lr = 0.00160625, m = 0.9
I0815 17:34:58.465956  8764 solver.cpp:312] Iteration 134400 (5.33045 iter/s, 18.7601s/100 iter), loss = 1.47109
I0815 17:34:58.466018  8764 solver.cpp:334]     Train net output #0: loss = 1.54395 (* 1 = 1.54395 loss)
I0815 17:34:58.466025  8764 sgd_solver.cpp:136] Iteration 134400, lr = 0.0016, m = 0.9
I0815 17:35:23.224927  8764 solver.cpp:312] Iteration 134500 (4.03905 iter/s, 24.7583s/100 iter), loss = 1.36413
I0815 17:35:23.224998  8764 solver.cpp:334]     Train net output #0: loss = 1.49748 (* 1 = 1.49748 loss)
I0815 17:35:23.225016  8764 sgd_solver.cpp:136] Iteration 134500, lr = 0.00159375, m = 0.9
I0815 17:35:43.034340  8764 solver.cpp:312] Iteration 134600 (5.04825 iter/s, 19.8089s/100 iter), loss = 1.569
I0815 17:35:43.034451  8764 solver.cpp:334]     Train net output #0: loss = 1.17108 (* 1 = 1.17108 loss)
I0815 17:35:43.034466  8764 sgd_solver.cpp:136] Iteration 134600, lr = 0.0015875, m = 0.9
I0815 17:36:01.234887  8764 solver.cpp:312] Iteration 134700 (5.49449 iter/s, 18.2s/100 iter), loss = 1.5279
I0815 17:36:01.234915  8764 solver.cpp:334]     Train net output #0: loss = 1.5855 (* 1 = 1.5855 loss)
I0815 17:36:01.234920  8764 sgd_solver.cpp:136] Iteration 134700, lr = 0.00158125, m = 0.9
I0815 17:36:20.340768  8764 solver.cpp:312] Iteration 134800 (5.23414 iter/s, 19.1054s/100 iter), loss = 1.11819
I0815 17:36:20.342906  8764 solver.cpp:334]     Train net output #0: loss = 1.23668 (* 1 = 1.23668 loss)
I0815 17:36:20.342917  8764 sgd_solver.cpp:136] Iteration 134800, lr = 0.001575, m = 0.9
I0815 17:36:40.411533  8764 solver.cpp:312] Iteration 134900 (4.98252 iter/s, 20.0702s/100 iter), loss = 1.14739
I0815 17:36:40.411630  8764 solver.cpp:334]     Train net output #0: loss = 1.10965 (* 1 = 1.10965 loss)
I0815 17:36:40.411669  8764 sgd_solver.cpp:136] Iteration 134900, lr = 0.00156875, m = 0.9
I0815 17:37:02.490578  8764 solver.cpp:363] Sparsity after update:
I0815 17:37:02.501132  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:37:02.501165  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:37:02.501184  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:37:02.501195  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:37:02.501205  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:37:02.501215  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:37:02.501225  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:37:02.501235  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:37:02.501245  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:37:02.501255  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:37:02.501265  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:37:02.501274  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:37:02.501284  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:37:02.722560  8764 solver.cpp:312] Iteration 135000 (4.48221 iter/s, 22.3104s/100 iter), loss = 1.76965
I0815 17:37:02.722590  8764 solver.cpp:334]     Train net output #0: loss = 2.32844 (* 1 = 2.32844 loss)
I0815 17:37:02.722597  8764 sgd_solver.cpp:136] Iteration 135000, lr = 0.0015625, m = 0.9
I0815 17:37:20.986145  8764 solver.cpp:312] Iteration 135100 (5.47553 iter/s, 18.2631s/100 iter), loss = 1.95622
I0815 17:37:20.986172  8764 solver.cpp:334]     Train net output #0: loss = 1.98313 (* 1 = 1.98313 loss)
I0815 17:37:20.986178  8764 sgd_solver.cpp:136] Iteration 135100, lr = 0.00155625, m = 0.9
I0815 17:37:40.394294  8764 solver.cpp:312] Iteration 135200 (5.15262 iter/s, 19.4076s/100 iter), loss = 1.83368
I0815 17:37:40.394356  8764 solver.cpp:334]     Train net output #0: loss = 2.08937 (* 1 = 2.08937 loss)
I0815 17:37:40.394363  8764 sgd_solver.cpp:136] Iteration 135200, lr = 0.00155, m = 0.9
I0815 17:37:59.102579  8764 solver.cpp:312] Iteration 135300 (5.34538 iter/s, 18.7078s/100 iter), loss = 1.31902
I0815 17:37:59.102675  8764 solver.cpp:334]     Train net output #0: loss = 1.35356 (* 1 = 1.35356 loss)
I0815 17:37:59.102701  8764 sgd_solver.cpp:136] Iteration 135300, lr = 0.00154375, m = 0.9
I0815 17:38:21.569319  8764 solver.cpp:312] Iteration 135400 (4.45115 iter/s, 22.4661s/100 iter), loss = 1.30169
I0815 17:38:21.569412  8764 solver.cpp:334]     Train net output #0: loss = 1.5134 (* 1 = 1.5134 loss)
I0815 17:38:21.569420  8764 sgd_solver.cpp:136] Iteration 135400, lr = 0.0015375, m = 0.9
I0815 17:38:41.524209  8764 solver.cpp:312] Iteration 135500 (5.01144 iter/s, 19.9543s/100 iter), loss = 1.32333
I0815 17:38:41.524232  8764 solver.cpp:334]     Train net output #0: loss = 1.12029 (* 1 = 1.12029 loss)
I0815 17:38:41.524240  8764 sgd_solver.cpp:136] Iteration 135500, lr = 0.00153125, m = 0.9
I0815 17:39:02.407090  8764 solver.cpp:312] Iteration 135600 (4.78874 iter/s, 20.8823s/100 iter), loss = 1.54498
I0815 17:39:02.407166  8764 solver.cpp:334]     Train net output #0: loss = 1.37691 (* 1 = 1.37691 loss)
I0815 17:39:02.407174  8764 sgd_solver.cpp:136] Iteration 135600, lr = 0.001525, m = 0.9
I0815 17:39:19.986604  8764 solver.cpp:312] Iteration 135700 (5.6886 iter/s, 17.579s/100 iter), loss = 1.50859
I0815 17:39:19.986630  8764 solver.cpp:334]     Train net output #0: loss = 1.81547 (* 1 = 1.81547 loss)
I0815 17:39:19.986635  8764 sgd_solver.cpp:136] Iteration 135700, lr = 0.00151875, m = 0.9
I0815 17:39:37.262624  8764 solver.cpp:312] Iteration 135800 (5.78853 iter/s, 17.2755s/100 iter), loss = 1.59621
I0815 17:39:37.263718  8764 solver.cpp:334]     Train net output #0: loss = 1.82919 (* 1 = 1.82919 loss)
I0815 17:39:37.263736  8764 sgd_solver.cpp:136] Iteration 135800, lr = 0.0015125, m = 0.9
I0815 17:39:55.039541  8764 solver.cpp:312] Iteration 135900 (5.62544 iter/s, 17.7764s/100 iter), loss = 0.91459
I0815 17:39:55.039619  8764 solver.cpp:334]     Train net output #0: loss = 1.07175 (* 1 = 1.07175 loss)
I0815 17:39:55.039641  8764 sgd_solver.cpp:136] Iteration 135900, lr = 0.00150625, m = 0.9
I0815 17:40:13.356025  8764 solver.cpp:363] Sparsity after update:
I0815 17:40:13.361065  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:40:13.361078  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:40:13.361086  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:40:13.361090  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:40:13.361094  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:40:13.361099  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:40:13.361104  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:40:13.361107  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:40:13.361111  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:40:13.361115  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:40:13.361119  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:40:13.361124  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:40:13.361129  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:40:13.361140  8764 solver.cpp:509] Iteration 136000, Testing net (#0)
I0815 17:40:20.952467  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:40:45.192246  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.59253
I0815 17:40:45.192304  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.823291
I0815 17:40:45.192311  8764 solver.cpp:594]     Test net output #2: loss = 1.76064 (* 1 = 1.76064 loss)
I0815 17:40:45.192332  8764 solver.cpp:264] [MultiGPU] Tests completed in 31.8303s
I0815 17:40:45.353452  8764 solver.cpp:312] Iteration 136000 (1.98758 iter/s, 50.3125s/100 iter), loss = 1.14895
I0815 17:40:45.353524  8764 solver.cpp:334]     Train net output #0: loss = 0.881727 (* 1 = 0.881727 loss)
I0815 17:40:45.353544  8764 sgd_solver.cpp:136] Iteration 136000, lr = 0.0015, m = 0.9
I0815 17:41:03.600164  8764 solver.cpp:312] Iteration 136100 (5.48059 iter/s, 18.2462s/100 iter), loss = 1.44576
I0815 17:41:03.600234  8764 solver.cpp:334]     Train net output #0: loss = 1.17001 (* 1 = 1.17001 loss)
I0815 17:41:03.600251  8764 sgd_solver.cpp:136] Iteration 136100, lr = 0.00149375, m = 0.9
I0815 17:41:23.109555  8764 solver.cpp:312] Iteration 136200 (5.12588 iter/s, 19.5089s/100 iter), loss = 1.11632
I0815 17:41:23.109630  8764 solver.cpp:334]     Train net output #0: loss = 1.11082 (* 1 = 1.11082 loss)
I0815 17:41:23.109644  8764 sgd_solver.cpp:136] Iteration 136200, lr = 0.0014875, m = 0.9
I0815 17:41:43.188300  8764 solver.cpp:312] Iteration 136300 (4.98053 iter/s, 20.0782s/100 iter), loss = 1.29399
I0815 17:41:43.188343  8764 solver.cpp:334]     Train net output #0: loss = 1.21433 (* 1 = 1.21433 loss)
I0815 17:41:43.188351  8764 sgd_solver.cpp:136] Iteration 136300, lr = 0.00148125, m = 0.9
I0815 17:42:01.328286  8764 solver.cpp:312] Iteration 136400 (5.51283 iter/s, 18.1395s/100 iter), loss = 1.2211
I0815 17:42:01.328364  8764 solver.cpp:334]     Train net output #0: loss = 0.924369 (* 1 = 0.924369 loss)
I0815 17:42:01.328380  8764 sgd_solver.cpp:136] Iteration 136400, lr = 0.001475, m = 0.9
I0815 17:42:17.747169  8764 solver.cpp:312] Iteration 136500 (6.09072 iter/s, 16.4184s/100 iter), loss = 1.32803
I0815 17:42:17.747220  8764 solver.cpp:334]     Train net output #0: loss = 1.32719 (* 1 = 1.32719 loss)
I0815 17:42:17.747233  8764 sgd_solver.cpp:136] Iteration 136500, lr = 0.00146875, m = 0.9
I0815 17:42:35.534116  8764 solver.cpp:312] Iteration 136600 (5.62225 iter/s, 17.7865s/100 iter), loss = 1.01832
I0815 17:42:35.534184  8764 solver.cpp:334]     Train net output #0: loss = 1.08146 (* 1 = 1.08146 loss)
I0815 17:42:35.534190  8764 sgd_solver.cpp:136] Iteration 136600, lr = 0.0014625, m = 0.9
I0815 17:42:54.665385  8764 solver.cpp:312] Iteration 136700 (5.22719 iter/s, 19.1307s/100 iter), loss = 1.18513
I0815 17:42:54.665410  8764 solver.cpp:334]     Train net output #0: loss = 1.51201 (* 1 = 1.51201 loss)
I0815 17:42:54.665415  8764 sgd_solver.cpp:136] Iteration 136700, lr = 0.00145625, m = 0.9
I0815 17:43:11.602516  8764 solver.cpp:312] Iteration 136800 (5.90435 iter/s, 16.9367s/100 iter), loss = 1.40795
I0815 17:43:11.602582  8764 solver.cpp:334]     Train net output #0: loss = 1.42816 (* 1 = 1.42816 loss)
I0815 17:43:11.602589  8764 sgd_solver.cpp:136] Iteration 136800, lr = 0.00145, m = 0.9
I0815 17:43:30.369717  8764 solver.cpp:312] Iteration 136900 (5.32859 iter/s, 18.7667s/100 iter), loss = 1.25333
I0815 17:43:30.369740  8764 solver.cpp:334]     Train net output #0: loss = 1.10217 (* 1 = 1.10217 loss)
I0815 17:43:30.369745  8764 sgd_solver.cpp:136] Iteration 136900, lr = 0.00144375, m = 0.9
I0815 17:43:47.454567  8764 solver.cpp:363] Sparsity after update:
I0815 17:43:47.467392  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:43:47.467432  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:43:47.467458  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:43:47.467473  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:43:47.467488  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:43:47.467504  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:43:47.467516  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:43:47.467531  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:43:47.467545  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:43:47.467561  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:43:47.467576  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:43:47.467591  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:43:47.467605  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:43:47.661599  8764 solver.cpp:312] Iteration 137000 (5.78322 iter/s, 17.2914s/100 iter), loss = 1.14758
I0815 17:43:47.661644  8764 solver.cpp:334]     Train net output #0: loss = 1.38834 (* 1 = 1.38834 loss)
I0815 17:43:47.661654  8764 sgd_solver.cpp:136] Iteration 137000, lr = 0.0014375, m = 0.9
I0815 17:44:06.105000  8764 solver.cpp:312] Iteration 137100 (5.42215 iter/s, 18.4429s/100 iter), loss = 1.27495
I0815 17:44:06.105067  8764 solver.cpp:334]     Train net output #0: loss = 1.47891 (* 1 = 1.47891 loss)
I0815 17:44:06.105079  8764 sgd_solver.cpp:136] Iteration 137100, lr = 0.00143125, m = 0.9
I0815 17:44:23.201130  8764 solver.cpp:312] Iteration 137200 (5.84943 iter/s, 17.0957s/100 iter), loss = 1.37619
I0815 17:44:23.201616  8764 solver.cpp:334]     Train net output #0: loss = 1.38702 (* 1 = 1.38702 loss)
I0815 17:44:23.201623  8764 sgd_solver.cpp:136] Iteration 137200, lr = 0.001425, m = 0.9
I0815 17:44:39.739867  8764 solver.cpp:312] Iteration 137300 (6.04658 iter/s, 16.5383s/100 iter), loss = 1.07997
I0815 17:44:39.739895  8764 solver.cpp:334]     Train net output #0: loss = 1.19601 (* 1 = 1.19601 loss)
I0815 17:44:39.739902  8764 sgd_solver.cpp:136] Iteration 137300, lr = 0.00141875, m = 0.9
I0815 17:44:57.247925  8764 solver.cpp:312] Iteration 137400 (5.71181 iter/s, 17.5076s/100 iter), loss = 1.59418
I0815 17:44:57.247987  8764 solver.cpp:334]     Train net output #0: loss = 1.52616 (* 1 = 1.52616 loss)
I0815 17:44:57.247993  8764 sgd_solver.cpp:136] Iteration 137400, lr = 0.0014125, m = 0.9
I0815 17:45:17.918146  8764 solver.cpp:312] Iteration 137500 (4.83801 iter/s, 20.6697s/100 iter), loss = 1.25178
I0815 17:45:17.918216  8764 solver.cpp:334]     Train net output #0: loss = 1.51097 (* 1 = 1.51097 loss)
I0815 17:45:17.918236  8764 sgd_solver.cpp:136] Iteration 137500, lr = 0.00140625, m = 0.9
I0815 17:45:36.116096  8764 solver.cpp:312] Iteration 137600 (5.49528 iter/s, 18.1974s/100 iter), loss = 1.31203
I0815 17:45:36.116900  8764 solver.cpp:334]     Train net output #0: loss = 1.29878 (* 1 = 1.29878 loss)
I0815 17:45:36.116919  8764 sgd_solver.cpp:136] Iteration 137600, lr = 0.0014, m = 0.9
I0815 17:45:54.031064  8764 solver.cpp:312] Iteration 137700 (5.58208 iter/s, 17.9145s/100 iter), loss = 1.18106
I0815 17:45:54.031128  8764 solver.cpp:334]     Train net output #0: loss = 1.08756 (* 1 = 1.08756 loss)
I0815 17:45:54.031146  8764 sgd_solver.cpp:136] Iteration 137700, lr = 0.00139375, m = 0.9
I0815 17:46:11.623597  8764 solver.cpp:312] Iteration 137800 (5.68439 iter/s, 17.592s/100 iter), loss = 1.62696
I0815 17:46:11.623664  8764 solver.cpp:334]     Train net output #0: loss = 1.65726 (* 1 = 1.65726 loss)
I0815 17:46:11.623670  8764 sgd_solver.cpp:136] Iteration 137800, lr = 0.0013875, m = 0.9
I0815 17:46:30.626822  8764 solver.cpp:312] Iteration 137900 (5.26241 iter/s, 19.0027s/100 iter), loss = 1.15448
I0815 17:46:30.627053  8764 solver.cpp:334]     Train net output #0: loss = 0.940812 (* 1 = 0.940812 loss)
I0815 17:46:30.627168  8764 sgd_solver.cpp:136] Iteration 137900, lr = 0.00138125, m = 0.9
I0815 17:46:46.779886  8764 solver.cpp:363] Sparsity after update:
I0815 17:46:46.784196  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:46:46.784209  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:46:46.784217  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:46:46.784221  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:46:46.784236  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:46:46.784245  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:46:46.784253  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:46:46.784262  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:46:46.784271  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:46:46.784279  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:46:46.784287  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:46:46.784296  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:46:46.784303  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:46:46.784318  8764 solver.cpp:509] Iteration 138000, Testing net (#0)
I0815 17:47:14.464735  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588764
I0815 17:47:14.464778  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812997
I0815 17:47:14.464797  8764 solver.cpp:594]     Test net output #2: loss = 1.79901 (* 1 = 1.79901 loss)
I0815 17:47:14.464834  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.6798s
I0815 17:47:14.631477  8764 solver.cpp:312] Iteration 138000 (2.27255 iter/s, 44.0034s/100 iter), loss = 1.32484
I0815 17:47:14.631507  8764 solver.cpp:334]     Train net output #0: loss = 1.36437 (* 1 = 1.36437 loss)
I0815 17:47:14.631515  8764 sgd_solver.cpp:136] Iteration 138000, lr = 0.001375, m = 0.9
I0815 17:47:33.162962  8764 solver.cpp:312] Iteration 138100 (5.39637 iter/s, 18.531s/100 iter), loss = 0.822635
I0815 17:47:33.163017  8764 solver.cpp:334]     Train net output #0: loss = 0.77485 (* 1 = 0.77485 loss)
I0815 17:47:33.163023  8764 sgd_solver.cpp:136] Iteration 138100, lr = 0.00136875, m = 0.9
I0815 17:47:50.674090  8764 solver.cpp:312] Iteration 138200 (5.71081 iter/s, 17.5106s/100 iter), loss = 1.43506
I0815 17:47:50.674161  8764 solver.cpp:334]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0815 17:47:50.674180  8764 sgd_solver.cpp:136] Iteration 138200, lr = 0.0013625, m = 0.9
I0815 17:48:09.449272  8764 solver.cpp:312] Iteration 138300 (5.32633 iter/s, 18.7747s/100 iter), loss = 1.4102
I0815 17:48:09.449333  8764 solver.cpp:334]     Train net output #0: loss = 1.36217 (* 1 = 1.36217 loss)
I0815 17:48:09.449339  8764 sgd_solver.cpp:136] Iteration 138300, lr = 0.00135625, m = 0.9
I0815 17:48:27.120164  8764 solver.cpp:312] Iteration 138400 (5.65918 iter/s, 17.6704s/100 iter), loss = 1.34344
I0815 17:48:27.120220  8764 solver.cpp:334]     Train net output #0: loss = 1.45101 (* 1 = 1.45101 loss)
I0815 17:48:27.120247  8764 sgd_solver.cpp:136] Iteration 138400, lr = 0.00135, m = 0.9
I0815 17:48:49.032104  8764 solver.cpp:312] Iteration 138500 (4.56385 iter/s, 21.9113s/100 iter), loss = 1.61092
I0815 17:48:49.032222  8764 solver.cpp:334]     Train net output #0: loss = 1.68261 (* 1 = 1.68261 loss)
I0815 17:48:49.032239  8764 sgd_solver.cpp:136] Iteration 138500, lr = 0.00134375, m = 0.9
I0815 17:49:10.591570  8764 solver.cpp:312] Iteration 138600 (4.63846 iter/s, 21.5589s/100 iter), loss = 1.41586
I0815 17:49:10.591791  8764 solver.cpp:334]     Train net output #0: loss = 1.41867 (* 1 = 1.41867 loss)
I0815 17:49:10.591905  8764 sgd_solver.cpp:136] Iteration 138600, lr = 0.0013375, m = 0.9
I0815 17:49:31.173488  8764 solver.cpp:312] Iteration 138700 (4.85877 iter/s, 20.5813s/100 iter), loss = 1.74588
I0815 17:49:31.173576  8764 solver.cpp:334]     Train net output #0: loss = 1.13694 (* 1 = 1.13694 loss)
I0815 17:49:31.173581  8764 sgd_solver.cpp:136] Iteration 138700, lr = 0.00133125, m = 0.9
I0815 17:49:54.938567  8764 solver.cpp:312] Iteration 138800 (4.20797 iter/s, 23.7644s/100 iter), loss = 1.28607
I0815 17:49:54.938591  8764 solver.cpp:334]     Train net output #0: loss = 1.28711 (* 1 = 1.28711 loss)
I0815 17:49:54.938597  8764 sgd_solver.cpp:136] Iteration 138800, lr = 0.001325, m = 0.9
I0815 17:50:15.272161  8764 solver.cpp:312] Iteration 138900 (4.91811 iter/s, 20.333s/100 iter), loss = 1.25435
I0815 17:50:15.272284  8764 solver.cpp:334]     Train net output #0: loss = 1.12658 (* 1 = 1.12658 loss)
I0815 17:50:15.272301  8764 sgd_solver.cpp:136] Iteration 138900, lr = 0.00131875, m = 0.9
I0815 17:50:38.089361  8764 solver.cpp:363] Sparsity after update:
I0815 17:50:38.117295  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:50:38.117332  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:50:38.117344  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:50:38.117352  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:50:38.117358  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:50:38.117364  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:50:38.117372  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:50:38.117377  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:50:38.117383  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:50:38.117389  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:50:38.117395  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:50:38.117401  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:50:38.117408  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:50:38.262748  8764 solver.cpp:312] Iteration 139000 (4.34973 iter/s, 22.99s/100 iter), loss = 1.48917
I0815 17:50:38.262776  8764 solver.cpp:334]     Train net output #0: loss = 1.17114 (* 1 = 1.17114 loss)
I0815 17:50:38.262784  8764 sgd_solver.cpp:136] Iteration 139000, lr = 0.0013125, m = 0.9
I0815 17:50:58.841826  8764 solver.cpp:312] Iteration 139100 (4.85944 iter/s, 20.5785s/100 iter), loss = 1.56421
I0815 17:50:58.852162  8764 solver.cpp:334]     Train net output #0: loss = 1.72726 (* 1 = 1.72726 loss)
I0815 17:50:58.852180  8764 sgd_solver.cpp:136] Iteration 139100, lr = 0.00130625, m = 0.9
I0815 17:51:18.768172  8764 solver.cpp:312] Iteration 139200 (5.01862 iter/s, 19.9258s/100 iter), loss = 1.13512
I0815 17:51:18.768220  8764 solver.cpp:334]     Train net output #0: loss = 1.1238 (* 1 = 1.1238 loss)
I0815 17:51:18.768230  8764 sgd_solver.cpp:136] Iteration 139200, lr = 0.0013, m = 0.9
I0815 17:51:29.867888  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:51:36.812602  8764 solver.cpp:312] Iteration 139300 (5.54203 iter/s, 18.0439s/100 iter), loss = 1.31009
I0815 17:51:36.812667  8764 solver.cpp:334]     Train net output #0: loss = 1.27005 (* 1 = 1.27005 loss)
I0815 17:51:36.812685  8764 sgd_solver.cpp:136] Iteration 139300, lr = 0.00129375, m = 0.9
I0815 17:51:57.532510  8764 solver.cpp:312] Iteration 139400 (4.82641 iter/s, 20.7193s/100 iter), loss = 1.52984
I0815 17:51:57.532624  8764 solver.cpp:334]     Train net output #0: loss = 1.46082 (* 1 = 1.46082 loss)
I0815 17:51:57.532636  8764 sgd_solver.cpp:136] Iteration 139400, lr = 0.0012875, m = 0.9
I0815 17:52:18.064563  8764 solver.cpp:312] Iteration 139500 (4.87057 iter/s, 20.5315s/100 iter), loss = 1.03846
I0815 17:52:18.064628  8764 solver.cpp:334]     Train net output #0: loss = 1.11443 (* 1 = 1.11443 loss)
I0815 17:52:18.064637  8764 sgd_solver.cpp:136] Iteration 139500, lr = 0.00128125, m = 0.9
I0815 17:52:37.906922  8764 solver.cpp:312] Iteration 139600 (5.03986 iter/s, 19.8418s/100 iter), loss = 1.49914
I0815 17:52:37.906947  8764 solver.cpp:334]     Train net output #0: loss = 1.51967 (* 1 = 1.51967 loss)
I0815 17:52:37.906951  8764 sgd_solver.cpp:136] Iteration 139600, lr = 0.001275, m = 0.9
I0815 17:52:56.683394  8764 solver.cpp:312] Iteration 139700 (5.32596 iter/s, 18.776s/100 iter), loss = 1.20373
I0815 17:52:56.683465  8764 solver.cpp:334]     Train net output #0: loss = 0.887653 (* 1 = 0.887653 loss)
I0815 17:52:56.683471  8764 sgd_solver.cpp:136] Iteration 139700, lr = 0.00126875, m = 0.9
I0815 17:53:16.098732  8764 solver.cpp:312] Iteration 139800 (5.15071 iter/s, 19.4148s/100 iter), loss = 1.21737
I0815 17:53:16.098759  8764 solver.cpp:334]     Train net output #0: loss = 1.14637 (* 1 = 1.14637 loss)
I0815 17:53:16.098765  8764 sgd_solver.cpp:136] Iteration 139800, lr = 0.0012625, m = 0.9
I0815 17:53:38.102414  8764 solver.cpp:312] Iteration 139900 (4.54482 iter/s, 22.0031s/100 iter), loss = 1.3324
I0815 17:53:38.102484  8764 solver.cpp:334]     Train net output #0: loss = 1.32427 (* 1 = 1.32427 loss)
I0815 17:53:38.102491  8764 sgd_solver.cpp:136] Iteration 139900, lr = 0.00125625, m = 0.9
I0815 17:53:55.182837  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_140000.caffemodel
I0815 17:53:55.307399  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_140000.solverstate
I0815 17:53:55.317852  8764 solver.cpp:363] Sparsity after update:
I0815 17:53:55.320282  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:53:55.320333  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:53:55.320365  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:53:55.320384  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:53:55.320401  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:53:55.320420  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:53:55.320436  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:53:55.320453  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:53:55.320469  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:53:55.320487  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:53:55.320502  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:53:55.320519  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:53:55.320536  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:53:55.320574  8764 solver.cpp:509] Iteration 140000, Testing net (#0)
I0815 17:54:30.001000  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58947
I0815 17:54:30.001096  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819997
I0815 17:54:30.001117  8764 solver.cpp:594]     Test net output #2: loss = 1.78096 (* 1 = 1.78096 loss)
I0815 17:54:30.001159  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.6796s
I0815 17:54:30.418001  8764 solver.cpp:312] Iteration 140000 (1.91153 iter/s, 52.3141s/100 iter), loss = 1.35551
I0815 17:54:30.418027  8764 solver.cpp:334]     Train net output #0: loss = 1.46042 (* 1 = 1.46042 loss)
I0815 17:54:30.418032  8764 sgd_solver.cpp:136] Iteration 140000, lr = 0.00125, m = 0.9
I0815 17:54:50.150614  8764 solver.cpp:312] Iteration 140100 (5.0679 iter/s, 19.7321s/100 iter), loss = 1.44753
I0815 17:54:50.150655  8764 solver.cpp:334]     Train net output #0: loss = 1.59608 (* 1 = 1.59608 loss)
I0815 17:54:50.150665  8764 sgd_solver.cpp:136] Iteration 140100, lr = 0.00124375, m = 0.9
I0815 17:55:06.584578  8764 solver.cpp:312] Iteration 140200 (6.08513 iter/s, 16.4335s/100 iter), loss = 1.70665
I0815 17:55:06.584668  8764 solver.cpp:334]     Train net output #0: loss = 1.81339 (* 1 = 1.81339 loss)
I0815 17:55:06.584686  8764 sgd_solver.cpp:136] Iteration 140200, lr = 0.0012375, m = 0.9
I0815 17:55:27.144171  8764 solver.cpp:312] Iteration 140300 (4.86405 iter/s, 20.559s/100 iter), loss = 1.25533
I0815 17:55:27.144229  8764 solver.cpp:334]     Train net output #0: loss = 1.36387 (* 1 = 1.36387 loss)
I0815 17:55:27.144243  8764 sgd_solver.cpp:136] Iteration 140300, lr = 0.00123125, m = 0.9
I0815 17:55:46.078588  8764 solver.cpp:312] Iteration 140400 (5.28154 iter/s, 18.9339s/100 iter), loss = 1.59459
I0815 17:55:46.078712  8764 solver.cpp:334]     Train net output #0: loss = 1.49622 (* 1 = 1.49622 loss)
I0815 17:55:46.078729  8764 sgd_solver.cpp:136] Iteration 140400, lr = 0.001225, m = 0.9
I0815 17:56:05.341476  8764 solver.cpp:312] Iteration 140500 (5.19147 iter/s, 19.2624s/100 iter), loss = 1.71288
I0815 17:56:05.341506  8764 solver.cpp:334]     Train net output #0: loss = 1.76404 (* 1 = 1.76404 loss)
I0815 17:56:05.341511  8764 sgd_solver.cpp:136] Iteration 140500, lr = 0.00121875, m = 0.9
I0815 17:56:22.166039  8764 solver.cpp:312] Iteration 140600 (5.94385 iter/s, 16.8241s/100 iter), loss = 1.34819
I0815 17:56:22.166093  8764 solver.cpp:334]     Train net output #0: loss = 1.40758 (* 1 = 1.40758 loss)
I0815 17:56:22.166100  8764 sgd_solver.cpp:136] Iteration 140600, lr = 0.0012125, m = 0.9
I0815 17:56:41.821146  8764 solver.cpp:312] Iteration 140700 (5.08788 iter/s, 19.6546s/100 iter), loss = 1.368
I0815 17:56:41.821171  8764 solver.cpp:334]     Train net output #0: loss = 1.13915 (* 1 = 1.13915 loss)
I0815 17:56:41.821177  8764 sgd_solver.cpp:136] Iteration 140700, lr = 0.00120625, m = 0.9
I0815 17:57:01.617628  8764 solver.cpp:312] Iteration 140800 (5.05154 iter/s, 19.7959s/100 iter), loss = 1.17013
I0815 17:57:01.620170  8764 solver.cpp:334]     Train net output #0: loss = 1.23675 (* 1 = 1.23675 loss)
I0815 17:57:01.620187  8764 sgd_solver.cpp:136] Iteration 140800, lr = 0.0012, m = 0.9
I0815 17:57:21.231899  8764 solver.cpp:312] Iteration 140900 (5.09847 iter/s, 19.6137s/100 iter), loss = 1.62627
I0815 17:57:21.231925  8764 solver.cpp:334]     Train net output #0: loss = 1.65989 (* 1 = 1.65989 loss)
I0815 17:57:21.231928  8764 sgd_solver.cpp:136] Iteration 140900, lr = 0.00119375, m = 0.9
I0815 17:57:38.646288  8764 solver.cpp:363] Sparsity after update:
I0815 17:57:38.662535  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:57:38.662631  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:57:38.662663  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:57:38.662674  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:57:38.662683  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:57:38.662693  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:57:38.662703  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:57:38.662714  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:57:38.662724  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:57:38.662732  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:57:38.662742  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:57:38.662751  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:57:38.662760  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:57:38.984992  8764 solver.cpp:312] Iteration 141000 (5.63298 iter/s, 17.7526s/100 iter), loss = 1.39315
I0815 17:57:38.985047  8764 solver.cpp:334]     Train net output #0: loss = 0.956002 (* 1 = 0.956002 loss)
I0815 17:57:38.985056  8764 sgd_solver.cpp:136] Iteration 141000, lr = 0.0011875, m = 0.9
I0815 17:57:58.056679  8764 solver.cpp:312] Iteration 141100 (5.24352 iter/s, 19.0712s/100 iter), loss = 1.41718
I0815 17:57:58.056756  8764 solver.cpp:334]     Train net output #0: loss = 1.97423 (* 1 = 1.97423 loss)
I0815 17:57:58.056785  8764 sgd_solver.cpp:136] Iteration 141100, lr = 0.00118125, m = 0.9
I0815 17:58:16.579645  8764 solver.cpp:312] Iteration 141200 (5.39885 iter/s, 18.5225s/100 iter), loss = 1.45915
I0815 17:58:16.579710  8764 solver.cpp:334]     Train net output #0: loss = 1.38899 (* 1 = 1.38899 loss)
I0815 17:58:16.579718  8764 sgd_solver.cpp:136] Iteration 141200, lr = 0.001175, m = 0.9
I0815 17:58:33.454031  8764 solver.cpp:312] Iteration 141300 (5.9263 iter/s, 16.8739s/100 iter), loss = 1.28441
I0815 17:58:33.454059  8764 solver.cpp:334]     Train net output #0: loss = 1.06432 (* 1 = 1.06432 loss)
I0815 17:58:33.454066  8764 sgd_solver.cpp:136] Iteration 141300, lr = 0.00116875, m = 0.9
I0815 17:58:50.292281  8764 solver.cpp:312] Iteration 141400 (5.93902 iter/s, 16.8378s/100 iter), loss = 1.45143
I0815 17:58:50.292361  8764 solver.cpp:334]     Train net output #0: loss = 1.41304 (* 1 = 1.41304 loss)
I0815 17:58:50.292368  8764 sgd_solver.cpp:136] Iteration 141400, lr = 0.0011625, m = 0.9
I0815 17:59:08.231704  8764 solver.cpp:312] Iteration 141500 (5.57447 iter/s, 17.9389s/100 iter), loss = 1.25331
I0815 17:59:08.231747  8764 solver.cpp:334]     Train net output #0: loss = 1.2311 (* 1 = 1.2311 loss)
I0815 17:59:08.231755  8764 sgd_solver.cpp:136] Iteration 141500, lr = 0.00115625, m = 0.9
I0815 17:59:27.437798  8764 solver.cpp:312] Iteration 141600 (5.20682 iter/s, 19.2056s/100 iter), loss = 1.40819
I0815 17:59:27.438048  8764 solver.cpp:334]     Train net output #0: loss = 1.45161 (* 1 = 1.45161 loss)
I0815 17:59:27.438158  8764 sgd_solver.cpp:136] Iteration 141600, lr = 0.00115, m = 0.9
I0815 17:59:47.669544  8764 solver.cpp:312] Iteration 141700 (4.94286 iter/s, 20.2312s/100 iter), loss = 1.35848
I0815 17:59:47.669713  8764 solver.cpp:334]     Train net output #0: loss = 1.3211 (* 1 = 1.3211 loss)
I0815 17:59:47.669801  8764 sgd_solver.cpp:136] Iteration 141700, lr = 0.00114375, m = 0.9
I0815 18:00:05.298149  8764 solver.cpp:312] Iteration 141800 (5.67276 iter/s, 17.6281s/100 iter), loss = 0.976221
I0815 18:00:05.298247  8764 solver.cpp:334]     Train net output #0: loss = 1.13249 (* 1 = 1.13249 loss)
I0815 18:00:05.298267  8764 sgd_solver.cpp:136] Iteration 141800, lr = 0.0011375, m = 0.9
I0815 18:00:23.264578  8764 solver.cpp:312] Iteration 141900 (5.5661 iter/s, 17.9659s/100 iter), loss = 1.55309
I0815 18:00:23.264652  8764 solver.cpp:334]     Train net output #0: loss = 1.21867 (* 1 = 1.21867 loss)
I0815 18:00:23.264672  8764 sgd_solver.cpp:136] Iteration 141900, lr = 0.00113125, m = 0.9
I0815 18:00:42.068178  8764 solver.cpp:363] Sparsity after update:
I0815 18:00:42.075137  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:00:42.075170  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:00:42.075212  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:00:42.075234  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:00:42.075266  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:00:42.075296  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:00:42.075327  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:00:42.075358  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:00:42.075374  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:00:42.075394  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:00:42.075414  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:00:42.075438  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:00:42.075466  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:00:42.075513  8764 solver.cpp:509] Iteration 142000, Testing net (#0)
I0815 18:01:08.625633  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.596529
I0815 18:01:08.625653  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.82088
I0815 18:01:08.625658  8764 solver.cpp:594]     Test net output #2: loss = 1.75786 (* 1 = 1.75786 loss)
I0815 18:01:08.625676  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.5494s
I0815 18:01:08.793457  8764 solver.cpp:312] Iteration 142000 (2.19647 iter/s, 45.5276s/100 iter), loss = 1.33667
I0815 18:01:08.801151  8764 solver.cpp:334]     Train net output #0: loss = 1.51418 (* 1 = 1.51418 loss)
I0815 18:01:08.801162  8764 sgd_solver.cpp:136] Iteration 142000, lr = 0.001125, m = 0.9
I0815 18:01:14.716812  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:01:28.478665  8764 solver.cpp:312] Iteration 142100 (5.0801 iter/s, 19.6847s/100 iter), loss = 1.188
I0815 18:01:28.478708  8764 solver.cpp:334]     Train net output #0: loss = 1.16061 (* 1 = 1.16061 loss)
I0815 18:01:28.478715  8764 sgd_solver.cpp:136] Iteration 142100, lr = 0.00111875, m = 0.9
I0815 18:01:47.555893  8764 solver.cpp:312] Iteration 142200 (5.242 iter/s, 19.0767s/100 iter), loss = 1.43272
I0815 18:01:47.555970  8764 solver.cpp:334]     Train net output #0: loss = 1.43287 (* 1 = 1.43287 loss)
I0815 18:01:47.555977  8764 sgd_solver.cpp:136] Iteration 142200, lr = 0.0011125, m = 0.9
I0815 18:02:04.430253  8764 solver.cpp:312] Iteration 142300 (5.92632 iter/s, 16.8739s/100 iter), loss = 1.25467
I0815 18:02:04.430275  8764 solver.cpp:334]     Train net output #0: loss = 1.25932 (* 1 = 1.25932 loss)
I0815 18:02:04.430279  8764 sgd_solver.cpp:136] Iteration 142300, lr = 0.00110625, m = 0.9
I0815 18:02:21.930901  8764 solver.cpp:312] Iteration 142400 (5.71423 iter/s, 17.5002s/100 iter), loss = 1.39794
I0815 18:02:21.936416  8764 solver.cpp:334]     Train net output #0: loss = 1.73288 (* 1 = 1.73288 loss)
I0815 18:02:21.936439  8764 sgd_solver.cpp:136] Iteration 142400, lr = 0.0011, m = 0.9
I0815 18:02:39.988863  8764 solver.cpp:312] Iteration 142500 (5.53788 iter/s, 18.0575s/100 iter), loss = 1.76049
I0815 18:02:39.988943  8764 solver.cpp:334]     Train net output #0: loss = 1.79612 (* 1 = 1.79612 loss)
I0815 18:02:39.988965  8764 sgd_solver.cpp:136] Iteration 142500, lr = 0.00109375, m = 0.9
I0815 18:02:58.274222  8764 solver.cpp:312] Iteration 142600 (5.46901 iter/s, 18.2849s/100 iter), loss = 1.41834
I0815 18:02:58.274286  8764 solver.cpp:334]     Train net output #0: loss = 1.17691 (* 1 = 1.17691 loss)
I0815 18:02:58.274293  8764 sgd_solver.cpp:136] Iteration 142600, lr = 0.0010875, m = 0.9
I0815 18:03:16.285676  8764 solver.cpp:312] Iteration 142700 (5.55218 iter/s, 18.0109s/100 iter), loss = 1.29608
I0815 18:03:16.285732  8764 solver.cpp:334]     Train net output #0: loss = 1.14727 (* 1 = 1.14727 loss)
I0815 18:03:16.285742  8764 sgd_solver.cpp:136] Iteration 142700, lr = 0.00108125, m = 0.9
I0815 18:03:33.383854  8764 solver.cpp:312] Iteration 142800 (5.84873 iter/s, 17.0977s/100 iter), loss = 1.21964
I0815 18:03:33.383919  8764 solver.cpp:334]     Train net output #0: loss = 1.11921 (* 1 = 1.11921 loss)
I0815 18:03:33.383924  8764 sgd_solver.cpp:136] Iteration 142800, lr = 0.001075, m = 0.9
I0815 18:03:52.402137  8764 solver.cpp:312] Iteration 142900 (5.25824 iter/s, 19.0178s/100 iter), loss = 1.41301
I0815 18:03:52.402166  8764 solver.cpp:334]     Train net output #0: loss = 1.68517 (* 1 = 1.68517 loss)
I0815 18:03:52.402173  8764 sgd_solver.cpp:136] Iteration 142900, lr = 0.00106875, m = 0.9
I0815 18:04:12.215577  8764 solver.cpp:363] Sparsity after update:
I0815 18:04:12.226796  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:04:12.226838  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:04:12.226855  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:04:12.226866  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:04:12.226876  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:04:12.226884  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:04:12.226893  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:04:12.226902  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:04:12.226910  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:04:12.226919  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:04:12.226928  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:04:12.226938  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:04:12.226946  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:04:12.419339  8764 solver.cpp:312] Iteration 143000 (4.99584 iter/s, 20.0166s/100 iter), loss = 1.2833
I0815 18:04:12.419371  8764 solver.cpp:334]     Train net output #0: loss = 1.30975 (* 1 = 1.30975 loss)
I0815 18:04:12.419376  8764 sgd_solver.cpp:136] Iteration 143000, lr = 0.0010625, m = 0.9
I0815 18:04:30.815928  8764 solver.cpp:312] Iteration 143100 (5.43594 iter/s, 18.3961s/100 iter), loss = 1.23688
I0815 18:04:30.815960  8764 solver.cpp:334]     Train net output #0: loss = 1.1033 (* 1 = 1.1033 loss)
I0815 18:04:30.815965  8764 sgd_solver.cpp:136] Iteration 143100, lr = 0.00105625, m = 0.9
I0815 18:04:48.096513  8764 solver.cpp:312] Iteration 143200 (5.787 iter/s, 17.2801s/100 iter), loss = 1.28413
I0815 18:04:48.096602  8764 solver.cpp:334]     Train net output #0: loss = 1.42287 (* 1 = 1.42287 loss)
I0815 18:04:48.096609  8764 sgd_solver.cpp:136] Iteration 143200, lr = 0.00105, m = 0.9
I0815 18:05:05.148787  8764 solver.cpp:312] Iteration 143300 (5.86448 iter/s, 17.0518s/100 iter), loss = 1.18326
I0815 18:05:05.148814  8764 solver.cpp:334]     Train net output #0: loss = 0.915706 (* 1 = 0.915706 loss)
I0815 18:05:05.148821  8764 sgd_solver.cpp:136] Iteration 143300, lr = 0.00104375, m = 0.9
I0815 18:05:22.618831  8764 solver.cpp:312] Iteration 143400 (5.72424 iter/s, 17.4696s/100 iter), loss = 1.41134
I0815 18:05:22.618914  8764 solver.cpp:334]     Train net output #0: loss = 1.51103 (* 1 = 1.51103 loss)
I0815 18:05:22.618927  8764 sgd_solver.cpp:136] Iteration 143400, lr = 0.0010375, m = 0.9
I0815 18:05:39.067729  8764 solver.cpp:312] Iteration 143500 (6.0796 iter/s, 16.4484s/100 iter), loss = 1.12462
I0815 18:05:39.067811  8764 solver.cpp:334]     Train net output #0: loss = 1.263 (* 1 = 1.263 loss)
I0815 18:05:39.067831  8764 sgd_solver.cpp:136] Iteration 143500, lr = 0.00103125, m = 0.9
I0815 18:05:55.295142  8764 solver.cpp:312] Iteration 143600 (6.16258 iter/s, 16.227s/100 iter), loss = 1.73647
I0815 18:05:55.295198  8764 solver.cpp:334]     Train net output #0: loss = 1.42315 (* 1 = 1.42315 loss)
I0815 18:05:55.295204  8764 sgd_solver.cpp:136] Iteration 143600, lr = 0.001025, m = 0.9
I0815 18:06:15.940618  8764 solver.cpp:312] Iteration 143700 (4.84381 iter/s, 20.6449s/100 iter), loss = 1.46177
I0815 18:06:15.940667  8764 solver.cpp:334]     Train net output #0: loss = 1.75691 (* 1 = 1.75691 loss)
I0815 18:06:15.940678  8764 sgd_solver.cpp:136] Iteration 143700, lr = 0.00101875, m = 0.9
I0815 18:06:35.436874  8764 solver.cpp:312] Iteration 143800 (5.12933 iter/s, 19.4957s/100 iter), loss = 1.19742
I0815 18:06:35.436985  8764 solver.cpp:334]     Train net output #0: loss = 0.896615 (* 1 = 0.896615 loss)
I0815 18:06:35.437001  8764 sgd_solver.cpp:136] Iteration 143800, lr = 0.0010125, m = 0.9
I0815 18:06:55.318963  8764 solver.cpp:312] Iteration 143900 (5.02979 iter/s, 19.8815s/100 iter), loss = 1.51174
I0815 18:06:55.318992  8764 solver.cpp:334]     Train net output #0: loss = 1.25507 (* 1 = 1.25507 loss)
I0815 18:06:55.318997  8764 sgd_solver.cpp:136] Iteration 143900, lr = 0.00100625, m = 0.9
I0815 18:07:13.808414  8764 solver.cpp:363] Sparsity after update:
I0815 18:07:13.812338  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:07:13.812369  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:07:13.812388  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:07:13.812402  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:07:13.812412  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:07:13.812425  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:07:13.812436  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:07:13.812448  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:07:13.812459  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:07:13.812470  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:07:13.812481  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:07:13.812494  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:07:13.812505  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:07:13.812525  8764 solver.cpp:509] Iteration 144000, Testing net (#0)
I0815 18:07:33.054859  8747 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 18:07:48.112546  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.593412
I0815 18:07:48.112669  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817938
I0815 18:07:48.112679  8764 solver.cpp:594]     Test net output #2: loss = 1.77148 (* 1 = 1.77148 loss)
I0815 18:07:48.112809  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.2993s
I0815 18:07:48.417847  8764 solver.cpp:312] Iteration 144000 (1.88333 iter/s, 53.0974s/100 iter), loss = 1.21107
I0815 18:07:48.417889  8764 solver.cpp:334]     Train net output #0: loss = 1.31603 (* 1 = 1.31603 loss)
I0815 18:07:48.417912  8764 sgd_solver.cpp:136] Iteration 144000, lr = 0.001, m = 0.9
I0815 18:08:06.167287  8764 solver.cpp:312] Iteration 144100 (5.63414 iter/s, 17.7489s/100 iter), loss = 1.08475
I0815 18:08:06.167310  8764 solver.cpp:334]     Train net output #0: loss = 0.880872 (* 1 = 0.880872 loss)
I0815 18:08:06.167316  8764 sgd_solver.cpp:136] Iteration 144100, lr = 0.00099375, m = 0.9
I0815 18:08:25.894873  8764 solver.cpp:312] Iteration 144200 (5.06918 iter/s, 19.727s/100 iter), loss = 1.59796
I0815 18:08:25.894991  8764 solver.cpp:334]     Train net output #0: loss = 1.13678 (* 1 = 1.13678 loss)
I0815 18:08:25.895009  8764 sgd_solver.cpp:136] Iteration 144200, lr = 0.0009875, m = 0.9
I0815 18:08:46.794845  8764 solver.cpp:312] Iteration 144300 (4.78483 iter/s, 20.8994s/100 iter), loss = 1.55355
I0815 18:08:46.794875  8764 solver.cpp:334]     Train net output #0: loss = 2.17058 (* 1 = 2.17058 loss)
I0815 18:08:46.794881  8764 sgd_solver.cpp:136] Iteration 144300, lr = 0.00098125, m = 0.9
I0815 18:09:09.836156  8764 solver.cpp:312] Iteration 144400 (4.34015 iter/s, 23.0407s/100 iter), loss = 1.17666
I0815 18:09:09.836259  8764 solver.cpp:334]     Train net output #0: loss = 1.14441 (* 1 = 1.14441 loss)
I0815 18:09:09.836406  8764 sgd_solver.cpp:136] Iteration 144400, lr = 0.000975, m = 0.9
I0815 18:09:31.270547  8764 solver.cpp:312] Iteration 144500 (4.66553 iter/s, 21.4338s/100 iter), loss = 1.03128
I0815 18:09:31.270680  8764 solver.cpp:334]     Train net output #0: loss = 0.871275 (* 1 = 0.871275 loss)
I0815 18:09:31.270725  8764 sgd_solver.cpp:136] Iteration 144500, lr = 0.00096875, m = 0.9
I0815 18:09:52.051170  8764 solver.cpp:312] Iteration 144600 (4.81231 iter/s, 20.7801s/100 iter), loss = 1.21673
I0815 18:09:52.051235  8764 solver.cpp:334]     Train net output #0: loss = 1.39767 (* 1 = 1.39767 loss)
I0815 18:09:52.051242  8764 sgd_solver.cpp:136] Iteration 144600, lr = 0.0009625, m = 0.9
I0815 18:10:15.356173  8764 solver.cpp:312] Iteration 144700 (4.29105 iter/s, 23.3043s/100 iter), loss = 1.22525
I0815 18:10:15.356442  8764 solver.cpp:334]     Train net output #0: loss = 1.57252 (* 1 = 1.57252 loss)
I0815 18:10:15.356572  8764 sgd_solver.cpp:136] Iteration 144700, lr = 0.00095625, m = 0.9
I0815 18:10:35.409934  8764 solver.cpp:312] Iteration 144800 (4.98676 iter/s, 20.0531s/100 iter), loss = 1.45445
I0815 18:10:35.416189  8764 solver.cpp:334]     Train net output #0: loss = 1.42374 (* 1 = 1.42374 loss)
I0815 18:10:35.416229  8764 sgd_solver.cpp:136] Iteration 144800, lr = 0.00095, m = 0.9
I0815 18:10:54.077018  8764 solver.cpp:312] Iteration 144900 (5.35716 iter/s, 18.6666s/100 iter), loss = 1.76503
I0815 18:10:54.077044  8764 solver.cpp:334]     Train net output #0: loss = 1.74115 (* 1 = 1.74115 loss)
I0815 18:10:54.077050  8764 sgd_solver.cpp:136] Iteration 144900, lr = 0.00094375, m = 0.9
I0815 18:11:14.843029  8764 solver.cpp:363] Sparsity after update:
I0815 18:11:14.857729  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:11:14.857755  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:11:14.857771  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:11:14.857777  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:11:14.857782  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:11:14.857789  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:11:14.857795  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:11:14.857801  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:11:14.857807  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:11:14.857813  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:11:14.857820  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:11:14.857826  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:11:14.857832  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:11:14.999878  8764 solver.cpp:312] Iteration 145000 (4.77959 iter/s, 20.9223s/100 iter), loss = 1.17689
I0815 18:11:14.999907  8764 solver.cpp:334]     Train net output #0: loss = 1.40365 (* 1 = 1.40365 loss)
I0815 18:11:14.999912  8764 sgd_solver.cpp:136] Iteration 145000, lr = 0.0009375, m = 0.9
I0815 18:11:35.153436  8764 solver.cpp:312] Iteration 145100 (4.96204 iter/s, 20.153s/100 iter), loss = 1.48255
I0815 18:11:35.153492  8764 solver.cpp:334]     Train net output #0: loss = 1.55291 (* 1 = 1.55291 loss)
I0815 18:11:35.153502  8764 sgd_solver.cpp:136] Iteration 145100, lr = 0.00093125, m = 0.9
I0815 18:11:53.893126  8764 solver.cpp:312] Iteration 145200 (5.33642 iter/s, 18.7392s/100 iter), loss = 1.47679
I0815 18:11:53.893246  8764 solver.cpp:334]     Train net output #0: loss = 1.99516 (* 1 = 1.99516 loss)
I0815 18:11:53.893261  8764 sgd_solver.cpp:136] Iteration 145200, lr = 0.000925, m = 0.9
I0815 18:12:16.416522  8764 solver.cpp:312] Iteration 145300 (4.43995 iter/s, 22.5228s/100 iter), loss = 1.55123
I0815 18:12:16.416563  8764 solver.cpp:334]     Train net output #0: loss = 1.583 (* 1 = 1.583 loss)
I0815 18:12:16.416574  8764 sgd_solver.cpp:136] Iteration 145300, lr = 0.00091875, m = 0.9
I0815 18:12:38.105657  8764 solver.cpp:312] Iteration 145400 (4.61073 iter/s, 21.6885s/100 iter), loss = 1.4198
I0815 18:12:38.105775  8764 solver.cpp:334]     Train net output #0: loss = 1.36147 (* 1 = 1.36147 loss)
I0815 18:12:38.105794  8764 sgd_solver.cpp:136] Iteration 145400, lr = 0.0009125, m = 0.9
I0815 18:12:57.528532  8764 solver.cpp:312] Iteration 145500 (5.14871 iter/s, 19.4223s/100 iter), loss = 1.2938
I0815 18:12:57.528556  8764 solver.cpp:334]     Train net output #0: loss = 1.23705 (* 1 = 1.23705 loss)
I0815 18:12:57.528560  8764 sgd_solver.cpp:136] Iteration 145500, lr = 0.00090625, m = 0.9
I0815 18:13:16.017442  8764 solver.cpp:312] Iteration 145600 (5.4088 iter/s, 18.4884s/100 iter), loss = 1.18555
I0815 18:13:16.017621  8764 solver.cpp:334]     Train net output #0: loss = 1.33523 (* 1 = 1.33523 loss)
I0815 18:13:16.017638  8764 sgd_solver.cpp:136] Iteration 145600, lr = 0.0009, m = 0.9
I0815 18:13:38.416702  8764 solver.cpp:312] Iteration 145700 (4.46456 iter/s, 22.3986s/100 iter), loss = 1.39726
I0815 18:13:38.416774  8764 solver.cpp:334]     Train net output #0: loss = 1.34397 (* 1 = 1.34397 loss)
I0815 18:13:38.416793  8764 sgd_solver.cpp:136] Iteration 145700, lr = 0.00089375, m = 0.9
I0815 18:13:57.486547  8764 solver.cpp:312] Iteration 145800 (5.24403 iter/s, 19.0693s/100 iter), loss = 1.35753
I0815 18:13:57.486598  8764 solver.cpp:334]     Train net output #0: loss = 1.41632 (* 1 = 1.41632 loss)
I0815 18:13:57.486604  8764 sgd_solver.cpp:136] Iteration 145800, lr = 0.0008875, m = 0.9
I0815 18:14:17.385567  8764 solver.cpp:312] Iteration 145900 (5.02551 iter/s, 19.8985s/100 iter), loss = 1.72988
I0815 18:14:17.385632  8764 solver.cpp:334]     Train net output #0: loss = 1.84464 (* 1 = 1.84464 loss)
I0815 18:14:17.385648  8764 sgd_solver.cpp:136] Iteration 145900, lr = 0.00088125, m = 0.9
I0815 18:14:40.530297  8764 solver.cpp:363] Sparsity after update:
I0815 18:14:40.534569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:14:40.534591  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:14:40.534612  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:14:40.534621  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:14:40.534628  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:14:40.534636  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:14:40.534644  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:14:40.534652  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:14:40.534660  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:14:40.534668  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:14:40.534677  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:14:40.534684  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:14:40.534693  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:14:40.534715  8764 solver.cpp:509] Iteration 146000, Testing net (#0)
I0815 18:14:48.654812  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:15:08.022887  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.591471
I0815 18:15:08.022924  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.822762
I0815 18:15:08.022943  8764 solver.cpp:594]     Test net output #2: loss = 1.76654 (* 1 = 1.76654 loss)
I0815 18:15:08.022987  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.4875s
I0815 18:15:08.202162  8764 solver.cpp:312] Iteration 146000 (1.96792 iter/s, 50.8152s/100 iter), loss = 1.30681
I0815 18:15:08.202229  8764 solver.cpp:334]     Train net output #0: loss = 1.2685 (* 1 = 1.2685 loss)
I0815 18:15:08.202327  8764 sgd_solver.cpp:136] Iteration 146000, lr = 0.000875, m = 0.9
I0815 18:15:25.561092  8764 solver.cpp:312] Iteration 146100 (5.76088 iter/s, 17.3585s/100 iter), loss = 1.16874
I0815 18:15:25.561199  8764 solver.cpp:334]     Train net output #0: loss = 0.961865 (* 1 = 0.961865 loss)
I0815 18:15:25.561218  8764 sgd_solver.cpp:136] Iteration 146100, lr = 0.00086875, m = 0.9
I0815 18:15:44.016202  8764 solver.cpp:312] Iteration 146200 (5.41871 iter/s, 18.4546s/100 iter), loss = 1.09653
I0815 18:15:44.016305  8764 solver.cpp:334]     Train net output #0: loss = 1.18996 (* 1 = 1.18996 loss)
I0815 18:15:44.016329  8764 sgd_solver.cpp:136] Iteration 146200, lr = 0.0008625, m = 0.9
I0815 18:16:04.459015  8764 solver.cpp:312] Iteration 146300 (4.89183 iter/s, 20.4423s/100 iter), loss = 1.2781
I0815 18:16:04.459086  8764 solver.cpp:334]     Train net output #0: loss = 1.18562 (* 1 = 1.18562 loss)
I0815 18:16:04.459095  8764 sgd_solver.cpp:136] Iteration 146300, lr = 0.00085625, m = 0.9
I0815 18:16:24.012154  8764 solver.cpp:312] Iteration 146400 (5.11441 iter/s, 19.5526s/100 iter), loss = 1.57977
I0815 18:16:24.012182  8764 solver.cpp:334]     Train net output #0: loss = 1.72565 (* 1 = 1.72565 loss)
I0815 18:16:24.012187  8764 sgd_solver.cpp:136] Iteration 146400, lr = 0.00085, m = 0.9
I0815 18:16:42.160375  8764 solver.cpp:312] Iteration 146500 (5.51034 iter/s, 18.1477s/100 iter), loss = 1.28108
I0815 18:16:42.168166  8764 solver.cpp:334]     Train net output #0: loss = 1.35219 (* 1 = 1.35219 loss)
I0815 18:16:42.168315  8764 sgd_solver.cpp:136] Iteration 146500, lr = 0.00084375, m = 0.9
I0815 18:17:00.498065  8764 solver.cpp:312] Iteration 146600 (5.4534 iter/s, 18.3372s/100 iter), loss = 1.52768
I0815 18:17:00.498088  8764 solver.cpp:334]     Train net output #0: loss = 1.60172 (* 1 = 1.60172 loss)
I0815 18:17:00.498095  8764 sgd_solver.cpp:136] Iteration 146600, lr = 0.0008375, m = 0.9
I0815 18:17:19.139434  8764 solver.cpp:312] Iteration 146700 (5.36456 iter/s, 18.6409s/100 iter), loss = 1.22082
I0815 18:17:19.139495  8764 solver.cpp:334]     Train net output #0: loss = 1.15194 (* 1 = 1.15194 loss)
I0815 18:17:19.139503  8764 sgd_solver.cpp:136] Iteration 146700, lr = 0.00083125, m = 0.9
I0815 18:17:37.327904  8764 solver.cpp:312] Iteration 146800 (5.49814 iter/s, 18.188s/100 iter), loss = 1.55639
I0815 18:17:37.327957  8764 solver.cpp:334]     Train net output #0: loss = 1.96382 (* 1 = 1.96382 loss)
I0815 18:17:37.327967  8764 sgd_solver.cpp:136] Iteration 146800, lr = 0.000825, m = 0.9
I0815 18:17:54.797890  8764 solver.cpp:312] Iteration 146900 (5.72426 iter/s, 17.4695s/100 iter), loss = 1.32375
I0815 18:17:54.797996  8764 solver.cpp:334]     Train net output #0: loss = 1.36496 (* 1 = 1.36496 loss)
I0815 18:17:54.798014  8764 sgd_solver.cpp:136] Iteration 146900, lr = 0.00081875, m = 0.9
I0815 18:18:14.038336  8764 solver.cpp:363] Sparsity after update:
I0815 18:18:14.040801  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:18:14.040819  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:18:14.040837  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:18:14.040845  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:18:14.040851  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:18:14.040858  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:18:14.040863  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:18:14.040868  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:18:14.040874  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:18:14.040879  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:18:14.040884  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:18:14.040891  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:18:14.040897  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:18:14.267163  8764 solver.cpp:312] Iteration 147000 (5.13644 iter/s, 19.4687s/100 iter), loss = 1.87763
I0815 18:18:14.267205  8764 solver.cpp:334]     Train net output #0: loss = 2.01834 (* 1 = 2.01834 loss)
I0815 18:18:14.267213  8764 sgd_solver.cpp:136] Iteration 147000, lr = 0.0008125, m = 0.9
I0815 18:18:32.495434  8764 solver.cpp:312] Iteration 147100 (5.48614 iter/s, 18.2278s/100 iter), loss = 1.44026
I0815 18:18:32.495548  8764 solver.cpp:334]     Train net output #0: loss = 1.18449 (* 1 = 1.18449 loss)
I0815 18:18:32.495561  8764 sgd_solver.cpp:136] Iteration 147100, lr = 0.00080625, m = 0.9
I0815 18:18:49.951290  8764 solver.cpp:312] Iteration 147200 (5.7289 iter/s, 17.4554s/100 iter), loss = 1.51572
I0815 18:18:49.951336  8764 solver.cpp:334]     Train net output #0: loss = 1.95641 (* 1 = 1.95641 loss)
I0815 18:18:49.951350  8764 sgd_solver.cpp:136] Iteration 147200, lr = 0.0008, m = 0.9
I0815 18:19:07.653311  8764 solver.cpp:312] Iteration 147300 (5.64923 iter/s, 17.7015s/100 iter), loss = 1.03106
I0815 18:19:07.653388  8764 solver.cpp:334]     Train net output #0: loss = 1.02803 (* 1 = 1.02803 loss)
I0815 18:19:07.653403  8764 sgd_solver.cpp:136] Iteration 147300, lr = 0.00079375, m = 0.9
I0815 18:19:27.797684  8764 solver.cpp:312] Iteration 147400 (4.9643 iter/s, 20.1438s/100 iter), loss = 1.32335
I0815 18:19:27.797710  8764 solver.cpp:334]     Train net output #0: loss = 1.41664 (* 1 = 1.41664 loss)
I0815 18:19:27.797716  8764 sgd_solver.cpp:136] Iteration 147400, lr = 0.0007875, m = 0.9
I0815 18:19:45.468873  8764 solver.cpp:312] Iteration 147500 (5.65909 iter/s, 17.6707s/100 iter), loss = 1.30397
I0815 18:19:45.468971  8764 solver.cpp:334]     Train net output #0: loss = 1.16632 (* 1 = 1.16632 loss)
I0815 18:19:45.468991  8764 sgd_solver.cpp:136] Iteration 147500, lr = 0.00078125, m = 0.9
I0815 18:20:02.325141  8764 solver.cpp:312] Iteration 147600 (5.93267 iter/s, 16.8558s/100 iter), loss = 1.28866
I0815 18:20:02.325173  8764 solver.cpp:334]     Train net output #0: loss = 1.43856 (* 1 = 1.43856 loss)
I0815 18:20:02.325178  8764 sgd_solver.cpp:136] Iteration 147600, lr = 0.000775, m = 0.9
I0815 18:20:20.612856  8764 solver.cpp:312] Iteration 147700 (5.4683 iter/s, 18.2872s/100 iter), loss = 1.37963
I0815 18:20:20.612926  8764 solver.cpp:334]     Train net output #0: loss = 1.32779 (* 1 = 1.32779 loss)
I0815 18:20:20.612934  8764 sgd_solver.cpp:136] Iteration 147700, lr = 0.00076875, m = 0.9
I0815 18:20:38.241430  8764 solver.cpp:312] Iteration 147800 (5.67276 iter/s, 17.6281s/100 iter), loss = 1.701
I0815 18:20:38.241629  8764 solver.cpp:334]     Train net output #0: loss = 1.56866 (* 1 = 1.56866 loss)
I0815 18:20:38.241737  8764 sgd_solver.cpp:136] Iteration 147800, lr = 0.0007625, m = 0.9
I0815 18:20:57.653668  8764 solver.cpp:312] Iteration 147900 (5.15153 iter/s, 19.4117s/100 iter), loss = 1.40123
I0815 18:20:57.653729  8764 solver.cpp:334]     Train net output #0: loss = 1.68219 (* 1 = 1.68219 loss)
I0815 18:20:57.653735  8764 sgd_solver.cpp:136] Iteration 147900, lr = 0.00075625, m = 0.9
I0815 18:21:14.048931  8764 solver.cpp:363] Sparsity after update:
I0815 18:21:14.053117  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:21:14.053128  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:21:14.053134  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:21:14.053138  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:21:14.053143  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:21:14.053145  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:21:14.053148  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:21:14.053151  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:21:14.053156  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:21:14.053159  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:21:14.053164  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:21:14.053166  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:21:14.053169  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:21:14.053179  8764 solver.cpp:509] Iteration 148000, Testing net (#0)
I0815 18:21:43.223417  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.598294
I0815 18:21:43.223523  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.821115
I0815 18:21:43.223534  8764 solver.cpp:594]     Test net output #2: loss = 1.75191 (* 1 = 1.75191 loss)
I0815 18:21:43.223557  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.1696s
I0815 18:21:43.385782  8764 solver.cpp:312] Iteration 148000 (2.18671 iter/s, 45.7309s/100 iter), loss = 0.953746
I0815 18:21:43.385814  8764 solver.cpp:334]     Train net output #0: loss = 1.10621 (* 1 = 1.10621 loss)
I0815 18:21:43.385820  8764 sgd_solver.cpp:136] Iteration 148000, lr = 0.00075, m = 0.9
I0815 18:22:00.613473  8764 solver.cpp:312] Iteration 148100 (5.80477 iter/s, 17.2272s/100 iter), loss = 1.38388
I0815 18:22:00.613497  8764 solver.cpp:334]     Train net output #0: loss = 1.53856 (* 1 = 1.53856 loss)
I0815 18:22:00.613503  8764 sgd_solver.cpp:136] Iteration 148100, lr = 0.00074375, m = 0.9
I0815 18:22:16.949339  8764 solver.cpp:312] Iteration 148200 (6.12167 iter/s, 16.3354s/100 iter), loss = 1.08029
I0815 18:22:16.949405  8764 solver.cpp:334]     Train net output #0: loss = 1.13575 (* 1 = 1.13575 loss)
I0815 18:22:16.949410  8764 sgd_solver.cpp:136] Iteration 148200, lr = 0.0007375, m = 0.9
I0815 18:22:33.588157  8764 solver.cpp:312] Iteration 148300 (6.01021 iter/s, 16.6384s/100 iter), loss = 1.29462
I0815 18:22:33.588222  8764 solver.cpp:334]     Train net output #0: loss = 0.888163 (* 1 = 0.888163 loss)
I0815 18:22:33.588239  8764 sgd_solver.cpp:136] Iteration 148300, lr = 0.00073125, m = 0.9
I0815 18:22:51.810058  8764 solver.cpp:312] Iteration 148400 (5.48805 iter/s, 18.2214s/100 iter), loss = 1.37311
I0815 18:22:51.810150  8764 solver.cpp:334]     Train net output #0: loss = 1.5012 (* 1 = 1.5012 loss)
I0815 18:22:51.810168  8764 sgd_solver.cpp:136] Iteration 148400, lr = 0.000725, m = 0.9
I0815 18:23:11.734082  8764 solver.cpp:312] Iteration 148500 (5.01921 iter/s, 19.9235s/100 iter), loss = 1.18009
I0815 18:23:11.734136  8764 solver.cpp:334]     Train net output #0: loss = 1.20323 (* 1 = 1.20323 loss)
I0815 18:23:11.734148  8764 sgd_solver.cpp:136] Iteration 148500, lr = 0.00071875, m = 0.9
I0815 18:23:29.408939  8764 solver.cpp:312] Iteration 148600 (5.65791 iter/s, 17.6744s/100 iter), loss = 1.48631
I0815 18:23:29.408999  8764 solver.cpp:334]     Train net output #0: loss = 1.36233 (* 1 = 1.36233 loss)
I0815 18:23:29.409006  8764 sgd_solver.cpp:136] Iteration 148600, lr = 0.0007125, m = 0.9
I0815 18:23:47.569486  8764 solver.cpp:312] Iteration 148700 (5.5066 iter/s, 18.16s/100 iter), loss = 1.40502
I0815 18:23:47.569527  8764 solver.cpp:334]     Train net output #0: loss = 1.43613 (* 1 = 1.43613 loss)
I0815 18:23:47.569537  8764 sgd_solver.cpp:136] Iteration 148700, lr = 0.00070625, m = 0.9
I0815 18:24:08.987157  8764 solver.cpp:312] Iteration 148800 (4.66917 iter/s, 21.4171s/100 iter), loss = 1.13462
I0815 18:24:08.987228  8764 solver.cpp:334]     Train net output #0: loss = 0.960904 (* 1 = 0.960904 loss)
I0815 18:24:08.987236  8764 sgd_solver.cpp:136] Iteration 148800, lr = 0.0007, m = 0.9
I0815 18:24:31.011000  8764 solver.cpp:312] Iteration 148900 (4.54066 iter/s, 22.0232s/100 iter), loss = 1.17485
I0815 18:24:31.011023  8764 solver.cpp:334]     Train net output #0: loss = 0.990165 (* 1 = 0.990165 loss)
I0815 18:24:31.011027  8764 sgd_solver.cpp:136] Iteration 148900, lr = 0.00069375, m = 0.9
I0815 18:24:50.457396  8764 solver.cpp:363] Sparsity after update:
I0815 18:24:50.463256  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:24:50.463536  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:24:50.463634  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:24:50.463892  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:24:50.463946  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:24:50.464149  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:24:50.464184  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:24:50.464213  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:24:50.464248  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:24:50.464282  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:24:50.464320  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:24:50.464356  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:24:50.464391  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:24:50.796201  8764 solver.cpp:312] Iteration 149000 (5.05442 iter/s, 19.7846s/100 iter), loss = 1.60309
I0815 18:24:50.796247  8764 solver.cpp:334]     Train net output #0: loss = 1.6185 (* 1 = 1.6185 loss)
I0815 18:24:50.796254  8764 sgd_solver.cpp:136] Iteration 149000, lr = 0.0006875, m = 0.9
I0815 18:25:11.172166  8764 solver.cpp:312] Iteration 149100 (4.90788 iter/s, 20.3754s/100 iter), loss = 1.4493
I0815 18:25:11.172214  8764 solver.cpp:334]     Train net output #0: loss = 1.64427 (* 1 = 1.64427 loss)
I0815 18:25:11.172225  8764 sgd_solver.cpp:136] Iteration 149100, lr = 0.00068125, m = 0.9
I0815 18:25:31.553834  8764 solver.cpp:312] Iteration 149200 (4.90651 iter/s, 20.3811s/100 iter), loss = 1.25752
I0815 18:25:31.553935  8764 solver.cpp:334]     Train net output #0: loss = 1.32981 (* 1 = 1.32981 loss)
I0815 18:25:31.553951  8764 sgd_solver.cpp:136] Iteration 149200, lr = 0.000675, m = 0.9
I0815 18:25:51.174134  8764 solver.cpp:312] Iteration 149300 (5.0969 iter/s, 19.6198s/100 iter), loss = 1.15842
I0815 18:25:51.174161  8764 solver.cpp:334]     Train net output #0: loss = 1.00472 (* 1 = 1.00472 loss)
I0815 18:25:51.174166  8764 sgd_solver.cpp:136] Iteration 149300, lr = 0.00066875, m = 0.9
I0815 18:26:09.737733  8764 solver.cpp:312] Iteration 149400 (5.38704 iter/s, 18.5631s/100 iter), loss = 1.57416
I0815 18:26:09.737799  8764 solver.cpp:334]     Train net output #0: loss = 1.73623 (* 1 = 1.73623 loss)
I0815 18:26:09.737809  8764 sgd_solver.cpp:136] Iteration 149400, lr = 0.0006625, m = 0.9
I0815 18:26:22.359629  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:26:29.145144  8764 solver.cpp:312] Iteration 149500 (5.15281 iter/s, 19.4069s/100 iter), loss = 1.36699
I0815 18:26:29.145171  8764 solver.cpp:334]     Train net output #0: loss = 1.3766 (* 1 = 1.3766 loss)
I0815 18:26:29.145176  8764 sgd_solver.cpp:136] Iteration 149500, lr = 0.00065625, m = 0.9
I0815 18:26:52.337752  8764 solver.cpp:312] Iteration 149600 (4.31184 iter/s, 23.192s/100 iter), loss = 1.34906
I0815 18:26:52.337983  8764 solver.cpp:334]     Train net output #0: loss = 1.36515 (* 1 = 1.36515 loss)
I0815 18:26:52.338003  8764 sgd_solver.cpp:136] Iteration 149600, lr = 0.00065, m = 0.9
I0815 18:27:09.333326  8764 solver.cpp:312] Iteration 149700 (5.88405 iter/s, 16.9951s/100 iter), loss = 1.50831
I0815 18:27:09.333369  8764 solver.cpp:334]     Train net output #0: loss = 1.32581 (* 1 = 1.32581 loss)
I0815 18:27:09.333376  8764 sgd_solver.cpp:136] Iteration 149700, lr = 0.00064375, m = 0.9
I0815 18:27:31.019954  8764 solver.cpp:312] Iteration 149800 (4.61126 iter/s, 21.686s/100 iter), loss = 1.17475
I0815 18:27:31.020043  8764 solver.cpp:334]     Train net output #0: loss = 1.20085 (* 1 = 1.20085 loss)
I0815 18:27:31.020061  8764 sgd_solver.cpp:136] Iteration 149800, lr = 0.0006375, m = 0.9
I0815 18:27:50.891664  8764 solver.cpp:312] Iteration 149900 (5.03242 iter/s, 19.8712s/100 iter), loss = 1.13607
I0815 18:27:50.891693  8764 solver.cpp:334]     Train net output #0: loss = 1.10972 (* 1 = 1.10972 loss)
I0815 18:27:50.891700  8764 sgd_solver.cpp:136] Iteration 149900, lr = 0.00063125, m = 0.9
I0815 18:28:10.720301  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_150000.caffemodel
I0815 18:28:11.453502  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_150000.solverstate
I0815 18:28:11.460384  8764 solver.cpp:363] Sparsity after update:
I0815 18:28:11.462697  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:28:11.462713  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:28:11.462725  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:28:11.462728  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:28:11.462733  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:28:11.462736  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:28:11.462739  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:28:11.462743  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:28:11.462745  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:28:11.462749  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:28:11.462751  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:28:11.462754  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:28:11.462757  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:28:11.462769  8764 solver.cpp:509] Iteration 150000, Testing net (#0)
I0815 18:28:45.506237  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.595353
I0815 18:28:45.506284  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819879
I0815 18:28:45.506292  8764 solver.cpp:594]     Test net output #2: loss = 1.77304 (* 1 = 1.77304 loss)
I0815 18:28:45.506314  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.0426s
I0815 18:28:45.655216  8764 solver.cpp:312] Iteration 150000 (1.82608 iter/s, 54.762s/100 iter), loss = 1.43232
I0815 18:28:45.655267  8764 solver.cpp:334]     Train net output #0: loss = 1.59865 (* 1 = 1.59865 loss)
I0815 18:28:45.655282  8764 sgd_solver.cpp:136] Iteration 150000, lr = 0.000625, m = 0.9
I0815 18:29:06.778203  8764 solver.cpp:312] Iteration 150100 (4.73432 iter/s, 21.1224s/100 iter), loss = 1.00255
I0815 18:29:06.778342  8764 solver.cpp:334]     Train net output #0: loss = 1.06394 (* 1 = 1.06394 loss)
I0815 18:29:06.778390  8764 sgd_solver.cpp:136] Iteration 150100, lr = 0.00061875, m = 0.9
I0815 18:29:25.715240  8764 solver.cpp:312] Iteration 150200 (5.2808 iter/s, 18.9365s/100 iter), loss = 1.15076
I0815 18:29:25.715296  8764 solver.cpp:334]     Train net output #0: loss = 1.16023 (* 1 = 1.16023 loss)
I0815 18:29:25.715301  8764 sgd_solver.cpp:136] Iteration 150200, lr = 0.0006125, m = 0.9
I0815 18:29:47.223006  8764 solver.cpp:312] Iteration 150300 (4.64961 iter/s, 21.5072s/100 iter), loss = 1.23269
I0815 18:29:47.223034  8764 solver.cpp:334]     Train net output #0: loss = 1.07968 (* 1 = 1.07968 loss)
I0815 18:29:47.223040  8764 sgd_solver.cpp:136] Iteration 150300, lr = 0.00060625, m = 0.9
I0815 18:30:08.687047  8764 solver.cpp:312] Iteration 150400 (4.65908 iter/s, 21.4634s/100 iter), loss = 1.45584
I0815 18:30:08.687104  8764 solver.cpp:334]     Train net output #0: loss = 1.42681 (* 1 = 1.42681 loss)
I0815 18:30:08.687111  8764 sgd_solver.cpp:136] Iteration 150400, lr = 0.0006, m = 0.9
I0815 18:30:28.392871  8764 solver.cpp:312] Iteration 150500 (5.07478 iter/s, 19.7053s/100 iter), loss = 1.29201
I0815 18:30:28.392897  8764 solver.cpp:334]     Train net output #0: loss = 1.14599 (* 1 = 1.14599 loss)
I0815 18:30:28.392902  8764 sgd_solver.cpp:136] Iteration 150500, lr = 0.00059375, m = 0.9
I0815 18:30:45.299581  8764 solver.cpp:312] Iteration 150600 (5.91498 iter/s, 16.9062s/100 iter), loss = 0.933011
I0815 18:30:45.299746  8764 solver.cpp:334]     Train net output #0: loss = 0.951353 (* 1 = 0.951353 loss)
I0815 18:30:45.299778  8764 sgd_solver.cpp:136] Iteration 150600, lr = 0.0005875, m = 0.9
I0815 18:31:06.404839  8764 solver.cpp:312] Iteration 150700 (4.73829 iter/s, 21.1047s/100 iter), loss = 1.40058
I0815 18:31:06.405057  8764 solver.cpp:334]     Train net output #0: loss = 1.58134 (* 1 = 1.58134 loss)
I0815 18:31:06.405167  8764 sgd_solver.cpp:136] Iteration 150700, lr = 0.00058125, m = 0.9
I0815 18:31:26.867621  8764 solver.cpp:312] Iteration 150800 (4.88706 iter/s, 20.4622s/100 iter), loss = 1.6061
I0815 18:31:26.867681  8764 solver.cpp:334]     Train net output #0: loss = 1.6731 (* 1 = 1.6731 loss)
I0815 18:31:26.867686  8764 sgd_solver.cpp:136] Iteration 150800, lr = 0.000575, m = 0.9
I0815 18:31:45.967932  8764 solver.cpp:312] Iteration 150900 (5.23566 iter/s, 19.0998s/100 iter), loss = 1.19172
I0815 18:31:45.967998  8764 solver.cpp:334]     Train net output #0: loss = 0.923005 (* 1 = 0.923005 loss)
I0815 18:31:45.968014  8764 sgd_solver.cpp:136] Iteration 150900, lr = 0.00056875, m = 0.9
I0815 18:32:04.068944  8764 solver.cpp:363] Sparsity after update:
I0815 18:32:04.080021  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:32:04.080034  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:32:04.080042  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:32:04.080046  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:32:04.080049  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:32:04.080055  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:32:04.080058  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:32:04.080062  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:32:04.080065  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:32:04.080067  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:32:04.080070  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:32:04.080073  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:32:04.080076  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:32:04.215924  8764 solver.cpp:312] Iteration 151000 (5.48021 iter/s, 18.2475s/100 iter), loss = 1.22529
I0815 18:32:04.215991  8764 solver.cpp:334]     Train net output #0: loss = 0.946673 (* 1 = 0.946673 loss)
I0815 18:32:04.216009  8764 sgd_solver.cpp:136] Iteration 151000, lr = 0.0005625, m = 0.9
I0815 18:32:23.696883  8764 solver.cpp:312] Iteration 151100 (5.13336 iter/s, 19.4804s/100 iter), loss = 1.23055
I0815 18:32:23.696921  8764 solver.cpp:334]     Train net output #0: loss = 1.28593 (* 1 = 1.28593 loss)
I0815 18:32:23.696928  8764 sgd_solver.cpp:136] Iteration 151100, lr = 0.00055625, m = 0.9
I0815 18:32:42.246331  8764 solver.cpp:312] Iteration 151200 (5.39114 iter/s, 18.5489s/100 iter), loss = 1.51923
I0815 18:32:42.246395  8764 solver.cpp:334]     Train net output #0: loss = 1.6087 (* 1 = 1.6087 loss)
I0815 18:32:42.246402  8764 sgd_solver.cpp:136] Iteration 151200, lr = 0.00055, m = 0.9
I0815 18:32:57.503809  8764 solver.cpp:312] Iteration 151300 (6.55435 iter/s, 15.2571s/100 iter), loss = 1.15101
I0815 18:32:57.503839  8764 solver.cpp:334]     Train net output #0: loss = 1.30019 (* 1 = 1.30019 loss)
I0815 18:32:57.503845  8764 sgd_solver.cpp:136] Iteration 151300, lr = 0.00054375, m = 0.9
I0815 18:33:17.384719  8764 solver.cpp:312] Iteration 151400 (5.03009 iter/s, 19.8804s/100 iter), loss = 1.35276
I0815 18:33:17.384814  8764 solver.cpp:334]     Train net output #0: loss = 1.43692 (* 1 = 1.43692 loss)
I0815 18:33:17.384824  8764 sgd_solver.cpp:136] Iteration 151400, lr = 0.0005375, m = 0.9
I0815 18:33:36.718017  8764 solver.cpp:312] Iteration 151500 (5.17256 iter/s, 19.3328s/100 iter), loss = 1.97151
I0815 18:33:36.718080  8764 solver.cpp:334]     Train net output #0: loss = 2.19349 (* 1 = 2.19349 loss)
I0815 18:33:36.718096  8764 sgd_solver.cpp:136] Iteration 151500, lr = 0.00053125, m = 0.9
I0815 18:33:58.061908  8764 solver.cpp:312] Iteration 151600 (4.68531 iter/s, 21.3433s/100 iter), loss = 1.30797
I0815 18:33:58.064055  8764 solver.cpp:334]     Train net output #0: loss = 1.10491 (* 1 = 1.10491 loss)
I0815 18:33:58.064064  8764 sgd_solver.cpp:136] Iteration 151600, lr = 0.000525, m = 0.9
I0815 18:34:13.996166  8764 solver.cpp:312] Iteration 151700 (6.27596 iter/s, 15.9338s/100 iter), loss = 1.69743
I0815 18:34:13.996194  8764 solver.cpp:334]     Train net output #0: loss = 1.84674 (* 1 = 1.84674 loss)
I0815 18:34:13.996201  8764 sgd_solver.cpp:136] Iteration 151700, lr = 0.00051875, m = 0.9
I0815 18:34:33.494230  8764 solver.cpp:312] Iteration 151800 (5.12886 iter/s, 19.4975s/100 iter), loss = 1.25187
I0815 18:34:33.494832  8764 solver.cpp:334]     Train net output #0: loss = 1.57915 (* 1 = 1.57915 loss)
I0815 18:34:33.494860  8764 sgd_solver.cpp:136] Iteration 151800, lr = 0.0005125, m = 0.9
I0815 18:34:51.897344  8764 solver.cpp:312] Iteration 151900 (5.43401 iter/s, 18.4026s/100 iter), loss = 1.94326
I0815 18:34:51.897385  8764 solver.cpp:334]     Train net output #0: loss = 2.24104 (* 1 = 2.24104 loss)
I0815 18:34:51.897399  8764 sgd_solver.cpp:136] Iteration 151900, lr = 0.00050625, m = 0.9
I0815 18:35:10.725005  8764 solver.cpp:363] Sparsity after update:
I0815 18:35:10.729004  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:35:10.729146  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:35:10.729241  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:35:10.729332  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:35:10.729423  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:35:10.729516  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:35:10.729607  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:35:10.729696  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:35:10.729784  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:35:10.729887  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:35:10.729982  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:35:10.730072  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:35:10.730165  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:35:10.730278  8764 solver.cpp:509] Iteration 152000, Testing net (#0)
I0815 18:35:39.845170  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.598764
I0815 18:35:39.845216  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.824056
I0815 18:35:39.845237  8764 solver.cpp:594]     Test net output #2: loss = 1.7396 (* 1 = 1.7396 loss)
I0815 18:35:39.845289  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.1142s
I0815 18:35:40.132589  8764 solver.cpp:312] Iteration 152000 (2.07323 iter/s, 48.2339s/100 iter), loss = 1.41104
I0815 18:35:40.132654  8764 solver.cpp:334]     Train net output #0: loss = 1.01892 (* 1 = 1.01892 loss)
I0815 18:35:40.132670  8764 sgd_solver.cpp:136] Iteration 152000, lr = 0.0005, m = 0.9
I0815 18:35:56.351090  8764 solver.cpp:312] Iteration 152100 (6.16597 iter/s, 16.218s/100 iter), loss = 1.63816
I0815 18:35:56.351186  8764 solver.cpp:334]     Train net output #0: loss = 1.9253 (* 1 = 1.9253 loss)
I0815 18:35:56.351209  8764 sgd_solver.cpp:136] Iteration 152100, lr = 0.00049375, m = 0.9
I0815 18:35:57.839457  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:36:15.620582  8764 solver.cpp:312] Iteration 152200 (5.18969 iter/s, 19.269s/100 iter), loss = 1.25786
I0815 18:36:15.620610  8764 solver.cpp:334]     Train net output #0: loss = 1.11758 (* 1 = 1.11758 loss)
I0815 18:36:15.620616  8764 sgd_solver.cpp:136] Iteration 152200, lr = 0.0004875, m = 0.9
I0815 18:36:33.645117  8764 solver.cpp:312] Iteration 152300 (5.54815 iter/s, 18.024s/100 iter), loss = 1.23997
I0815 18:36:33.645179  8764 solver.cpp:334]     Train net output #0: loss = 1.01218 (* 1 = 1.01218 loss)
I0815 18:36:33.645186  8764 sgd_solver.cpp:136] Iteration 152300, lr = 0.00048125, m = 0.9
I0815 18:36:51.432621  8764 solver.cpp:312] Iteration 152400 (5.62208 iter/s, 17.787s/100 iter), loss = 1.58834
I0815 18:36:51.432845  8764 solver.cpp:334]     Train net output #0: loss = 1.51593 (* 1 = 1.51593 loss)
I0815 18:36:51.432962  8764 sgd_solver.cpp:136] Iteration 152400, lr = 0.000475, m = 0.9
I0815 18:37:09.708428  8764 solver.cpp:312] Iteration 152500 (5.47187 iter/s, 18.2753s/100 iter), loss = 1.28746
I0815 18:37:09.708549  8764 solver.cpp:334]     Train net output #0: loss = 1.62995 (* 1 = 1.62995 loss)
I0815 18:37:09.708562  8764 sgd_solver.cpp:136] Iteration 152500, lr = 0.00046875, m = 0.9
I0815 18:37:31.352792  8764 solver.cpp:312] Iteration 152600 (4.62027 iter/s, 21.6438s/100 iter), loss = 1.47707
I0815 18:37:31.352818  8764 solver.cpp:334]     Train net output #0: loss = 1.62689 (* 1 = 1.62689 loss)
I0815 18:37:31.352823  8764 sgd_solver.cpp:136] Iteration 152600, lr = 0.0004625, m = 0.9
I0815 18:37:48.125290  8764 solver.cpp:312] Iteration 152700 (5.96231 iter/s, 16.772s/100 iter), loss = 1.02447
I0815 18:37:48.125550  8764 solver.cpp:334]     Train net output #0: loss = 1.13782 (* 1 = 1.13782 loss)
I0815 18:37:48.125660  8764 sgd_solver.cpp:136] Iteration 152700, lr = 0.00045625, m = 0.9
I0815 18:38:05.260617  8764 solver.cpp:312] Iteration 152800 (5.83606 iter/s, 17.1349s/100 iter), loss = 1.19726
I0815 18:38:05.260643  8764 solver.cpp:334]     Train net output #0: loss = 1.23304 (* 1 = 1.23304 loss)
I0815 18:38:05.260649  8764 sgd_solver.cpp:136] Iteration 152800, lr = 0.00045, m = 0.9
I0815 18:38:22.648862  8764 solver.cpp:312] Iteration 152900 (5.75117 iter/s, 17.3878s/100 iter), loss = 1.35153
I0815 18:38:22.648927  8764 solver.cpp:334]     Train net output #0: loss = 1.34392 (* 1 = 1.34392 loss)
I0815 18:38:22.648934  8764 sgd_solver.cpp:136] Iteration 152900, lr = 0.00044375, m = 0.9
I0815 18:38:39.218204  8764 solver.cpp:363] Sparsity after update:
I0815 18:38:39.230659  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:38:39.230674  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:38:39.230682  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:38:39.230685  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:38:39.230690  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:38:39.230705  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:38:39.230713  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:38:39.230721  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:38:39.230728  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:38:39.230736  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:38:39.230743  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:38:39.230751  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:38:39.230759  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:38:39.371408  8764 solver.cpp:312] Iteration 153000 (5.98012 iter/s, 16.7221s/100 iter), loss = 1.96595
I0815 18:38:39.371435  8764 solver.cpp:334]     Train net output #0: loss = 1.71888 (* 1 = 1.71888 loss)
I0815 18:38:39.371441  8764 sgd_solver.cpp:136] Iteration 153000, lr = 0.0004375, m = 0.9
I0815 18:38:55.656560  8764 solver.cpp:312] Iteration 153100 (6.14073 iter/s, 16.2847s/100 iter), loss = 1.41584
I0815 18:38:55.656666  8764 solver.cpp:334]     Train net output #0: loss = 1.53891 (* 1 = 1.53891 loss)
I0815 18:38:55.656687  8764 sgd_solver.cpp:136] Iteration 153100, lr = 0.00043125, m = 0.9
I0815 18:39:12.923862  8764 solver.cpp:312] Iteration 153200 (5.79145 iter/s, 17.2668s/100 iter), loss = 1.23655
I0815 18:39:12.923887  8764 solver.cpp:334]     Train net output #0: loss = 1.1194 (* 1 = 1.1194 loss)
I0815 18:39:12.923892  8764 sgd_solver.cpp:136] Iteration 153200, lr = 0.000425, m = 0.9
I0815 18:39:30.870236  8764 solver.cpp:312] Iteration 153300 (5.57231 iter/s, 17.9459s/100 iter), loss = 1.40606
I0815 18:39:30.870291  8764 solver.cpp:334]     Train net output #0: loss = 1.42648 (* 1 = 1.42648 loss)
I0815 18:39:30.870298  8764 sgd_solver.cpp:136] Iteration 153300, lr = 0.00041875, m = 0.9
I0815 18:39:48.920145  8764 solver.cpp:312] Iteration 153400 (5.54035 iter/s, 18.0494s/100 iter), loss = 1.57363
I0815 18:39:48.920215  8764 solver.cpp:334]     Train net output #0: loss = 1.4369 (* 1 = 1.4369 loss)
I0815 18:39:48.920233  8764 sgd_solver.cpp:136] Iteration 153400, lr = 0.0004125, m = 0.9
I0815 18:40:04.744233  8764 solver.cpp:312] Iteration 153500 (6.31965 iter/s, 15.8236s/100 iter), loss = 1.07969
I0815 18:40:04.744349  8764 solver.cpp:334]     Train net output #0: loss = 1.10668 (* 1 = 1.10668 loss)
I0815 18:40:04.744366  8764 sgd_solver.cpp:136] Iteration 153500, lr = 0.00040625, m = 0.9
I0815 18:40:24.102942  8764 solver.cpp:312] Iteration 153600 (5.16579 iter/s, 19.3581s/100 iter), loss = 1.17691
I0815 18:40:24.103044  8764 solver.cpp:334]     Train net output #0: loss = 0.976355 (* 1 = 0.976355 loss)
I0815 18:40:24.103091  8764 sgd_solver.cpp:136] Iteration 153600, lr = 0.0004, m = 0.9
I0815 18:40:45.753482  8764 solver.cpp:312] Iteration 153700 (4.61895 iter/s, 21.65s/100 iter), loss = 1.56353
I0815 18:40:45.753837  8764 solver.cpp:334]     Train net output #0: loss = 1.36288 (* 1 = 1.36288 loss)
I0815 18:40:45.753945  8764 sgd_solver.cpp:136] Iteration 153700, lr = 0.00039375, m = 0.9
I0815 18:41:10.269384  8764 solver.cpp:312] Iteration 153800 (4.0791 iter/s, 24.5152s/100 iter), loss = 1.50103
I0815 18:41:10.269410  8764 solver.cpp:334]     Train net output #0: loss = 1.79272 (* 1 = 1.79272 loss)
I0815 18:41:10.269417  8764 sgd_solver.cpp:136] Iteration 153800, lr = 0.0003875, m = 0.9
I0815 18:41:28.730636  8764 solver.cpp:312] Iteration 153900 (5.4169 iter/s, 18.4607s/100 iter), loss = 1.21363
I0815 18:41:28.733196  8764 solver.cpp:334]     Train net output #0: loss = 1.37718 (* 1 = 1.37718 loss)
I0815 18:41:28.733225  8764 sgd_solver.cpp:136] Iteration 153900, lr = 0.00038125, m = 0.9
I0815 18:41:50.036556  8764 solver.cpp:363] Sparsity after update:
I0815 18:41:50.040364  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:41:50.040374  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:41:50.040381  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:41:50.040395  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:41:50.040405  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:41:50.040415  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:41:50.040422  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:41:50.040431  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:41:50.040439  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:41:50.040448  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:41:50.040457  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:41:50.040467  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:41:50.040477  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:41:50.040495  8764 solver.cpp:509] Iteration 154000, Testing net (#0)
I0815 18:42:28.575345  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.595
I0815 18:42:28.575445  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817762
I0815 18:42:28.575455  8764 solver.cpp:594]     Test net output #2: loss = 1.76586 (* 1 = 1.76586 loss)
I0815 18:42:28.575475  8764 solver.cpp:264] [MultiGPU] Tests completed in 38.5339s
I0815 18:42:28.748162  8764 solver.cpp:312] Iteration 154000 (1.66623 iter/s, 60.0159s/100 iter), loss = 1.16872
I0815 18:42:28.748189  8764 solver.cpp:334]     Train net output #0: loss = 1.31151 (* 1 = 1.31151 loss)
I0815 18:42:28.748195  8764 sgd_solver.cpp:136] Iteration 154000, lr = 0.000375, m = 0.9
I0815 18:42:50.195693  8764 solver.cpp:312] Iteration 154100 (4.66268 iter/s, 21.4469s/100 iter), loss = 1.42239
I0815 18:42:50.195894  8764 solver.cpp:334]     Train net output #0: loss = 1.23233 (* 1 = 1.23233 loss)
I0815 18:42:50.195955  8764 sgd_solver.cpp:136] Iteration 154100, lr = 0.00036875, m = 0.9
I0815 18:43:10.727771  8764 solver.cpp:312] Iteration 154200 (4.87056 iter/s, 20.5315s/100 iter), loss = 1.4063
I0815 18:43:10.727905  8764 solver.cpp:334]     Train net output #0: loss = 1.48207 (* 1 = 1.48207 loss)
I0815 18:43:10.727921  8764 sgd_solver.cpp:136] Iteration 154200, lr = 0.0003625, m = 0.9
I0815 18:43:29.665632  8764 solver.cpp:312] Iteration 154300 (5.28057 iter/s, 18.9373s/100 iter), loss = 1.48915
I0815 18:43:29.665697  8764 solver.cpp:334]     Train net output #0: loss = 1.7187 (* 1 = 1.7187 loss)
I0815 18:43:29.665716  8764 sgd_solver.cpp:136] Iteration 154300, lr = 0.00035625, m = 0.9
I0815 18:43:48.284703  8764 solver.cpp:312] Iteration 154400 (5.37099 iter/s, 18.6185s/100 iter), loss = 1.31694
I0815 18:43:48.284821  8764 solver.cpp:334]     Train net output #0: loss = 1.4617 (* 1 = 1.4617 loss)
I0815 18:43:48.284837  8764 sgd_solver.cpp:136] Iteration 154400, lr = 0.00035, m = 0.9
I0815 18:44:09.060143  8764 solver.cpp:312] Iteration 154500 (4.81437 iter/s, 20.7712s/100 iter), loss = 1.27142
I0815 18:44:09.060216  8764 solver.cpp:334]     Train net output #0: loss = 1.35171 (* 1 = 1.35171 loss)
I0815 18:44:09.060235  8764 sgd_solver.cpp:136] Iteration 154500, lr = 0.00034375, m = 0.9
I0815 18:44:28.131482  8764 solver.cpp:312] Iteration 154600 (5.2426 iter/s, 19.0745s/100 iter), loss = 1.00581
I0815 18:44:28.131561  8764 solver.cpp:334]     Train net output #0: loss = 1.20939 (* 1 = 1.20939 loss)
I0815 18:44:28.131572  8764 sgd_solver.cpp:136] Iteration 154600, lr = 0.0003375, m = 0.9
I0815 18:44:45.821696  8764 solver.cpp:312] Iteration 154700 (5.653 iter/s, 17.6897s/100 iter), loss = 1.6365
I0815 18:44:45.821722  8764 solver.cpp:334]     Train net output #0: loss = 1.47154 (* 1 = 1.47154 loss)
I0815 18:44:45.821727  8764 sgd_solver.cpp:136] Iteration 154700, lr = 0.00033125, m = 0.9
I0815 18:45:04.731978  8764 solver.cpp:312] Iteration 154800 (5.28828 iter/s, 18.9098s/100 iter), loss = 1.16141
I0815 18:45:04.732070  8764 solver.cpp:334]     Train net output #0: loss = 1.09887 (* 1 = 1.09887 loss)
I0815 18:45:04.732086  8764 sgd_solver.cpp:136] Iteration 154800, lr = 0.000325, m = 0.9
I0815 18:45:25.410691  8764 solver.cpp:312] Iteration 154900 (4.83603 iter/s, 20.6781s/100 iter), loss = 1.15684
I0815 18:45:25.410782  8764 solver.cpp:334]     Train net output #0: loss = 0.904558 (* 1 = 0.904558 loss)
I0815 18:45:25.410809  8764 sgd_solver.cpp:136] Iteration 154900, lr = 0.00031875, m = 0.9
I0815 18:45:44.878413  8764 solver.cpp:363] Sparsity after update:
I0815 18:45:44.888825  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:45:44.888839  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:45:44.888849  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:45:44.888852  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:45:44.888856  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:45:44.888860  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:45:44.888864  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:45:44.888866  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:45:44.888870  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:45:44.888872  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:45:44.888875  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:45:44.888877  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:45:44.888881  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:45:45.052816  8764 solver.cpp:312] Iteration 155000 (5.09124 iter/s, 19.6416s/100 iter), loss = 1.23533
I0815 18:45:45.052839  8764 solver.cpp:334]     Train net output #0: loss = 1.02986 (* 1 = 1.02986 loss)
I0815 18:45:45.052845  8764 sgd_solver.cpp:136] Iteration 155000, lr = 0.0003125, m = 0.9
I0815 18:46:01.943240  8764 solver.cpp:312] Iteration 155100 (5.92068 iter/s, 16.89s/100 iter), loss = 1.12267
I0815 18:46:01.943266  8764 solver.cpp:334]     Train net output #0: loss = 1.02877 (* 1 = 1.02877 loss)
I0815 18:46:01.943272  8764 sgd_solver.cpp:136] Iteration 155100, lr = 0.00030625, m = 0.9
I0815 18:46:21.578686  8764 solver.cpp:312] Iteration 155200 (5.09297 iter/s, 19.6349s/100 iter), loss = 1.24077
I0815 18:46:21.578794  8764 solver.cpp:334]     Train net output #0: loss = 1.09811 (* 1 = 1.09811 loss)
I0815 18:46:21.578807  8764 sgd_solver.cpp:136] Iteration 155200, lr = 0.0003, m = 0.9
I0815 18:46:40.829329  8764 solver.cpp:312] Iteration 155300 (5.19477 iter/s, 19.2501s/100 iter), loss = 1.32439
I0815 18:46:40.829545  8764 solver.cpp:334]     Train net output #0: loss = 1.16146 (* 1 = 1.16146 loss)
I0815 18:46:40.829654  8764 sgd_solver.cpp:136] Iteration 155300, lr = 0.00029375, m = 0.9
I0815 18:46:59.990130  8764 solver.cpp:312] Iteration 155400 (5.21913 iter/s, 19.1603s/100 iter), loss = 1.23928
I0815 18:46:59.990218  8764 solver.cpp:334]     Train net output #0: loss = 1.17727 (* 1 = 1.17727 loss)
I0815 18:46:59.990236  8764 sgd_solver.cpp:136] Iteration 155400, lr = 0.0002875, m = 0.9
I0815 18:47:17.016304  8764 solver.cpp:312] Iteration 155500 (5.87348 iter/s, 17.0257s/100 iter), loss = 1.28226
I0815 18:47:17.016355  8764 solver.cpp:334]     Train net output #0: loss = 1.1649 (* 1 = 1.1649 loss)
I0815 18:47:17.016366  8764 sgd_solver.cpp:136] Iteration 155500, lr = 0.00028125, m = 0.9
I0815 18:47:38.730672  8764 solver.cpp:312] Iteration 155600 (4.60537 iter/s, 21.7138s/100 iter), loss = 1.49242
I0815 18:47:38.730725  8764 solver.cpp:334]     Train net output #0: loss = 1.52554 (* 1 = 1.52554 loss)
I0815 18:47:38.730728  8764 sgd_solver.cpp:136] Iteration 155600, lr = 0.000275, m = 0.9
I0815 18:47:56.452909  8764 solver.cpp:312] Iteration 155700 (5.64278 iter/s, 17.7217s/100 iter), loss = 1.40468
I0815 18:47:56.452931  8764 solver.cpp:334]     Train net output #0: loss = 1.15485 (* 1 = 1.15485 loss)
I0815 18:47:56.452935  8764 sgd_solver.cpp:136] Iteration 155700, lr = 0.00026875, m = 0.9
I0815 18:48:15.494017  8764 solver.cpp:312] Iteration 155800 (5.25194 iter/s, 19.0406s/100 iter), loss = 1.26209
I0815 18:48:15.494102  8764 solver.cpp:334]     Train net output #0: loss = 1.84643 (* 1 = 1.84643 loss)
I0815 18:48:15.494119  8764 sgd_solver.cpp:136] Iteration 155800, lr = 0.0002625, m = 0.9
I0815 18:48:35.472816  8764 solver.cpp:312] Iteration 155900 (5.00544 iter/s, 19.9782s/100 iter), loss = 1.29878
I0815 18:48:35.472837  8764 solver.cpp:334]     Train net output #0: loss = 1.4207 (* 1 = 1.4207 loss)
I0815 18:48:35.472843  8764 sgd_solver.cpp:136] Iteration 155900, lr = 0.00025625, m = 0.9
I0815 18:48:54.588459  8764 solver.cpp:363] Sparsity after update:
I0815 18:48:54.598816  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:48:54.598842  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:48:54.598853  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:48:54.598856  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:48:54.598860  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:48:54.598863  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:48:54.598870  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:48:54.598872  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:48:54.598875  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:48:54.598878  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:48:54.598881  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:48:54.598884  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:48:54.598887  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:48:54.598899  8764 solver.cpp:509] Iteration 156000, Testing net (#0)
I0815 18:49:06.967571  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:49:25.279548  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.591941
I0815 18:49:25.279600  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.822585
I0815 18:49:25.279606  8764 solver.cpp:594]     Test net output #2: loss = 1.763 (* 1 = 1.763 loss)
I0815 18:49:25.279628  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.6799s
I0815 18:49:25.450697  8764 solver.cpp:312] Iteration 156000 (2.00094 iter/s, 49.9765s/100 iter), loss = 1.33885
I0815 18:49:25.450728  8764 solver.cpp:334]     Train net output #0: loss = 0.956053 (* 1 = 0.956053 loss)
I0815 18:49:25.450736  8764 sgd_solver.cpp:136] Iteration 156000, lr = 0.00025, m = 0.9
I0815 18:49:44.420570  8764 solver.cpp:312] Iteration 156100 (5.27166 iter/s, 18.9693s/100 iter), loss = 1.79744
I0815 18:49:44.420598  8764 solver.cpp:334]     Train net output #0: loss = 1.53345 (* 1 = 1.53345 loss)
I0815 18:49:44.420604  8764 sgd_solver.cpp:136] Iteration 156100, lr = 0.00024375, m = 0.9
I0815 18:50:02.348085  8764 solver.cpp:312] Iteration 156200 (5.57817 iter/s, 17.927s/100 iter), loss = 1.3352
I0815 18:50:02.348199  8764 solver.cpp:334]     Train net output #0: loss = 1.41296 (* 1 = 1.41296 loss)
I0815 18:50:02.348218  8764 sgd_solver.cpp:136] Iteration 156200, lr = 0.0002375, m = 0.9
I0815 18:50:22.292476  8764 solver.cpp:312] Iteration 156300 (5.01408 iter/s, 19.9438s/100 iter), loss = 1.48692
I0815 18:50:22.292502  8764 solver.cpp:334]     Train net output #0: loss = 1.79811 (* 1 = 1.79811 loss)
I0815 18:50:22.292507  8764 sgd_solver.cpp:136] Iteration 156300, lr = 0.00023125, m = 0.9
I0815 18:50:41.927470  8764 solver.cpp:312] Iteration 156400 (5.09309 iter/s, 19.6344s/100 iter), loss = 1.17254
I0815 18:50:41.927536  8764 solver.cpp:334]     Train net output #0: loss = 1.15651 (* 1 = 1.15651 loss)
I0815 18:50:41.927546  8764 sgd_solver.cpp:136] Iteration 156400, lr = 0.000225, m = 0.9
I0815 18:51:00.399458  8764 solver.cpp:312] Iteration 156500 (5.41375 iter/s, 18.4715s/100 iter), loss = 1.29194
I0815 18:51:00.399642  8764 solver.cpp:334]     Train net output #0: loss = 1.31863 (* 1 = 1.31863 loss)
I0815 18:51:00.399732  8764 sgd_solver.cpp:136] Iteration 156500, lr = 0.00021875, m = 0.9
I0815 18:51:19.857738  8764 solver.cpp:312] Iteration 156600 (5.13934 iter/s, 19.4577s/100 iter), loss = 1.36625
I0815 18:51:19.857796  8764 solver.cpp:334]     Train net output #0: loss = 1.53626 (* 1 = 1.53626 loss)
I0815 18:51:19.857803  8764 sgd_solver.cpp:136] Iteration 156600, lr = 0.0002125, m = 0.9
I0815 18:51:39.703359  8764 solver.cpp:312] Iteration 156700 (5.03904 iter/s, 19.8451s/100 iter), loss = 1.13723
I0815 18:51:39.703400  8764 solver.cpp:334]     Train net output #0: loss = 1.08302 (* 1 = 1.08302 loss)
I0815 18:51:39.703408  8764 sgd_solver.cpp:136] Iteration 156700, lr = 0.00020625, m = 0.9
I0815 18:51:56.900867  8764 solver.cpp:312] Iteration 156800 (5.81496 iter/s, 17.197s/100 iter), loss = 1.46413
I0815 18:51:56.900964  8764 solver.cpp:334]     Train net output #0: loss = 1.38787 (* 1 = 1.38787 loss)
I0815 18:51:56.900981  8764 sgd_solver.cpp:136] Iteration 156800, lr = 0.0002, m = 0.9
I0815 18:52:17.465224  8764 solver.cpp:312] Iteration 156900 (4.86292 iter/s, 20.5638s/100 iter), loss = 1.19512
I0815 18:52:17.465275  8764 solver.cpp:334]     Train net output #0: loss = 1.10139 (* 1 = 1.10139 loss)
I0815 18:52:17.465288  8764 sgd_solver.cpp:136] Iteration 156900, lr = 0.00019375, m = 0.9
I0815 18:52:36.403556  8764 solver.cpp:363] Sparsity after update:
I0815 18:52:36.417870  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:52:36.417912  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:52:36.417932  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:52:36.417945  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:52:36.417958  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:52:36.417970  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:52:36.417984  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:52:36.417995  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:52:36.418007  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:52:36.418020  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:52:36.418032  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:52:36.418045  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:52:36.418057  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:52:36.559213  8764 solver.cpp:312] Iteration 157000 (5.23739 iter/s, 19.0935s/100 iter), loss = 1.30169
I0815 18:52:36.559242  8764 solver.cpp:334]     Train net output #0: loss = 1.22552 (* 1 = 1.22552 loss)
I0815 18:52:36.559247  8764 sgd_solver.cpp:136] Iteration 157000, lr = 0.0001875, m = 0.9
I0815 18:52:54.445412  8764 solver.cpp:312] Iteration 157100 (5.59106 iter/s, 17.8857s/100 iter), loss = 1.09729
I0815 18:52:54.445441  8764 solver.cpp:334]     Train net output #0: loss = 1.3234 (* 1 = 1.3234 loss)
I0815 18:52:54.445446  8764 sgd_solver.cpp:136] Iteration 157100, lr = 0.00018125, m = 0.9
I0815 18:53:10.117681  8764 solver.cpp:312] Iteration 157200 (6.38088 iter/s, 15.6718s/100 iter), loss = 1.72391
I0815 18:53:10.117817  8764 solver.cpp:334]     Train net output #0: loss = 2.03706 (* 1 = 2.03706 loss)
I0815 18:53:10.117841  8764 sgd_solver.cpp:136] Iteration 157200, lr = 0.000175, m = 0.9
I0815 18:53:29.774924  8764 solver.cpp:312] Iteration 157300 (5.08732 iter/s, 19.6567s/100 iter), loss = 1.61389
I0815 18:53:29.774947  8764 solver.cpp:334]     Train net output #0: loss = 1.54747 (* 1 = 1.54747 loss)
I0815 18:53:29.774952  8764 sgd_solver.cpp:136] Iteration 157300, lr = 0.00016875, m = 0.9
I0815 18:53:49.892237  8764 solver.cpp:312] Iteration 157400 (4.97098 iter/s, 20.1168s/100 iter), loss = 1.35175
I0815 18:53:49.892320  8764 solver.cpp:334]     Train net output #0: loss = 1.2872 (* 1 = 1.2872 loss)
I0815 18:53:49.892333  8764 sgd_solver.cpp:136] Iteration 157400, lr = 0.0001625, m = 0.9
I0815 18:54:07.419236  8764 solver.cpp:312] Iteration 157500 (5.70564 iter/s, 17.5265s/100 iter), loss = 1.17605
I0815 18:54:07.419286  8764 solver.cpp:334]     Train net output #0: loss = 1.33095 (* 1 = 1.33095 loss)
I0815 18:54:07.419298  8764 sgd_solver.cpp:136] Iteration 157500, lr = 0.00015625, m = 0.9
I0815 18:54:25.262441  8764 solver.cpp:312] Iteration 157600 (5.60453 iter/s, 17.8427s/100 iter), loss = 1.37864
I0815 18:54:25.262501  8764 solver.cpp:334]     Train net output #0: loss = 1.12817 (* 1 = 1.12817 loss)
I0815 18:54:25.262507  8764 sgd_solver.cpp:136] Iteration 157600, lr = 0.00015, m = 0.9
I0815 18:54:45.116144  8764 solver.cpp:312] Iteration 157700 (5.03698 iter/s, 19.8532s/100 iter), loss = 1.26332
I0815 18:54:45.116171  8764 solver.cpp:334]     Train net output #0: loss = 1.53083 (* 1 = 1.53083 loss)
I0815 18:54:45.116178  8764 sgd_solver.cpp:136] Iteration 157700, lr = 0.00014375, m = 0.9
I0815 18:55:03.654886  8764 solver.cpp:312] Iteration 157800 (5.39426 iter/s, 18.5382s/100 iter), loss = 1.33705
I0815 18:55:03.654974  8764 solver.cpp:334]     Train net output #0: loss = 1.36447 (* 1 = 1.36447 loss)
I0815 18:55:03.654992  8764 sgd_solver.cpp:136] Iteration 157800, lr = 0.0001375, m = 0.9
I0815 18:55:20.970484  8764 solver.cpp:312] Iteration 157900 (5.7753 iter/s, 17.3151s/100 iter), loss = 1.43534
I0815 18:55:20.970511  8764 solver.cpp:334]     Train net output #0: loss = 1.48158 (* 1 = 1.48158 loss)
I0815 18:55:20.970517  8764 sgd_solver.cpp:136] Iteration 157900, lr = 0.00013125, m = 0.9
I0815 18:55:37.896369  8764 solver.cpp:363] Sparsity after update:
I0815 18:55:37.902242  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:55:37.902256  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:55:37.902266  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:55:37.902271  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:55:37.902273  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:55:37.902277  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:55:37.902281  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:55:37.902283  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:55:37.902287  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:55:37.902288  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:55:37.902292  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:55:37.902294  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:55:37.902297  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:55:37.902308  8764 solver.cpp:509] Iteration 158000, Testing net (#0)
I0815 18:55:49.445585  8747 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 18:56:04.486696  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.600412
I0815 18:56:04.486721  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.820115
I0815 18:56:04.486727  8764 solver.cpp:594]     Test net output #2: loss = 1.73733 (* 1 = 1.73733 loss)
I0815 18:56:04.486748  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.5837s
I0815 18:56:04.636364  8764 solver.cpp:312] Iteration 158000 (2.29018 iter/s, 43.6647s/100 iter), loss = 1.15153
I0815 18:56:04.636389  8764 solver.cpp:334]     Train net output #0: loss = 0.878272 (* 1 = 0.878272 loss)
I0815 18:56:04.636395  8764 sgd_solver.cpp:136] Iteration 158000, lr = 0.000125, m = 0.9
I0815 18:56:22.733921  8764 solver.cpp:312] Iteration 158100 (5.52576 iter/s, 18.0971s/100 iter), loss = 1.31369
I0815 18:56:22.734015  8764 solver.cpp:334]     Train net output #0: loss = 1.41089 (* 1 = 1.41089 loss)
I0815 18:56:22.734033  8764 sgd_solver.cpp:136] Iteration 158100, lr = 0.00011875, m = 0.9
I0815 18:56:38.225419  8764 solver.cpp:312] Iteration 158200 (6.45533 iter/s, 15.4911s/100 iter), loss = 1.30583
I0815 18:56:38.225443  8764 solver.cpp:334]     Train net output #0: loss = 1.22907 (* 1 = 1.22907 loss)
I0815 18:56:38.225448  8764 sgd_solver.cpp:136] Iteration 158200, lr = 0.0001125, m = 0.9
I0815 18:56:56.883030  8764 solver.cpp:312] Iteration 158300 (5.35989 iter/s, 18.6571s/100 iter), loss = 1.26269
I0815 18:56:56.883107  8764 solver.cpp:334]     Train net output #0: loss = 1.31755 (* 1 = 1.31755 loss)
I0815 18:56:56.883114  8764 sgd_solver.cpp:136] Iteration 158300, lr = 0.00010625, m = 0.9
I0815 18:57:15.986559  8764 solver.cpp:312] Iteration 158400 (5.23479 iter/s, 19.103s/100 iter), loss = 1.43858
I0815 18:57:15.986619  8764 solver.cpp:334]     Train net output #0: loss = 1.41753 (* 1 = 1.41753 loss)
I0815 18:57:15.986637  8764 sgd_solver.cpp:136] Iteration 158400, lr = 9.99999e-05, m = 0.9
I0815 18:57:35.712939  8764 solver.cpp:312] Iteration 158500 (5.06949 iter/s, 19.7258s/100 iter), loss = 1.38257
I0815 18:57:35.713028  8764 solver.cpp:334]     Train net output #0: loss = 1.66007 (* 1 = 1.66007 loss)
I0815 18:57:35.713038  8764 sgd_solver.cpp:136] Iteration 158500, lr = 9.37498e-05, m = 0.9
I0815 18:57:53.040740  8764 solver.cpp:312] Iteration 158600 (5.77123 iter/s, 17.3273s/100 iter), loss = 1.27794
I0815 18:57:53.040781  8764 solver.cpp:334]     Train net output #0: loss = 1.16817 (* 1 = 1.16817 loss)
I0815 18:57:53.040792  8764 sgd_solver.cpp:136] Iteration 158600, lr = 8.75002e-05, m = 0.9
I0815 18:58:13.963186  8764 solver.cpp:312] Iteration 158700 (4.77969 iter/s, 20.9219s/100 iter), loss = 1.31456
I0815 18:58:13.963289  8764 solver.cpp:334]     Train net output #0: loss = 1.13414 (* 1 = 1.13414 loss)
I0815 18:58:13.963312  8764 sgd_solver.cpp:136] Iteration 158700, lr = 8.12501e-05, m = 0.9
I0815 18:58:35.560240  8764 solver.cpp:312] Iteration 158800 (4.63039 iter/s, 21.5965s/100 iter), loss = 1.4967
I0815 18:58:35.560267  8764 solver.cpp:334]     Train net output #0: loss = 1.34251 (* 1 = 1.34251 loss)
I0815 18:58:35.560271  8764 sgd_solver.cpp:136] Iteration 158800, lr = 7.49999e-05, m = 0.9
I0815 18:58:56.826381  8764 solver.cpp:312] Iteration 158900 (4.70244 iter/s, 21.2655s/100 iter), loss = 1.15607
I0815 18:58:56.826593  8764 solver.cpp:334]     Train net output #0: loss = 1.25582 (* 1 = 1.25582 loss)
I0815 18:58:56.826617  8764 sgd_solver.cpp:136] Iteration 158900, lr = 6.87498e-05, m = 0.9
I0815 18:59:16.293910  8764 solver.cpp:363] Sparsity after update:
I0815 18:59:16.305222  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:59:16.305243  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:59:16.305256  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:59:16.305260  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:59:16.305263  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:59:16.305266  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:59:16.305270  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:59:16.305274  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:59:16.305276  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:59:16.305280  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:59:16.305284  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:59:16.305286  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:59:16.305289  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:59:16.646153  8764 solver.cpp:312] Iteration 159000 (5.04561 iter/s, 19.8192s/100 iter), loss = 1.72539
I0815 18:59:16.646199  8764 solver.cpp:334]     Train net output #0: loss = 1.72858 (* 1 = 1.72858 loss)
I0815 18:59:16.646210  8764 sgd_solver.cpp:136] Iteration 159000, lr = 6.25002e-05, m = 0.9
I0815 18:59:38.536093  8764 solver.cpp:312] Iteration 159100 (4.56843 iter/s, 21.8893s/100 iter), loss = 1.46191
I0815 18:59:38.536201  8764 solver.cpp:334]     Train net output #0: loss = 1.51227 (* 1 = 1.51227 loss)
I0815 18:59:38.536218  8764 sgd_solver.cpp:136] Iteration 159100, lr = 5.62501e-05, m = 0.9
I0815 18:59:59.253667  8764 solver.cpp:312] Iteration 159200 (4.82696 iter/s, 20.717s/100 iter), loss = 1.45128
I0815 18:59:59.253697  8764 solver.cpp:334]     Train net output #0: loss = 1.90562 (* 1 = 1.90562 loss)
I0815 18:59:59.253703  8764 sgd_solver.cpp:136] Iteration 159200, lr = 5e-05, m = 0.9
I0815 19:00:18.853642  8764 solver.cpp:312] Iteration 159300 (5.10219 iter/s, 19.5994s/100 iter), loss = 1.28292
I0815 19:00:18.853704  8764 solver.cpp:334]     Train net output #0: loss = 1.19643 (* 1 = 1.19643 loss)
I0815 19:00:18.853710  8764 sgd_solver.cpp:136] Iteration 159300, lr = 4.37498e-05, m = 0.9
I0815 19:00:33.789043  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 19:00:38.260581  8764 solver.cpp:312] Iteration 159400 (5.15294 iter/s, 19.4064s/100 iter), loss = 1.57283
I0815 19:00:38.260622  8764 solver.cpp:334]     Train net output #0: loss = 1.80493 (* 1 = 1.80493 loss)
I0815 19:00:38.260635  8764 sgd_solver.cpp:136] Iteration 159400, lr = 3.75003e-05, m = 0.9
I0815 19:00:58.882683  8764 solver.cpp:312] Iteration 159500 (4.8493 iter/s, 20.6215s/100 iter), loss = 1.35713
I0815 19:00:58.882767  8764 solver.cpp:334]     Train net output #0: loss = 1.32614 (* 1 = 1.32614 loss)
I0815 19:00:58.882781  8764 sgd_solver.cpp:136] Iteration 159500, lr = 3.12501e-05, m = 0.9
I0815 19:01:18.123251  8764 solver.cpp:312] Iteration 159600 (5.19749 iter/s, 19.24s/100 iter), loss = 1.53796
I0815 19:01:18.123327  8764 solver.cpp:334]     Train net output #0: loss = 1.79531 (* 1 = 1.79531 loss)
I0815 19:01:18.123347  8764 sgd_solver.cpp:136] Iteration 159600, lr = 2.5e-05, m = 0.9
I0815 19:01:37.260257  8764 solver.cpp:312] Iteration 159700 (5.22563 iter/s, 19.1365s/100 iter), loss = 1.31863
I0815 19:01:37.260359  8764 solver.cpp:334]     Train net output #0: loss = 1.22497 (* 1 = 1.22497 loss)
I0815 19:01:37.260371  8764 sgd_solver.cpp:136] Iteration 159700, lr = 1.87498e-05, m = 0.9
I0815 19:01:59.484719  8764 solver.cpp:312] Iteration 159800 (4.49967 iter/s, 22.2238s/100 iter), loss = 1.52736
I0815 19:01:59.484827  8764 solver.cpp:334]     Train net output #0: loss = 1.44795 (* 1 = 1.44795 loss)
I0815 19:01:59.484848  8764 sgd_solver.cpp:136] Iteration 159800, lr = 1.25003e-05, m = 0.9
I0815 19:02:19.288565  8764 solver.cpp:312] Iteration 159900 (5.04966 iter/s, 19.8033s/100 iter), loss = 0.941216
I0815 19:02:19.288841  8764 solver.cpp:334]     Train net output #0: loss = 0.732497 (* 1 = 0.732497 loss)
I0815 19:02:19.288954  8764 sgd_solver.cpp:136] Iteration 159900, lr = 6.25014e-06, m = 0.9
I0815 19:02:37.516433  8764 solver.cpp:312] Iteration 159999 (5.43139 iter/s, 18.2274s/99 iter), loss = 1.5553
I0815 19:02:37.516458  8764 solver.cpp:334]     Train net output #0: loss = 1.76265 (* 1 = 1.76265 loss)
I0815 19:02:37.516463  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_160000.caffemodel
I0815 19:02:37.843142  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_160000.solverstate
I0815 19:02:37.849011  8764 solver.cpp:363] Sparsity after update:
I0815 19:02:37.854293  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 19:02:37.854305  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 19:02:37.854311  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 19:02:37.854313  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 19:02:37.854315  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 19:02:37.854318  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 19:02:37.854326  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 19:02:37.854328  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 19:02:37.854332  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 19:02:37.854336  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 19:02:37.854338  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 19:02:37.854342  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 19:02:37.854346  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 19:02:37.878444  8764 solver.cpp:486] Iteration 160000, loss = 1.25444
I0815 19:02:37.878471  8764 solver.cpp:509] Iteration 160000, Testing net (#0)
I0815 19:03:13.394717  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.596824
I0815 19:03:13.394825  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.821644
I0815 19:03:13.394842  8764 solver.cpp:594]     Test net output #2: loss = 1.74932 (* 1 = 1.74932 loss)
I0815 19:03:13.423722  8671 parallel.cpp:71] Root Solver performance on device 0: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423768  8671 parallel.cpp:76]      Solver performance on device 1: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423787  8671 parallel.cpp:76]      Solver performance on device 2: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423797  8671 parallel.cpp:79] Overall multi-GPU performance: 623.075 img/sec
I0815 19:03:13.949198  8671 caffe.cpp:247] Optimization Done in 9h 12m 46s
I0815 19:03:16.642503 19104 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 19:03:15 2017
I0815 19:03:16.643635 19104 caffe.cpp:611] CuDNN version: 6021
I0815 19:03:16.643640 19104 caffe.cpp:612] CuBLAS version: 8000
I0815 19:03:16.643642 19104 caffe.cpp:613] CUDA version: 8000
I0815 19:03:16.643643 19104 caffe.cpp:614] CUDA driver version: 8000
I0815 19:03:16.643649 19104 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0815 19:03:16.643652 19104 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0815 19:03:16.644224 19104 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 19:03:16.644770 19104 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 19:03:16.644775 19104 caffe.cpp:275] Use GPU with device ID 0
I0815 19:03:16.645098 19104 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0815 19:03:16.680893 19104 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0815 19:03:16.681063 19104 net.cpp:104] Using FLOAT as default forward math type
I0815 19:03:16.681071 19104 net.cpp:110] Using FLOAT as default backward math type
I0815 19:03:16.681076 19104 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0815 19:03:16.681083 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:16.692891 19104 net.cpp:184] Created Layer data (0)
I0815 19:03:16.693349 19104 net.cpp:530] data -> data
I0815 19:03:16.693367 19104 net.cpp:530] data -> label
I0815 19:03:16.693384 19104 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0815 19:03:16.693691 19104 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:03:16.754305 19171 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 19:03:16.755575 19104 data_layer.cpp:185] (0) ReshapePrefetch 50, 3, 224, 224
I0815 19:03:16.755614 19104 data_layer.cpp:209] (0) Output data size: 50, 3, 224, 224
I0815 19:03:16.755620 19104 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:03:16.755647 19104 net.cpp:245] Setting up data
I0815 19:03:16.755658 19104 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 224 224 (7526400)
I0815 19:03:16.755666 19104 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0815 19:03:16.755676 19104 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 19:03:16.755681 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:16.755694 19104 net.cpp:184] Created Layer label_data_1_split (1)
I0815 19:03:16.755699 19104 net.cpp:561] label_data_1_split <- label
I0815 19:03:16.755708 19104 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 19:03:16.755714 19104 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 19:03:16.755729 19104 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 19:03:16.755759 19104 net.cpp:245] Setting up label_data_1_split
I0815 19:03:16.755764 19104 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0815 19:03:16.755770 19104 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0815 19:03:16.755774 19104 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0815 19:03:16.755779 19104 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 19:03:16.755784 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:16.755797 19104 net.cpp:184] Created Layer data/bias (2)
I0815 19:03:16.755801 19104 net.cpp:561] data/bias <- data
I0815 19:03:16.755805 19104 net.cpp:530] data/bias -> data/bias
I0815 19:03:16.756814 19172 data_layer.cpp:97] (0) Parser threads: 1
I0815 19:03:16.756822 19172 data_layer.cpp:99] (0) Transformer threads: 1
I0815 19:03:16.762274 19104 net.cpp:245] Setting up data/bias
I0815 19:03:16.762312 19104 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 224 224 (7526400)
I0815 19:03:16.762331 19104 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 19:03:16.762341 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:16.762717 19104 net.cpp:184] Created Layer conv1a (3)
I0815 19:03:16.762725 19104 net.cpp:561] conv1a <- data/bias
I0815 19:03:16.762732 19104 net.cpp:530] conv1a -> conv1a
I0815 19:03:17.322698 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.02G, req 0G)
I0815 19:03:17.322717 19104 net.cpp:245] Setting up conv1a
I0815 19:03:17.322722 19104 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 112 112 (20070400)
I0815 19:03:17.322731 19104 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 19:03:17.322736 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.322746 19104 net.cpp:184] Created Layer conv1a/bn (4)
I0815 19:03:17.322749 19104 net.cpp:561] conv1a/bn <- conv1a
I0815 19:03:17.322752 19104 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 19:03:17.323199 19104 net.cpp:245] Setting up conv1a/bn
I0815 19:03:17.323205 19104 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 112 112 (20070400)
I0815 19:03:17.323212 19104 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 19:03:17.323215 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.323220 19104 net.cpp:184] Created Layer conv1a/relu (5)
I0815 19:03:17.323222 19104 net.cpp:561] conv1a/relu <- conv1a
I0815 19:03:17.323225 19104 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 19:03:17.323233 19104 net.cpp:245] Setting up conv1a/relu
I0815 19:03:17.323236 19104 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 112 112 (20070400)
I0815 19:03:17.323240 19104 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 19:03:17.323242 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.323251 19104 net.cpp:184] Created Layer conv1b (6)
I0815 19:03:17.323253 19104 net.cpp:561] conv1b <- conv1a
I0815 19:03:17.323256 19104 net.cpp:530] conv1b -> conv1b
I0815 19:03:17.332025 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0815 19:03:17.332036 19104 net.cpp:245] Setting up conv1b
I0815 19:03:17.332039 19104 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 112 112 (20070400)
I0815 19:03:17.332044 19104 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 19:03:17.332046 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.332051 19104 net.cpp:184] Created Layer conv1b/bn (7)
I0815 19:03:17.332053 19104 net.cpp:561] conv1b/bn <- conv1b
I0815 19:03:17.332057 19104 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 19:03:17.332481 19104 net.cpp:245] Setting up conv1b/bn
I0815 19:03:17.332489 19104 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 112 112 (20070400)
I0815 19:03:17.332495 19104 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 19:03:17.332497 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.332501 19104 net.cpp:184] Created Layer conv1b/relu (8)
I0815 19:03:17.332504 19104 net.cpp:561] conv1b/relu <- conv1b
I0815 19:03:17.332505 19104 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 19:03:17.332509 19104 net.cpp:245] Setting up conv1b/relu
I0815 19:03:17.332511 19104 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 112 112 (20070400)
I0815 19:03:17.332514 19104 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 19:03:17.332515 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.332520 19104 net.cpp:184] Created Layer pool1 (9)
I0815 19:03:17.332523 19104 net.cpp:561] pool1 <- conv1b
I0815 19:03:17.332526 19104 net.cpp:530] pool1 -> pool1
I0815 19:03:17.332569 19104 net.cpp:245] Setting up pool1
I0815 19:03:17.332574 19104 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 56 56 (5017600)
I0815 19:03:17.332576 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 19:03:17.332578 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.332584 19104 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 19:03:17.332587 19104 net.cpp:561] res2a_branch2a <- pool1
I0815 19:03:17.332589 19104 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 19:03:17.340934 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0815 19:03:17.340945 19104 net.cpp:245] Setting up res2a_branch2a
I0815 19:03:17.340948 19104 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 56 56 (10035200)
I0815 19:03:17.340953 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 19:03:17.340956 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.340960 19104 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 19:03:17.340962 19104 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 19:03:17.340965 19104 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 19:03:17.341373 19104 net.cpp:245] Setting up res2a_branch2a/bn
I0815 19:03:17.341380 19104 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 56 56 (10035200)
I0815 19:03:17.341387 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 19:03:17.341389 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.341392 19104 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 19:03:17.341394 19104 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 19:03:17.341397 19104 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 19:03:17.341399 19104 net.cpp:245] Setting up res2a_branch2a/relu
I0815 19:03:17.341403 19104 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 56 56 (10035200)
I0815 19:03:17.341404 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 19:03:17.341406 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.341413 19104 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 19:03:17.341416 19104 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 19:03:17.341418 19104 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 19:03:17.345943 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.82G, req 0G)
I0815 19:03:17.345954 19104 net.cpp:245] Setting up res2a_branch2b
I0815 19:03:17.345958 19104 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 56 56 (10035200)
I0815 19:03:17.345970 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 19:03:17.345974 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.345979 19104 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 19:03:17.345983 19104 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 19:03:17.345984 19104 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 19:03:17.346390 19104 net.cpp:245] Setting up res2a_branch2b/bn
I0815 19:03:17.346397 19104 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 56 56 (10035200)
I0815 19:03:17.346403 19104 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 19:03:17.346406 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.346410 19104 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 19:03:17.346412 19104 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 19:03:17.346415 19104 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 19:03:17.346417 19104 net.cpp:245] Setting up res2a_branch2b/relu
I0815 19:03:17.346421 19104 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 56 56 (10035200)
I0815 19:03:17.346422 19104 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 19:03:17.346424 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.346428 19104 net.cpp:184] Created Layer pool2 (16)
I0815 19:03:17.346431 19104 net.cpp:561] pool2 <- res2a_branch2b
I0815 19:03:17.346433 19104 net.cpp:530] pool2 -> pool2
I0815 19:03:17.346464 19104 net.cpp:245] Setting up pool2
I0815 19:03:17.346469 19104 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 28 28 (2508800)
I0815 19:03:17.346472 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 19:03:17.346474 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.346479 19104 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 19:03:17.346482 19104 net.cpp:561] res3a_branch2a <- pool2
I0815 19:03:17.346483 19104 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 19:03:17.355317 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.79G, req 0G)
I0815 19:03:17.355353 19104 net.cpp:245] Setting up res3a_branch2a
I0815 19:03:17.355363 19104 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 28 28 (5017600)
I0815 19:03:17.355379 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 19:03:17.355401 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.355427 19104 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 19:03:17.355439 19104 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 19:03:17.355453 19104 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 19:03:17.356086 19104 net.cpp:245] Setting up res3a_branch2a/bn
I0815 19:03:17.356103 19104 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 28 28 (5017600)
I0815 19:03:17.356122 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 19:03:17.356153 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.356166 19104 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 19:03:17.356175 19104 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 19:03:17.356184 19104 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 19:03:17.356196 19104 net.cpp:245] Setting up res3a_branch2a/relu
I0815 19:03:17.356206 19104 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 28 28 (5017600)
I0815 19:03:17.356215 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 19:03:17.356230 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.356256 19104 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 19:03:17.356266 19104 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 19:03:17.356274 19104 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 19:03:17.360229 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.77G, req 0G)
I0815 19:03:17.360250 19104 net.cpp:245] Setting up res3a_branch2b
I0815 19:03:17.360258 19104 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 28 28 (5017600)
I0815 19:03:17.360270 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 19:03:17.360275 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.360286 19104 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 19:03:17.360291 19104 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 19:03:17.360297 19104 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 19:03:17.360771 19104 net.cpp:245] Setting up res3a_branch2b/bn
I0815 19:03:17.360781 19104 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 28 28 (5017600)
I0815 19:03:17.360790 19104 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 19:03:17.360796 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.360801 19104 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 19:03:17.360807 19104 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 19:03:17.360812 19104 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 19:03:17.360818 19104 net.cpp:245] Setting up res3a_branch2b/relu
I0815 19:03:17.360824 19104 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 28 28 (5017600)
I0815 19:03:17.360829 19104 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 19:03:17.360833 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.360841 19104 net.cpp:184] Created Layer pool3 (23)
I0815 19:03:17.360846 19104 net.cpp:561] pool3 <- res3a_branch2b
I0815 19:03:17.360849 19104 net.cpp:530] pool3 -> pool3
I0815 19:03:17.360885 19104 net.cpp:245] Setting up pool3
I0815 19:03:17.360891 19104 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 14 14 (1254400)
I0815 19:03:17.360896 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 19:03:17.360900 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.360914 19104 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 19:03:17.360918 19104 net.cpp:561] res4a_branch2a <- pool3
I0815 19:03:17.360924 19104 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 19:03:17.373513 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.75G, req 0G)
I0815 19:03:17.373539 19104 net.cpp:245] Setting up res4a_branch2a
I0815 19:03:17.373548 19104 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 14 14 (2508800)
I0815 19:03:17.373558 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 19:03:17.373564 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.373574 19104 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 19:03:17.373577 19104 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 19:03:17.373582 19104 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 19:03:17.374055 19104 net.cpp:245] Setting up res4a_branch2a/bn
I0815 19:03:17.374063 19104 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 14 14 (2508800)
I0815 19:03:17.374069 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 19:03:17.374071 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.374075 19104 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 19:03:17.374089 19104 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 19:03:17.374091 19104 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 19:03:17.374095 19104 net.cpp:245] Setting up res4a_branch2a/relu
I0815 19:03:17.374099 19104 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 14 14 (2508800)
I0815 19:03:17.374102 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 19:03:17.374107 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.374116 19104 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 19:03:17.374119 19104 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 19:03:17.374121 19104 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 19:03:17.380152 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.74G, req 0G)
I0815 19:03:17.380165 19104 net.cpp:245] Setting up res4a_branch2b
I0815 19:03:17.380170 19104 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 14 14 (2508800)
I0815 19:03:17.380177 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 19:03:17.380179 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.380185 19104 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 19:03:17.380189 19104 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 19:03:17.380192 19104 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 19:03:17.380619 19104 net.cpp:245] Setting up res4a_branch2b/bn
I0815 19:03:17.380626 19104 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 14 14 (2508800)
I0815 19:03:17.380632 19104 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 19:03:17.380635 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.380638 19104 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 19:03:17.380640 19104 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 19:03:17.380642 19104 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 19:03:17.380646 19104 net.cpp:245] Setting up res4a_branch2b/relu
I0815 19:03:17.380650 19104 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 14 14 (2508800)
I0815 19:03:17.380651 19104 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 19:03:17.380653 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.380657 19104 net.cpp:184] Created Layer pool4 (30)
I0815 19:03:17.380659 19104 net.cpp:561] pool4 <- res4a_branch2b
I0815 19:03:17.380662 19104 net.cpp:530] pool4 -> pool4
I0815 19:03:17.380698 19104 net.cpp:245] Setting up pool4
I0815 19:03:17.380708 19104 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 7 7 (627200)
I0815 19:03:17.380712 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 19:03:17.380714 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.380720 19104 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 19:03:17.380725 19104 net.cpp:561] res5a_branch2a <- pool4
I0815 19:03:17.380729 19104 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 19:03:17.413189 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0815 19:03:17.413206 19104 net.cpp:245] Setting up res5a_branch2a
I0815 19:03:17.413211 19104 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 7 7 (1254400)
I0815 19:03:17.413218 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 19:03:17.413223 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.413230 19104 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 19:03:17.413233 19104 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 19:03:17.413249 19104 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 19:03:17.413689 19104 net.cpp:245] Setting up res5a_branch2a/bn
I0815 19:03:17.413697 19104 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 7 7 (1254400)
I0815 19:03:17.413702 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 19:03:17.413705 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.413708 19104 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 19:03:17.413710 19104 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 19:03:17.413713 19104 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 19:03:17.413717 19104 net.cpp:245] Setting up res5a_branch2a/relu
I0815 19:03:17.413719 19104 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 7 7 (1254400)
I0815 19:03:17.413722 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 19:03:17.413723 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.413729 19104 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 19:03:17.413733 19104 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 19:03:17.413735 19104 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 19:03:17.429404 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.71G, req 0G)
I0815 19:03:17.429415 19104 net.cpp:245] Setting up res5a_branch2b
I0815 19:03:17.429420 19104 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 7 7 (1254400)
I0815 19:03:17.429427 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 19:03:17.429431 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.429436 19104 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 19:03:17.429440 19104 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 19:03:17.429441 19104 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 19:03:17.429872 19104 net.cpp:245] Setting up res5a_branch2b/bn
I0815 19:03:17.429880 19104 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 7 7 (1254400)
I0815 19:03:17.429886 19104 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 19:03:17.429889 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.429893 19104 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 19:03:17.429894 19104 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 19:03:17.429896 19104 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 19:03:17.429900 19104 net.cpp:245] Setting up res5a_branch2b/relu
I0815 19:03:17.429903 19104 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 7 7 (1254400)
I0815 19:03:17.429905 19104 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0815 19:03:17.429908 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.429910 19104 net.cpp:184] Created Layer pool5 (37)
I0815 19:03:17.429914 19104 net.cpp:561] pool5 <- res5a_branch2b
I0815 19:03:17.429918 19104 net.cpp:530] pool5 -> pool5
I0815 19:03:17.429934 19104 net.cpp:245] Setting up pool5
I0815 19:03:17.429937 19104 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0815 19:03:17.429939 19104 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0815 19:03:17.429942 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.429947 19104 net.cpp:184] Created Layer fc1000 (38)
I0815 19:03:17.429949 19104 net.cpp:561] fc1000 <- pool5
I0815 19:03:17.429952 19104 net.cpp:530] fc1000 -> fc1000
I0815 19:03:17.440667 19104 net.cpp:245] Setting up fc1000
I0815 19:03:17.440677 19104 net.cpp:252] TEST Top shape for layer 38 'fc1000' 50 1000 (50000)
I0815 19:03:17.440691 19104 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0815 19:03:17.440696 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.440698 19104 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0815 19:03:17.440701 19104 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0815 19:03:17.440703 19104 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0815 19:03:17.440706 19104 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0815 19:03:17.440709 19104 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0815 19:03:17.440745 19104 net.cpp:245] Setting up fc1000_fc1000_0_split
I0815 19:03:17.440749 19104 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0815 19:03:17.440752 19104 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0815 19:03:17.440754 19104 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0815 19:03:17.440757 19104 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 19:03:17.440758 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.440768 19104 net.cpp:184] Created Layer loss (40)
I0815 19:03:17.440770 19104 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0815 19:03:17.440773 19104 net.cpp:561] loss <- label_data_1_split_0
I0815 19:03:17.440776 19104 net.cpp:530] loss -> loss
I0815 19:03:17.441150 19104 net.cpp:245] Setting up loss
I0815 19:03:17.441157 19104 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0815 19:03:17.441159 19104 net.cpp:256]     with loss weight 1
I0815 19:03:17.441164 19104 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 19:03:17.441166 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.441174 19104 net.cpp:184] Created Layer accuracy/top1 (41)
I0815 19:03:17.441177 19104 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0815 19:03:17.441179 19104 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 19:03:17.441182 19104 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 19:03:17.441186 19104 net.cpp:245] Setting up accuracy/top1
I0815 19:03:17.441190 19104 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0815 19:03:17.441191 19104 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 19:03:17.441195 19104 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:03:17.441202 19104 net.cpp:184] Created Layer accuracy/top5 (42)
I0815 19:03:17.441205 19104 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0815 19:03:17.441207 19104 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 19:03:17.441210 19104 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 19:03:17.441213 19104 net.cpp:245] Setting up accuracy/top5
I0815 19:03:17.441216 19104 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0815 19:03:17.441218 19104 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 19:03:17.441220 19104 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 19:03:17.441222 19104 net.cpp:323] loss needs backward computation.
I0815 19:03:17.441224 19104 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0815 19:03:17.441226 19104 net.cpp:323] fc1000 needs backward computation.
I0815 19:03:17.441228 19104 net.cpp:323] pool5 needs backward computation.
I0815 19:03:17.441231 19104 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 19:03:17.441232 19104 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 19:03:17.441234 19104 net.cpp:323] res5a_branch2b needs backward computation.
I0815 19:03:17.441236 19104 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 19:03:17.441237 19104 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 19:03:17.441239 19104 net.cpp:323] res5a_branch2a needs backward computation.
I0815 19:03:17.441249 19104 net.cpp:323] pool4 needs backward computation.
I0815 19:03:17.441251 19104 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 19:03:17.441253 19104 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 19:03:17.441256 19104 net.cpp:323] res4a_branch2b needs backward computation.
I0815 19:03:17.441259 19104 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 19:03:17.441262 19104 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 19:03:17.441264 19104 net.cpp:323] res4a_branch2a needs backward computation.
I0815 19:03:17.441267 19104 net.cpp:323] pool3 needs backward computation.
I0815 19:03:17.441270 19104 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 19:03:17.441272 19104 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 19:03:17.441274 19104 net.cpp:323] res3a_branch2b needs backward computation.
I0815 19:03:17.441277 19104 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 19:03:17.441279 19104 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 19:03:17.441282 19104 net.cpp:323] res3a_branch2a needs backward computation.
I0815 19:03:17.441283 19104 net.cpp:323] pool2 needs backward computation.
I0815 19:03:17.441287 19104 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 19:03:17.441288 19104 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 19:03:17.441290 19104 net.cpp:323] res2a_branch2b needs backward computation.
I0815 19:03:17.441293 19104 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 19:03:17.441295 19104 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 19:03:17.441298 19104 net.cpp:323] res2a_branch2a needs backward computation.
I0815 19:03:17.441299 19104 net.cpp:323] pool1 needs backward computation.
I0815 19:03:17.441303 19104 net.cpp:323] conv1b/relu needs backward computation.
I0815 19:03:17.441304 19104 net.cpp:323] conv1b/bn needs backward computation.
I0815 19:03:17.441305 19104 net.cpp:323] conv1b needs backward computation.
I0815 19:03:17.441308 19104 net.cpp:323] conv1a/relu needs backward computation.
I0815 19:03:17.441309 19104 net.cpp:323] conv1a/bn needs backward computation.
I0815 19:03:17.441311 19104 net.cpp:323] conv1a needs backward computation.
I0815 19:03:17.441315 19104 net.cpp:325] data/bias does not need backward computation.
I0815 19:03:17.441318 19104 net.cpp:325] label_data_1_split does not need backward computation.
I0815 19:03:17.441321 19104 net.cpp:325] data does not need backward computation.
I0815 19:03:17.441324 19104 net.cpp:367] This network produces output accuracy/top1
I0815 19:03:17.441326 19104 net.cpp:367] This network produces output accuracy/top5
I0815 19:03:17.441329 19104 net.cpp:367] This network produces output loss
I0815 19:03:17.441356 19104 net.cpp:389] Top memory (TEST) required for data: 933273600 diff: 8
I0815 19:03:17.441359 19104 net.cpp:392] Bottom memory (TEST) required for data: 933273600 diff: 933273600
I0815 19:03:17.441361 19104 net.cpp:395] Shared (in-place) memory (TEST) by data: 622182400 diff: 622182400
I0815 19:03:17.441364 19104 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0815 19:03:17.441365 19104 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 19:03:17.441367 19104 net.cpp:407] Network initialization done.
I0815 19:03:17.446975 19104 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 19:03:17.447000 19104 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 19:03:17.447032 19104 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 19:03:17.447044 19104 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447194 19104 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 19:03:17.447199 19104 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 19:03:17.447208 19104 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447310 19104 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 19:03:17.447314 19104 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 19:03:17.447317 19104 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 19:03:17.447334 19104 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447419 19104 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 19:03:17.447423 19104 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 19:03:17.447434 19104 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447518 19104 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 19:03:17.447522 19104 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 19:03:17.447525 19104 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 19:03:17.447566 19104 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447648 19104 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 19:03:17.447651 19104 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 19:03:17.447674 19104 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447747 19104 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 19:03:17.447751 19104 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 19:03:17.447754 19104 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 19:03:17.447870 19104 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:03:17.447947 19104 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 19:03:17.447952 19104 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 19:03:17.448010 19104 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:03:17.448087 19104 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 19:03:17.448091 19104 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 19:03:17.448094 19104 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 19:03:17.448501 19104 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:03:17.448586 19104 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 19:03:17.448590 19104 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 19:03:17.448783 19104 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:03:17.448856 19104 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 19:03:17.448861 19104 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0815 19:03:17.448865 19104 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0815 19:03:17.448992 19104 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 19:03:17.449038 19104 caffe.cpp:290] Running for 1000 iterations.
I0815 19:03:17.454710 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.68G, req 0G)
I0815 19:03:17.467864 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.52G, req 0G)
I0815 19:03:17.483260 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0815 19:03:17.490015 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.25G, req 0G)
I0815 19:03:17.500272 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.15G, req 0G)
I0815 19:03:17.505986 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0815 19:03:17.514366 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 19:03:17.518512 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.04G, req 0G)
I0815 19:03:17.526720 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.02G, req 0G)
I0815 19:03:17.530949 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 1  (limit 7G, req 0G)
I0815 19:03:17.533818 19104 caffe.cpp:313] Batch 0, accuracy/top1 = 0.56
I0815 19:03:17.533833 19104 caffe.cpp:313] Batch 0, accuracy/top5 = 0.8
I0815 19:03:17.533838 19104 caffe.cpp:313] Batch 0, loss = 1.76059
I0815 19:03:17.541828 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 6.27G, req 0G)
I0815 19:03:17.559728 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 6  (limit 5.53G, req 0G)
I0815 19:03:17.582355 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 6  (limit 5.53G, req 0G)
I0815 19:03:17.592144 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 5.53G, req 0G)
I0815 19:03:17.605990 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 5.53G, req 0G)
I0815 19:03:17.614333 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 5.53G, req 0G)
I0815 19:03:17.636992 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 5.53G, req 0G)
I0815 19:03:17.642041 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 5.53G, req 0G)
I0815 19:03:17.661392 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 5.53G, req 0.05G)
I0815 19:03:17.667196 19104 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 6  (limit 5.53G, req 0.05G)
I0815 19:03:17.668525 19104 caffe.cpp:313] Batch 1, accuracy/top1 = 0.58
I0815 19:03:17.668535 19104 caffe.cpp:313] Batch 1, accuracy/top5 = 0.76
I0815 19:03:17.668537 19104 caffe.cpp:313] Batch 1, loss = 1.82315
I0815 19:03:17.693320 19104 caffe.cpp:313] Batch 2, accuracy/top1 = 0.54
I0815 19:03:17.693341 19104 caffe.cpp:313] Batch 2, accuracy/top5 = 0.78
I0815 19:03:17.693344 19104 caffe.cpp:313] Batch 2, loss = 2.14728
I0815 19:03:17.693351 19104 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 19:03:17.741799 19104 caffe.cpp:313] Batch 3, accuracy/top1 = 0.68
I0815 19:03:17.741823 19104 caffe.cpp:313] Batch 3, accuracy/top5 = 0.8
I0815 19:03:17.741827 19104 caffe.cpp:313] Batch 3, loss = 1.52213
I0815 19:03:17.789705 19104 caffe.cpp:313] Batch 4, accuracy/top1 = 0.56
I0815 19:03:17.789731 19104 caffe.cpp:313] Batch 4, accuracy/top5 = 0.8
I0815 19:03:17.789733 19104 caffe.cpp:313] Batch 4, loss = 1.73153
I0815 19:03:17.838114 19104 caffe.cpp:313] Batch 5, accuracy/top1 = 0.62
I0815 19:03:17.838140 19104 caffe.cpp:313] Batch 5, accuracy/top5 = 0.86
I0815 19:03:17.838143 19104 caffe.cpp:313] Batch 5, loss = 1.43906
I0815 19:03:17.887301 19104 caffe.cpp:313] Batch 6, accuracy/top1 = 0.7
I0815 19:03:17.887326 19104 caffe.cpp:313] Batch 6, accuracy/top5 = 0.84
I0815 19:03:17.887331 19104 caffe.cpp:313] Batch 6, loss = 1.34
I0815 19:03:17.936149 19104 caffe.cpp:313] Batch 7, accuracy/top1 = 0.66
I0815 19:03:17.936177 19104 caffe.cpp:313] Batch 7, accuracy/top5 = 0.84
I0815 19:03:17.936180 19104 caffe.cpp:313] Batch 7, loss = 1.53514
I0815 19:03:17.985179 19104 caffe.cpp:313] Batch 8, accuracy/top1 = 0.66
I0815 19:03:17.985203 19104 caffe.cpp:313] Batch 8, accuracy/top5 = 0.88
I0815 19:03:17.985206 19104 caffe.cpp:313] Batch 8, loss = 1.40596
I0815 19:03:18.033491 19104 caffe.cpp:313] Batch 9, accuracy/top1 = 0.54
I0815 19:03:18.033514 19104 caffe.cpp:313] Batch 9, accuracy/top5 = 0.9
I0815 19:03:18.033517 19104 caffe.cpp:313] Batch 9, loss = 1.6993
I0815 19:03:18.082231 19104 caffe.cpp:313] Batch 10, accuracy/top1 = 0.58
I0815 19:03:18.082248 19104 caffe.cpp:313] Batch 10, accuracy/top5 = 0.88
I0815 19:03:18.082252 19104 caffe.cpp:313] Batch 10, loss = 1.45642
I0815 19:03:18.131734 19104 caffe.cpp:313] Batch 11, accuracy/top1 = 0.64
I0815 19:03:18.131772 19104 caffe.cpp:313] Batch 11, accuracy/top5 = 0.86
I0815 19:03:18.131777 19104 caffe.cpp:313] Batch 11, loss = 1.37909
I0815 19:03:18.179090 19104 caffe.cpp:313] Batch 12, accuracy/top1 = 0.62
I0815 19:03:18.179116 19104 caffe.cpp:313] Batch 12, accuracy/top5 = 0.92
I0815 19:03:18.179119 19104 caffe.cpp:313] Batch 12, loss = 1.48717
I0815 19:03:18.227613 19104 caffe.cpp:313] Batch 13, accuracy/top1 = 0.66
I0815 19:03:18.227638 19104 caffe.cpp:313] Batch 13, accuracy/top5 = 0.84
I0815 19:03:18.227641 19104 caffe.cpp:313] Batch 13, loss = 1.44886
I0815 19:03:18.275418 19104 caffe.cpp:313] Batch 14, accuracy/top1 = 0.42
I0815 19:03:18.275441 19104 caffe.cpp:313] Batch 14, accuracy/top5 = 0.82
I0815 19:03:18.275444 19104 caffe.cpp:313] Batch 14, loss = 2.09651
I0815 19:03:18.324750 19104 caffe.cpp:313] Batch 15, accuracy/top1 = 0.52
I0815 19:03:18.324776 19104 caffe.cpp:313] Batch 15, accuracy/top5 = 0.66
I0815 19:03:18.324779 19104 caffe.cpp:313] Batch 15, loss = 2.50692
I0815 19:03:18.374452 19104 caffe.cpp:313] Batch 16, accuracy/top1 = 0.46
I0815 19:03:18.374475 19104 caffe.cpp:313] Batch 16, accuracy/top5 = 0.76
I0815 19:03:18.374477 19104 caffe.cpp:313] Batch 16, loss = 2.06704
I0815 19:03:18.424901 19104 caffe.cpp:313] Batch 17, accuracy/top1 = 0.52
I0815 19:03:18.424924 19104 caffe.cpp:313] Batch 17, accuracy/top5 = 0.78
I0815 19:03:18.424927 19104 caffe.cpp:313] Batch 17, loss = 1.94288
I0815 19:03:18.473942 19104 caffe.cpp:313] Batch 18, accuracy/top1 = 0.66
I0815 19:03:18.473964 19104 caffe.cpp:313] Batch 18, accuracy/top5 = 0.94
I0815 19:03:18.473968 19104 caffe.cpp:313] Batch 18, loss = 1.5206
I0815 19:03:18.521714 19104 caffe.cpp:313] Batch 19, accuracy/top1 = 0.62
I0815 19:03:18.521736 19104 caffe.cpp:313] Batch 19, accuracy/top5 = 0.88
I0815 19:03:18.521739 19104 caffe.cpp:313] Batch 19, loss = 1.43675
I0815 19:03:18.570421 19104 caffe.cpp:313] Batch 20, accuracy/top1 = 0.66
I0815 19:03:18.570442 19104 caffe.cpp:313] Batch 20, accuracy/top5 = 0.82
I0815 19:03:18.570446 19104 caffe.cpp:313] Batch 20, loss = 1.74615
I0815 19:03:18.619076 19104 caffe.cpp:313] Batch 21, accuracy/top1 = 0.64
I0815 19:03:18.619102 19104 caffe.cpp:313] Batch 21, accuracy/top5 = 0.78
I0815 19:03:18.619104 19104 caffe.cpp:313] Batch 21, loss = 1.76472
I0815 19:03:18.667508 19104 caffe.cpp:313] Batch 22, accuracy/top1 = 0.56
I0815 19:03:18.667531 19104 caffe.cpp:313] Batch 22, accuracy/top5 = 0.78
I0815 19:03:18.667534 19104 caffe.cpp:313] Batch 22, loss = 2.25469
I0815 19:03:18.716253 19104 caffe.cpp:313] Batch 23, accuracy/top1 = 0.64
I0815 19:03:18.716276 19104 caffe.cpp:313] Batch 23, accuracy/top5 = 0.78
I0815 19:03:18.716279 19104 caffe.cpp:313] Batch 23, loss = 1.83938
I0815 19:03:18.765566 19104 caffe.cpp:313] Batch 24, accuracy/top1 = 0.58
I0815 19:03:18.765591 19104 caffe.cpp:313] Batch 24, accuracy/top5 = 0.86
I0815 19:03:18.765594 19104 caffe.cpp:313] Batch 24, loss = 1.43041
I0815 19:03:18.814815 19104 caffe.cpp:313] Batch 25, accuracy/top1 = 0.58
I0815 19:03:18.814841 19104 caffe.cpp:313] Batch 25, accuracy/top5 = 0.88
I0815 19:03:18.814843 19104 caffe.cpp:313] Batch 25, loss = 1.61865
I0815 19:03:18.863970 19104 caffe.cpp:313] Batch 26, accuracy/top1 = 0.52
I0815 19:03:18.863996 19104 caffe.cpp:313] Batch 26, accuracy/top5 = 0.8
I0815 19:03:18.863999 19104 caffe.cpp:313] Batch 26, loss = 1.79027
I0815 19:03:18.911761 19104 caffe.cpp:313] Batch 27, accuracy/top1 = 0.4
I0815 19:03:18.911785 19104 caffe.cpp:313] Batch 27, accuracy/top5 = 0.6
I0815 19:03:18.911788 19104 caffe.cpp:313] Batch 27, loss = 2.86465
I0815 19:03:18.959949 19104 caffe.cpp:313] Batch 28, accuracy/top1 = 0.64
I0815 19:03:18.959975 19104 caffe.cpp:313] Batch 28, accuracy/top5 = 0.82
I0815 19:03:18.959977 19104 caffe.cpp:313] Batch 28, loss = 1.531
I0815 19:03:19.008759 19104 caffe.cpp:313] Batch 29, accuracy/top1 = 0.58
I0815 19:03:19.008781 19104 caffe.cpp:313] Batch 29, accuracy/top5 = 0.8
I0815 19:03:19.008785 19104 caffe.cpp:313] Batch 29, loss = 1.98492
I0815 19:03:19.057111 19104 caffe.cpp:313] Batch 30, accuracy/top1 = 0.6
I0815 19:03:19.057129 19104 caffe.cpp:313] Batch 30, accuracy/top5 = 0.76
I0815 19:03:19.057132 19104 caffe.cpp:313] Batch 30, loss = 2.17374
I0815 19:03:19.104461 19104 caffe.cpp:313] Batch 31, accuracy/top1 = 0.6
I0815 19:03:19.104476 19104 caffe.cpp:313] Batch 31, accuracy/top5 = 0.84
I0815 19:03:19.104480 19104 caffe.cpp:313] Batch 31, loss = 1.8113
I0815 19:03:19.152837 19104 caffe.cpp:313] Batch 32, accuracy/top1 = 0.58
I0815 19:03:19.152863 19104 caffe.cpp:313] Batch 32, accuracy/top5 = 0.76
I0815 19:03:19.152864 19104 caffe.cpp:313] Batch 32, loss = 2.06734
I0815 19:03:19.202033 19104 caffe.cpp:313] Batch 33, accuracy/top1 = 0.6
I0815 19:03:19.202056 19104 caffe.cpp:313] Batch 33, accuracy/top5 = 0.84
I0815 19:03:19.202059 19104 caffe.cpp:313] Batch 33, loss = 1.64632
I0815 19:03:19.249910 19104 caffe.cpp:313] Batch 34, accuracy/top1 = 0.58
I0815 19:03:19.249935 19104 caffe.cpp:313] Batch 34, accuracy/top5 = 0.86
I0815 19:03:19.249938 19104 caffe.cpp:313] Batch 34, loss = 1.78808
I0815 19:03:19.299656 19104 caffe.cpp:313] Batch 35, accuracy/top1 = 0.7
I0815 19:03:19.299680 19104 caffe.cpp:313] Batch 35, accuracy/top5 = 0.82
I0815 19:03:19.299684 19104 caffe.cpp:313] Batch 35, loss = 1.46016
I0815 19:03:19.348124 19104 caffe.cpp:313] Batch 36, accuracy/top1 = 0.56
I0815 19:03:19.348150 19104 caffe.cpp:313] Batch 36, accuracy/top5 = 0.78
I0815 19:03:19.348153 19104 caffe.cpp:313] Batch 36, loss = 1.88528
I0815 19:03:19.397771 19104 caffe.cpp:313] Batch 37, accuracy/top1 = 0.66
I0815 19:03:19.397790 19104 caffe.cpp:313] Batch 37, accuracy/top5 = 0.84
I0815 19:03:19.397794 19104 caffe.cpp:313] Batch 37, loss = 1.45864
I0815 19:03:19.446919 19104 caffe.cpp:313] Batch 38, accuracy/top1 = 0.48
I0815 19:03:19.446943 19104 caffe.cpp:313] Batch 38, accuracy/top5 = 0.72
I0815 19:03:19.446946 19104 caffe.cpp:313] Batch 38, loss = 2.6765
I0815 19:03:19.495275 19104 caffe.cpp:313] Batch 39, accuracy/top1 = 0.62
I0815 19:03:19.495298 19104 caffe.cpp:313] Batch 39, accuracy/top5 = 0.84
I0815 19:03:19.495301 19104 caffe.cpp:313] Batch 39, loss = 1.74615
I0815 19:03:19.543812 19104 caffe.cpp:313] Batch 40, accuracy/top1 = 0.48
I0815 19:03:19.543838 19104 caffe.cpp:313] Batch 40, accuracy/top5 = 0.74
I0815 19:03:19.543841 19104 caffe.cpp:313] Batch 40, loss = 2.43288
I0815 19:03:19.592393 19104 caffe.cpp:313] Batch 41, accuracy/top1 = 0.6
I0815 19:03:19.592418 19104 caffe.cpp:313] Batch 41, accuracy/top5 = 0.9
I0815 19:03:19.592422 19104 caffe.cpp:313] Batch 41, loss = 1.46655
I0815 19:03:19.641109 19104 caffe.cpp:313] Batch 42, accuracy/top1 = 0.64
I0815 19:03:19.641129 19104 caffe.cpp:313] Batch 42, accuracy/top5 = 0.8
I0815 19:03:19.641132 19104 caffe.cpp:313] Batch 42, loss = 1.80487
I0815 19:03:19.690277 19104 caffe.cpp:313] Batch 43, accuracy/top1 = 0.56
I0815 19:03:19.690299 19104 caffe.cpp:313] Batch 43, accuracy/top5 = 0.9
I0815 19:03:19.690301 19104 caffe.cpp:313] Batch 43, loss = 1.37384
I0815 19:03:19.739109 19104 caffe.cpp:313] Batch 44, accuracy/top1 = 0.54
I0815 19:03:19.739135 19104 caffe.cpp:313] Batch 44, accuracy/top5 = 0.82
I0815 19:03:19.739137 19104 caffe.cpp:313] Batch 44, loss = 1.76475
I0815 19:03:19.787415 19104 caffe.cpp:313] Batch 45, accuracy/top1 = 0.66
I0815 19:03:19.787439 19104 caffe.cpp:313] Batch 45, accuracy/top5 = 0.76
I0815 19:03:19.787442 19104 caffe.cpp:313] Batch 45, loss = 2.09761
I0815 19:03:19.835782 19104 caffe.cpp:313] Batch 46, accuracy/top1 = 0.68
I0815 19:03:19.835808 19104 caffe.cpp:313] Batch 46, accuracy/top5 = 0.88
I0815 19:03:19.835811 19104 caffe.cpp:313] Batch 46, loss = 1.42787
I0815 19:03:19.885264 19104 caffe.cpp:313] Batch 47, accuracy/top1 = 0.54
I0815 19:03:19.885288 19104 caffe.cpp:313] Batch 47, accuracy/top5 = 0.8
I0815 19:03:19.885291 19104 caffe.cpp:313] Batch 47, loss = 2.05514
I0815 19:03:19.933768 19104 caffe.cpp:313] Batch 48, accuracy/top1 = 0.58
I0815 19:03:19.933787 19104 caffe.cpp:313] Batch 48, accuracy/top5 = 0.78
I0815 19:03:19.933790 19104 caffe.cpp:313] Batch 48, loss = 1.86891
I0815 19:03:19.982321 19104 caffe.cpp:313] Batch 49, accuracy/top1 = 0.66
I0815 19:03:19.982347 19104 caffe.cpp:313] Batch 49, accuracy/top5 = 0.86
I0815 19:03:19.982349 19104 caffe.cpp:313] Batch 49, loss = 1.5049
I0815 19:03:20.031179 19104 caffe.cpp:313] Batch 50, accuracy/top1 = 0.54
I0815 19:03:20.031201 19104 caffe.cpp:313] Batch 50, accuracy/top5 = 0.82
I0815 19:03:20.031204 19104 caffe.cpp:313] Batch 50, loss = 1.90454
I0815 19:03:20.079622 19104 caffe.cpp:313] Batch 51, accuracy/top1 = 0.6
I0815 19:03:20.079640 19104 caffe.cpp:313] Batch 51, accuracy/top5 = 0.74
I0815 19:03:20.079643 19104 caffe.cpp:313] Batch 51, loss = 1.93938
I0815 19:03:20.127678 19104 caffe.cpp:313] Batch 52, accuracy/top1 = 0.6
I0815 19:03:20.127693 19104 caffe.cpp:313] Batch 52, accuracy/top5 = 0.8
I0815 19:03:20.127696 19104 caffe.cpp:313] Batch 52, loss = 1.80244
I0815 19:03:20.176666 19104 caffe.cpp:313] Batch 53, accuracy/top1 = 0.48
I0815 19:03:20.176681 19104 caffe.cpp:313] Batch 53, accuracy/top5 = 0.76
I0815 19:03:20.176683 19104 caffe.cpp:313] Batch 53, loss = 2.25648
I0815 19:03:20.226197 19104 caffe.cpp:313] Batch 54, accuracy/top1 = 0.64
I0815 19:03:20.226222 19104 caffe.cpp:313] Batch 54, accuracy/top5 = 0.92
I0815 19:03:20.226227 19104 caffe.cpp:313] Batch 54, loss = 1.21529
I0815 19:03:20.274940 19104 caffe.cpp:313] Batch 55, accuracy/top1 = 0.58
I0815 19:03:20.274965 19104 caffe.cpp:313] Batch 55, accuracy/top5 = 0.9
I0815 19:03:20.274967 19104 caffe.cpp:313] Batch 55, loss = 1.46642
I0815 19:03:20.323026 19104 caffe.cpp:313] Batch 56, accuracy/top1 = 0.68
I0815 19:03:20.323051 19104 caffe.cpp:313] Batch 56, accuracy/top5 = 0.92
I0815 19:03:20.323055 19104 caffe.cpp:313] Batch 56, loss = 1.14245
I0815 19:03:20.371793 19104 caffe.cpp:313] Batch 57, accuracy/top1 = 0.64
I0815 19:03:20.371815 19104 caffe.cpp:313] Batch 57, accuracy/top5 = 0.82
I0815 19:03:20.371819 19104 caffe.cpp:313] Batch 57, loss = 1.9436
I0815 19:03:20.422602 19104 caffe.cpp:313] Batch 58, accuracy/top1 = 0.66
I0815 19:03:20.422626 19104 caffe.cpp:313] Batch 58, accuracy/top5 = 0.9
I0815 19:03:20.422631 19104 caffe.cpp:313] Batch 58, loss = 1.39074
I0815 19:03:20.471062 19104 caffe.cpp:313] Batch 59, accuracy/top1 = 0.7
I0815 19:03:20.471087 19104 caffe.cpp:313] Batch 59, accuracy/top5 = 0.88
I0815 19:03:20.471091 19104 caffe.cpp:313] Batch 59, loss = 1.29473
I0815 19:03:20.519856 19104 caffe.cpp:313] Batch 60, accuracy/top1 = 0.66
I0815 19:03:20.519879 19104 caffe.cpp:313] Batch 60, accuracy/top5 = 0.88
I0815 19:03:20.519882 19104 caffe.cpp:313] Batch 60, loss = 1.26675
I0815 19:03:20.567618 19104 caffe.cpp:313] Batch 61, accuracy/top1 = 0.62
I0815 19:03:20.567643 19104 caffe.cpp:313] Batch 61, accuracy/top5 = 0.88
I0815 19:03:20.567646 19104 caffe.cpp:313] Batch 61, loss = 1.80526
I0815 19:03:20.616566 19104 caffe.cpp:313] Batch 62, accuracy/top1 = 0.6
I0815 19:03:20.616587 19104 caffe.cpp:313] Batch 62, accuracy/top5 = 0.9
I0815 19:03:20.616590 19104 caffe.cpp:313] Batch 62, loss = 1.30419
I0815 19:03:20.665642 19104 caffe.cpp:313] Batch 63, accuracy/top1 = 0.62
I0815 19:03:20.665663 19104 caffe.cpp:313] Batch 63, accuracy/top5 = 0.8
I0815 19:03:20.665666 19104 caffe.cpp:313] Batch 63, loss = 1.6844
I0815 19:03:20.714931 19104 caffe.cpp:313] Batch 64, accuracy/top1 = 0.52
I0815 19:03:20.714958 19104 caffe.cpp:313] Batch 64, accuracy/top5 = 0.76
I0815 19:03:20.714963 19104 caffe.cpp:313] Batch 64, loss = 1.96599
I0815 19:03:20.762706 19104 caffe.cpp:313] Batch 65, accuracy/top1 = 0.54
I0815 19:03:20.762730 19104 caffe.cpp:313] Batch 65, accuracy/top5 = 0.8
I0815 19:03:20.762733 19104 caffe.cpp:313] Batch 65, loss = 1.85613
I0815 19:03:20.810689 19104 caffe.cpp:313] Batch 66, accuracy/top1 = 0.64
I0815 19:03:20.810714 19104 caffe.cpp:313] Batch 66, accuracy/top5 = 0.82
I0815 19:03:20.810717 19104 caffe.cpp:313] Batch 66, loss = 1.54863
I0815 19:03:20.859941 19104 caffe.cpp:313] Batch 67, accuracy/top1 = 0.54
I0815 19:03:20.859966 19104 caffe.cpp:313] Batch 67, accuracy/top5 = 0.82
I0815 19:03:20.859972 19104 caffe.cpp:313] Batch 67, loss = 1.82875
I0815 19:03:20.908690 19104 caffe.cpp:313] Batch 68, accuracy/top1 = 0.66
I0815 19:03:20.908715 19104 caffe.cpp:313] Batch 68, accuracy/top5 = 0.88
I0815 19:03:20.908720 19104 caffe.cpp:313] Batch 68, loss = 1.23368
I0815 19:03:20.958233 19104 caffe.cpp:313] Batch 69, accuracy/top1 = 0.56
I0815 19:03:20.958259 19104 caffe.cpp:313] Batch 69, accuracy/top5 = 0.82
I0815 19:03:20.958263 19104 caffe.cpp:313] Batch 69, loss = 1.82019
I0815 19:03:21.007226 19104 caffe.cpp:313] Batch 70, accuracy/top1 = 0.48
I0815 19:03:21.007247 19104 caffe.cpp:313] Batch 70, accuracy/top5 = 0.74
I0815 19:03:21.007252 19104 caffe.cpp:313] Batch 70, loss = 2.29745
I0815 19:03:21.056188 19104 caffe.cpp:313] Batch 71, accuracy/top1 = 0.46
I0815 19:03:21.056205 19104 caffe.cpp:313] Batch 71, accuracy/top5 = 0.76
I0815 19:03:21.056210 19104 caffe.cpp:313] Batch 71, loss = 2.34246
I0815 19:03:21.103513 19104 caffe.cpp:313] Batch 72, accuracy/top1 = 0.64
I0815 19:03:21.103528 19104 caffe.cpp:313] Batch 72, accuracy/top5 = 0.84
I0815 19:03:21.103531 19104 caffe.cpp:313] Batch 72, loss = 1.47205
I0815 19:03:21.152110 19104 caffe.cpp:313] Batch 73, accuracy/top1 = 0.62
I0815 19:03:21.152125 19104 caffe.cpp:313] Batch 73, accuracy/top5 = 0.88
I0815 19:03:21.152132 19104 caffe.cpp:313] Batch 73, loss = 1.50391
I0815 19:03:21.200803 19104 caffe.cpp:313] Batch 74, accuracy/top1 = 0.56
I0815 19:03:21.200829 19104 caffe.cpp:313] Batch 74, accuracy/top5 = 0.86
I0815 19:03:21.200834 19104 caffe.cpp:313] Batch 74, loss = 1.60104
I0815 19:03:21.250073 19104 caffe.cpp:313] Batch 75, accuracy/top1 = 0.58
I0815 19:03:21.250103 19104 caffe.cpp:313] Batch 75, accuracy/top5 = 0.78
I0815 19:03:21.250108 19104 caffe.cpp:313] Batch 75, loss = 1.78384
I0815 19:03:21.297731 19104 caffe.cpp:313] Batch 76, accuracy/top1 = 0.6
I0815 19:03:21.297755 19104 caffe.cpp:313] Batch 76, accuracy/top5 = 0.74
I0815 19:03:21.297758 19104 caffe.cpp:313] Batch 76, loss = 1.9228
I0815 19:03:21.346307 19104 caffe.cpp:313] Batch 77, accuracy/top1 = 0.6
I0815 19:03:21.346330 19104 caffe.cpp:313] Batch 77, accuracy/top5 = 0.86
I0815 19:03:21.346333 19104 caffe.cpp:313] Batch 77, loss = 1.48795
I0815 19:03:21.395056 19104 caffe.cpp:313] Batch 78, accuracy/top1 = 0.62
I0815 19:03:21.395076 19104 caffe.cpp:313] Batch 78, accuracy/top5 = 0.78
I0815 19:03:21.395078 19104 caffe.cpp:313] Batch 78, loss = 2.16036
I0815 19:03:21.443735 19104 caffe.cpp:313] Batch 79, accuracy/top1 = 0.74
I0815 19:03:21.443759 19104 caffe.cpp:313] Batch 79, accuracy/top5 = 0.86
I0815 19:03:21.443763 19104 caffe.cpp:313] Batch 79, loss = 1.27364
I0815 19:03:21.493167 19104 caffe.cpp:313] Batch 80, accuracy/top1 = 0.48
I0815 19:03:21.493192 19104 caffe.cpp:313] Batch 80, accuracy/top5 = 0.76
I0815 19:03:21.493196 19104 caffe.cpp:313] Batch 80, loss = 2.21195
I0815 19:03:21.542003 19104 caffe.cpp:313] Batch 81, accuracy/top1 = 0.68
I0815 19:03:21.542027 19104 caffe.cpp:313] Batch 81, accuracy/top5 = 0.82
I0815 19:03:21.542031 19104 caffe.cpp:313] Batch 81, loss = 1.70625
I0815 19:03:21.591549 19104 caffe.cpp:313] Batch 82, accuracy/top1 = 0.56
I0815 19:03:21.591573 19104 caffe.cpp:313] Batch 82, accuracy/top5 = 0.76
I0815 19:03:21.591576 19104 caffe.cpp:313] Batch 82, loss = 1.82646
I0815 19:03:21.639717 19104 caffe.cpp:313] Batch 83, accuracy/top1 = 0.56
I0815 19:03:21.639744 19104 caffe.cpp:313] Batch 83, accuracy/top5 = 0.7
I0815 19:03:21.639746 19104 caffe.cpp:313] Batch 83, loss = 2.22821
I0815 19:03:21.689012 19104 caffe.cpp:313] Batch 84, accuracy/top1 = 0.64
I0815 19:03:21.689031 19104 caffe.cpp:313] Batch 84, accuracy/top5 = 0.76
I0815 19:03:21.689034 19104 caffe.cpp:313] Batch 84, loss = 1.8449
I0815 19:03:21.738884 19104 caffe.cpp:313] Batch 85, accuracy/top1 = 0.8
I0815 19:03:21.738909 19104 caffe.cpp:313] Batch 85, accuracy/top5 = 0.92
I0815 19:03:21.738912 19104 caffe.cpp:313] Batch 85, loss = 1.15547
I0815 19:03:21.787569 19104 caffe.cpp:313] Batch 86, accuracy/top1 = 0.54
I0815 19:03:21.787593 19104 caffe.cpp:313] Batch 86, accuracy/top5 = 0.68
I0815 19:03:21.787596 19104 caffe.cpp:313] Batch 86, loss = 2.11934
I0815 19:03:21.835753 19104 caffe.cpp:313] Batch 87, accuracy/top1 = 0.64
I0815 19:03:21.835778 19104 caffe.cpp:313] Batch 87, accuracy/top5 = 0.8
I0815 19:03:21.835782 19104 caffe.cpp:313] Batch 87, loss = 1.5746
I0815 19:03:21.884771 19104 caffe.cpp:313] Batch 88, accuracy/top1 = 0.66
I0815 19:03:21.884795 19104 caffe.cpp:313] Batch 88, accuracy/top5 = 0.82
I0815 19:03:21.884799 19104 caffe.cpp:313] Batch 88, loss = 1.37921
I0815 19:03:21.933223 19104 caffe.cpp:313] Batch 89, accuracy/top1 = 0.58
I0815 19:03:21.933248 19104 caffe.cpp:313] Batch 89, accuracy/top5 = 0.76
I0815 19:03:21.933250 19104 caffe.cpp:313] Batch 89, loss = 2.01858
I0815 19:03:21.982296 19104 caffe.cpp:313] Batch 90, accuracy/top1 = 0.7
I0815 19:03:21.982321 19104 caffe.cpp:313] Batch 90, accuracy/top5 = 0.9
I0815 19:03:21.982324 19104 caffe.cpp:313] Batch 90, loss = 1.2386
I0815 19:03:22.031219 19104 caffe.cpp:313] Batch 91, accuracy/top1 = 0.58
I0815 19:03:22.031240 19104 caffe.cpp:313] Batch 91, accuracy/top5 = 0.74
I0815 19:03:22.031244 19104 caffe.cpp:313] Batch 91, loss = 2.10451
I0815 19:03:22.079651 19104 caffe.cpp:313] Batch 92, accuracy/top1 = 0.58
I0815 19:03:22.079669 19104 caffe.cpp:313] Batch 92, accuracy/top5 = 0.74
I0815 19:03:22.079674 19104 caffe.cpp:313] Batch 92, loss = 2.17496
I0815 19:03:22.128307 19104 caffe.cpp:313] Batch 93, accuracy/top1 = 0.64
I0815 19:03:22.128321 19104 caffe.cpp:313] Batch 93, accuracy/top5 = 0.88
I0815 19:03:22.128324 19104 caffe.cpp:313] Batch 93, loss = 1.87156
I0815 19:03:22.177412 19104 caffe.cpp:313] Batch 94, accuracy/top1 = 0.74
I0815 19:03:22.177428 19104 caffe.cpp:313] Batch 94, accuracy/top5 = 0.9
I0815 19:03:22.177430 19104 caffe.cpp:313] Batch 94, loss = 1.14213
I0815 19:03:22.226826 19104 caffe.cpp:313] Batch 95, accuracy/top1 = 0.54
I0815 19:03:22.226855 19104 caffe.cpp:313] Batch 95, accuracy/top5 = 0.76
I0815 19:03:22.226857 19104 caffe.cpp:313] Batch 95, loss = 1.99947
I0815 19:03:22.275964 19104 caffe.cpp:313] Batch 96, accuracy/top1 = 0.48
I0815 19:03:22.275988 19104 caffe.cpp:313] Batch 96, accuracy/top5 = 0.74
I0815 19:03:22.275991 19104 caffe.cpp:313] Batch 96, loss = 2.12469
I0815 19:03:22.324867 19104 caffe.cpp:313] Batch 97, accuracy/top1 = 0.68
I0815 19:03:22.324892 19104 caffe.cpp:313] Batch 97, accuracy/top5 = 0.84
I0815 19:03:22.324894 19104 caffe.cpp:313] Batch 97, loss = 1.46076
I0815 19:03:22.374218 19104 caffe.cpp:313] Batch 98, accuracy/top1 = 0.52
I0815 19:03:22.374241 19104 caffe.cpp:313] Batch 98, accuracy/top5 = 0.92
I0815 19:03:22.374244 19104 caffe.cpp:313] Batch 98, loss = 1.78685
I0815 19:03:22.423765 19104 caffe.cpp:313] Batch 99, accuracy/top1 = 0.68
I0815 19:03:22.423789 19104 caffe.cpp:313] Batch 99, accuracy/top5 = 0.82
I0815 19:03:22.423794 19104 caffe.cpp:313] Batch 99, loss = 1.68423
I0815 19:03:22.472188 19104 caffe.cpp:313] Batch 100, accuracy/top1 = 0.68
I0815 19:03:22.472211 19104 caffe.cpp:313] Batch 100, accuracy/top5 = 0.84
I0815 19:03:22.472214 19104 caffe.cpp:313] Batch 100, loss = 1.34939
I0815 19:03:22.520558 19104 caffe.cpp:313] Batch 101, accuracy/top1 = 0.64
I0815 19:03:22.520582 19104 caffe.cpp:313] Batch 101, accuracy/top5 = 0.86
I0815 19:03:22.520586 19104 caffe.cpp:313] Batch 101, loss = 1.50978
I0815 19:03:22.568902 19104 caffe.cpp:313] Batch 102, accuracy/top1 = 0.54
I0815 19:03:22.568925 19104 caffe.cpp:313] Batch 102, accuracy/top5 = 0.72
I0815 19:03:22.568928 19104 caffe.cpp:313] Batch 102, loss = 2.63271
I0815 19:03:22.618229 19104 caffe.cpp:313] Batch 103, accuracy/top1 = 0.72
I0815 19:03:22.618254 19104 caffe.cpp:313] Batch 103, accuracy/top5 = 0.86
I0815 19:03:22.618257 19104 caffe.cpp:313] Batch 103, loss = 1.23623
I0815 19:03:22.666322 19104 caffe.cpp:313] Batch 104, accuracy/top1 = 0.58
I0815 19:03:22.666345 19104 caffe.cpp:313] Batch 104, accuracy/top5 = 0.84
I0815 19:03:22.666348 19104 caffe.cpp:313] Batch 104, loss = 1.64958
I0815 19:03:22.715123 19104 caffe.cpp:313] Batch 105, accuracy/top1 = 0.6
I0815 19:03:22.715147 19104 caffe.cpp:313] Batch 105, accuracy/top5 = 0.74
I0815 19:03:22.715150 19104 caffe.cpp:313] Batch 105, loss = 1.91248
I0815 19:03:22.763671 19104 caffe.cpp:313] Batch 106, accuracy/top1 = 0.56
I0815 19:03:22.763695 19104 caffe.cpp:313] Batch 106, accuracy/top5 = 0.8
I0815 19:03:22.763698 19104 caffe.cpp:313] Batch 106, loss = 1.6977
I0815 19:03:22.812285 19104 caffe.cpp:313] Batch 107, accuracy/top1 = 0.52
I0815 19:03:22.812314 19104 caffe.cpp:313] Batch 107, accuracy/top5 = 0.72
I0815 19:03:22.812320 19104 caffe.cpp:313] Batch 107, loss = 2.01526
I0815 19:03:22.859859 19104 caffe.cpp:313] Batch 108, accuracy/top1 = 0.6
I0815 19:03:22.859882 19104 caffe.cpp:313] Batch 108, accuracy/top5 = 0.86
I0815 19:03:22.859885 19104 caffe.cpp:313] Batch 108, loss = 1.49979
I0815 19:03:22.909073 19104 caffe.cpp:313] Batch 109, accuracy/top1 = 0.68
I0815 19:03:22.909098 19104 caffe.cpp:313] Batch 109, accuracy/top5 = 0.8
I0815 19:03:22.909101 19104 caffe.cpp:313] Batch 109, loss = 1.35014
I0815 19:03:22.957475 19104 caffe.cpp:313] Batch 110, accuracy/top1 = 0.6
I0815 19:03:22.957499 19104 caffe.cpp:313] Batch 110, accuracy/top5 = 0.74
I0815 19:03:22.957501 19104 caffe.cpp:313] Batch 110, loss = 2.06556
I0815 19:03:23.006881 19104 caffe.cpp:313] Batch 111, accuracy/top1 = 0.5
I0815 19:03:23.006906 19104 caffe.cpp:313] Batch 111, accuracy/top5 = 0.8
I0815 19:03:23.006908 19104 caffe.cpp:313] Batch 111, loss = 1.94714
I0815 19:03:23.055254 19104 caffe.cpp:313] Batch 112, accuracy/top1 = 0.58
I0815 19:03:23.055276 19104 caffe.cpp:313] Batch 112, accuracy/top5 = 0.76
I0815 19:03:23.055279 19104 caffe.cpp:313] Batch 112, loss = 2.02529
I0815 19:03:23.104768 19104 caffe.cpp:313] Batch 113, accuracy/top1 = 0.66
I0815 19:03:23.104784 19104 caffe.cpp:313] Batch 113, accuracy/top5 = 0.86
I0815 19:03:23.104786 19104 caffe.cpp:313] Batch 113, loss = 1.31416
I0815 19:03:23.152269 19104 caffe.cpp:313] Batch 114, accuracy/top1 = 0.56
I0815 19:03:23.152297 19104 caffe.cpp:313] Batch 114, accuracy/top5 = 0.84
I0815 19:03:23.152300 19104 caffe.cpp:313] Batch 114, loss = 1.64265
I0815 19:03:23.201144 19104 caffe.cpp:313] Batch 115, accuracy/top1 = 0.6
I0815 19:03:23.201169 19104 caffe.cpp:313] Batch 115, accuracy/top5 = 0.82
I0815 19:03:23.201174 19104 caffe.cpp:313] Batch 115, loss = 1.88113
I0815 19:03:23.250869 19104 caffe.cpp:313] Batch 116, accuracy/top1 = 0.54
I0815 19:03:23.250893 19104 caffe.cpp:313] Batch 116, accuracy/top5 = 0.76
I0815 19:03:23.250896 19104 caffe.cpp:313] Batch 116, loss = 1.95117
I0815 19:03:23.299612 19104 caffe.cpp:313] Batch 117, accuracy/top1 = 0.48
I0815 19:03:23.299638 19104 caffe.cpp:313] Batch 117, accuracy/top5 = 0.84
I0815 19:03:23.299640 19104 caffe.cpp:313] Batch 117, loss = 2.03543
I0815 19:03:23.348168 19104 caffe.cpp:313] Batch 118, accuracy/top1 = 0.42
I0815 19:03:23.348192 19104 caffe.cpp:313] Batch 118, accuracy/top5 = 0.74
I0815 19:03:23.348196 19104 caffe.cpp:313] Batch 118, loss = 2.75341
I0815 19:03:23.398339 19104 caffe.cpp:313] Batch 119, accuracy/top1 = 0.68
I0815 19:03:23.398363 19104 caffe.cpp:313] Batch 119, accuracy/top5 = 0.88
I0815 19:03:23.398366 19104 caffe.cpp:313] Batch 119, loss = 1.36808
I0815 19:03:23.446454 19104 caffe.cpp:313] Batch 120, accuracy/top1 = 0.56
I0815 19:03:23.446477 19104 caffe.cpp:313] Batch 120, accuracy/top5 = 0.82
I0815 19:03:23.446480 19104 caffe.cpp:313] Batch 120, loss = 1.99321
I0815 19:03:23.494295 19104 caffe.cpp:313] Batch 121, accuracy/top1 = 0.62
I0815 19:03:23.494319 19104 caffe.cpp:313] Batch 121, accuracy/top5 = 0.84
I0815 19:03:23.494323 19104 caffe.cpp:313] Batch 121, loss = 1.62891
I0815 19:03:23.542361 19104 caffe.cpp:313] Batch 122, accuracy/top1 = 0.52
I0815 19:03:23.542384 19104 caffe.cpp:313] Batch 122, accuracy/top5 = 0.78
I0815 19:03:23.542387 19104 caffe.cpp:313] Batch 122, loss = 2.45327
I0815 19:03:23.590744 19104 caffe.cpp:313] Batch 123, accuracy/top1 = 0.56
I0815 19:03:23.590770 19104 caffe.cpp:313] Batch 123, accuracy/top5 = 0.8
I0815 19:03:23.590788 19104 caffe.cpp:313] Batch 123, loss = 1.96829
I0815 19:03:23.639876 19104 caffe.cpp:313] Batch 124, accuracy/top1 = 0.66
I0815 19:03:23.639900 19104 caffe.cpp:313] Batch 124, accuracy/top5 = 0.78
I0815 19:03:23.639904 19104 caffe.cpp:313] Batch 124, loss = 1.76799
I0815 19:03:23.688335 19104 caffe.cpp:313] Batch 125, accuracy/top1 = 0.6
I0815 19:03:23.688359 19104 caffe.cpp:313] Batch 125, accuracy/top5 = 0.86
I0815 19:03:23.688362 19104 caffe.cpp:313] Batch 125, loss = 1.46112
I0815 19:03:23.737247 19104 caffe.cpp:313] Batch 126, accuracy/top1 = 0.48
I0815 19:03:23.737268 19104 caffe.cpp:313] Batch 126, accuracy/top5 = 0.84
I0815 19:03:23.737272 19104 caffe.cpp:313] Batch 126, loss = 1.87629
I0815 19:03:23.785234 19104 caffe.cpp:313] Batch 127, accuracy/top1 = 0.48
I0815 19:03:23.785259 19104 caffe.cpp:313] Batch 127, accuracy/top5 = 0.8
I0815 19:03:23.785262 19104 caffe.cpp:313] Batch 127, loss = 1.9864
I0815 19:03:23.834264 19104 caffe.cpp:313] Batch 128, accuracy/top1 = 0.7
I0815 19:03:23.834287 19104 caffe.cpp:313] Batch 128, accuracy/top5 = 0.86
I0815 19:03:23.834291 19104 caffe.cpp:313] Batch 128, loss = 1.48882
I0815 19:03:23.883086 19104 caffe.cpp:313] Batch 129, accuracy/top1 = 0.54
I0815 19:03:23.883111 19104 caffe.cpp:313] Batch 129, accuracy/top5 = 0.86
I0815 19:03:23.883114 19104 caffe.cpp:313] Batch 129, loss = 1.64512
I0815 19:03:23.932095 19104 caffe.cpp:313] Batch 130, accuracy/top1 = 0.56
I0815 19:03:23.932119 19104 caffe.cpp:313] Batch 130, accuracy/top5 = 0.86
I0815 19:03:23.932122 19104 caffe.cpp:313] Batch 130, loss = 1.4806
I0815 19:03:23.979140 19104 caffe.cpp:313] Batch 131, accuracy/top1 = 0.6
I0815 19:03:23.979164 19104 caffe.cpp:313] Batch 131, accuracy/top5 = 0.9
I0815 19:03:23.979168 19104 caffe.cpp:313] Batch 131, loss = 1.3306
I0815 19:03:24.026967 19104 caffe.cpp:313] Batch 132, accuracy/top1 = 0.64
I0815 19:03:24.026986 19104 caffe.cpp:313] Batch 132, accuracy/top5 = 0.82
I0815 19:03:24.026989 19104 caffe.cpp:313] Batch 132, loss = 1.86517
I0815 19:03:24.075132 19104 caffe.cpp:313] Batch 133, accuracy/top1 = 0.6
I0815 19:03:24.075150 19104 caffe.cpp:313] Batch 133, accuracy/top5 = 0.82
I0815 19:03:24.075152 19104 caffe.cpp:313] Batch 133, loss = 1.93756
I0815 19:03:24.123128 19104 caffe.cpp:313] Batch 134, accuracy/top1 = 0.7
I0815 19:03:24.123142 19104 caffe.cpp:313] Batch 134, accuracy/top5 = 0.86
I0815 19:03:24.123145 19104 caffe.cpp:313] Batch 134, loss = 1.42434
I0815 19:03:24.170518 19104 caffe.cpp:313] Batch 135, accuracy/top1 = 0.52
I0815 19:03:24.170532 19104 caffe.cpp:313] Batch 135, accuracy/top5 = 0.88
I0815 19:03:24.170536 19104 caffe.cpp:313] Batch 135, loss = 1.72848
I0815 19:03:24.219782 19104 caffe.cpp:313] Batch 136, accuracy/top1 = 0.5
I0815 19:03:24.219808 19104 caffe.cpp:313] Batch 136, accuracy/top5 = 0.72
I0815 19:03:24.219811 19104 caffe.cpp:313] Batch 136, loss = 2.19518
I0815 19:03:24.268115 19104 caffe.cpp:313] Batch 137, accuracy/top1 = 0.64
I0815 19:03:24.268153 19104 caffe.cpp:313] Batch 137, accuracy/top5 = 0.9
I0815 19:03:24.268157 19104 caffe.cpp:313] Batch 137, loss = 1.38344
I0815 19:03:24.316365 19104 caffe.cpp:313] Batch 138, accuracy/top1 = 0.66
I0815 19:03:24.316396 19104 caffe.cpp:313] Batch 138, accuracy/top5 = 0.88
I0815 19:03:24.316401 19104 caffe.cpp:313] Batch 138, loss = 1.47518
I0815 19:03:24.364414 19104 caffe.cpp:313] Batch 139, accuracy/top1 = 0.6
I0815 19:03:24.364445 19104 caffe.cpp:313] Batch 139, accuracy/top5 = 0.72
I0815 19:03:24.364450 19104 caffe.cpp:313] Batch 139, loss = 2.13779
I0815 19:03:24.412587 19104 caffe.cpp:313] Batch 140, accuracy/top1 = 0.58
I0815 19:03:24.412607 19104 caffe.cpp:313] Batch 140, accuracy/top5 = 0.78
I0815 19:03:24.412611 19104 caffe.cpp:313] Batch 140, loss = 1.91539
I0815 19:03:24.461082 19104 caffe.cpp:313] Batch 141, accuracy/top1 = 0.66
I0815 19:03:24.461107 19104 caffe.cpp:313] Batch 141, accuracy/top5 = 0.9
I0815 19:03:24.461110 19104 caffe.cpp:313] Batch 141, loss = 1.38566
I0815 19:03:24.510109 19104 caffe.cpp:313] Batch 142, accuracy/top1 = 0.58
I0815 19:03:24.510134 19104 caffe.cpp:313] Batch 142, accuracy/top5 = 0.8
I0815 19:03:24.510152 19104 caffe.cpp:313] Batch 142, loss = 1.9333
I0815 19:03:24.559497 19104 caffe.cpp:313] Batch 143, accuracy/top1 = 0.62
I0815 19:03:24.559520 19104 caffe.cpp:313] Batch 143, accuracy/top5 = 0.78
I0815 19:03:24.559525 19104 caffe.cpp:313] Batch 143, loss = 2.08065
I0815 19:03:24.608386 19104 caffe.cpp:313] Batch 144, accuracy/top1 = 0.68
I0815 19:03:24.608412 19104 caffe.cpp:313] Batch 144, accuracy/top5 = 0.82
I0815 19:03:24.608414 19104 caffe.cpp:313] Batch 144, loss = 1.74605
I0815 19:03:24.656893 19104 caffe.cpp:313] Batch 145, accuracy/top1 = 0.7
I0815 19:03:24.656918 19104 caffe.cpp:313] Batch 145, accuracy/top5 = 0.88
I0815 19:03:24.656920 19104 caffe.cpp:313] Batch 145, loss = 1.56794
I0815 19:03:24.706032 19104 caffe.cpp:313] Batch 146, accuracy/top1 = 0.62
I0815 19:03:24.706055 19104 caffe.cpp:313] Batch 146, accuracy/top5 = 0.88
I0815 19:03:24.706058 19104 caffe.cpp:313] Batch 146, loss = 1.3441
I0815 19:03:24.754853 19104 caffe.cpp:313] Batch 147, accuracy/top1 = 0.6
I0815 19:03:24.754879 19104 caffe.cpp:313] Batch 147, accuracy/top5 = 0.9
I0815 19:03:24.754883 19104 caffe.cpp:313] Batch 147, loss = 1.33184
I0815 19:03:24.803722 19104 caffe.cpp:313] Batch 148, accuracy/top1 = 0.58
I0815 19:03:24.803746 19104 caffe.cpp:313] Batch 148, accuracy/top5 = 0.78
I0815 19:03:24.803750 19104 caffe.cpp:313] Batch 148, loss = 2.00164
I0815 19:03:24.852463 19104 caffe.cpp:313] Batch 149, accuracy/top1 = 0.62
I0815 19:03:24.852488 19104 caffe.cpp:313] Batch 149, accuracy/top5 = 0.9
I0815 19:03:24.852491 19104 caffe.cpp:313] Batch 149, loss = 1.30441
I0815 19:03:24.900984 19104 caffe.cpp:313] Batch 150, accuracy/top1 = 0.62
I0815 19:03:24.901006 19104 caffe.cpp:313] Batch 150, accuracy/top5 = 0.78
I0815 19:03:24.901010 19104 caffe.cpp:313] Batch 150, loss = 1.87493
I0815 19:03:24.949832 19104 caffe.cpp:313] Batch 151, accuracy/top1 = 0.62
I0815 19:03:24.949854 19104 caffe.cpp:313] Batch 151, accuracy/top5 = 0.86
I0815 19:03:24.949858 19104 caffe.cpp:313] Batch 151, loss = 1.47413
I0815 19:03:24.997762 19104 caffe.cpp:313] Batch 152, accuracy/top1 = 0.58
I0815 19:03:24.997786 19104 caffe.cpp:313] Batch 152, accuracy/top5 = 0.8
I0815 19:03:24.997790 19104 caffe.cpp:313] Batch 152, loss = 1.78376
I0815 19:03:25.045924 19104 caffe.cpp:313] Batch 153, accuracy/top1 = 0.54
I0815 19:03:25.045943 19104 caffe.cpp:313] Batch 153, accuracy/top5 = 0.8
I0815 19:03:25.045948 19104 caffe.cpp:313] Batch 153, loss = 2.17635
I0815 19:03:25.094341 19104 caffe.cpp:313] Batch 154, accuracy/top1 = 0.6
I0815 19:03:25.094358 19104 caffe.cpp:313] Batch 154, accuracy/top5 = 0.86
I0815 19:03:25.094362 19104 caffe.cpp:313] Batch 154, loss = 1.31343
I0815 19:03:25.143379 19104 caffe.cpp:313] Batch 155, accuracy/top1 = 0.58
I0815 19:03:25.143404 19104 caffe.cpp:313] Batch 155, accuracy/top5 = 0.8
I0815 19:03:25.143409 19104 caffe.cpp:313] Batch 155, loss = 1.72808
I0815 19:03:25.191957 19104 caffe.cpp:313] Batch 156, accuracy/top1 = 0.64
I0815 19:03:25.191982 19104 caffe.cpp:313] Batch 156, accuracy/top5 = 0.8
I0815 19:03:25.191985 19104 caffe.cpp:313] Batch 156, loss = 1.77667
I0815 19:03:25.241457 19104 caffe.cpp:313] Batch 157, accuracy/top1 = 0.56
I0815 19:03:25.241482 19104 caffe.cpp:313] Batch 157, accuracy/top5 = 0.84
I0815 19:03:25.241487 19104 caffe.cpp:313] Batch 157, loss = 1.47891
I0815 19:03:25.290784 19104 caffe.cpp:313] Batch 158, accuracy/top1 = 0.7
I0815 19:03:25.290809 19104 caffe.cpp:313] Batch 158, accuracy/top5 = 0.86
I0815 19:03:25.290813 19104 caffe.cpp:313] Batch 158, loss = 1.54274
I0815 19:03:25.339843 19104 caffe.cpp:313] Batch 159, accuracy/top1 = 0.58
I0815 19:03:25.339869 19104 caffe.cpp:313] Batch 159, accuracy/top5 = 0.8
I0815 19:03:25.339872 19104 caffe.cpp:313] Batch 159, loss = 1.95098
I0815 19:03:25.389572 19104 caffe.cpp:313] Batch 160, accuracy/top1 = 0.68
I0815 19:03:25.389591 19104 caffe.cpp:313] Batch 160, accuracy/top5 = 0.82
I0815 19:03:25.389595 19104 caffe.cpp:313] Batch 160, loss = 1.64206
I0815 19:03:25.438604 19104 caffe.cpp:313] Batch 161, accuracy/top1 = 0.58
I0815 19:03:25.438644 19104 caffe.cpp:313] Batch 161, accuracy/top5 = 0.84
I0815 19:03:25.438648 19104 caffe.cpp:313] Batch 161, loss = 1.94052
I0815 19:03:25.487746 19104 caffe.cpp:313] Batch 162, accuracy/top1 = 0.56
I0815 19:03:25.487771 19104 caffe.cpp:313] Batch 162, accuracy/top5 = 0.82
I0815 19:03:25.487773 19104 caffe.cpp:313] Batch 162, loss = 1.82953
I0815 19:03:25.537255 19104 caffe.cpp:313] Batch 163, accuracy/top1 = 0.64
I0815 19:03:25.537278 19104 caffe.cpp:313] Batch 163, accuracy/top5 = 0.84
I0815 19:03:25.537281 19104 caffe.cpp:313] Batch 163, loss = 1.62104
I0815 19:03:25.587323 19104 caffe.cpp:313] Batch 164, accuracy/top1 = 0.68
I0815 19:03:25.587347 19104 caffe.cpp:313] Batch 164, accuracy/top5 = 0.78
I0815 19:03:25.587350 19104 caffe.cpp:313] Batch 164, loss = 1.46656
I0815 19:03:25.636121 19104 caffe.cpp:313] Batch 165, accuracy/top1 = 0.74
I0815 19:03:25.636149 19104 caffe.cpp:313] Batch 165, accuracy/top5 = 0.9
I0815 19:03:25.636153 19104 caffe.cpp:313] Batch 165, loss = 1.1188
I0815 19:03:25.685045 19104 caffe.cpp:313] Batch 166, accuracy/top1 = 0.42
I0815 19:03:25.685070 19104 caffe.cpp:313] Batch 166, accuracy/top5 = 0.82
I0815 19:03:25.685072 19104 caffe.cpp:313] Batch 166, loss = 2.09795
I0815 19:03:25.733343 19104 caffe.cpp:313] Batch 167, accuracy/top1 = 0.66
I0815 19:03:25.733364 19104 caffe.cpp:313] Batch 167, accuracy/top5 = 0.78
I0815 19:03:25.733368 19104 caffe.cpp:313] Batch 167, loss = 1.46295
I0815 19:03:25.782287 19104 caffe.cpp:313] Batch 168, accuracy/top1 = 0.66
I0815 19:03:25.782310 19104 caffe.cpp:313] Batch 168, accuracy/top5 = 0.92
I0815 19:03:25.782313 19104 caffe.cpp:313] Batch 168, loss = 1.31573
I0815 19:03:25.830837 19104 caffe.cpp:313] Batch 169, accuracy/top1 = 0.76
I0815 19:03:25.830863 19104 caffe.cpp:313] Batch 169, accuracy/top5 = 0.86
I0815 19:03:25.830868 19104 caffe.cpp:313] Batch 169, loss = 1.31835
I0815 19:03:25.878311 19104 caffe.cpp:313] Batch 170, accuracy/top1 = 0.62
I0815 19:03:25.878334 19104 caffe.cpp:313] Batch 170, accuracy/top5 = 0.74
I0815 19:03:25.878338 19104 caffe.cpp:313] Batch 170, loss = 1.73738
I0815 19:03:25.926419 19104 caffe.cpp:313] Batch 171, accuracy/top1 = 0.54
I0815 19:03:25.926443 19104 caffe.cpp:313] Batch 171, accuracy/top5 = 0.86
I0815 19:03:25.926447 19104 caffe.cpp:313] Batch 171, loss = 1.79608
I0815 19:03:25.975000 19104 caffe.cpp:313] Batch 172, accuracy/top1 = 0.66
I0815 19:03:25.975024 19104 caffe.cpp:313] Batch 172, accuracy/top5 = 0.88
I0815 19:03:25.975029 19104 caffe.cpp:313] Batch 172, loss = 1.56078
I0815 19:03:26.023939 19104 caffe.cpp:313] Batch 173, accuracy/top1 = 0.62
I0815 19:03:26.023962 19104 caffe.cpp:313] Batch 173, accuracy/top5 = 0.8
I0815 19:03:26.023965 19104 caffe.cpp:313] Batch 173, loss = 1.83274
I0815 19:03:26.072111 19104 caffe.cpp:313] Batch 174, accuracy/top1 = 0.64
I0815 19:03:26.072132 19104 caffe.cpp:313] Batch 174, accuracy/top5 = 0.82
I0815 19:03:26.072137 19104 caffe.cpp:313] Batch 174, loss = 1.75956
I0815 19:03:26.120540 19104 caffe.cpp:313] Batch 175, accuracy/top1 = 0.66
I0815 19:03:26.120571 19104 caffe.cpp:313] Batch 175, accuracy/top5 = 0.78
I0815 19:03:26.120578 19104 caffe.cpp:313] Batch 175, loss = 1.91159
I0815 19:03:26.168547 19104 caffe.cpp:313] Batch 176, accuracy/top1 = 0.54
I0815 19:03:26.168571 19104 caffe.cpp:313] Batch 176, accuracy/top5 = 0.78
I0815 19:03:26.168576 19104 caffe.cpp:313] Batch 176, loss = 2.02342
I0815 19:03:26.216243 19104 caffe.cpp:313] Batch 177, accuracy/top1 = 0.58
I0815 19:03:26.216269 19104 caffe.cpp:313] Batch 177, accuracy/top5 = 0.76
I0815 19:03:26.216272 19104 caffe.cpp:313] Batch 177, loss = 1.84821
I0815 19:03:26.264494 19104 caffe.cpp:313] Batch 178, accuracy/top1 = 0.58
I0815 19:03:26.264521 19104 caffe.cpp:313] Batch 178, accuracy/top5 = 0.84
I0815 19:03:26.264525 19104 caffe.cpp:313] Batch 178, loss = 1.64629
I0815 19:03:26.314527 19104 caffe.cpp:313] Batch 179, accuracy/top1 = 0.68
I0815 19:03:26.314551 19104 caffe.cpp:313] Batch 179, accuracy/top5 = 0.86
I0815 19:03:26.314556 19104 caffe.cpp:313] Batch 179, loss = 1.52227
I0815 19:03:26.363021 19104 caffe.cpp:313] Batch 180, accuracy/top1 = 0.66
I0815 19:03:26.363049 19104 caffe.cpp:313] Batch 180, accuracy/top5 = 0.84
I0815 19:03:26.363054 19104 caffe.cpp:313] Batch 180, loss = 1.87215
I0815 19:03:26.412632 19104 caffe.cpp:313] Batch 181, accuracy/top1 = 0.54
I0815 19:03:26.412655 19104 caffe.cpp:313] Batch 181, accuracy/top5 = 0.7
I0815 19:03:26.412659 19104 caffe.cpp:313] Batch 181, loss = 2.13018
I0815 19:03:26.461491 19104 caffe.cpp:313] Batch 182, accuracy/top1 = 0.56
I0815 19:03:26.461518 19104 caffe.cpp:313] Batch 182, accuracy/top5 = 0.82
I0815 19:03:26.461521 19104 caffe.cpp:313] Batch 182, loss = 2.16955
I0815 19:03:26.510238 19104 caffe.cpp:313] Batch 183, accuracy/top1 = 0.54
I0815 19:03:26.510263 19104 caffe.cpp:313] Batch 183, accuracy/top5 = 0.76
I0815 19:03:26.510267 19104 caffe.cpp:313] Batch 183, loss = 2.1296
I0815 19:03:26.558157 19104 caffe.cpp:313] Batch 184, accuracy/top1 = 0.62
I0815 19:03:26.558181 19104 caffe.cpp:313] Batch 184, accuracy/top5 = 0.84
I0815 19:03:26.558185 19104 caffe.cpp:313] Batch 184, loss = 1.74815
I0815 19:03:26.606906 19104 caffe.cpp:313] Batch 185, accuracy/top1 = 0.54
I0815 19:03:26.606931 19104 caffe.cpp:313] Batch 185, accuracy/top5 = 0.78
I0815 19:03:26.606935 19104 caffe.cpp:313] Batch 185, loss = 1.8815
I0815 19:03:26.655841 19104 caffe.cpp:313] Batch 186, accuracy/top1 = 0.56
I0815 19:03:26.655865 19104 caffe.cpp:313] Batch 186, accuracy/top5 = 0.86
I0815 19:03:26.655869 19104 caffe.cpp:313] Batch 186, loss = 1.51485
I0815 19:03:26.705183 19104 caffe.cpp:313] Batch 187, accuracy/top1 = 0.58
I0815 19:03:26.705207 19104 caffe.cpp:313] Batch 187, accuracy/top5 = 0.8
I0815 19:03:26.705212 19104 caffe.cpp:313] Batch 187, loss = 1.8065
I0815 19:03:26.753237 19104 caffe.cpp:313] Batch 188, accuracy/top1 = 0.62
I0815 19:03:26.753260 19104 caffe.cpp:313] Batch 188, accuracy/top5 = 0.82
I0815 19:03:26.753264 19104 caffe.cpp:313] Batch 188, loss = 1.6045
I0815 19:03:26.802165 19104 caffe.cpp:313] Batch 189, accuracy/top1 = 0.6
I0815 19:03:26.802188 19104 caffe.cpp:313] Batch 189, accuracy/top5 = 0.78
I0815 19:03:26.802192 19104 caffe.cpp:313] Batch 189, loss = 1.94114
I0815 19:03:26.850509 19104 caffe.cpp:313] Batch 190, accuracy/top1 = 0.58
I0815 19:03:26.850534 19104 caffe.cpp:313] Batch 190, accuracy/top5 = 0.9
I0815 19:03:26.850538 19104 caffe.cpp:313] Batch 190, loss = 1.52102
I0815 19:03:26.899281 19104 caffe.cpp:313] Batch 191, accuracy/top1 = 0.56
I0815 19:03:26.899307 19104 caffe.cpp:313] Batch 191, accuracy/top5 = 0.78
I0815 19:03:26.899310 19104 caffe.cpp:313] Batch 191, loss = 2.18466
I0815 19:03:26.947813 19104 caffe.cpp:313] Batch 192, accuracy/top1 = 0.56
I0815 19:03:26.947839 19104 caffe.cpp:313] Batch 192, accuracy/top5 = 0.84
I0815 19:03:26.947842 19104 caffe.cpp:313] Batch 192, loss = 1.83682
I0815 19:03:26.996810 19104 caffe.cpp:313] Batch 193, accuracy/top1 = 0.62
I0815 19:03:26.996832 19104 caffe.cpp:313] Batch 193, accuracy/top5 = 0.8
I0815 19:03:26.996836 19104 caffe.cpp:313] Batch 193, loss = 1.67753
I0815 19:03:27.045310 19104 caffe.cpp:313] Batch 194, accuracy/top1 = 0.6
I0815 19:03:27.045332 19104 caffe.cpp:313] Batch 194, accuracy/top5 = 0.88
I0815 19:03:27.045336 19104 caffe.cpp:313] Batch 194, loss = 1.55903
I0815 19:03:27.093855 19104 caffe.cpp:313] Batch 195, accuracy/top1 = 0.6
I0815 19:03:27.093875 19104 caffe.cpp:313] Batch 195, accuracy/top5 = 0.88
I0815 19:03:27.093879 19104 caffe.cpp:313] Batch 195, loss = 1.61149
I0815 19:03:27.142899 19104 caffe.cpp:313] Batch 196, accuracy/top1 = 0.7
I0815 19:03:27.142922 19104 caffe.cpp:313] Batch 196, accuracy/top5 = 0.86
I0815 19:03:27.142925 19104 caffe.cpp:313] Batch 196, loss = 1.23445
I0815 19:03:27.191815 19104 caffe.cpp:313] Batch 197, accuracy/top1 = 0.7
I0815 19:03:27.191843 19104 caffe.cpp:313] Batch 197, accuracy/top5 = 0.9
I0815 19:03:27.191848 19104 caffe.cpp:313] Batch 197, loss = 1.37325
I0815 19:03:27.241379 19104 caffe.cpp:313] Batch 198, accuracy/top1 = 0.62
I0815 19:03:27.241405 19104 caffe.cpp:313] Batch 198, accuracy/top5 = 0.86
I0815 19:03:27.241423 19104 caffe.cpp:313] Batch 198, loss = 1.75827
I0815 19:03:27.289254 19104 caffe.cpp:313] Batch 199, accuracy/top1 = 0.72
I0815 19:03:27.289279 19104 caffe.cpp:313] Batch 199, accuracy/top5 = 0.82
I0815 19:03:27.289283 19104 caffe.cpp:313] Batch 199, loss = 1.22874
I0815 19:03:27.337949 19104 caffe.cpp:313] Batch 200, accuracy/top1 = 0.62
I0815 19:03:27.337971 19104 caffe.cpp:313] Batch 200, accuracy/top5 = 0.86
I0815 19:03:27.337975 19104 caffe.cpp:313] Batch 200, loss = 1.57791
I0815 19:03:27.388837 19104 caffe.cpp:313] Batch 201, accuracy/top1 = 0.44
I0815 19:03:27.388857 19104 caffe.cpp:313] Batch 201, accuracy/top5 = 0.74
I0815 19:03:27.388861 19104 caffe.cpp:313] Batch 201, loss = 2.09519
I0815 19:03:27.437497 19104 caffe.cpp:313] Batch 202, accuracy/top1 = 0.48
I0815 19:03:27.437522 19104 caffe.cpp:313] Batch 202, accuracy/top5 = 0.84
I0815 19:03:27.437526 19104 caffe.cpp:313] Batch 202, loss = 1.87855
I0815 19:03:27.485780 19104 caffe.cpp:313] Batch 203, accuracy/top1 = 0.64
I0815 19:03:27.485803 19104 caffe.cpp:313] Batch 203, accuracy/top5 = 0.82
I0815 19:03:27.485807 19104 caffe.cpp:313] Batch 203, loss = 1.59294
I0815 19:03:27.534523 19104 caffe.cpp:313] Batch 204, accuracy/top1 = 0.68
I0815 19:03:27.534548 19104 caffe.cpp:313] Batch 204, accuracy/top5 = 0.86
I0815 19:03:27.534553 19104 caffe.cpp:313] Batch 204, loss = 1.35293
I0815 19:03:27.583176 19104 caffe.cpp:313] Batch 205, accuracy/top1 = 0.54
I0815 19:03:27.583200 19104 caffe.cpp:313] Batch 205, accuracy/top5 = 0.86
I0815 19:03:27.583204 19104 caffe.cpp:313] Batch 205, loss = 1.61055
I0815 19:03:27.631728 19104 caffe.cpp:313] Batch 206, accuracy/top1 = 0.6
I0815 19:03:27.631753 19104 caffe.cpp:313] Batch 206, accuracy/top5 = 0.9
I0815 19:03:27.631757 19104 caffe.cpp:313] Batch 206, loss = 1.53589
I0815 19:03:27.680197 19104 caffe.cpp:313] Batch 207, accuracy/top1 = 0.58
I0815 19:03:27.680222 19104 caffe.cpp:313] Batch 207, accuracy/top5 = 0.82
I0815 19:03:27.680227 19104 caffe.cpp:313] Batch 207, loss = 1.808
I0815 19:03:27.728354 19104 caffe.cpp:313] Batch 208, accuracy/top1 = 0.56
I0815 19:03:27.728379 19104 caffe.cpp:313] Batch 208, accuracy/top5 = 0.72
I0815 19:03:27.728384 19104 caffe.cpp:313] Batch 208, loss = 2.29278
I0815 19:03:27.776489 19104 caffe.cpp:313] Batch 209, accuracy/top1 = 0.62
I0815 19:03:27.776509 19104 caffe.cpp:313] Batch 209, accuracy/top5 = 0.88
I0815 19:03:27.776512 19104 caffe.cpp:313] Batch 209, loss = 1.49
I0815 19:03:27.826282 19104 caffe.cpp:313] Batch 210, accuracy/top1 = 0.4
I0815 19:03:27.826305 19104 caffe.cpp:313] Batch 210, accuracy/top5 = 0.66
I0815 19:03:27.826309 19104 caffe.cpp:313] Batch 210, loss = 2.97224
I0815 19:03:27.874745 19104 caffe.cpp:313] Batch 211, accuracy/top1 = 0.58
I0815 19:03:27.874773 19104 caffe.cpp:313] Batch 211, accuracy/top5 = 0.84
I0815 19:03:27.874776 19104 caffe.cpp:313] Batch 211, loss = 1.77085
I0815 19:03:27.923859 19104 caffe.cpp:313] Batch 212, accuracy/top1 = 0.62
I0815 19:03:27.923887 19104 caffe.cpp:313] Batch 212, accuracy/top5 = 0.8
I0815 19:03:27.923892 19104 caffe.cpp:313] Batch 212, loss = 1.67786
I0815 19:03:27.972615 19104 caffe.cpp:313] Batch 213, accuracy/top1 = 0.66
I0815 19:03:27.972640 19104 caffe.cpp:313] Batch 213, accuracy/top5 = 0.84
I0815 19:03:27.972645 19104 caffe.cpp:313] Batch 213, loss = 1.72325
I0815 19:03:28.021247 19104 caffe.cpp:313] Batch 214, accuracy/top1 = 0.58
I0815 19:03:28.021270 19104 caffe.cpp:313] Batch 214, accuracy/top5 = 0.86
I0815 19:03:28.021275 19104 caffe.cpp:313] Batch 214, loss = 1.89092
I0815 19:03:28.068744 19104 caffe.cpp:313] Batch 215, accuracy/top1 = 0.58
I0815 19:03:28.068763 19104 caffe.cpp:313] Batch 215, accuracy/top5 = 0.76
I0815 19:03:28.068768 19104 caffe.cpp:313] Batch 215, loss = 1.82391
I0815 19:03:28.116905 19104 caffe.cpp:313] Batch 216, accuracy/top1 = 0.7
I0815 19:03:28.116921 19104 caffe.cpp:313] Batch 216, accuracy/top5 = 0.88
I0815 19:03:28.116925 19104 caffe.cpp:313] Batch 216, loss = 1.28102
I0815 19:03:28.165719 19104 caffe.cpp:313] Batch 217, accuracy/top1 = 0.64
I0815 19:03:28.165735 19104 caffe.cpp:313] Batch 217, accuracy/top5 = 0.82
I0815 19:03:28.165751 19104 caffe.cpp:313] Batch 217, loss = 1.63195
I0815 19:03:28.214927 19104 caffe.cpp:313] Batch 218, accuracy/top1 = 0.58
I0815 19:03:28.214954 19104 caffe.cpp:313] Batch 218, accuracy/top5 = 0.86
I0815 19:03:28.214958 19104 caffe.cpp:313] Batch 218, loss = 1.69627
I0815 19:03:28.263886 19104 caffe.cpp:313] Batch 219, accuracy/top1 = 0.62
I0815 19:03:28.263912 19104 caffe.cpp:313] Batch 219, accuracy/top5 = 0.88
I0815 19:03:28.263916 19104 caffe.cpp:313] Batch 219, loss = 1.40603
I0815 19:03:28.313076 19104 caffe.cpp:313] Batch 220, accuracy/top1 = 0.54
I0815 19:03:28.313102 19104 caffe.cpp:313] Batch 220, accuracy/top5 = 0.76
I0815 19:03:28.313107 19104 caffe.cpp:313] Batch 220, loss = 2.17351
I0815 19:03:28.361721 19104 caffe.cpp:313] Batch 221, accuracy/top1 = 0.64
I0815 19:03:28.361744 19104 caffe.cpp:313] Batch 221, accuracy/top5 = 0.9
I0815 19:03:28.361748 19104 caffe.cpp:313] Batch 221, loss = 1.46439
I0815 19:03:28.411049 19104 caffe.cpp:313] Batch 222, accuracy/top1 = 0.64
I0815 19:03:28.411074 19104 caffe.cpp:313] Batch 222, accuracy/top5 = 0.78
I0815 19:03:28.411078 19104 caffe.cpp:313] Batch 222, loss = 2.06549
I0815 19:03:28.459421 19104 caffe.cpp:313] Batch 223, accuracy/top1 = 0.52
I0815 19:03:28.459446 19104 caffe.cpp:313] Batch 223, accuracy/top5 = 0.8
I0815 19:03:28.459450 19104 caffe.cpp:313] Batch 223, loss = 1.99362
I0815 19:03:28.508489 19104 caffe.cpp:313] Batch 224, accuracy/top1 = 0.48
I0815 19:03:28.508514 19104 caffe.cpp:313] Batch 224, accuracy/top5 = 0.78
I0815 19:03:28.508518 19104 caffe.cpp:313] Batch 224, loss = 2.23844
I0815 19:03:28.555901 19104 caffe.cpp:313] Batch 225, accuracy/top1 = 0.74
I0815 19:03:28.555927 19104 caffe.cpp:313] Batch 225, accuracy/top5 = 0.84
I0815 19:03:28.555930 19104 caffe.cpp:313] Batch 225, loss = 1.51389
I0815 19:03:28.605379 19104 caffe.cpp:313] Batch 226, accuracy/top1 = 0.66
I0815 19:03:28.605406 19104 caffe.cpp:313] Batch 226, accuracy/top5 = 0.84
I0815 19:03:28.605409 19104 caffe.cpp:313] Batch 226, loss = 1.58049
I0815 19:03:28.655100 19104 caffe.cpp:313] Batch 227, accuracy/top1 = 0.6
I0815 19:03:28.655125 19104 caffe.cpp:313] Batch 227, accuracy/top5 = 0.88
I0815 19:03:28.655129 19104 caffe.cpp:313] Batch 227, loss = 1.55147
I0815 19:03:28.704385 19104 caffe.cpp:313] Batch 228, accuracy/top1 = 0.62
I0815 19:03:28.704409 19104 caffe.cpp:313] Batch 228, accuracy/top5 = 0.8
I0815 19:03:28.704413 19104 caffe.cpp:313] Batch 228, loss = 1.45404
I0815 19:03:28.753950 19104 caffe.cpp:313] Batch 229, accuracy/top1 = 0.66
I0815 19:03:28.753976 19104 caffe.cpp:313] Batch 229, accuracy/top5 = 0.82
I0815 19:03:28.753980 19104 caffe.cpp:313] Batch 229, loss = 1.38906
I0815 19:03:28.803560 19104 caffe.cpp:313] Batch 230, accuracy/top1 = 0.76
I0815 19:03:28.803587 19104 caffe.cpp:313] Batch 230, accuracy/top5 = 0.92
I0815 19:03:28.803591 19104 caffe.cpp:313] Batch 230, loss = 1.11742
I0815 19:03:28.852545 19104 caffe.cpp:313] Batch 231, accuracy/top1 = 0.48
I0815 19:03:28.852571 19104 caffe.cpp:313] Batch 231, accuracy/top5 = 0.8
I0815 19:03:28.852574 19104 caffe.cpp:313] Batch 231, loss = 2.24129
I0815 19:03:28.901824 19104 caffe.cpp:313] Batch 232, accuracy/top1 = 0.54
I0815 19:03:28.901850 19104 caffe.cpp:313] Batch 232, accuracy/top5 = 0.8
I0815 19:03:28.901854 19104 caffe.cpp:313] Batch 232, loss = 2.01184
I0815 19:03:28.950016 19104 caffe.cpp:313] Batch 233, accuracy/top1 = 0.62
I0815 19:03:28.950042 19104 caffe.cpp:313] Batch 233, accuracy/top5 = 0.86
I0815 19:03:28.950045 19104 caffe.cpp:313] Batch 233, loss = 1.55327
I0815 19:03:28.999429 19104 caffe.cpp:313] Batch 234, accuracy/top1 = 0.6
I0815 19:03:28.999455 19104 caffe.cpp:313] Batch 234, accuracy/top5 = 0.82
I0815 19:03:28.999459 19104 caffe.cpp:313] Batch 234, loss = 1.80995
I0815 19:03:29.048478 19104 caffe.cpp:313] Batch 235, accuracy/top1 = 0.58
I0815 19:03:29.048499 19104 caffe.cpp:313] Batch 235, accuracy/top5 = 0.86
I0815 19:03:29.048504 19104 caffe.cpp:313] Batch 235, loss = 1.53632
I0815 19:03:29.097039 19104 caffe.cpp:313] Batch 236, accuracy/top1 = 0.58
I0815 19:03:29.097065 19104 caffe.cpp:313] Batch 236, accuracy/top5 = 0.8
I0815 19:03:29.097069 19104 caffe.cpp:313] Batch 236, loss = 1.98005
I0815 19:03:29.147099 19104 caffe.cpp:313] Batch 237, accuracy/top1 = 0.66
I0815 19:03:29.147125 19104 caffe.cpp:313] Batch 237, accuracy/top5 = 0.92
I0815 19:03:29.147130 19104 caffe.cpp:313] Batch 237, loss = 1.19207
I0815 19:03:29.195293 19104 caffe.cpp:313] Batch 238, accuracy/top1 = 0.6
I0815 19:03:29.195341 19104 caffe.cpp:313] Batch 238, accuracy/top5 = 0.86
I0815 19:03:29.195353 19104 caffe.cpp:313] Batch 238, loss = 1.72139
I0815 19:03:29.244532 19104 caffe.cpp:313] Batch 239, accuracy/top1 = 0.68
I0815 19:03:29.244556 19104 caffe.cpp:313] Batch 239, accuracy/top5 = 0.9
I0815 19:03:29.244560 19104 caffe.cpp:313] Batch 239, loss = 1.27451
I0815 19:03:29.292997 19104 caffe.cpp:313] Batch 240, accuracy/top1 = 0.56
I0815 19:03:29.293023 19104 caffe.cpp:313] Batch 240, accuracy/top5 = 0.82
I0815 19:03:29.293027 19104 caffe.cpp:313] Batch 240, loss = 1.56569
I0815 19:03:29.341222 19104 caffe.cpp:313] Batch 241, accuracy/top1 = 0.54
I0815 19:03:29.341246 19104 caffe.cpp:313] Batch 241, accuracy/top5 = 0.72
I0815 19:03:29.341250 19104 caffe.cpp:313] Batch 241, loss = 2.21261
I0815 19:03:29.390802 19104 caffe.cpp:313] Batch 242, accuracy/top1 = 0.6
I0815 19:03:29.390822 19104 caffe.cpp:313] Batch 242, accuracy/top5 = 0.84
I0815 19:03:29.390825 19104 caffe.cpp:313] Batch 242, loss = 1.57119
I0815 19:03:29.439666 19104 caffe.cpp:313] Batch 243, accuracy/top1 = 0.5
I0815 19:03:29.439689 19104 caffe.cpp:313] Batch 243, accuracy/top5 = 0.82
I0815 19:03:29.439693 19104 caffe.cpp:313] Batch 243, loss = 1.68554
I0815 19:03:29.488086 19104 caffe.cpp:313] Batch 244, accuracy/top1 = 0.52
I0815 19:03:29.488111 19104 caffe.cpp:313] Batch 244, accuracy/top5 = 0.86
I0815 19:03:29.488114 19104 caffe.cpp:313] Batch 244, loss = 1.89907
I0815 19:03:29.536730 19104 caffe.cpp:313] Batch 245, accuracy/top1 = 0.48
I0815 19:03:29.536753 19104 caffe.cpp:313] Batch 245, accuracy/top5 = 0.68
I0815 19:03:29.536756 19104 caffe.cpp:313] Batch 245, loss = 2.54722
I0815 19:03:29.585589 19104 caffe.cpp:313] Batch 246, accuracy/top1 = 0.54
I0815 19:03:29.585613 19104 caffe.cpp:313] Batch 246, accuracy/top5 = 0.78
I0815 19:03:29.585616 19104 caffe.cpp:313] Batch 246, loss = 2.05759
I0815 19:03:29.635354 19104 caffe.cpp:313] Batch 247, accuracy/top1 = 0.44
I0815 19:03:29.635380 19104 caffe.cpp:313] Batch 247, accuracy/top5 = 0.7
I0815 19:03:29.635382 19104 caffe.cpp:313] Batch 247, loss = 2.2518
I0815 19:03:29.684448 19104 caffe.cpp:313] Batch 248, accuracy/top1 = 0.52
I0815 19:03:29.684473 19104 caffe.cpp:313] Batch 248, accuracy/top5 = 0.68
I0815 19:03:29.684475 19104 caffe.cpp:313] Batch 248, loss = 2.33689
I0815 19:03:29.733427 19104 caffe.cpp:313] Batch 249, accuracy/top1 = 0.52
I0815 19:03:29.733450 19104 caffe.cpp:313] Batch 249, accuracy/top5 = 0.84
I0815 19:03:29.733453 19104 caffe.cpp:313] Batch 249, loss = 1.93496
I0815 19:03:29.782902 19104 caffe.cpp:313] Batch 250, accuracy/top1 = 0.7
I0815 19:03:29.782927 19104 caffe.cpp:313] Batch 250, accuracy/top5 = 0.94
I0815 19:03:29.782930 19104 caffe.cpp:313] Batch 250, loss = 1.20309
I0815 19:03:29.831951 19104 caffe.cpp:313] Batch 251, accuracy/top1 = 0.68
I0815 19:03:29.831974 19104 caffe.cpp:313] Batch 251, accuracy/top5 = 0.86
I0815 19:03:29.831976 19104 caffe.cpp:313] Batch 251, loss = 1.49695
I0815 19:03:29.880846 19104 caffe.cpp:313] Batch 252, accuracy/top1 = 0.52
I0815 19:03:29.880870 19104 caffe.cpp:313] Batch 252, accuracy/top5 = 0.76
I0815 19:03:29.880873 19104 caffe.cpp:313] Batch 252, loss = 2.41813
I0815 19:03:29.929080 19104 caffe.cpp:313] Batch 253, accuracy/top1 = 0.58
I0815 19:03:29.929103 19104 caffe.cpp:313] Batch 253, accuracy/top5 = 0.78
I0815 19:03:29.929106 19104 caffe.cpp:313] Batch 253, loss = 1.7955
I0815 19:03:29.977588 19104 caffe.cpp:313] Batch 254, accuracy/top1 = 0.52
I0815 19:03:29.977612 19104 caffe.cpp:313] Batch 254, accuracy/top5 = 0.76
I0815 19:03:29.977615 19104 caffe.cpp:313] Batch 254, loss = 2.0072
I0815 19:03:30.026299 19104 caffe.cpp:313] Batch 255, accuracy/top1 = 0.48
I0815 19:03:30.026319 19104 caffe.cpp:313] Batch 255, accuracy/top5 = 0.78
I0815 19:03:30.026322 19104 caffe.cpp:313] Batch 255, loss = 2.42493
I0815 19:03:30.075485 19104 caffe.cpp:313] Batch 256, accuracy/top1 = 0.6
I0815 19:03:30.075503 19104 caffe.cpp:313] Batch 256, accuracy/top5 = 0.88
I0815 19:03:30.075506 19104 caffe.cpp:313] Batch 256, loss = 1.57175
I0815 19:03:30.124347 19104 caffe.cpp:313] Batch 257, accuracy/top1 = 0.6
I0815 19:03:30.124362 19104 caffe.cpp:313] Batch 257, accuracy/top5 = 0.84
I0815 19:03:30.124366 19104 caffe.cpp:313] Batch 257, loss = 1.89555
I0815 19:03:30.172161 19104 caffe.cpp:313] Batch 258, accuracy/top1 = 0.56
I0815 19:03:30.172176 19104 caffe.cpp:313] Batch 258, accuracy/top5 = 0.74
I0815 19:03:30.172179 19104 caffe.cpp:313] Batch 258, loss = 2.22462
I0815 19:03:30.220935 19104 caffe.cpp:313] Batch 259, accuracy/top1 = 0.52
I0815 19:03:30.220959 19104 caffe.cpp:313] Batch 259, accuracy/top5 = 0.86
I0815 19:03:30.220963 19104 caffe.cpp:313] Batch 259, loss = 1.93512
I0815 19:03:30.269258 19104 caffe.cpp:313] Batch 260, accuracy/top1 = 0.58
I0815 19:03:30.269289 19104 caffe.cpp:313] Batch 260, accuracy/top5 = 0.8
I0815 19:03:30.269292 19104 caffe.cpp:313] Batch 260, loss = 1.66898
I0815 19:03:30.318066 19104 caffe.cpp:313] Batch 261, accuracy/top1 = 0.6
I0815 19:03:30.318089 19104 caffe.cpp:313] Batch 261, accuracy/top5 = 0.8
I0815 19:03:30.318092 19104 caffe.cpp:313] Batch 261, loss = 1.85891
I0815 19:03:30.366386 19104 caffe.cpp:313] Batch 262, accuracy/top1 = 0.54
I0815 19:03:30.366418 19104 caffe.cpp:313] Batch 262, accuracy/top5 = 0.8
I0815 19:03:30.366423 19104 caffe.cpp:313] Batch 262, loss = 1.89179
I0815 19:03:30.415596 19104 caffe.cpp:313] Batch 263, accuracy/top1 = 0.58
I0815 19:03:30.415616 19104 caffe.cpp:313] Batch 263, accuracy/top5 = 0.84
I0815 19:03:30.415621 19104 caffe.cpp:313] Batch 263, loss = 1.73854
I0815 19:03:30.464485 19104 caffe.cpp:313] Batch 264, accuracy/top1 = 0.64
I0815 19:03:30.464509 19104 caffe.cpp:313] Batch 264, accuracy/top5 = 0.8
I0815 19:03:30.464514 19104 caffe.cpp:313] Batch 264, loss = 1.65628
I0815 19:03:30.512368 19104 caffe.cpp:313] Batch 265, accuracy/top1 = 0.7
I0815 19:03:30.512393 19104 caffe.cpp:313] Batch 265, accuracy/top5 = 0.88
I0815 19:03:30.512398 19104 caffe.cpp:313] Batch 265, loss = 1.37985
I0815 19:03:30.560309 19104 caffe.cpp:313] Batch 266, accuracy/top1 = 0.62
I0815 19:03:30.560333 19104 caffe.cpp:313] Batch 266, accuracy/top5 = 0.86
I0815 19:03:30.560338 19104 caffe.cpp:313] Batch 266, loss = 1.67788
I0815 19:03:30.608649 19104 caffe.cpp:313] Batch 267, accuracy/top1 = 0.58
I0815 19:03:30.608675 19104 caffe.cpp:313] Batch 267, accuracy/top5 = 0.88
I0815 19:03:30.608678 19104 caffe.cpp:313] Batch 267, loss = 1.4988
I0815 19:03:30.657997 19104 caffe.cpp:313] Batch 268, accuracy/top1 = 0.62
I0815 19:03:30.658022 19104 caffe.cpp:313] Batch 268, accuracy/top5 = 0.82
I0815 19:03:30.658026 19104 caffe.cpp:313] Batch 268, loss = 1.68924
I0815 19:03:30.706625 19104 caffe.cpp:313] Batch 269, accuracy/top1 = 0.66
I0815 19:03:30.706650 19104 caffe.cpp:313] Batch 269, accuracy/top5 = 0.88
I0815 19:03:30.706653 19104 caffe.cpp:313] Batch 269, loss = 1.67952
I0815 19:03:30.755496 19104 caffe.cpp:313] Batch 270, accuracy/top1 = 0.56
I0815 19:03:30.755520 19104 caffe.cpp:313] Batch 270, accuracy/top5 = 0.78
I0815 19:03:30.755525 19104 caffe.cpp:313] Batch 270, loss = 1.97958
I0815 19:03:30.804497 19104 caffe.cpp:313] Batch 271, accuracy/top1 = 0.6
I0815 19:03:30.804522 19104 caffe.cpp:313] Batch 271, accuracy/top5 = 0.76
I0815 19:03:30.804525 19104 caffe.cpp:313] Batch 271, loss = 2.20586
I0815 19:03:30.853160 19104 caffe.cpp:313] Batch 272, accuracy/top1 = 0.56
I0815 19:03:30.853184 19104 caffe.cpp:313] Batch 272, accuracy/top5 = 0.78
I0815 19:03:30.853188 19104 caffe.cpp:313] Batch 272, loss = 1.87431
I0815 19:03:30.902707 19104 caffe.cpp:313] Batch 273, accuracy/top1 = 0.66
I0815 19:03:30.902731 19104 caffe.cpp:313] Batch 273, accuracy/top5 = 0.86
I0815 19:03:30.902755 19104 caffe.cpp:313] Batch 273, loss = 1.48903
I0815 19:03:30.951409 19104 caffe.cpp:313] Batch 274, accuracy/top1 = 0.5
I0815 19:03:30.951433 19104 caffe.cpp:313] Batch 274, accuracy/top5 = 0.84
I0815 19:03:30.951437 19104 caffe.cpp:313] Batch 274, loss = 1.85547
I0815 19:03:31.000069 19104 caffe.cpp:313] Batch 275, accuracy/top1 = 0.62
I0815 19:03:31.000095 19104 caffe.cpp:313] Batch 275, accuracy/top5 = 0.84
I0815 19:03:31.000099 19104 caffe.cpp:313] Batch 275, loss = 1.68585
I0815 19:03:31.049356 19104 caffe.cpp:313] Batch 276, accuracy/top1 = 0.66
I0815 19:03:31.049376 19104 caffe.cpp:313] Batch 276, accuracy/top5 = 0.86
I0815 19:03:31.049379 19104 caffe.cpp:313] Batch 276, loss = 1.5198
I0815 19:03:31.098050 19104 caffe.cpp:313] Batch 277, accuracy/top1 = 0.62
I0815 19:03:31.098067 19104 caffe.cpp:313] Batch 277, accuracy/top5 = 0.76
I0815 19:03:31.098070 19104 caffe.cpp:313] Batch 277, loss = 1.76231
I0815 19:03:31.147982 19104 caffe.cpp:313] Batch 278, accuracy/top1 = 0.46
I0815 19:03:31.148008 19104 caffe.cpp:313] Batch 278, accuracy/top5 = 0.78
I0815 19:03:31.148012 19104 caffe.cpp:313] Batch 278, loss = 2.39187
I0815 19:03:31.196846 19104 caffe.cpp:313] Batch 279, accuracy/top1 = 0.52
I0815 19:03:31.196871 19104 caffe.cpp:313] Batch 279, accuracy/top5 = 0.78
I0815 19:03:31.196876 19104 caffe.cpp:313] Batch 279, loss = 2.34954
I0815 19:03:31.245789 19104 caffe.cpp:313] Batch 280, accuracy/top1 = 0.54
I0815 19:03:31.245813 19104 caffe.cpp:313] Batch 280, accuracy/top5 = 0.9
I0815 19:03:31.245817 19104 caffe.cpp:313] Batch 280, loss = 1.67495
I0815 19:03:31.294728 19104 caffe.cpp:313] Batch 281, accuracy/top1 = 0.62
I0815 19:03:31.294751 19104 caffe.cpp:313] Batch 281, accuracy/top5 = 0.84
I0815 19:03:31.294755 19104 caffe.cpp:313] Batch 281, loss = 1.87042
I0815 19:03:31.343583 19104 caffe.cpp:313] Batch 282, accuracy/top1 = 0.56
I0815 19:03:31.343606 19104 caffe.cpp:313] Batch 282, accuracy/top5 = 0.86
I0815 19:03:31.343611 19104 caffe.cpp:313] Batch 282, loss = 1.57315
I0815 19:03:31.391978 19104 caffe.cpp:313] Batch 283, accuracy/top1 = 0.56
I0815 19:03:31.391999 19104 caffe.cpp:313] Batch 283, accuracy/top5 = 0.78
I0815 19:03:31.392004 19104 caffe.cpp:313] Batch 283, loss = 1.99482
I0815 19:03:31.441308 19104 caffe.cpp:313] Batch 284, accuracy/top1 = 0.58
I0815 19:03:31.441331 19104 caffe.cpp:313] Batch 284, accuracy/top5 = 0.8
I0815 19:03:31.441334 19104 caffe.cpp:313] Batch 284, loss = 1.89105
I0815 19:03:31.489600 19104 caffe.cpp:313] Batch 285, accuracy/top1 = 0.58
I0815 19:03:31.489624 19104 caffe.cpp:313] Batch 285, accuracy/top5 = 0.82
I0815 19:03:31.489627 19104 caffe.cpp:313] Batch 285, loss = 1.46262
I0815 19:03:31.537362 19104 caffe.cpp:313] Batch 286, accuracy/top1 = 0.62
I0815 19:03:31.537386 19104 caffe.cpp:313] Batch 286, accuracy/top5 = 0.88
I0815 19:03:31.537390 19104 caffe.cpp:313] Batch 286, loss = 1.28375
I0815 19:03:31.585572 19104 caffe.cpp:313] Batch 287, accuracy/top1 = 0.58
I0815 19:03:31.585597 19104 caffe.cpp:313] Batch 287, accuracy/top5 = 0.78
I0815 19:03:31.585600 19104 caffe.cpp:313] Batch 287, loss = 2.0101
I0815 19:03:31.634057 19104 caffe.cpp:313] Batch 288, accuracy/top1 = 0.58
I0815 19:03:31.634083 19104 caffe.cpp:313] Batch 288, accuracy/top5 = 0.84
I0815 19:03:31.634085 19104 caffe.cpp:313] Batch 288, loss = 2.01663
I0815 19:03:31.683593 19104 caffe.cpp:313] Batch 289, accuracy/top1 = 0.56
I0815 19:03:31.683619 19104 caffe.cpp:313] Batch 289, accuracy/top5 = 0.86
I0815 19:03:31.683621 19104 caffe.cpp:313] Batch 289, loss = 1.32811
I0815 19:03:31.732465 19104 caffe.cpp:313] Batch 290, accuracy/top1 = 0.62
I0815 19:03:31.732489 19104 caffe.cpp:313] Batch 290, accuracy/top5 = 0.76
I0815 19:03:31.732491 19104 caffe.cpp:313] Batch 290, loss = 1.95122
I0815 19:03:31.781038 19104 caffe.cpp:313] Batch 291, accuracy/top1 = 0.56
I0815 19:03:31.781064 19104 caffe.cpp:313] Batch 291, accuracy/top5 = 0.78
I0815 19:03:31.781066 19104 caffe.cpp:313] Batch 291, loss = 2.06819
I0815 19:03:31.830020 19104 caffe.cpp:313] Batch 292, accuracy/top1 = 0.6
I0815 19:03:31.830039 19104 caffe.cpp:313] Batch 292, accuracy/top5 = 0.8
I0815 19:03:31.830058 19104 caffe.cpp:313] Batch 292, loss = 1.68964
I0815 19:03:31.879812 19104 caffe.cpp:313] Batch 293, accuracy/top1 = 0.7
I0815 19:03:31.879832 19104 caffe.cpp:313] Batch 293, accuracy/top5 = 0.82
I0815 19:03:31.879837 19104 caffe.cpp:313] Batch 293, loss = 1.37382
I0815 19:03:31.927273 19104 caffe.cpp:313] Batch 294, accuracy/top1 = 0.7
I0815 19:03:31.927296 19104 caffe.cpp:313] Batch 294, accuracy/top5 = 0.94
I0815 19:03:31.927299 19104 caffe.cpp:313] Batch 294, loss = 0.822007
I0815 19:03:31.976227 19104 caffe.cpp:313] Batch 295, accuracy/top1 = 0.52
I0815 19:03:31.976249 19104 caffe.cpp:313] Batch 295, accuracy/top5 = 0.74
I0815 19:03:31.976253 19104 caffe.cpp:313] Batch 295, loss = 2.056
I0815 19:03:32.024214 19104 caffe.cpp:313] Batch 296, accuracy/top1 = 0.62
I0815 19:03:32.024235 19104 caffe.cpp:313] Batch 296, accuracy/top5 = 0.88
I0815 19:03:32.024240 19104 caffe.cpp:313] Batch 296, loss = 1.46649
I0815 19:03:32.072717 19104 caffe.cpp:313] Batch 297, accuracy/top1 = 0.68
I0815 19:03:32.072736 19104 caffe.cpp:313] Batch 297, accuracy/top5 = 0.84
I0815 19:03:32.072741 19104 caffe.cpp:313] Batch 297, loss = 1.32296
I0815 19:03:32.121220 19104 caffe.cpp:313] Batch 298, accuracy/top1 = 0.52
I0815 19:03:32.121237 19104 caffe.cpp:313] Batch 298, accuracy/top5 = 0.88
I0815 19:03:32.121240 19104 caffe.cpp:313] Batch 298, loss = 1.95223
I0815 19:03:32.169492 19104 caffe.cpp:313] Batch 299, accuracy/top1 = 0.52
I0815 19:03:32.169507 19104 caffe.cpp:313] Batch 299, accuracy/top5 = 0.74
I0815 19:03:32.169512 19104 caffe.cpp:313] Batch 299, loss = 2.14923
I0815 19:03:32.218859 19104 caffe.cpp:313] Batch 300, accuracy/top1 = 0.54
I0815 19:03:32.218884 19104 caffe.cpp:313] Batch 300, accuracy/top5 = 0.74
I0815 19:03:32.218888 19104 caffe.cpp:313] Batch 300, loss = 2.10074
I0815 19:03:32.268124 19104 caffe.cpp:313] Batch 301, accuracy/top1 = 0.52
I0815 19:03:32.268151 19104 caffe.cpp:313] Batch 301, accuracy/top5 = 0.72
I0815 19:03:32.268155 19104 caffe.cpp:313] Batch 301, loss = 2.20311
I0815 19:03:32.316916 19104 caffe.cpp:313] Batch 302, accuracy/top1 = 0.7
I0815 19:03:32.316942 19104 caffe.cpp:313] Batch 302, accuracy/top5 = 0.88
I0815 19:03:32.316946 19104 caffe.cpp:313] Batch 302, loss = 1.39995
I0815 19:03:32.366194 19104 caffe.cpp:313] Batch 303, accuracy/top1 = 0.52
I0815 19:03:32.366216 19104 caffe.cpp:313] Batch 303, accuracy/top5 = 0.88
I0815 19:03:32.366220 19104 caffe.cpp:313] Batch 303, loss = 1.54783
I0815 19:03:32.415383 19104 caffe.cpp:313] Batch 304, accuracy/top1 = 0.58
I0815 19:03:32.415408 19104 caffe.cpp:313] Batch 304, accuracy/top5 = 0.84
I0815 19:03:32.415411 19104 caffe.cpp:313] Batch 304, loss = 1.46384
I0815 19:03:32.464824 19104 caffe.cpp:313] Batch 305, accuracy/top1 = 0.64
I0815 19:03:32.464848 19104 caffe.cpp:313] Batch 305, accuracy/top5 = 0.9
I0815 19:03:32.464853 19104 caffe.cpp:313] Batch 305, loss = 1.28912
I0815 19:03:32.512786 19104 caffe.cpp:313] Batch 306, accuracy/top1 = 0.62
I0815 19:03:32.512811 19104 caffe.cpp:313] Batch 306, accuracy/top5 = 0.88
I0815 19:03:32.512814 19104 caffe.cpp:313] Batch 306, loss = 1.36265
I0815 19:03:32.560910 19104 caffe.cpp:313] Batch 307, accuracy/top1 = 0.66
I0815 19:03:32.560935 19104 caffe.cpp:313] Batch 307, accuracy/top5 = 0.84
I0815 19:03:32.560938 19104 caffe.cpp:313] Batch 307, loss = 1.71973
I0815 19:03:32.608556 19104 caffe.cpp:313] Batch 308, accuracy/top1 = 0.76
I0815 19:03:32.608580 19104 caffe.cpp:313] Batch 308, accuracy/top5 = 0.86
I0815 19:03:32.608583 19104 caffe.cpp:313] Batch 308, loss = 1.41306
I0815 19:03:32.657997 19104 caffe.cpp:313] Batch 309, accuracy/top1 = 0.54
I0815 19:03:32.658021 19104 caffe.cpp:313] Batch 309, accuracy/top5 = 0.8
I0815 19:03:32.658025 19104 caffe.cpp:313] Batch 309, loss = 2.06557
I0815 19:03:32.707546 19104 caffe.cpp:313] Batch 310, accuracy/top1 = 0.7
I0815 19:03:32.707571 19104 caffe.cpp:313] Batch 310, accuracy/top5 = 0.86
I0815 19:03:32.707573 19104 caffe.cpp:313] Batch 310, loss = 1.20091
I0815 19:03:32.755204 19104 caffe.cpp:313] Batch 311, accuracy/top1 = 0.54
I0815 19:03:32.755246 19104 caffe.cpp:313] Batch 311, accuracy/top5 = 0.8
I0815 19:03:32.755250 19104 caffe.cpp:313] Batch 311, loss = 2.31066
I0815 19:03:32.804724 19104 caffe.cpp:313] Batch 312, accuracy/top1 = 0.62
I0815 19:03:32.804749 19104 caffe.cpp:313] Batch 312, accuracy/top5 = 0.82
I0815 19:03:32.804751 19104 caffe.cpp:313] Batch 312, loss = 1.785
I0815 19:03:32.853577 19104 caffe.cpp:313] Batch 313, accuracy/top1 = 0.66
I0815 19:03:32.853603 19104 caffe.cpp:313] Batch 313, accuracy/top5 = 0.88
I0815 19:03:32.853607 19104 caffe.cpp:313] Batch 313, loss = 1.55695
I0815 19:03:32.902534 19104 caffe.cpp:313] Batch 314, accuracy/top1 = 0.7
I0815 19:03:32.902559 19104 caffe.cpp:313] Batch 314, accuracy/top5 = 0.88
I0815 19:03:32.902562 19104 caffe.cpp:313] Batch 314, loss = 1.27381
I0815 19:03:32.950606 19104 caffe.cpp:313] Batch 315, accuracy/top1 = 0.6
I0815 19:03:32.950631 19104 caffe.cpp:313] Batch 315, accuracy/top5 = 0.74
I0815 19:03:32.950634 19104 caffe.cpp:313] Batch 315, loss = 1.76849
I0815 19:03:32.999119 19104 caffe.cpp:313] Batch 316, accuracy/top1 = 0.5
I0815 19:03:32.999143 19104 caffe.cpp:313] Batch 316, accuracy/top5 = 0.76
I0815 19:03:32.999146 19104 caffe.cpp:313] Batch 316, loss = 2.23436
I0815 19:03:33.047052 19104 caffe.cpp:313] Batch 317, accuracy/top1 = 0.62
I0815 19:03:33.047070 19104 caffe.cpp:313] Batch 317, accuracy/top5 = 0.82
I0815 19:03:33.047072 19104 caffe.cpp:313] Batch 317, loss = 1.7432
I0815 19:03:33.094666 19104 caffe.cpp:313] Batch 318, accuracy/top1 = 0.66
I0815 19:03:33.094681 19104 caffe.cpp:313] Batch 318, accuracy/top5 = 0.82
I0815 19:03:33.094683 19104 caffe.cpp:313] Batch 318, loss = 1.50595
I0815 19:03:33.143301 19104 caffe.cpp:313] Batch 319, accuracy/top1 = 0.56
I0815 19:03:33.143314 19104 caffe.cpp:313] Batch 319, accuracy/top5 = 0.82
I0815 19:03:33.143317 19104 caffe.cpp:313] Batch 319, loss = 1.7908
I0815 19:03:33.191751 19104 caffe.cpp:313] Batch 320, accuracy/top1 = 0.52
I0815 19:03:33.191772 19104 caffe.cpp:313] Batch 320, accuracy/top5 = 0.76
I0815 19:03:33.191776 19104 caffe.cpp:313] Batch 320, loss = 2.21334
I0815 19:03:33.241274 19104 caffe.cpp:313] Batch 321, accuracy/top1 = 0.58
I0815 19:03:33.241299 19104 caffe.cpp:313] Batch 321, accuracy/top5 = 0.82
I0815 19:03:33.241302 19104 caffe.cpp:313] Batch 321, loss = 1.92259
I0815 19:03:33.290362 19104 caffe.cpp:313] Batch 322, accuracy/top1 = 0.62
I0815 19:03:33.290387 19104 caffe.cpp:313] Batch 322, accuracy/top5 = 0.94
I0815 19:03:33.290390 19104 caffe.cpp:313] Batch 322, loss = 1.15931
I0815 19:03:33.339645 19104 caffe.cpp:313] Batch 323, accuracy/top1 = 0.6
I0815 19:03:33.339670 19104 caffe.cpp:313] Batch 323, accuracy/top5 = 0.8
I0815 19:03:33.339673 19104 caffe.cpp:313] Batch 323, loss = 1.92089
I0815 19:03:33.389327 19104 caffe.cpp:313] Batch 324, accuracy/top1 = 0.5
I0815 19:03:33.389348 19104 caffe.cpp:313] Batch 324, accuracy/top5 = 0.68
I0815 19:03:33.389351 19104 caffe.cpp:313] Batch 324, loss = 2.07425
I0815 19:03:33.438858 19104 caffe.cpp:313] Batch 325, accuracy/top1 = 0.58
I0815 19:03:33.438886 19104 caffe.cpp:313] Batch 325, accuracy/top5 = 0.76
I0815 19:03:33.438892 19104 caffe.cpp:313] Batch 325, loss = 1.92488
I0815 19:03:33.486410 19104 caffe.cpp:313] Batch 326, accuracy/top1 = 0.6
I0815 19:03:33.486433 19104 caffe.cpp:313] Batch 326, accuracy/top5 = 0.84
I0815 19:03:33.486436 19104 caffe.cpp:313] Batch 326, loss = 1.90771
I0815 19:03:33.535306 19104 caffe.cpp:313] Batch 327, accuracy/top1 = 0.6
I0815 19:03:33.535331 19104 caffe.cpp:313] Batch 327, accuracy/top5 = 0.72
I0815 19:03:33.535333 19104 caffe.cpp:313] Batch 327, loss = 2.04402
I0815 19:03:33.583539 19104 caffe.cpp:313] Batch 328, accuracy/top1 = 0.42
I0815 19:03:33.583564 19104 caffe.cpp:313] Batch 328, accuracy/top5 = 0.86
I0815 19:03:33.583566 19104 caffe.cpp:313] Batch 328, loss = 1.89567
I0815 19:03:33.631765 19104 caffe.cpp:313] Batch 329, accuracy/top1 = 0.56
I0815 19:03:33.631789 19104 caffe.cpp:313] Batch 329, accuracy/top5 = 0.82
I0815 19:03:33.631793 19104 caffe.cpp:313] Batch 329, loss = 1.63578
I0815 19:03:33.678675 19104 caffe.cpp:313] Batch 330, accuracy/top1 = 0.56
I0815 19:03:33.678700 19104 caffe.cpp:313] Batch 330, accuracy/top5 = 0.84
I0815 19:03:33.678704 19104 caffe.cpp:313] Batch 330, loss = 1.79845
I0815 19:03:33.727087 19104 caffe.cpp:313] Batch 331, accuracy/top1 = 0.48
I0815 19:03:33.727110 19104 caffe.cpp:313] Batch 331, accuracy/top5 = 0.84
I0815 19:03:33.727113 19104 caffe.cpp:313] Batch 331, loss = 1.9989
I0815 19:03:33.775895 19104 caffe.cpp:313] Batch 332, accuracy/top1 = 0.64
I0815 19:03:33.775919 19104 caffe.cpp:313] Batch 332, accuracy/top5 = 0.84
I0815 19:03:33.775923 19104 caffe.cpp:313] Batch 332, loss = 1.67016
I0815 19:03:33.825021 19104 caffe.cpp:313] Batch 333, accuracy/top1 = 0.58
I0815 19:03:33.825044 19104 caffe.cpp:313] Batch 333, accuracy/top5 = 0.82
I0815 19:03:33.825047 19104 caffe.cpp:313] Batch 333, loss = 1.63254
I0815 19:03:33.873365 19104 caffe.cpp:313] Batch 334, accuracy/top1 = 0.74
I0815 19:03:33.873389 19104 caffe.cpp:313] Batch 334, accuracy/top5 = 0.92
I0815 19:03:33.873392 19104 caffe.cpp:313] Batch 334, loss = 1.17895
I0815 19:03:33.922906 19104 caffe.cpp:313] Batch 335, accuracy/top1 = 0.52
I0815 19:03:33.922932 19104 caffe.cpp:313] Batch 335, accuracy/top5 = 0.74
I0815 19:03:33.922936 19104 caffe.cpp:313] Batch 335, loss = 2.14293
I0815 19:03:33.972290 19104 caffe.cpp:313] Batch 336, accuracy/top1 = 0.7
I0815 19:03:33.972316 19104 caffe.cpp:313] Batch 336, accuracy/top5 = 0.88
I0815 19:03:33.972319 19104 caffe.cpp:313] Batch 336, loss = 1.27531
I0815 19:03:34.021383 19104 caffe.cpp:313] Batch 337, accuracy/top1 = 0.7
I0815 19:03:34.021407 19104 caffe.cpp:313] Batch 337, accuracy/top5 = 0.88
I0815 19:03:34.021412 19104 caffe.cpp:313] Batch 337, loss = 1.40648
I0815 19:03:34.069864 19104 caffe.cpp:313] Batch 338, accuracy/top1 = 0.64
I0815 19:03:34.069885 19104 caffe.cpp:313] Batch 338, accuracy/top5 = 0.78
I0815 19:03:34.069888 19104 caffe.cpp:313] Batch 338, loss = 1.6902
I0815 19:03:34.117561 19104 caffe.cpp:313] Batch 339, accuracy/top1 = 0.48
I0815 19:03:34.117576 19104 caffe.cpp:313] Batch 339, accuracy/top5 = 0.82
I0815 19:03:34.117580 19104 caffe.cpp:313] Batch 339, loss = 1.91049
I0815 19:03:34.166060 19104 caffe.cpp:313] Batch 340, accuracy/top1 = 0.56
I0815 19:03:34.166075 19104 caffe.cpp:313] Batch 340, accuracy/top5 = 0.8
I0815 19:03:34.166079 19104 caffe.cpp:313] Batch 340, loss = 2.24124
I0815 19:03:34.215658 19104 caffe.cpp:313] Batch 341, accuracy/top1 = 0.58
I0815 19:03:34.215683 19104 caffe.cpp:313] Batch 341, accuracy/top5 = 0.76
I0815 19:03:34.215687 19104 caffe.cpp:313] Batch 341, loss = 1.66688
I0815 19:03:34.263761 19104 caffe.cpp:313] Batch 342, accuracy/top1 = 0.62
I0815 19:03:34.263785 19104 caffe.cpp:313] Batch 342, accuracy/top5 = 0.86
I0815 19:03:34.263789 19104 caffe.cpp:313] Batch 342, loss = 1.56009
I0815 19:03:34.312870 19104 caffe.cpp:313] Batch 343, accuracy/top1 = 0.52
I0815 19:03:34.312896 19104 caffe.cpp:313] Batch 343, accuracy/top5 = 0.8
I0815 19:03:34.312899 19104 caffe.cpp:313] Batch 343, loss = 1.72143
I0815 19:03:34.362071 19104 caffe.cpp:313] Batch 344, accuracy/top1 = 0.58
I0815 19:03:34.362098 19104 caffe.cpp:313] Batch 344, accuracy/top5 = 0.86
I0815 19:03:34.362102 19104 caffe.cpp:313] Batch 344, loss = 1.3729
I0815 19:03:34.411820 19104 caffe.cpp:313] Batch 345, accuracy/top1 = 0.56
I0815 19:03:34.411841 19104 caffe.cpp:313] Batch 345, accuracy/top5 = 0.86
I0815 19:03:34.411844 19104 caffe.cpp:313] Batch 345, loss = 1.72418
I0815 19:03:34.460021 19104 caffe.cpp:313] Batch 346, accuracy/top1 = 0.48
I0815 19:03:34.460047 19104 caffe.cpp:313] Batch 346, accuracy/top5 = 0.72
I0815 19:03:34.460052 19104 caffe.cpp:313] Batch 346, loss = 2.23184
I0815 19:03:34.508648 19104 caffe.cpp:313] Batch 347, accuracy/top1 = 0.58
I0815 19:03:34.508673 19104 caffe.cpp:313] Batch 347, accuracy/top5 = 0.88
I0815 19:03:34.508677 19104 caffe.cpp:313] Batch 347, loss = 1.54022
I0815 19:03:34.557804 19104 caffe.cpp:313] Batch 348, accuracy/top1 = 0.68
I0815 19:03:34.557832 19104 caffe.cpp:313] Batch 348, accuracy/top5 = 0.86
I0815 19:03:34.557855 19104 caffe.cpp:313] Batch 348, loss = 1.30392
I0815 19:03:34.606350 19104 caffe.cpp:313] Batch 349, accuracy/top1 = 0.58
I0815 19:03:34.606374 19104 caffe.cpp:313] Batch 349, accuracy/top5 = 0.8
I0815 19:03:34.606379 19104 caffe.cpp:313] Batch 349, loss = 2.1546
I0815 19:03:34.654924 19104 caffe.cpp:313] Batch 350, accuracy/top1 = 0.72
I0815 19:03:34.654949 19104 caffe.cpp:313] Batch 350, accuracy/top5 = 0.84
I0815 19:03:34.654954 19104 caffe.cpp:313] Batch 350, loss = 1.46156
I0815 19:03:34.703348 19104 caffe.cpp:313] Batch 351, accuracy/top1 = 0.66
I0815 19:03:34.703373 19104 caffe.cpp:313] Batch 351, accuracy/top5 = 0.82
I0815 19:03:34.703377 19104 caffe.cpp:313] Batch 351, loss = 1.51408
I0815 19:03:34.752475 19104 caffe.cpp:313] Batch 352, accuracy/top1 = 0.6
I0815 19:03:34.752501 19104 caffe.cpp:313] Batch 352, accuracy/top5 = 0.76
I0815 19:03:34.752506 19104 caffe.cpp:313] Batch 352, loss = 2.38468
I0815 19:03:34.801630 19104 caffe.cpp:313] Batch 353, accuracy/top1 = 0.58
I0815 19:03:34.801656 19104 caffe.cpp:313] Batch 353, accuracy/top5 = 0.84
I0815 19:03:34.801659 19104 caffe.cpp:313] Batch 353, loss = 1.875
I0815 19:03:34.850739 19104 caffe.cpp:313] Batch 354, accuracy/top1 = 0.56
I0815 19:03:34.850765 19104 caffe.cpp:313] Batch 354, accuracy/top5 = 0.78
I0815 19:03:34.850770 19104 caffe.cpp:313] Batch 354, loss = 2.26948
I0815 19:03:34.899672 19104 caffe.cpp:313] Batch 355, accuracy/top1 = 0.6
I0815 19:03:34.899696 19104 caffe.cpp:313] Batch 355, accuracy/top5 = 0.86
I0815 19:03:34.899700 19104 caffe.cpp:313] Batch 355, loss = 1.83711
I0815 19:03:34.948762 19104 caffe.cpp:313] Batch 356, accuracy/top1 = 0.6
I0815 19:03:34.948788 19104 caffe.cpp:313] Batch 356, accuracy/top5 = 0.76
I0815 19:03:34.948792 19104 caffe.cpp:313] Batch 356, loss = 1.74325
I0815 19:03:34.997381 19104 caffe.cpp:313] Batch 357, accuracy/top1 = 0.66
I0815 19:03:34.997406 19104 caffe.cpp:313] Batch 357, accuracy/top5 = 0.88
I0815 19:03:34.997411 19104 caffe.cpp:313] Batch 357, loss = 1.4811
I0815 19:03:35.045368 19104 caffe.cpp:313] Batch 358, accuracy/top1 = 0.6
I0815 19:03:35.045387 19104 caffe.cpp:313] Batch 358, accuracy/top5 = 0.76
I0815 19:03:35.045392 19104 caffe.cpp:313] Batch 358, loss = 2.12936
I0815 19:03:35.095125 19104 caffe.cpp:313] Batch 359, accuracy/top1 = 0.5
I0815 19:03:35.095141 19104 caffe.cpp:313] Batch 359, accuracy/top5 = 0.8
I0815 19:03:35.095145 19104 caffe.cpp:313] Batch 359, loss = 2.24129
I0815 19:03:35.143611 19104 caffe.cpp:313] Batch 360, accuracy/top1 = 0.66
I0815 19:03:35.143625 19104 caffe.cpp:313] Batch 360, accuracy/top5 = 0.84
I0815 19:03:35.143628 19104 caffe.cpp:313] Batch 360, loss = 1.88032
I0815 19:03:35.192370 19104 caffe.cpp:313] Batch 361, accuracy/top1 = 0.56
I0815 19:03:35.192394 19104 caffe.cpp:313] Batch 361, accuracy/top5 = 0.86
I0815 19:03:35.192397 19104 caffe.cpp:313] Batch 361, loss = 1.51585
I0815 19:03:35.241852 19104 caffe.cpp:313] Batch 362, accuracy/top1 = 0.6
I0815 19:03:35.241878 19104 caffe.cpp:313] Batch 362, accuracy/top5 = 0.78
I0815 19:03:35.241881 19104 caffe.cpp:313] Batch 362, loss = 1.96707
I0815 19:03:35.290341 19104 caffe.cpp:313] Batch 363, accuracy/top1 = 0.6
I0815 19:03:35.290367 19104 caffe.cpp:313] Batch 363, accuracy/top5 = 0.84
I0815 19:03:35.290371 19104 caffe.cpp:313] Batch 363, loss = 1.86373
I0815 19:03:35.340030 19104 caffe.cpp:313] Batch 364, accuracy/top1 = 0.64
I0815 19:03:35.340054 19104 caffe.cpp:313] Batch 364, accuracy/top5 = 0.82
I0815 19:03:35.340059 19104 caffe.cpp:313] Batch 364, loss = 1.48646
I0815 19:03:35.390817 19104 caffe.cpp:313] Batch 365, accuracy/top1 = 0.66
I0815 19:03:35.390837 19104 caffe.cpp:313] Batch 365, accuracy/top5 = 0.82
I0815 19:03:35.390842 19104 caffe.cpp:313] Batch 365, loss = 1.51455
I0815 19:03:35.441046 19104 caffe.cpp:313] Batch 366, accuracy/top1 = 0.58
I0815 19:03:35.441071 19104 caffe.cpp:313] Batch 366, accuracy/top5 = 0.82
I0815 19:03:35.441076 19104 caffe.cpp:313] Batch 366, loss = 1.85894
I0815 19:03:35.490252 19104 caffe.cpp:313] Batch 367, accuracy/top1 = 0.64
I0815 19:03:35.490278 19104 caffe.cpp:313] Batch 367, accuracy/top5 = 0.84
I0815 19:03:35.490298 19104 caffe.cpp:313] Batch 367, loss = 1.64288
I0815 19:03:35.539475 19104 caffe.cpp:313] Batch 368, accuracy/top1 = 0.54
I0815 19:03:35.539497 19104 caffe.cpp:313] Batch 368, accuracy/top5 = 0.82
I0815 19:03:35.539502 19104 caffe.cpp:313] Batch 368, loss = 1.74349
I0815 19:03:35.588255 19104 caffe.cpp:313] Batch 369, accuracy/top1 = 0.68
I0815 19:03:35.588277 19104 caffe.cpp:313] Batch 369, accuracy/top5 = 0.88
I0815 19:03:35.588282 19104 caffe.cpp:313] Batch 369, loss = 1.46346
I0815 19:03:35.635781 19104 caffe.cpp:313] Batch 370, accuracy/top1 = 0.72
I0815 19:03:35.635804 19104 caffe.cpp:313] Batch 370, accuracy/top5 = 0.82
I0815 19:03:35.635808 19104 caffe.cpp:313] Batch 370, loss = 1.52775
I0815 19:03:35.684383 19104 caffe.cpp:313] Batch 371, accuracy/top1 = 0.72
I0815 19:03:35.684408 19104 caffe.cpp:313] Batch 371, accuracy/top5 = 0.8
I0815 19:03:35.684413 19104 caffe.cpp:313] Batch 371, loss = 1.55048
I0815 19:03:35.733122 19104 caffe.cpp:313] Batch 372, accuracy/top1 = 0.68
I0815 19:03:35.733146 19104 caffe.cpp:313] Batch 372, accuracy/top5 = 0.9
I0815 19:03:35.733150 19104 caffe.cpp:313] Batch 372, loss = 1.40059
I0815 19:03:35.780936 19104 caffe.cpp:313] Batch 373, accuracy/top1 = 0.5
I0815 19:03:35.780959 19104 caffe.cpp:313] Batch 373, accuracy/top5 = 0.78
I0815 19:03:35.780964 19104 caffe.cpp:313] Batch 373, loss = 2.45936
I0815 19:03:35.829447 19104 caffe.cpp:313] Batch 374, accuracy/top1 = 0.62
I0815 19:03:35.829470 19104 caffe.cpp:313] Batch 374, accuracy/top5 = 0.8
I0815 19:03:35.829474 19104 caffe.cpp:313] Batch 374, loss = 1.67501
I0815 19:03:35.877779 19104 caffe.cpp:313] Batch 375, accuracy/top1 = 0.72
I0815 19:03:35.877804 19104 caffe.cpp:313] Batch 375, accuracy/top5 = 0.92
I0815 19:03:35.877807 19104 caffe.cpp:313] Batch 375, loss = 1.12366
I0815 19:03:35.926506 19104 caffe.cpp:313] Batch 376, accuracy/top1 = 0.54
I0815 19:03:35.926528 19104 caffe.cpp:313] Batch 376, accuracy/top5 = 0.88
I0815 19:03:35.926532 19104 caffe.cpp:313] Batch 376, loss = 1.46213
I0815 19:03:35.975581 19104 caffe.cpp:313] Batch 377, accuracy/top1 = 0.5
I0815 19:03:35.975606 19104 caffe.cpp:313] Batch 377, accuracy/top5 = 0.8
I0815 19:03:35.975610 19104 caffe.cpp:313] Batch 377, loss = 1.80365
I0815 19:03:36.024353 19104 caffe.cpp:313] Batch 378, accuracy/top1 = 0.72
I0815 19:03:36.024374 19104 caffe.cpp:313] Batch 378, accuracy/top5 = 0.88
I0815 19:03:36.024379 19104 caffe.cpp:313] Batch 378, loss = 1.48613
I0815 19:03:36.072391 19104 caffe.cpp:313] Batch 379, accuracy/top1 = 0.72
I0815 19:03:36.072410 19104 caffe.cpp:313] Batch 379, accuracy/top5 = 0.92
I0815 19:03:36.072414 19104 caffe.cpp:313] Batch 379, loss = 1.27613
I0815 19:03:36.120522 19104 caffe.cpp:313] Batch 380, accuracy/top1 = 0.58
I0815 19:03:36.120537 19104 caffe.cpp:313] Batch 380, accuracy/top5 = 0.86
I0815 19:03:36.120542 19104 caffe.cpp:313] Batch 380, loss = 1.64242
I0815 19:03:36.168965 19104 caffe.cpp:313] Batch 381, accuracy/top1 = 0.46
I0815 19:03:36.168980 19104 caffe.cpp:313] Batch 381, accuracy/top5 = 0.7
I0815 19:03:36.168985 19104 caffe.cpp:313] Batch 381, loss = 2.44388
I0815 19:03:36.218610 19104 caffe.cpp:313] Batch 382, accuracy/top1 = 0.6
I0815 19:03:36.218634 19104 caffe.cpp:313] Batch 382, accuracy/top5 = 0.72
I0815 19:03:36.218638 19104 caffe.cpp:313] Batch 382, loss = 2.10349
I0815 19:03:36.267210 19104 caffe.cpp:313] Batch 383, accuracy/top1 = 0.66
I0815 19:03:36.267235 19104 caffe.cpp:313] Batch 383, accuracy/top5 = 0.86
I0815 19:03:36.267240 19104 caffe.cpp:313] Batch 383, loss = 1.56229
I0815 19:03:36.315770 19104 caffe.cpp:313] Batch 384, accuracy/top1 = 0.62
I0815 19:03:36.315794 19104 caffe.cpp:313] Batch 384, accuracy/top5 = 0.86
I0815 19:03:36.315799 19104 caffe.cpp:313] Batch 384, loss = 1.57075
I0815 19:03:36.365644 19104 caffe.cpp:313] Batch 385, accuracy/top1 = 0.64
I0815 19:03:36.365669 19104 caffe.cpp:313] Batch 385, accuracy/top5 = 0.82
I0815 19:03:36.365674 19104 caffe.cpp:313] Batch 385, loss = 1.51646
I0815 19:03:36.415808 19104 caffe.cpp:313] Batch 386, accuracy/top1 = 0.66
I0815 19:03:36.415848 19104 caffe.cpp:313] Batch 386, accuracy/top5 = 0.82
I0815 19:03:36.415853 19104 caffe.cpp:313] Batch 386, loss = 1.66583
I0815 19:03:36.463989 19104 caffe.cpp:313] Batch 387, accuracy/top1 = 0.52
I0815 19:03:36.464012 19104 caffe.cpp:313] Batch 387, accuracy/top5 = 0.78
I0815 19:03:36.464016 19104 caffe.cpp:313] Batch 387, loss = 1.64881
I0815 19:03:36.512250 19104 caffe.cpp:313] Batch 388, accuracy/top1 = 0.56
I0815 19:03:36.512276 19104 caffe.cpp:313] Batch 388, accuracy/top5 = 0.88
I0815 19:03:36.512280 19104 caffe.cpp:313] Batch 388, loss = 1.37011
I0815 19:03:36.560194 19104 caffe.cpp:313] Batch 389, accuracy/top1 = 0.58
I0815 19:03:36.560220 19104 caffe.cpp:313] Batch 389, accuracy/top5 = 0.72
I0815 19:03:36.560225 19104 caffe.cpp:313] Batch 389, loss = 2.03961
I0815 19:03:36.608798 19104 caffe.cpp:313] Batch 390, accuracy/top1 = 0.6
I0815 19:03:36.608824 19104 caffe.cpp:313] Batch 390, accuracy/top5 = 0.9
I0815 19:03:36.608827 19104 caffe.cpp:313] Batch 390, loss = 1.47601
I0815 19:03:36.657541 19104 caffe.cpp:313] Batch 391, accuracy/top1 = 0.62
I0815 19:03:36.657564 19104 caffe.cpp:313] Batch 391, accuracy/top5 = 0.82
I0815 19:03:36.657568 19104 caffe.cpp:313] Batch 391, loss = 1.81013
I0815 19:03:36.706096 19104 caffe.cpp:313] Batch 392, accuracy/top1 = 0.6
I0815 19:03:36.706121 19104 caffe.cpp:313] Batch 392, accuracy/top5 = 0.86
I0815 19:03:36.706125 19104 caffe.cpp:313] Batch 392, loss = 1.4713
I0815 19:03:36.755117 19104 caffe.cpp:313] Batch 393, accuracy/top1 = 0.6
I0815 19:03:36.755142 19104 caffe.cpp:313] Batch 393, accuracy/top5 = 0.76
I0815 19:03:36.755146 19104 caffe.cpp:313] Batch 393, loss = 2.11406
I0815 19:03:36.803856 19104 caffe.cpp:313] Batch 394, accuracy/top1 = 0.66
I0815 19:03:36.803880 19104 caffe.cpp:313] Batch 394, accuracy/top5 = 0.86
I0815 19:03:36.803884 19104 caffe.cpp:313] Batch 394, loss = 1.43563
I0815 19:03:36.852738 19104 caffe.cpp:313] Batch 395, accuracy/top1 = 0.74
I0815 19:03:36.852762 19104 caffe.cpp:313] Batch 395, accuracy/top5 = 0.88
I0815 19:03:36.852766 19104 caffe.cpp:313] Batch 395, loss = 1.44175
I0815 19:03:36.901409 19104 caffe.cpp:313] Batch 396, accuracy/top1 = 0.58
I0815 19:03:36.901434 19104 caffe.cpp:313] Batch 396, accuracy/top5 = 0.82
I0815 19:03:36.901438 19104 caffe.cpp:313] Batch 396, loss = 1.7999
I0815 19:03:36.951125 19104 caffe.cpp:313] Batch 397, accuracy/top1 = 0.48
I0815 19:03:36.951150 19104 caffe.cpp:313] Batch 397, accuracy/top5 = 0.76
I0815 19:03:36.951154 19104 caffe.cpp:313] Batch 397, loss = 2.38507
I0815 19:03:36.999464 19104 caffe.cpp:313] Batch 398, accuracy/top1 = 0.62
I0815 19:03:36.999490 19104 caffe.cpp:313] Batch 398, accuracy/top5 = 0.8
I0815 19:03:36.999493 19104 caffe.cpp:313] Batch 398, loss = 1.87008
I0815 19:03:37.047597 19104 caffe.cpp:313] Batch 399, accuracy/top1 = 0.6
I0815 19:03:37.047617 19104 caffe.cpp:313] Batch 399, accuracy/top5 = 0.84
I0815 19:03:37.047621 19104 caffe.cpp:313] Batch 399, loss = 1.67948
I0815 19:03:37.096524 19104 caffe.cpp:313] Batch 400, accuracy/top1 = 0.54
I0815 19:03:37.096539 19104 caffe.cpp:313] Batch 400, accuracy/top5 = 0.8
I0815 19:03:37.096544 19104 caffe.cpp:313] Batch 400, loss = 2.05511
I0815 19:03:37.145121 19104 caffe.cpp:313] Batch 401, accuracy/top1 = 0.62
I0815 19:03:37.145138 19104 caffe.cpp:313] Batch 401, accuracy/top5 = 0.9
I0815 19:03:37.145143 19104 caffe.cpp:313] Batch 401, loss = 1.38384
I0815 19:03:37.193938 19104 caffe.cpp:313] Batch 402, accuracy/top1 = 0.62
I0815 19:03:37.193965 19104 caffe.cpp:313] Batch 402, accuracy/top5 = 0.78
I0815 19:03:37.193971 19104 caffe.cpp:313] Batch 402, loss = 1.90241
I0815 19:03:37.243366 19104 caffe.cpp:313] Batch 403, accuracy/top1 = 0.58
I0815 19:03:37.243393 19104 caffe.cpp:313] Batch 403, accuracy/top5 = 0.8
I0815 19:03:37.243398 19104 caffe.cpp:313] Batch 403, loss = 1.79493
I0815 19:03:37.292731 19104 caffe.cpp:313] Batch 404, accuracy/top1 = 0.68
I0815 19:03:37.292754 19104 caffe.cpp:313] Batch 404, accuracy/top5 = 0.88
I0815 19:03:37.292757 19104 caffe.cpp:313] Batch 404, loss = 1.34549
I0815 19:03:37.342154 19104 caffe.cpp:313] Batch 405, accuracy/top1 = 0.58
I0815 19:03:37.342175 19104 caffe.cpp:313] Batch 405, accuracy/top5 = 0.88
I0815 19:03:37.342180 19104 caffe.cpp:313] Batch 405, loss = 1.67328
I0815 19:03:37.392642 19104 caffe.cpp:313] Batch 406, accuracy/top1 = 0.68
I0815 19:03:37.392662 19104 caffe.cpp:313] Batch 406, accuracy/top5 = 0.9
I0815 19:03:37.392664 19104 caffe.cpp:313] Batch 406, loss = 1.45697
I0815 19:03:37.441572 19104 caffe.cpp:313] Batch 407, accuracy/top1 = 0.76
I0815 19:03:37.441596 19104 caffe.cpp:313] Batch 407, accuracy/top5 = 0.9
I0815 19:03:37.441598 19104 caffe.cpp:313] Batch 407, loss = 1.11418
I0815 19:03:37.489249 19104 caffe.cpp:313] Batch 408, accuracy/top1 = 0.52
I0815 19:03:37.489274 19104 caffe.cpp:313] Batch 408, accuracy/top5 = 0.78
I0815 19:03:37.489276 19104 caffe.cpp:313] Batch 408, loss = 2.18693
I0815 19:03:37.537652 19104 caffe.cpp:313] Batch 409, accuracy/top1 = 0.54
I0815 19:03:37.537677 19104 caffe.cpp:313] Batch 409, accuracy/top5 = 0.82
I0815 19:03:37.537679 19104 caffe.cpp:313] Batch 409, loss = 1.94915
I0815 19:03:37.585477 19104 caffe.cpp:313] Batch 410, accuracy/top1 = 0.56
I0815 19:03:37.585502 19104 caffe.cpp:313] Batch 410, accuracy/top5 = 0.78
I0815 19:03:37.585505 19104 caffe.cpp:313] Batch 410, loss = 1.96771
I0815 19:03:37.634582 19104 caffe.cpp:313] Batch 411, accuracy/top1 = 0.66
I0815 19:03:37.634605 19104 caffe.cpp:313] Batch 411, accuracy/top5 = 0.82
I0815 19:03:37.634609 19104 caffe.cpp:313] Batch 411, loss = 1.61488
I0815 19:03:37.682474 19104 caffe.cpp:313] Batch 412, accuracy/top1 = 0.66
I0815 19:03:37.682497 19104 caffe.cpp:313] Batch 412, accuracy/top5 = 0.88
I0815 19:03:37.682502 19104 caffe.cpp:313] Batch 412, loss = 1.34091
I0815 19:03:37.731277 19104 caffe.cpp:313] Batch 413, accuracy/top1 = 0.56
I0815 19:03:37.731299 19104 caffe.cpp:313] Batch 413, accuracy/top5 = 0.78
I0815 19:03:37.731302 19104 caffe.cpp:313] Batch 413, loss = 1.93393
I0815 19:03:37.780689 19104 caffe.cpp:313] Batch 414, accuracy/top1 = 0.5
I0815 19:03:37.780714 19104 caffe.cpp:313] Batch 414, accuracy/top5 = 0.8
I0815 19:03:37.780716 19104 caffe.cpp:313] Batch 414, loss = 1.95314
I0815 19:03:37.828224 19104 caffe.cpp:313] Batch 415, accuracy/top1 = 0.66
I0815 19:03:37.828248 19104 caffe.cpp:313] Batch 415, accuracy/top5 = 0.8
I0815 19:03:37.828251 19104 caffe.cpp:313] Batch 415, loss = 1.65035
I0815 19:03:37.875625 19104 caffe.cpp:313] Batch 416, accuracy/top1 = 0.48
I0815 19:03:37.875650 19104 caffe.cpp:313] Batch 416, accuracy/top5 = 0.8
I0815 19:03:37.875653 19104 caffe.cpp:313] Batch 416, loss = 2.06754
I0815 19:03:37.925436 19104 caffe.cpp:313] Batch 417, accuracy/top1 = 0.64
I0815 19:03:37.925456 19104 caffe.cpp:313] Batch 417, accuracy/top5 = 0.78
I0815 19:03:37.925458 19104 caffe.cpp:313] Batch 417, loss = 1.85389
I0815 19:03:37.975159 19104 caffe.cpp:313] Batch 418, accuracy/top1 = 0.54
I0815 19:03:37.975183 19104 caffe.cpp:313] Batch 418, accuracy/top5 = 0.84
I0815 19:03:37.975186 19104 caffe.cpp:313] Batch 418, loss = 1.63258
I0815 19:03:38.023918 19104 caffe.cpp:313] Batch 419, accuracy/top1 = 0.58
I0815 19:03:38.023938 19104 caffe.cpp:313] Batch 419, accuracy/top5 = 0.78
I0815 19:03:38.023941 19104 caffe.cpp:313] Batch 419, loss = 1.95403
I0815 19:03:38.072492 19104 caffe.cpp:313] Batch 420, accuracy/top1 = 0.54
I0815 19:03:38.072510 19104 caffe.cpp:313] Batch 420, accuracy/top5 = 0.84
I0815 19:03:38.072512 19104 caffe.cpp:313] Batch 420, loss = 1.70335
I0815 19:03:38.121156 19104 caffe.cpp:313] Batch 421, accuracy/top1 = 0.66
I0815 19:03:38.121173 19104 caffe.cpp:313] Batch 421, accuracy/top5 = 0.84
I0815 19:03:38.121176 19104 caffe.cpp:313] Batch 421, loss = 1.6563
I0815 19:03:38.169560 19104 caffe.cpp:313] Batch 422, accuracy/top1 = 0.58
I0815 19:03:38.169575 19104 caffe.cpp:313] Batch 422, accuracy/top5 = 0.78
I0815 19:03:38.169579 19104 caffe.cpp:313] Batch 422, loss = 1.82465
I0815 19:03:38.219883 19104 caffe.cpp:313] Batch 423, accuracy/top1 = 0.6
I0815 19:03:38.219908 19104 caffe.cpp:313] Batch 423, accuracy/top5 = 0.74
I0815 19:03:38.219926 19104 caffe.cpp:313] Batch 423, loss = 2.16529
I0815 19:03:38.268497 19104 caffe.cpp:313] Batch 424, accuracy/top1 = 0.64
I0815 19:03:38.268523 19104 caffe.cpp:313] Batch 424, accuracy/top5 = 0.92
I0815 19:03:38.268527 19104 caffe.cpp:313] Batch 424, loss = 1.19156
I0815 19:03:38.317927 19104 caffe.cpp:313] Batch 425, accuracy/top1 = 0.66
I0815 19:03:38.317952 19104 caffe.cpp:313] Batch 425, accuracy/top5 = 0.84
I0815 19:03:38.317955 19104 caffe.cpp:313] Batch 425, loss = 1.8256
I0815 19:03:38.367672 19104 caffe.cpp:313] Batch 426, accuracy/top1 = 0.52
I0815 19:03:38.367707 19104 caffe.cpp:313] Batch 426, accuracy/top5 = 0.76
I0815 19:03:38.367713 19104 caffe.cpp:313] Batch 426, loss = 1.89157
I0815 19:03:38.416613 19104 caffe.cpp:313] Batch 427, accuracy/top1 = 0.68
I0815 19:03:38.416633 19104 caffe.cpp:313] Batch 427, accuracy/top5 = 0.84
I0815 19:03:38.416637 19104 caffe.cpp:313] Batch 427, loss = 1.58873
I0815 19:03:38.464835 19104 caffe.cpp:313] Batch 428, accuracy/top1 = 0.7
I0815 19:03:38.464860 19104 caffe.cpp:313] Batch 428, accuracy/top5 = 0.9
I0815 19:03:38.464864 19104 caffe.cpp:313] Batch 428, loss = 1.31936
I0815 19:03:38.513480 19104 caffe.cpp:313] Batch 429, accuracy/top1 = 0.68
I0815 19:03:38.513505 19104 caffe.cpp:313] Batch 429, accuracy/top5 = 0.86
I0815 19:03:38.513509 19104 caffe.cpp:313] Batch 429, loss = 1.41539
I0815 19:03:38.562084 19104 caffe.cpp:313] Batch 430, accuracy/top1 = 0.64
I0815 19:03:38.562109 19104 caffe.cpp:313] Batch 430, accuracy/top5 = 0.86
I0815 19:03:38.562114 19104 caffe.cpp:313] Batch 430, loss = 1.30496
I0815 19:03:38.610821 19104 caffe.cpp:313] Batch 431, accuracy/top1 = 0.62
I0815 19:03:38.610844 19104 caffe.cpp:313] Batch 431, accuracy/top5 = 0.86
I0815 19:03:38.610848 19104 caffe.cpp:313] Batch 431, loss = 1.87258
I0815 19:03:38.659476 19104 caffe.cpp:313] Batch 432, accuracy/top1 = 0.54
I0815 19:03:38.659502 19104 caffe.cpp:313] Batch 432, accuracy/top5 = 0.76
I0815 19:03:38.659505 19104 caffe.cpp:313] Batch 432, loss = 2.29663
I0815 19:03:38.708874 19104 caffe.cpp:313] Batch 433, accuracy/top1 = 0.58
I0815 19:03:38.708901 19104 caffe.cpp:313] Batch 433, accuracy/top5 = 0.88
I0815 19:03:38.708904 19104 caffe.cpp:313] Batch 433, loss = 1.57122
I0815 19:03:38.757736 19104 caffe.cpp:313] Batch 434, accuracy/top1 = 0.52
I0815 19:03:38.757761 19104 caffe.cpp:313] Batch 434, accuracy/top5 = 0.82
I0815 19:03:38.757766 19104 caffe.cpp:313] Batch 434, loss = 2.19882
I0815 19:03:38.806758 19104 caffe.cpp:313] Batch 435, accuracy/top1 = 0.68
I0815 19:03:38.806782 19104 caffe.cpp:313] Batch 435, accuracy/top5 = 0.86
I0815 19:03:38.806787 19104 caffe.cpp:313] Batch 435, loss = 1.4769
I0815 19:03:38.855562 19104 caffe.cpp:313] Batch 436, accuracy/top1 = 0.56
I0815 19:03:38.855588 19104 caffe.cpp:313] Batch 436, accuracy/top5 = 0.8
I0815 19:03:38.855592 19104 caffe.cpp:313] Batch 436, loss = 1.96668
I0815 19:03:38.904500 19104 caffe.cpp:313] Batch 437, accuracy/top1 = 0.48
I0815 19:03:38.904525 19104 caffe.cpp:313] Batch 437, accuracy/top5 = 0.74
I0815 19:03:38.904530 19104 caffe.cpp:313] Batch 437, loss = 2.12169
I0815 19:03:38.954632 19104 caffe.cpp:313] Batch 438, accuracy/top1 = 0.64
I0815 19:03:38.954658 19104 caffe.cpp:313] Batch 438, accuracy/top5 = 0.78
I0815 19:03:38.954663 19104 caffe.cpp:313] Batch 438, loss = 1.76542
I0815 19:03:39.002923 19104 caffe.cpp:313] Batch 439, accuracy/top1 = 0.58
I0815 19:03:39.002948 19104 caffe.cpp:313] Batch 439, accuracy/top5 = 0.82
I0815 19:03:39.002951 19104 caffe.cpp:313] Batch 439, loss = 2.11523
I0815 19:03:39.051926 19104 caffe.cpp:313] Batch 440, accuracy/top1 = 0.56
I0815 19:03:39.051944 19104 caffe.cpp:313] Batch 440, accuracy/top5 = 0.82
I0815 19:03:39.051949 19104 caffe.cpp:313] Batch 440, loss = 1.65753
I0815 19:03:39.099269 19104 caffe.cpp:313] Batch 441, accuracy/top1 = 0.5
I0815 19:03:39.099284 19104 caffe.cpp:313] Batch 441, accuracy/top5 = 0.88
I0815 19:03:39.099288 19104 caffe.cpp:313] Batch 441, loss = 1.5428
I0815 19:03:39.149183 19104 caffe.cpp:313] Batch 442, accuracy/top1 = 0.66
I0815 19:03:39.149226 19104 caffe.cpp:313] Batch 442, accuracy/top5 = 0.88
I0815 19:03:39.149231 19104 caffe.cpp:313] Batch 442, loss = 1.52139
I0815 19:03:39.197839 19104 caffe.cpp:313] Batch 443, accuracy/top1 = 0.72
I0815 19:03:39.197865 19104 caffe.cpp:313] Batch 443, accuracy/top5 = 0.8
I0815 19:03:39.197868 19104 caffe.cpp:313] Batch 443, loss = 1.44638
I0815 19:03:39.246489 19104 caffe.cpp:313] Batch 444, accuracy/top1 = 0.74
I0815 19:03:39.246515 19104 caffe.cpp:313] Batch 444, accuracy/top5 = 0.88
I0815 19:03:39.246518 19104 caffe.cpp:313] Batch 444, loss = 1.14464
I0815 19:03:39.295852 19104 caffe.cpp:313] Batch 445, accuracy/top1 = 0.6
I0815 19:03:39.295877 19104 caffe.cpp:313] Batch 445, accuracy/top5 = 0.86
I0815 19:03:39.295881 19104 caffe.cpp:313] Batch 445, loss = 1.70005
I0815 19:03:39.345363 19104 caffe.cpp:313] Batch 446, accuracy/top1 = 0.62
I0815 19:03:39.345388 19104 caffe.cpp:313] Batch 446, accuracy/top5 = 0.9
I0815 19:03:39.345392 19104 caffe.cpp:313] Batch 446, loss = 1.4522
I0815 19:03:39.394582 19104 caffe.cpp:313] Batch 447, accuracy/top1 = 0.62
I0815 19:03:39.394603 19104 caffe.cpp:313] Batch 447, accuracy/top5 = 0.8
I0815 19:03:39.394605 19104 caffe.cpp:313] Batch 447, loss = 1.63002
I0815 19:03:39.444120 19104 caffe.cpp:313] Batch 448, accuracy/top1 = 0.62
I0815 19:03:39.444149 19104 caffe.cpp:313] Batch 448, accuracy/top5 = 0.82
I0815 19:03:39.444152 19104 caffe.cpp:313] Batch 448, loss = 1.4428
I0815 19:03:39.493348 19104 caffe.cpp:313] Batch 449, accuracy/top1 = 0.6
I0815 19:03:39.493371 19104 caffe.cpp:313] Batch 449, accuracy/top5 = 0.78
I0815 19:03:39.493374 19104 caffe.cpp:313] Batch 449, loss = 1.87162
I0815 19:03:39.542124 19104 caffe.cpp:313] Batch 450, accuracy/top1 = 0.62
I0815 19:03:39.542147 19104 caffe.cpp:313] Batch 450, accuracy/top5 = 0.82
I0815 19:03:39.542150 19104 caffe.cpp:313] Batch 450, loss = 1.74479
I0815 19:03:39.589495 19104 caffe.cpp:313] Batch 451, accuracy/top1 = 0.62
I0815 19:03:39.589524 19104 caffe.cpp:313] Batch 451, accuracy/top5 = 0.84
I0815 19:03:39.589526 19104 caffe.cpp:313] Batch 451, loss = 1.6888
I0815 19:03:39.637948 19104 caffe.cpp:313] Batch 452, accuracy/top1 = 0.62
I0815 19:03:39.637972 19104 caffe.cpp:313] Batch 452, accuracy/top5 = 0.84
I0815 19:03:39.637976 19104 caffe.cpp:313] Batch 452, loss = 1.32788
I0815 19:03:39.686730 19104 caffe.cpp:313] Batch 453, accuracy/top1 = 0.64
I0815 19:03:39.686756 19104 caffe.cpp:313] Batch 453, accuracy/top5 = 0.86
I0815 19:03:39.686761 19104 caffe.cpp:313] Batch 453, loss = 1.61907
I0815 19:03:39.736352 19104 caffe.cpp:313] Batch 454, accuracy/top1 = 0.54
I0815 19:03:39.736377 19104 caffe.cpp:313] Batch 454, accuracy/top5 = 0.8
I0815 19:03:39.736380 19104 caffe.cpp:313] Batch 454, loss = 2.15862
I0815 19:03:39.785028 19104 caffe.cpp:313] Batch 455, accuracy/top1 = 0.62
I0815 19:03:39.785049 19104 caffe.cpp:313] Batch 455, accuracy/top5 = 0.82
I0815 19:03:39.785053 19104 caffe.cpp:313] Batch 455, loss = 1.7138
I0815 19:03:39.833830 19104 caffe.cpp:313] Batch 456, accuracy/top1 = 0.58
I0815 19:03:39.833853 19104 caffe.cpp:313] Batch 456, accuracy/top5 = 0.88
I0815 19:03:39.833856 19104 caffe.cpp:313] Batch 456, loss = 1.94018
I0815 19:03:39.885592 19104 caffe.cpp:313] Batch 457, accuracy/top1 = 0.54
I0815 19:03:39.885614 19104 caffe.cpp:313] Batch 457, accuracy/top5 = 0.76
I0815 19:03:39.885618 19104 caffe.cpp:313] Batch 457, loss = 1.93335
I0815 19:03:39.933495 19104 caffe.cpp:313] Batch 458, accuracy/top1 = 0.54
I0815 19:03:39.933521 19104 caffe.cpp:313] Batch 458, accuracy/top5 = 0.7
I0815 19:03:39.933526 19104 caffe.cpp:313] Batch 458, loss = 2.30893
I0815 19:03:39.982205 19104 caffe.cpp:313] Batch 459, accuracy/top1 = 0.62
I0815 19:03:39.982228 19104 caffe.cpp:313] Batch 459, accuracy/top5 = 0.88
I0815 19:03:39.982231 19104 caffe.cpp:313] Batch 459, loss = 1.20449
I0815 19:03:40.030391 19104 caffe.cpp:313] Batch 460, accuracy/top1 = 0.64
I0815 19:03:40.030412 19104 caffe.cpp:313] Batch 460, accuracy/top5 = 0.8
I0815 19:03:40.030416 19104 caffe.cpp:313] Batch 460, loss = 1.58438
I0815 19:03:40.078615 19104 caffe.cpp:313] Batch 461, accuracy/top1 = 0.68
I0815 19:03:40.078655 19104 caffe.cpp:313] Batch 461, accuracy/top5 = 0.9
I0815 19:03:40.078660 19104 caffe.cpp:313] Batch 461, loss = 1.39715
I0815 19:03:40.127646 19104 caffe.cpp:313] Batch 462, accuracy/top1 = 0.64
I0815 19:03:40.127662 19104 caffe.cpp:313] Batch 462, accuracy/top5 = 0.86
I0815 19:03:40.127666 19104 caffe.cpp:313] Batch 462, loss = 1.36649
I0815 19:03:40.176921 19104 caffe.cpp:313] Batch 463, accuracy/top1 = 0.52
I0815 19:03:40.176936 19104 caffe.cpp:313] Batch 463, accuracy/top5 = 0.7
I0815 19:03:40.176940 19104 caffe.cpp:313] Batch 463, loss = 2.00927
I0815 19:03:40.226426 19104 caffe.cpp:313] Batch 464, accuracy/top1 = 0.58
I0815 19:03:40.226451 19104 caffe.cpp:313] Batch 464, accuracy/top5 = 0.86
I0815 19:03:40.226456 19104 caffe.cpp:313] Batch 464, loss = 1.67533
I0815 19:03:40.275063 19104 caffe.cpp:313] Batch 465, accuracy/top1 = 0.6
I0815 19:03:40.275087 19104 caffe.cpp:313] Batch 465, accuracy/top5 = 0.86
I0815 19:03:40.275092 19104 caffe.cpp:313] Batch 465, loss = 1.59948
I0815 19:03:40.323359 19104 caffe.cpp:313] Batch 466, accuracy/top1 = 0.6
I0815 19:03:40.323385 19104 caffe.cpp:313] Batch 466, accuracy/top5 = 0.8
I0815 19:03:40.323388 19104 caffe.cpp:313] Batch 466, loss = 1.8115
I0815 19:03:40.371026 19104 caffe.cpp:313] Batch 467, accuracy/top1 = 0.66
I0815 19:03:40.371052 19104 caffe.cpp:313] Batch 467, accuracy/top5 = 0.9
I0815 19:03:40.371055 19104 caffe.cpp:313] Batch 467, loss = 1.35115
I0815 19:03:40.420701 19104 caffe.cpp:313] Batch 468, accuracy/top1 = 0.54
I0815 19:03:40.420723 19104 caffe.cpp:313] Batch 468, accuracy/top5 = 0.84
I0815 19:03:40.420727 19104 caffe.cpp:313] Batch 468, loss = 1.78154
I0815 19:03:40.469429 19104 caffe.cpp:313] Batch 469, accuracy/top1 = 0.58
I0815 19:03:40.469454 19104 caffe.cpp:313] Batch 469, accuracy/top5 = 0.86
I0815 19:03:40.469457 19104 caffe.cpp:313] Batch 469, loss = 1.54961
I0815 19:03:40.517894 19104 caffe.cpp:313] Batch 470, accuracy/top1 = 0.56
I0815 19:03:40.517920 19104 caffe.cpp:313] Batch 470, accuracy/top5 = 0.84
I0815 19:03:40.517925 19104 caffe.cpp:313] Batch 470, loss = 1.70462
I0815 19:03:40.566035 19104 caffe.cpp:313] Batch 471, accuracy/top1 = 0.68
I0815 19:03:40.566059 19104 caffe.cpp:313] Batch 471, accuracy/top5 = 0.92
I0815 19:03:40.566063 19104 caffe.cpp:313] Batch 471, loss = 1.37669
I0815 19:03:40.614284 19104 caffe.cpp:313] Batch 472, accuracy/top1 = 0.62
I0815 19:03:40.614310 19104 caffe.cpp:313] Batch 472, accuracy/top5 = 0.86
I0815 19:03:40.614313 19104 caffe.cpp:313] Batch 472, loss = 1.48337
I0815 19:03:40.663159 19104 caffe.cpp:313] Batch 473, accuracy/top1 = 0.7
I0815 19:03:40.663184 19104 caffe.cpp:313] Batch 473, accuracy/top5 = 0.84
I0815 19:03:40.663188 19104 caffe.cpp:313] Batch 473, loss = 1.68692
I0815 19:03:40.711781 19104 caffe.cpp:313] Batch 474, accuracy/top1 = 0.6
I0815 19:03:40.711805 19104 caffe.cpp:313] Batch 474, accuracy/top5 = 0.82
I0815 19:03:40.711809 19104 caffe.cpp:313] Batch 474, loss = 1.62201
I0815 19:03:40.760308 19104 caffe.cpp:313] Batch 475, accuracy/top1 = 0.56
I0815 19:03:40.760334 19104 caffe.cpp:313] Batch 475, accuracy/top5 = 0.82
I0815 19:03:40.760339 19104 caffe.cpp:313] Batch 475, loss = 1.75083
I0815 19:03:40.808539 19104 caffe.cpp:313] Batch 476, accuracy/top1 = 0.56
I0815 19:03:40.808563 19104 caffe.cpp:313] Batch 476, accuracy/top5 = 0.78
I0815 19:03:40.808568 19104 caffe.cpp:313] Batch 476, loss = 1.91788
I0815 19:03:40.856325 19104 caffe.cpp:313] Batch 477, accuracy/top1 = 0.48
I0815 19:03:40.856349 19104 caffe.cpp:313] Batch 477, accuracy/top5 = 0.78
I0815 19:03:40.856353 19104 caffe.cpp:313] Batch 477, loss = 1.80312
I0815 19:03:40.905562 19104 caffe.cpp:313] Batch 478, accuracy/top1 = 0.62
I0815 19:03:40.905586 19104 caffe.cpp:313] Batch 478, accuracy/top5 = 0.82
I0815 19:03:40.905591 19104 caffe.cpp:313] Batch 478, loss = 1.67892
I0815 19:03:40.953994 19104 caffe.cpp:313] Batch 479, accuracy/top1 = 0.6
I0815 19:03:40.954017 19104 caffe.cpp:313] Batch 479, accuracy/top5 = 0.74
I0815 19:03:40.954022 19104 caffe.cpp:313] Batch 479, loss = 2.02046
I0815 19:03:41.002506 19104 caffe.cpp:313] Batch 480, accuracy/top1 = 0.64
I0815 19:03:41.002526 19104 caffe.cpp:313] Batch 480, accuracy/top5 = 0.86
I0815 19:03:41.002530 19104 caffe.cpp:313] Batch 480, loss = 1.435
I0815 19:03:41.050963 19104 caffe.cpp:313] Batch 481, accuracy/top1 = 0.6
I0815 19:03:41.050981 19104 caffe.cpp:313] Batch 481, accuracy/top5 = 0.8
I0815 19:03:41.050984 19104 caffe.cpp:313] Batch 481, loss = 1.7907
I0815 19:03:41.100119 19104 caffe.cpp:313] Batch 482, accuracy/top1 = 0.6
I0815 19:03:41.100145 19104 caffe.cpp:313] Batch 482, accuracy/top5 = 0.76
I0815 19:03:41.100150 19104 caffe.cpp:313] Batch 482, loss = 2.29524
I0815 19:03:41.149255 19104 caffe.cpp:313] Batch 483, accuracy/top1 = 0.62
I0815 19:03:41.149269 19104 caffe.cpp:313] Batch 483, accuracy/top5 = 0.84
I0815 19:03:41.149273 19104 caffe.cpp:313] Batch 483, loss = 1.46532
I0815 19:03:41.197510 19104 caffe.cpp:313] Batch 484, accuracy/top1 = 0.58
I0815 19:03:41.197537 19104 caffe.cpp:313] Batch 484, accuracy/top5 = 0.74
I0815 19:03:41.197541 19104 caffe.cpp:313] Batch 484, loss = 2.24383
I0815 19:03:41.246847 19104 caffe.cpp:313] Batch 485, accuracy/top1 = 0.64
I0815 19:03:41.246871 19104 caffe.cpp:313] Batch 485, accuracy/top5 = 0.76
I0815 19:03:41.246876 19104 caffe.cpp:313] Batch 485, loss = 1.61468
I0815 19:03:41.296090 19104 caffe.cpp:313] Batch 486, accuracy/top1 = 0.68
I0815 19:03:41.296115 19104 caffe.cpp:313] Batch 486, accuracy/top5 = 0.88
I0815 19:03:41.296119 19104 caffe.cpp:313] Batch 486, loss = 1.09839
I0815 19:03:41.344259 19104 caffe.cpp:313] Batch 487, accuracy/top1 = 0.56
I0815 19:03:41.344285 19104 caffe.cpp:313] Batch 487, accuracy/top5 = 0.74
I0815 19:03:41.344288 19104 caffe.cpp:313] Batch 487, loss = 2.3107
I0815 19:03:41.393196 19104 caffe.cpp:313] Batch 488, accuracy/top1 = 0.52
I0815 19:03:41.393216 19104 caffe.cpp:313] Batch 488, accuracy/top5 = 0.84
I0815 19:03:41.393219 19104 caffe.cpp:313] Batch 488, loss = 1.85705
I0815 19:03:41.441840 19104 caffe.cpp:313] Batch 489, accuracy/top1 = 0.54
I0815 19:03:41.441864 19104 caffe.cpp:313] Batch 489, accuracy/top5 = 0.84
I0815 19:03:41.441866 19104 caffe.cpp:313] Batch 489, loss = 1.72204
I0815 19:03:41.491367 19104 caffe.cpp:313] Batch 490, accuracy/top1 = 0.58
I0815 19:03:41.491391 19104 caffe.cpp:313] Batch 490, accuracy/top5 = 0.84
I0815 19:03:41.491394 19104 caffe.cpp:313] Batch 490, loss = 1.65853
I0815 19:03:41.540347 19104 caffe.cpp:313] Batch 491, accuracy/top1 = 0.54
I0815 19:03:41.540370 19104 caffe.cpp:313] Batch 491, accuracy/top5 = 0.84
I0815 19:03:41.540374 19104 caffe.cpp:313] Batch 491, loss = 1.68605
I0815 19:03:41.588563 19104 caffe.cpp:313] Batch 492, accuracy/top1 = 0.6
I0815 19:03:41.588587 19104 caffe.cpp:313] Batch 492, accuracy/top5 = 0.76
I0815 19:03:41.588589 19104 caffe.cpp:313] Batch 492, loss = 2.23606
I0815 19:03:41.637405 19104 caffe.cpp:313] Batch 493, accuracy/top1 = 0.62
I0815 19:03:41.637429 19104 caffe.cpp:313] Batch 493, accuracy/top5 = 0.78
I0815 19:03:41.637432 19104 caffe.cpp:313] Batch 493, loss = 1.86226
I0815 19:03:41.687089 19104 caffe.cpp:313] Batch 494, accuracy/top1 = 0.56
I0815 19:03:41.687113 19104 caffe.cpp:313] Batch 494, accuracy/top5 = 0.76
I0815 19:03:41.687115 19104 caffe.cpp:313] Batch 494, loss = 1.85138
I0815 19:03:41.736344 19104 caffe.cpp:313] Batch 495, accuracy/top1 = 0.66
I0815 19:03:41.736368 19104 caffe.cpp:313] Batch 495, accuracy/top5 = 0.9
I0815 19:03:41.736372 19104 caffe.cpp:313] Batch 495, loss = 1.35988
I0815 19:03:41.784730 19104 caffe.cpp:313] Batch 496, accuracy/top1 = 0.56
I0815 19:03:41.784755 19104 caffe.cpp:313] Batch 496, accuracy/top5 = 0.8
I0815 19:03:41.784759 19104 caffe.cpp:313] Batch 496, loss = 1.78393
I0815 19:03:41.833328 19104 caffe.cpp:313] Batch 497, accuracy/top1 = 0.58
I0815 19:03:41.833351 19104 caffe.cpp:313] Batch 497, accuracy/top5 = 0.72
I0815 19:03:41.833355 19104 caffe.cpp:313] Batch 497, loss = 1.84087
I0815 19:03:41.881700 19104 caffe.cpp:313] Batch 498, accuracy/top1 = 0.74
I0815 19:03:41.881723 19104 caffe.cpp:313] Batch 498, accuracy/top5 = 0.88
I0815 19:03:41.881747 19104 caffe.cpp:313] Batch 498, loss = 1.20785
I0815 19:03:41.930344 19104 caffe.cpp:313] Batch 499, accuracy/top1 = 0.58
I0815 19:03:41.930366 19104 caffe.cpp:313] Batch 499, accuracy/top5 = 0.78
I0815 19:03:41.930369 19104 caffe.cpp:313] Batch 499, loss = 1.79284
I0815 19:03:41.978868 19104 caffe.cpp:313] Batch 500, accuracy/top1 = 0.64
I0815 19:03:41.978889 19104 caffe.cpp:313] Batch 500, accuracy/top5 = 0.82
I0815 19:03:41.978894 19104 caffe.cpp:313] Batch 500, loss = 1.96436
I0815 19:03:42.029175 19104 caffe.cpp:313] Batch 501, accuracy/top1 = 0.6
I0815 19:03:42.029197 19104 caffe.cpp:313] Batch 501, accuracy/top5 = 0.76
I0815 19:03:42.029202 19104 caffe.cpp:313] Batch 501, loss = 1.97525
I0815 19:03:42.078518 19104 caffe.cpp:313] Batch 502, accuracy/top1 = 0.4
I0815 19:03:42.078541 19104 caffe.cpp:313] Batch 502, accuracy/top5 = 0.78
I0815 19:03:42.078546 19104 caffe.cpp:313] Batch 502, loss = 2.37588
I0815 19:03:42.126973 19104 caffe.cpp:313] Batch 503, accuracy/top1 = 0.62
I0815 19:03:42.126991 19104 caffe.cpp:313] Batch 503, accuracy/top5 = 0.8
I0815 19:03:42.126994 19104 caffe.cpp:313] Batch 503, loss = 1.87247
I0815 19:03:42.174693 19104 caffe.cpp:313] Batch 504, accuracy/top1 = 0.6
I0815 19:03:42.174708 19104 caffe.cpp:313] Batch 504, accuracy/top5 = 0.88
I0815 19:03:42.174712 19104 caffe.cpp:313] Batch 504, loss = 1.22945
I0815 19:03:42.224654 19104 caffe.cpp:313] Batch 505, accuracy/top1 = 0.56
I0815 19:03:42.224679 19104 caffe.cpp:313] Batch 505, accuracy/top5 = 0.9
I0815 19:03:42.224684 19104 caffe.cpp:313] Batch 505, loss = 1.78277
I0815 19:03:42.274261 19104 caffe.cpp:313] Batch 506, accuracy/top1 = 0.64
I0815 19:03:42.274286 19104 caffe.cpp:313] Batch 506, accuracy/top5 = 0.92
I0815 19:03:42.274291 19104 caffe.cpp:313] Batch 506, loss = 1.29923
I0815 19:03:42.323626 19104 caffe.cpp:313] Batch 507, accuracy/top1 = 0.52
I0815 19:03:42.323652 19104 caffe.cpp:313] Batch 507, accuracy/top5 = 0.76
I0815 19:03:42.323655 19104 caffe.cpp:313] Batch 507, loss = 2.33233
I0815 19:03:42.373303 19104 caffe.cpp:313] Batch 508, accuracy/top1 = 0.64
I0815 19:03:42.373332 19104 caffe.cpp:313] Batch 508, accuracy/top5 = 0.9
I0815 19:03:42.373337 19104 caffe.cpp:313] Batch 508, loss = 1.80396
I0815 19:03:42.424085 19104 caffe.cpp:313] Batch 509, accuracy/top1 = 0.52
I0815 19:03:42.424104 19104 caffe.cpp:313] Batch 509, accuracy/top5 = 0.84
I0815 19:03:42.424108 19104 caffe.cpp:313] Batch 509, loss = 1.75133
I0815 19:03:42.474294 19104 caffe.cpp:313] Batch 510, accuracy/top1 = 0.54
I0815 19:03:42.474319 19104 caffe.cpp:313] Batch 510, accuracy/top5 = 0.82
I0815 19:03:42.474323 19104 caffe.cpp:313] Batch 510, loss = 1.78261
I0815 19:03:42.523041 19104 caffe.cpp:313] Batch 511, accuracy/top1 = 0.66
I0815 19:03:42.523066 19104 caffe.cpp:313] Batch 511, accuracy/top5 = 0.84
I0815 19:03:42.523069 19104 caffe.cpp:313] Batch 511, loss = 1.68571
I0815 19:03:42.572101 19104 caffe.cpp:313] Batch 512, accuracy/top1 = 0.64
I0815 19:03:42.572127 19104 caffe.cpp:313] Batch 512, accuracy/top5 = 0.86
I0815 19:03:42.572136 19104 caffe.cpp:313] Batch 512, loss = 1.41807
I0815 19:03:42.622234 19104 caffe.cpp:313] Batch 513, accuracy/top1 = 0.54
I0815 19:03:42.622259 19104 caffe.cpp:313] Batch 513, accuracy/top5 = 0.78
I0815 19:03:42.622263 19104 caffe.cpp:313] Batch 513, loss = 1.90242
I0815 19:03:42.671536 19104 caffe.cpp:313] Batch 514, accuracy/top1 = 0.52
I0815 19:03:42.671561 19104 caffe.cpp:313] Batch 514, accuracy/top5 = 0.8
I0815 19:03:42.671566 19104 caffe.cpp:313] Batch 514, loss = 1.8715
I0815 19:03:42.720753 19104 caffe.cpp:313] Batch 515, accuracy/top1 = 0.56
I0815 19:03:42.720778 19104 caffe.cpp:313] Batch 515, accuracy/top5 = 0.78
I0815 19:03:42.720782 19104 caffe.cpp:313] Batch 515, loss = 1.8724
I0815 19:03:42.769708 19104 caffe.cpp:313] Batch 516, accuracy/top1 = 0.58
I0815 19:03:42.769733 19104 caffe.cpp:313] Batch 516, accuracy/top5 = 0.8
I0815 19:03:42.769737 19104 caffe.cpp:313] Batch 516, loss = 2.13759
I0815 19:03:42.817314 19104 caffe.cpp:313] Batch 517, accuracy/top1 = 0.68
I0815 19:03:42.817363 19104 caffe.cpp:313] Batch 517, accuracy/top5 = 0.86
I0815 19:03:42.817368 19104 caffe.cpp:313] Batch 517, loss = 1.33646
I0815 19:03:42.866225 19104 caffe.cpp:313] Batch 518, accuracy/top1 = 0.62
I0815 19:03:42.866250 19104 caffe.cpp:313] Batch 518, accuracy/top5 = 0.82
I0815 19:03:42.866255 19104 caffe.cpp:313] Batch 518, loss = 1.72917
I0815 19:03:42.915213 19104 caffe.cpp:313] Batch 519, accuracy/top1 = 0.66
I0815 19:03:42.915238 19104 caffe.cpp:313] Batch 519, accuracy/top5 = 0.82
I0815 19:03:42.915242 19104 caffe.cpp:313] Batch 519, loss = 1.70374
I0815 19:03:42.964598 19104 caffe.cpp:313] Batch 520, accuracy/top1 = 0.54
I0815 19:03:42.964622 19104 caffe.cpp:313] Batch 520, accuracy/top5 = 0.76
I0815 19:03:42.964625 19104 caffe.cpp:313] Batch 520, loss = 2.1351
I0815 19:03:43.013725 19104 caffe.cpp:313] Batch 521, accuracy/top1 = 0.58
I0815 19:03:43.013749 19104 caffe.cpp:313] Batch 521, accuracy/top5 = 0.9
I0815 19:03:43.013752 19104 caffe.cpp:313] Batch 521, loss = 1.72649
I0815 19:03:43.063082 19104 caffe.cpp:313] Batch 522, accuracy/top1 = 0.6
I0815 19:03:43.063099 19104 caffe.cpp:313] Batch 522, accuracy/top5 = 0.88
I0815 19:03:43.063102 19104 caffe.cpp:313] Batch 522, loss = 1.41145
I0815 19:03:43.111866 19104 caffe.cpp:313] Batch 523, accuracy/top1 = 0.64
I0815 19:03:43.111884 19104 caffe.cpp:313] Batch 523, accuracy/top5 = 0.88
I0815 19:03:43.111887 19104 caffe.cpp:313] Batch 523, loss = 1.36342
I0815 19:03:43.161155 19104 caffe.cpp:313] Batch 524, accuracy/top1 = 0.52
I0815 19:03:43.161170 19104 caffe.cpp:313] Batch 524, accuracy/top5 = 0.78
I0815 19:03:43.161175 19104 caffe.cpp:313] Batch 524, loss = 1.76952
I0815 19:03:43.210441 19104 caffe.cpp:313] Batch 525, accuracy/top1 = 0.66
I0815 19:03:43.210469 19104 caffe.cpp:313] Batch 525, accuracy/top5 = 0.78
I0815 19:03:43.210471 19104 caffe.cpp:313] Batch 525, loss = 1.65912
I0815 19:03:43.259857 19104 caffe.cpp:313] Batch 526, accuracy/top1 = 0.52
I0815 19:03:43.259882 19104 caffe.cpp:313] Batch 526, accuracy/top5 = 0.7
I0815 19:03:43.259886 19104 caffe.cpp:313] Batch 526, loss = 2.52668
I0815 19:03:43.309501 19104 caffe.cpp:313] Batch 527, accuracy/top1 = 0.56
I0815 19:03:43.309527 19104 caffe.cpp:313] Batch 527, accuracy/top5 = 0.84
I0815 19:03:43.309530 19104 caffe.cpp:313] Batch 527, loss = 1.64189
I0815 19:03:43.358975 19104 caffe.cpp:313] Batch 528, accuracy/top1 = 0.66
I0815 19:03:43.358999 19104 caffe.cpp:313] Batch 528, accuracy/top5 = 0.86
I0815 19:03:43.359002 19104 caffe.cpp:313] Batch 528, loss = 1.5667
I0815 19:03:43.409126 19104 caffe.cpp:313] Batch 529, accuracy/top1 = 0.54
I0815 19:03:43.409145 19104 caffe.cpp:313] Batch 529, accuracy/top5 = 0.86
I0815 19:03:43.409149 19104 caffe.cpp:313] Batch 529, loss = 1.68123
I0815 19:03:43.458578 19104 caffe.cpp:313] Batch 530, accuracy/top1 = 0.6
I0815 19:03:43.458602 19104 caffe.cpp:313] Batch 530, accuracy/top5 = 0.86
I0815 19:03:43.458606 19104 caffe.cpp:313] Batch 530, loss = 1.70948
I0815 19:03:43.508407 19104 caffe.cpp:313] Batch 531, accuracy/top1 = 0.54
I0815 19:03:43.508431 19104 caffe.cpp:313] Batch 531, accuracy/top5 = 0.84
I0815 19:03:43.508435 19104 caffe.cpp:313] Batch 531, loss = 1.87211
I0815 19:03:43.558254 19104 caffe.cpp:313] Batch 532, accuracy/top1 = 0.62
I0815 19:03:43.558277 19104 caffe.cpp:313] Batch 532, accuracy/top5 = 0.92
I0815 19:03:43.558280 19104 caffe.cpp:313] Batch 532, loss = 1.32813
I0815 19:03:43.606881 19104 caffe.cpp:313] Batch 533, accuracy/top1 = 0.52
I0815 19:03:43.606906 19104 caffe.cpp:313] Batch 533, accuracy/top5 = 0.74
I0815 19:03:43.606909 19104 caffe.cpp:313] Batch 533, loss = 2.27195
I0815 19:03:43.655318 19104 caffe.cpp:313] Batch 534, accuracy/top1 = 0.64
I0815 19:03:43.655341 19104 caffe.cpp:313] Batch 534, accuracy/top5 = 0.82
I0815 19:03:43.655344 19104 caffe.cpp:313] Batch 534, loss = 1.44293
I0815 19:03:43.702919 19104 caffe.cpp:313] Batch 535, accuracy/top1 = 0.64
I0815 19:03:43.702947 19104 caffe.cpp:313] Batch 535, accuracy/top5 = 0.84
I0815 19:03:43.702952 19104 caffe.cpp:313] Batch 535, loss = 1.56424
I0815 19:03:43.751694 19104 caffe.cpp:313] Batch 536, accuracy/top1 = 0.66
I0815 19:03:43.751735 19104 caffe.cpp:313] Batch 536, accuracy/top5 = 0.9
I0815 19:03:43.751739 19104 caffe.cpp:313] Batch 536, loss = 1.13261
I0815 19:03:43.800753 19104 caffe.cpp:313] Batch 537, accuracy/top1 = 0.64
I0815 19:03:43.800778 19104 caffe.cpp:313] Batch 537, accuracy/top5 = 0.86
I0815 19:03:43.800781 19104 caffe.cpp:313] Batch 537, loss = 1.41575
I0815 19:03:43.850714 19104 caffe.cpp:313] Batch 538, accuracy/top1 = 0.54
I0815 19:03:43.850738 19104 caffe.cpp:313] Batch 538, accuracy/top5 = 0.82
I0815 19:03:43.850740 19104 caffe.cpp:313] Batch 538, loss = 1.77483
I0815 19:03:43.900615 19104 caffe.cpp:313] Batch 539, accuracy/top1 = 0.44
I0815 19:03:43.900640 19104 caffe.cpp:313] Batch 539, accuracy/top5 = 0.72
I0815 19:03:43.900643 19104 caffe.cpp:313] Batch 539, loss = 2.39562
I0815 19:03:43.949612 19104 caffe.cpp:313] Batch 540, accuracy/top1 = 0.54
I0815 19:03:43.949636 19104 caffe.cpp:313] Batch 540, accuracy/top5 = 0.8
I0815 19:03:43.949640 19104 caffe.cpp:313] Batch 540, loss = 1.79663
I0815 19:03:43.998616 19104 caffe.cpp:313] Batch 541, accuracy/top1 = 0.58
I0815 19:03:43.998641 19104 caffe.cpp:313] Batch 541, accuracy/top5 = 0.8
I0815 19:03:43.998644 19104 caffe.cpp:313] Batch 541, loss = 1.86668
I0815 19:03:44.047031 19104 caffe.cpp:313] Batch 542, accuracy/top1 = 0.62
I0815 19:03:44.047052 19104 caffe.cpp:313] Batch 542, accuracy/top5 = 0.82
I0815 19:03:44.047057 19104 caffe.cpp:313] Batch 542, loss = 1.86119
I0815 19:03:44.095743 19104 caffe.cpp:313] Batch 543, accuracy/top1 = 0.64
I0815 19:03:44.095765 19104 caffe.cpp:313] Batch 543, accuracy/top5 = 0.9
I0815 19:03:44.095769 19104 caffe.cpp:313] Batch 543, loss = 1.34225
I0815 19:03:44.144445 19104 caffe.cpp:313] Batch 544, accuracy/top1 = 0.74
I0815 19:03:44.144471 19104 caffe.cpp:313] Batch 544, accuracy/top5 = 0.82
I0815 19:03:44.144477 19104 caffe.cpp:313] Batch 544, loss = 1.39222
I0815 19:03:44.193572 19104 caffe.cpp:313] Batch 545, accuracy/top1 = 0.54
I0815 19:03:44.193608 19104 caffe.cpp:313] Batch 545, accuracy/top5 = 0.78
I0815 19:03:44.193612 19104 caffe.cpp:313] Batch 545, loss = 1.75666
I0815 19:03:44.241879 19104 caffe.cpp:313] Batch 546, accuracy/top1 = 0.74
I0815 19:03:44.241904 19104 caffe.cpp:313] Batch 546, accuracy/top5 = 0.88
I0815 19:03:44.241909 19104 caffe.cpp:313] Batch 546, loss = 1.6302
I0815 19:03:44.290226 19104 caffe.cpp:313] Batch 547, accuracy/top1 = 0.54
I0815 19:03:44.290252 19104 caffe.cpp:313] Batch 547, accuracy/top5 = 0.86
I0815 19:03:44.290256 19104 caffe.cpp:313] Batch 547, loss = 1.67561
I0815 19:03:44.338678 19104 caffe.cpp:313] Batch 548, accuracy/top1 = 0.54
I0815 19:03:44.338702 19104 caffe.cpp:313] Batch 548, accuracy/top5 = 0.78
I0815 19:03:44.338706 19104 caffe.cpp:313] Batch 548, loss = 2.15843
I0815 19:03:44.388034 19104 caffe.cpp:313] Batch 549, accuracy/top1 = 0.64
I0815 19:03:44.388056 19104 caffe.cpp:313] Batch 549, accuracy/top5 = 0.72
I0815 19:03:44.388061 19104 caffe.cpp:313] Batch 549, loss = 2.01618
I0815 19:03:44.437139 19104 caffe.cpp:313] Batch 550, accuracy/top1 = 0.56
I0815 19:03:44.437161 19104 caffe.cpp:313] Batch 550, accuracy/top5 = 0.82
I0815 19:03:44.437165 19104 caffe.cpp:313] Batch 550, loss = 1.75929
I0815 19:03:44.486263 19104 caffe.cpp:313] Batch 551, accuracy/top1 = 0.68
I0815 19:03:44.486289 19104 caffe.cpp:313] Batch 551, accuracy/top5 = 0.84
I0815 19:03:44.486292 19104 caffe.cpp:313] Batch 551, loss = 1.43219
I0815 19:03:44.534921 19104 caffe.cpp:313] Batch 552, accuracy/top1 = 0.64
I0815 19:03:44.534945 19104 caffe.cpp:313] Batch 552, accuracy/top5 = 0.82
I0815 19:03:44.534950 19104 caffe.cpp:313] Batch 552, loss = 1.54463
I0815 19:03:44.583510 19104 caffe.cpp:313] Batch 553, accuracy/top1 = 0.62
I0815 19:03:44.583535 19104 caffe.cpp:313] Batch 553, accuracy/top5 = 0.8
I0815 19:03:44.583539 19104 caffe.cpp:313] Batch 553, loss = 1.51717
I0815 19:03:44.631965 19104 caffe.cpp:313] Batch 554, accuracy/top1 = 0.62
I0815 19:03:44.631990 19104 caffe.cpp:313] Batch 554, accuracy/top5 = 0.82
I0815 19:03:44.631994 19104 caffe.cpp:313] Batch 554, loss = 1.72435
I0815 19:03:44.681000 19104 caffe.cpp:313] Batch 555, accuracy/top1 = 0.62
I0815 19:03:44.681026 19104 caffe.cpp:313] Batch 555, accuracy/top5 = 0.96
I0815 19:03:44.681030 19104 caffe.cpp:313] Batch 555, loss = 1.15844
I0815 19:03:44.730087 19104 caffe.cpp:313] Batch 556, accuracy/top1 = 0.66
I0815 19:03:44.730113 19104 caffe.cpp:313] Batch 556, accuracy/top5 = 0.8
I0815 19:03:44.730116 19104 caffe.cpp:313] Batch 556, loss = 1.69557
I0815 19:03:44.779103 19104 caffe.cpp:313] Batch 557, accuracy/top1 = 0.58
I0815 19:03:44.779127 19104 caffe.cpp:313] Batch 557, accuracy/top5 = 0.84
I0815 19:03:44.779131 19104 caffe.cpp:313] Batch 557, loss = 1.65297
I0815 19:03:44.827584 19104 caffe.cpp:313] Batch 558, accuracy/top1 = 0.5
I0815 19:03:44.827608 19104 caffe.cpp:313] Batch 558, accuracy/top5 = 0.72
I0815 19:03:44.827612 19104 caffe.cpp:313] Batch 558, loss = 2.15672
I0815 19:03:44.876710 19104 caffe.cpp:313] Batch 559, accuracy/top1 = 0.58
I0815 19:03:44.876734 19104 caffe.cpp:313] Batch 559, accuracy/top5 = 0.78
I0815 19:03:44.876739 19104 caffe.cpp:313] Batch 559, loss = 1.64327
I0815 19:03:44.924860 19104 caffe.cpp:313] Batch 560, accuracy/top1 = 0.66
I0815 19:03:44.924886 19104 caffe.cpp:313] Batch 560, accuracy/top5 = 0.82
I0815 19:03:44.924890 19104 caffe.cpp:313] Batch 560, loss = 1.71087
I0815 19:03:44.973471 19104 caffe.cpp:313] Batch 561, accuracy/top1 = 0.62
I0815 19:03:44.973497 19104 caffe.cpp:313] Batch 561, accuracy/top5 = 0.78
I0815 19:03:44.973501 19104 caffe.cpp:313] Batch 561, loss = 1.97071
I0815 19:03:45.021550 19104 caffe.cpp:313] Batch 562, accuracy/top1 = 0.58
I0815 19:03:45.021572 19104 caffe.cpp:313] Batch 562, accuracy/top5 = 0.84
I0815 19:03:45.021576 19104 caffe.cpp:313] Batch 562, loss = 1.71478
I0815 19:03:45.070288 19104 caffe.cpp:313] Batch 563, accuracy/top1 = 0.58
I0815 19:03:45.070307 19104 caffe.cpp:313] Batch 563, accuracy/top5 = 0.78
I0815 19:03:45.070310 19104 caffe.cpp:313] Batch 563, loss = 1.8201
I0815 19:03:45.118870 19104 caffe.cpp:313] Batch 564, accuracy/top1 = 0.64
I0815 19:03:45.118885 19104 caffe.cpp:313] Batch 564, accuracy/top5 = 0.88
I0815 19:03:45.118890 19104 caffe.cpp:313] Batch 564, loss = 1.69794
I0815 19:03:45.167781 19104 caffe.cpp:313] Batch 565, accuracy/top1 = 0.62
I0815 19:03:45.167796 19104 caffe.cpp:313] Batch 565, accuracy/top5 = 0.92
I0815 19:03:45.167800 19104 caffe.cpp:313] Batch 565, loss = 1.36946
I0815 19:03:45.216444 19104 caffe.cpp:313] Batch 566, accuracy/top1 = 0.6
I0815 19:03:45.216469 19104 caffe.cpp:313] Batch 566, accuracy/top5 = 0.8
I0815 19:03:45.216472 19104 caffe.cpp:313] Batch 566, loss = 1.91007
I0815 19:03:45.265202 19104 caffe.cpp:313] Batch 567, accuracy/top1 = 0.54
I0815 19:03:45.265228 19104 caffe.cpp:313] Batch 567, accuracy/top5 = 0.8
I0815 19:03:45.265231 19104 caffe.cpp:313] Batch 567, loss = 1.92552
I0815 19:03:45.314575 19104 caffe.cpp:313] Batch 568, accuracy/top1 = 0.5
I0815 19:03:45.314600 19104 caffe.cpp:313] Batch 568, accuracy/top5 = 0.7
I0815 19:03:45.314604 19104 caffe.cpp:313] Batch 568, loss = 2.04968
I0815 19:03:45.362155 19104 caffe.cpp:313] Batch 569, accuracy/top1 = 0.46
I0815 19:03:45.362181 19104 caffe.cpp:313] Batch 569, accuracy/top5 = 0.72
I0815 19:03:45.362185 19104 caffe.cpp:313] Batch 569, loss = 2.65839
I0815 19:03:45.415091 19104 caffe.cpp:313] Batch 570, accuracy/top1 = 0.64
I0815 19:03:45.415112 19104 caffe.cpp:313] Batch 570, accuracy/top5 = 0.8
I0815 19:03:45.415115 19104 caffe.cpp:313] Batch 570, loss = 1.60682
I0815 19:03:45.462960 19104 caffe.cpp:313] Batch 571, accuracy/top1 = 0.48
I0815 19:03:45.462985 19104 caffe.cpp:313] Batch 571, accuracy/top5 = 0.64
I0815 19:03:45.462990 19104 caffe.cpp:313] Batch 571, loss = 2.46521
I0815 19:03:45.512449 19104 caffe.cpp:313] Batch 572, accuracy/top1 = 0.5
I0815 19:03:45.512473 19104 caffe.cpp:313] Batch 572, accuracy/top5 = 0.84
I0815 19:03:45.512477 19104 caffe.cpp:313] Batch 572, loss = 1.72011
I0815 19:03:45.561794 19104 caffe.cpp:313] Batch 573, accuracy/top1 = 0.66
I0815 19:03:45.561818 19104 caffe.cpp:313] Batch 573, accuracy/top5 = 0.84
I0815 19:03:45.561841 19104 caffe.cpp:313] Batch 573, loss = 1.93864
I0815 19:03:45.611464 19104 caffe.cpp:313] Batch 574, accuracy/top1 = 0.56
I0815 19:03:45.611488 19104 caffe.cpp:313] Batch 574, accuracy/top5 = 0.8
I0815 19:03:45.611492 19104 caffe.cpp:313] Batch 574, loss = 2.06816
I0815 19:03:45.660540 19104 caffe.cpp:313] Batch 575, accuracy/top1 = 0.58
I0815 19:03:45.660565 19104 caffe.cpp:313] Batch 575, accuracy/top5 = 0.78
I0815 19:03:45.660569 19104 caffe.cpp:313] Batch 575, loss = 1.96317
I0815 19:03:45.709492 19104 caffe.cpp:313] Batch 576, accuracy/top1 = 0.72
I0815 19:03:45.709517 19104 caffe.cpp:313] Batch 576, accuracy/top5 = 0.9
I0815 19:03:45.709520 19104 caffe.cpp:313] Batch 576, loss = 1.15665
I0815 19:03:45.758595 19104 caffe.cpp:313] Batch 577, accuracy/top1 = 0.56
I0815 19:03:45.758621 19104 caffe.cpp:313] Batch 577, accuracy/top5 = 0.86
I0815 19:03:45.758626 19104 caffe.cpp:313] Batch 577, loss = 1.51516
I0815 19:03:45.808742 19104 caffe.cpp:313] Batch 578, accuracy/top1 = 0.56
I0815 19:03:45.808768 19104 caffe.cpp:313] Batch 578, accuracy/top5 = 0.78
I0815 19:03:45.808771 19104 caffe.cpp:313] Batch 578, loss = 1.86915
I0815 19:03:45.857705 19104 caffe.cpp:313] Batch 579, accuracy/top1 = 0.64
I0815 19:03:45.857729 19104 caffe.cpp:313] Batch 579, accuracy/top5 = 0.74
I0815 19:03:45.857733 19104 caffe.cpp:313] Batch 579, loss = 2.09225
I0815 19:03:45.907392 19104 caffe.cpp:313] Batch 580, accuracy/top1 = 0.68
I0815 19:03:45.907418 19104 caffe.cpp:313] Batch 580, accuracy/top5 = 0.86
I0815 19:03:45.907423 19104 caffe.cpp:313] Batch 580, loss = 1.51958
I0815 19:03:45.957268 19104 caffe.cpp:313] Batch 581, accuracy/top1 = 0.58
I0815 19:03:45.957293 19104 caffe.cpp:313] Batch 581, accuracy/top5 = 0.86
I0815 19:03:45.957296 19104 caffe.cpp:313] Batch 581, loss = 1.75851
I0815 19:03:46.006544 19104 caffe.cpp:313] Batch 582, accuracy/top1 = 0.64
I0815 19:03:46.006569 19104 caffe.cpp:313] Batch 582, accuracy/top5 = 0.86
I0815 19:03:46.006574 19104 caffe.cpp:313] Batch 582, loss = 1.4092
I0815 19:03:46.055932 19104 caffe.cpp:313] Batch 583, accuracy/top1 = 0.64
I0815 19:03:46.055953 19104 caffe.cpp:313] Batch 583, accuracy/top5 = 0.82
I0815 19:03:46.055956 19104 caffe.cpp:313] Batch 583, loss = 1.57166
I0815 19:03:46.104346 19104 caffe.cpp:313] Batch 584, accuracy/top1 = 0.64
I0815 19:03:46.104367 19104 caffe.cpp:313] Batch 584, accuracy/top5 = 0.92
I0815 19:03:46.104372 19104 caffe.cpp:313] Batch 584, loss = 1.15761
I0815 19:03:46.154650 19104 caffe.cpp:313] Batch 585, accuracy/top1 = 0.7
I0815 19:03:46.154675 19104 caffe.cpp:313] Batch 585, accuracy/top5 = 0.86
I0815 19:03:46.154680 19104 caffe.cpp:313] Batch 585, loss = 1.31032
I0815 19:03:46.203364 19104 caffe.cpp:313] Batch 586, accuracy/top1 = 0.6
I0815 19:03:46.203387 19104 caffe.cpp:313] Batch 586, accuracy/top5 = 0.8
I0815 19:03:46.203390 19104 caffe.cpp:313] Batch 586, loss = 1.72924
I0815 19:03:46.251780 19104 caffe.cpp:313] Batch 587, accuracy/top1 = 0.62
I0815 19:03:46.251802 19104 caffe.cpp:313] Batch 587, accuracy/top5 = 0.9
I0815 19:03:46.251806 19104 caffe.cpp:313] Batch 587, loss = 1.39685
I0815 19:03:46.300701 19104 caffe.cpp:313] Batch 588, accuracy/top1 = 0.66
I0815 19:03:46.300724 19104 caffe.cpp:313] Batch 588, accuracy/top5 = 0.84
I0815 19:03:46.300727 19104 caffe.cpp:313] Batch 588, loss = 1.46495
I0815 19:03:46.350368 19104 caffe.cpp:313] Batch 589, accuracy/top1 = 0.64
I0815 19:03:46.350392 19104 caffe.cpp:313] Batch 589, accuracy/top5 = 0.9
I0815 19:03:46.350397 19104 caffe.cpp:313] Batch 589, loss = 1.60509
I0815 19:03:46.400684 19104 caffe.cpp:313] Batch 590, accuracy/top1 = 0.56
I0815 19:03:46.400703 19104 caffe.cpp:313] Batch 590, accuracy/top5 = 0.82
I0815 19:03:46.400707 19104 caffe.cpp:313] Batch 590, loss = 1.98036
I0815 19:03:46.451428 19104 caffe.cpp:313] Batch 591, accuracy/top1 = 0.6
I0815 19:03:46.451452 19104 caffe.cpp:313] Batch 591, accuracy/top5 = 0.78
I0815 19:03:46.451454 19104 caffe.cpp:313] Batch 591, loss = 1.65596
I0815 19:03:46.500236 19104 caffe.cpp:313] Batch 592, accuracy/top1 = 0.68
I0815 19:03:46.500274 19104 caffe.cpp:313] Batch 592, accuracy/top5 = 0.84
I0815 19:03:46.500278 19104 caffe.cpp:313] Batch 592, loss = 1.42929
I0815 19:03:46.549047 19104 caffe.cpp:313] Batch 593, accuracy/top1 = 0.5
I0815 19:03:46.549072 19104 caffe.cpp:313] Batch 593, accuracy/top5 = 0.74
I0815 19:03:46.549075 19104 caffe.cpp:313] Batch 593, loss = 2.26348
I0815 19:03:46.598141 19104 caffe.cpp:313] Batch 594, accuracy/top1 = 0.58
I0815 19:03:46.598166 19104 caffe.cpp:313] Batch 594, accuracy/top5 = 0.84
I0815 19:03:46.598170 19104 caffe.cpp:313] Batch 594, loss = 1.79473
I0815 19:03:46.647655 19104 caffe.cpp:313] Batch 595, accuracy/top1 = 0.62
I0815 19:03:46.647814 19104 caffe.cpp:313] Batch 595, accuracy/top5 = 0.84
I0815 19:03:46.647822 19104 caffe.cpp:313] Batch 595, loss = 1.50225
I0815 19:03:46.696496 19104 caffe.cpp:313] Batch 596, accuracy/top1 = 0.52
I0815 19:03:46.696521 19104 caffe.cpp:313] Batch 596, accuracy/top5 = 0.72
I0815 19:03:46.696524 19104 caffe.cpp:313] Batch 596, loss = 2.33242
I0815 19:03:46.744544 19104 caffe.cpp:313] Batch 597, accuracy/top1 = 0.74
I0815 19:03:46.744567 19104 caffe.cpp:313] Batch 597, accuracy/top5 = 0.88
I0815 19:03:46.744570 19104 caffe.cpp:313] Batch 597, loss = 1.15657
I0815 19:03:46.793094 19104 caffe.cpp:313] Batch 598, accuracy/top1 = 0.46
I0815 19:03:46.793120 19104 caffe.cpp:313] Batch 598, accuracy/top5 = 0.72
I0815 19:03:46.793123 19104 caffe.cpp:313] Batch 598, loss = 2.12429
I0815 19:03:46.841683 19104 caffe.cpp:313] Batch 599, accuracy/top1 = 0.56
I0815 19:03:46.841707 19104 caffe.cpp:313] Batch 599, accuracy/top5 = 0.76
I0815 19:03:46.841711 19104 caffe.cpp:313] Batch 599, loss = 1.95199
I0815 19:03:46.890295 19104 caffe.cpp:313] Batch 600, accuracy/top1 = 0.62
I0815 19:03:46.890319 19104 caffe.cpp:313] Batch 600, accuracy/top5 = 0.82
I0815 19:03:46.890322 19104 caffe.cpp:313] Batch 600, loss = 1.80976
I0815 19:03:46.939216 19104 caffe.cpp:313] Batch 601, accuracy/top1 = 0.56
I0815 19:03:46.939240 19104 caffe.cpp:313] Batch 601, accuracy/top5 = 0.9
I0815 19:03:46.939244 19104 caffe.cpp:313] Batch 601, loss = 1.42908
I0815 19:03:46.989540 19104 caffe.cpp:313] Batch 602, accuracy/top1 = 0.5
I0815 19:03:46.989565 19104 caffe.cpp:313] Batch 602, accuracy/top5 = 0.74
I0815 19:03:46.989568 19104 caffe.cpp:313] Batch 602, loss = 2.26806
I0815 19:03:47.039463 19104 caffe.cpp:313] Batch 603, accuracy/top1 = 0.66
I0815 19:03:47.039482 19104 caffe.cpp:313] Batch 603, accuracy/top5 = 0.84
I0815 19:03:47.039485 19104 caffe.cpp:313] Batch 603, loss = 1.27343
I0815 19:03:47.088135 19104 caffe.cpp:313] Batch 604, accuracy/top1 = 0.72
I0815 19:03:47.088151 19104 caffe.cpp:313] Batch 604, accuracy/top5 = 0.96
I0815 19:03:47.088155 19104 caffe.cpp:313] Batch 604, loss = 0.948548
I0815 19:03:47.136811 19104 caffe.cpp:313] Batch 605, accuracy/top1 = 0.54
I0815 19:03:47.136826 19104 caffe.cpp:313] Batch 605, accuracy/top5 = 0.68
I0815 19:03:47.136828 19104 caffe.cpp:313] Batch 605, loss = 2.12864
I0815 19:03:47.186022 19104 caffe.cpp:313] Batch 606, accuracy/top1 = 0.62
I0815 19:03:47.186046 19104 caffe.cpp:313] Batch 606, accuracy/top5 = 0.82
I0815 19:03:47.186049 19104 caffe.cpp:313] Batch 606, loss = 1.70309
I0815 19:03:47.235608 19104 caffe.cpp:313] Batch 607, accuracy/top1 = 0.52
I0815 19:03:47.235632 19104 caffe.cpp:313] Batch 607, accuracy/top5 = 0.78
I0815 19:03:47.235635 19104 caffe.cpp:313] Batch 607, loss = 1.84329
I0815 19:03:47.285006 19104 caffe.cpp:313] Batch 608, accuracy/top1 = 0.62
I0815 19:03:47.285030 19104 caffe.cpp:313] Batch 608, accuracy/top5 = 0.84
I0815 19:03:47.285033 19104 caffe.cpp:313] Batch 608, loss = 1.56698
I0815 19:03:47.334789 19104 caffe.cpp:313] Batch 609, accuracy/top1 = 0.62
I0815 19:03:47.334815 19104 caffe.cpp:313] Batch 609, accuracy/top5 = 0.8
I0815 19:03:47.334817 19104 caffe.cpp:313] Batch 609, loss = 1.96106
I0815 19:03:47.384606 19104 caffe.cpp:313] Batch 610, accuracy/top1 = 0.54
I0815 19:03:47.384627 19104 caffe.cpp:313] Batch 610, accuracy/top5 = 0.82
I0815 19:03:47.384630 19104 caffe.cpp:313] Batch 610, loss = 1.92317
I0815 19:03:47.433423 19104 caffe.cpp:313] Batch 611, accuracy/top1 = 0.64
I0815 19:03:47.433442 19104 caffe.cpp:313] Batch 611, accuracy/top5 = 0.8
I0815 19:03:47.433445 19104 caffe.cpp:313] Batch 611, loss = 1.68746
I0815 19:03:47.482506 19104 caffe.cpp:313] Batch 612, accuracy/top1 = 0.66
I0815 19:03:47.482529 19104 caffe.cpp:313] Batch 612, accuracy/top5 = 0.82
I0815 19:03:47.482532 19104 caffe.cpp:313] Batch 612, loss = 1.39454
I0815 19:03:47.530863 19104 caffe.cpp:313] Batch 613, accuracy/top1 = 0.54
I0815 19:03:47.530886 19104 caffe.cpp:313] Batch 613, accuracy/top5 = 0.74
I0815 19:03:47.530889 19104 caffe.cpp:313] Batch 613, loss = 2.16581
I0815 19:03:47.581065 19104 caffe.cpp:313] Batch 614, accuracy/top1 = 0.5
I0815 19:03:47.581105 19104 caffe.cpp:313] Batch 614, accuracy/top5 = 0.78
I0815 19:03:47.581110 19104 caffe.cpp:313] Batch 614, loss = 2.12194
I0815 19:03:47.631148 19104 caffe.cpp:313] Batch 615, accuracy/top1 = 0.42
I0815 19:03:47.631172 19104 caffe.cpp:313] Batch 615, accuracy/top5 = 0.82
I0815 19:03:47.631175 19104 caffe.cpp:313] Batch 615, loss = 2.06266
I0815 19:03:47.679687 19104 caffe.cpp:313] Batch 616, accuracy/top1 = 0.58
I0815 19:03:47.679713 19104 caffe.cpp:313] Batch 616, accuracy/top5 = 0.82
I0815 19:03:47.679715 19104 caffe.cpp:313] Batch 616, loss = 1.80626
I0815 19:03:47.727499 19104 caffe.cpp:313] Batch 617, accuracy/top1 = 0.6
I0815 19:03:47.727526 19104 caffe.cpp:313] Batch 617, accuracy/top5 = 0.78
I0815 19:03:47.727530 19104 caffe.cpp:313] Batch 617, loss = 1.86607
I0815 19:03:47.775315 19104 caffe.cpp:313] Batch 618, accuracy/top1 = 0.46
I0815 19:03:47.775342 19104 caffe.cpp:313] Batch 618, accuracy/top5 = 0.82
I0815 19:03:47.775347 19104 caffe.cpp:313] Batch 618, loss = 2.0262
I0815 19:03:47.824641 19104 caffe.cpp:313] Batch 619, accuracy/top1 = 0.68
I0815 19:03:47.824666 19104 caffe.cpp:313] Batch 619, accuracy/top5 = 0.86
I0815 19:03:47.824671 19104 caffe.cpp:313] Batch 619, loss = 1.29606
I0815 19:03:47.874523 19104 caffe.cpp:313] Batch 620, accuracy/top1 = 0.5
I0815 19:03:47.874550 19104 caffe.cpp:313] Batch 620, accuracy/top5 = 0.86
I0815 19:03:47.874554 19104 caffe.cpp:313] Batch 620, loss = 1.73942
I0815 19:03:47.923837 19104 caffe.cpp:313] Batch 621, accuracy/top1 = 0.58
I0815 19:03:47.923862 19104 caffe.cpp:313] Batch 621, accuracy/top5 = 0.84
I0815 19:03:47.923866 19104 caffe.cpp:313] Batch 621, loss = 1.85894
I0815 19:03:47.972863 19104 caffe.cpp:313] Batch 622, accuracy/top1 = 0.6
I0815 19:03:47.972887 19104 caffe.cpp:313] Batch 622, accuracy/top5 = 0.78
I0815 19:03:47.972892 19104 caffe.cpp:313] Batch 622, loss = 1.96036
I0815 19:03:48.021046 19104 caffe.cpp:313] Batch 623, accuracy/top1 = 0.5
I0815 19:03:48.021070 19104 caffe.cpp:313] Batch 623, accuracy/top5 = 0.86
I0815 19:03:48.021073 19104 caffe.cpp:313] Batch 623, loss = 1.8758
I0815 19:03:48.069257 19104 caffe.cpp:313] Batch 624, accuracy/top1 = 0.64
I0815 19:03:48.069278 19104 caffe.cpp:313] Batch 624, accuracy/top5 = 0.84
I0815 19:03:48.069281 19104 caffe.cpp:313] Batch 624, loss = 1.73092
I0815 19:03:48.119065 19104 caffe.cpp:313] Batch 625, accuracy/top1 = 0.58
I0815 19:03:48.119086 19104 caffe.cpp:313] Batch 625, accuracy/top5 = 0.82
I0815 19:03:48.119091 19104 caffe.cpp:313] Batch 625, loss = 2.28362
I0815 19:03:48.168892 19104 caffe.cpp:313] Batch 626, accuracy/top1 = 0.44
I0815 19:03:48.168908 19104 caffe.cpp:313] Batch 626, accuracy/top5 = 0.7
I0815 19:03:48.168912 19104 caffe.cpp:313] Batch 626, loss = 2.68779
I0815 19:03:48.218818 19104 caffe.cpp:313] Batch 627, accuracy/top1 = 0.54
I0815 19:03:48.218843 19104 caffe.cpp:313] Batch 627, accuracy/top5 = 0.78
I0815 19:03:48.218847 19104 caffe.cpp:313] Batch 627, loss = 2.201
I0815 19:03:48.268754 19104 caffe.cpp:313] Batch 628, accuracy/top1 = 0.62
I0815 19:03:48.268777 19104 caffe.cpp:313] Batch 628, accuracy/top5 = 0.82
I0815 19:03:48.268781 19104 caffe.cpp:313] Batch 628, loss = 2.03651
I0815 19:03:48.318836 19104 caffe.cpp:313] Batch 629, accuracy/top1 = 0.54
I0815 19:03:48.318858 19104 caffe.cpp:313] Batch 629, accuracy/top5 = 0.76
I0815 19:03:48.318862 19104 caffe.cpp:313] Batch 629, loss = 2.00992
I0815 19:03:48.369151 19104 caffe.cpp:313] Batch 630, accuracy/top1 = 0.5
I0815 19:03:48.369174 19104 caffe.cpp:313] Batch 630, accuracy/top5 = 0.66
I0815 19:03:48.369177 19104 caffe.cpp:313] Batch 630, loss = 2.45497
I0815 19:03:48.418673 19104 caffe.cpp:313] Batch 631, accuracy/top1 = 0.66
I0815 19:03:48.418694 19104 caffe.cpp:313] Batch 631, accuracy/top5 = 0.9
I0815 19:03:48.418697 19104 caffe.cpp:313] Batch 631, loss = 1.18192
I0815 19:03:48.469362 19104 caffe.cpp:313] Batch 632, accuracy/top1 = 0.62
I0815 19:03:48.469385 19104 caffe.cpp:313] Batch 632, accuracy/top5 = 0.86
I0815 19:03:48.469388 19104 caffe.cpp:313] Batch 632, loss = 1.50926
I0815 19:03:48.517503 19104 caffe.cpp:313] Batch 633, accuracy/top1 = 0.62
I0815 19:03:48.517529 19104 caffe.cpp:313] Batch 633, accuracy/top5 = 0.74
I0815 19:03:48.517532 19104 caffe.cpp:313] Batch 633, loss = 2.01222
I0815 19:03:48.565531 19104 caffe.cpp:313] Batch 634, accuracy/top1 = 0.58
I0815 19:03:48.565556 19104 caffe.cpp:313] Batch 634, accuracy/top5 = 0.78
I0815 19:03:48.565558 19104 caffe.cpp:313] Batch 634, loss = 2.27036
I0815 19:03:48.614132 19104 caffe.cpp:313] Batch 635, accuracy/top1 = 0.64
I0815 19:03:48.614158 19104 caffe.cpp:313] Batch 635, accuracy/top5 = 0.84
I0815 19:03:48.614161 19104 caffe.cpp:313] Batch 635, loss = 1.77205
I0815 19:03:48.663264 19104 caffe.cpp:313] Batch 636, accuracy/top1 = 0.56
I0815 19:03:48.663286 19104 caffe.cpp:313] Batch 636, accuracy/top5 = 0.78
I0815 19:03:48.663290 19104 caffe.cpp:313] Batch 636, loss = 2.11373
I0815 19:03:48.711354 19104 caffe.cpp:313] Batch 637, accuracy/top1 = 0.62
I0815 19:03:48.711377 19104 caffe.cpp:313] Batch 637, accuracy/top5 = 0.86
I0815 19:03:48.711380 19104 caffe.cpp:313] Batch 637, loss = 1.47771
I0815 19:03:48.760762 19104 caffe.cpp:313] Batch 638, accuracy/top1 = 0.6
I0815 19:03:48.760787 19104 caffe.cpp:313] Batch 638, accuracy/top5 = 0.86
I0815 19:03:48.760790 19104 caffe.cpp:313] Batch 638, loss = 1.71207
I0815 19:03:48.809340 19104 caffe.cpp:313] Batch 639, accuracy/top1 = 0.58
I0815 19:03:48.809365 19104 caffe.cpp:313] Batch 639, accuracy/top5 = 0.84
I0815 19:03:48.809367 19104 caffe.cpp:313] Batch 639, loss = 1.65299
I0815 19:03:48.858548 19104 caffe.cpp:313] Batch 640, accuracy/top1 = 0.54
I0815 19:03:48.858572 19104 caffe.cpp:313] Batch 640, accuracy/top5 = 0.8
I0815 19:03:48.858575 19104 caffe.cpp:313] Batch 640, loss = 1.93199
I0815 19:03:48.907840 19104 caffe.cpp:313] Batch 641, accuracy/top1 = 0.56
I0815 19:03:48.907863 19104 caffe.cpp:313] Batch 641, accuracy/top5 = 0.84
I0815 19:03:48.907867 19104 caffe.cpp:313] Batch 641, loss = 1.53426
I0815 19:03:48.956854 19104 caffe.cpp:313] Batch 642, accuracy/top1 = 0.6
I0815 19:03:48.956879 19104 caffe.cpp:313] Batch 642, accuracy/top5 = 0.82
I0815 19:03:48.956882 19104 caffe.cpp:313] Batch 642, loss = 1.85793
I0815 19:03:49.006171 19104 caffe.cpp:313] Batch 643, accuracy/top1 = 0.56
I0815 19:03:49.006196 19104 caffe.cpp:313] Batch 643, accuracy/top5 = 0.84
I0815 19:03:49.006199 19104 caffe.cpp:313] Batch 643, loss = 1.68968
I0815 19:03:49.054646 19104 caffe.cpp:313] Batch 644, accuracy/top1 = 0.56
I0815 19:03:49.054667 19104 caffe.cpp:313] Batch 644, accuracy/top5 = 0.76
I0815 19:03:49.054671 19104 caffe.cpp:313] Batch 644, loss = 1.93216
I0815 19:03:49.103236 19104 caffe.cpp:313] Batch 645, accuracy/top1 = 0.58
I0815 19:03:49.103251 19104 caffe.cpp:313] Batch 645, accuracy/top5 = 0.84
I0815 19:03:49.103255 19104 caffe.cpp:313] Batch 645, loss = 1.55956
I0815 19:03:49.152254 19104 caffe.cpp:313] Batch 646, accuracy/top1 = 0.6
I0815 19:03:49.152269 19104 caffe.cpp:313] Batch 646, accuracy/top5 = 0.86
I0815 19:03:49.152272 19104 caffe.cpp:313] Batch 646, loss = 1.59655
I0815 19:03:49.201061 19104 caffe.cpp:313] Batch 647, accuracy/top1 = 0.52
I0815 19:03:49.201092 19104 caffe.cpp:313] Batch 647, accuracy/top5 = 0.74
I0815 19:03:49.201097 19104 caffe.cpp:313] Batch 647, loss = 2.29097
I0815 19:03:49.250283 19104 caffe.cpp:313] Batch 648, accuracy/top1 = 0.62
I0815 19:03:49.250308 19104 caffe.cpp:313] Batch 648, accuracy/top5 = 0.84
I0815 19:03:49.250310 19104 caffe.cpp:313] Batch 648, loss = 1.36244
I0815 19:03:49.298769 19104 caffe.cpp:313] Batch 649, accuracy/top1 = 0.6
I0815 19:03:49.298795 19104 caffe.cpp:313] Batch 649, accuracy/top5 = 0.76
I0815 19:03:49.298799 19104 caffe.cpp:313] Batch 649, loss = 1.72409
I0815 19:03:49.347694 19104 caffe.cpp:313] Batch 650, accuracy/top1 = 0.58
I0815 19:03:49.347718 19104 caffe.cpp:313] Batch 650, accuracy/top5 = 0.78
I0815 19:03:49.347721 19104 caffe.cpp:313] Batch 650, loss = 1.59951
I0815 19:03:49.398175 19104 caffe.cpp:313] Batch 651, accuracy/top1 = 0.5
I0815 19:03:49.398193 19104 caffe.cpp:313] Batch 651, accuracy/top5 = 0.74
I0815 19:03:49.398212 19104 caffe.cpp:313] Batch 651, loss = 2.26295
I0815 19:03:49.447010 19104 caffe.cpp:313] Batch 652, accuracy/top1 = 0.56
I0815 19:03:49.447036 19104 caffe.cpp:313] Batch 652, accuracy/top5 = 0.74
I0815 19:03:49.447038 19104 caffe.cpp:313] Batch 652, loss = 2.18022
I0815 19:03:49.496641 19104 caffe.cpp:313] Batch 653, accuracy/top1 = 0.42
I0815 19:03:49.496666 19104 caffe.cpp:313] Batch 653, accuracy/top5 = 0.78
I0815 19:03:49.496670 19104 caffe.cpp:313] Batch 653, loss = 2.1209
I0815 19:03:49.544723 19104 caffe.cpp:313] Batch 654, accuracy/top1 = 0.56
I0815 19:03:49.544746 19104 caffe.cpp:313] Batch 654, accuracy/top5 = 0.86
I0815 19:03:49.544749 19104 caffe.cpp:313] Batch 654, loss = 1.84101
I0815 19:03:49.593430 19104 caffe.cpp:313] Batch 655, accuracy/top1 = 0.68
I0815 19:03:49.593454 19104 caffe.cpp:313] Batch 655, accuracy/top5 = 0.84
I0815 19:03:49.593457 19104 caffe.cpp:313] Batch 655, loss = 1.30428
I0815 19:03:49.641975 19104 caffe.cpp:313] Batch 656, accuracy/top1 = 0.62
I0815 19:03:49.641999 19104 caffe.cpp:313] Batch 656, accuracy/top5 = 0.9
I0815 19:03:49.642002 19104 caffe.cpp:313] Batch 656, loss = 1.52156
I0815 19:03:49.690752 19104 caffe.cpp:313] Batch 657, accuracy/top1 = 0.56
I0815 19:03:49.690776 19104 caffe.cpp:313] Batch 657, accuracy/top5 = 0.78
I0815 19:03:49.690779 19104 caffe.cpp:313] Batch 657, loss = 1.71036
I0815 19:03:49.738819 19104 caffe.cpp:313] Batch 658, accuracy/top1 = 0.54
I0815 19:03:49.738843 19104 caffe.cpp:313] Batch 658, accuracy/top5 = 0.78
I0815 19:03:49.738847 19104 caffe.cpp:313] Batch 658, loss = 1.79525
I0815 19:03:49.788115 19104 caffe.cpp:313] Batch 659, accuracy/top1 = 0.54
I0815 19:03:49.788142 19104 caffe.cpp:313] Batch 659, accuracy/top5 = 0.76
I0815 19:03:49.788146 19104 caffe.cpp:313] Batch 659, loss = 2.04579
I0815 19:03:49.836112 19104 caffe.cpp:313] Batch 660, accuracy/top1 = 0.66
I0815 19:03:49.836138 19104 caffe.cpp:313] Batch 660, accuracy/top5 = 0.8
I0815 19:03:49.836141 19104 caffe.cpp:313] Batch 660, loss = 1.49738
I0815 19:03:49.884637 19104 caffe.cpp:313] Batch 661, accuracy/top1 = 0.64
I0815 19:03:49.884662 19104 caffe.cpp:313] Batch 661, accuracy/top5 = 0.86
I0815 19:03:49.884665 19104 caffe.cpp:313] Batch 661, loss = 1.49594
I0815 19:03:49.933377 19104 caffe.cpp:313] Batch 662, accuracy/top1 = 0.48
I0815 19:03:49.933403 19104 caffe.cpp:313] Batch 662, accuracy/top5 = 0.74
I0815 19:03:49.933405 19104 caffe.cpp:313] Batch 662, loss = 1.97699
I0815 19:03:49.982234 19104 caffe.cpp:313] Batch 663, accuracy/top1 = 0.6
I0815 19:03:49.982259 19104 caffe.cpp:313] Batch 663, accuracy/top5 = 0.78
I0815 19:03:49.982261 19104 caffe.cpp:313] Batch 663, loss = 2.04717
I0815 19:03:50.031092 19104 caffe.cpp:313] Batch 664, accuracy/top1 = 0.56
I0815 19:03:50.031114 19104 caffe.cpp:313] Batch 664, accuracy/top5 = 0.8
I0815 19:03:50.031117 19104 caffe.cpp:313] Batch 664, loss = 1.87323
I0815 19:03:50.079203 19104 caffe.cpp:313] Batch 665, accuracy/top1 = 0.56
I0815 19:03:50.079222 19104 caffe.cpp:313] Batch 665, accuracy/top5 = 0.72
I0815 19:03:50.079226 19104 caffe.cpp:313] Batch 665, loss = 1.88256
I0815 19:03:50.128386 19104 caffe.cpp:313] Batch 666, accuracy/top1 = 0.66
I0815 19:03:50.128407 19104 caffe.cpp:313] Batch 666, accuracy/top5 = 0.82
I0815 19:03:50.128410 19104 caffe.cpp:313] Batch 666, loss = 1.7153
I0815 19:03:50.177994 19104 caffe.cpp:313] Batch 667, accuracy/top1 = 0.62
I0815 19:03:50.178011 19104 caffe.cpp:313] Batch 667, accuracy/top5 = 0.8
I0815 19:03:50.178014 19104 caffe.cpp:313] Batch 667, loss = 1.79926
I0815 19:03:50.227818 19104 caffe.cpp:313] Batch 668, accuracy/top1 = 0.58
I0815 19:03:50.227843 19104 caffe.cpp:313] Batch 668, accuracy/top5 = 0.76
I0815 19:03:50.227845 19104 caffe.cpp:313] Batch 668, loss = 1.98995
I0815 19:03:50.276392 19104 caffe.cpp:313] Batch 669, accuracy/top1 = 0.66
I0815 19:03:50.276417 19104 caffe.cpp:313] Batch 669, accuracy/top5 = 0.9
I0815 19:03:50.276420 19104 caffe.cpp:313] Batch 669, loss = 1.15807
I0815 19:03:50.324774 19104 caffe.cpp:313] Batch 670, accuracy/top1 = 0.6
I0815 19:03:50.324812 19104 caffe.cpp:313] Batch 670, accuracy/top5 = 0.8
I0815 19:03:50.324816 19104 caffe.cpp:313] Batch 670, loss = 1.72087
I0815 19:03:50.373275 19104 caffe.cpp:313] Batch 671, accuracy/top1 = 0.62
I0815 19:03:50.373311 19104 caffe.cpp:313] Batch 671, accuracy/top5 = 0.8
I0815 19:03:50.373313 19104 caffe.cpp:313] Batch 671, loss = 1.61148
I0815 19:03:50.422917 19104 caffe.cpp:313] Batch 672, accuracy/top1 = 0.54
I0815 19:03:50.422937 19104 caffe.cpp:313] Batch 672, accuracy/top5 = 0.84
I0815 19:03:50.422940 19104 caffe.cpp:313] Batch 672, loss = 2.00513
I0815 19:03:50.471541 19104 caffe.cpp:313] Batch 673, accuracy/top1 = 0.7
I0815 19:03:50.471563 19104 caffe.cpp:313] Batch 673, accuracy/top5 = 0.9
I0815 19:03:50.471566 19104 caffe.cpp:313] Batch 673, loss = 1.20502
I0815 19:03:50.520433 19104 caffe.cpp:313] Batch 674, accuracy/top1 = 0.72
I0815 19:03:50.520454 19104 caffe.cpp:313] Batch 674, accuracy/top5 = 0.9
I0815 19:03:50.520457 19104 caffe.cpp:313] Batch 674, loss = 1.06969
I0815 19:03:50.570086 19104 caffe.cpp:313] Batch 675, accuracy/top1 = 0.56
I0815 19:03:50.570113 19104 caffe.cpp:313] Batch 675, accuracy/top5 = 0.78
I0815 19:03:50.570119 19104 caffe.cpp:313] Batch 675, loss = 1.97034
I0815 19:03:50.617844 19104 caffe.cpp:313] Batch 676, accuracy/top1 = 0.54
I0815 19:03:50.617867 19104 caffe.cpp:313] Batch 676, accuracy/top5 = 0.78
I0815 19:03:50.617871 19104 caffe.cpp:313] Batch 676, loss = 1.83817
I0815 19:03:50.666359 19104 caffe.cpp:313] Batch 677, accuracy/top1 = 0.6
I0815 19:03:50.666384 19104 caffe.cpp:313] Batch 677, accuracy/top5 = 0.82
I0815 19:03:50.666388 19104 caffe.cpp:313] Batch 677, loss = 1.43369
I0815 19:03:50.715903 19104 caffe.cpp:313] Batch 678, accuracy/top1 = 0.58
I0815 19:03:50.715927 19104 caffe.cpp:313] Batch 678, accuracy/top5 = 0.82
I0815 19:03:50.715931 19104 caffe.cpp:313] Batch 678, loss = 1.92249
I0815 19:03:50.763903 19104 caffe.cpp:313] Batch 679, accuracy/top1 = 0.7
I0815 19:03:50.763928 19104 caffe.cpp:313] Batch 679, accuracy/top5 = 0.88
I0815 19:03:50.763931 19104 caffe.cpp:313] Batch 679, loss = 1.42956
I0815 19:03:50.813231 19104 caffe.cpp:313] Batch 680, accuracy/top1 = 0.66
I0815 19:03:50.813256 19104 caffe.cpp:313] Batch 680, accuracy/top5 = 0.9
I0815 19:03:50.813258 19104 caffe.cpp:313] Batch 680, loss = 1.27299
I0815 19:03:50.862296 19104 caffe.cpp:313] Batch 681, accuracy/top1 = 0.64
I0815 19:03:50.862321 19104 caffe.cpp:313] Batch 681, accuracy/top5 = 0.92
I0815 19:03:50.862326 19104 caffe.cpp:313] Batch 681, loss = 1.48148
I0815 19:03:50.911135 19104 caffe.cpp:313] Batch 682, accuracy/top1 = 0.54
I0815 19:03:50.911159 19104 caffe.cpp:313] Batch 682, accuracy/top5 = 0.82
I0815 19:03:50.911162 19104 caffe.cpp:313] Batch 682, loss = 1.82196
I0815 19:03:50.960021 19104 caffe.cpp:313] Batch 683, accuracy/top1 = 0.54
I0815 19:03:50.960047 19104 caffe.cpp:313] Batch 683, accuracy/top5 = 0.84
I0815 19:03:50.960049 19104 caffe.cpp:313] Batch 683, loss = 2.08651
I0815 19:03:51.009353 19104 caffe.cpp:313] Batch 684, accuracy/top1 = 0.52
I0815 19:03:51.009376 19104 caffe.cpp:313] Batch 684, accuracy/top5 = 0.78
I0815 19:03:51.009378 19104 caffe.cpp:313] Batch 684, loss = 2.10137
I0815 19:03:51.058264 19104 caffe.cpp:313] Batch 685, accuracy/top1 = 0.58
I0815 19:03:51.058280 19104 caffe.cpp:313] Batch 685, accuracy/top5 = 0.84
I0815 19:03:51.058284 19104 caffe.cpp:313] Batch 685, loss = 1.89342
I0815 19:03:51.107692 19104 caffe.cpp:313] Batch 686, accuracy/top1 = 0.5
I0815 19:03:51.107717 19104 caffe.cpp:313] Batch 686, accuracy/top5 = 0.7
I0815 19:03:51.107720 19104 caffe.cpp:313] Batch 686, loss = 1.98936
I0815 19:03:51.155302 19104 caffe.cpp:313] Batch 687, accuracy/top1 = 0.58
I0815 19:03:51.155315 19104 caffe.cpp:313] Batch 687, accuracy/top5 = 0.8
I0815 19:03:51.155318 19104 caffe.cpp:313] Batch 687, loss = 2.35887
I0815 19:03:51.203770 19104 caffe.cpp:313] Batch 688, accuracy/top1 = 0.58
I0815 19:03:51.203794 19104 caffe.cpp:313] Batch 688, accuracy/top5 = 0.84
I0815 19:03:51.203797 19104 caffe.cpp:313] Batch 688, loss = 1.72725
I0815 19:03:51.252313 19104 caffe.cpp:313] Batch 689, accuracy/top1 = 0.58
I0815 19:03:51.252357 19104 caffe.cpp:313] Batch 689, accuracy/top5 = 0.84
I0815 19:03:51.252360 19104 caffe.cpp:313] Batch 689, loss = 1.6081
I0815 19:03:51.301054 19104 caffe.cpp:313] Batch 690, accuracy/top1 = 0.52
I0815 19:03:51.301079 19104 caffe.cpp:313] Batch 690, accuracy/top5 = 0.8
I0815 19:03:51.301082 19104 caffe.cpp:313] Batch 690, loss = 1.94878
I0815 19:03:51.349385 19104 caffe.cpp:313] Batch 691, accuracy/top1 = 0.58
I0815 19:03:51.349411 19104 caffe.cpp:313] Batch 691, accuracy/top5 = 0.78
I0815 19:03:51.349413 19104 caffe.cpp:313] Batch 691, loss = 1.84617
I0815 19:03:51.399009 19104 caffe.cpp:313] Batch 692, accuracy/top1 = 0.5
I0815 19:03:51.399029 19104 caffe.cpp:313] Batch 692, accuracy/top5 = 0.82
I0815 19:03:51.399032 19104 caffe.cpp:313] Batch 692, loss = 2.04364
I0815 19:03:51.447532 19104 caffe.cpp:313] Batch 693, accuracy/top1 = 0.68
I0815 19:03:51.447556 19104 caffe.cpp:313] Batch 693, accuracy/top5 = 0.82
I0815 19:03:51.447559 19104 caffe.cpp:313] Batch 693, loss = 1.75531
I0815 19:03:51.497076 19104 caffe.cpp:313] Batch 694, accuracy/top1 = 0.56
I0815 19:03:51.497099 19104 caffe.cpp:313] Batch 694, accuracy/top5 = 0.82
I0815 19:03:51.497102 19104 caffe.cpp:313] Batch 694, loss = 1.73964
I0815 19:03:51.546358 19104 caffe.cpp:313] Batch 695, accuracy/top1 = 0.56
I0815 19:03:51.546383 19104 caffe.cpp:313] Batch 695, accuracy/top5 = 0.76
I0815 19:03:51.546387 19104 caffe.cpp:313] Batch 695, loss = 1.72721
I0815 19:03:51.595804 19104 caffe.cpp:313] Batch 696, accuracy/top1 = 0.48
I0815 19:03:51.595829 19104 caffe.cpp:313] Batch 696, accuracy/top5 = 0.76
I0815 19:03:51.595832 19104 caffe.cpp:313] Batch 696, loss = 2.01167
I0815 19:03:51.644421 19104 caffe.cpp:313] Batch 697, accuracy/top1 = 0.68
I0815 19:03:51.644448 19104 caffe.cpp:313] Batch 697, accuracy/top5 = 0.88
I0815 19:03:51.644450 19104 caffe.cpp:313] Batch 697, loss = 1.34342
I0815 19:03:51.692955 19104 caffe.cpp:313] Batch 698, accuracy/top1 = 0.56
I0815 19:03:51.692981 19104 caffe.cpp:313] Batch 698, accuracy/top5 = 0.8
I0815 19:03:51.692984 19104 caffe.cpp:313] Batch 698, loss = 2.20738
I0815 19:03:51.740252 19104 caffe.cpp:313] Batch 699, accuracy/top1 = 0.7
I0815 19:03:51.740276 19104 caffe.cpp:313] Batch 699, accuracy/top5 = 0.94
I0815 19:03:51.740280 19104 caffe.cpp:313] Batch 699, loss = 1.23178
I0815 19:03:51.789047 19104 caffe.cpp:313] Batch 700, accuracy/top1 = 0.58
I0815 19:03:51.789072 19104 caffe.cpp:313] Batch 700, accuracy/top5 = 0.82
I0815 19:03:51.789075 19104 caffe.cpp:313] Batch 700, loss = 2.19072
I0815 19:03:51.836920 19104 caffe.cpp:313] Batch 701, accuracy/top1 = 0.52
I0815 19:03:51.836946 19104 caffe.cpp:313] Batch 701, accuracy/top5 = 0.7
I0815 19:03:51.836948 19104 caffe.cpp:313] Batch 701, loss = 2.331
I0815 19:03:51.884934 19104 caffe.cpp:313] Batch 702, accuracy/top1 = 0.64
I0815 19:03:51.884959 19104 caffe.cpp:313] Batch 702, accuracy/top5 = 0.74
I0815 19:03:51.884963 19104 caffe.cpp:313] Batch 702, loss = 2.14895
I0815 19:03:51.933043 19104 caffe.cpp:313] Batch 703, accuracy/top1 = 0.68
I0815 19:03:51.933066 19104 caffe.cpp:313] Batch 703, accuracy/top5 = 0.92
I0815 19:03:51.933069 19104 caffe.cpp:313] Batch 703, loss = 1.32981
I0815 19:03:51.981601 19104 caffe.cpp:313] Batch 704, accuracy/top1 = 0.6
I0815 19:03:51.981624 19104 caffe.cpp:313] Batch 704, accuracy/top5 = 0.8
I0815 19:03:51.981627 19104 caffe.cpp:313] Batch 704, loss = 2.09896
I0815 19:03:52.029932 19104 caffe.cpp:313] Batch 705, accuracy/top1 = 0.68
I0815 19:03:52.029953 19104 caffe.cpp:313] Batch 705, accuracy/top5 = 0.86
I0815 19:03:52.029955 19104 caffe.cpp:313] Batch 705, loss = 1.30841
I0815 19:03:52.078524 19104 caffe.cpp:313] Batch 706, accuracy/top1 = 0.6
I0815 19:03:52.078542 19104 caffe.cpp:313] Batch 706, accuracy/top5 = 0.84
I0815 19:03:52.078546 19104 caffe.cpp:313] Batch 706, loss = 1.62789
I0815 19:03:52.126878 19104 caffe.cpp:313] Batch 707, accuracy/top1 = 0.62
I0815 19:03:52.126898 19104 caffe.cpp:313] Batch 707, accuracy/top5 = 0.84
I0815 19:03:52.126900 19104 caffe.cpp:313] Batch 707, loss = 1.66587
I0815 19:03:52.175143 19104 caffe.cpp:313] Batch 708, accuracy/top1 = 0.6
I0815 19:03:52.175163 19104 caffe.cpp:313] Batch 708, accuracy/top5 = 0.88
I0815 19:03:52.175166 19104 caffe.cpp:313] Batch 708, loss = 1.44405
I0815 19:03:52.223958 19104 caffe.cpp:313] Batch 709, accuracy/top1 = 0.56
I0815 19:03:52.223985 19104 caffe.cpp:313] Batch 709, accuracy/top5 = 0.78
I0815 19:03:52.223989 19104 caffe.cpp:313] Batch 709, loss = 2.0953
I0815 19:03:52.271749 19104 caffe.cpp:313] Batch 710, accuracy/top1 = 0.56
I0815 19:03:52.271775 19104 caffe.cpp:313] Batch 710, accuracy/top5 = 0.72
I0815 19:03:52.271777 19104 caffe.cpp:313] Batch 710, loss = 2.06903
I0815 19:03:52.320765 19104 caffe.cpp:313] Batch 711, accuracy/top1 = 0.54
I0815 19:03:52.320791 19104 caffe.cpp:313] Batch 711, accuracy/top5 = 0.8
I0815 19:03:52.320794 19104 caffe.cpp:313] Batch 711, loss = 1.91414
I0815 19:03:52.370631 19104 caffe.cpp:313] Batch 712, accuracy/top1 = 0.56
I0815 19:03:52.370657 19104 caffe.cpp:313] Batch 712, accuracy/top5 = 0.78
I0815 19:03:52.370661 19104 caffe.cpp:313] Batch 712, loss = 1.74835
I0815 19:03:52.421015 19104 caffe.cpp:313] Batch 713, accuracy/top1 = 0.48
I0815 19:03:52.421039 19104 caffe.cpp:313] Batch 713, accuracy/top5 = 0.72
I0815 19:03:52.421042 19104 caffe.cpp:313] Batch 713, loss = 2.3557
I0815 19:03:52.469030 19104 caffe.cpp:313] Batch 714, accuracy/top1 = 0.6
I0815 19:03:52.469053 19104 caffe.cpp:313] Batch 714, accuracy/top5 = 0.78
I0815 19:03:52.469056 19104 caffe.cpp:313] Batch 714, loss = 1.58062
I0815 19:03:52.517699 19104 caffe.cpp:313] Batch 715, accuracy/top1 = 0.48
I0815 19:03:52.517724 19104 caffe.cpp:313] Batch 715, accuracy/top5 = 0.88
I0815 19:03:52.517726 19104 caffe.cpp:313] Batch 715, loss = 1.93495
I0815 19:03:52.566443 19104 caffe.cpp:313] Batch 716, accuracy/top1 = 0.6
I0815 19:03:52.566468 19104 caffe.cpp:313] Batch 716, accuracy/top5 = 0.74
I0815 19:03:52.566470 19104 caffe.cpp:313] Batch 716, loss = 2.49731
I0815 19:03:52.615908 19104 caffe.cpp:313] Batch 717, accuracy/top1 = 0.74
I0815 19:03:52.615933 19104 caffe.cpp:313] Batch 717, accuracy/top5 = 0.88
I0815 19:03:52.615941 19104 caffe.cpp:313] Batch 717, loss = 1.38296
I0815 19:03:52.663303 19104 caffe.cpp:313] Batch 718, accuracy/top1 = 0.54
I0815 19:03:52.663323 19104 caffe.cpp:313] Batch 718, accuracy/top5 = 0.74
I0815 19:03:52.663327 19104 caffe.cpp:313] Batch 718, loss = 2.32113
I0815 19:03:52.713855 19104 caffe.cpp:313] Batch 719, accuracy/top1 = 0.64
I0815 19:03:52.713883 19104 caffe.cpp:313] Batch 719, accuracy/top5 = 0.84
I0815 19:03:52.713886 19104 caffe.cpp:313] Batch 719, loss = 1.7338
I0815 19:03:52.759670 19104 caffe.cpp:313] Batch 720, accuracy/top1 = 0.56
I0815 19:03:52.759694 19104 caffe.cpp:313] Batch 720, accuracy/top5 = 0.84
I0815 19:03:52.759697 19104 caffe.cpp:313] Batch 720, loss = 1.64557
I0815 19:03:52.808341 19104 caffe.cpp:313] Batch 721, accuracy/top1 = 0.62
I0815 19:03:52.808365 19104 caffe.cpp:313] Batch 721, accuracy/top5 = 0.82
I0815 19:03:52.808368 19104 caffe.cpp:313] Batch 721, loss = 1.87838
I0815 19:03:52.856802 19104 caffe.cpp:313] Batch 722, accuracy/top1 = 0.58
I0815 19:03:52.856827 19104 caffe.cpp:313] Batch 722, accuracy/top5 = 0.86
I0815 19:03:52.856829 19104 caffe.cpp:313] Batch 722, loss = 1.89215
I0815 19:03:52.905696 19104 caffe.cpp:313] Batch 723, accuracy/top1 = 0.52
I0815 19:03:52.905724 19104 caffe.cpp:313] Batch 723, accuracy/top5 = 0.82
I0815 19:03:52.905726 19104 caffe.cpp:313] Batch 723, loss = 1.80571
I0815 19:03:52.953043 19104 caffe.cpp:313] Batch 724, accuracy/top1 = 0.62
I0815 19:03:52.953068 19104 caffe.cpp:313] Batch 724, accuracy/top5 = 0.92
I0815 19:03:52.953071 19104 caffe.cpp:313] Batch 724, loss = 1.23398
I0815 19:03:53.001148 19104 caffe.cpp:313] Batch 725, accuracy/top1 = 0.62
I0815 19:03:53.001173 19104 caffe.cpp:313] Batch 725, accuracy/top5 = 0.92
I0815 19:03:53.001175 19104 caffe.cpp:313] Batch 725, loss = 1.33705
I0815 19:03:53.049952 19104 caffe.cpp:313] Batch 726, accuracy/top1 = 0.46
I0815 19:03:53.049974 19104 caffe.cpp:313] Batch 726, accuracy/top5 = 0.86
I0815 19:03:53.049993 19104 caffe.cpp:313] Batch 726, loss = 1.72246
I0815 19:03:53.098513 19104 caffe.cpp:313] Batch 727, accuracy/top1 = 0.6
I0815 19:03:53.098528 19104 caffe.cpp:313] Batch 727, accuracy/top5 = 0.84
I0815 19:03:53.098531 19104 caffe.cpp:313] Batch 727, loss = 1.78232
I0815 19:03:53.147413 19104 caffe.cpp:313] Batch 728, accuracy/top1 = 0.68
I0815 19:03:53.147429 19104 caffe.cpp:313] Batch 728, accuracy/top5 = 0.8
I0815 19:03:53.147433 19104 caffe.cpp:313] Batch 728, loss = 1.53381
I0815 19:03:53.195711 19104 caffe.cpp:313] Batch 729, accuracy/top1 = 0.72
I0815 19:03:53.195740 19104 caffe.cpp:313] Batch 729, accuracy/top5 = 0.88
I0815 19:03:53.195744 19104 caffe.cpp:313] Batch 729, loss = 1.24155
I0815 19:03:53.244482 19104 caffe.cpp:313] Batch 730, accuracy/top1 = 0.58
I0815 19:03:53.244505 19104 caffe.cpp:313] Batch 730, accuracy/top5 = 0.82
I0815 19:03:53.244508 19104 caffe.cpp:313] Batch 730, loss = 1.86416
I0815 19:03:53.291929 19104 caffe.cpp:313] Batch 731, accuracy/top1 = 0.54
I0815 19:03:53.291954 19104 caffe.cpp:313] Batch 731, accuracy/top5 = 0.8
I0815 19:03:53.291957 19104 caffe.cpp:313] Batch 731, loss = 1.92947
I0815 19:03:53.340421 19104 caffe.cpp:313] Batch 732, accuracy/top1 = 0.68
I0815 19:03:53.340445 19104 caffe.cpp:313] Batch 732, accuracy/top5 = 0.86
I0815 19:03:53.340447 19104 caffe.cpp:313] Batch 732, loss = 1.47391
I0815 19:03:53.389891 19104 caffe.cpp:313] Batch 733, accuracy/top1 = 0.46
I0815 19:03:53.389914 19104 caffe.cpp:313] Batch 733, accuracy/top5 = 0.78
I0815 19:03:53.389917 19104 caffe.cpp:313] Batch 733, loss = 2.17407
I0815 19:03:53.439580 19104 caffe.cpp:313] Batch 734, accuracy/top1 = 0.56
I0815 19:03:53.439605 19104 caffe.cpp:313] Batch 734, accuracy/top5 = 0.82
I0815 19:03:53.439610 19104 caffe.cpp:313] Batch 734, loss = 1.77572
I0815 19:03:53.488298 19104 caffe.cpp:313] Batch 735, accuracy/top1 = 0.62
I0815 19:03:53.488323 19104 caffe.cpp:313] Batch 735, accuracy/top5 = 0.76
I0815 19:03:53.488327 19104 caffe.cpp:313] Batch 735, loss = 1.89295
I0815 19:03:53.536650 19104 caffe.cpp:313] Batch 736, accuracy/top1 = 0.58
I0815 19:03:53.536675 19104 caffe.cpp:313] Batch 736, accuracy/top5 = 0.82
I0815 19:03:53.536679 19104 caffe.cpp:313] Batch 736, loss = 1.69235
I0815 19:03:53.585830 19104 caffe.cpp:313] Batch 737, accuracy/top1 = 0.62
I0815 19:03:53.585855 19104 caffe.cpp:313] Batch 737, accuracy/top5 = 0.88
I0815 19:03:53.585857 19104 caffe.cpp:313] Batch 737, loss = 1.74511
I0815 19:03:53.634837 19104 caffe.cpp:313] Batch 738, accuracy/top1 = 0.6
I0815 19:03:53.634862 19104 caffe.cpp:313] Batch 738, accuracy/top5 = 0.82
I0815 19:03:53.634865 19104 caffe.cpp:313] Batch 738, loss = 1.59445
I0815 19:03:53.682365 19104 caffe.cpp:313] Batch 739, accuracy/top1 = 0.64
I0815 19:03:53.682392 19104 caffe.cpp:313] Batch 739, accuracy/top5 = 0.86
I0815 19:03:53.682395 19104 caffe.cpp:313] Batch 739, loss = 1.60597
I0815 19:03:53.730891 19104 caffe.cpp:313] Batch 740, accuracy/top1 = 0.64
I0815 19:03:53.730916 19104 caffe.cpp:313] Batch 740, accuracy/top5 = 0.88
I0815 19:03:53.730919 19104 caffe.cpp:313] Batch 740, loss = 1.4045
I0815 19:03:53.779372 19104 caffe.cpp:313] Batch 741, accuracy/top1 = 0.66
I0815 19:03:53.779397 19104 caffe.cpp:313] Batch 741, accuracy/top5 = 0.86
I0815 19:03:53.779400 19104 caffe.cpp:313] Batch 741, loss = 1.54164
I0815 19:03:53.828629 19104 caffe.cpp:313] Batch 742, accuracy/top1 = 0.64
I0815 19:03:53.828655 19104 caffe.cpp:313] Batch 742, accuracy/top5 = 0.82
I0815 19:03:53.828657 19104 caffe.cpp:313] Batch 742, loss = 1.66434
I0815 19:03:53.877724 19104 caffe.cpp:313] Batch 743, accuracy/top1 = 0.52
I0815 19:03:53.877750 19104 caffe.cpp:313] Batch 743, accuracy/top5 = 0.8
I0815 19:03:53.877753 19104 caffe.cpp:313] Batch 743, loss = 1.93619
I0815 19:03:53.926434 19104 caffe.cpp:313] Batch 744, accuracy/top1 = 0.52
I0815 19:03:53.926458 19104 caffe.cpp:313] Batch 744, accuracy/top5 = 0.8
I0815 19:03:53.926462 19104 caffe.cpp:313] Batch 744, loss = 1.93865
I0815 19:03:53.975561 19104 caffe.cpp:313] Batch 745, accuracy/top1 = 0.64
I0815 19:03:53.975601 19104 caffe.cpp:313] Batch 745, accuracy/top5 = 0.82
I0815 19:03:53.975605 19104 caffe.cpp:313] Batch 745, loss = 1.39977
I0815 19:03:54.024418 19104 caffe.cpp:313] Batch 746, accuracy/top1 = 0.62
I0815 19:03:54.024438 19104 caffe.cpp:313] Batch 746, accuracy/top5 = 0.82
I0815 19:03:54.024441 19104 caffe.cpp:313] Batch 746, loss = 1.61552
I0815 19:03:54.073287 19104 caffe.cpp:313] Batch 747, accuracy/top1 = 0.56
I0815 19:03:54.073303 19104 caffe.cpp:313] Batch 747, accuracy/top5 = 0.78
I0815 19:03:54.073307 19104 caffe.cpp:313] Batch 747, loss = 1.72194
I0815 19:03:54.122050 19104 caffe.cpp:313] Batch 748, accuracy/top1 = 0.62
I0815 19:03:54.122074 19104 caffe.cpp:313] Batch 748, accuracy/top5 = 0.84
I0815 19:03:54.122077 19104 caffe.cpp:313] Batch 748, loss = 1.38799
I0815 19:03:54.170919 19104 caffe.cpp:313] Batch 749, accuracy/top1 = 0.66
I0815 19:03:54.170943 19104 caffe.cpp:313] Batch 749, accuracy/top5 = 0.82
I0815 19:03:54.170946 19104 caffe.cpp:313] Batch 749, loss = 1.34272
I0815 19:03:54.220302 19104 caffe.cpp:313] Batch 750, accuracy/top1 = 0.66
I0815 19:03:54.220325 19104 caffe.cpp:313] Batch 750, accuracy/top5 = 0.84
I0815 19:03:54.220329 19104 caffe.cpp:313] Batch 750, loss = 1.40473
I0815 19:03:54.268467 19104 caffe.cpp:313] Batch 751, accuracy/top1 = 0.6
I0815 19:03:54.268494 19104 caffe.cpp:313] Batch 751, accuracy/top5 = 0.82
I0815 19:03:54.268497 19104 caffe.cpp:313] Batch 751, loss = 1.64299
I0815 19:03:54.316819 19104 caffe.cpp:313] Batch 752, accuracy/top1 = 0.66
I0815 19:03:54.316843 19104 caffe.cpp:313] Batch 752, accuracy/top5 = 0.92
I0815 19:03:54.316848 19104 caffe.cpp:313] Batch 752, loss = 1.11636
I0815 19:03:54.366310 19104 caffe.cpp:313] Batch 753, accuracy/top1 = 0.46
I0815 19:03:54.366335 19104 caffe.cpp:313] Batch 753, accuracy/top5 = 0.82
I0815 19:03:54.366338 19104 caffe.cpp:313] Batch 753, loss = 2.0782
I0815 19:03:54.415711 19104 caffe.cpp:313] Batch 754, accuracy/top1 = 0.54
I0815 19:03:54.415731 19104 caffe.cpp:313] Batch 754, accuracy/top5 = 0.82
I0815 19:03:54.415735 19104 caffe.cpp:313] Batch 754, loss = 1.91264
I0815 19:03:54.464136 19104 caffe.cpp:313] Batch 755, accuracy/top1 = 0.48
I0815 19:03:54.464161 19104 caffe.cpp:313] Batch 755, accuracy/top5 = 0.78
I0815 19:03:54.464164 19104 caffe.cpp:313] Batch 755, loss = 2.27613
I0815 19:03:54.513324 19104 caffe.cpp:313] Batch 756, accuracy/top1 = 0.66
I0815 19:03:54.513348 19104 caffe.cpp:313] Batch 756, accuracy/top5 = 0.86
I0815 19:03:54.513351 19104 caffe.cpp:313] Batch 756, loss = 1.45088
I0815 19:03:54.562330 19104 caffe.cpp:313] Batch 757, accuracy/top1 = 0.58
I0815 19:03:54.562355 19104 caffe.cpp:313] Batch 757, accuracy/top5 = 0.86
I0815 19:03:54.562357 19104 caffe.cpp:313] Batch 757, loss = 1.38909
I0815 19:03:54.611239 19104 caffe.cpp:313] Batch 758, accuracy/top1 = 0.76
I0815 19:03:54.611264 19104 caffe.cpp:313] Batch 758, accuracy/top5 = 0.88
I0815 19:03:54.611268 19104 caffe.cpp:313] Batch 758, loss = 1.1825
I0815 19:03:54.660059 19104 caffe.cpp:313] Batch 759, accuracy/top1 = 0.72
I0815 19:03:54.660085 19104 caffe.cpp:313] Batch 759, accuracy/top5 = 0.82
I0815 19:03:54.660089 19104 caffe.cpp:313] Batch 759, loss = 1.76387
I0815 19:03:54.708386 19104 caffe.cpp:313] Batch 760, accuracy/top1 = 0.58
I0815 19:03:54.708411 19104 caffe.cpp:313] Batch 760, accuracy/top5 = 0.94
I0815 19:03:54.708415 19104 caffe.cpp:313] Batch 760, loss = 1.41209
I0815 19:03:54.757406 19104 caffe.cpp:313] Batch 761, accuracy/top1 = 0.68
I0815 19:03:54.757432 19104 caffe.cpp:313] Batch 761, accuracy/top5 = 0.84
I0815 19:03:54.757438 19104 caffe.cpp:313] Batch 761, loss = 1.63907
I0815 19:03:54.805235 19104 caffe.cpp:313] Batch 762, accuracy/top1 = 0.56
I0815 19:03:54.805256 19104 caffe.cpp:313] Batch 762, accuracy/top5 = 0.82
I0815 19:03:54.805259 19104 caffe.cpp:313] Batch 762, loss = 1.73671
I0815 19:03:54.852867 19104 caffe.cpp:313] Batch 763, accuracy/top1 = 0.56
I0815 19:03:54.852890 19104 caffe.cpp:313] Batch 763, accuracy/top5 = 0.88
I0815 19:03:54.852892 19104 caffe.cpp:313] Batch 763, loss = 1.49216
I0815 19:03:54.901790 19104 caffe.cpp:313] Batch 764, accuracy/top1 = 0.46
I0815 19:03:54.901815 19104 caffe.cpp:313] Batch 764, accuracy/top5 = 0.7
I0815 19:03:54.901818 19104 caffe.cpp:313] Batch 764, loss = 2.27833
I0815 19:03:54.950398 19104 caffe.cpp:313] Batch 765, accuracy/top1 = 0.66
I0815 19:03:54.950423 19104 caffe.cpp:313] Batch 765, accuracy/top5 = 0.88
I0815 19:03:54.950425 19104 caffe.cpp:313] Batch 765, loss = 1.51887
I0815 19:03:54.998878 19104 caffe.cpp:313] Batch 766, accuracy/top1 = 0.54
I0815 19:03:54.998903 19104 caffe.cpp:313] Batch 766, accuracy/top5 = 0.84
I0815 19:03:54.998905 19104 caffe.cpp:313] Batch 766, loss = 1.75016
I0815 19:03:55.047297 19104 caffe.cpp:313] Batch 767, accuracy/top1 = 0.62
I0815 19:03:55.047313 19104 caffe.cpp:313] Batch 767, accuracy/top5 = 0.82
I0815 19:03:55.047317 19104 caffe.cpp:313] Batch 767, loss = 1.66007
I0815 19:03:55.095654 19104 caffe.cpp:313] Batch 768, accuracy/top1 = 0.56
I0815 19:03:55.095669 19104 caffe.cpp:313] Batch 768, accuracy/top5 = 0.84
I0815 19:03:55.095671 19104 caffe.cpp:313] Batch 768, loss = 1.62282
I0815 19:03:55.144754 19104 caffe.cpp:313] Batch 769, accuracy/top1 = 0.64
I0815 19:03:55.144780 19104 caffe.cpp:313] Batch 769, accuracy/top5 = 0.84
I0815 19:03:55.144784 19104 caffe.cpp:313] Batch 769, loss = 1.41797
I0815 19:03:55.193656 19104 caffe.cpp:313] Batch 770, accuracy/top1 = 0.66
I0815 19:03:55.193681 19104 caffe.cpp:313] Batch 770, accuracy/top5 = 0.86
I0815 19:03:55.193684 19104 caffe.cpp:313] Batch 770, loss = 1.72547
I0815 19:03:55.242509 19104 caffe.cpp:313] Batch 771, accuracy/top1 = 0.62
I0815 19:03:55.242534 19104 caffe.cpp:313] Batch 771, accuracy/top5 = 0.78
I0815 19:03:55.242537 19104 caffe.cpp:313] Batch 771, loss = 2.40251
I0815 19:03:55.291108 19104 caffe.cpp:313] Batch 772, accuracy/top1 = 0.6
I0815 19:03:55.291132 19104 caffe.cpp:313] Batch 772, accuracy/top5 = 0.82
I0815 19:03:55.291136 19104 caffe.cpp:313] Batch 772, loss = 2.03003
I0815 19:03:55.339447 19104 caffe.cpp:313] Batch 773, accuracy/top1 = 0.44
I0815 19:03:55.339471 19104 caffe.cpp:313] Batch 773, accuracy/top5 = 0.74
I0815 19:03:55.339474 19104 caffe.cpp:313] Batch 773, loss = 2.4951
I0815 19:03:55.388870 19104 caffe.cpp:313] Batch 774, accuracy/top1 = 0.56
I0815 19:03:55.388890 19104 caffe.cpp:313] Batch 774, accuracy/top5 = 0.82
I0815 19:03:55.388893 19104 caffe.cpp:313] Batch 774, loss = 1.61696
I0815 19:03:55.437481 19104 caffe.cpp:313] Batch 775, accuracy/top1 = 0.64
I0815 19:03:55.437505 19104 caffe.cpp:313] Batch 775, accuracy/top5 = 0.8
I0815 19:03:55.437508 19104 caffe.cpp:313] Batch 775, loss = 1.65251
I0815 19:03:55.487066 19104 caffe.cpp:313] Batch 776, accuracy/top1 = 0.66
I0815 19:03:55.487089 19104 caffe.cpp:313] Batch 776, accuracy/top5 = 0.8
I0815 19:03:55.487092 19104 caffe.cpp:313] Batch 776, loss = 2.09904
I0815 19:03:55.536605 19104 caffe.cpp:313] Batch 777, accuracy/top1 = 0.56
I0815 19:03:55.536630 19104 caffe.cpp:313] Batch 777, accuracy/top5 = 0.84
I0815 19:03:55.536633 19104 caffe.cpp:313] Batch 777, loss = 1.8393
I0815 19:03:55.586258 19104 caffe.cpp:313] Batch 778, accuracy/top1 = 0.62
I0815 19:03:55.586283 19104 caffe.cpp:313] Batch 778, accuracy/top5 = 0.82
I0815 19:03:55.586287 19104 caffe.cpp:313] Batch 778, loss = 1.4519
I0815 19:03:55.634632 19104 caffe.cpp:313] Batch 779, accuracy/top1 = 0.62
I0815 19:03:55.634657 19104 caffe.cpp:313] Batch 779, accuracy/top5 = 0.8
I0815 19:03:55.634660 19104 caffe.cpp:313] Batch 779, loss = 1.93718
I0815 19:03:55.683445 19104 caffe.cpp:313] Batch 780, accuracy/top1 = 0.6
I0815 19:03:55.683470 19104 caffe.cpp:313] Batch 780, accuracy/top5 = 0.82
I0815 19:03:55.683473 19104 caffe.cpp:313] Batch 780, loss = 1.59082
I0815 19:03:55.732262 19104 caffe.cpp:313] Batch 781, accuracy/top1 = 0.62
I0815 19:03:55.732287 19104 caffe.cpp:313] Batch 781, accuracy/top5 = 0.84
I0815 19:03:55.732290 19104 caffe.cpp:313] Batch 781, loss = 1.48953
I0815 19:03:55.781124 19104 caffe.cpp:313] Batch 782, accuracy/top1 = 0.74
I0815 19:03:55.781149 19104 caffe.cpp:313] Batch 782, accuracy/top5 = 0.9
I0815 19:03:55.781152 19104 caffe.cpp:313] Batch 782, loss = 1.12816
I0815 19:03:55.830036 19104 caffe.cpp:313] Batch 783, accuracy/top1 = 0.64
I0815 19:03:55.830061 19104 caffe.cpp:313] Batch 783, accuracy/top5 = 0.86
I0815 19:03:55.830065 19104 caffe.cpp:313] Batch 783, loss = 1.69485
I0815 19:03:55.879580 19104 caffe.cpp:313] Batch 784, accuracy/top1 = 0.68
I0815 19:03:55.879604 19104 caffe.cpp:313] Batch 784, accuracy/top5 = 0.9
I0815 19:03:55.879607 19104 caffe.cpp:313] Batch 784, loss = 1.27473
I0815 19:03:55.928596 19104 caffe.cpp:313] Batch 785, accuracy/top1 = 0.56
I0815 19:03:55.928620 19104 caffe.cpp:313] Batch 785, accuracy/top5 = 0.8
I0815 19:03:55.928623 19104 caffe.cpp:313] Batch 785, loss = 1.91758
I0815 19:03:55.977370 19104 caffe.cpp:313] Batch 786, accuracy/top1 = 0.72
I0815 19:03:55.977393 19104 caffe.cpp:313] Batch 786, accuracy/top5 = 0.94
I0815 19:03:55.977398 19104 caffe.cpp:313] Batch 786, loss = 0.939702
I0815 19:03:56.026366 19104 caffe.cpp:313] Batch 787, accuracy/top1 = 0.48
I0815 19:03:56.026386 19104 caffe.cpp:313] Batch 787, accuracy/top5 = 0.88
I0815 19:03:56.026389 19104 caffe.cpp:313] Batch 787, loss = 1.50613
I0815 19:03:56.074908 19104 caffe.cpp:313] Batch 788, accuracy/top1 = 0.56
I0815 19:03:56.074928 19104 caffe.cpp:313] Batch 788, accuracy/top5 = 0.74
I0815 19:03:56.074930 19104 caffe.cpp:313] Batch 788, loss = 1.9839
I0815 19:03:56.122999 19104 caffe.cpp:313] Batch 789, accuracy/top1 = 0.68
I0815 19:03:56.123013 19104 caffe.cpp:313] Batch 789, accuracy/top5 = 0.86
I0815 19:03:56.123016 19104 caffe.cpp:313] Batch 789, loss = 1.39571
I0815 19:03:56.170611 19104 caffe.cpp:313] Batch 790, accuracy/top1 = 0.62
I0815 19:03:56.170625 19104 caffe.cpp:313] Batch 790, accuracy/top5 = 0.92
I0815 19:03:56.170629 19104 caffe.cpp:313] Batch 790, loss = 1.56313
I0815 19:03:56.220067 19104 caffe.cpp:313] Batch 791, accuracy/top1 = 0.52
I0815 19:03:56.220093 19104 caffe.cpp:313] Batch 791, accuracy/top5 = 0.76
I0815 19:03:56.220096 19104 caffe.cpp:313] Batch 791, loss = 2.0676
I0815 19:03:56.267138 19104 caffe.cpp:313] Batch 792, accuracy/top1 = 0.44
I0815 19:03:56.267161 19104 caffe.cpp:313] Batch 792, accuracy/top5 = 0.78
I0815 19:03:56.267164 19104 caffe.cpp:313] Batch 792, loss = 1.86228
I0815 19:03:56.315665 19104 caffe.cpp:313] Batch 793, accuracy/top1 = 0.6
I0815 19:03:56.315690 19104 caffe.cpp:313] Batch 793, accuracy/top5 = 0.82
I0815 19:03:56.315692 19104 caffe.cpp:313] Batch 793, loss = 1.77082
I0815 19:03:56.364588 19104 caffe.cpp:313] Batch 794, accuracy/top1 = 0.64
I0815 19:03:56.364612 19104 caffe.cpp:313] Batch 794, accuracy/top5 = 0.8
I0815 19:03:56.364615 19104 caffe.cpp:313] Batch 794, loss = 1.58243
I0815 19:03:56.413523 19104 caffe.cpp:313] Batch 795, accuracy/top1 = 0.56
I0815 19:03:56.413543 19104 caffe.cpp:313] Batch 795, accuracy/top5 = 0.8
I0815 19:03:56.413547 19104 caffe.cpp:313] Batch 795, loss = 1.9535
I0815 19:03:56.462441 19104 caffe.cpp:313] Batch 796, accuracy/top1 = 0.62
I0815 19:03:56.462466 19104 caffe.cpp:313] Batch 796, accuracy/top5 = 0.78
I0815 19:03:56.462469 19104 caffe.cpp:313] Batch 796, loss = 1.8742
I0815 19:03:56.510993 19104 caffe.cpp:313] Batch 797, accuracy/top1 = 0.58
I0815 19:03:56.511019 19104 caffe.cpp:313] Batch 797, accuracy/top5 = 0.78
I0815 19:03:56.511023 19104 caffe.cpp:313] Batch 797, loss = 1.97558
I0815 19:03:56.560340 19104 caffe.cpp:313] Batch 798, accuracy/top1 = 0.56
I0815 19:03:56.560365 19104 caffe.cpp:313] Batch 798, accuracy/top5 = 0.86
I0815 19:03:56.560369 19104 caffe.cpp:313] Batch 798, loss = 1.44184
I0815 19:03:56.608844 19104 caffe.cpp:313] Batch 799, accuracy/top1 = 0.66
I0815 19:03:56.608867 19104 caffe.cpp:313] Batch 799, accuracy/top5 = 0.84
I0815 19:03:56.608870 19104 caffe.cpp:313] Batch 799, loss = 1.74617
I0815 19:03:56.657876 19104 caffe.cpp:313] Batch 800, accuracy/top1 = 0.52
I0815 19:03:56.657902 19104 caffe.cpp:313] Batch 800, accuracy/top5 = 0.64
I0815 19:03:56.657904 19104 caffe.cpp:313] Batch 800, loss = 2.81273
I0815 19:03:56.706913 19104 caffe.cpp:313] Batch 801, accuracy/top1 = 0.62
I0815 19:03:56.706936 19104 caffe.cpp:313] Batch 801, accuracy/top5 = 0.82
I0815 19:03:56.706955 19104 caffe.cpp:313] Batch 801, loss = 1.64714
I0815 19:03:56.755121 19104 caffe.cpp:313] Batch 802, accuracy/top1 = 0.6
I0815 19:03:56.755146 19104 caffe.cpp:313] Batch 802, accuracy/top5 = 0.86
I0815 19:03:56.755149 19104 caffe.cpp:313] Batch 802, loss = 1.58027
I0815 19:03:56.804031 19104 caffe.cpp:313] Batch 803, accuracy/top1 = 0.62
I0815 19:03:56.804055 19104 caffe.cpp:313] Batch 803, accuracy/top5 = 0.78
I0815 19:03:56.804059 19104 caffe.cpp:313] Batch 803, loss = 1.59467
I0815 19:03:56.852290 19104 caffe.cpp:313] Batch 804, accuracy/top1 = 0.56
I0815 19:03:56.852313 19104 caffe.cpp:313] Batch 804, accuracy/top5 = 0.94
I0815 19:03:56.852316 19104 caffe.cpp:313] Batch 804, loss = 1.61488
I0815 19:03:56.900928 19104 caffe.cpp:313] Batch 805, accuracy/top1 = 0.68
I0815 19:03:56.900949 19104 caffe.cpp:313] Batch 805, accuracy/top5 = 0.86
I0815 19:03:56.900952 19104 caffe.cpp:313] Batch 805, loss = 1.65925
I0815 19:03:56.949061 19104 caffe.cpp:313] Batch 806, accuracy/top1 = 0.72
I0815 19:03:56.949082 19104 caffe.cpp:313] Batch 806, accuracy/top5 = 0.88
I0815 19:03:56.949085 19104 caffe.cpp:313] Batch 806, loss = 1.09325
I0815 19:03:56.997817 19104 caffe.cpp:313] Batch 807, accuracy/top1 = 0.56
I0815 19:03:56.997839 19104 caffe.cpp:313] Batch 807, accuracy/top5 = 0.84
I0815 19:03:56.997843 19104 caffe.cpp:313] Batch 807, loss = 1.93531
I0815 19:03:57.046824 19104 caffe.cpp:313] Batch 808, accuracy/top1 = 0.64
I0815 19:03:57.046844 19104 caffe.cpp:313] Batch 808, accuracy/top5 = 0.86
I0815 19:03:57.046846 19104 caffe.cpp:313] Batch 808, loss = 1.69432
I0815 19:03:57.095978 19104 caffe.cpp:313] Batch 809, accuracy/top1 = 0.44
I0815 19:03:57.095993 19104 caffe.cpp:313] Batch 809, accuracy/top5 = 0.74
I0815 19:03:57.095995 19104 caffe.cpp:313] Batch 809, loss = 2.56109
I0815 19:03:57.144173 19104 caffe.cpp:313] Batch 810, accuracy/top1 = 0.56
I0815 19:03:57.144187 19104 caffe.cpp:313] Batch 810, accuracy/top5 = 0.82
I0815 19:03:57.144191 19104 caffe.cpp:313] Batch 810, loss = 1.71398
I0815 19:03:57.193123 19104 caffe.cpp:313] Batch 811, accuracy/top1 = 0.58
I0815 19:03:57.193148 19104 caffe.cpp:313] Batch 811, accuracy/top5 = 0.84
I0815 19:03:57.193150 19104 caffe.cpp:313] Batch 811, loss = 2.01223
I0815 19:03:57.241585 19104 caffe.cpp:313] Batch 812, accuracy/top1 = 0.6
I0815 19:03:57.241611 19104 caffe.cpp:313] Batch 812, accuracy/top5 = 0.86
I0815 19:03:57.241614 19104 caffe.cpp:313] Batch 812, loss = 1.73547
I0815 19:03:57.289511 19104 caffe.cpp:313] Batch 813, accuracy/top1 = 0.58
I0815 19:03:57.289536 19104 caffe.cpp:313] Batch 813, accuracy/top5 = 0.78
I0815 19:03:57.289539 19104 caffe.cpp:313] Batch 813, loss = 1.93948
I0815 19:03:57.337591 19104 caffe.cpp:313] Batch 814, accuracy/top1 = 0.38
I0815 19:03:57.337616 19104 caffe.cpp:313] Batch 814, accuracy/top5 = 0.74
I0815 19:03:57.337620 19104 caffe.cpp:313] Batch 814, loss = 2.60454
I0815 19:03:57.386569 19104 caffe.cpp:313] Batch 815, accuracy/top1 = 0.54
I0815 19:03:57.386605 19104 caffe.cpp:313] Batch 815, accuracy/top5 = 0.92
I0815 19:03:57.386608 19104 caffe.cpp:313] Batch 815, loss = 1.39757
I0815 19:03:57.436216 19104 caffe.cpp:313] Batch 816, accuracy/top1 = 0.5
I0815 19:03:57.436241 19104 caffe.cpp:313] Batch 816, accuracy/top5 = 0.78
I0815 19:03:57.436244 19104 caffe.cpp:313] Batch 816, loss = 2.17071
I0815 19:03:57.485615 19104 caffe.cpp:313] Batch 817, accuracy/top1 = 0.68
I0815 19:03:57.485640 19104 caffe.cpp:313] Batch 817, accuracy/top5 = 0.88
I0815 19:03:57.485642 19104 caffe.cpp:313] Batch 817, loss = 1.66915
I0815 19:03:57.534518 19104 caffe.cpp:313] Batch 818, accuracy/top1 = 0.64
I0815 19:03:57.534543 19104 caffe.cpp:313] Batch 818, accuracy/top5 = 0.9
I0815 19:03:57.534545 19104 caffe.cpp:313] Batch 818, loss = 1.42406
I0815 19:03:57.582551 19104 caffe.cpp:313] Batch 819, accuracy/top1 = 0.72
I0815 19:03:57.582574 19104 caffe.cpp:313] Batch 819, accuracy/top5 = 0.84
I0815 19:03:57.582577 19104 caffe.cpp:313] Batch 819, loss = 1.58014
I0815 19:03:57.630918 19104 caffe.cpp:313] Batch 820, accuracy/top1 = 0.6
I0815 19:03:57.630956 19104 caffe.cpp:313] Batch 820, accuracy/top5 = 0.82
I0815 19:03:57.630960 19104 caffe.cpp:313] Batch 820, loss = 1.83465
I0815 19:03:57.679175 19104 caffe.cpp:313] Batch 821, accuracy/top1 = 0.56
I0815 19:03:57.679199 19104 caffe.cpp:313] Batch 821, accuracy/top5 = 0.84
I0815 19:03:57.679203 19104 caffe.cpp:313] Batch 821, loss = 1.74896
I0815 19:03:57.728253 19104 caffe.cpp:313] Batch 822, accuracy/top1 = 0.54
I0815 19:03:57.728281 19104 caffe.cpp:313] Batch 822, accuracy/top5 = 0.84
I0815 19:03:57.728286 19104 caffe.cpp:313] Batch 822, loss = 1.97044
I0815 19:03:57.777062 19104 caffe.cpp:313] Batch 823, accuracy/top1 = 0.68
I0815 19:03:57.777087 19104 caffe.cpp:313] Batch 823, accuracy/top5 = 0.92
I0815 19:03:57.777091 19104 caffe.cpp:313] Batch 823, loss = 1.36482
I0815 19:03:57.825601 19104 caffe.cpp:313] Batch 824, accuracy/top1 = 0.66
I0815 19:03:57.825626 19104 caffe.cpp:313] Batch 824, accuracy/top5 = 0.82
I0815 19:03:57.825629 19104 caffe.cpp:313] Batch 824, loss = 1.71762
I0815 19:03:57.875514 19104 caffe.cpp:313] Batch 825, accuracy/top1 = 0.66
I0815 19:03:57.875538 19104 caffe.cpp:313] Batch 825, accuracy/top5 = 0.82
I0815 19:03:57.875541 19104 caffe.cpp:313] Batch 825, loss = 1.42388
I0815 19:03:57.923337 19104 caffe.cpp:313] Batch 826, accuracy/top1 = 0.58
I0815 19:03:57.923362 19104 caffe.cpp:313] Batch 826, accuracy/top5 = 0.86
I0815 19:03:57.923364 19104 caffe.cpp:313] Batch 826, loss = 1.72961
I0815 19:03:57.971879 19104 caffe.cpp:313] Batch 827, accuracy/top1 = 0.62
I0815 19:03:57.971902 19104 caffe.cpp:313] Batch 827, accuracy/top5 = 0.92
I0815 19:03:57.971905 19104 caffe.cpp:313] Batch 827, loss = 1.27004
I0815 19:03:58.020624 19104 caffe.cpp:313] Batch 828, accuracy/top1 = 0.56
I0815 19:03:58.020647 19104 caffe.cpp:313] Batch 828, accuracy/top5 = 0.78
I0815 19:03:58.020649 19104 caffe.cpp:313] Batch 828, loss = 2.02376
I0815 19:03:58.069380 19104 caffe.cpp:313] Batch 829, accuracy/top1 = 0.52
I0815 19:03:58.069396 19104 caffe.cpp:313] Batch 829, accuracy/top5 = 0.76
I0815 19:03:58.069398 19104 caffe.cpp:313] Batch 829, loss = 2.66922
I0815 19:03:58.118156 19104 caffe.cpp:313] Batch 830, accuracy/top1 = 0.66
I0815 19:03:58.118181 19104 caffe.cpp:313] Batch 830, accuracy/top5 = 0.84
I0815 19:03:58.118185 19104 caffe.cpp:313] Batch 830, loss = 1.39586
I0815 19:03:58.167220 19104 caffe.cpp:313] Batch 831, accuracy/top1 = 0.64
I0815 19:03:58.167245 19104 caffe.cpp:313] Batch 831, accuracy/top5 = 0.88
I0815 19:03:58.167248 19104 caffe.cpp:313] Batch 831, loss = 1.57388
I0815 19:03:58.217483 19104 caffe.cpp:313] Batch 832, accuracy/top1 = 0.54
I0815 19:03:58.217501 19104 caffe.cpp:313] Batch 832, accuracy/top5 = 0.78
I0815 19:03:58.217505 19104 caffe.cpp:313] Batch 832, loss = 1.97403
I0815 19:03:58.266216 19104 caffe.cpp:313] Batch 833, accuracy/top1 = 0.62
I0815 19:03:58.266242 19104 caffe.cpp:313] Batch 833, accuracy/top5 = 0.86
I0815 19:03:58.266244 19104 caffe.cpp:313] Batch 833, loss = 1.41602
I0815 19:03:58.313958 19104 caffe.cpp:313] Batch 834, accuracy/top1 = 0.6
I0815 19:03:58.313982 19104 caffe.cpp:313] Batch 834, accuracy/top5 = 0.84
I0815 19:03:58.313985 19104 caffe.cpp:313] Batch 834, loss = 2.00042
I0815 19:03:58.362413 19104 caffe.cpp:313] Batch 835, accuracy/top1 = 0.6
I0815 19:03:58.362439 19104 caffe.cpp:313] Batch 835, accuracy/top5 = 0.8
I0815 19:03:58.362442 19104 caffe.cpp:313] Batch 835, loss = 1.89937
I0815 19:03:58.411070 19104 caffe.cpp:313] Batch 836, accuracy/top1 = 0.68
I0815 19:03:58.411090 19104 caffe.cpp:313] Batch 836, accuracy/top5 = 0.86
I0815 19:03:58.411093 19104 caffe.cpp:313] Batch 836, loss = 1.40188
I0815 19:03:58.459951 19104 caffe.cpp:313] Batch 837, accuracy/top1 = 0.66
I0815 19:03:58.459976 19104 caffe.cpp:313] Batch 837, accuracy/top5 = 0.86
I0815 19:03:58.459980 19104 caffe.cpp:313] Batch 837, loss = 1.35803
I0815 19:03:58.508764 19104 caffe.cpp:313] Batch 838, accuracy/top1 = 0.52
I0815 19:03:58.508787 19104 caffe.cpp:313] Batch 838, accuracy/top5 = 0.84
I0815 19:03:58.508790 19104 caffe.cpp:313] Batch 838, loss = 1.74697
I0815 19:03:58.556473 19104 caffe.cpp:313] Batch 839, accuracy/top1 = 0.64
I0815 19:03:58.556498 19104 caffe.cpp:313] Batch 839, accuracy/top5 = 0.76
I0815 19:03:58.556500 19104 caffe.cpp:313] Batch 839, loss = 1.86308
I0815 19:03:58.605118 19104 caffe.cpp:313] Batch 840, accuracy/top1 = 0.56
I0815 19:03:58.605141 19104 caffe.cpp:313] Batch 840, accuracy/top5 = 0.76
I0815 19:03:58.605144 19104 caffe.cpp:313] Batch 840, loss = 1.91136
I0815 19:03:58.653950 19104 caffe.cpp:313] Batch 841, accuracy/top1 = 0.56
I0815 19:03:58.653975 19104 caffe.cpp:313] Batch 841, accuracy/top5 = 0.78
I0815 19:03:58.653978 19104 caffe.cpp:313] Batch 841, loss = 1.8415
I0815 19:03:58.702908 19104 caffe.cpp:313] Batch 842, accuracy/top1 = 0.6
I0815 19:03:58.702931 19104 caffe.cpp:313] Batch 842, accuracy/top5 = 0.88
I0815 19:03:58.702934 19104 caffe.cpp:313] Batch 842, loss = 1.55796
I0815 19:03:58.751190 19104 caffe.cpp:313] Batch 843, accuracy/top1 = 0.5
I0815 19:03:58.751215 19104 caffe.cpp:313] Batch 843, accuracy/top5 = 0.86
I0815 19:03:58.751219 19104 caffe.cpp:313] Batch 843, loss = 1.74143
I0815 19:03:58.800009 19104 caffe.cpp:313] Batch 844, accuracy/top1 = 0.6
I0815 19:03:58.800032 19104 caffe.cpp:313] Batch 844, accuracy/top5 = 0.86
I0815 19:03:58.800036 19104 caffe.cpp:313] Batch 844, loss = 1.52134
I0815 19:03:58.848042 19104 caffe.cpp:313] Batch 845, accuracy/top1 = 0.62
I0815 19:03:58.848067 19104 caffe.cpp:313] Batch 845, accuracy/top5 = 0.78
I0815 19:03:58.848070 19104 caffe.cpp:313] Batch 845, loss = 1.87534
I0815 19:03:58.896914 19104 caffe.cpp:313] Batch 846, accuracy/top1 = 0.58
I0815 19:03:58.896939 19104 caffe.cpp:313] Batch 846, accuracy/top5 = 0.82
I0815 19:03:58.896941 19104 caffe.cpp:313] Batch 846, loss = 1.80372
I0815 19:03:58.945487 19104 caffe.cpp:313] Batch 847, accuracy/top1 = 0.52
I0815 19:03:58.945511 19104 caffe.cpp:313] Batch 847, accuracy/top5 = 0.78
I0815 19:03:58.945514 19104 caffe.cpp:313] Batch 847, loss = 2.2629
I0815 19:03:58.994664 19104 caffe.cpp:313] Batch 848, accuracy/top1 = 0.62
I0815 19:03:58.994689 19104 caffe.cpp:313] Batch 848, accuracy/top5 = 0.82
I0815 19:03:58.994693 19104 caffe.cpp:313] Batch 848, loss = 1.78391
I0815 19:03:59.043406 19104 caffe.cpp:313] Batch 849, accuracy/top1 = 0.62
I0815 19:03:59.043428 19104 caffe.cpp:313] Batch 849, accuracy/top5 = 0.92
I0815 19:03:59.043431 19104 caffe.cpp:313] Batch 849, loss = 1.46538
I0815 19:03:59.091853 19104 caffe.cpp:313] Batch 850, accuracy/top1 = 0.62
I0815 19:03:59.091881 19104 caffe.cpp:313] Batch 850, accuracy/top5 = 0.86
I0815 19:03:59.091886 19104 caffe.cpp:313] Batch 850, loss = 1.71596
I0815 19:03:59.140058 19104 caffe.cpp:313] Batch 851, accuracy/top1 = 0.54
I0815 19:03:59.140084 19104 caffe.cpp:313] Batch 851, accuracy/top5 = 0.86
I0815 19:03:59.140089 19104 caffe.cpp:313] Batch 851, loss = 1.8944
I0815 19:03:59.187894 19104 caffe.cpp:313] Batch 852, accuracy/top1 = 0.52
I0815 19:03:59.187918 19104 caffe.cpp:313] Batch 852, accuracy/top5 = 0.76
I0815 19:03:59.187922 19104 caffe.cpp:313] Batch 852, loss = 1.92907
I0815 19:03:59.237118 19104 caffe.cpp:313] Batch 853, accuracy/top1 = 0.72
I0815 19:03:59.237143 19104 caffe.cpp:313] Batch 853, accuracy/top5 = 0.88
I0815 19:03:59.237146 19104 caffe.cpp:313] Batch 853, loss = 1.31956
I0815 19:03:59.285658 19104 caffe.cpp:313] Batch 854, accuracy/top1 = 0.7
I0815 19:03:59.285683 19104 caffe.cpp:313] Batch 854, accuracy/top5 = 0.82
I0815 19:03:59.285686 19104 caffe.cpp:313] Batch 854, loss = 1.58217
I0815 19:03:59.334103 19104 caffe.cpp:313] Batch 855, accuracy/top1 = 0.46
I0815 19:03:59.334128 19104 caffe.cpp:313] Batch 855, accuracy/top5 = 0.76
I0815 19:03:59.334131 19104 caffe.cpp:313] Batch 855, loss = 2.29089
I0815 19:03:59.384033 19104 caffe.cpp:313] Batch 856, accuracy/top1 = 0.56
I0815 19:03:59.384054 19104 caffe.cpp:313] Batch 856, accuracy/top5 = 0.84
I0815 19:03:59.384057 19104 caffe.cpp:313] Batch 856, loss = 1.82797
I0815 19:03:59.432991 19104 caffe.cpp:313] Batch 857, accuracy/top1 = 0.62
I0815 19:03:59.433017 19104 caffe.cpp:313] Batch 857, accuracy/top5 = 0.9
I0815 19:03:59.433037 19104 caffe.cpp:313] Batch 857, loss = 1.36335
I0815 19:03:59.480478 19104 caffe.cpp:313] Batch 858, accuracy/top1 = 0.6
I0815 19:03:59.480504 19104 caffe.cpp:313] Batch 858, accuracy/top5 = 0.94
I0815 19:03:59.480507 19104 caffe.cpp:313] Batch 858, loss = 1.36502
I0815 19:03:59.528764 19104 caffe.cpp:313] Batch 859, accuracy/top1 = 0.6
I0815 19:03:59.528787 19104 caffe.cpp:313] Batch 859, accuracy/top5 = 0.8
I0815 19:03:59.528790 19104 caffe.cpp:313] Batch 859, loss = 1.9378
I0815 19:03:59.578339 19104 caffe.cpp:313] Batch 860, accuracy/top1 = 0.6
I0815 19:03:59.578364 19104 caffe.cpp:313] Batch 860, accuracy/top5 = 0.78
I0815 19:03:59.578367 19104 caffe.cpp:313] Batch 860, loss = 1.73148
I0815 19:03:59.626693 19104 caffe.cpp:313] Batch 861, accuracy/top1 = 0.7
I0815 19:03:59.626718 19104 caffe.cpp:313] Batch 861, accuracy/top5 = 0.84
I0815 19:03:59.626720 19104 caffe.cpp:313] Batch 861, loss = 1.44962
I0815 19:03:59.675317 19104 caffe.cpp:313] Batch 862, accuracy/top1 = 0.6
I0815 19:03:59.675341 19104 caffe.cpp:313] Batch 862, accuracy/top5 = 0.82
I0815 19:03:59.675345 19104 caffe.cpp:313] Batch 862, loss = 1.55195
I0815 19:03:59.723882 19104 caffe.cpp:313] Batch 863, accuracy/top1 = 0.6
I0815 19:03:59.723906 19104 caffe.cpp:313] Batch 863, accuracy/top5 = 0.78
I0815 19:03:59.723909 19104 caffe.cpp:313] Batch 863, loss = 1.68936
I0815 19:03:59.772419 19104 caffe.cpp:313] Batch 864, accuracy/top1 = 0.66
I0815 19:03:59.772444 19104 caffe.cpp:313] Batch 864, accuracy/top5 = 0.88
I0815 19:03:59.772449 19104 caffe.cpp:313] Batch 864, loss = 1.35825
I0815 19:03:59.821732 19104 caffe.cpp:313] Batch 865, accuracy/top1 = 0.62
I0815 19:03:59.821756 19104 caffe.cpp:313] Batch 865, accuracy/top5 = 0.8
I0815 19:03:59.821759 19104 caffe.cpp:313] Batch 865, loss = 1.93699
I0815 19:03:59.870515 19104 caffe.cpp:313] Batch 866, accuracy/top1 = 0.6
I0815 19:03:59.870540 19104 caffe.cpp:313] Batch 866, accuracy/top5 = 0.86
I0815 19:03:59.870543 19104 caffe.cpp:313] Batch 866, loss = 1.48121
I0815 19:03:59.920208 19104 caffe.cpp:313] Batch 867, accuracy/top1 = 0.56
I0815 19:03:59.920233 19104 caffe.cpp:313] Batch 867, accuracy/top5 = 0.74
I0815 19:03:59.920236 19104 caffe.cpp:313] Batch 867, loss = 2.04407
I0815 19:03:59.969540 19104 caffe.cpp:313] Batch 868, accuracy/top1 = 0.58
I0815 19:03:59.969565 19104 caffe.cpp:313] Batch 868, accuracy/top5 = 0.82
I0815 19:03:59.969569 19104 caffe.cpp:313] Batch 868, loss = 1.95355
I0815 19:04:00.018148 19104 caffe.cpp:313] Batch 869, accuracy/top1 = 0.52
I0815 19:04:00.018172 19104 caffe.cpp:313] Batch 869, accuracy/top5 = 0.78
I0815 19:04:00.018175 19104 caffe.cpp:313] Batch 869, loss = 1.97022
I0815 19:04:00.066890 19104 caffe.cpp:313] Batch 870, accuracy/top1 = 0.54
I0815 19:04:00.066910 19104 caffe.cpp:313] Batch 870, accuracy/top5 = 0.72
I0815 19:04:00.066912 19104 caffe.cpp:313] Batch 870, loss = 2.18846
I0815 19:04:00.115969 19104 caffe.cpp:313] Batch 871, accuracy/top1 = 0.64
I0815 19:04:00.115988 19104 caffe.cpp:313] Batch 871, accuracy/top5 = 0.88
I0815 19:04:00.115990 19104 caffe.cpp:313] Batch 871, loss = 1.53984
I0815 19:04:00.163568 19104 caffe.cpp:313] Batch 872, accuracy/top1 = 0.56
I0815 19:04:00.163583 19104 caffe.cpp:313] Batch 872, accuracy/top5 = 0.8
I0815 19:04:00.163586 19104 caffe.cpp:313] Batch 872, loss = 1.80262
I0815 19:04:00.213177 19104 caffe.cpp:313] Batch 873, accuracy/top1 = 0.58
I0815 19:04:00.213203 19104 caffe.cpp:313] Batch 873, accuracy/top5 = 0.86
I0815 19:04:00.213207 19104 caffe.cpp:313] Batch 873, loss = 1.74909
I0815 19:04:00.262132 19104 caffe.cpp:313] Batch 874, accuracy/top1 = 0.72
I0815 19:04:00.262176 19104 caffe.cpp:313] Batch 874, accuracy/top5 = 0.82
I0815 19:04:00.262184 19104 caffe.cpp:313] Batch 874, loss = 1.66746
I0815 19:04:00.310905 19104 caffe.cpp:313] Batch 875, accuracy/top1 = 0.66
I0815 19:04:00.310931 19104 caffe.cpp:313] Batch 875, accuracy/top5 = 0.84
I0815 19:04:00.310933 19104 caffe.cpp:313] Batch 875, loss = 1.56515
I0815 19:04:00.359191 19104 caffe.cpp:313] Batch 876, accuracy/top1 = 0.64
I0815 19:04:00.359215 19104 caffe.cpp:313] Batch 876, accuracy/top5 = 0.84
I0815 19:04:00.359235 19104 caffe.cpp:313] Batch 876, loss = 1.37068
I0815 19:04:00.409282 19104 caffe.cpp:313] Batch 877, accuracy/top1 = 0.58
I0815 19:04:00.409302 19104 caffe.cpp:313] Batch 877, accuracy/top5 = 0.82
I0815 19:04:00.409306 19104 caffe.cpp:313] Batch 877, loss = 1.64551
I0815 19:04:00.457895 19104 caffe.cpp:313] Batch 878, accuracy/top1 = 0.56
I0815 19:04:00.457919 19104 caffe.cpp:313] Batch 878, accuracy/top5 = 0.76
I0815 19:04:00.457922 19104 caffe.cpp:313] Batch 878, loss = 2.1686
I0815 19:04:00.506739 19104 caffe.cpp:313] Batch 879, accuracy/top1 = 0.6
I0815 19:04:00.506763 19104 caffe.cpp:313] Batch 879, accuracy/top5 = 0.84
I0815 19:04:00.506767 19104 caffe.cpp:313] Batch 879, loss = 1.4516
I0815 19:04:00.554443 19104 caffe.cpp:313] Batch 880, accuracy/top1 = 0.64
I0815 19:04:00.554467 19104 caffe.cpp:313] Batch 880, accuracy/top5 = 0.88
I0815 19:04:00.554471 19104 caffe.cpp:313] Batch 880, loss = 1.38118
I0815 19:04:00.602679 19104 caffe.cpp:313] Batch 881, accuracy/top1 = 0.54
I0815 19:04:00.602705 19104 caffe.cpp:313] Batch 881, accuracy/top5 = 0.86
I0815 19:04:00.602707 19104 caffe.cpp:313] Batch 881, loss = 1.77862
I0815 19:04:00.651264 19104 caffe.cpp:313] Batch 882, accuracy/top1 = 0.64
I0815 19:04:00.651289 19104 caffe.cpp:313] Batch 882, accuracy/top5 = 0.88
I0815 19:04:00.651293 19104 caffe.cpp:313] Batch 882, loss = 1.28744
I0815 19:04:00.700068 19104 caffe.cpp:313] Batch 883, accuracy/top1 = 0.62
I0815 19:04:00.700093 19104 caffe.cpp:313] Batch 883, accuracy/top5 = 0.82
I0815 19:04:00.700096 19104 caffe.cpp:313] Batch 883, loss = 1.9138
I0815 19:04:00.749114 19104 caffe.cpp:313] Batch 884, accuracy/top1 = 0.44
I0815 19:04:00.749137 19104 caffe.cpp:313] Batch 884, accuracy/top5 = 0.76
I0815 19:04:00.749141 19104 caffe.cpp:313] Batch 884, loss = 2.02955
I0815 19:04:00.798387 19104 caffe.cpp:313] Batch 885, accuracy/top1 = 0.56
I0815 19:04:00.798413 19104 caffe.cpp:313] Batch 885, accuracy/top5 = 0.9
I0815 19:04:00.798415 19104 caffe.cpp:313] Batch 885, loss = 1.63837
I0815 19:04:00.845787 19104 caffe.cpp:313] Batch 886, accuracy/top1 = 0.68
I0815 19:04:00.845810 19104 caffe.cpp:313] Batch 886, accuracy/top5 = 0.86
I0815 19:04:00.845814 19104 caffe.cpp:313] Batch 886, loss = 1.67394
I0815 19:04:00.894098 19104 caffe.cpp:313] Batch 887, accuracy/top1 = 0.52
I0815 19:04:00.894122 19104 caffe.cpp:313] Batch 887, accuracy/top5 = 0.82
I0815 19:04:00.894125 19104 caffe.cpp:313] Batch 887, loss = 1.81188
I0815 19:04:00.942848 19104 caffe.cpp:313] Batch 888, accuracy/top1 = 0.58
I0815 19:04:00.942873 19104 caffe.cpp:313] Batch 888, accuracy/top5 = 0.88
I0815 19:04:00.942876 19104 caffe.cpp:313] Batch 888, loss = 1.83269
I0815 19:04:00.992197 19104 caffe.cpp:313] Batch 889, accuracy/top1 = 0.62
I0815 19:04:00.992223 19104 caffe.cpp:313] Batch 889, accuracy/top5 = 0.86
I0815 19:04:00.992228 19104 caffe.cpp:313] Batch 889, loss = 1.78205
I0815 19:04:01.041313 19104 caffe.cpp:313] Batch 890, accuracy/top1 = 0.52
I0815 19:04:01.041333 19104 caffe.cpp:313] Batch 890, accuracy/top5 = 0.74
I0815 19:04:01.041337 19104 caffe.cpp:313] Batch 890, loss = 2.01596
I0815 19:04:01.090502 19104 caffe.cpp:313] Batch 891, accuracy/top1 = 0.64
I0815 19:04:01.090518 19104 caffe.cpp:313] Batch 891, accuracy/top5 = 0.76
I0815 19:04:01.090522 19104 caffe.cpp:313] Batch 891, loss = 1.75911
I0815 19:04:01.139291 19104 caffe.cpp:313] Batch 892, accuracy/top1 = 0.56
I0815 19:04:01.139315 19104 caffe.cpp:313] Batch 892, accuracy/top5 = 0.84
I0815 19:04:01.139319 19104 caffe.cpp:313] Batch 892, loss = 1.76492
I0815 19:04:01.188787 19104 caffe.cpp:313] Batch 893, accuracy/top1 = 0.64
I0815 19:04:01.188817 19104 caffe.cpp:313] Batch 893, accuracy/top5 = 0.86
I0815 19:04:01.188820 19104 caffe.cpp:313] Batch 893, loss = 1.60346
I0815 19:04:01.238425 19104 caffe.cpp:313] Batch 894, accuracy/top1 = 0.44
I0815 19:04:01.238450 19104 caffe.cpp:313] Batch 894, accuracy/top5 = 0.76
I0815 19:04:01.238456 19104 caffe.cpp:313] Batch 894, loss = 2.27067
I0815 19:04:01.286847 19104 caffe.cpp:313] Batch 895, accuracy/top1 = 0.78
I0815 19:04:01.286880 19104 caffe.cpp:313] Batch 895, accuracy/top5 = 0.9
I0815 19:04:01.286883 19104 caffe.cpp:313] Batch 895, loss = 1.01485
I0815 19:04:01.335455 19104 caffe.cpp:313] Batch 896, accuracy/top1 = 0.56
I0815 19:04:01.335481 19104 caffe.cpp:313] Batch 896, accuracy/top5 = 0.78
I0815 19:04:01.335484 19104 caffe.cpp:313] Batch 896, loss = 1.78396
I0815 19:04:01.383591 19104 caffe.cpp:313] Batch 897, accuracy/top1 = 0.66
I0815 19:04:01.383625 19104 caffe.cpp:313] Batch 897, accuracy/top5 = 0.78
I0815 19:04:01.383627 19104 caffe.cpp:313] Batch 897, loss = 1.58308
I0815 19:04:01.432938 19104 caffe.cpp:313] Batch 898, accuracy/top1 = 0.6
I0815 19:04:01.432958 19104 caffe.cpp:313] Batch 898, accuracy/top5 = 0.88
I0815 19:04:01.432961 19104 caffe.cpp:313] Batch 898, loss = 1.48046
I0815 19:04:01.481561 19104 caffe.cpp:313] Batch 899, accuracy/top1 = 0.72
I0815 19:04:01.481586 19104 caffe.cpp:313] Batch 899, accuracy/top5 = 0.86
I0815 19:04:01.481590 19104 caffe.cpp:313] Batch 899, loss = 1.49164
I0815 19:04:01.530553 19104 caffe.cpp:313] Batch 900, accuracy/top1 = 0.64
I0815 19:04:01.530578 19104 caffe.cpp:313] Batch 900, accuracy/top5 = 0.88
I0815 19:04:01.530581 19104 caffe.cpp:313] Batch 900, loss = 1.32761
I0815 19:04:01.579493 19104 caffe.cpp:313] Batch 901, accuracy/top1 = 0.58
I0815 19:04:01.579519 19104 caffe.cpp:313] Batch 901, accuracy/top5 = 0.76
I0815 19:04:01.579521 19104 caffe.cpp:313] Batch 901, loss = 1.84637
I0815 19:04:01.628280 19104 caffe.cpp:313] Batch 902, accuracy/top1 = 0.58
I0815 19:04:01.628305 19104 caffe.cpp:313] Batch 902, accuracy/top5 = 0.88
I0815 19:04:01.628307 19104 caffe.cpp:313] Batch 902, loss = 1.56558
I0815 19:04:01.677009 19104 caffe.cpp:313] Batch 903, accuracy/top1 = 0.66
I0815 19:04:01.677034 19104 caffe.cpp:313] Batch 903, accuracy/top5 = 0.84
I0815 19:04:01.677037 19104 caffe.cpp:313] Batch 903, loss = 1.43593
I0815 19:04:01.726231 19104 caffe.cpp:313] Batch 904, accuracy/top1 = 0.68
I0815 19:04:01.726256 19104 caffe.cpp:313] Batch 904, accuracy/top5 = 0.88
I0815 19:04:01.726259 19104 caffe.cpp:313] Batch 904, loss = 1.52756
I0815 19:04:01.775059 19104 caffe.cpp:313] Batch 905, accuracy/top1 = 0.54
I0815 19:04:01.775084 19104 caffe.cpp:313] Batch 905, accuracy/top5 = 0.8
I0815 19:04:01.775089 19104 caffe.cpp:313] Batch 905, loss = 1.90294
I0815 19:04:01.823572 19104 caffe.cpp:313] Batch 906, accuracy/top1 = 0.64
I0815 19:04:01.823598 19104 caffe.cpp:313] Batch 906, accuracy/top5 = 0.82
I0815 19:04:01.823601 19104 caffe.cpp:313] Batch 906, loss = 1.60319
I0815 19:04:01.872016 19104 caffe.cpp:313] Batch 907, accuracy/top1 = 0.6
I0815 19:04:01.872041 19104 caffe.cpp:313] Batch 907, accuracy/top5 = 0.86
I0815 19:04:01.872045 19104 caffe.cpp:313] Batch 907, loss = 1.63017
I0815 19:04:01.921147 19104 caffe.cpp:313] Batch 908, accuracy/top1 = 0.54
I0815 19:04:01.921172 19104 caffe.cpp:313] Batch 908, accuracy/top5 = 0.9
I0815 19:04:01.921175 19104 caffe.cpp:313] Batch 908, loss = 1.5156
I0815 19:04:01.970669 19104 caffe.cpp:313] Batch 909, accuracy/top1 = 0.68
I0815 19:04:01.970695 19104 caffe.cpp:313] Batch 909, accuracy/top5 = 0.9
I0815 19:04:01.970697 19104 caffe.cpp:313] Batch 909, loss = 1.41027
I0815 19:04:02.019032 19104 caffe.cpp:313] Batch 910, accuracy/top1 = 0.56
I0815 19:04:02.019057 19104 caffe.cpp:313] Batch 910, accuracy/top5 = 0.76
I0815 19:04:02.019059 19104 caffe.cpp:313] Batch 910, loss = 2.15668
I0815 19:04:02.067334 19104 caffe.cpp:313] Batch 911, accuracy/top1 = 0.56
I0815 19:04:02.067353 19104 caffe.cpp:313] Batch 911, accuracy/top5 = 0.78
I0815 19:04:02.067356 19104 caffe.cpp:313] Batch 911, loss = 1.86382
I0815 19:04:02.115744 19104 caffe.cpp:313] Batch 912, accuracy/top1 = 0.56
I0815 19:04:02.115761 19104 caffe.cpp:313] Batch 912, accuracy/top5 = 0.78
I0815 19:04:02.115763 19104 caffe.cpp:313] Batch 912, loss = 1.69932
I0815 19:04:02.164309 19104 caffe.cpp:313] Batch 913, accuracy/top1 = 0.58
I0815 19:04:02.164324 19104 caffe.cpp:313] Batch 913, accuracy/top5 = 0.78
I0815 19:04:02.164327 19104 caffe.cpp:313] Batch 913, loss = 1.85645
I0815 19:04:02.213662 19104 caffe.cpp:313] Batch 914, accuracy/top1 = 0.64
I0815 19:04:02.213688 19104 caffe.cpp:313] Batch 914, accuracy/top5 = 0.8
I0815 19:04:02.213692 19104 caffe.cpp:313] Batch 914, loss = 1.8789
I0815 19:04:02.262198 19104 caffe.cpp:313] Batch 915, accuracy/top1 = 0.7
I0815 19:04:02.262218 19104 caffe.cpp:313] Batch 915, accuracy/top5 = 0.8
I0815 19:04:02.262221 19104 caffe.cpp:313] Batch 915, loss = 1.6131
I0815 19:04:02.310451 19104 caffe.cpp:313] Batch 916, accuracy/top1 = 0.66
I0815 19:04:02.310473 19104 caffe.cpp:313] Batch 916, accuracy/top5 = 0.9
I0815 19:04:02.310477 19104 caffe.cpp:313] Batch 916, loss = 1.50354
I0815 19:04:02.359323 19104 caffe.cpp:313] Batch 917, accuracy/top1 = 0.64
I0815 19:04:02.359347 19104 caffe.cpp:313] Batch 917, accuracy/top5 = 0.8
I0815 19:04:02.359350 19104 caffe.cpp:313] Batch 917, loss = 1.58837
I0815 19:04:02.409597 19104 caffe.cpp:313] Batch 918, accuracy/top1 = 0.62
I0815 19:04:02.409616 19104 caffe.cpp:313] Batch 918, accuracy/top5 = 0.78
I0815 19:04:02.409621 19104 caffe.cpp:313] Batch 918, loss = 1.75335
I0815 19:04:02.457665 19104 caffe.cpp:313] Batch 919, accuracy/top1 = 0.62
I0815 19:04:02.457690 19104 caffe.cpp:313] Batch 919, accuracy/top5 = 0.78
I0815 19:04:02.457692 19104 caffe.cpp:313] Batch 919, loss = 2.00636
I0815 19:04:02.505800 19104 caffe.cpp:313] Batch 920, accuracy/top1 = 0.64
I0815 19:04:02.505825 19104 caffe.cpp:313] Batch 920, accuracy/top5 = 0.82
I0815 19:04:02.505827 19104 caffe.cpp:313] Batch 920, loss = 1.81249
I0815 19:04:02.555627 19104 caffe.cpp:313] Batch 921, accuracy/top1 = 0.54
I0815 19:04:02.555652 19104 caffe.cpp:313] Batch 921, accuracy/top5 = 0.82
I0815 19:04:02.555655 19104 caffe.cpp:313] Batch 921, loss = 1.94239
I0815 19:04:02.604269 19104 caffe.cpp:313] Batch 922, accuracy/top1 = 0.68
I0815 19:04:02.604291 19104 caffe.cpp:313] Batch 922, accuracy/top5 = 0.86
I0815 19:04:02.604295 19104 caffe.cpp:313] Batch 922, loss = 1.24503
I0815 19:04:02.652818 19104 caffe.cpp:313] Batch 923, accuracy/top1 = 0.66
I0815 19:04:02.652843 19104 caffe.cpp:313] Batch 923, accuracy/top5 = 0.82
I0815 19:04:02.652846 19104 caffe.cpp:313] Batch 923, loss = 1.93892
I0815 19:04:02.702431 19104 caffe.cpp:313] Batch 924, accuracy/top1 = 0.58
I0815 19:04:02.702456 19104 caffe.cpp:313] Batch 924, accuracy/top5 = 0.86
I0815 19:04:02.702460 19104 caffe.cpp:313] Batch 924, loss = 1.53382
I0815 19:04:02.751780 19104 caffe.cpp:313] Batch 925, accuracy/top1 = 0.56
I0815 19:04:02.751804 19104 caffe.cpp:313] Batch 925, accuracy/top5 = 0.8
I0815 19:04:02.751808 19104 caffe.cpp:313] Batch 925, loss = 1.79726
I0815 19:04:02.801336 19104 caffe.cpp:313] Batch 926, accuracy/top1 = 0.66
I0815 19:04:02.801360 19104 caffe.cpp:313] Batch 926, accuracy/top5 = 0.8
I0815 19:04:02.801363 19104 caffe.cpp:313] Batch 926, loss = 1.94627
I0815 19:04:02.849189 19104 caffe.cpp:313] Batch 927, accuracy/top1 = 0.54
I0815 19:04:02.849215 19104 caffe.cpp:313] Batch 927, accuracy/top5 = 0.7
I0815 19:04:02.849217 19104 caffe.cpp:313] Batch 927, loss = 2.12844
I0815 19:04:02.897382 19104 caffe.cpp:313] Batch 928, accuracy/top1 = 0.74
I0815 19:04:02.897406 19104 caffe.cpp:313] Batch 928, accuracy/top5 = 0.88
I0815 19:04:02.897409 19104 caffe.cpp:313] Batch 928, loss = 1.2899
I0815 19:04:02.945762 19104 caffe.cpp:313] Batch 929, accuracy/top1 = 0.56
I0815 19:04:02.945787 19104 caffe.cpp:313] Batch 929, accuracy/top5 = 0.84
I0815 19:04:02.945791 19104 caffe.cpp:313] Batch 929, loss = 1.60012
I0815 19:04:02.994796 19104 caffe.cpp:313] Batch 930, accuracy/top1 = 0.6
I0815 19:04:02.994820 19104 caffe.cpp:313] Batch 930, accuracy/top5 = 0.8
I0815 19:04:02.994823 19104 caffe.cpp:313] Batch 930, loss = 1.74811
I0815 19:04:03.043846 19104 caffe.cpp:313] Batch 931, accuracy/top1 = 0.6
I0815 19:04:03.043862 19104 caffe.cpp:313] Batch 931, accuracy/top5 = 0.82
I0815 19:04:03.043866 19104 caffe.cpp:313] Batch 931, loss = 2.2172
I0815 19:04:03.092751 19104 caffe.cpp:313] Batch 932, accuracy/top1 = 0.64
I0815 19:04:03.092767 19104 caffe.cpp:313] Batch 932, accuracy/top5 = 0.86
I0815 19:04:03.092783 19104 caffe.cpp:313] Batch 932, loss = 1.53631
I0815 19:04:03.141914 19104 caffe.cpp:313] Batch 933, accuracy/top1 = 0.6
I0815 19:04:03.141940 19104 caffe.cpp:313] Batch 933, accuracy/top5 = 0.82
I0815 19:04:03.141943 19104 caffe.cpp:313] Batch 933, loss = 2.06665
I0815 19:04:03.191553 19104 caffe.cpp:313] Batch 934, accuracy/top1 = 0.58
I0815 19:04:03.191578 19104 caffe.cpp:313] Batch 934, accuracy/top5 = 0.78
I0815 19:04:03.191581 19104 caffe.cpp:313] Batch 934, loss = 1.94966
I0815 19:04:03.239579 19104 caffe.cpp:313] Batch 935, accuracy/top1 = 0.66
I0815 19:04:03.239605 19104 caffe.cpp:313] Batch 935, accuracy/top5 = 0.78
I0815 19:04:03.239609 19104 caffe.cpp:313] Batch 935, loss = 2.06228
I0815 19:04:03.287863 19104 caffe.cpp:313] Batch 936, accuracy/top1 = 0.72
I0815 19:04:03.287885 19104 caffe.cpp:313] Batch 936, accuracy/top5 = 0.86
I0815 19:04:03.287889 19104 caffe.cpp:313] Batch 936, loss = 1.5218
I0815 19:04:03.337231 19104 caffe.cpp:313] Batch 937, accuracy/top1 = 0.62
I0815 19:04:03.337254 19104 caffe.cpp:313] Batch 937, accuracy/top5 = 0.84
I0815 19:04:03.337257 19104 caffe.cpp:313] Batch 937, loss = 1.78711
I0815 19:04:03.385695 19104 caffe.cpp:313] Batch 938, accuracy/top1 = 0.54
I0815 19:04:03.385727 19104 caffe.cpp:313] Batch 938, accuracy/top5 = 0.88
I0815 19:04:03.385731 19104 caffe.cpp:313] Batch 938, loss = 1.8431
I0815 19:04:03.434140 19104 caffe.cpp:313] Batch 939, accuracy/top1 = 0.54
I0815 19:04:03.434162 19104 caffe.cpp:313] Batch 939, accuracy/top5 = 0.78
I0815 19:04:03.434166 19104 caffe.cpp:313] Batch 939, loss = 1.84867
I0815 19:04:03.482028 19104 caffe.cpp:313] Batch 940, accuracy/top1 = 0.58
I0815 19:04:03.482053 19104 caffe.cpp:313] Batch 940, accuracy/top5 = 0.82
I0815 19:04:03.482055 19104 caffe.cpp:313] Batch 940, loss = 1.7596
I0815 19:04:03.530237 19104 caffe.cpp:313] Batch 941, accuracy/top1 = 0.64
I0815 19:04:03.530262 19104 caffe.cpp:313] Batch 941, accuracy/top5 = 0.8
I0815 19:04:03.530266 19104 caffe.cpp:313] Batch 941, loss = 1.93712
I0815 19:04:03.578760 19104 caffe.cpp:313] Batch 942, accuracy/top1 = 0.5
I0815 19:04:03.578784 19104 caffe.cpp:313] Batch 942, accuracy/top5 = 0.78
I0815 19:04:03.578788 19104 caffe.cpp:313] Batch 942, loss = 2.05127
I0815 19:04:03.626781 19104 caffe.cpp:313] Batch 943, accuracy/top1 = 0.58
I0815 19:04:03.626806 19104 caffe.cpp:313] Batch 943, accuracy/top5 = 0.86
I0815 19:04:03.626809 19104 caffe.cpp:313] Batch 943, loss = 1.61686
I0815 19:04:03.675259 19104 caffe.cpp:313] Batch 944, accuracy/top1 = 0.58
I0815 19:04:03.675283 19104 caffe.cpp:313] Batch 944, accuracy/top5 = 0.86
I0815 19:04:03.675287 19104 caffe.cpp:313] Batch 944, loss = 1.46748
I0815 19:04:03.724200 19104 caffe.cpp:313] Batch 945, accuracy/top1 = 0.54
I0815 19:04:03.724223 19104 caffe.cpp:313] Batch 945, accuracy/top5 = 0.8
I0815 19:04:03.724227 19104 caffe.cpp:313] Batch 945, loss = 1.99383
I0815 19:04:03.772927 19104 caffe.cpp:313] Batch 946, accuracy/top1 = 0.62
I0815 19:04:03.772951 19104 caffe.cpp:313] Batch 946, accuracy/top5 = 0.8
I0815 19:04:03.772954 19104 caffe.cpp:313] Batch 946, loss = 1.89045
I0815 19:04:03.823257 19104 caffe.cpp:313] Batch 947, accuracy/top1 = 0.62
I0815 19:04:03.823282 19104 caffe.cpp:313] Batch 947, accuracy/top5 = 0.86
I0815 19:04:03.823287 19104 caffe.cpp:313] Batch 947, loss = 1.30794
I0815 19:04:03.872661 19104 caffe.cpp:313] Batch 948, accuracy/top1 = 0.54
I0815 19:04:03.872685 19104 caffe.cpp:313] Batch 948, accuracy/top5 = 0.78
I0815 19:04:03.872689 19104 caffe.cpp:313] Batch 948, loss = 2.20007
I0815 19:04:03.920970 19104 caffe.cpp:313] Batch 949, accuracy/top1 = 0.68
I0815 19:04:03.920994 19104 caffe.cpp:313] Batch 949, accuracy/top5 = 0.86
I0815 19:04:03.920999 19104 caffe.cpp:313] Batch 949, loss = 1.24183
I0815 19:04:03.968812 19104 caffe.cpp:313] Batch 950, accuracy/top1 = 0.62
I0815 19:04:03.968837 19104 caffe.cpp:313] Batch 950, accuracy/top5 = 0.78
I0815 19:04:03.968842 19104 caffe.cpp:313] Batch 950, loss = 2.03322
I0815 19:04:04.018705 19104 caffe.cpp:313] Batch 951, accuracy/top1 = 0.76
I0815 19:04:04.018729 19104 caffe.cpp:313] Batch 951, accuracy/top5 = 0.9
I0815 19:04:04.018748 19104 caffe.cpp:313] Batch 951, loss = 1.2072
I0815 19:04:04.066922 19104 caffe.cpp:313] Batch 952, accuracy/top1 = 0.66
I0815 19:04:04.066941 19104 caffe.cpp:313] Batch 952, accuracy/top5 = 0.82
I0815 19:04:04.066943 19104 caffe.cpp:313] Batch 952, loss = 2.09996
I0815 19:04:04.115150 19104 caffe.cpp:313] Batch 953, accuracy/top1 = 0.56
I0815 19:04:04.115177 19104 caffe.cpp:313] Batch 953, accuracy/top5 = 0.76
I0815 19:04:04.115180 19104 caffe.cpp:313] Batch 953, loss = 2.16887
I0815 19:04:04.163748 19104 caffe.cpp:313] Batch 954, accuracy/top1 = 0.56
I0815 19:04:04.163774 19104 caffe.cpp:313] Batch 954, accuracy/top5 = 0.76
I0815 19:04:04.163779 19104 caffe.cpp:313] Batch 954, loss = 2.22487
I0815 19:04:04.213136 19104 caffe.cpp:313] Batch 955, accuracy/top1 = 0.58
I0815 19:04:04.213160 19104 caffe.cpp:313] Batch 955, accuracy/top5 = 0.78
I0815 19:04:04.213163 19104 caffe.cpp:313] Batch 955, loss = 2.09388
I0815 19:04:04.262066 19104 caffe.cpp:313] Batch 956, accuracy/top1 = 0.56
I0815 19:04:04.262090 19104 caffe.cpp:313] Batch 956, accuracy/top5 = 0.9
I0815 19:04:04.262094 19104 caffe.cpp:313] Batch 956, loss = 1.72752
I0815 19:04:04.311766 19104 caffe.cpp:313] Batch 957, accuracy/top1 = 0.58
I0815 19:04:04.311784 19104 caffe.cpp:313] Batch 957, accuracy/top5 = 0.8
I0815 19:04:04.311787 19104 caffe.cpp:313] Batch 957, loss = 2.04935
I0815 19:04:04.360414 19104 caffe.cpp:313] Batch 958, accuracy/top1 = 0.56
I0815 19:04:04.360438 19104 caffe.cpp:313] Batch 958, accuracy/top5 = 0.86
I0815 19:04:04.360441 19104 caffe.cpp:313] Batch 958, loss = 1.64275
I0815 19:04:04.409528 19104 caffe.cpp:313] Batch 959, accuracy/top1 = 0.5
I0815 19:04:04.409548 19104 caffe.cpp:313] Batch 959, accuracy/top5 = 0.82
I0815 19:04:04.409551 19104 caffe.cpp:313] Batch 959, loss = 1.94524
I0815 19:04:04.457420 19104 caffe.cpp:313] Batch 960, accuracy/top1 = 0.64
I0815 19:04:04.457444 19104 caffe.cpp:313] Batch 960, accuracy/top5 = 0.82
I0815 19:04:04.457448 19104 caffe.cpp:313] Batch 960, loss = 1.78503
I0815 19:04:04.505342 19104 caffe.cpp:313] Batch 961, accuracy/top1 = 0.56
I0815 19:04:04.505365 19104 caffe.cpp:313] Batch 961, accuracy/top5 = 0.76
I0815 19:04:04.505368 19104 caffe.cpp:313] Batch 961, loss = 2.33152
I0815 19:04:04.554101 19104 caffe.cpp:313] Batch 962, accuracy/top1 = 0.56
I0815 19:04:04.554126 19104 caffe.cpp:313] Batch 962, accuracy/top5 = 0.9
I0815 19:04:04.554128 19104 caffe.cpp:313] Batch 962, loss = 1.35157
I0815 19:04:04.602890 19104 caffe.cpp:313] Batch 963, accuracy/top1 = 0.52
I0815 19:04:04.602915 19104 caffe.cpp:313] Batch 963, accuracy/top5 = 0.76
I0815 19:04:04.602917 19104 caffe.cpp:313] Batch 963, loss = 2.06885
I0815 19:04:04.651465 19104 caffe.cpp:313] Batch 964, accuracy/top1 = 0.62
I0815 19:04:04.651489 19104 caffe.cpp:313] Batch 964, accuracy/top5 = 0.8
I0815 19:04:04.651492 19104 caffe.cpp:313] Batch 964, loss = 1.73302
I0815 19:04:04.699908 19104 caffe.cpp:313] Batch 965, accuracy/top1 = 0.56
I0815 19:04:04.699931 19104 caffe.cpp:313] Batch 965, accuracy/top5 = 0.8
I0815 19:04:04.699935 19104 caffe.cpp:313] Batch 965, loss = 1.93909
I0815 19:04:04.748567 19104 caffe.cpp:313] Batch 966, accuracy/top1 = 0.6
I0815 19:04:04.748594 19104 caffe.cpp:313] Batch 966, accuracy/top5 = 0.84
I0815 19:04:04.748597 19104 caffe.cpp:313] Batch 966, loss = 2.0276
I0815 19:04:04.798308 19104 caffe.cpp:313] Batch 967, accuracy/top1 = 0.56
I0815 19:04:04.798332 19104 caffe.cpp:313] Batch 967, accuracy/top5 = 0.84
I0815 19:04:04.798336 19104 caffe.cpp:313] Batch 967, loss = 1.90505
I0815 19:04:04.847854 19104 caffe.cpp:313] Batch 968, accuracy/top1 = 0.54
I0815 19:04:04.847879 19104 caffe.cpp:313] Batch 968, accuracy/top5 = 0.84
I0815 19:04:04.847882 19104 caffe.cpp:313] Batch 968, loss = 2.08462
I0815 19:04:04.897434 19104 caffe.cpp:313] Batch 969, accuracy/top1 = 0.54
I0815 19:04:04.897459 19104 caffe.cpp:313] Batch 969, accuracy/top5 = 0.82
I0815 19:04:04.897462 19104 caffe.cpp:313] Batch 969, loss = 1.59068
I0815 19:04:04.946346 19104 caffe.cpp:313] Batch 970, accuracy/top1 = 0.66
I0815 19:04:04.946389 19104 caffe.cpp:313] Batch 970, accuracy/top5 = 0.82
I0815 19:04:04.946393 19104 caffe.cpp:313] Batch 970, loss = 1.57514
I0815 19:04:04.995250 19104 caffe.cpp:313] Batch 971, accuracy/top1 = 0.58
I0815 19:04:04.995275 19104 caffe.cpp:313] Batch 971, accuracy/top5 = 0.9
I0815 19:04:04.995278 19104 caffe.cpp:313] Batch 971, loss = 1.49031
I0815 19:04:05.044714 19104 caffe.cpp:313] Batch 972, accuracy/top1 = 0.7
I0815 19:04:05.044731 19104 caffe.cpp:313] Batch 972, accuracy/top5 = 0.92
I0815 19:04:05.044734 19104 caffe.cpp:313] Batch 972, loss = 1.38482
I0815 19:04:05.094677 19104 caffe.cpp:313] Batch 973, accuracy/top1 = 0.44
I0815 19:04:05.094691 19104 caffe.cpp:313] Batch 973, accuracy/top5 = 0.74
I0815 19:04:05.094694 19104 caffe.cpp:313] Batch 973, loss = 2.38644
I0815 19:04:05.143575 19104 caffe.cpp:313] Batch 974, accuracy/top1 = 0.54
I0815 19:04:05.143601 19104 caffe.cpp:313] Batch 974, accuracy/top5 = 0.82
I0815 19:04:05.143604 19104 caffe.cpp:313] Batch 974, loss = 1.77725
I0815 19:04:05.192500 19104 caffe.cpp:313] Batch 975, accuracy/top1 = 0.66
I0815 19:04:05.192524 19104 caffe.cpp:313] Batch 975, accuracy/top5 = 0.8
I0815 19:04:05.192528 19104 caffe.cpp:313] Batch 975, loss = 1.61327
I0815 19:04:05.240913 19104 caffe.cpp:313] Batch 976, accuracy/top1 = 0.5
I0815 19:04:05.240939 19104 caffe.cpp:313] Batch 976, accuracy/top5 = 0.78
I0815 19:04:05.240942 19104 caffe.cpp:313] Batch 976, loss = 1.94725
I0815 19:04:05.289388 19104 caffe.cpp:313] Batch 977, accuracy/top1 = 0.52
I0815 19:04:05.289413 19104 caffe.cpp:313] Batch 977, accuracy/top5 = 0.82
I0815 19:04:05.289417 19104 caffe.cpp:313] Batch 977, loss = 1.71875
I0815 19:04:05.338464 19104 caffe.cpp:313] Batch 978, accuracy/top1 = 0.54
I0815 19:04:05.338490 19104 caffe.cpp:313] Batch 978, accuracy/top5 = 0.74
I0815 19:04:05.338492 19104 caffe.cpp:313] Batch 978, loss = 2.1059
I0815 19:04:05.387974 19104 caffe.cpp:313] Batch 979, accuracy/top1 = 0.58
I0815 19:04:05.387996 19104 caffe.cpp:313] Batch 979, accuracy/top5 = 0.84
I0815 19:04:05.388000 19104 caffe.cpp:313] Batch 979, loss = 1.80819
I0815 19:04:05.436859 19104 caffe.cpp:313] Batch 980, accuracy/top1 = 0.54
I0815 19:04:05.436880 19104 caffe.cpp:313] Batch 980, accuracy/top5 = 0.78
I0815 19:04:05.436883 19104 caffe.cpp:313] Batch 980, loss = 2.18106
I0815 19:04:05.485692 19104 caffe.cpp:313] Batch 981, accuracy/top1 = 0.56
I0815 19:04:05.485713 19104 caffe.cpp:313] Batch 981, accuracy/top5 = 0.78
I0815 19:04:05.485716 19104 caffe.cpp:313] Batch 981, loss = 2.05784
I0815 19:04:05.534109 19104 caffe.cpp:313] Batch 982, accuracy/top1 = 0.58
I0815 19:04:05.534132 19104 caffe.cpp:313] Batch 982, accuracy/top5 = 0.68
I0815 19:04:05.534134 19104 caffe.cpp:313] Batch 982, loss = 2.74856
I0815 19:04:05.582079 19104 caffe.cpp:313] Batch 983, accuracy/top1 = 0.58
I0815 19:04:05.582103 19104 caffe.cpp:313] Batch 983, accuracy/top5 = 0.82
I0815 19:04:05.582106 19104 caffe.cpp:313] Batch 983, loss = 2.28672
I0815 19:04:05.630520 19104 caffe.cpp:313] Batch 984, accuracy/top1 = 0.68
I0815 19:04:05.630544 19104 caffe.cpp:313] Batch 984, accuracy/top5 = 0.9
I0815 19:04:05.630548 19104 caffe.cpp:313] Batch 984, loss = 1.34445
I0815 19:04:05.678499 19104 caffe.cpp:313] Batch 985, accuracy/top1 = 0.62
I0815 19:04:05.678524 19104 caffe.cpp:313] Batch 985, accuracy/top5 = 0.88
I0815 19:04:05.678529 19104 caffe.cpp:313] Batch 985, loss = 1.42385
I0815 19:04:05.727854 19104 caffe.cpp:313] Batch 986, accuracy/top1 = 0.52
I0815 19:04:05.727880 19104 caffe.cpp:313] Batch 986, accuracy/top5 = 0.78
I0815 19:04:05.727882 19104 caffe.cpp:313] Batch 986, loss = 2.06306
I0815 19:04:05.776386 19104 caffe.cpp:313] Batch 987, accuracy/top1 = 0.54
I0815 19:04:05.776412 19104 caffe.cpp:313] Batch 987, accuracy/top5 = 0.84
I0815 19:04:05.776415 19104 caffe.cpp:313] Batch 987, loss = 1.78857
I0815 19:04:05.825325 19104 caffe.cpp:313] Batch 988, accuracy/top1 = 0.6
I0815 19:04:05.825350 19104 caffe.cpp:313] Batch 988, accuracy/top5 = 0.84
I0815 19:04:05.825352 19104 caffe.cpp:313] Batch 988, loss = 1.57777
I0815 19:04:05.873829 19104 caffe.cpp:313] Batch 989, accuracy/top1 = 0.52
I0815 19:04:05.873854 19104 caffe.cpp:313] Batch 989, accuracy/top5 = 0.78
I0815 19:04:05.873857 19104 caffe.cpp:313] Batch 989, loss = 1.85212
I0815 19:04:05.923123 19104 caffe.cpp:313] Batch 990, accuracy/top1 = 0.68
I0815 19:04:05.923148 19104 caffe.cpp:313] Batch 990, accuracy/top5 = 0.88
I0815 19:04:05.923151 19104 caffe.cpp:313] Batch 990, loss = 1.22964
I0815 19:04:05.972560 19104 caffe.cpp:313] Batch 991, accuracy/top1 = 0.62
I0815 19:04:05.972585 19104 caffe.cpp:313] Batch 991, accuracy/top5 = 0.9
I0815 19:04:05.972589 19104 caffe.cpp:313] Batch 991, loss = 1.56787
I0815 19:04:06.022150 19104 caffe.cpp:313] Batch 992, accuracy/top1 = 0.62
I0815 19:04:06.022172 19104 caffe.cpp:313] Batch 992, accuracy/top5 = 0.86
I0815 19:04:06.022174 19104 caffe.cpp:313] Batch 992, loss = 1.33036
I0815 19:04:06.071439 19104 caffe.cpp:313] Batch 993, accuracy/top1 = 0.74
I0815 19:04:06.071455 19104 caffe.cpp:313] Batch 993, accuracy/top5 = 0.86
I0815 19:04:06.071458 19104 caffe.cpp:313] Batch 993, loss = 1.27014
I0815 19:04:06.120482 19104 caffe.cpp:313] Batch 994, accuracy/top1 = 0.7
I0815 19:04:06.120509 19104 caffe.cpp:313] Batch 994, accuracy/top5 = 0.88
I0815 19:04:06.120512 19104 caffe.cpp:313] Batch 994, loss = 1.331
I0815 19:04:06.169186 19104 caffe.cpp:313] Batch 995, accuracy/top1 = 0.62
I0815 19:04:06.169212 19104 caffe.cpp:313] Batch 995, accuracy/top5 = 0.82
I0815 19:04:06.169214 19104 caffe.cpp:313] Batch 995, loss = 1.69874
I0815 19:04:06.218339 19104 caffe.cpp:313] Batch 996, accuracy/top1 = 0.56
I0815 19:04:06.218364 19104 caffe.cpp:313] Batch 996, accuracy/top5 = 0.8
I0815 19:04:06.218369 19104 caffe.cpp:313] Batch 996, loss = 1.63502
I0815 19:04:06.266835 19104 caffe.cpp:313] Batch 997, accuracy/top1 = 0.6
I0815 19:04:06.266860 19104 caffe.cpp:313] Batch 997, accuracy/top5 = 0.88
I0815 19:04:06.266863 19104 caffe.cpp:313] Batch 997, loss = 1.59658
I0815 19:04:06.288101 19171 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:04:06.315234 19104 caffe.cpp:313] Batch 998, accuracy/top1 = 0.46
I0815 19:04:06.315259 19104 caffe.cpp:313] Batch 998, accuracy/top5 = 0.76
I0815 19:04:06.315263 19104 caffe.cpp:313] Batch 998, loss = 2.02025
I0815 19:04:06.363961 19104 caffe.cpp:313] Batch 999, accuracy/top1 = 0.72
I0815 19:04:06.363983 19104 caffe.cpp:313] Batch 999, accuracy/top5 = 0.9
I0815 19:04:06.363987 19104 caffe.cpp:313] Batch 999, loss = 1.32605
I0815 19:04:06.363989 19104 caffe.cpp:318] Loss: 1.74816
I0815 19:04:06.363997 19104 caffe.cpp:330] accuracy/top1 = 0.59618
I0815 19:04:06.364001 19104 caffe.cpp:330] accuracy/top5 = 0.822022
I0815 19:04:06.364006 19104 caffe.cpp:330] loss = 1.74816 (* 1 = 1.74816 loss)
I0817 10:57:50.166539 14586 caffe.cpp:608] This is NVCaffe 0.16.3 started at Thu Aug 17 10:57:49 2017
I0817 10:57:50.166674 14586 caffe.cpp:611] CuDNN version: 6021
I0817 10:57:50.166679 14586 caffe.cpp:612] CuBLAS version: 8000
I0817 10:57:50.166682 14586 caffe.cpp:613] CUDA version: 8000
I0817 10:57:50.166684 14586 caffe.cpp:614] CUDA driver version: 8000
I0817 10:57:50.166692 14586 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0817 10:57:50.166698 14586 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0817 10:57:50.167306 14586 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0817 10:57:50.167898 14586 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0817 10:57:50.167906 14586 caffe.cpp:275] Use GPU with device ID 0
I0817 10:57:50.168280 14586 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0817 10:57:50.169555 14586 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
quantize: true
I0817 10:57:50.169690 14586 net.cpp:104] Using FLOAT as default forward math type
I0817 10:57:50.169697 14586 net.cpp:110] Using FLOAT as default backward math type
I0817 10:57:50.169700 14586 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0817 10:57:50.169704 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.169755 14586 net.cpp:184] Created Layer data (0)
I0817 10:57:50.169760 14586 net.cpp:530] data -> data
I0817 10:57:50.169771 14586 net.cpp:530] data -> label
I0817 10:57:50.169791 14586 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0817 10:57:50.170094 14586 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0817 10:57:50.178534 14619 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0817 10:57:50.179951 14586 data_layer.cpp:185] (0) ReshapePrefetch 50, 3, 224, 224
I0817 10:57:50.179986 14586 data_layer.cpp:209] (0) Output data size: 50, 3, 224, 224
I0817 10:57:50.179991 14586 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0817 10:57:50.180012 14586 net.cpp:245] Setting up data
I0817 10:57:50.180022 14586 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 224 224 (7526400)
I0817 10:57:50.180027 14586 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0817 10:57:50.180032 14586 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0817 10:57:50.180037 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.180047 14586 net.cpp:184] Created Layer label_data_1_split (1)
I0817 10:57:50.180052 14586 net.cpp:561] label_data_1_split <- label
I0817 10:57:50.180058 14586 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0817 10:57:50.180063 14586 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0817 10:57:50.180074 14586 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0817 10:57:50.180104 14586 net.cpp:245] Setting up label_data_1_split
I0817 10:57:50.180107 14586 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:57:50.180109 14586 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:57:50.180112 14586 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:57:50.180114 14586 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0817 10:57:50.180116 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.180125 14586 net.cpp:184] Created Layer data/bias (2)
I0817 10:57:50.180128 14586 net.cpp:561] data/bias <- data
I0817 10:57:50.180142 14586 net.cpp:530] data/bias -> data/bias
I0817 10:57:50.181164 14620 data_layer.cpp:97] (0) Parser threads: 1
I0817 10:57:50.181174 14620 data_layer.cpp:99] (0) Transformer threads: 1
I0817 10:57:50.186360 14586 net.cpp:245] Setting up data/bias
I0817 10:57:50.186398 14586 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 224 224 (7526400)
I0817 10:57:50.186414 14586 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0817 10:57:50.186420 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.186444 14586 net.cpp:184] Created Layer conv1a (3)
I0817 10:57:50.186450 14586 net.cpp:561] conv1a <- data/bias
I0817 10:57:50.186455 14586 net.cpp:530] conv1a -> conv1a
I0817 10:57:50.480576 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.02G, req 0G)
I0817 10:57:50.480595 14586 net.cpp:245] Setting up conv1a
I0817 10:57:50.480602 14586 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 112 112 (20070400)
I0817 10:57:50.480610 14586 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0817 10:57:50.480614 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.480626 14586 net.cpp:184] Created Layer conv1a/bn (4)
I0817 10:57:50.480630 14586 net.cpp:561] conv1a/bn <- conv1a
I0817 10:57:50.480633 14586 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0817 10:57:50.481076 14586 net.cpp:245] Setting up conv1a/bn
I0817 10:57:50.481082 14586 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 112 112 (20070400)
I0817 10:57:50.481091 14586 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0817 10:57:50.481092 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.481097 14586 net.cpp:184] Created Layer conv1a/relu (5)
I0817 10:57:50.481099 14586 net.cpp:561] conv1a/relu <- conv1a
I0817 10:57:50.481102 14586 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0817 10:57:50.481112 14586 net.cpp:245] Setting up conv1a/relu
I0817 10:57:50.481115 14586 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 112 112 (20070400)
I0817 10:57:50.481117 14586 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0817 10:57:50.481122 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.481133 14586 net.cpp:184] Created Layer conv1b (6)
I0817 10:57:50.481137 14586 net.cpp:561] conv1b <- conv1a
I0817 10:57:50.481139 14586 net.cpp:530] conv1b -> conv1b
I0817 10:57:50.490617 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0817 10:57:50.490627 14586 net.cpp:245] Setting up conv1b
I0817 10:57:50.490630 14586 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 112 112 (20070400)
I0817 10:57:50.490635 14586 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0817 10:57:50.490638 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.490643 14586 net.cpp:184] Created Layer conv1b/bn (7)
I0817 10:57:50.490645 14586 net.cpp:561] conv1b/bn <- conv1b
I0817 10:57:50.490648 14586 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0817 10:57:50.491063 14586 net.cpp:245] Setting up conv1b/bn
I0817 10:57:50.491071 14586 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 112 112 (20070400)
I0817 10:57:50.491077 14586 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0817 10:57:50.491080 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.491082 14586 net.cpp:184] Created Layer conv1b/relu (8)
I0817 10:57:50.491086 14586 net.cpp:561] conv1b/relu <- conv1b
I0817 10:57:50.491087 14586 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0817 10:57:50.491091 14586 net.cpp:245] Setting up conv1b/relu
I0817 10:57:50.491093 14586 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 112 112 (20070400)
I0817 10:57:50.491096 14586 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0817 10:57:50.491097 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.491103 14586 net.cpp:184] Created Layer pool1 (9)
I0817 10:57:50.491106 14586 net.cpp:561] pool1 <- conv1b
I0817 10:57:50.491107 14586 net.cpp:530] pool1 -> pool1
I0817 10:57:50.491154 14586 net.cpp:245] Setting up pool1
I0817 10:57:50.491160 14586 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 56 56 (5017600)
I0817 10:57:50.491163 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0817 10:57:50.491164 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.491170 14586 net.cpp:184] Created Layer res2a_branch2a (10)
I0817 10:57:50.491173 14586 net.cpp:561] res2a_branch2a <- pool1
I0817 10:57:50.491175 14586 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0817 10:57:50.500325 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0817 10:57:50.500336 14586 net.cpp:245] Setting up res2a_branch2a
I0817 10:57:50.500340 14586 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 56 56 (10035200)
I0817 10:57:50.500345 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0817 10:57:50.500349 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.500352 14586 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0817 10:57:50.500355 14586 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0817 10:57:50.500357 14586 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0817 10:57:50.500772 14586 net.cpp:245] Setting up res2a_branch2a/bn
I0817 10:57:50.500779 14586 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 56 56 (10035200)
I0817 10:57:50.500784 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0817 10:57:50.500787 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.500793 14586 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0817 10:57:50.500797 14586 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0817 10:57:50.500798 14586 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0817 10:57:50.500802 14586 net.cpp:245] Setting up res2a_branch2a/relu
I0817 10:57:50.500804 14586 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 56 56 (10035200)
I0817 10:57:50.500807 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0817 10:57:50.500808 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.500823 14586 net.cpp:184] Created Layer res2a_branch2b (13)
I0817 10:57:50.500825 14586 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0817 10:57:50.500828 14586 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0817 10:57:50.505569 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.82G, req 0G)
I0817 10:57:50.505589 14586 net.cpp:245] Setting up res2a_branch2b
I0817 10:57:50.505595 14586 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 56 56 (10035200)
I0817 10:57:50.505615 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0817 10:57:50.505623 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.505640 14586 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0817 10:57:50.505645 14586 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0817 10:57:50.505648 14586 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0817 10:57:50.506122 14586 net.cpp:245] Setting up res2a_branch2b/bn
I0817 10:57:50.506132 14586 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 56 56 (10035200)
I0817 10:57:50.506142 14586 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0817 10:57:50.506146 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.506152 14586 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0817 10:57:50.506156 14586 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0817 10:57:50.506160 14586 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0817 10:57:50.506168 14586 net.cpp:245] Setting up res2a_branch2b/relu
I0817 10:57:50.506175 14586 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 56 56 (10035200)
I0817 10:57:50.506177 14586 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0817 10:57:50.506181 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.506189 14586 net.cpp:184] Created Layer pool2 (16)
I0817 10:57:50.506193 14586 net.cpp:561] pool2 <- res2a_branch2b
I0817 10:57:50.506196 14586 net.cpp:530] pool2 -> pool2
I0817 10:57:50.506229 14586 net.cpp:245] Setting up pool2
I0817 10:57:50.506234 14586 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 28 28 (2508800)
I0817 10:57:50.506239 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0817 10:57:50.506244 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.506253 14586 net.cpp:184] Created Layer res3a_branch2a (17)
I0817 10:57:50.506258 14586 net.cpp:561] res3a_branch2a <- pool2
I0817 10:57:50.506261 14586 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0817 10:57:50.533380 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.79G, req 0G)
I0817 10:57:50.533401 14586 net.cpp:245] Setting up res3a_branch2a
I0817 10:57:50.533407 14586 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 28 28 (5017600)
I0817 10:57:50.533417 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0817 10:57:50.533422 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.533432 14586 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0817 10:57:50.533437 14586 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0817 10:57:50.533442 14586 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0817 10:57:50.533995 14586 net.cpp:245] Setting up res3a_branch2a/bn
I0817 10:57:50.534008 14586 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 28 28 (5017600)
I0817 10:57:50.534018 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0817 10:57:50.534023 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.534027 14586 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0817 10:57:50.534030 14586 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0817 10:57:50.534034 14586 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0817 10:57:50.534039 14586 net.cpp:245] Setting up res3a_branch2a/relu
I0817 10:57:50.534044 14586 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 28 28 (5017600)
I0817 10:57:50.534049 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0817 10:57:50.534052 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.534076 14586 net.cpp:184] Created Layer res3a_branch2b (20)
I0817 10:57:50.534082 14586 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0817 10:57:50.534088 14586 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0817 10:57:50.538180 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.77G, req 0G)
I0817 10:57:50.538192 14586 net.cpp:245] Setting up res3a_branch2b
I0817 10:57:50.538195 14586 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 28 28 (5017600)
I0817 10:57:50.538200 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0817 10:57:50.538203 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.538208 14586 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0817 10:57:50.538210 14586 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0817 10:57:50.538213 14586 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0817 10:57:50.538612 14586 net.cpp:245] Setting up res3a_branch2b/bn
I0817 10:57:50.538619 14586 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 28 28 (5017600)
I0817 10:57:50.538625 14586 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0817 10:57:50.538627 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.538630 14586 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0817 10:57:50.538632 14586 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0817 10:57:50.538635 14586 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0817 10:57:50.538638 14586 net.cpp:245] Setting up res3a_branch2b/relu
I0817 10:57:50.538641 14586 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 28 28 (5017600)
I0817 10:57:50.538643 14586 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0817 10:57:50.538645 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.538650 14586 net.cpp:184] Created Layer pool3 (23)
I0817 10:57:50.538651 14586 net.cpp:561] pool3 <- res3a_branch2b
I0817 10:57:50.538653 14586 net.cpp:530] pool3 -> pool3
I0817 10:57:50.538687 14586 net.cpp:245] Setting up pool3
I0817 10:57:50.538691 14586 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 14 14 (1254400)
I0817 10:57:50.538694 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0817 10:57:50.538697 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.538707 14586 net.cpp:184] Created Layer res4a_branch2a (24)
I0817 10:57:50.538710 14586 net.cpp:561] res4a_branch2a <- pool3
I0817 10:57:50.538713 14586 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0817 10:57:50.552230 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.75G, req 0G)
I0817 10:57:50.552248 14586 net.cpp:245] Setting up res4a_branch2a
I0817 10:57:50.552253 14586 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 14 14 (2508800)
I0817 10:57:50.552258 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0817 10:57:50.552263 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.552268 14586 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0817 10:57:50.552271 14586 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0817 10:57:50.552274 14586 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0817 10:57:50.552709 14586 net.cpp:245] Setting up res4a_branch2a/bn
I0817 10:57:50.552716 14586 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 14 14 (2508800)
I0817 10:57:50.552722 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0817 10:57:50.552724 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.552739 14586 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0817 10:57:50.552742 14586 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0817 10:57:50.552744 14586 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0817 10:57:50.552748 14586 net.cpp:245] Setting up res4a_branch2a/relu
I0817 10:57:50.552752 14586 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 14 14 (2508800)
I0817 10:57:50.552753 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0817 10:57:50.552755 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.552762 14586 net.cpp:184] Created Layer res4a_branch2b (27)
I0817 10:57:50.552765 14586 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0817 10:57:50.552767 14586 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0817 10:57:50.558807 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.74G, req 0G)
I0817 10:57:50.558817 14586 net.cpp:245] Setting up res4a_branch2b
I0817 10:57:50.558822 14586 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 14 14 (2508800)
I0817 10:57:50.558826 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0817 10:57:50.558830 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.558835 14586 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0817 10:57:50.558837 14586 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0817 10:57:50.558840 14586 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0817 10:57:50.559240 14586 net.cpp:245] Setting up res4a_branch2b/bn
I0817 10:57:50.559299 14586 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 14 14 (2508800)
I0817 10:57:50.559306 14586 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0817 10:57:50.559309 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.559312 14586 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0817 10:57:50.559315 14586 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0817 10:57:50.559317 14586 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0817 10:57:50.559320 14586 net.cpp:245] Setting up res4a_branch2b/relu
I0817 10:57:50.559324 14586 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 14 14 (2508800)
I0817 10:57:50.559325 14586 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0817 10:57:50.559327 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.559331 14586 net.cpp:184] Created Layer pool4 (30)
I0817 10:57:50.559334 14586 net.cpp:561] pool4 <- res4a_branch2b
I0817 10:57:50.559336 14586 net.cpp:530] pool4 -> pool4
I0817 10:57:50.559368 14586 net.cpp:245] Setting up pool4
I0817 10:57:50.559372 14586 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 7 7 (627200)
I0817 10:57:50.559376 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0817 10:57:50.559377 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.559382 14586 net.cpp:184] Created Layer res5a_branch2a (31)
I0817 10:57:50.559386 14586 net.cpp:561] res5a_branch2a <- pool4
I0817 10:57:50.559388 14586 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0817 10:57:50.592303 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0817 10:57:50.592319 14586 net.cpp:245] Setting up res5a_branch2a
I0817 10:57:50.592324 14586 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 7 7 (1254400)
I0817 10:57:50.592330 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0817 10:57:50.592334 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.592344 14586 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0817 10:57:50.592346 14586 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0817 10:57:50.592365 14586 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0817 10:57:50.592803 14586 net.cpp:245] Setting up res5a_branch2a/bn
I0817 10:57:50.592809 14586 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 7 7 (1254400)
I0817 10:57:50.592815 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0817 10:57:50.592818 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.592821 14586 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0817 10:57:50.592823 14586 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0817 10:57:50.592825 14586 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0817 10:57:50.592829 14586 net.cpp:245] Setting up res5a_branch2a/relu
I0817 10:57:50.592831 14586 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 7 7 (1254400)
I0817 10:57:50.592833 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0817 10:57:50.592836 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.592842 14586 net.cpp:184] Created Layer res5a_branch2b (34)
I0817 10:57:50.592844 14586 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0817 10:57:50.592846 14586 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0817 10:57:50.608578 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 1  (limit 7.71G, req 0G)
I0817 10:57:50.608595 14586 net.cpp:245] Setting up res5a_branch2b
I0817 10:57:50.608600 14586 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 7 7 (1254400)
I0817 10:57:50.608610 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0817 10:57:50.608613 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.608621 14586 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0817 10:57:50.608624 14586 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0817 10:57:50.608628 14586 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0817 10:57:50.609069 14586 net.cpp:245] Setting up res5a_branch2b/bn
I0817 10:57:50.609076 14586 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 7 7 (1254400)
I0817 10:57:50.609082 14586 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0817 10:57:50.609086 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.609088 14586 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0817 10:57:50.609091 14586 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0817 10:57:50.609093 14586 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0817 10:57:50.609097 14586 net.cpp:245] Setting up res5a_branch2b/relu
I0817 10:57:50.609099 14586 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 7 7 (1254400)
I0817 10:57:50.609102 14586 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0817 10:57:50.609104 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.609108 14586 net.cpp:184] Created Layer pool5 (37)
I0817 10:57:50.609110 14586 net.cpp:561] pool5 <- res5a_branch2b
I0817 10:57:50.609112 14586 net.cpp:530] pool5 -> pool5
I0817 10:57:50.609130 14586 net.cpp:245] Setting up pool5
I0817 10:57:50.609134 14586 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0817 10:57:50.609136 14586 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0817 10:57:50.609138 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.609143 14586 net.cpp:184] Created Layer fc1000 (38)
I0817 10:57:50.609146 14586 net.cpp:561] fc1000 <- pool5
I0817 10:57:50.609148 14586 net.cpp:530] fc1000 -> fc1000
I0817 10:57:50.619825 14586 net.cpp:245] Setting up fc1000
I0817 10:57:50.619835 14586 net.cpp:252] TEST Top shape for layer 38 'fc1000' 50 1000 (50000)
I0817 10:57:50.619848 14586 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0817 10:57:50.619851 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.619855 14586 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0817 10:57:50.619858 14586 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0817 10:57:50.619861 14586 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0817 10:57:50.619864 14586 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0817 10:57:50.619868 14586 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0817 10:57:50.619902 14586 net.cpp:245] Setting up fc1000_fc1000_0_split
I0817 10:57:50.619906 14586 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0817 10:57:50.619910 14586 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0817 10:57:50.619911 14586 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0817 10:57:50.619913 14586 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0817 10:57:50.619916 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.619925 14586 net.cpp:184] Created Layer loss (40)
I0817 10:57:50.619927 14586 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0817 10:57:50.619930 14586 net.cpp:561] loss <- label_data_1_split_0
I0817 10:57:50.619933 14586 net.cpp:530] loss -> loss
I0817 10:57:50.620051 14586 net.cpp:245] Setting up loss
I0817 10:57:50.620057 14586 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0817 10:57:50.620060 14586 net.cpp:256]     with loss weight 1
I0817 10:57:50.620065 14586 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0817 10:57:50.620067 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.620076 14586 net.cpp:184] Created Layer accuracy/top1 (41)
I0817 10:57:50.620079 14586 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0817 10:57:50.620081 14586 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0817 10:57:50.620085 14586 net.cpp:530] accuracy/top1 -> accuracy/top1
I0817 10:57:50.620090 14586 net.cpp:245] Setting up accuracy/top1
I0817 10:57:50.620093 14586 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0817 10:57:50.620096 14586 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0817 10:57:50.620098 14586 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:57:50.620105 14586 net.cpp:184] Created Layer accuracy/top5 (42)
I0817 10:57:50.620108 14586 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0817 10:57:50.620110 14586 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0817 10:57:50.620113 14586 net.cpp:530] accuracy/top5 -> accuracy/top5
I0817 10:57:50.620117 14586 net.cpp:245] Setting up accuracy/top5
I0817 10:57:50.620121 14586 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0817 10:57:50.620122 14586 net.cpp:325] accuracy/top5 does not need backward computation.
I0817 10:57:50.620126 14586 net.cpp:325] accuracy/top1 does not need backward computation.
I0817 10:57:50.620127 14586 net.cpp:323] loss needs backward computation.
I0817 10:57:50.620133 14586 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0817 10:57:50.620137 14586 net.cpp:323] fc1000 needs backward computation.
I0817 10:57:50.620139 14586 net.cpp:323] pool5 needs backward computation.
I0817 10:57:50.620141 14586 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0817 10:57:50.620143 14586 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0817 10:57:50.620146 14586 net.cpp:323] res5a_branch2b needs backward computation.
I0817 10:57:50.620147 14586 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0817 10:57:50.620149 14586 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0817 10:57:50.620151 14586 net.cpp:323] res5a_branch2a needs backward computation.
I0817 10:57:50.620159 14586 net.cpp:323] pool4 needs backward computation.
I0817 10:57:50.620162 14586 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0817 10:57:50.620163 14586 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0817 10:57:50.620165 14586 net.cpp:323] res4a_branch2b needs backward computation.
I0817 10:57:50.620167 14586 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0817 10:57:50.620169 14586 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0817 10:57:50.620170 14586 net.cpp:323] res4a_branch2a needs backward computation.
I0817 10:57:50.620172 14586 net.cpp:323] pool3 needs backward computation.
I0817 10:57:50.620174 14586 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0817 10:57:50.620177 14586 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0817 10:57:50.620178 14586 net.cpp:323] res3a_branch2b needs backward computation.
I0817 10:57:50.620180 14586 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0817 10:57:50.620182 14586 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0817 10:57:50.620184 14586 net.cpp:323] res3a_branch2a needs backward computation.
I0817 10:57:50.620185 14586 net.cpp:323] pool2 needs backward computation.
I0817 10:57:50.620187 14586 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0817 10:57:50.620189 14586 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0817 10:57:50.620192 14586 net.cpp:323] res2a_branch2b needs backward computation.
I0817 10:57:50.620193 14586 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0817 10:57:50.620195 14586 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0817 10:57:50.620196 14586 net.cpp:323] res2a_branch2a needs backward computation.
I0817 10:57:50.620198 14586 net.cpp:323] pool1 needs backward computation.
I0817 10:57:50.620200 14586 net.cpp:323] conv1b/relu needs backward computation.
I0817 10:57:50.620203 14586 net.cpp:323] conv1b/bn needs backward computation.
I0817 10:57:50.620205 14586 net.cpp:323] conv1b needs backward computation.
I0817 10:57:50.620208 14586 net.cpp:323] conv1a/relu needs backward computation.
I0817 10:57:50.620209 14586 net.cpp:323] conv1a/bn needs backward computation.
I0817 10:57:50.620211 14586 net.cpp:323] conv1a needs backward computation.
I0817 10:57:50.620213 14586 net.cpp:325] data/bias does not need backward computation.
I0817 10:57:50.620218 14586 net.cpp:325] label_data_1_split does not need backward computation.
I0817 10:57:50.620221 14586 net.cpp:325] data does not need backward computation.
I0817 10:57:50.620224 14586 net.cpp:367] This network produces output accuracy/top1
I0817 10:57:50.620229 14586 net.cpp:367] This network produces output accuracy/top5
I0817 10:57:50.620231 14586 net.cpp:367] This network produces output loss
I0817 10:57:50.620263 14586 net.cpp:389] Top memory (TEST) required for data: 933273600 diff: 8
I0817 10:57:50.620267 14586 net.cpp:392] Bottom memory (TEST) required for data: 933273600 diff: 933273600
I0817 10:57:50.620270 14586 net.cpp:395] Shared (in-place) memory (TEST) by data: 622182400 diff: 622182400
I0817 10:57:50.620273 14586 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0817 10:57:50.620276 14586 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0817 10:57:50.620280 14586 net.cpp:407] Network initialization done.
I0817 10:57:50.624609 14586 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0817 10:57:50.624630 14586 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0817 10:57:50.624663 14586 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0817 10:57:50.624678 14586 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0817 10:57:50.624826 14586 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0817 10:57:50.624831 14586 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0817 10:57:50.624843 14586 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0817 10:57:50.624948 14586 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0817 10:57:50.624953 14586 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0817 10:57:50.624956 14586 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0817 10:57:50.624974 14586 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625066 14586 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0817 10:57:50.625072 14586 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0817 10:57:50.625087 14586 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625175 14586 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0817 10:57:50.625180 14586 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0817 10:57:50.625183 14586 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0817 10:57:50.625222 14586 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625308 14586 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0817 10:57:50.625313 14586 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0817 10:57:50.625337 14586 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625417 14586 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0817 10:57:50.625422 14586 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0817 10:57:50.625425 14586 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0817 10:57:50.625540 14586 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625622 14586 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0817 10:57:50.625627 14586 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0817 10:57:50.625685 14586 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:57:50.625766 14586 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0817 10:57:50.625771 14586 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0817 10:57:50.625774 14586 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0817 10:57:50.626142 14586 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:57:50.626229 14586 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0817 10:57:50.626233 14586 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0817 10:57:50.626410 14586 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:57:50.626488 14586 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0817 10:57:50.626492 14586 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0817 10:57:50.626497 14586 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0817 10:57:50.626602 14586 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0817 10:57:50.626646 14586 caffe.cpp:290] Running for 1000 iterations.
I0817 10:57:50.632796 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.68G, req 0G)
I0817 10:57:50.646978 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.52G, req 0G)
I0817 10:57:50.663513 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0817 10:57:50.670653 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.25G, req 0G)
I0817 10:57:50.682188 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.15G, req 0G)
I0817 10:57:50.687273 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0817 10:57:50.696476 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0817 10:57:50.700922 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.04G, req 0G)
I0817 10:57:50.709882 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.02G, req 0G)
I0817 10:57:50.714414 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 1  (limit 7G, req 0G)
I0817 10:57:50.749246 14586 caffe.cpp:313] Batch 0, accuracy/top1 = 0.56
I0817 10:57:50.749270 14586 caffe.cpp:313] Batch 0, accuracy/top5 = 0.8
I0817 10:57:50.749274 14586 caffe.cpp:313] Batch 0, loss = 1.76059
I0817 10:57:50.749276 14586 net.cpp:1620] Adding quantization params at infer/iter index: 1
I0817 10:57:50.757930 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 6.25G, req 0G)
I0817 10:57:50.777915 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0817 10:57:50.802887 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 6  (limit 5.51G, req 0G)
I0817 10:57:50.815687 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0817 10:57:50.830795 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 5.51G, req 0G)
I0817 10:57:50.836982 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0817 10:57:50.850697 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 5.51G, req 0G)
I0817 10:57:50.855952 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0817 10:57:50.876296 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 5.51G, req 0.05G)
I0817 10:57:50.882169 14586 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 1  (limit 5.51G, req 0.05G)
I0817 10:57:50.916241 14586 caffe.cpp:313] Batch 1, accuracy/top1 = 0.6
I0817 10:57:50.916267 14586 caffe.cpp:313] Batch 1, accuracy/top5 = 0.78
I0817 10:57:50.916272 14586 caffe.cpp:313] Batch 1, loss = 1.89292
I0817 10:57:50.979591 14586 caffe.cpp:313] Batch 2, accuracy/top1 = 0.54
I0817 10:57:50.979614 14586 caffe.cpp:313] Batch 2, accuracy/top5 = 0.76
I0817 10:57:50.979617 14586 caffe.cpp:313] Batch 2, loss = 2.22387
I0817 10:57:51.043289 14586 caffe.cpp:313] Batch 3, accuracy/top1 = 0.66
I0817 10:57:51.043308 14586 caffe.cpp:313] Batch 3, accuracy/top5 = 0.78
I0817 10:57:51.043310 14586 caffe.cpp:313] Batch 3, loss = 1.55652
I0817 10:57:51.106704 14586 caffe.cpp:313] Batch 4, accuracy/top1 = 0.58
I0817 10:57:51.106727 14586 caffe.cpp:313] Batch 4, accuracy/top5 = 0.8
I0817 10:57:51.106730 14586 caffe.cpp:313] Batch 4, loss = 1.84934
I0817 10:57:51.170087 14586 caffe.cpp:313] Batch 5, accuracy/top1 = 0.62
I0817 10:57:51.170110 14586 caffe.cpp:313] Batch 5, accuracy/top5 = 0.82
I0817 10:57:51.170114 14586 caffe.cpp:313] Batch 5, loss = 1.49525
I0817 10:57:51.233534 14586 caffe.cpp:313] Batch 6, accuracy/top1 = 0.68
I0817 10:57:51.233557 14586 caffe.cpp:313] Batch 6, accuracy/top5 = 0.84
I0817 10:57:51.233561 14586 caffe.cpp:313] Batch 6, loss = 1.49569
I0817 10:57:51.296880 14586 caffe.cpp:313] Batch 7, accuracy/top1 = 0.6
I0817 10:57:51.296902 14586 caffe.cpp:313] Batch 7, accuracy/top5 = 0.88
I0817 10:57:51.296905 14586 caffe.cpp:313] Batch 7, loss = 1.64795
I0817 10:57:51.360206 14586 caffe.cpp:313] Batch 8, accuracy/top1 = 0.68
I0817 10:57:51.360229 14586 caffe.cpp:313] Batch 8, accuracy/top5 = 0.86
I0817 10:57:51.360231 14586 caffe.cpp:313] Batch 8, loss = 1.40594
I0817 10:57:51.423601 14586 caffe.cpp:313] Batch 9, accuracy/top1 = 0.54
I0817 10:57:51.423624 14586 caffe.cpp:313] Batch 9, accuracy/top5 = 0.88
I0817 10:57:51.423627 14586 caffe.cpp:313] Batch 9, loss = 1.76956
I0817 10:57:51.486692 14586 caffe.cpp:313] Batch 10, accuracy/top1 = 0.56
I0817 10:57:51.486714 14586 caffe.cpp:313] Batch 10, accuracy/top5 = 0.9
I0817 10:57:51.486717 14586 caffe.cpp:313] Batch 10, loss = 1.49453
I0817 10:57:51.549417 14586 caffe.cpp:313] Batch 11, accuracy/top1 = 0.56
I0817 10:57:51.549435 14586 caffe.cpp:313] Batch 11, accuracy/top5 = 0.84
I0817 10:57:51.549438 14586 caffe.cpp:313] Batch 11, loss = 1.46103
I0817 10:57:51.612306 14586 caffe.cpp:313] Batch 12, accuracy/top1 = 0.62
I0817 10:57:51.612326 14586 caffe.cpp:313] Batch 12, accuracy/top5 = 0.92
I0817 10:57:51.612329 14586 caffe.cpp:313] Batch 12, loss = 1.52319
I0817 10:57:51.675375 14586 caffe.cpp:313] Batch 13, accuracy/top1 = 0.6
I0817 10:57:51.675396 14586 caffe.cpp:313] Batch 13, accuracy/top5 = 0.84
I0817 10:57:51.675400 14586 caffe.cpp:313] Batch 13, loss = 1.43364
I0817 10:57:51.738251 14586 caffe.cpp:313] Batch 14, accuracy/top1 = 0.4
I0817 10:57:51.738273 14586 caffe.cpp:313] Batch 14, accuracy/top5 = 0.82
I0817 10:57:51.738276 14586 caffe.cpp:313] Batch 14, loss = 2.19524
I0817 10:57:51.800796 14586 caffe.cpp:313] Batch 15, accuracy/top1 = 0.54
I0817 10:57:51.800817 14586 caffe.cpp:313] Batch 15, accuracy/top5 = 0.64
I0817 10:57:51.800820 14586 caffe.cpp:313] Batch 15, loss = 2.49439
I0817 10:57:51.863694 14586 caffe.cpp:313] Batch 16, accuracy/top1 = 0.46
I0817 10:57:51.863718 14586 caffe.cpp:313] Batch 16, accuracy/top5 = 0.84
I0817 10:57:51.863720 14586 caffe.cpp:313] Batch 16, loss = 2.05766
I0817 10:57:51.926304 14586 caffe.cpp:313] Batch 17, accuracy/top1 = 0.54
I0817 10:57:51.926326 14586 caffe.cpp:313] Batch 17, accuracy/top5 = 0.78
I0817 10:57:51.926329 14586 caffe.cpp:313] Batch 17, loss = 2.0313
I0817 10:57:51.989045 14586 caffe.cpp:313] Batch 18, accuracy/top1 = 0.62
I0817 10:57:51.989068 14586 caffe.cpp:313] Batch 18, accuracy/top5 = 0.88
I0817 10:57:51.989071 14586 caffe.cpp:313] Batch 18, loss = 1.68275
I0817 10:57:52.051730 14586 caffe.cpp:313] Batch 19, accuracy/top1 = 0.58
I0817 10:57:52.051749 14586 caffe.cpp:313] Batch 19, accuracy/top5 = 0.9
I0817 10:57:52.051753 14586 caffe.cpp:313] Batch 19, loss = 1.4788
I0817 10:57:52.114521 14586 caffe.cpp:313] Batch 20, accuracy/top1 = 0.58
I0817 10:57:52.114543 14586 caffe.cpp:313] Batch 20, accuracy/top5 = 0.78
I0817 10:57:52.114545 14586 caffe.cpp:313] Batch 20, loss = 1.73114
I0817 10:57:52.177261 14586 caffe.cpp:313] Batch 21, accuracy/top1 = 0.6
I0817 10:57:52.177283 14586 caffe.cpp:313] Batch 21, accuracy/top5 = 0.76
I0817 10:57:52.177285 14586 caffe.cpp:313] Batch 21, loss = 1.83138
I0817 10:57:52.240026 14586 caffe.cpp:313] Batch 22, accuracy/top1 = 0.5
I0817 10:57:52.240048 14586 caffe.cpp:313] Batch 22, accuracy/top5 = 0.8
I0817 10:57:52.240051 14586 caffe.cpp:313] Batch 22, loss = 2.14195
I0817 10:57:52.302752 14586 caffe.cpp:313] Batch 23, accuracy/top1 = 0.62
I0817 10:57:52.302774 14586 caffe.cpp:313] Batch 23, accuracy/top5 = 0.78
I0817 10:57:52.302778 14586 caffe.cpp:313] Batch 23, loss = 1.92474
I0817 10:57:52.365460 14586 caffe.cpp:313] Batch 24, accuracy/top1 = 0.62
I0817 10:57:52.365483 14586 caffe.cpp:313] Batch 24, accuracy/top5 = 0.9
I0817 10:57:52.365485 14586 caffe.cpp:313] Batch 24, loss = 1.46839
I0817 10:57:52.428242 14586 caffe.cpp:313] Batch 25, accuracy/top1 = 0.58
I0817 10:57:52.428264 14586 caffe.cpp:313] Batch 25, accuracy/top5 = 0.84
I0817 10:57:52.428267 14586 caffe.cpp:313] Batch 25, loss = 1.70408
I0817 10:57:52.490773 14586 caffe.cpp:313] Batch 26, accuracy/top1 = 0.5
I0817 10:57:52.490795 14586 caffe.cpp:313] Batch 26, accuracy/top5 = 0.84
I0817 10:57:52.490798 14586 caffe.cpp:313] Batch 26, loss = 1.88408
I0817 10:57:52.553411 14586 caffe.cpp:313] Batch 27, accuracy/top1 = 0.4
I0817 10:57:52.553428 14586 caffe.cpp:313] Batch 27, accuracy/top5 = 0.58
I0817 10:57:52.553431 14586 caffe.cpp:313] Batch 27, loss = 2.77319
I0817 10:57:52.616106 14586 caffe.cpp:313] Batch 28, accuracy/top1 = 0.62
I0817 10:57:52.616127 14586 caffe.cpp:313] Batch 28, accuracy/top5 = 0.8
I0817 10:57:52.616132 14586 caffe.cpp:313] Batch 28, loss = 1.56521
I0817 10:57:52.678856 14586 caffe.cpp:313] Batch 29, accuracy/top1 = 0.52
I0817 10:57:52.678877 14586 caffe.cpp:313] Batch 29, accuracy/top5 = 0.74
I0817 10:57:52.678880 14586 caffe.cpp:313] Batch 29, loss = 2.18478
I0817 10:57:52.741576 14586 caffe.cpp:313] Batch 30, accuracy/top1 = 0.56
I0817 10:57:52.741597 14586 caffe.cpp:313] Batch 30, accuracy/top5 = 0.78
I0817 10:57:52.741600 14586 caffe.cpp:313] Batch 30, loss = 2.1273
I0817 10:57:52.804214 14586 caffe.cpp:313] Batch 31, accuracy/top1 = 0.6
I0817 10:57:52.804235 14586 caffe.cpp:313] Batch 31, accuracy/top5 = 0.82
I0817 10:57:52.804239 14586 caffe.cpp:313] Batch 31, loss = 1.83202
I0817 10:57:52.867027 14586 caffe.cpp:313] Batch 32, accuracy/top1 = 0.56
I0817 10:57:52.867048 14586 caffe.cpp:313] Batch 32, accuracy/top5 = 0.78
I0817 10:57:52.867050 14586 caffe.cpp:313] Batch 32, loss = 2.07202
I0817 10:57:52.929792 14586 caffe.cpp:313] Batch 33, accuracy/top1 = 0.6
I0817 10:57:52.929814 14586 caffe.cpp:313] Batch 33, accuracy/top5 = 0.8
I0817 10:57:52.929817 14586 caffe.cpp:313] Batch 33, loss = 1.77043
I0817 10:57:52.992394 14586 caffe.cpp:313] Batch 34, accuracy/top1 = 0.48
I0817 10:57:52.992416 14586 caffe.cpp:313] Batch 34, accuracy/top5 = 0.84
I0817 10:57:52.992419 14586 caffe.cpp:313] Batch 34, loss = 1.94878
I0817 10:57:53.055146 14586 caffe.cpp:313] Batch 35, accuracy/top1 = 0.68
I0817 10:57:53.055166 14586 caffe.cpp:313] Batch 35, accuracy/top5 = 0.8
I0817 10:57:53.055169 14586 caffe.cpp:313] Batch 35, loss = 1.58228
I0817 10:57:53.117908 14586 caffe.cpp:313] Batch 36, accuracy/top1 = 0.56
I0817 10:57:53.117929 14586 caffe.cpp:313] Batch 36, accuracy/top5 = 0.72
I0817 10:57:53.117933 14586 caffe.cpp:313] Batch 36, loss = 2.00805
I0817 10:57:53.180560 14586 caffe.cpp:313] Batch 37, accuracy/top1 = 0.66
I0817 10:57:53.180583 14586 caffe.cpp:313] Batch 37, accuracy/top5 = 0.84
I0817 10:57:53.180585 14586 caffe.cpp:313] Batch 37, loss = 1.54862
I0817 10:57:53.243149 14586 caffe.cpp:313] Batch 38, accuracy/top1 = 0.46
I0817 10:57:53.243170 14586 caffe.cpp:313] Batch 38, accuracy/top5 = 0.72
I0817 10:57:53.243173 14586 caffe.cpp:313] Batch 38, loss = 2.7449
I0817 10:57:53.305663 14586 caffe.cpp:313] Batch 39, accuracy/top1 = 0.62
I0817 10:57:53.305685 14586 caffe.cpp:313] Batch 39, accuracy/top5 = 0.82
I0817 10:57:53.305688 14586 caffe.cpp:313] Batch 39, loss = 1.74577
I0817 10:57:53.368383 14586 caffe.cpp:313] Batch 40, accuracy/top1 = 0.5
I0817 10:57:53.368404 14586 caffe.cpp:313] Batch 40, accuracy/top5 = 0.76
I0817 10:57:53.368407 14586 caffe.cpp:313] Batch 40, loss = 2.37991
I0817 10:57:53.431022 14586 caffe.cpp:313] Batch 41, accuracy/top1 = 0.6
I0817 10:57:53.431044 14586 caffe.cpp:313] Batch 41, accuracy/top5 = 0.86
I0817 10:57:53.431047 14586 caffe.cpp:313] Batch 41, loss = 1.54302
I0817 10:57:53.493621 14586 caffe.cpp:313] Batch 42, accuracy/top1 = 0.58
I0817 10:57:53.493643 14586 caffe.cpp:313] Batch 42, accuracy/top5 = 0.8
I0817 10:57:53.493646 14586 caffe.cpp:313] Batch 42, loss = 1.94869
I0817 10:57:53.556147 14586 caffe.cpp:313] Batch 43, accuracy/top1 = 0.56
I0817 10:57:53.556169 14586 caffe.cpp:313] Batch 43, accuracy/top5 = 0.88
I0817 10:57:53.556172 14586 caffe.cpp:313] Batch 43, loss = 1.43864
I0817 10:57:53.618865 14586 caffe.cpp:313] Batch 44, accuracy/top1 = 0.52
I0817 10:57:53.618887 14586 caffe.cpp:313] Batch 44, accuracy/top5 = 0.8
I0817 10:57:53.618891 14586 caffe.cpp:313] Batch 44, loss = 1.75962
I0817 10:57:53.681476 14586 caffe.cpp:313] Batch 45, accuracy/top1 = 0.64
I0817 10:57:53.681498 14586 caffe.cpp:313] Batch 45, accuracy/top5 = 0.78
I0817 10:57:53.681500 14586 caffe.cpp:313] Batch 45, loss = 2.19617
I0817 10:57:53.744374 14586 caffe.cpp:313] Batch 46, accuracy/top1 = 0.68
I0817 10:57:53.744392 14586 caffe.cpp:313] Batch 46, accuracy/top5 = 0.86
I0817 10:57:53.744395 14586 caffe.cpp:313] Batch 46, loss = 1.47133
I0817 10:57:53.807435 14586 caffe.cpp:313] Batch 47, accuracy/top1 = 0.54
I0817 10:57:53.807459 14586 caffe.cpp:313] Batch 47, accuracy/top5 = 0.82
I0817 10:57:53.807466 14586 caffe.cpp:313] Batch 47, loss = 2.07006
I0817 10:57:53.870391 14586 caffe.cpp:313] Batch 48, accuracy/top1 = 0.56
I0817 10:57:53.870411 14586 caffe.cpp:313] Batch 48, accuracy/top5 = 0.8
I0817 10:57:53.870414 14586 caffe.cpp:313] Batch 48, loss = 1.9907
I0817 10:57:53.932920 14586 caffe.cpp:313] Batch 49, accuracy/top1 = 0.6
I0817 10:57:53.932941 14586 caffe.cpp:313] Batch 49, accuracy/top5 = 0.84
I0817 10:57:53.932945 14586 caffe.cpp:313] Batch 49, loss = 1.58422
I0817 10:57:53.995560 14586 caffe.cpp:313] Batch 50, accuracy/top1 = 0.56
I0817 10:57:53.995581 14586 caffe.cpp:313] Batch 50, accuracy/top5 = 0.8
I0817 10:57:53.995584 14586 caffe.cpp:313] Batch 50, loss = 1.98026
I0817 10:57:54.058305 14586 caffe.cpp:313] Batch 51, accuracy/top1 = 0.62
I0817 10:57:54.058324 14586 caffe.cpp:313] Batch 51, accuracy/top5 = 0.76
I0817 10:57:54.058327 14586 caffe.cpp:313] Batch 51, loss = 1.97373
I0817 10:57:54.120982 14586 caffe.cpp:313] Batch 52, accuracy/top1 = 0.58
I0817 10:57:54.121003 14586 caffe.cpp:313] Batch 52, accuracy/top5 = 0.8
I0817 10:57:54.121006 14586 caffe.cpp:313] Batch 52, loss = 1.80765
I0817 10:57:54.183593 14586 caffe.cpp:313] Batch 53, accuracy/top1 = 0.52
I0817 10:57:54.183614 14586 caffe.cpp:313] Batch 53, accuracy/top5 = 0.74
I0817 10:57:54.183616 14586 caffe.cpp:313] Batch 53, loss = 2.31881
I0817 10:57:54.246362 14586 caffe.cpp:313] Batch 54, accuracy/top1 = 0.66
I0817 10:57:54.246383 14586 caffe.cpp:313] Batch 54, accuracy/top5 = 0.9
I0817 10:57:54.246387 14586 caffe.cpp:313] Batch 54, loss = 1.30271
I0817 10:57:54.308897 14586 caffe.cpp:313] Batch 55, accuracy/top1 = 0.52
I0817 10:57:54.308918 14586 caffe.cpp:313] Batch 55, accuracy/top5 = 0.86
I0817 10:57:54.308920 14586 caffe.cpp:313] Batch 55, loss = 1.58272
I0817 10:57:54.371451 14586 caffe.cpp:313] Batch 56, accuracy/top1 = 0.6
I0817 10:57:54.371474 14586 caffe.cpp:313] Batch 56, accuracy/top5 = 0.92
I0817 10:57:54.371476 14586 caffe.cpp:313] Batch 56, loss = 1.26041
I0817 10:57:54.433982 14586 caffe.cpp:313] Batch 57, accuracy/top1 = 0.62
I0817 10:57:54.434003 14586 caffe.cpp:313] Batch 57, accuracy/top5 = 0.82
I0817 10:57:54.434006 14586 caffe.cpp:313] Batch 57, loss = 1.94489
I0817 10:57:54.496585 14586 caffe.cpp:313] Batch 58, accuracy/top1 = 0.68
I0817 10:57:54.496608 14586 caffe.cpp:313] Batch 58, accuracy/top5 = 0.88
I0817 10:57:54.496611 14586 caffe.cpp:313] Batch 58, loss = 1.41544
I0817 10:57:54.559353 14586 caffe.cpp:313] Batch 59, accuracy/top1 = 0.72
I0817 10:57:54.559375 14586 caffe.cpp:313] Batch 59, accuracy/top5 = 0.86
I0817 10:57:54.559377 14586 caffe.cpp:313] Batch 59, loss = 1.2853
I0817 10:57:54.622099 14586 caffe.cpp:313] Batch 60, accuracy/top1 = 0.68
I0817 10:57:54.622120 14586 caffe.cpp:313] Batch 60, accuracy/top5 = 0.88
I0817 10:57:54.622123 14586 caffe.cpp:313] Batch 60, loss = 1.45283
I0817 10:57:54.684844 14586 caffe.cpp:313] Batch 61, accuracy/top1 = 0.56
I0817 10:57:54.684865 14586 caffe.cpp:313] Batch 61, accuracy/top5 = 0.86
I0817 10:57:54.684867 14586 caffe.cpp:313] Batch 61, loss = 1.86753
I0817 10:57:54.747694 14586 caffe.cpp:313] Batch 62, accuracy/top1 = 0.52
I0817 10:57:54.747715 14586 caffe.cpp:313] Batch 62, accuracy/top5 = 0.88
I0817 10:57:54.747719 14586 caffe.cpp:313] Batch 62, loss = 1.44831
I0817 10:57:54.810582 14586 caffe.cpp:313] Batch 63, accuracy/top1 = 0.6
I0817 10:57:54.810602 14586 caffe.cpp:313] Batch 63, accuracy/top5 = 0.78
I0817 10:57:54.810606 14586 caffe.cpp:313] Batch 63, loss = 1.7154
I0817 10:57:54.873379 14586 caffe.cpp:313] Batch 64, accuracy/top1 = 0.52
I0817 10:57:54.873400 14586 caffe.cpp:313] Batch 64, accuracy/top5 = 0.74
I0817 10:57:54.873404 14586 caffe.cpp:313] Batch 64, loss = 1.95209
I0817 10:57:54.935953 14586 caffe.cpp:313] Batch 65, accuracy/top1 = 0.54
I0817 10:57:54.935976 14586 caffe.cpp:313] Batch 65, accuracy/top5 = 0.76
I0817 10:57:54.935978 14586 caffe.cpp:313] Batch 65, loss = 1.97649
I0817 10:57:54.998503 14586 caffe.cpp:313] Batch 66, accuracy/top1 = 0.66
I0817 10:57:54.998524 14586 caffe.cpp:313] Batch 66, accuracy/top5 = 0.78
I0817 10:57:54.998528 14586 caffe.cpp:313] Batch 66, loss = 1.56599
I0817 10:57:55.061188 14586 caffe.cpp:313] Batch 67, accuracy/top1 = 0.54
I0817 10:57:55.061209 14586 caffe.cpp:313] Batch 67, accuracy/top5 = 0.8
I0817 10:57:55.061213 14586 caffe.cpp:313] Batch 67, loss = 1.86031
I0817 10:57:55.123952 14586 caffe.cpp:313] Batch 68, accuracy/top1 = 0.66
I0817 10:57:55.123972 14586 caffe.cpp:313] Batch 68, accuracy/top5 = 0.88
I0817 10:57:55.123975 14586 caffe.cpp:313] Batch 68, loss = 1.33767
I0817 10:57:55.186655 14586 caffe.cpp:313] Batch 69, accuracy/top1 = 0.58
I0817 10:57:55.186677 14586 caffe.cpp:313] Batch 69, accuracy/top5 = 0.78
I0817 10:57:55.186681 14586 caffe.cpp:313] Batch 69, loss = 1.8876
I0817 10:57:55.249305 14586 caffe.cpp:313] Batch 70, accuracy/top1 = 0.46
I0817 10:57:55.249326 14586 caffe.cpp:313] Batch 70, accuracy/top5 = 0.72
I0817 10:57:55.249330 14586 caffe.cpp:313] Batch 70, loss = 2.24997
I0817 10:57:55.311908 14586 caffe.cpp:313] Batch 71, accuracy/top1 = 0.46
I0817 10:57:55.311928 14586 caffe.cpp:313] Batch 71, accuracy/top5 = 0.76
I0817 10:57:55.311931 14586 caffe.cpp:313] Batch 71, loss = 2.31639
I0817 10:57:55.374552 14586 caffe.cpp:313] Batch 72, accuracy/top1 = 0.62
I0817 10:57:55.374574 14586 caffe.cpp:313] Batch 72, accuracy/top5 = 0.82
I0817 10:57:55.374577 14586 caffe.cpp:313] Batch 72, loss = 1.55839
I0817 10:57:55.437122 14586 caffe.cpp:313] Batch 73, accuracy/top1 = 0.62
I0817 10:57:55.437144 14586 caffe.cpp:313] Batch 73, accuracy/top5 = 0.86
I0817 10:57:55.437147 14586 caffe.cpp:313] Batch 73, loss = 1.60004
I0817 10:57:55.499799 14586 caffe.cpp:313] Batch 74, accuracy/top1 = 0.58
I0817 10:57:55.499821 14586 caffe.cpp:313] Batch 74, accuracy/top5 = 0.84
I0817 10:57:55.499825 14586 caffe.cpp:313] Batch 74, loss = 1.65913
I0817 10:57:55.562505 14586 caffe.cpp:313] Batch 75, accuracy/top1 = 0.6
I0817 10:57:55.562527 14586 caffe.cpp:313] Batch 75, accuracy/top5 = 0.78
I0817 10:57:55.562530 14586 caffe.cpp:313] Batch 75, loss = 1.85807
I0817 10:57:55.625185 14586 caffe.cpp:313] Batch 76, accuracy/top1 = 0.6
I0817 10:57:55.625207 14586 caffe.cpp:313] Batch 76, accuracy/top5 = 0.72
I0817 10:57:55.625211 14586 caffe.cpp:313] Batch 76, loss = 1.92032
I0817 10:57:55.687929 14586 caffe.cpp:313] Batch 77, accuracy/top1 = 0.6
I0817 10:57:55.687949 14586 caffe.cpp:313] Batch 77, accuracy/top5 = 0.88
I0817 10:57:55.687952 14586 caffe.cpp:313] Batch 77, loss = 1.56644
I0817 10:57:55.750633 14586 caffe.cpp:313] Batch 78, accuracy/top1 = 0.54
I0817 10:57:55.750653 14586 caffe.cpp:313] Batch 78, accuracy/top5 = 0.74
I0817 10:57:55.750656 14586 caffe.cpp:313] Batch 78, loss = 2.17431
I0817 10:57:55.813415 14586 caffe.cpp:313] Batch 79, accuracy/top1 = 0.74
I0817 10:57:55.813438 14586 caffe.cpp:313] Batch 79, accuracy/top5 = 0.88
I0817 10:57:55.813442 14586 caffe.cpp:313] Batch 79, loss = 1.28216
I0817 10:57:55.876536 14586 caffe.cpp:313] Batch 80, accuracy/top1 = 0.42
I0817 10:57:55.876556 14586 caffe.cpp:313] Batch 80, accuracy/top5 = 0.72
I0817 10:57:55.876560 14586 caffe.cpp:313] Batch 80, loss = 2.35615
I0817 10:57:55.939347 14586 caffe.cpp:313] Batch 81, accuracy/top1 = 0.68
I0817 10:57:55.939368 14586 caffe.cpp:313] Batch 81, accuracy/top5 = 0.82
I0817 10:57:55.939371 14586 caffe.cpp:313] Batch 81, loss = 1.6862
I0817 10:57:56.002151 14586 caffe.cpp:313] Batch 82, accuracy/top1 = 0.58
I0817 10:57:56.002174 14586 caffe.cpp:313] Batch 82, accuracy/top5 = 0.76
I0817 10:57:56.002178 14586 caffe.cpp:313] Batch 82, loss = 1.79458
I0817 10:57:56.064882 14586 caffe.cpp:313] Batch 83, accuracy/top1 = 0.58
I0817 10:57:56.064903 14586 caffe.cpp:313] Batch 83, accuracy/top5 = 0.72
I0817 10:57:56.064908 14586 caffe.cpp:313] Batch 83, loss = 2.27786
I0817 10:57:56.127538 14586 caffe.cpp:313] Batch 84, accuracy/top1 = 0.62
I0817 10:57:56.127562 14586 caffe.cpp:313] Batch 84, accuracy/top5 = 0.72
I0817 10:57:56.127565 14586 caffe.cpp:313] Batch 84, loss = 1.94173
I0817 10:57:56.190114 14586 caffe.cpp:313] Batch 85, accuracy/top1 = 0.72
I0817 10:57:56.190135 14586 caffe.cpp:313] Batch 85, accuracy/top5 = 0.92
I0817 10:57:56.190140 14586 caffe.cpp:313] Batch 85, loss = 1.1818
I0817 10:57:56.252650 14586 caffe.cpp:313] Batch 86, accuracy/top1 = 0.52
I0817 10:57:56.252671 14586 caffe.cpp:313] Batch 86, accuracy/top5 = 0.68
I0817 10:57:56.252673 14586 caffe.cpp:313] Batch 86, loss = 2.04404
I0817 10:57:56.315187 14586 caffe.cpp:313] Batch 87, accuracy/top1 = 0.62
I0817 10:57:56.315210 14586 caffe.cpp:313] Batch 87, accuracy/top5 = 0.8
I0817 10:57:56.315214 14586 caffe.cpp:313] Batch 87, loss = 1.69167
I0817 10:57:56.377946 14586 caffe.cpp:313] Batch 88, accuracy/top1 = 0.68
I0817 10:57:56.377969 14586 caffe.cpp:313] Batch 88, accuracy/top5 = 0.84
I0817 10:57:56.377972 14586 caffe.cpp:313] Batch 88, loss = 1.47968
I0817 10:57:56.440668 14586 caffe.cpp:313] Batch 89, accuracy/top1 = 0.56
I0817 10:57:56.440690 14586 caffe.cpp:313] Batch 89, accuracy/top5 = 0.78
I0817 10:57:56.440695 14586 caffe.cpp:313] Batch 89, loss = 2.03301
I0817 10:57:56.503404 14586 caffe.cpp:313] Batch 90, accuracy/top1 = 0.66
I0817 10:57:56.503427 14586 caffe.cpp:313] Batch 90, accuracy/top5 = 0.88
I0817 10:57:56.503430 14586 caffe.cpp:313] Batch 90, loss = 1.2904
I0817 10:57:56.566108 14586 caffe.cpp:313] Batch 91, accuracy/top1 = 0.62
I0817 10:57:56.566129 14586 caffe.cpp:313] Batch 91, accuracy/top5 = 0.74
I0817 10:57:56.566133 14586 caffe.cpp:313] Batch 91, loss = 2.06707
I0817 10:57:56.629509 14586 caffe.cpp:313] Batch 92, accuracy/top1 = 0.58
I0817 10:57:56.629528 14586 caffe.cpp:313] Batch 92, accuracy/top5 = 0.74
I0817 10:57:56.629531 14586 caffe.cpp:313] Batch 92, loss = 2.2136
I0817 10:57:56.692783 14586 caffe.cpp:313] Batch 93, accuracy/top1 = 0.6
I0817 10:57:56.692806 14586 caffe.cpp:313] Batch 93, accuracy/top5 = 0.88
I0817 10:57:56.692809 14586 caffe.cpp:313] Batch 93, loss = 1.80411
I0817 10:57:56.755609 14586 caffe.cpp:313] Batch 94, accuracy/top1 = 0.7
I0817 10:57:56.755632 14586 caffe.cpp:313] Batch 94, accuracy/top5 = 0.9
I0817 10:57:56.755635 14586 caffe.cpp:313] Batch 94, loss = 1.20374
I0817 10:57:56.818567 14586 caffe.cpp:313] Batch 95, accuracy/top1 = 0.52
I0817 10:57:56.818589 14586 caffe.cpp:313] Batch 95, accuracy/top5 = 0.76
I0817 10:57:56.818594 14586 caffe.cpp:313] Batch 95, loss = 1.99482
I0817 10:57:56.881438 14586 caffe.cpp:313] Batch 96, accuracy/top1 = 0.5
I0817 10:57:56.881460 14586 caffe.cpp:313] Batch 96, accuracy/top5 = 0.7
I0817 10:57:56.881465 14586 caffe.cpp:313] Batch 96, loss = 2.16252
I0817 10:57:56.944123 14586 caffe.cpp:313] Batch 97, accuracy/top1 = 0.64
I0817 10:57:56.944145 14586 caffe.cpp:313] Batch 97, accuracy/top5 = 0.82
I0817 10:57:56.944150 14586 caffe.cpp:313] Batch 97, loss = 1.58355
I0817 10:57:57.006845 14586 caffe.cpp:313] Batch 98, accuracy/top1 = 0.56
I0817 10:57:57.006868 14586 caffe.cpp:313] Batch 98, accuracy/top5 = 0.9
I0817 10:57:57.006872 14586 caffe.cpp:313] Batch 98, loss = 1.75274
I0817 10:57:57.069597 14586 caffe.cpp:313] Batch 99, accuracy/top1 = 0.6
I0817 10:57:57.069617 14586 caffe.cpp:313] Batch 99, accuracy/top5 = 0.82
I0817 10:57:57.069622 14586 caffe.cpp:313] Batch 99, loss = 1.73168
I0817 10:57:57.132164 14586 caffe.cpp:313] Batch 100, accuracy/top1 = 0.64
I0817 10:57:57.132187 14586 caffe.cpp:313] Batch 100, accuracy/top5 = 0.82
I0817 10:57:57.132191 14586 caffe.cpp:313] Batch 100, loss = 1.41638
I0817 10:57:57.194962 14586 caffe.cpp:313] Batch 101, accuracy/top1 = 0.68
I0817 10:57:57.194984 14586 caffe.cpp:313] Batch 101, accuracy/top5 = 0.84
I0817 10:57:57.194988 14586 caffe.cpp:313] Batch 101, loss = 1.54682
I0817 10:57:57.257711 14586 caffe.cpp:313] Batch 102, accuracy/top1 = 0.54
I0817 10:57:57.257733 14586 caffe.cpp:313] Batch 102, accuracy/top5 = 0.74
I0817 10:57:57.257738 14586 caffe.cpp:313] Batch 102, loss = 2.64861
I0817 10:57:57.320371 14586 caffe.cpp:313] Batch 103, accuracy/top1 = 0.72
I0817 10:57:57.320394 14586 caffe.cpp:313] Batch 103, accuracy/top5 = 0.9
I0817 10:57:57.320397 14586 caffe.cpp:313] Batch 103, loss = 1.32351
I0817 10:57:57.383038 14586 caffe.cpp:313] Batch 104, accuracy/top1 = 0.58
I0817 10:57:57.383060 14586 caffe.cpp:313] Batch 104, accuracy/top5 = 0.82
I0817 10:57:57.383064 14586 caffe.cpp:313] Batch 104, loss = 1.70292
I0817 10:57:57.445639 14586 caffe.cpp:313] Batch 105, accuracy/top1 = 0.62
I0817 10:57:57.445662 14586 caffe.cpp:313] Batch 105, accuracy/top5 = 0.72
I0817 10:57:57.445665 14586 caffe.cpp:313] Batch 105, loss = 1.93917
I0817 10:57:57.508272 14586 caffe.cpp:313] Batch 106, accuracy/top1 = 0.54
I0817 10:57:57.508294 14586 caffe.cpp:313] Batch 106, accuracy/top5 = 0.82
I0817 10:57:57.508298 14586 caffe.cpp:313] Batch 106, loss = 1.83209
I0817 10:57:57.570838 14586 caffe.cpp:313] Batch 107, accuracy/top1 = 0.48
I0817 10:57:57.570860 14586 caffe.cpp:313] Batch 107, accuracy/top5 = 0.72
I0817 10:57:57.570864 14586 caffe.cpp:313] Batch 107, loss = 2.02659
I0817 10:57:57.633514 14586 caffe.cpp:313] Batch 108, accuracy/top1 = 0.56
I0817 10:57:57.633538 14586 caffe.cpp:313] Batch 108, accuracy/top5 = 0.86
I0817 10:57:57.633543 14586 caffe.cpp:313] Batch 108, loss = 1.51517
I0817 10:57:57.696262 14586 caffe.cpp:313] Batch 109, accuracy/top1 = 0.68
I0817 10:57:57.696285 14586 caffe.cpp:313] Batch 109, accuracy/top5 = 0.76
I0817 10:57:57.696290 14586 caffe.cpp:313] Batch 109, loss = 1.38229
I0817 10:57:57.759160 14586 caffe.cpp:313] Batch 110, accuracy/top1 = 0.56
I0817 10:57:57.759182 14586 caffe.cpp:313] Batch 110, accuracy/top5 = 0.74
I0817 10:57:57.759186 14586 caffe.cpp:313] Batch 110, loss = 2.2348
I0817 10:57:57.822077 14586 caffe.cpp:313] Batch 111, accuracy/top1 = 0.48
I0817 10:57:57.822098 14586 caffe.cpp:313] Batch 111, accuracy/top5 = 0.8
I0817 10:57:57.822101 14586 caffe.cpp:313] Batch 111, loss = 2.02209
I0817 10:57:57.884855 14586 caffe.cpp:313] Batch 112, accuracy/top1 = 0.6
I0817 10:57:57.884876 14586 caffe.cpp:313] Batch 112, accuracy/top5 = 0.7
I0817 10:57:57.884879 14586 caffe.cpp:313] Batch 112, loss = 2.04168
I0817 10:57:57.947598 14586 caffe.cpp:313] Batch 113, accuracy/top1 = 0.6
I0817 10:57:57.947620 14586 caffe.cpp:313] Batch 113, accuracy/top5 = 0.84
I0817 10:57:57.947624 14586 caffe.cpp:313] Batch 113, loss = 1.37556
I0817 10:57:58.010315 14586 caffe.cpp:313] Batch 114, accuracy/top1 = 0.58
I0817 10:57:58.010335 14586 caffe.cpp:313] Batch 114, accuracy/top5 = 0.8
I0817 10:57:58.010339 14586 caffe.cpp:313] Batch 114, loss = 1.74078
I0817 10:57:58.073732 14586 caffe.cpp:313] Batch 115, accuracy/top1 = 0.56
I0817 10:57:58.073753 14586 caffe.cpp:313] Batch 115, accuracy/top5 = 0.82
I0817 10:57:58.073758 14586 caffe.cpp:313] Batch 115, loss = 1.91175
I0817 10:57:58.137236 14586 caffe.cpp:313] Batch 116, accuracy/top1 = 0.52
I0817 10:57:58.137259 14586 caffe.cpp:313] Batch 116, accuracy/top5 = 0.76
I0817 10:57:58.137262 14586 caffe.cpp:313] Batch 116, loss = 1.94746
I0817 10:57:58.199870 14586 caffe.cpp:313] Batch 117, accuracy/top1 = 0.48
I0817 10:57:58.199893 14586 caffe.cpp:313] Batch 117, accuracy/top5 = 0.84
I0817 10:57:58.199898 14586 caffe.cpp:313] Batch 117, loss = 2.14356
I0817 10:57:58.262686 14586 caffe.cpp:313] Batch 118, accuracy/top1 = 0.44
I0817 10:57:58.262707 14586 caffe.cpp:313] Batch 118, accuracy/top5 = 0.74
I0817 10:57:58.262712 14586 caffe.cpp:313] Batch 118, loss = 2.75622
I0817 10:57:58.325318 14586 caffe.cpp:313] Batch 119, accuracy/top1 = 0.62
I0817 10:57:58.325340 14586 caffe.cpp:313] Batch 119, accuracy/top5 = 0.88
I0817 10:57:58.325345 14586 caffe.cpp:313] Batch 119, loss = 1.50651
I0817 10:57:58.388072 14586 caffe.cpp:313] Batch 120, accuracy/top1 = 0.56
I0817 10:57:58.388094 14586 caffe.cpp:313] Batch 120, accuracy/top5 = 0.8
I0817 10:57:58.388098 14586 caffe.cpp:313] Batch 120, loss = 1.96157
I0817 10:57:58.450742 14586 caffe.cpp:313] Batch 121, accuracy/top1 = 0.6
I0817 10:57:58.450764 14586 caffe.cpp:313] Batch 121, accuracy/top5 = 0.82
I0817 10:57:58.450768 14586 caffe.cpp:313] Batch 121, loss = 1.74989
I0817 10:57:58.513425 14586 caffe.cpp:313] Batch 122, accuracy/top1 = 0.56
I0817 10:57:58.513448 14586 caffe.cpp:313] Batch 122, accuracy/top5 = 0.76
I0817 10:57:58.513453 14586 caffe.cpp:313] Batch 122, loss = 2.48693
I0817 10:57:58.576114 14586 caffe.cpp:313] Batch 123, accuracy/top1 = 0.56
I0817 10:57:58.576139 14586 caffe.cpp:313] Batch 123, accuracy/top5 = 0.76
I0817 10:57:58.576160 14586 caffe.cpp:313] Batch 123, loss = 2.07288
I0817 10:57:58.638845 14586 caffe.cpp:313] Batch 124, accuracy/top1 = 0.6
I0817 10:57:58.638864 14586 caffe.cpp:313] Batch 124, accuracy/top5 = 0.8
I0817 10:57:58.638867 14586 caffe.cpp:313] Batch 124, loss = 1.80008
I0817 10:57:58.701387 14586 caffe.cpp:313] Batch 125, accuracy/top1 = 0.6
I0817 10:57:58.701409 14586 caffe.cpp:313] Batch 125, accuracy/top5 = 0.9
I0817 10:57:58.701414 14586 caffe.cpp:313] Batch 125, loss = 1.49647
I0817 10:57:58.764155 14586 caffe.cpp:313] Batch 126, accuracy/top1 = 0.46
I0817 10:57:58.764178 14586 caffe.cpp:313] Batch 126, accuracy/top5 = 0.82
I0817 10:57:58.764183 14586 caffe.cpp:313] Batch 126, loss = 1.90199
I0817 10:57:58.827327 14586 caffe.cpp:313] Batch 127, accuracy/top1 = 0.48
I0817 10:57:58.827345 14586 caffe.cpp:313] Batch 127, accuracy/top5 = 0.8
I0817 10:57:58.827349 14586 caffe.cpp:313] Batch 127, loss = 2.13003
I0817 10:57:58.890012 14586 caffe.cpp:313] Batch 128, accuracy/top1 = 0.66
I0817 10:57:58.890033 14586 caffe.cpp:313] Batch 128, accuracy/top5 = 0.8
I0817 10:57:58.890038 14586 caffe.cpp:313] Batch 128, loss = 1.56317
I0817 10:57:58.952783 14586 caffe.cpp:313] Batch 129, accuracy/top1 = 0.52
I0817 10:57:58.952805 14586 caffe.cpp:313] Batch 129, accuracy/top5 = 0.82
I0817 10:57:58.952810 14586 caffe.cpp:313] Batch 129, loss = 1.70511
I0817 10:57:59.015434 14586 caffe.cpp:313] Batch 130, accuracy/top1 = 0.56
I0817 10:57:59.015456 14586 caffe.cpp:313] Batch 130, accuracy/top5 = 0.86
I0817 10:57:59.015460 14586 caffe.cpp:313] Batch 130, loss = 1.63818
I0817 10:57:59.078198 14586 caffe.cpp:313] Batch 131, accuracy/top1 = 0.62
I0817 10:57:59.078219 14586 caffe.cpp:313] Batch 131, accuracy/top5 = 0.86
I0817 10:57:59.078223 14586 caffe.cpp:313] Batch 131, loss = 1.4631
I0817 10:57:59.140825 14586 caffe.cpp:313] Batch 132, accuracy/top1 = 0.7
I0817 10:57:59.140848 14586 caffe.cpp:313] Batch 132, accuracy/top5 = 0.82
I0817 10:57:59.140852 14586 caffe.cpp:313] Batch 132, loss = 1.89883
I0817 10:57:59.203533 14586 caffe.cpp:313] Batch 133, accuracy/top1 = 0.56
I0817 10:57:59.203557 14586 caffe.cpp:313] Batch 133, accuracy/top5 = 0.8
I0817 10:57:59.203560 14586 caffe.cpp:313] Batch 133, loss = 2.06515
I0817 10:57:59.266214 14586 caffe.cpp:313] Batch 134, accuracy/top1 = 0.7
I0817 10:57:59.266237 14586 caffe.cpp:313] Batch 134, accuracy/top5 = 0.86
I0817 10:57:59.266242 14586 caffe.cpp:313] Batch 134, loss = 1.46174
I0817 10:57:59.329000 14586 caffe.cpp:313] Batch 135, accuracy/top1 = 0.5
I0817 10:57:59.329021 14586 caffe.cpp:313] Batch 135, accuracy/top5 = 0.84
I0817 10:57:59.329025 14586 caffe.cpp:313] Batch 135, loss = 1.81297
I0817 10:57:59.391571 14586 caffe.cpp:313] Batch 136, accuracy/top1 = 0.5
I0817 10:57:59.391593 14586 caffe.cpp:313] Batch 136, accuracy/top5 = 0.72
I0817 10:57:59.391597 14586 caffe.cpp:313] Batch 136, loss = 2.18077
I0817 10:57:59.454111 14586 caffe.cpp:313] Batch 137, accuracy/top1 = 0.56
I0817 10:57:59.454134 14586 caffe.cpp:313] Batch 137, accuracy/top5 = 0.88
I0817 10:57:59.454138 14586 caffe.cpp:313] Batch 137, loss = 1.4792
I0817 10:57:59.516649 14586 caffe.cpp:313] Batch 138, accuracy/top1 = 0.64
I0817 10:57:59.516672 14586 caffe.cpp:313] Batch 138, accuracy/top5 = 0.84
I0817 10:57:59.516676 14586 caffe.cpp:313] Batch 138, loss = 1.55811
I0817 10:57:59.579275 14586 caffe.cpp:313] Batch 139, accuracy/top1 = 0.62
I0817 10:57:59.579298 14586 caffe.cpp:313] Batch 139, accuracy/top5 = 0.74
I0817 10:57:59.579301 14586 caffe.cpp:313] Batch 139, loss = 2.11356
I0817 10:57:59.642007 14586 caffe.cpp:313] Batch 140, accuracy/top1 = 0.58
I0817 10:57:59.642030 14586 caffe.cpp:313] Batch 140, accuracy/top5 = 0.76
I0817 10:57:59.642033 14586 caffe.cpp:313] Batch 140, loss = 2.06228
I0817 10:57:59.704699 14586 caffe.cpp:313] Batch 141, accuracy/top1 = 0.6
I0817 10:57:59.704721 14586 caffe.cpp:313] Batch 141, accuracy/top5 = 0.88
I0817 10:57:59.704725 14586 caffe.cpp:313] Batch 141, loss = 1.44511
I0817 10:57:59.767439 14586 caffe.cpp:313] Batch 142, accuracy/top1 = 0.58
I0817 10:57:59.767474 14586 caffe.cpp:313] Batch 142, accuracy/top5 = 0.8
I0817 10:57:59.767478 14586 caffe.cpp:313] Batch 142, loss = 1.94373
I0817 10:57:59.830643 14586 caffe.cpp:313] Batch 143, accuracy/top1 = 0.52
I0817 10:57:59.830668 14586 caffe.cpp:313] Batch 143, accuracy/top5 = 0.74
I0817 10:57:59.830672 14586 caffe.cpp:313] Batch 143, loss = 2.16175
I0817 10:57:59.893448 14586 caffe.cpp:313] Batch 144, accuracy/top1 = 0.56
I0817 10:57:59.893471 14586 caffe.cpp:313] Batch 144, accuracy/top5 = 0.82
I0817 10:57:59.893474 14586 caffe.cpp:313] Batch 144, loss = 1.81736
I0817 10:57:59.956032 14586 caffe.cpp:313] Batch 145, accuracy/top1 = 0.68
I0817 10:57:59.956053 14586 caffe.cpp:313] Batch 145, accuracy/top5 = 0.88
I0817 10:57:59.956058 14586 caffe.cpp:313] Batch 145, loss = 1.56382
I0817 10:58:00.018630 14586 caffe.cpp:313] Batch 146, accuracy/top1 = 0.64
I0817 10:58:00.018652 14586 caffe.cpp:313] Batch 146, accuracy/top5 = 0.86
I0817 10:58:00.018656 14586 caffe.cpp:313] Batch 146, loss = 1.42823
I0817 10:58:00.081171 14586 caffe.cpp:313] Batch 147, accuracy/top1 = 0.62
I0817 10:58:00.081192 14586 caffe.cpp:313] Batch 147, accuracy/top5 = 0.92
I0817 10:58:00.081195 14586 caffe.cpp:313] Batch 147, loss = 1.39673
I0817 10:58:00.144297 14586 caffe.cpp:313] Batch 148, accuracy/top1 = 0.52
I0817 10:58:00.144318 14586 caffe.cpp:313] Batch 148, accuracy/top5 = 0.82
I0817 10:58:00.144322 14586 caffe.cpp:313] Batch 148, loss = 2.05899
I0817 10:58:00.206985 14586 caffe.cpp:313] Batch 149, accuracy/top1 = 0.62
I0817 10:58:00.207006 14586 caffe.cpp:313] Batch 149, accuracy/top5 = 0.88
I0817 10:58:00.207010 14586 caffe.cpp:313] Batch 149, loss = 1.37888
I0817 10:58:00.269794 14586 caffe.cpp:313] Batch 150, accuracy/top1 = 0.58
I0817 10:58:00.269816 14586 caffe.cpp:313] Batch 150, accuracy/top5 = 0.8
I0817 10:58:00.269820 14586 caffe.cpp:313] Batch 150, loss = 1.91818
I0817 10:58:00.332412 14586 caffe.cpp:313] Batch 151, accuracy/top1 = 0.6
I0817 10:58:00.332434 14586 caffe.cpp:313] Batch 151, accuracy/top5 = 0.84
I0817 10:58:00.332438 14586 caffe.cpp:313] Batch 151, loss = 1.51594
I0817 10:58:00.395045 14586 caffe.cpp:313] Batch 152, accuracy/top1 = 0.56
I0817 10:58:00.395066 14586 caffe.cpp:313] Batch 152, accuracy/top5 = 0.8
I0817 10:58:00.395071 14586 caffe.cpp:313] Batch 152, loss = 1.85548
I0817 10:58:00.457640 14586 caffe.cpp:313] Batch 153, accuracy/top1 = 0.52
I0817 10:58:00.457662 14586 caffe.cpp:313] Batch 153, accuracy/top5 = 0.82
I0817 10:58:00.457666 14586 caffe.cpp:313] Batch 153, loss = 2.18714
I0817 10:58:00.520334 14586 caffe.cpp:313] Batch 154, accuracy/top1 = 0.62
I0817 10:58:00.520356 14586 caffe.cpp:313] Batch 154, accuracy/top5 = 0.82
I0817 10:58:00.520360 14586 caffe.cpp:313] Batch 154, loss = 1.4433
I0817 10:58:00.582932 14586 caffe.cpp:313] Batch 155, accuracy/top1 = 0.58
I0817 10:58:00.582954 14586 caffe.cpp:313] Batch 155, accuracy/top5 = 0.8
I0817 10:58:00.582958 14586 caffe.cpp:313] Batch 155, loss = 1.70884
I0817 10:58:00.645548 14586 caffe.cpp:313] Batch 156, accuracy/top1 = 0.68
I0817 10:58:00.645571 14586 caffe.cpp:313] Batch 156, accuracy/top5 = 0.84
I0817 10:58:00.645576 14586 caffe.cpp:313] Batch 156, loss = 1.84108
I0817 10:58:00.708426 14586 caffe.cpp:313] Batch 157, accuracy/top1 = 0.58
I0817 10:58:00.708448 14586 caffe.cpp:313] Batch 157, accuracy/top5 = 0.84
I0817 10:58:00.708452 14586 caffe.cpp:313] Batch 157, loss = 1.60762
I0817 10:58:00.771106 14586 caffe.cpp:313] Batch 158, accuracy/top1 = 0.68
I0817 10:58:00.771127 14586 caffe.cpp:313] Batch 158, accuracy/top5 = 0.84
I0817 10:58:00.771131 14586 caffe.cpp:313] Batch 158, loss = 1.56315
I0817 10:58:00.834374 14586 caffe.cpp:313] Batch 159, accuracy/top1 = 0.58
I0817 10:58:00.834393 14586 caffe.cpp:313] Batch 159, accuracy/top5 = 0.78
I0817 10:58:00.834396 14586 caffe.cpp:313] Batch 159, loss = 1.99483
I0817 10:58:00.897104 14586 caffe.cpp:313] Batch 160, accuracy/top1 = 0.68
I0817 10:58:00.897126 14586 caffe.cpp:313] Batch 160, accuracy/top5 = 0.84
I0817 10:58:00.897130 14586 caffe.cpp:313] Batch 160, loss = 1.67234
I0817 10:58:00.959772 14586 caffe.cpp:313] Batch 161, accuracy/top1 = 0.62
I0817 10:58:00.959808 14586 caffe.cpp:313] Batch 161, accuracy/top5 = 0.78
I0817 10:58:00.959812 14586 caffe.cpp:313] Batch 161, loss = 2.05324
I0817 10:58:01.022431 14586 caffe.cpp:313] Batch 162, accuracy/top1 = 0.54
I0817 10:58:01.022449 14586 caffe.cpp:313] Batch 162, accuracy/top5 = 0.84
I0817 10:58:01.022454 14586 caffe.cpp:313] Batch 162, loss = 1.76707
I0817 10:58:01.085216 14586 caffe.cpp:313] Batch 163, accuracy/top1 = 0.64
I0817 10:58:01.085238 14586 caffe.cpp:313] Batch 163, accuracy/top5 = 0.84
I0817 10:58:01.085242 14586 caffe.cpp:313] Batch 163, loss = 1.64564
I0817 10:58:01.147927 14586 caffe.cpp:313] Batch 164, accuracy/top1 = 0.66
I0817 10:58:01.147949 14586 caffe.cpp:313] Batch 164, accuracy/top5 = 0.8
I0817 10:58:01.147953 14586 caffe.cpp:313] Batch 164, loss = 1.56335
I0817 10:58:01.210594 14586 caffe.cpp:313] Batch 165, accuracy/top1 = 0.66
I0817 10:58:01.210616 14586 caffe.cpp:313] Batch 165, accuracy/top5 = 0.86
I0817 10:58:01.210620 14586 caffe.cpp:313] Batch 165, loss = 1.1788
I0817 10:58:01.273208 14586 caffe.cpp:313] Batch 166, accuracy/top1 = 0.42
I0817 10:58:01.273231 14586 caffe.cpp:313] Batch 166, accuracy/top5 = 0.82
I0817 10:58:01.273234 14586 caffe.cpp:313] Batch 166, loss = 2.04778
I0817 10:58:01.335878 14586 caffe.cpp:313] Batch 167, accuracy/top1 = 0.58
I0817 10:58:01.335901 14586 caffe.cpp:313] Batch 167, accuracy/top5 = 0.8
I0817 10:58:01.335906 14586 caffe.cpp:313] Batch 167, loss = 1.55265
I0817 10:58:01.398377 14586 caffe.cpp:313] Batch 168, accuracy/top1 = 0.68
I0817 10:58:01.398399 14586 caffe.cpp:313] Batch 168, accuracy/top5 = 0.9
I0817 10:58:01.398403 14586 caffe.cpp:313] Batch 168, loss = 1.36179
I0817 10:58:01.461006 14586 caffe.cpp:313] Batch 169, accuracy/top1 = 0.72
I0817 10:58:01.461028 14586 caffe.cpp:313] Batch 169, accuracy/top5 = 0.86
I0817 10:58:01.461032 14586 caffe.cpp:313] Batch 169, loss = 1.33113
I0817 10:58:01.523690 14586 caffe.cpp:313] Batch 170, accuracy/top1 = 0.58
I0817 10:58:01.523713 14586 caffe.cpp:313] Batch 170, accuracy/top5 = 0.76
I0817 10:58:01.523717 14586 caffe.cpp:313] Batch 170, loss = 1.84724
I0817 10:58:01.586421 14586 caffe.cpp:313] Batch 171, accuracy/top1 = 0.56
I0817 10:58:01.586444 14586 caffe.cpp:313] Batch 171, accuracy/top5 = 0.88
I0817 10:58:01.586448 14586 caffe.cpp:313] Batch 171, loss = 1.8214
I0817 10:58:01.649055 14586 caffe.cpp:313] Batch 172, accuracy/top1 = 0.68
I0817 10:58:01.649076 14586 caffe.cpp:313] Batch 172, accuracy/top5 = 0.88
I0817 10:58:01.649080 14586 caffe.cpp:313] Batch 172, loss = 1.59137
I0817 10:58:01.711681 14586 caffe.cpp:313] Batch 173, accuracy/top1 = 0.6
I0817 10:58:01.711702 14586 caffe.cpp:313] Batch 173, accuracy/top5 = 0.8
I0817 10:58:01.711706 14586 caffe.cpp:313] Batch 173, loss = 1.89474
I0817 10:58:01.774436 14586 caffe.cpp:313] Batch 174, accuracy/top1 = 0.68
I0817 10:58:01.774457 14586 caffe.cpp:313] Batch 174, accuracy/top5 = 0.78
I0817 10:58:01.774461 14586 caffe.cpp:313] Batch 174, loss = 1.81483
I0817 10:58:01.837893 14586 caffe.cpp:313] Batch 175, accuracy/top1 = 0.6
I0817 10:58:01.837914 14586 caffe.cpp:313] Batch 175, accuracy/top5 = 0.78
I0817 10:58:01.837918 14586 caffe.cpp:313] Batch 175, loss = 1.95247
I0817 10:58:01.901087 14586 caffe.cpp:313] Batch 176, accuracy/top1 = 0.5
I0817 10:58:01.901109 14586 caffe.cpp:313] Batch 176, accuracy/top5 = 0.76
I0817 10:58:01.901113 14586 caffe.cpp:313] Batch 176, loss = 2.14348
I0817 10:58:01.963793 14586 caffe.cpp:313] Batch 177, accuracy/top1 = 0.54
I0817 10:58:01.963815 14586 caffe.cpp:313] Batch 177, accuracy/top5 = 0.76
I0817 10:58:01.963819 14586 caffe.cpp:313] Batch 177, loss = 1.94652
I0817 10:58:02.026535 14586 caffe.cpp:313] Batch 178, accuracy/top1 = 0.52
I0817 10:58:02.026553 14586 caffe.cpp:313] Batch 178, accuracy/top5 = 0.84
I0817 10:58:02.026557 14586 caffe.cpp:313] Batch 178, loss = 1.80856
I0817 10:58:02.089201 14586 caffe.cpp:313] Batch 179, accuracy/top1 = 0.62
I0817 10:58:02.089224 14586 caffe.cpp:313] Batch 179, accuracy/top5 = 0.86
I0817 10:58:02.089228 14586 caffe.cpp:313] Batch 179, loss = 1.6656
I0817 10:58:02.151921 14586 caffe.cpp:313] Batch 180, accuracy/top1 = 0.62
I0817 10:58:02.151942 14586 caffe.cpp:313] Batch 180, accuracy/top5 = 0.8
I0817 10:58:02.151947 14586 caffe.cpp:313] Batch 180, loss = 1.89718
I0817 10:58:02.214615 14586 caffe.cpp:313] Batch 181, accuracy/top1 = 0.52
I0817 10:58:02.214638 14586 caffe.cpp:313] Batch 181, accuracy/top5 = 0.72
I0817 10:58:02.214643 14586 caffe.cpp:313] Batch 181, loss = 2.07226
I0817 10:58:02.277458 14586 caffe.cpp:313] Batch 182, accuracy/top1 = 0.52
I0817 10:58:02.277478 14586 caffe.cpp:313] Batch 182, accuracy/top5 = 0.8
I0817 10:58:02.277482 14586 caffe.cpp:313] Batch 182, loss = 2.22678
I0817 10:58:02.340342 14586 caffe.cpp:313] Batch 183, accuracy/top1 = 0.48
I0817 10:58:02.340363 14586 caffe.cpp:313] Batch 183, accuracy/top5 = 0.74
I0817 10:58:02.340368 14586 caffe.cpp:313] Batch 183, loss = 2.1848
I0817 10:58:02.403136 14586 caffe.cpp:313] Batch 184, accuracy/top1 = 0.64
I0817 10:58:02.403157 14586 caffe.cpp:313] Batch 184, accuracy/top5 = 0.88
I0817 10:58:02.403162 14586 caffe.cpp:313] Batch 184, loss = 1.69824
I0817 10:58:02.465687 14586 caffe.cpp:313] Batch 185, accuracy/top1 = 0.56
I0817 10:58:02.465708 14586 caffe.cpp:313] Batch 185, accuracy/top5 = 0.76
I0817 10:58:02.465713 14586 caffe.cpp:313] Batch 185, loss = 1.93168
I0817 10:58:02.528369 14586 caffe.cpp:313] Batch 186, accuracy/top1 = 0.54
I0817 10:58:02.528393 14586 caffe.cpp:313] Batch 186, accuracy/top5 = 0.82
I0817 10:58:02.528396 14586 caffe.cpp:313] Batch 186, loss = 1.57362
I0817 10:58:02.591014 14586 caffe.cpp:313] Batch 187, accuracy/top1 = 0.58
I0817 10:58:02.591037 14586 caffe.cpp:313] Batch 187, accuracy/top5 = 0.8
I0817 10:58:02.591040 14586 caffe.cpp:313] Batch 187, loss = 1.79742
I0817 10:58:02.653678 14586 caffe.cpp:313] Batch 188, accuracy/top1 = 0.62
I0817 10:58:02.653699 14586 caffe.cpp:313] Batch 188, accuracy/top5 = 0.76
I0817 10:58:02.653703 14586 caffe.cpp:313] Batch 188, loss = 1.59224
I0817 10:58:02.716238 14586 caffe.cpp:313] Batch 189, accuracy/top1 = 0.58
I0817 10:58:02.716256 14586 caffe.cpp:313] Batch 189, accuracy/top5 = 0.78
I0817 10:58:02.716260 14586 caffe.cpp:313] Batch 189, loss = 1.98888
I0817 10:58:02.778870 14586 caffe.cpp:313] Batch 190, accuracy/top1 = 0.6
I0817 10:58:02.778892 14586 caffe.cpp:313] Batch 190, accuracy/top5 = 0.86
I0817 10:58:02.778897 14586 caffe.cpp:313] Batch 190, loss = 1.49972
I0817 10:58:02.842000 14586 caffe.cpp:313] Batch 191, accuracy/top1 = 0.48
I0817 10:58:02.842017 14586 caffe.cpp:313] Batch 191, accuracy/top5 = 0.76
I0817 10:58:02.842021 14586 caffe.cpp:313] Batch 191, loss = 2.29793
I0817 10:58:02.904749 14586 caffe.cpp:313] Batch 192, accuracy/top1 = 0.54
I0817 10:58:02.904772 14586 caffe.cpp:313] Batch 192, accuracy/top5 = 0.8
I0817 10:58:02.904775 14586 caffe.cpp:313] Batch 192, loss = 1.90314
I0817 10:58:02.967314 14586 caffe.cpp:313] Batch 193, accuracy/top1 = 0.56
I0817 10:58:02.967336 14586 caffe.cpp:313] Batch 193, accuracy/top5 = 0.84
I0817 10:58:02.967340 14586 caffe.cpp:313] Batch 193, loss = 1.75635
I0817 10:58:03.029983 14586 caffe.cpp:313] Batch 194, accuracy/top1 = 0.6
I0817 10:58:03.030002 14586 caffe.cpp:313] Batch 194, accuracy/top5 = 0.92
I0817 10:58:03.030006 14586 caffe.cpp:313] Batch 194, loss = 1.59329
I0817 10:58:03.092579 14586 caffe.cpp:313] Batch 195, accuracy/top1 = 0.62
I0817 10:58:03.092602 14586 caffe.cpp:313] Batch 195, accuracy/top5 = 0.8
I0817 10:58:03.092605 14586 caffe.cpp:313] Batch 195, loss = 1.66721
I0817 10:58:03.155165 14586 caffe.cpp:313] Batch 196, accuracy/top1 = 0.7
I0817 10:58:03.155187 14586 caffe.cpp:313] Batch 196, accuracy/top5 = 0.84
I0817 10:58:03.155191 14586 caffe.cpp:313] Batch 196, loss = 1.23476
I0817 10:58:03.217680 14586 caffe.cpp:313] Batch 197, accuracy/top1 = 0.66
I0817 10:58:03.217701 14586 caffe.cpp:313] Batch 197, accuracy/top5 = 0.88
I0817 10:58:03.217705 14586 caffe.cpp:313] Batch 197, loss = 1.54326
I0817 10:58:03.280421 14586 caffe.cpp:313] Batch 198, accuracy/top1 = 0.62
I0817 10:58:03.280442 14586 caffe.cpp:313] Batch 198, accuracy/top5 = 0.84
I0817 10:58:03.280464 14586 caffe.cpp:313] Batch 198, loss = 1.78899
I0817 10:58:03.343202 14586 caffe.cpp:313] Batch 199, accuracy/top1 = 0.7
I0817 10:58:03.343225 14586 caffe.cpp:313] Batch 199, accuracy/top5 = 0.82
I0817 10:58:03.343230 14586 caffe.cpp:313] Batch 199, loss = 1.32199
I0817 10:58:03.405717 14586 caffe.cpp:313] Batch 200, accuracy/top1 = 0.64
I0817 10:58:03.405740 14586 caffe.cpp:313] Batch 200, accuracy/top5 = 0.82
I0817 10:58:03.405743 14586 caffe.cpp:313] Batch 200, loss = 1.71636
I0817 10:58:03.468343 14586 caffe.cpp:313] Batch 201, accuracy/top1 = 0.46
I0817 10:58:03.468365 14586 caffe.cpp:313] Batch 201, accuracy/top5 = 0.72
I0817 10:58:03.468369 14586 caffe.cpp:313] Batch 201, loss = 2.13017
I0817 10:58:03.532161 14586 caffe.cpp:313] Batch 202, accuracy/top1 = 0.48
I0817 10:58:03.532184 14586 caffe.cpp:313] Batch 202, accuracy/top5 = 0.86
I0817 10:58:03.532188 14586 caffe.cpp:313] Batch 202, loss = 1.95993
I0817 10:58:03.597218 14586 caffe.cpp:313] Batch 203, accuracy/top1 = 0.64
I0817 10:58:03.597241 14586 caffe.cpp:313] Batch 203, accuracy/top5 = 0.8
I0817 10:58:03.597245 14586 caffe.cpp:313] Batch 203, loss = 1.69303
I0817 10:58:03.662339 14586 caffe.cpp:313] Batch 204, accuracy/top1 = 0.7
I0817 10:58:03.662362 14586 caffe.cpp:313] Batch 204, accuracy/top5 = 0.86
I0817 10:58:03.662366 14586 caffe.cpp:313] Batch 204, loss = 1.38816
I0817 10:58:03.727290 14586 caffe.cpp:313] Batch 205, accuracy/top1 = 0.6
I0817 10:58:03.727313 14586 caffe.cpp:313] Batch 205, accuracy/top5 = 0.86
I0817 10:58:03.727318 14586 caffe.cpp:313] Batch 205, loss = 1.62723
I0817 10:58:03.792264 14586 caffe.cpp:313] Batch 206, accuracy/top1 = 0.52
I0817 10:58:03.792287 14586 caffe.cpp:313] Batch 206, accuracy/top5 = 0.84
I0817 10:58:03.792291 14586 caffe.cpp:313] Batch 206, loss = 1.67004
I0817 10:58:03.855684 14586 caffe.cpp:313] Batch 207, accuracy/top1 = 0.54
I0817 10:58:03.855701 14586 caffe.cpp:313] Batch 207, accuracy/top5 = 0.82
I0817 10:58:03.855705 14586 caffe.cpp:313] Batch 207, loss = 1.87749
I0817 10:58:03.918193 14586 caffe.cpp:313] Batch 208, accuracy/top1 = 0.56
I0817 10:58:03.918215 14586 caffe.cpp:313] Batch 208, accuracy/top5 = 0.7
I0817 10:58:03.918218 14586 caffe.cpp:313] Batch 208, loss = 2.28529
I0817 10:58:03.980708 14586 caffe.cpp:313] Batch 209, accuracy/top1 = 0.62
I0817 10:58:03.980731 14586 caffe.cpp:313] Batch 209, accuracy/top5 = 0.88
I0817 10:58:03.980736 14586 caffe.cpp:313] Batch 209, loss = 1.52869
I0817 10:58:04.043272 14586 caffe.cpp:313] Batch 210, accuracy/top1 = 0.42
I0817 10:58:04.043289 14586 caffe.cpp:313] Batch 210, accuracy/top5 = 0.68
I0817 10:58:04.043293 14586 caffe.cpp:313] Batch 210, loss = 2.98014
I0817 10:58:04.105875 14586 caffe.cpp:313] Batch 211, accuracy/top1 = 0.6
I0817 10:58:04.105897 14586 caffe.cpp:313] Batch 211, accuracy/top5 = 0.86
I0817 10:58:04.105901 14586 caffe.cpp:313] Batch 211, loss = 1.7596
I0817 10:58:04.168510 14586 caffe.cpp:313] Batch 212, accuracy/top1 = 0.6
I0817 10:58:04.168534 14586 caffe.cpp:313] Batch 212, accuracy/top5 = 0.8
I0817 10:58:04.168536 14586 caffe.cpp:313] Batch 212, loss = 1.67117
I0817 10:58:04.231225 14586 caffe.cpp:313] Batch 213, accuracy/top1 = 0.68
I0817 10:58:04.231247 14586 caffe.cpp:313] Batch 213, accuracy/top5 = 0.84
I0817 10:58:04.231251 14586 caffe.cpp:313] Batch 213, loss = 1.73366
I0817 10:58:04.293900 14586 caffe.cpp:313] Batch 214, accuracy/top1 = 0.52
I0817 10:58:04.293922 14586 caffe.cpp:313] Batch 214, accuracy/top5 = 0.86
I0817 10:58:04.293926 14586 caffe.cpp:313] Batch 214, loss = 1.90302
I0817 10:58:04.356633 14586 caffe.cpp:313] Batch 215, accuracy/top1 = 0.46
I0817 10:58:04.356652 14586 caffe.cpp:313] Batch 215, accuracy/top5 = 0.76
I0817 10:58:04.356657 14586 caffe.cpp:313] Batch 215, loss = 2.05142
I0817 10:58:04.419385 14586 caffe.cpp:313] Batch 216, accuracy/top1 = 0.68
I0817 10:58:04.419405 14586 caffe.cpp:313] Batch 216, accuracy/top5 = 0.9
I0817 10:58:04.419409 14586 caffe.cpp:313] Batch 216, loss = 1.34544
I0817 10:58:04.482869 14586 caffe.cpp:313] Batch 217, accuracy/top1 = 0.58
I0817 10:58:04.482903 14586 caffe.cpp:313] Batch 217, accuracy/top5 = 0.82
I0817 10:58:04.482908 14586 caffe.cpp:313] Batch 217, loss = 1.68961
I0817 10:58:04.545661 14586 caffe.cpp:313] Batch 218, accuracy/top1 = 0.62
I0817 10:58:04.545684 14586 caffe.cpp:313] Batch 218, accuracy/top5 = 0.82
I0817 10:58:04.545688 14586 caffe.cpp:313] Batch 218, loss = 1.7744
I0817 10:58:04.608343 14586 caffe.cpp:313] Batch 219, accuracy/top1 = 0.68
I0817 10:58:04.608366 14586 caffe.cpp:313] Batch 219, accuracy/top5 = 0.86
I0817 10:58:04.608369 14586 caffe.cpp:313] Batch 219, loss = 1.48242
I0817 10:58:04.670861 14586 caffe.cpp:313] Batch 220, accuracy/top1 = 0.5
I0817 10:58:04.670883 14586 caffe.cpp:313] Batch 220, accuracy/top5 = 0.74
I0817 10:58:04.670887 14586 caffe.cpp:313] Batch 220, loss = 2.21341
I0817 10:58:04.733466 14586 caffe.cpp:313] Batch 221, accuracy/top1 = 0.64
I0817 10:58:04.733484 14586 caffe.cpp:313] Batch 221, accuracy/top5 = 0.88
I0817 10:58:04.733489 14586 caffe.cpp:313] Batch 221, loss = 1.49062
I0817 10:58:04.796252 14586 caffe.cpp:313] Batch 222, accuracy/top1 = 0.64
I0817 10:58:04.796273 14586 caffe.cpp:313] Batch 222, accuracy/top5 = 0.78
I0817 10:58:04.796277 14586 caffe.cpp:313] Batch 222, loss = 2.05984
I0817 10:58:04.859333 14586 caffe.cpp:313] Batch 223, accuracy/top1 = 0.54
I0817 10:58:04.859349 14586 caffe.cpp:313] Batch 223, accuracy/top5 = 0.82
I0817 10:58:04.859354 14586 caffe.cpp:313] Batch 223, loss = 1.94483
I0817 10:58:04.921967 14586 caffe.cpp:313] Batch 224, accuracy/top1 = 0.46
I0817 10:58:04.921989 14586 caffe.cpp:313] Batch 224, accuracy/top5 = 0.76
I0817 10:58:04.921993 14586 caffe.cpp:313] Batch 224, loss = 2.27245
I0817 10:58:04.984697 14586 caffe.cpp:313] Batch 225, accuracy/top1 = 0.7
I0817 10:58:04.984720 14586 caffe.cpp:313] Batch 225, accuracy/top5 = 0.84
I0817 10:58:04.984724 14586 caffe.cpp:313] Batch 225, loss = 1.53834
I0817 10:58:05.047430 14586 caffe.cpp:313] Batch 226, accuracy/top1 = 0.62
I0817 10:58:05.047448 14586 caffe.cpp:313] Batch 226, accuracy/top5 = 0.86
I0817 10:58:05.047452 14586 caffe.cpp:313] Batch 226, loss = 1.6627
I0817 10:58:05.110153 14586 caffe.cpp:313] Batch 227, accuracy/top1 = 0.62
I0817 10:58:05.110177 14586 caffe.cpp:313] Batch 227, accuracy/top5 = 0.88
I0817 10:58:05.110180 14586 caffe.cpp:313] Batch 227, loss = 1.47746
I0817 10:58:05.172847 14586 caffe.cpp:313] Batch 228, accuracy/top1 = 0.62
I0817 10:58:05.172868 14586 caffe.cpp:313] Batch 228, accuracy/top5 = 0.82
I0817 10:58:05.172873 14586 caffe.cpp:313] Batch 228, loss = 1.50591
I0817 10:58:05.235534 14586 caffe.cpp:313] Batch 229, accuracy/top1 = 0.62
I0817 10:58:05.235556 14586 caffe.cpp:313] Batch 229, accuracy/top5 = 0.82
I0817 10:58:05.235560 14586 caffe.cpp:313] Batch 229, loss = 1.49238
I0817 10:58:05.298282 14586 caffe.cpp:313] Batch 230, accuracy/top1 = 0.72
I0817 10:58:05.298305 14586 caffe.cpp:313] Batch 230, accuracy/top5 = 0.92
I0817 10:58:05.298308 14586 caffe.cpp:313] Batch 230, loss = 1.20117
I0817 10:58:05.360996 14586 caffe.cpp:313] Batch 231, accuracy/top1 = 0.48
I0817 10:58:05.361019 14586 caffe.cpp:313] Batch 231, accuracy/top5 = 0.76
I0817 10:58:05.361022 14586 caffe.cpp:313] Batch 231, loss = 2.26207
I0817 10:58:05.423673 14586 caffe.cpp:313] Batch 232, accuracy/top1 = 0.54
I0817 10:58:05.423696 14586 caffe.cpp:313] Batch 232, accuracy/top5 = 0.76
I0817 10:58:05.423701 14586 caffe.cpp:313] Batch 232, loss = 2.12355
I0817 10:58:05.486347 14586 caffe.cpp:313] Batch 233, accuracy/top1 = 0.62
I0817 10:58:05.486371 14586 caffe.cpp:313] Batch 233, accuracy/top5 = 0.86
I0817 10:58:05.486376 14586 caffe.cpp:313] Batch 233, loss = 1.6159
I0817 10:58:05.549089 14586 caffe.cpp:313] Batch 234, accuracy/top1 = 0.54
I0817 10:58:05.549111 14586 caffe.cpp:313] Batch 234, accuracy/top5 = 0.8
I0817 10:58:05.549115 14586 caffe.cpp:313] Batch 234, loss = 1.90732
I0817 10:58:05.611856 14586 caffe.cpp:313] Batch 235, accuracy/top1 = 0.56
I0817 10:58:05.611881 14586 caffe.cpp:313] Batch 235, accuracy/top5 = 0.84
I0817 10:58:05.611884 14586 caffe.cpp:313] Batch 235, loss = 1.63012
I0817 10:58:05.674657 14586 caffe.cpp:313] Batch 236, accuracy/top1 = 0.56
I0817 10:58:05.674679 14586 caffe.cpp:313] Batch 236, accuracy/top5 = 0.78
I0817 10:58:05.674685 14586 caffe.cpp:313] Batch 236, loss = 2.06833
I0817 10:58:05.737502 14586 caffe.cpp:313] Batch 237, accuracy/top1 = 0.62
I0817 10:58:05.737525 14586 caffe.cpp:313] Batch 237, accuracy/top5 = 0.9
I0817 10:58:05.737529 14586 caffe.cpp:313] Batch 237, loss = 1.32638
I0817 10:58:05.800124 14586 caffe.cpp:313] Batch 238, accuracy/top1 = 0.6
I0817 10:58:05.800149 14586 caffe.cpp:313] Batch 238, accuracy/top5 = 0.88
I0817 10:58:05.800153 14586 caffe.cpp:313] Batch 238, loss = 1.76464
I0817 10:58:05.863075 14586 caffe.cpp:313] Batch 239, accuracy/top1 = 0.66
I0817 10:58:05.863095 14586 caffe.cpp:313] Batch 239, accuracy/top5 = 0.86
I0817 10:58:05.863098 14586 caffe.cpp:313] Batch 239, loss = 1.26473
I0817 10:58:05.925864 14586 caffe.cpp:313] Batch 240, accuracy/top1 = 0.52
I0817 10:58:05.925885 14586 caffe.cpp:313] Batch 240, accuracy/top5 = 0.82
I0817 10:58:05.925890 14586 caffe.cpp:313] Batch 240, loss = 1.60567
I0817 10:58:05.988605 14586 caffe.cpp:313] Batch 241, accuracy/top1 = 0.46
I0817 10:58:05.988627 14586 caffe.cpp:313] Batch 241, accuracy/top5 = 0.72
I0817 10:58:05.988631 14586 caffe.cpp:313] Batch 241, loss = 2.34283
I0817 10:58:06.051347 14586 caffe.cpp:313] Batch 242, accuracy/top1 = 0.62
I0817 10:58:06.051367 14586 caffe.cpp:313] Batch 242, accuracy/top5 = 0.86
I0817 10:58:06.051371 14586 caffe.cpp:313] Batch 242, loss = 1.59151
I0817 10:58:06.113965 14586 caffe.cpp:313] Batch 243, accuracy/top1 = 0.46
I0817 10:58:06.113986 14586 caffe.cpp:313] Batch 243, accuracy/top5 = 0.78
I0817 10:58:06.113991 14586 caffe.cpp:313] Batch 243, loss = 1.79035
I0817 10:58:06.176581 14586 caffe.cpp:313] Batch 244, accuracy/top1 = 0.5
I0817 10:58:06.176604 14586 caffe.cpp:313] Batch 244, accuracy/top5 = 0.78
I0817 10:58:06.176607 14586 caffe.cpp:313] Batch 244, loss = 2.10096
I0817 10:58:06.239315 14586 caffe.cpp:313] Batch 245, accuracy/top1 = 0.44
I0817 10:58:06.239337 14586 caffe.cpp:313] Batch 245, accuracy/top5 = 0.64
I0817 10:58:06.239341 14586 caffe.cpp:313] Batch 245, loss = 2.51529
I0817 10:58:06.302108 14586 caffe.cpp:313] Batch 246, accuracy/top1 = 0.54
I0817 10:58:06.302129 14586 caffe.cpp:313] Batch 246, accuracy/top5 = 0.8
I0817 10:58:06.302134 14586 caffe.cpp:313] Batch 246, loss = 2.06284
I0817 10:58:06.364820 14586 caffe.cpp:313] Batch 247, accuracy/top1 = 0.42
I0817 10:58:06.364842 14586 caffe.cpp:313] Batch 247, accuracy/top5 = 0.68
I0817 10:58:06.364846 14586 caffe.cpp:313] Batch 247, loss = 2.33254
I0817 10:58:06.427544 14586 caffe.cpp:313] Batch 248, accuracy/top1 = 0.54
I0817 10:58:06.427567 14586 caffe.cpp:313] Batch 248, accuracy/top5 = 0.66
I0817 10:58:06.427572 14586 caffe.cpp:313] Batch 248, loss = 2.35005
I0817 10:58:06.490276 14586 caffe.cpp:313] Batch 249, accuracy/top1 = 0.5
I0817 10:58:06.490294 14586 caffe.cpp:313] Batch 249, accuracy/top5 = 0.76
I0817 10:58:06.490298 14586 caffe.cpp:313] Batch 249, loss = 2.03351
I0817 10:58:06.553200 14586 caffe.cpp:313] Batch 250, accuracy/top1 = 0.66
I0817 10:58:06.553218 14586 caffe.cpp:313] Batch 250, accuracy/top5 = 0.94
I0817 10:58:06.553222 14586 caffe.cpp:313] Batch 250, loss = 1.26529
I0817 10:58:06.616050 14586 caffe.cpp:313] Batch 251, accuracy/top1 = 0.6
I0817 10:58:06.616071 14586 caffe.cpp:313] Batch 251, accuracy/top5 = 0.8
I0817 10:58:06.616075 14586 caffe.cpp:313] Batch 251, loss = 1.60513
I0817 10:58:06.678741 14586 caffe.cpp:313] Batch 252, accuracy/top1 = 0.54
I0817 10:58:06.678766 14586 caffe.cpp:313] Batch 252, accuracy/top5 = 0.7
I0817 10:58:06.678769 14586 caffe.cpp:313] Batch 252, loss = 2.48661
I0817 10:58:06.741683 14586 caffe.cpp:313] Batch 253, accuracy/top1 = 0.54
I0817 10:58:06.741701 14586 caffe.cpp:313] Batch 253, accuracy/top5 = 0.82
I0817 10:58:06.741705 14586 caffe.cpp:313] Batch 253, loss = 1.79506
I0817 10:58:06.804250 14586 caffe.cpp:313] Batch 254, accuracy/top1 = 0.58
I0817 10:58:06.804271 14586 caffe.cpp:313] Batch 254, accuracy/top5 = 0.76
I0817 10:58:06.804275 14586 caffe.cpp:313] Batch 254, loss = 1.97716
I0817 10:58:06.867089 14586 caffe.cpp:313] Batch 255, accuracy/top1 = 0.54
I0817 10:58:06.867110 14586 caffe.cpp:313] Batch 255, accuracy/top5 = 0.78
I0817 10:58:06.867115 14586 caffe.cpp:313] Batch 255, loss = 2.36197
I0817 10:58:06.929690 14586 caffe.cpp:313] Batch 256, accuracy/top1 = 0.6
I0817 10:58:06.929713 14586 caffe.cpp:313] Batch 256, accuracy/top5 = 0.84
I0817 10:58:06.929718 14586 caffe.cpp:313] Batch 256, loss = 1.54749
I0817 10:58:06.992321 14586 caffe.cpp:313] Batch 257, accuracy/top1 = 0.56
I0817 10:58:06.992343 14586 caffe.cpp:313] Batch 257, accuracy/top5 = 0.8
I0817 10:58:06.992347 14586 caffe.cpp:313] Batch 257, loss = 1.98515
I0817 10:58:07.055172 14586 caffe.cpp:313] Batch 258, accuracy/top1 = 0.58
I0817 10:58:07.055191 14586 caffe.cpp:313] Batch 258, accuracy/top5 = 0.78
I0817 10:58:07.055194 14586 caffe.cpp:313] Batch 258, loss = 2.26442
I0817 10:58:07.117868 14586 caffe.cpp:313] Batch 259, accuracy/top1 = 0.5
I0817 10:58:07.117892 14586 caffe.cpp:313] Batch 259, accuracy/top5 = 0.86
I0817 10:58:07.117895 14586 caffe.cpp:313] Batch 259, loss = 1.97115
I0817 10:58:07.180627 14586 caffe.cpp:313] Batch 260, accuracy/top1 = 0.56
I0817 10:58:07.180649 14586 caffe.cpp:313] Batch 260, accuracy/top5 = 0.78
I0817 10:58:07.180654 14586 caffe.cpp:313] Batch 260, loss = 1.71798
I0817 10:58:07.243331 14586 caffe.cpp:313] Batch 261, accuracy/top1 = 0.56
I0817 10:58:07.243352 14586 caffe.cpp:313] Batch 261, accuracy/top5 = 0.76
I0817 10:58:07.243356 14586 caffe.cpp:313] Batch 261, loss = 1.82607
I0817 10:58:07.305910 14586 caffe.cpp:313] Batch 262, accuracy/top1 = 0.52
I0817 10:58:07.305932 14586 caffe.cpp:313] Batch 262, accuracy/top5 = 0.76
I0817 10:58:07.305936 14586 caffe.cpp:313] Batch 262, loss = 1.96789
I0817 10:58:07.368585 14586 caffe.cpp:313] Batch 263, accuracy/top1 = 0.62
I0817 10:58:07.368607 14586 caffe.cpp:313] Batch 263, accuracy/top5 = 0.82
I0817 10:58:07.368612 14586 caffe.cpp:313] Batch 263, loss = 1.80793
I0817 10:58:07.431290 14586 caffe.cpp:313] Batch 264, accuracy/top1 = 0.56
I0817 10:58:07.431313 14586 caffe.cpp:313] Batch 264, accuracy/top5 = 0.82
I0817 10:58:07.431318 14586 caffe.cpp:313] Batch 264, loss = 1.86878
I0817 10:58:07.494016 14586 caffe.cpp:313] Batch 265, accuracy/top1 = 0.66
I0817 10:58:07.494040 14586 caffe.cpp:313] Batch 265, accuracy/top5 = 0.84
I0817 10:58:07.494043 14586 caffe.cpp:313] Batch 265, loss = 1.42142
I0817 10:58:07.556614 14586 caffe.cpp:313] Batch 266, accuracy/top1 = 0.64
I0817 10:58:07.556637 14586 caffe.cpp:313] Batch 266, accuracy/top5 = 0.78
I0817 10:58:07.556640 14586 caffe.cpp:313] Batch 266, loss = 1.69376
I0817 10:58:07.619361 14586 caffe.cpp:313] Batch 267, accuracy/top1 = 0.56
I0817 10:58:07.619384 14586 caffe.cpp:313] Batch 267, accuracy/top5 = 0.88
I0817 10:58:07.619387 14586 caffe.cpp:313] Batch 267, loss = 1.64015
I0817 10:58:07.682073 14586 caffe.cpp:313] Batch 268, accuracy/top1 = 0.6
I0817 10:58:07.682096 14586 caffe.cpp:313] Batch 268, accuracy/top5 = 0.82
I0817 10:58:07.682101 14586 caffe.cpp:313] Batch 268, loss = 1.76973
I0817 10:58:07.745007 14586 caffe.cpp:313] Batch 269, accuracy/top1 = 0.66
I0817 10:58:07.745030 14586 caffe.cpp:313] Batch 269, accuracy/top5 = 0.84
I0817 10:58:07.745034 14586 caffe.cpp:313] Batch 269, loss = 1.69788
I0817 10:58:07.807749 14586 caffe.cpp:313] Batch 270, accuracy/top1 = 0.56
I0817 10:58:07.807771 14586 caffe.cpp:313] Batch 270, accuracy/top5 = 0.78
I0817 10:58:07.807775 14586 caffe.cpp:313] Batch 270, loss = 1.99432
I0817 10:58:07.870638 14586 caffe.cpp:313] Batch 271, accuracy/top1 = 0.58
I0817 10:58:07.870661 14586 caffe.cpp:313] Batch 271, accuracy/top5 = 0.78
I0817 10:58:07.870664 14586 caffe.cpp:313] Batch 271, loss = 2.2456
I0817 10:58:07.933384 14586 caffe.cpp:313] Batch 272, accuracy/top1 = 0.56
I0817 10:58:07.933406 14586 caffe.cpp:313] Batch 272, accuracy/top5 = 0.8
I0817 10:58:07.933409 14586 caffe.cpp:313] Batch 272, loss = 1.82373
I0817 10:58:07.996179 14586 caffe.cpp:313] Batch 273, accuracy/top1 = 0.58
I0817 10:58:07.996201 14586 caffe.cpp:313] Batch 273, accuracy/top5 = 0.88
I0817 10:58:07.996222 14586 caffe.cpp:313] Batch 273, loss = 1.44508
I0817 10:58:08.058887 14586 caffe.cpp:313] Batch 274, accuracy/top1 = 0.56
I0817 10:58:08.058907 14586 caffe.cpp:313] Batch 274, accuracy/top5 = 0.86
I0817 10:58:08.058912 14586 caffe.cpp:313] Batch 274, loss = 1.85531
I0817 10:58:08.121588 14586 caffe.cpp:313] Batch 275, accuracy/top1 = 0.62
I0817 10:58:08.121611 14586 caffe.cpp:313] Batch 275, accuracy/top5 = 0.82
I0817 10:58:08.121615 14586 caffe.cpp:313] Batch 275, loss = 1.67349
I0817 10:58:08.184303 14586 caffe.cpp:313] Batch 276, accuracy/top1 = 0.66
I0817 10:58:08.184325 14586 caffe.cpp:313] Batch 276, accuracy/top5 = 0.82
I0817 10:58:08.184329 14586 caffe.cpp:313] Batch 276, loss = 1.61744
I0817 10:58:08.247027 14586 caffe.cpp:313] Batch 277, accuracy/top1 = 0.56
I0817 10:58:08.247051 14586 caffe.cpp:313] Batch 277, accuracy/top5 = 0.76
I0817 10:58:08.247054 14586 caffe.cpp:313] Batch 277, loss = 1.86262
I0817 10:58:08.309700 14586 caffe.cpp:313] Batch 278, accuracy/top1 = 0.46
I0817 10:58:08.309723 14586 caffe.cpp:313] Batch 278, accuracy/top5 = 0.76
I0817 10:58:08.309727 14586 caffe.cpp:313] Batch 278, loss = 2.39255
I0817 10:58:08.372252 14586 caffe.cpp:313] Batch 279, accuracy/top1 = 0.56
I0817 10:58:08.372274 14586 caffe.cpp:313] Batch 279, accuracy/top5 = 0.76
I0817 10:58:08.372278 14586 caffe.cpp:313] Batch 279, loss = 2.37236
I0817 10:58:08.434864 14586 caffe.cpp:313] Batch 280, accuracy/top1 = 0.56
I0817 10:58:08.434885 14586 caffe.cpp:313] Batch 280, accuracy/top5 = 0.88
I0817 10:58:08.434890 14586 caffe.cpp:313] Batch 280, loss = 1.75514
I0817 10:58:08.497557 14586 caffe.cpp:313] Batch 281, accuracy/top1 = 0.64
I0817 10:58:08.497581 14586 caffe.cpp:313] Batch 281, accuracy/top5 = 0.84
I0817 10:58:08.497584 14586 caffe.cpp:313] Batch 281, loss = 1.89763
I0817 10:58:08.560328 14586 caffe.cpp:313] Batch 282, accuracy/top1 = 0.58
I0817 10:58:08.560349 14586 caffe.cpp:313] Batch 282, accuracy/top5 = 0.8
I0817 10:58:08.560353 14586 caffe.cpp:313] Batch 282, loss = 1.66016
I0817 10:58:08.623054 14586 caffe.cpp:313] Batch 283, accuracy/top1 = 0.52
I0817 10:58:08.623075 14586 caffe.cpp:313] Batch 283, accuracy/top5 = 0.78
I0817 10:58:08.623078 14586 caffe.cpp:313] Batch 283, loss = 2.06996
I0817 10:58:08.686431 14586 caffe.cpp:313] Batch 284, accuracy/top1 = 0.56
I0817 10:58:08.686450 14586 caffe.cpp:313] Batch 284, accuracy/top5 = 0.78
I0817 10:58:08.686455 14586 caffe.cpp:313] Batch 284, loss = 1.97785
I0817 10:58:08.749987 14586 caffe.cpp:313] Batch 285, accuracy/top1 = 0.64
I0817 10:58:08.750008 14586 caffe.cpp:313] Batch 285, accuracy/top5 = 0.84
I0817 10:58:08.750012 14586 caffe.cpp:313] Batch 285, loss = 1.44595
I0817 10:58:08.813221 14586 caffe.cpp:313] Batch 286, accuracy/top1 = 0.64
I0817 10:58:08.813241 14586 caffe.cpp:313] Batch 286, accuracy/top5 = 0.84
I0817 10:58:08.813246 14586 caffe.cpp:313] Batch 286, loss = 1.40205
I0817 10:58:08.876662 14586 caffe.cpp:313] Batch 287, accuracy/top1 = 0.54
I0817 10:58:08.876684 14586 caffe.cpp:313] Batch 287, accuracy/top5 = 0.76
I0817 10:58:08.876688 14586 caffe.cpp:313] Batch 287, loss = 2.12217
I0817 10:58:08.939350 14586 caffe.cpp:313] Batch 288, accuracy/top1 = 0.58
I0817 10:58:08.939373 14586 caffe.cpp:313] Batch 288, accuracy/top5 = 0.84
I0817 10:58:08.939378 14586 caffe.cpp:313] Batch 288, loss = 2.04259
I0817 10:58:09.002043 14586 caffe.cpp:313] Batch 289, accuracy/top1 = 0.58
I0817 10:58:09.002065 14586 caffe.cpp:313] Batch 289, accuracy/top5 = 0.88
I0817 10:58:09.002069 14586 caffe.cpp:313] Batch 289, loss = 1.35913
I0817 10:58:09.064745 14586 caffe.cpp:313] Batch 290, accuracy/top1 = 0.62
I0817 10:58:09.064765 14586 caffe.cpp:313] Batch 290, accuracy/top5 = 0.78
I0817 10:58:09.064769 14586 caffe.cpp:313] Batch 290, loss = 2.06299
I0817 10:58:09.127435 14586 caffe.cpp:313] Batch 291, accuracy/top1 = 0.56
I0817 10:58:09.127459 14586 caffe.cpp:313] Batch 291, accuracy/top5 = 0.78
I0817 10:58:09.127462 14586 caffe.cpp:313] Batch 291, loss = 2.07777
I0817 10:58:09.190038 14586 caffe.cpp:313] Batch 292, accuracy/top1 = 0.58
I0817 10:58:09.190075 14586 caffe.cpp:313] Batch 292, accuracy/top5 = 0.82
I0817 10:58:09.190080 14586 caffe.cpp:313] Batch 292, loss = 1.67249
I0817 10:58:09.252661 14586 caffe.cpp:313] Batch 293, accuracy/top1 = 0.74
I0817 10:58:09.252683 14586 caffe.cpp:313] Batch 293, accuracy/top5 = 0.82
I0817 10:58:09.252687 14586 caffe.cpp:313] Batch 293, loss = 1.4071
I0817 10:58:09.315310 14586 caffe.cpp:313] Batch 294, accuracy/top1 = 0.68
I0817 10:58:09.315332 14586 caffe.cpp:313] Batch 294, accuracy/top5 = 0.92
I0817 10:58:09.315337 14586 caffe.cpp:313] Batch 294, loss = 1.01465
I0817 10:58:09.378080 14586 caffe.cpp:313] Batch 295, accuracy/top1 = 0.5
I0817 10:58:09.378103 14586 caffe.cpp:313] Batch 295, accuracy/top5 = 0.74
I0817 10:58:09.378108 14586 caffe.cpp:313] Batch 295, loss = 2.04932
I0817 10:58:09.440768 14586 caffe.cpp:313] Batch 296, accuracy/top1 = 0.62
I0817 10:58:09.440790 14586 caffe.cpp:313] Batch 296, accuracy/top5 = 0.86
I0817 10:58:09.440794 14586 caffe.cpp:313] Batch 296, loss = 1.44275
I0817 10:58:09.503500 14586 caffe.cpp:313] Batch 297, accuracy/top1 = 0.66
I0817 10:58:09.503523 14586 caffe.cpp:313] Batch 297, accuracy/top5 = 0.82
I0817 10:58:09.503527 14586 caffe.cpp:313] Batch 297, loss = 1.43874
I0817 10:58:09.566134 14586 caffe.cpp:313] Batch 298, accuracy/top1 = 0.52
I0817 10:58:09.566157 14586 caffe.cpp:313] Batch 298, accuracy/top5 = 0.84
I0817 10:58:09.566161 14586 caffe.cpp:313] Batch 298, loss = 1.95869
I0817 10:58:09.628835 14586 caffe.cpp:313] Batch 299, accuracy/top1 = 0.58
I0817 10:58:09.628859 14586 caffe.cpp:313] Batch 299, accuracy/top5 = 0.72
I0817 10:58:09.628862 14586 caffe.cpp:313] Batch 299, loss = 2.07586
I0817 10:58:09.691658 14586 caffe.cpp:313] Batch 300, accuracy/top1 = 0.48
I0817 10:58:09.691680 14586 caffe.cpp:313] Batch 300, accuracy/top5 = 0.68
I0817 10:58:09.691684 14586 caffe.cpp:313] Batch 300, loss = 2.2139
I0817 10:58:09.754464 14586 caffe.cpp:313] Batch 301, accuracy/top1 = 0.52
I0817 10:58:09.754487 14586 caffe.cpp:313] Batch 301, accuracy/top5 = 0.76
I0817 10:58:09.754490 14586 caffe.cpp:313] Batch 301, loss = 2.29234
I0817 10:58:09.817008 14586 caffe.cpp:313] Batch 302, accuracy/top1 = 0.7
I0817 10:58:09.817025 14586 caffe.cpp:313] Batch 302, accuracy/top5 = 0.86
I0817 10:58:09.817029 14586 caffe.cpp:313] Batch 302, loss = 1.58396
I0817 10:58:09.880022 14586 caffe.cpp:313] Batch 303, accuracy/top1 = 0.54
I0817 10:58:09.880041 14586 caffe.cpp:313] Batch 303, accuracy/top5 = 0.86
I0817 10:58:09.880045 14586 caffe.cpp:313] Batch 303, loss = 1.61727
I0817 10:58:09.942729 14586 caffe.cpp:313] Batch 304, accuracy/top1 = 0.6
I0817 10:58:09.942751 14586 caffe.cpp:313] Batch 304, accuracy/top5 = 0.84
I0817 10:58:09.942755 14586 caffe.cpp:313] Batch 304, loss = 1.52109
I0817 10:58:10.005496 14586 caffe.cpp:313] Batch 305, accuracy/top1 = 0.7
I0817 10:58:10.005518 14586 caffe.cpp:313] Batch 305, accuracy/top5 = 0.88
I0817 10:58:10.005522 14586 caffe.cpp:313] Batch 305, loss = 1.22257
I0817 10:58:10.068300 14586 caffe.cpp:313] Batch 306, accuracy/top1 = 0.64
I0817 10:58:10.068321 14586 caffe.cpp:313] Batch 306, accuracy/top5 = 0.86
I0817 10:58:10.068325 14586 caffe.cpp:313] Batch 306, loss = 1.4936
I0817 10:58:10.131053 14586 caffe.cpp:313] Batch 307, accuracy/top1 = 0.62
I0817 10:58:10.131075 14586 caffe.cpp:313] Batch 307, accuracy/top5 = 0.82
I0817 10:58:10.131079 14586 caffe.cpp:313] Batch 307, loss = 1.81389
I0817 10:58:10.193823 14586 caffe.cpp:313] Batch 308, accuracy/top1 = 0.72
I0817 10:58:10.193845 14586 caffe.cpp:313] Batch 308, accuracy/top5 = 0.86
I0817 10:58:10.193850 14586 caffe.cpp:313] Batch 308, loss = 1.43404
I0817 10:58:10.256613 14586 caffe.cpp:313] Batch 309, accuracy/top1 = 0.52
I0817 10:58:10.256635 14586 caffe.cpp:313] Batch 309, accuracy/top5 = 0.78
I0817 10:58:10.256639 14586 caffe.cpp:313] Batch 309, loss = 2.0551
I0817 10:58:10.319392 14586 caffe.cpp:313] Batch 310, accuracy/top1 = 0.64
I0817 10:58:10.319412 14586 caffe.cpp:313] Batch 310, accuracy/top5 = 0.9
I0817 10:58:10.319416 14586 caffe.cpp:313] Batch 310, loss = 1.30838
I0817 10:58:10.382149 14586 caffe.cpp:313] Batch 311, accuracy/top1 = 0.52
I0817 10:58:10.382172 14586 caffe.cpp:313] Batch 311, accuracy/top5 = 0.76
I0817 10:58:10.382176 14586 caffe.cpp:313] Batch 311, loss = 2.40503
I0817 10:58:10.444835 14586 caffe.cpp:313] Batch 312, accuracy/top1 = 0.64
I0817 10:58:10.444857 14586 caffe.cpp:313] Batch 312, accuracy/top5 = 0.8
I0817 10:58:10.444861 14586 caffe.cpp:313] Batch 312, loss = 1.8671
I0817 10:58:10.507457 14586 caffe.cpp:313] Batch 313, accuracy/top1 = 0.68
I0817 10:58:10.507479 14586 caffe.cpp:313] Batch 313, accuracy/top5 = 0.88
I0817 10:58:10.507483 14586 caffe.cpp:313] Batch 313, loss = 1.62735
I0817 10:58:10.570188 14586 caffe.cpp:313] Batch 314, accuracy/top1 = 0.66
I0817 10:58:10.570211 14586 caffe.cpp:313] Batch 314, accuracy/top5 = 0.86
I0817 10:58:10.570215 14586 caffe.cpp:313] Batch 314, loss = 1.36887
I0817 10:58:10.632910 14586 caffe.cpp:313] Batch 315, accuracy/top1 = 0.6
I0817 10:58:10.632932 14586 caffe.cpp:313] Batch 315, accuracy/top5 = 0.78
I0817 10:58:10.632936 14586 caffe.cpp:313] Batch 315, loss = 1.84653
I0817 10:58:10.695653 14586 caffe.cpp:313] Batch 316, accuracy/top1 = 0.48
I0817 10:58:10.695675 14586 caffe.cpp:313] Batch 316, accuracy/top5 = 0.74
I0817 10:58:10.695679 14586 caffe.cpp:313] Batch 316, loss = 2.25582
I0817 10:58:10.758530 14586 caffe.cpp:313] Batch 317, accuracy/top1 = 0.58
I0817 10:58:10.758550 14586 caffe.cpp:313] Batch 317, accuracy/top5 = 0.8
I0817 10:58:10.758554 14586 caffe.cpp:313] Batch 317, loss = 1.80937
I0817 10:58:10.821616 14586 caffe.cpp:313] Batch 318, accuracy/top1 = 0.64
I0817 10:58:10.821640 14586 caffe.cpp:313] Batch 318, accuracy/top5 = 0.84
I0817 10:58:10.821643 14586 caffe.cpp:313] Batch 318, loss = 1.57829
I0817 10:58:10.884580 14586 caffe.cpp:313] Batch 319, accuracy/top1 = 0.56
I0817 10:58:10.884601 14586 caffe.cpp:313] Batch 319, accuracy/top5 = 0.82
I0817 10:58:10.884605 14586 caffe.cpp:313] Batch 319, loss = 1.78256
I0817 10:58:10.947325 14586 caffe.cpp:313] Batch 320, accuracy/top1 = 0.54
I0817 10:58:10.947347 14586 caffe.cpp:313] Batch 320, accuracy/top5 = 0.74
I0817 10:58:10.947351 14586 caffe.cpp:313] Batch 320, loss = 2.19136
I0817 10:58:11.009865 14586 caffe.cpp:313] Batch 321, accuracy/top1 = 0.62
I0817 10:58:11.009883 14586 caffe.cpp:313] Batch 321, accuracy/top5 = 0.8
I0817 10:58:11.009887 14586 caffe.cpp:313] Batch 321, loss = 1.92429
I0817 10:58:11.072578 14586 caffe.cpp:313] Batch 322, accuracy/top1 = 0.6
I0817 10:58:11.072600 14586 caffe.cpp:313] Batch 322, accuracy/top5 = 0.94
I0817 10:58:11.072604 14586 caffe.cpp:313] Batch 322, loss = 1.17425
I0817 10:58:11.135313 14586 caffe.cpp:313] Batch 323, accuracy/top1 = 0.6
I0817 10:58:11.135334 14586 caffe.cpp:313] Batch 323, accuracy/top5 = 0.8
I0817 10:58:11.135339 14586 caffe.cpp:313] Batch 323, loss = 1.97181
I0817 10:58:11.197995 14586 caffe.cpp:313] Batch 324, accuracy/top1 = 0.5
I0817 10:58:11.198017 14586 caffe.cpp:313] Batch 324, accuracy/top5 = 0.7
I0817 10:58:11.198021 14586 caffe.cpp:313] Batch 324, loss = 2.10905
I0817 10:58:11.260745 14586 caffe.cpp:313] Batch 325, accuracy/top1 = 0.56
I0817 10:58:11.260766 14586 caffe.cpp:313] Batch 325, accuracy/top5 = 0.74
I0817 10:58:11.260769 14586 caffe.cpp:313] Batch 325, loss = 2.00012
I0817 10:58:11.323245 14586 caffe.cpp:313] Batch 326, accuracy/top1 = 0.58
I0817 10:58:11.323267 14586 caffe.cpp:313] Batch 326, accuracy/top5 = 0.84
I0817 10:58:11.323271 14586 caffe.cpp:313] Batch 326, loss = 1.96719
I0817 10:58:11.385921 14586 caffe.cpp:313] Batch 327, accuracy/top1 = 0.58
I0817 10:58:11.385944 14586 caffe.cpp:313] Batch 327, accuracy/top5 = 0.7
I0817 10:58:11.385948 14586 caffe.cpp:313] Batch 327, loss = 2.11381
I0817 10:58:11.448624 14586 caffe.cpp:313] Batch 328, accuracy/top1 = 0.44
I0817 10:58:11.448647 14586 caffe.cpp:313] Batch 328, accuracy/top5 = 0.82
I0817 10:58:11.448650 14586 caffe.cpp:313] Batch 328, loss = 2.02692
I0817 10:58:11.511348 14586 caffe.cpp:313] Batch 329, accuracy/top1 = 0.58
I0817 10:58:11.511370 14586 caffe.cpp:313] Batch 329, accuracy/top5 = 0.82
I0817 10:58:11.511391 14586 caffe.cpp:313] Batch 329, loss = 1.74828
I0817 10:58:11.574162 14586 caffe.cpp:313] Batch 330, accuracy/top1 = 0.56
I0817 10:58:11.574183 14586 caffe.cpp:313] Batch 330, accuracy/top5 = 0.84
I0817 10:58:11.574187 14586 caffe.cpp:313] Batch 330, loss = 1.8365
I0817 10:58:11.636987 14586 caffe.cpp:313] Batch 331, accuracy/top1 = 0.48
I0817 10:58:11.637011 14586 caffe.cpp:313] Batch 331, accuracy/top5 = 0.78
I0817 10:58:11.637014 14586 caffe.cpp:313] Batch 331, loss = 2.08876
I0817 10:58:11.699697 14586 caffe.cpp:313] Batch 332, accuracy/top1 = 0.6
I0817 10:58:11.699720 14586 caffe.cpp:313] Batch 332, accuracy/top5 = 0.84
I0817 10:58:11.699724 14586 caffe.cpp:313] Batch 332, loss = 1.63103
I0817 10:58:11.762538 14586 caffe.cpp:313] Batch 333, accuracy/top1 = 0.5
I0817 10:58:11.762560 14586 caffe.cpp:313] Batch 333, accuracy/top5 = 0.82
I0817 10:58:11.762564 14586 caffe.cpp:313] Batch 333, loss = 1.7452
I0817 10:58:11.825505 14586 caffe.cpp:313] Batch 334, accuracy/top1 = 0.7
I0817 10:58:11.825541 14586 caffe.cpp:313] Batch 334, accuracy/top5 = 0.92
I0817 10:58:11.825547 14586 caffe.cpp:313] Batch 334, loss = 1.30991
I0817 10:58:11.888463 14586 caffe.cpp:313] Batch 335, accuracy/top1 = 0.5
I0817 10:58:11.888484 14586 caffe.cpp:313] Batch 335, accuracy/top5 = 0.78
I0817 10:58:11.888489 14586 caffe.cpp:313] Batch 335, loss = 2.15712
I0817 10:58:11.951125 14586 caffe.cpp:313] Batch 336, accuracy/top1 = 0.72
I0817 10:58:11.951148 14586 caffe.cpp:313] Batch 336, accuracy/top5 = 0.88
I0817 10:58:11.951151 14586 caffe.cpp:313] Batch 336, loss = 1.33304
I0817 10:58:12.013702 14586 caffe.cpp:313] Batch 337, accuracy/top1 = 0.72
I0817 10:58:12.013725 14586 caffe.cpp:313] Batch 337, accuracy/top5 = 0.88
I0817 10:58:12.013730 14586 caffe.cpp:313] Batch 337, loss = 1.386
I0817 10:58:12.076406 14586 caffe.cpp:313] Batch 338, accuracy/top1 = 0.62
I0817 10:58:12.076426 14586 caffe.cpp:313] Batch 338, accuracy/top5 = 0.78
I0817 10:58:12.076431 14586 caffe.cpp:313] Batch 338, loss = 1.71656
I0817 10:58:12.138983 14586 caffe.cpp:313] Batch 339, accuracy/top1 = 0.52
I0817 10:58:12.139005 14586 caffe.cpp:313] Batch 339, accuracy/top5 = 0.84
I0817 10:58:12.139009 14586 caffe.cpp:313] Batch 339, loss = 1.8881
I0817 10:58:12.201581 14586 caffe.cpp:313] Batch 340, accuracy/top1 = 0.48
I0817 10:58:12.201603 14586 caffe.cpp:313] Batch 340, accuracy/top5 = 0.82
I0817 10:58:12.201607 14586 caffe.cpp:313] Batch 340, loss = 2.27282
I0817 10:58:12.264327 14586 caffe.cpp:313] Batch 341, accuracy/top1 = 0.6
I0817 10:58:12.264348 14586 caffe.cpp:313] Batch 341, accuracy/top5 = 0.74
I0817 10:58:12.264353 14586 caffe.cpp:313] Batch 341, loss = 1.69482
I0817 10:58:12.327095 14586 caffe.cpp:313] Batch 342, accuracy/top1 = 0.6
I0817 10:58:12.327118 14586 caffe.cpp:313] Batch 342, accuracy/top5 = 0.84
I0817 10:58:12.327122 14586 caffe.cpp:313] Batch 342, loss = 1.62344
I0817 10:58:12.389679 14586 caffe.cpp:313] Batch 343, accuracy/top1 = 0.54
I0817 10:58:12.389701 14586 caffe.cpp:313] Batch 343, accuracy/top5 = 0.8
I0817 10:58:12.389705 14586 caffe.cpp:313] Batch 343, loss = 1.76622
I0817 10:58:12.452280 14586 caffe.cpp:313] Batch 344, accuracy/top1 = 0.6
I0817 10:58:12.452302 14586 caffe.cpp:313] Batch 344, accuracy/top5 = 0.84
I0817 10:58:12.452306 14586 caffe.cpp:313] Batch 344, loss = 1.47115
I0817 10:58:12.515055 14586 caffe.cpp:313] Batch 345, accuracy/top1 = 0.58
I0817 10:58:12.515079 14586 caffe.cpp:313] Batch 345, accuracy/top5 = 0.88
I0817 10:58:12.515082 14586 caffe.cpp:313] Batch 345, loss = 1.65578
I0817 10:58:12.577872 14586 caffe.cpp:313] Batch 346, accuracy/top1 = 0.48
I0817 10:58:12.577895 14586 caffe.cpp:313] Batch 346, accuracy/top5 = 0.72
I0817 10:58:12.577899 14586 caffe.cpp:313] Batch 346, loss = 2.23447
I0817 10:58:12.640589 14586 caffe.cpp:313] Batch 347, accuracy/top1 = 0.54
I0817 10:58:12.640612 14586 caffe.cpp:313] Batch 347, accuracy/top5 = 0.88
I0817 10:58:12.640616 14586 caffe.cpp:313] Batch 347, loss = 1.59444
I0817 10:58:12.703331 14586 caffe.cpp:313] Batch 348, accuracy/top1 = 0.66
I0817 10:58:12.703352 14586 caffe.cpp:313] Batch 348, accuracy/top5 = 0.84
I0817 10:58:12.703373 14586 caffe.cpp:313] Batch 348, loss = 1.41879
I0817 10:58:12.766034 14586 caffe.cpp:313] Batch 349, accuracy/top1 = 0.5
I0817 10:58:12.766057 14586 caffe.cpp:313] Batch 349, accuracy/top5 = 0.82
I0817 10:58:12.766060 14586 caffe.cpp:313] Batch 349, loss = 2.1617
I0817 10:58:12.829007 14586 caffe.cpp:313] Batch 350, accuracy/top1 = 0.68
I0817 10:58:12.829027 14586 caffe.cpp:313] Batch 350, accuracy/top5 = 0.8
I0817 10:58:12.829031 14586 caffe.cpp:313] Batch 350, loss = 1.49721
I0817 10:58:12.892062 14586 caffe.cpp:313] Batch 351, accuracy/top1 = 0.64
I0817 10:58:12.892082 14586 caffe.cpp:313] Batch 351, accuracy/top5 = 0.78
I0817 10:58:12.892087 14586 caffe.cpp:313] Batch 351, loss = 1.60202
I0817 10:58:12.954933 14586 caffe.cpp:313] Batch 352, accuracy/top1 = 0.6
I0817 10:58:12.954952 14586 caffe.cpp:313] Batch 352, accuracy/top5 = 0.74
I0817 10:58:12.954957 14586 caffe.cpp:313] Batch 352, loss = 2.37324
I0817 10:58:13.018626 14586 caffe.cpp:313] Batch 353, accuracy/top1 = 0.6
I0817 10:58:13.018648 14586 caffe.cpp:313] Batch 353, accuracy/top5 = 0.78
I0817 10:58:13.018652 14586 caffe.cpp:313] Batch 353, loss = 1.97939
I0817 10:58:13.081252 14586 caffe.cpp:313] Batch 354, accuracy/top1 = 0.54
I0817 10:58:13.081274 14586 caffe.cpp:313] Batch 354, accuracy/top5 = 0.76
I0817 10:58:13.081279 14586 caffe.cpp:313] Batch 354, loss = 2.40583
I0817 10:58:13.143972 14586 caffe.cpp:313] Batch 355, accuracy/top1 = 0.56
I0817 10:58:13.143996 14586 caffe.cpp:313] Batch 355, accuracy/top5 = 0.78
I0817 10:58:13.143999 14586 caffe.cpp:313] Batch 355, loss = 1.90712
I0817 10:58:13.206617 14586 caffe.cpp:313] Batch 356, accuracy/top1 = 0.56
I0817 10:58:13.206640 14586 caffe.cpp:313] Batch 356, accuracy/top5 = 0.78
I0817 10:58:13.206645 14586 caffe.cpp:313] Batch 356, loss = 1.80684
I0817 10:58:13.269284 14586 caffe.cpp:313] Batch 357, accuracy/top1 = 0.64
I0817 10:58:13.269306 14586 caffe.cpp:313] Batch 357, accuracy/top5 = 0.88
I0817 10:58:13.269311 14586 caffe.cpp:313] Batch 357, loss = 1.50174
I0817 10:58:13.331918 14586 caffe.cpp:313] Batch 358, accuracy/top1 = 0.5
I0817 10:58:13.331940 14586 caffe.cpp:313] Batch 358, accuracy/top5 = 0.76
I0817 10:58:13.331944 14586 caffe.cpp:313] Batch 358, loss = 2.27703
I0817 10:58:13.394660 14586 caffe.cpp:313] Batch 359, accuracy/top1 = 0.48
I0817 10:58:13.394682 14586 caffe.cpp:313] Batch 359, accuracy/top5 = 0.84
I0817 10:58:13.394686 14586 caffe.cpp:313] Batch 359, loss = 2.24128
I0817 10:58:13.457229 14586 caffe.cpp:313] Batch 360, accuracy/top1 = 0.66
I0817 10:58:13.457252 14586 caffe.cpp:313] Batch 360, accuracy/top5 = 0.86
I0817 10:58:13.457255 14586 caffe.cpp:313] Batch 360, loss = 1.90759
I0817 10:58:13.519726 14586 caffe.cpp:313] Batch 361, accuracy/top1 = 0.56
I0817 10:58:13.519749 14586 caffe.cpp:313] Batch 361, accuracy/top5 = 0.86
I0817 10:58:13.519753 14586 caffe.cpp:313] Batch 361, loss = 1.59288
I0817 10:58:13.582239 14586 caffe.cpp:313] Batch 362, accuracy/top1 = 0.58
I0817 10:58:13.582260 14586 caffe.cpp:313] Batch 362, accuracy/top5 = 0.74
I0817 10:58:13.582264 14586 caffe.cpp:313] Batch 362, loss = 2.04033
I0817 10:58:13.645076 14586 caffe.cpp:313] Batch 363, accuracy/top1 = 0.56
I0817 10:58:13.645098 14586 caffe.cpp:313] Batch 363, accuracy/top5 = 0.82
I0817 10:58:13.645102 14586 caffe.cpp:313] Batch 363, loss = 1.95665
I0817 10:58:13.707733 14586 caffe.cpp:313] Batch 364, accuracy/top1 = 0.6
I0817 10:58:13.707756 14586 caffe.cpp:313] Batch 364, accuracy/top5 = 0.82
I0817 10:58:13.707761 14586 caffe.cpp:313] Batch 364, loss = 1.55344
I0817 10:58:13.770548 14586 caffe.cpp:313] Batch 365, accuracy/top1 = 0.66
I0817 10:58:13.770571 14586 caffe.cpp:313] Batch 365, accuracy/top5 = 0.82
I0817 10:58:13.770575 14586 caffe.cpp:313] Batch 365, loss = 1.59383
I0817 10:58:13.833577 14586 caffe.cpp:313] Batch 366, accuracy/top1 = 0.48
I0817 10:58:13.833595 14586 caffe.cpp:313] Batch 366, accuracy/top5 = 0.78
I0817 10:58:13.833600 14586 caffe.cpp:313] Batch 366, loss = 1.99148
I0817 10:58:13.896281 14586 caffe.cpp:313] Batch 367, accuracy/top1 = 0.6
I0817 10:58:13.896315 14586 caffe.cpp:313] Batch 367, accuracy/top5 = 0.82
I0817 10:58:13.896319 14586 caffe.cpp:313] Batch 367, loss = 1.67384
I0817 10:58:13.959026 14586 caffe.cpp:313] Batch 368, accuracy/top1 = 0.52
I0817 10:58:13.959048 14586 caffe.cpp:313] Batch 368, accuracy/top5 = 0.84
I0817 10:58:13.959053 14586 caffe.cpp:313] Batch 368, loss = 1.79547
I0817 10:58:14.021697 14586 caffe.cpp:313] Batch 369, accuracy/top1 = 0.66
I0817 10:58:14.021716 14586 caffe.cpp:313] Batch 369, accuracy/top5 = 0.88
I0817 10:58:14.021720 14586 caffe.cpp:313] Batch 369, loss = 1.45645
I0817 10:58:14.084373 14586 caffe.cpp:313] Batch 370, accuracy/top1 = 0.7
I0817 10:58:14.084396 14586 caffe.cpp:313] Batch 370, accuracy/top5 = 0.8
I0817 10:58:14.084400 14586 caffe.cpp:313] Batch 370, loss = 1.59143
I0817 10:58:14.147035 14586 caffe.cpp:313] Batch 371, accuracy/top1 = 0.68
I0817 10:58:14.147058 14586 caffe.cpp:313] Batch 371, accuracy/top5 = 0.8
I0817 10:58:14.147061 14586 caffe.cpp:313] Batch 371, loss = 1.69905
I0817 10:58:14.209573 14586 caffe.cpp:313] Batch 372, accuracy/top1 = 0.72
I0817 10:58:14.209595 14586 caffe.cpp:313] Batch 372, accuracy/top5 = 0.9
I0817 10:58:14.209599 14586 caffe.cpp:313] Batch 372, loss = 1.38197
I0817 10:58:14.272274 14586 caffe.cpp:313] Batch 373, accuracy/top1 = 0.46
I0817 10:58:14.272297 14586 caffe.cpp:313] Batch 373, accuracy/top5 = 0.76
I0817 10:58:14.272301 14586 caffe.cpp:313] Batch 373, loss = 2.46941
I0817 10:58:14.334951 14586 caffe.cpp:313] Batch 374, accuracy/top1 = 0.62
I0817 10:58:14.334974 14586 caffe.cpp:313] Batch 374, accuracy/top5 = 0.76
I0817 10:58:14.334977 14586 caffe.cpp:313] Batch 374, loss = 1.72137
I0817 10:58:14.397652 14586 caffe.cpp:313] Batch 375, accuracy/top1 = 0.68
I0817 10:58:14.397673 14586 caffe.cpp:313] Batch 375, accuracy/top5 = 0.94
I0817 10:58:14.397678 14586 caffe.cpp:313] Batch 375, loss = 1.17112
I0817 10:58:14.460444 14586 caffe.cpp:313] Batch 376, accuracy/top1 = 0.56
I0817 10:58:14.460467 14586 caffe.cpp:313] Batch 376, accuracy/top5 = 0.86
I0817 10:58:14.460471 14586 caffe.cpp:313] Batch 376, loss = 1.55789
I0817 10:58:14.523129 14586 caffe.cpp:313] Batch 377, accuracy/top1 = 0.48
I0817 10:58:14.523152 14586 caffe.cpp:313] Batch 377, accuracy/top5 = 0.78
I0817 10:58:14.523156 14586 caffe.cpp:313] Batch 377, loss = 1.90143
I0817 10:58:14.585697 14586 caffe.cpp:313] Batch 378, accuracy/top1 = 0.66
I0817 10:58:14.585719 14586 caffe.cpp:313] Batch 378, accuracy/top5 = 0.86
I0817 10:58:14.585723 14586 caffe.cpp:313] Batch 378, loss = 1.51374
I0817 10:58:14.648233 14586 caffe.cpp:313] Batch 379, accuracy/top1 = 0.68
I0817 10:58:14.648255 14586 caffe.cpp:313] Batch 379, accuracy/top5 = 0.9
I0817 10:58:14.648259 14586 caffe.cpp:313] Batch 379, loss = 1.35905
I0817 10:58:14.710813 14586 caffe.cpp:313] Batch 380, accuracy/top1 = 0.6
I0817 10:58:14.710835 14586 caffe.cpp:313] Batch 380, accuracy/top5 = 0.84
I0817 10:58:14.710839 14586 caffe.cpp:313] Batch 380, loss = 1.61545
I0817 10:58:14.773533 14586 caffe.cpp:313] Batch 381, accuracy/top1 = 0.42
I0817 10:58:14.773556 14586 caffe.cpp:313] Batch 381, accuracy/top5 = 0.74
I0817 10:58:14.773561 14586 caffe.cpp:313] Batch 381, loss = 2.46216
I0817 10:58:14.836607 14586 caffe.cpp:313] Batch 382, accuracy/top1 = 0.58
I0817 10:58:14.836627 14586 caffe.cpp:313] Batch 382, accuracy/top5 = 0.7
I0817 10:58:14.836630 14586 caffe.cpp:313] Batch 382, loss = 2.13146
I0817 10:58:14.899473 14586 caffe.cpp:313] Batch 383, accuracy/top1 = 0.64
I0817 10:58:14.899492 14586 caffe.cpp:313] Batch 383, accuracy/top5 = 0.86
I0817 10:58:14.899497 14586 caffe.cpp:313] Batch 383, loss = 1.62703
I0817 10:58:14.962366 14586 caffe.cpp:313] Batch 384, accuracy/top1 = 0.64
I0817 10:58:14.962388 14586 caffe.cpp:313] Batch 384, accuracy/top5 = 0.84
I0817 10:58:14.962393 14586 caffe.cpp:313] Batch 384, loss = 1.66705
I0817 10:58:15.025156 14586 caffe.cpp:313] Batch 385, accuracy/top1 = 0.6
I0817 10:58:15.025174 14586 caffe.cpp:313] Batch 385, accuracy/top5 = 0.82
I0817 10:58:15.025178 14586 caffe.cpp:313] Batch 385, loss = 1.57569
I0817 10:58:15.087860 14586 caffe.cpp:313] Batch 386, accuracy/top1 = 0.62
I0817 10:58:15.087882 14586 caffe.cpp:313] Batch 386, accuracy/top5 = 0.86
I0817 10:58:15.087885 14586 caffe.cpp:313] Batch 386, loss = 1.69903
I0817 10:58:15.150576 14586 caffe.cpp:313] Batch 387, accuracy/top1 = 0.54
I0817 10:58:15.150599 14586 caffe.cpp:313] Batch 387, accuracy/top5 = 0.78
I0817 10:58:15.150602 14586 caffe.cpp:313] Batch 387, loss = 1.68459
I0817 10:58:15.213342 14586 caffe.cpp:313] Batch 388, accuracy/top1 = 0.54
I0817 10:58:15.213364 14586 caffe.cpp:313] Batch 388, accuracy/top5 = 0.84
I0817 10:58:15.213369 14586 caffe.cpp:313] Batch 388, loss = 1.58311
I0817 10:58:15.275969 14586 caffe.cpp:313] Batch 389, accuracy/top1 = 0.52
I0817 10:58:15.275992 14586 caffe.cpp:313] Batch 389, accuracy/top5 = 0.74
I0817 10:58:15.275995 14586 caffe.cpp:313] Batch 389, loss = 2.12611
I0817 10:58:15.338548 14586 caffe.cpp:313] Batch 390, accuracy/top1 = 0.62
I0817 10:58:15.338572 14586 caffe.cpp:313] Batch 390, accuracy/top5 = 0.92
I0817 10:58:15.338575 14586 caffe.cpp:313] Batch 390, loss = 1.51823
I0817 10:58:15.401067 14586 caffe.cpp:313] Batch 391, accuracy/top1 = 0.58
I0817 10:58:15.401089 14586 caffe.cpp:313] Batch 391, accuracy/top5 = 0.82
I0817 10:58:15.401093 14586 caffe.cpp:313] Batch 391, loss = 1.81176
I0817 10:58:15.463722 14586 caffe.cpp:313] Batch 392, accuracy/top1 = 0.5
I0817 10:58:15.463744 14586 caffe.cpp:313] Batch 392, accuracy/top5 = 0.9
I0817 10:58:15.463748 14586 caffe.cpp:313] Batch 392, loss = 1.59182
I0817 10:58:15.526273 14586 caffe.cpp:313] Batch 393, accuracy/top1 = 0.52
I0817 10:58:15.526295 14586 caffe.cpp:313] Batch 393, accuracy/top5 = 0.72
I0817 10:58:15.526299 14586 caffe.cpp:313] Batch 393, loss = 2.20562
I0817 10:58:15.588874 14586 caffe.cpp:313] Batch 394, accuracy/top1 = 0.64
I0817 10:58:15.588896 14586 caffe.cpp:313] Batch 394, accuracy/top5 = 0.9
I0817 10:58:15.588901 14586 caffe.cpp:313] Batch 394, loss = 1.38045
I0817 10:58:15.651337 14586 caffe.cpp:313] Batch 395, accuracy/top1 = 0.74
I0817 10:58:15.651360 14586 caffe.cpp:313] Batch 395, accuracy/top5 = 0.88
I0817 10:58:15.651365 14586 caffe.cpp:313] Batch 395, loss = 1.52942
I0817 10:58:15.713937 14586 caffe.cpp:313] Batch 396, accuracy/top1 = 0.56
I0817 10:58:15.713960 14586 caffe.cpp:313] Batch 396, accuracy/top5 = 0.82
I0817 10:58:15.713964 14586 caffe.cpp:313] Batch 396, loss = 1.91394
I0817 10:58:15.776602 14586 caffe.cpp:313] Batch 397, accuracy/top1 = 0.52
I0817 10:58:15.776624 14586 caffe.cpp:313] Batch 397, accuracy/top5 = 0.78
I0817 10:58:15.776628 14586 caffe.cpp:313] Batch 397, loss = 2.40765
I0817 10:58:15.839665 14586 caffe.cpp:313] Batch 398, accuracy/top1 = 0.66
I0817 10:58:15.839684 14586 caffe.cpp:313] Batch 398, accuracy/top5 = 0.82
I0817 10:58:15.839687 14586 caffe.cpp:313] Batch 398, loss = 1.80101
I0817 10:58:15.902326 14586 caffe.cpp:313] Batch 399, accuracy/top1 = 0.62
I0817 10:58:15.902348 14586 caffe.cpp:313] Batch 399, accuracy/top5 = 0.82
I0817 10:58:15.902353 14586 caffe.cpp:313] Batch 399, loss = 1.71398
I0817 10:58:15.964879 14586 caffe.cpp:313] Batch 400, accuracy/top1 = 0.52
I0817 10:58:15.964901 14586 caffe.cpp:313] Batch 400, accuracy/top5 = 0.74
I0817 10:58:15.964905 14586 caffe.cpp:313] Batch 400, loss = 2.10752
I0817 10:58:16.027473 14586 caffe.cpp:313] Batch 401, accuracy/top1 = 0.58
I0817 10:58:16.027493 14586 caffe.cpp:313] Batch 401, accuracy/top5 = 0.9
I0817 10:58:16.027498 14586 caffe.cpp:313] Batch 401, loss = 1.56755
I0817 10:58:16.090203 14586 caffe.cpp:313] Batch 402, accuracy/top1 = 0.62
I0817 10:58:16.090224 14586 caffe.cpp:313] Batch 402, accuracy/top5 = 0.74
I0817 10:58:16.090229 14586 caffe.cpp:313] Batch 402, loss = 1.9469
I0817 10:58:16.152832 14586 caffe.cpp:313] Batch 403, accuracy/top1 = 0.54
I0817 10:58:16.152853 14586 caffe.cpp:313] Batch 403, accuracy/top5 = 0.82
I0817 10:58:16.152858 14586 caffe.cpp:313] Batch 403, loss = 1.91725
I0817 10:58:16.215512 14586 caffe.cpp:313] Batch 404, accuracy/top1 = 0.72
I0817 10:58:16.215533 14586 caffe.cpp:313] Batch 404, accuracy/top5 = 0.88
I0817 10:58:16.215555 14586 caffe.cpp:313] Batch 404, loss = 1.38111
I0817 10:58:16.278195 14586 caffe.cpp:313] Batch 405, accuracy/top1 = 0.64
I0817 10:58:16.278218 14586 caffe.cpp:313] Batch 405, accuracy/top5 = 0.84
I0817 10:58:16.278221 14586 caffe.cpp:313] Batch 405, loss = 1.78734
I0817 10:58:16.340906 14586 caffe.cpp:313] Batch 406, accuracy/top1 = 0.64
I0817 10:58:16.340929 14586 caffe.cpp:313] Batch 406, accuracy/top5 = 0.88
I0817 10:58:16.340932 14586 caffe.cpp:313] Batch 406, loss = 1.58687
I0817 10:58:16.403610 14586 caffe.cpp:313] Batch 407, accuracy/top1 = 0.7
I0817 10:58:16.403633 14586 caffe.cpp:313] Batch 407, accuracy/top5 = 0.88
I0817 10:58:16.403636 14586 caffe.cpp:313] Batch 407, loss = 1.19757
I0817 10:58:16.466231 14586 caffe.cpp:313] Batch 408, accuracy/top1 = 0.5
I0817 10:58:16.466253 14586 caffe.cpp:313] Batch 408, accuracy/top5 = 0.76
I0817 10:58:16.466256 14586 caffe.cpp:313] Batch 408, loss = 2.20845
I0817 10:58:16.528952 14586 caffe.cpp:313] Batch 409, accuracy/top1 = 0.5
I0817 10:58:16.528975 14586 caffe.cpp:313] Batch 409, accuracy/top5 = 0.84
I0817 10:58:16.528978 14586 caffe.cpp:313] Batch 409, loss = 1.94125
I0817 10:58:16.591687 14586 caffe.cpp:313] Batch 410, accuracy/top1 = 0.64
I0817 10:58:16.591711 14586 caffe.cpp:313] Batch 410, accuracy/top5 = 0.78
I0817 10:58:16.591714 14586 caffe.cpp:313] Batch 410, loss = 1.88615
I0817 10:58:16.654420 14586 caffe.cpp:313] Batch 411, accuracy/top1 = 0.62
I0817 10:58:16.654443 14586 caffe.cpp:313] Batch 411, accuracy/top5 = 0.84
I0817 10:58:16.654448 14586 caffe.cpp:313] Batch 411, loss = 1.65261
I0817 10:58:16.716946 14586 caffe.cpp:313] Batch 412, accuracy/top1 = 0.64
I0817 10:58:16.716969 14586 caffe.cpp:313] Batch 412, accuracy/top5 = 0.86
I0817 10:58:16.716972 14586 caffe.cpp:313] Batch 412, loss = 1.40815
I0817 10:58:16.779587 14586 caffe.cpp:313] Batch 413, accuracy/top1 = 0.56
I0817 10:58:16.779609 14586 caffe.cpp:313] Batch 413, accuracy/top5 = 0.78
I0817 10:58:16.779613 14586 caffe.cpp:313] Batch 413, loss = 2.02461
I0817 10:58:16.842766 14586 caffe.cpp:313] Batch 414, accuracy/top1 = 0.56
I0817 10:58:16.842783 14586 caffe.cpp:313] Batch 414, accuracy/top5 = 0.78
I0817 10:58:16.842787 14586 caffe.cpp:313] Batch 414, loss = 1.93394
I0817 10:58:16.905385 14586 caffe.cpp:313] Batch 415, accuracy/top1 = 0.62
I0817 10:58:16.905402 14586 caffe.cpp:313] Batch 415, accuracy/top5 = 0.8
I0817 10:58:16.905406 14586 caffe.cpp:313] Batch 415, loss = 1.63926
I0817 10:58:16.968083 14586 caffe.cpp:313] Batch 416, accuracy/top1 = 0.5
I0817 10:58:16.968106 14586 caffe.cpp:313] Batch 416, accuracy/top5 = 0.82
I0817 10:58:16.968111 14586 caffe.cpp:313] Batch 416, loss = 2.20563
I0817 10:58:17.030831 14586 caffe.cpp:313] Batch 417, accuracy/top1 = 0.58
I0817 10:58:17.030849 14586 caffe.cpp:313] Batch 417, accuracy/top5 = 0.8
I0817 10:58:17.030854 14586 caffe.cpp:313] Batch 417, loss = 1.86891
I0817 10:58:17.093542 14586 caffe.cpp:313] Batch 418, accuracy/top1 = 0.48
I0817 10:58:17.093565 14586 caffe.cpp:313] Batch 418, accuracy/top5 = 0.86
I0817 10:58:17.093569 14586 caffe.cpp:313] Batch 418, loss = 1.63302
I0817 10:58:17.156282 14586 caffe.cpp:313] Batch 419, accuracy/top1 = 0.54
I0817 10:58:17.156302 14586 caffe.cpp:313] Batch 419, accuracy/top5 = 0.78
I0817 10:58:17.156306 14586 caffe.cpp:313] Batch 419, loss = 1.98433
I0817 10:58:17.219076 14586 caffe.cpp:313] Batch 420, accuracy/top1 = 0.56
I0817 10:58:17.219096 14586 caffe.cpp:313] Batch 420, accuracy/top5 = 0.84
I0817 10:58:17.219100 14586 caffe.cpp:313] Batch 420, loss = 1.79187
I0817 10:58:17.281790 14586 caffe.cpp:313] Batch 421, accuracy/top1 = 0.66
I0817 10:58:17.281811 14586 caffe.cpp:313] Batch 421, accuracy/top5 = 0.82
I0817 10:58:17.281816 14586 caffe.cpp:313] Batch 421, loss = 1.72961
I0817 10:58:17.344396 14586 caffe.cpp:313] Batch 422, accuracy/top1 = 0.58
I0817 10:58:17.344419 14586 caffe.cpp:313] Batch 422, accuracy/top5 = 0.78
I0817 10:58:17.344422 14586 caffe.cpp:313] Batch 422, loss = 1.87048
I0817 10:58:17.406930 14586 caffe.cpp:313] Batch 423, accuracy/top1 = 0.6
I0817 10:58:17.406952 14586 caffe.cpp:313] Batch 423, accuracy/top5 = 0.74
I0817 10:58:17.406975 14586 caffe.cpp:313] Batch 423, loss = 2.22354
I0817 10:58:17.469669 14586 caffe.cpp:313] Batch 424, accuracy/top1 = 0.62
I0817 10:58:17.469691 14586 caffe.cpp:313] Batch 424, accuracy/top5 = 0.9
I0817 10:58:17.469696 14586 caffe.cpp:313] Batch 424, loss = 1.28437
I0817 10:58:17.532356 14586 caffe.cpp:313] Batch 425, accuracy/top1 = 0.66
I0817 10:58:17.532378 14586 caffe.cpp:313] Batch 425, accuracy/top5 = 0.86
I0817 10:58:17.532382 14586 caffe.cpp:313] Batch 425, loss = 1.86003
I0817 10:58:17.595079 14586 caffe.cpp:313] Batch 426, accuracy/top1 = 0.54
I0817 10:58:17.595101 14586 caffe.cpp:313] Batch 426, accuracy/top5 = 0.76
I0817 10:58:17.595105 14586 caffe.cpp:313] Batch 426, loss = 1.92858
I0817 10:58:17.657624 14586 caffe.cpp:313] Batch 427, accuracy/top1 = 0.6
I0817 10:58:17.657647 14586 caffe.cpp:313] Batch 427, accuracy/top5 = 0.82
I0817 10:58:17.657651 14586 caffe.cpp:313] Batch 427, loss = 1.69056
I0817 10:58:17.720294 14586 caffe.cpp:313] Batch 428, accuracy/top1 = 0.66
I0817 10:58:17.720317 14586 caffe.cpp:313] Batch 428, accuracy/top5 = 0.86
I0817 10:58:17.720320 14586 caffe.cpp:313] Batch 428, loss = 1.372
I0817 10:58:17.783032 14586 caffe.cpp:313] Batch 429, accuracy/top1 = 0.64
I0817 10:58:17.783056 14586 caffe.cpp:313] Batch 429, accuracy/top5 = 0.86
I0817 10:58:17.783059 14586 caffe.cpp:313] Batch 429, loss = 1.45442
I0817 10:58:17.846195 14586 caffe.cpp:313] Batch 430, accuracy/top1 = 0.54
I0817 10:58:17.846213 14586 caffe.cpp:313] Batch 430, accuracy/top5 = 0.86
I0817 10:58:17.846217 14586 caffe.cpp:313] Batch 430, loss = 1.49924
I0817 10:58:17.908921 14586 caffe.cpp:313] Batch 431, accuracy/top1 = 0.6
I0817 10:58:17.908941 14586 caffe.cpp:313] Batch 431, accuracy/top5 = 0.84
I0817 10:58:17.908946 14586 caffe.cpp:313] Batch 431, loss = 1.92214
I0817 10:58:17.971575 14586 caffe.cpp:313] Batch 432, accuracy/top1 = 0.54
I0817 10:58:17.971597 14586 caffe.cpp:313] Batch 432, accuracy/top5 = 0.74
I0817 10:58:17.971601 14586 caffe.cpp:313] Batch 432, loss = 2.32511
I0817 10:58:18.034270 14586 caffe.cpp:313] Batch 433, accuracy/top1 = 0.56
I0817 10:58:18.034289 14586 caffe.cpp:313] Batch 433, accuracy/top5 = 0.88
I0817 10:58:18.034293 14586 caffe.cpp:313] Batch 433, loss = 1.55602
I0817 10:58:18.097024 14586 caffe.cpp:313] Batch 434, accuracy/top1 = 0.44
I0817 10:58:18.097046 14586 caffe.cpp:313] Batch 434, accuracy/top5 = 0.8
I0817 10:58:18.097051 14586 caffe.cpp:313] Batch 434, loss = 2.22084
I0817 10:58:18.159704 14586 caffe.cpp:313] Batch 435, accuracy/top1 = 0.64
I0817 10:58:18.159726 14586 caffe.cpp:313] Batch 435, accuracy/top5 = 0.88
I0817 10:58:18.159730 14586 caffe.cpp:313] Batch 435, loss = 1.50534
I0817 10:58:18.222415 14586 caffe.cpp:313] Batch 436, accuracy/top1 = 0.58
I0817 10:58:18.222439 14586 caffe.cpp:313] Batch 436, accuracy/top5 = 0.8
I0817 10:58:18.222442 14586 caffe.cpp:313] Batch 436, loss = 1.9984
I0817 10:58:18.285095 14586 caffe.cpp:313] Batch 437, accuracy/top1 = 0.48
I0817 10:58:18.285117 14586 caffe.cpp:313] Batch 437, accuracy/top5 = 0.7
I0817 10:58:18.285121 14586 caffe.cpp:313] Batch 437, loss = 2.15024
I0817 10:58:18.347853 14586 caffe.cpp:313] Batch 438, accuracy/top1 = 0.58
I0817 10:58:18.347877 14586 caffe.cpp:313] Batch 438, accuracy/top5 = 0.8
I0817 10:58:18.347880 14586 caffe.cpp:313] Batch 438, loss = 1.78893
I0817 10:58:18.410636 14586 caffe.cpp:313] Batch 439, accuracy/top1 = 0.64
I0817 10:58:18.410658 14586 caffe.cpp:313] Batch 439, accuracy/top5 = 0.82
I0817 10:58:18.410662 14586 caffe.cpp:313] Batch 439, loss = 2.13394
I0817 10:58:18.473273 14586 caffe.cpp:313] Batch 440, accuracy/top1 = 0.52
I0817 10:58:18.473295 14586 caffe.cpp:313] Batch 440, accuracy/top5 = 0.74
I0817 10:58:18.473299 14586 caffe.cpp:313] Batch 440, loss = 1.74012
I0817 10:58:18.535835 14586 caffe.cpp:313] Batch 441, accuracy/top1 = 0.52
I0817 10:58:18.535856 14586 caffe.cpp:313] Batch 441, accuracy/top5 = 0.88
I0817 10:58:18.535861 14586 caffe.cpp:313] Batch 441, loss = 1.60023
I0817 10:58:18.598573 14586 caffe.cpp:313] Batch 442, accuracy/top1 = 0.6
I0817 10:58:18.598610 14586 caffe.cpp:313] Batch 442, accuracy/top5 = 0.9
I0817 10:58:18.598615 14586 caffe.cpp:313] Batch 442, loss = 1.64609
I0817 10:58:18.661372 14586 caffe.cpp:313] Batch 443, accuracy/top1 = 0.68
I0817 10:58:18.661394 14586 caffe.cpp:313] Batch 443, accuracy/top5 = 0.8
I0817 10:58:18.661398 14586 caffe.cpp:313] Batch 443, loss = 1.48518
I0817 10:58:18.724097 14586 caffe.cpp:313] Batch 444, accuracy/top1 = 0.68
I0817 10:58:18.724118 14586 caffe.cpp:313] Batch 444, accuracy/top5 = 0.84
I0817 10:58:18.724123 14586 caffe.cpp:313] Batch 444, loss = 1.27999
I0817 10:58:18.786743 14586 caffe.cpp:313] Batch 445, accuracy/top1 = 0.6
I0817 10:58:18.786765 14586 caffe.cpp:313] Batch 445, accuracy/top5 = 0.86
I0817 10:58:18.786769 14586 caffe.cpp:313] Batch 445, loss = 1.69574
I0817 10:58:18.849824 14586 caffe.cpp:313] Batch 446, accuracy/top1 = 0.6
I0817 10:58:18.849841 14586 caffe.cpp:313] Batch 446, accuracy/top5 = 0.86
I0817 10:58:18.849845 14586 caffe.cpp:313] Batch 446, loss = 1.45465
I0817 10:58:18.912554 14586 caffe.cpp:313] Batch 447, accuracy/top1 = 0.6
I0817 10:58:18.912576 14586 caffe.cpp:313] Batch 447, accuracy/top5 = 0.82
I0817 10:58:18.912580 14586 caffe.cpp:313] Batch 447, loss = 1.67553
I0817 10:58:18.975240 14586 caffe.cpp:313] Batch 448, accuracy/top1 = 0.56
I0817 10:58:18.975260 14586 caffe.cpp:313] Batch 448, accuracy/top5 = 0.8
I0817 10:58:18.975265 14586 caffe.cpp:313] Batch 448, loss = 1.51133
I0817 10:58:19.037999 14586 caffe.cpp:313] Batch 449, accuracy/top1 = 0.54
I0817 10:58:19.038017 14586 caffe.cpp:313] Batch 449, accuracy/top5 = 0.8
I0817 10:58:19.038022 14586 caffe.cpp:313] Batch 449, loss = 2.03282
I0817 10:58:19.100725 14586 caffe.cpp:313] Batch 450, accuracy/top1 = 0.58
I0817 10:58:19.100749 14586 caffe.cpp:313] Batch 450, accuracy/top5 = 0.8
I0817 10:58:19.100752 14586 caffe.cpp:313] Batch 450, loss = 1.74968
I0817 10:58:19.163367 14586 caffe.cpp:313] Batch 451, accuracy/top1 = 0.56
I0817 10:58:19.163389 14586 caffe.cpp:313] Batch 451, accuracy/top5 = 0.84
I0817 10:58:19.163393 14586 caffe.cpp:313] Batch 451, loss = 1.7026
I0817 10:58:19.226065 14586 caffe.cpp:313] Batch 452, accuracy/top1 = 0.62
I0817 10:58:19.226089 14586 caffe.cpp:313] Batch 452, accuracy/top5 = 0.84
I0817 10:58:19.226092 14586 caffe.cpp:313] Batch 452, loss = 1.44832
I0817 10:58:19.288889 14586 caffe.cpp:313] Batch 453, accuracy/top1 = 0.6
I0817 10:58:19.288909 14586 caffe.cpp:313] Batch 453, accuracy/top5 = 0.84
I0817 10:58:19.288913 14586 caffe.cpp:313] Batch 453, loss = 1.67717
I0817 10:58:19.351614 14586 caffe.cpp:313] Batch 454, accuracy/top1 = 0.54
I0817 10:58:19.351634 14586 caffe.cpp:313] Batch 454, accuracy/top5 = 0.82
I0817 10:58:19.351637 14586 caffe.cpp:313] Batch 454, loss = 2.14393
I0817 10:58:19.414415 14586 caffe.cpp:313] Batch 455, accuracy/top1 = 0.54
I0817 10:58:19.414436 14586 caffe.cpp:313] Batch 455, accuracy/top5 = 0.8
I0817 10:58:19.414440 14586 caffe.cpp:313] Batch 455, loss = 1.81812
I0817 10:58:19.477030 14586 caffe.cpp:313] Batch 456, accuracy/top1 = 0.58
I0817 10:58:19.477052 14586 caffe.cpp:313] Batch 456, accuracy/top5 = 0.86
I0817 10:58:19.477056 14586 caffe.cpp:313] Batch 456, loss = 1.968
I0817 10:58:19.539768 14586 caffe.cpp:313] Batch 457, accuracy/top1 = 0.48
I0817 10:58:19.539791 14586 caffe.cpp:313] Batch 457, accuracy/top5 = 0.7
I0817 10:58:19.539795 14586 caffe.cpp:313] Batch 457, loss = 2.03943
I0817 10:58:19.602282 14586 caffe.cpp:313] Batch 458, accuracy/top1 = 0.52
I0817 10:58:19.602304 14586 caffe.cpp:313] Batch 458, accuracy/top5 = 0.7
I0817 10:58:19.602308 14586 caffe.cpp:313] Batch 458, loss = 2.23806
I0817 10:58:19.665029 14586 caffe.cpp:313] Batch 459, accuracy/top1 = 0.64
I0817 10:58:19.665050 14586 caffe.cpp:313] Batch 459, accuracy/top5 = 0.86
I0817 10:58:19.665055 14586 caffe.cpp:313] Batch 459, loss = 1.29553
I0817 10:58:19.727862 14586 caffe.cpp:313] Batch 460, accuracy/top1 = 0.64
I0817 10:58:19.727885 14586 caffe.cpp:313] Batch 460, accuracy/top5 = 0.78
I0817 10:58:19.727890 14586 caffe.cpp:313] Batch 460, loss = 1.75856
I0817 10:58:19.790606 14586 caffe.cpp:313] Batch 461, accuracy/top1 = 0.66
I0817 10:58:19.790628 14586 caffe.cpp:313] Batch 461, accuracy/top5 = 0.9
I0817 10:58:19.790632 14586 caffe.cpp:313] Batch 461, loss = 1.41554
I0817 10:58:19.854233 14586 caffe.cpp:313] Batch 462, accuracy/top1 = 0.66
I0817 10:58:19.854250 14586 caffe.cpp:313] Batch 462, accuracy/top5 = 0.84
I0817 10:58:19.854254 14586 caffe.cpp:313] Batch 462, loss = 1.44045
I0817 10:58:19.916965 14586 caffe.cpp:313] Batch 463, accuracy/top1 = 0.56
I0817 10:58:19.916987 14586 caffe.cpp:313] Batch 463, accuracy/top5 = 0.7
I0817 10:58:19.916991 14586 caffe.cpp:313] Batch 463, loss = 1.9855
I0817 10:58:19.979729 14586 caffe.cpp:313] Batch 464, accuracy/top1 = 0.5
I0817 10:58:19.979751 14586 caffe.cpp:313] Batch 464, accuracy/top5 = 0.84
I0817 10:58:19.979755 14586 caffe.cpp:313] Batch 464, loss = 1.81637
I0817 10:58:20.042587 14586 caffe.cpp:313] Batch 465, accuracy/top1 = 0.58
I0817 10:58:20.042604 14586 caffe.cpp:313] Batch 465, accuracy/top5 = 0.84
I0817 10:58:20.042608 14586 caffe.cpp:313] Batch 465, loss = 1.60198
I0817 10:58:20.105366 14586 caffe.cpp:313] Batch 466, accuracy/top1 = 0.58
I0817 10:58:20.105389 14586 caffe.cpp:313] Batch 466, accuracy/top5 = 0.82
I0817 10:58:20.105393 14586 caffe.cpp:313] Batch 466, loss = 1.79263
I0817 10:58:20.168038 14586 caffe.cpp:313] Batch 467, accuracy/top1 = 0.7
I0817 10:58:20.168145 14586 caffe.cpp:313] Batch 467, accuracy/top5 = 0.9
I0817 10:58:20.168151 14586 caffe.cpp:313] Batch 467, loss = 1.46586
I0817 10:58:20.230851 14586 caffe.cpp:313] Batch 468, accuracy/top1 = 0.56
I0817 10:58:20.230875 14586 caffe.cpp:313] Batch 468, accuracy/top5 = 0.78
I0817 10:58:20.230878 14586 caffe.cpp:313] Batch 468, loss = 1.85395
I0817 10:58:20.293727 14586 caffe.cpp:313] Batch 469, accuracy/top1 = 0.54
I0817 10:58:20.293751 14586 caffe.cpp:313] Batch 469, accuracy/top5 = 0.84
I0817 10:58:20.293754 14586 caffe.cpp:313] Batch 469, loss = 1.63964
I0817 10:58:20.356356 14586 caffe.cpp:313] Batch 470, accuracy/top1 = 0.58
I0817 10:58:20.356379 14586 caffe.cpp:313] Batch 470, accuracy/top5 = 0.82
I0817 10:58:20.356382 14586 caffe.cpp:313] Batch 470, loss = 1.6709
I0817 10:58:20.418951 14586 caffe.cpp:313] Batch 471, accuracy/top1 = 0.66
I0817 10:58:20.418973 14586 caffe.cpp:313] Batch 471, accuracy/top5 = 0.88
I0817 10:58:20.418977 14586 caffe.cpp:313] Batch 471, loss = 1.38115
I0817 10:58:20.481564 14586 caffe.cpp:313] Batch 472, accuracy/top1 = 0.62
I0817 10:58:20.481585 14586 caffe.cpp:313] Batch 472, accuracy/top5 = 0.82
I0817 10:58:20.481588 14586 caffe.cpp:313] Batch 472, loss = 1.5708
I0817 10:58:20.544406 14586 caffe.cpp:313] Batch 473, accuracy/top1 = 0.66
I0817 10:58:20.544430 14586 caffe.cpp:313] Batch 473, accuracy/top5 = 0.84
I0817 10:58:20.544433 14586 caffe.cpp:313] Batch 473, loss = 1.71834
I0817 10:58:20.607041 14586 caffe.cpp:313] Batch 474, accuracy/top1 = 0.62
I0817 10:58:20.607064 14586 caffe.cpp:313] Batch 474, accuracy/top5 = 0.82
I0817 10:58:20.607069 14586 caffe.cpp:313] Batch 474, loss = 1.6987
I0817 10:58:20.669664 14586 caffe.cpp:313] Batch 475, accuracy/top1 = 0.58
I0817 10:58:20.669687 14586 caffe.cpp:313] Batch 475, accuracy/top5 = 0.82
I0817 10:58:20.669690 14586 caffe.cpp:313] Batch 475, loss = 1.84431
I0817 10:58:20.732285 14586 caffe.cpp:313] Batch 476, accuracy/top1 = 0.56
I0817 10:58:20.732307 14586 caffe.cpp:313] Batch 476, accuracy/top5 = 0.8
I0817 10:58:20.732311 14586 caffe.cpp:313] Batch 476, loss = 1.9123
I0817 10:58:20.795166 14586 caffe.cpp:313] Batch 477, accuracy/top1 = 0.46
I0817 10:58:20.795188 14586 caffe.cpp:313] Batch 477, accuracy/top5 = 0.8
I0817 10:58:20.795192 14586 caffe.cpp:313] Batch 477, loss = 1.81698
I0817 10:58:20.858440 14586 caffe.cpp:313] Batch 478, accuracy/top1 = 0.58
I0817 10:58:20.858458 14586 caffe.cpp:313] Batch 478, accuracy/top5 = 0.84
I0817 10:58:20.858461 14586 caffe.cpp:313] Batch 478, loss = 1.69877
I0817 10:58:20.921171 14586 caffe.cpp:313] Batch 479, accuracy/top1 = 0.56
I0817 10:58:20.921193 14586 caffe.cpp:313] Batch 479, accuracy/top5 = 0.76
I0817 10:58:20.921197 14586 caffe.cpp:313] Batch 479, loss = 2.08901
I0817 10:58:20.984048 14586 caffe.cpp:313] Batch 480, accuracy/top1 = 0.64
I0817 10:58:20.984066 14586 caffe.cpp:313] Batch 480, accuracy/top5 = 0.84
I0817 10:58:20.984071 14586 caffe.cpp:313] Batch 480, loss = 1.53824
I0817 10:58:21.046792 14586 caffe.cpp:313] Batch 481, accuracy/top1 = 0.58
I0817 10:58:21.046811 14586 caffe.cpp:313] Batch 481, accuracy/top5 = 0.8
I0817 10:58:21.046815 14586 caffe.cpp:313] Batch 481, loss = 1.92409
I0817 10:58:21.109576 14586 caffe.cpp:313] Batch 482, accuracy/top1 = 0.48
I0817 10:58:21.109598 14586 caffe.cpp:313] Batch 482, accuracy/top5 = 0.72
I0817 10:58:21.109603 14586 caffe.cpp:313] Batch 482, loss = 2.3611
I0817 10:58:21.172348 14586 caffe.cpp:313] Batch 483, accuracy/top1 = 0.58
I0817 10:58:21.172370 14586 caffe.cpp:313] Batch 483, accuracy/top5 = 0.86
I0817 10:58:21.172374 14586 caffe.cpp:313] Batch 483, loss = 1.51483
I0817 10:58:21.235055 14586 caffe.cpp:313] Batch 484, accuracy/top1 = 0.54
I0817 10:58:21.235079 14586 caffe.cpp:313] Batch 484, accuracy/top5 = 0.76
I0817 10:58:21.235082 14586 caffe.cpp:313] Batch 484, loss = 2.32078
I0817 10:58:21.297693 14586 caffe.cpp:313] Batch 485, accuracy/top1 = 0.72
I0817 10:58:21.297715 14586 caffe.cpp:313] Batch 485, accuracy/top5 = 0.76
I0817 10:58:21.297719 14586 caffe.cpp:313] Batch 485, loss = 1.68464
I0817 10:58:21.360342 14586 caffe.cpp:313] Batch 486, accuracy/top1 = 0.64
I0817 10:58:21.360378 14586 caffe.cpp:313] Batch 486, accuracy/top5 = 0.88
I0817 10:58:21.360383 14586 caffe.cpp:313] Batch 486, loss = 1.18143
I0817 10:58:21.423173 14586 caffe.cpp:313] Batch 487, accuracy/top1 = 0.56
I0817 10:58:21.423193 14586 caffe.cpp:313] Batch 487, accuracy/top5 = 0.72
I0817 10:58:21.423197 14586 caffe.cpp:313] Batch 487, loss = 2.31629
I0817 10:58:21.485894 14586 caffe.cpp:313] Batch 488, accuracy/top1 = 0.48
I0817 10:58:21.485913 14586 caffe.cpp:313] Batch 488, accuracy/top5 = 0.82
I0817 10:58:21.485916 14586 caffe.cpp:313] Batch 488, loss = 1.96422
I0817 10:58:21.549299 14586 caffe.cpp:313] Batch 489, accuracy/top1 = 0.56
I0817 10:58:21.549319 14586 caffe.cpp:313] Batch 489, accuracy/top5 = 0.82
I0817 10:58:21.549324 14586 caffe.cpp:313] Batch 489, loss = 1.74685
I0817 10:58:21.612484 14586 caffe.cpp:313] Batch 490, accuracy/top1 = 0.58
I0817 10:58:21.612506 14586 caffe.cpp:313] Batch 490, accuracy/top5 = 0.84
I0817 10:58:21.612510 14586 caffe.cpp:313] Batch 490, loss = 1.68557
I0817 10:58:21.675119 14586 caffe.cpp:313] Batch 491, accuracy/top1 = 0.6
I0817 10:58:21.675142 14586 caffe.cpp:313] Batch 491, accuracy/top5 = 0.84
I0817 10:58:21.675145 14586 caffe.cpp:313] Batch 491, loss = 1.7429
I0817 10:58:21.737762 14586 caffe.cpp:313] Batch 492, accuracy/top1 = 0.56
I0817 10:58:21.737784 14586 caffe.cpp:313] Batch 492, accuracy/top5 = 0.7
I0817 10:58:21.737788 14586 caffe.cpp:313] Batch 492, loss = 2.27191
I0817 10:58:21.800472 14586 caffe.cpp:313] Batch 493, accuracy/top1 = 0.56
I0817 10:58:21.800494 14586 caffe.cpp:313] Batch 493, accuracy/top5 = 0.82
I0817 10:58:21.800498 14586 caffe.cpp:313] Batch 493, loss = 1.85382
I0817 10:58:21.863617 14586 caffe.cpp:313] Batch 494, accuracy/top1 = 0.54
I0817 10:58:21.863634 14586 caffe.cpp:313] Batch 494, accuracy/top5 = 0.76
I0817 10:58:21.863638 14586 caffe.cpp:313] Batch 494, loss = 1.88968
I0817 10:58:21.926190 14586 caffe.cpp:313] Batch 495, accuracy/top1 = 0.66
I0817 10:58:21.926213 14586 caffe.cpp:313] Batch 495, accuracy/top5 = 0.88
I0817 10:58:21.926216 14586 caffe.cpp:313] Batch 495, loss = 1.42837
I0817 10:58:21.988831 14586 caffe.cpp:313] Batch 496, accuracy/top1 = 0.56
I0817 10:58:21.988854 14586 caffe.cpp:313] Batch 496, accuracy/top5 = 0.82
I0817 10:58:21.988858 14586 caffe.cpp:313] Batch 496, loss = 1.8436
I0817 10:58:22.051542 14586 caffe.cpp:313] Batch 497, accuracy/top1 = 0.56
I0817 10:58:22.051560 14586 caffe.cpp:313] Batch 497, accuracy/top5 = 0.74
I0817 10:58:22.051564 14586 caffe.cpp:313] Batch 497, loss = 1.95493
I0817 10:58:22.114318 14586 caffe.cpp:313] Batch 498, accuracy/top1 = 0.68
I0817 10:58:22.114341 14586 caffe.cpp:313] Batch 498, accuracy/top5 = 0.86
I0817 10:58:22.114346 14586 caffe.cpp:313] Batch 498, loss = 1.2727
I0817 10:58:22.177089 14586 caffe.cpp:313] Batch 499, accuracy/top1 = 0.56
I0817 10:58:22.177110 14586 caffe.cpp:313] Batch 499, accuracy/top5 = 0.78
I0817 10:58:22.177114 14586 caffe.cpp:313] Batch 499, loss = 1.77856
I0817 10:58:22.239699 14586 caffe.cpp:313] Batch 500, accuracy/top1 = 0.58
I0817 10:58:22.239722 14586 caffe.cpp:313] Batch 500, accuracy/top5 = 0.8
I0817 10:58:22.239727 14586 caffe.cpp:313] Batch 500, loss = 1.98417
I0817 10:58:22.302407 14586 caffe.cpp:313] Batch 501, accuracy/top1 = 0.54
I0817 10:58:22.302428 14586 caffe.cpp:313] Batch 501, accuracy/top5 = 0.76
I0817 10:58:22.302431 14586 caffe.cpp:313] Batch 501, loss = 1.95916
I0817 10:58:22.365170 14586 caffe.cpp:313] Batch 502, accuracy/top1 = 0.38
I0817 10:58:22.365192 14586 caffe.cpp:313] Batch 502, accuracy/top5 = 0.74
I0817 10:58:22.365196 14586 caffe.cpp:313] Batch 502, loss = 2.38848
I0817 10:58:22.427901 14586 caffe.cpp:313] Batch 503, accuracy/top1 = 0.6
I0817 10:58:22.427923 14586 caffe.cpp:313] Batch 503, accuracy/top5 = 0.78
I0817 10:58:22.427927 14586 caffe.cpp:313] Batch 503, loss = 1.91557
I0817 10:58:22.490602 14586 caffe.cpp:313] Batch 504, accuracy/top1 = 0.62
I0817 10:58:22.490623 14586 caffe.cpp:313] Batch 504, accuracy/top5 = 0.88
I0817 10:58:22.490628 14586 caffe.cpp:313] Batch 504, loss = 1.38235
I0817 10:58:22.553316 14586 caffe.cpp:313] Batch 505, accuracy/top1 = 0.58
I0817 10:58:22.553339 14586 caffe.cpp:313] Batch 505, accuracy/top5 = 0.88
I0817 10:58:22.553342 14586 caffe.cpp:313] Batch 505, loss = 1.8062
I0817 10:58:22.615986 14586 caffe.cpp:313] Batch 506, accuracy/top1 = 0.66
I0817 10:58:22.616008 14586 caffe.cpp:313] Batch 506, accuracy/top5 = 0.86
I0817 10:58:22.616013 14586 caffe.cpp:313] Batch 506, loss = 1.33164
I0817 10:58:22.678632 14586 caffe.cpp:313] Batch 507, accuracy/top1 = 0.48
I0817 10:58:22.678655 14586 caffe.cpp:313] Batch 507, accuracy/top5 = 0.78
I0817 10:58:22.678659 14586 caffe.cpp:313] Batch 507, loss = 2.28912
I0817 10:58:22.741204 14586 caffe.cpp:313] Batch 508, accuracy/top1 = 0.64
I0817 10:58:22.741225 14586 caffe.cpp:313] Batch 508, accuracy/top5 = 0.9
I0817 10:58:22.741230 14586 caffe.cpp:313] Batch 508, loss = 1.81346
I0817 10:58:22.803983 14586 caffe.cpp:313] Batch 509, accuracy/top1 = 0.54
I0817 10:58:22.804004 14586 caffe.cpp:313] Batch 509, accuracy/top5 = 0.82
I0817 10:58:22.804008 14586 caffe.cpp:313] Batch 509, loss = 1.7712
I0817 10:58:22.867058 14586 caffe.cpp:313] Batch 510, accuracy/top1 = 0.54
I0817 10:58:22.867075 14586 caffe.cpp:313] Batch 510, accuracy/top5 = 0.78
I0817 10:58:22.867079 14586 caffe.cpp:313] Batch 510, loss = 1.87499
I0817 10:58:22.929795 14586 caffe.cpp:313] Batch 511, accuracy/top1 = 0.62
I0817 10:58:22.929816 14586 caffe.cpp:313] Batch 511, accuracy/top5 = 0.86
I0817 10:58:22.929819 14586 caffe.cpp:313] Batch 511, loss = 1.72642
I0817 10:58:22.992604 14586 caffe.cpp:313] Batch 512, accuracy/top1 = 0.66
I0817 10:58:22.992620 14586 caffe.cpp:313] Batch 512, accuracy/top5 = 0.86
I0817 10:58:22.992624 14586 caffe.cpp:313] Batch 512, loss = 1.43488
I0817 10:58:23.055330 14586 caffe.cpp:313] Batch 513, accuracy/top1 = 0.54
I0817 10:58:23.055348 14586 caffe.cpp:313] Batch 513, accuracy/top5 = 0.78
I0817 10:58:23.055351 14586 caffe.cpp:313] Batch 513, loss = 2.01205
I0817 10:58:23.117918 14586 caffe.cpp:313] Batch 514, accuracy/top1 = 0.54
I0817 10:58:23.117940 14586 caffe.cpp:313] Batch 514, accuracy/top5 = 0.8
I0817 10:58:23.117944 14586 caffe.cpp:313] Batch 514, loss = 1.85805
I0817 10:58:23.180717 14586 caffe.cpp:313] Batch 515, accuracy/top1 = 0.54
I0817 10:58:23.180739 14586 caffe.cpp:313] Batch 515, accuracy/top5 = 0.76
I0817 10:58:23.180743 14586 caffe.cpp:313] Batch 515, loss = 1.96289
I0817 10:58:23.243371 14586 caffe.cpp:313] Batch 516, accuracy/top1 = 0.62
I0817 10:58:23.243392 14586 caffe.cpp:313] Batch 516, accuracy/top5 = 0.82
I0817 10:58:23.243396 14586 caffe.cpp:313] Batch 516, loss = 2.08052
I0817 10:58:23.306107 14586 caffe.cpp:313] Batch 517, accuracy/top1 = 0.64
I0817 10:58:23.306128 14586 caffe.cpp:313] Batch 517, accuracy/top5 = 0.84
I0817 10:58:23.306133 14586 caffe.cpp:313] Batch 517, loss = 1.43032
I0817 10:58:23.368860 14586 caffe.cpp:313] Batch 518, accuracy/top1 = 0.66
I0817 10:58:23.368880 14586 caffe.cpp:313] Batch 518, accuracy/top5 = 0.82
I0817 10:58:23.368885 14586 caffe.cpp:313] Batch 518, loss = 1.79119
I0817 10:58:23.431547 14586 caffe.cpp:313] Batch 519, accuracy/top1 = 0.62
I0817 10:58:23.431569 14586 caffe.cpp:313] Batch 519, accuracy/top5 = 0.82
I0817 10:58:23.431573 14586 caffe.cpp:313] Batch 519, loss = 1.7433
I0817 10:58:23.494299 14586 caffe.cpp:313] Batch 520, accuracy/top1 = 0.54
I0817 10:58:23.494323 14586 caffe.cpp:313] Batch 520, accuracy/top5 = 0.76
I0817 10:58:23.494325 14586 caffe.cpp:313] Batch 520, loss = 2.112
I0817 10:58:23.557088 14586 caffe.cpp:313] Batch 521, accuracy/top1 = 0.56
I0817 10:58:23.557108 14586 caffe.cpp:313] Batch 521, accuracy/top5 = 0.84
I0817 10:58:23.557112 14586 caffe.cpp:313] Batch 521, loss = 1.74338
I0817 10:58:23.620127 14586 caffe.cpp:313] Batch 522, accuracy/top1 = 0.56
I0817 10:58:23.620152 14586 caffe.cpp:313] Batch 522, accuracy/top5 = 0.92
I0817 10:58:23.620157 14586 caffe.cpp:313] Batch 522, loss = 1.53932
I0817 10:58:23.683351 14586 caffe.cpp:313] Batch 523, accuracy/top1 = 0.62
I0817 10:58:23.683372 14586 caffe.cpp:313] Batch 523, accuracy/top5 = 0.88
I0817 10:58:23.683392 14586 caffe.cpp:313] Batch 523, loss = 1.4266
I0817 10:58:23.746096 14586 caffe.cpp:313] Batch 524, accuracy/top1 = 0.56
I0817 10:58:23.746119 14586 caffe.cpp:313] Batch 524, accuracy/top5 = 0.8
I0817 10:58:23.746122 14586 caffe.cpp:313] Batch 524, loss = 1.6986
I0817 10:58:23.808715 14586 caffe.cpp:313] Batch 525, accuracy/top1 = 0.64
I0817 10:58:23.808737 14586 caffe.cpp:313] Batch 525, accuracy/top5 = 0.82
I0817 10:58:23.808742 14586 caffe.cpp:313] Batch 525, loss = 1.75775
I0817 10:58:23.872192 14586 caffe.cpp:313] Batch 526, accuracy/top1 = 0.44
I0817 10:58:23.872210 14586 caffe.cpp:313] Batch 526, accuracy/top5 = 0.74
I0817 10:58:23.872213 14586 caffe.cpp:313] Batch 526, loss = 2.574
I0817 10:58:23.934784 14586 caffe.cpp:313] Batch 527, accuracy/top1 = 0.56
I0817 10:58:23.934806 14586 caffe.cpp:313] Batch 527, accuracy/top5 = 0.82
I0817 10:58:23.934810 14586 caffe.cpp:313] Batch 527, loss = 1.71257
I0817 10:58:23.997488 14586 caffe.cpp:313] Batch 528, accuracy/top1 = 0.68
I0817 10:58:23.997509 14586 caffe.cpp:313] Batch 528, accuracy/top5 = 0.84
I0817 10:58:23.997514 14586 caffe.cpp:313] Batch 528, loss = 1.5722
I0817 10:58:24.060210 14586 caffe.cpp:313] Batch 529, accuracy/top1 = 0.52
I0817 10:58:24.060232 14586 caffe.cpp:313] Batch 529, accuracy/top5 = 0.82
I0817 10:58:24.060236 14586 caffe.cpp:313] Batch 529, loss = 1.81266
I0817 10:58:24.122915 14586 caffe.cpp:313] Batch 530, accuracy/top1 = 0.58
I0817 10:58:24.122937 14586 caffe.cpp:313] Batch 530, accuracy/top5 = 0.82
I0817 10:58:24.122941 14586 caffe.cpp:313] Batch 530, loss = 1.72799
I0817 10:58:24.185570 14586 caffe.cpp:313] Batch 531, accuracy/top1 = 0.52
I0817 10:58:24.185591 14586 caffe.cpp:313] Batch 531, accuracy/top5 = 0.82
I0817 10:58:24.185595 14586 caffe.cpp:313] Batch 531, loss = 1.86934
I0817 10:58:24.248253 14586 caffe.cpp:313] Batch 532, accuracy/top1 = 0.56
I0817 10:58:24.248275 14586 caffe.cpp:313] Batch 532, accuracy/top5 = 0.9
I0817 10:58:24.248280 14586 caffe.cpp:313] Batch 532, loss = 1.50367
I0817 10:58:24.310909 14586 caffe.cpp:313] Batch 533, accuracy/top1 = 0.52
I0817 10:58:24.310930 14586 caffe.cpp:313] Batch 533, accuracy/top5 = 0.7
I0817 10:58:24.310933 14586 caffe.cpp:313] Batch 533, loss = 2.27857
I0817 10:58:24.373678 14586 caffe.cpp:313] Batch 534, accuracy/top1 = 0.64
I0817 10:58:24.373699 14586 caffe.cpp:313] Batch 534, accuracy/top5 = 0.76
I0817 10:58:24.373703 14586 caffe.cpp:313] Batch 534, loss = 1.57798
I0817 10:58:24.436399 14586 caffe.cpp:313] Batch 535, accuracy/top1 = 0.52
I0817 10:58:24.436421 14586 caffe.cpp:313] Batch 535, accuracy/top5 = 0.84
I0817 10:58:24.436425 14586 caffe.cpp:313] Batch 535, loss = 1.59736
I0817 10:58:24.499126 14586 caffe.cpp:313] Batch 536, accuracy/top1 = 0.66
I0817 10:58:24.499147 14586 caffe.cpp:313] Batch 536, accuracy/top5 = 0.86
I0817 10:58:24.499151 14586 caffe.cpp:313] Batch 536, loss = 1.2885
I0817 10:58:24.561915 14586 caffe.cpp:313] Batch 537, accuracy/top1 = 0.62
I0817 10:58:24.561936 14586 caffe.cpp:313] Batch 537, accuracy/top5 = 0.88
I0817 10:58:24.561940 14586 caffe.cpp:313] Batch 537, loss = 1.44175
I0817 10:58:24.624584 14586 caffe.cpp:313] Batch 538, accuracy/top1 = 0.54
I0817 10:58:24.624605 14586 caffe.cpp:313] Batch 538, accuracy/top5 = 0.8
I0817 10:58:24.624609 14586 caffe.cpp:313] Batch 538, loss = 1.79878
I0817 10:58:24.687273 14586 caffe.cpp:313] Batch 539, accuracy/top1 = 0.38
I0817 10:58:24.687295 14586 caffe.cpp:313] Batch 539, accuracy/top5 = 0.74
I0817 10:58:24.687299 14586 caffe.cpp:313] Batch 539, loss = 2.3626
I0817 10:58:24.750063 14586 caffe.cpp:313] Batch 540, accuracy/top1 = 0.56
I0817 10:58:24.750085 14586 caffe.cpp:313] Batch 540, accuracy/top5 = 0.76
I0817 10:58:24.750089 14586 caffe.cpp:313] Batch 540, loss = 1.89251
I0817 10:58:24.812841 14586 caffe.cpp:313] Batch 541, accuracy/top1 = 0.6
I0817 10:58:24.812863 14586 caffe.cpp:313] Batch 541, accuracy/top5 = 0.8
I0817 10:58:24.812867 14586 caffe.cpp:313] Batch 541, loss = 1.77949
I0817 10:58:24.875604 14586 caffe.cpp:313] Batch 542, accuracy/top1 = 0.6
I0817 10:58:24.875635 14586 caffe.cpp:313] Batch 542, accuracy/top5 = 0.78
I0817 10:58:24.875640 14586 caffe.cpp:313] Batch 542, loss = 1.98425
I0817 10:58:24.938215 14586 caffe.cpp:313] Batch 543, accuracy/top1 = 0.66
I0817 10:58:24.938237 14586 caffe.cpp:313] Batch 543, accuracy/top5 = 0.9
I0817 10:58:24.938241 14586 caffe.cpp:313] Batch 543, loss = 1.45344
I0817 10:58:25.000707 14586 caffe.cpp:313] Batch 544, accuracy/top1 = 0.74
I0817 10:58:25.000730 14586 caffe.cpp:313] Batch 544, accuracy/top5 = 0.84
I0817 10:58:25.000733 14586 caffe.cpp:313] Batch 544, loss = 1.48295
I0817 10:58:25.063432 14586 caffe.cpp:313] Batch 545, accuracy/top1 = 0.54
I0817 10:58:25.063453 14586 caffe.cpp:313] Batch 545, accuracy/top5 = 0.82
I0817 10:58:25.063458 14586 caffe.cpp:313] Batch 545, loss = 1.72931
I0817 10:58:25.126291 14586 caffe.cpp:313] Batch 546, accuracy/top1 = 0.68
I0817 10:58:25.126314 14586 caffe.cpp:313] Batch 546, accuracy/top5 = 0.88
I0817 10:58:25.126317 14586 caffe.cpp:313] Batch 546, loss = 1.70154
I0817 10:58:25.189069 14586 caffe.cpp:313] Batch 547, accuracy/top1 = 0.54
I0817 10:58:25.189092 14586 caffe.cpp:313] Batch 547, accuracy/top5 = 0.84
I0817 10:58:25.189096 14586 caffe.cpp:313] Batch 547, loss = 1.64711
I0817 10:58:25.251801 14586 caffe.cpp:313] Batch 548, accuracy/top1 = 0.56
I0817 10:58:25.251824 14586 caffe.cpp:313] Batch 548, accuracy/top5 = 0.76
I0817 10:58:25.251828 14586 caffe.cpp:313] Batch 548, loss = 2.20654
I0817 10:58:25.314677 14586 caffe.cpp:313] Batch 549, accuracy/top1 = 0.62
I0817 10:58:25.314699 14586 caffe.cpp:313] Batch 549, accuracy/top5 = 0.74
I0817 10:58:25.314703 14586 caffe.cpp:313] Batch 549, loss = 2.04438
I0817 10:58:25.377370 14586 caffe.cpp:313] Batch 550, accuracy/top1 = 0.54
I0817 10:58:25.377393 14586 caffe.cpp:313] Batch 550, accuracy/top5 = 0.82
I0817 10:58:25.377396 14586 caffe.cpp:313] Batch 550, loss = 1.79407
I0817 10:58:25.440105 14586 caffe.cpp:313] Batch 551, accuracy/top1 = 0.68
I0817 10:58:25.440127 14586 caffe.cpp:313] Batch 551, accuracy/top5 = 0.84
I0817 10:58:25.440135 14586 caffe.cpp:313] Batch 551, loss = 1.44803
I0817 10:58:25.502746 14586 caffe.cpp:313] Batch 552, accuracy/top1 = 0.64
I0817 10:58:25.502769 14586 caffe.cpp:313] Batch 552, accuracy/top5 = 0.8
I0817 10:58:25.502774 14586 caffe.cpp:313] Batch 552, loss = 1.60829
I0817 10:58:25.565397 14586 caffe.cpp:313] Batch 553, accuracy/top1 = 0.6
I0817 10:58:25.565419 14586 caffe.cpp:313] Batch 553, accuracy/top5 = 0.84
I0817 10:58:25.565423 14586 caffe.cpp:313] Batch 553, loss = 1.60118
I0817 10:58:25.628259 14586 caffe.cpp:313] Batch 554, accuracy/top1 = 0.6
I0817 10:58:25.628283 14586 caffe.cpp:313] Batch 554, accuracy/top5 = 0.86
I0817 10:58:25.628286 14586 caffe.cpp:313] Batch 554, loss = 1.76207
I0817 10:58:25.690973 14586 caffe.cpp:313] Batch 555, accuracy/top1 = 0.62
I0817 10:58:25.690991 14586 caffe.cpp:313] Batch 555, accuracy/top5 = 0.94
I0817 10:58:25.690995 14586 caffe.cpp:313] Batch 555, loss = 1.30685
I0817 10:58:25.753751 14586 caffe.cpp:313] Batch 556, accuracy/top1 = 0.58
I0817 10:58:25.753772 14586 caffe.cpp:313] Batch 556, accuracy/top5 = 0.78
I0817 10:58:25.753774 14586 caffe.cpp:313] Batch 556, loss = 1.7394
I0817 10:58:25.816949 14586 caffe.cpp:313] Batch 557, accuracy/top1 = 0.54
I0817 10:58:25.816967 14586 caffe.cpp:313] Batch 557, accuracy/top5 = 0.86
I0817 10:58:25.816970 14586 caffe.cpp:313] Batch 557, loss = 1.77948
I0817 10:58:25.879963 14586 caffe.cpp:313] Batch 558, accuracy/top1 = 0.46
I0817 10:58:25.879984 14586 caffe.cpp:313] Batch 558, accuracy/top5 = 0.76
I0817 10:58:25.879988 14586 caffe.cpp:313] Batch 558, loss = 2.18957
I0817 10:58:25.942772 14586 caffe.cpp:313] Batch 559, accuracy/top1 = 0.6
I0817 10:58:25.942795 14586 caffe.cpp:313] Batch 559, accuracy/top5 = 0.8
I0817 10:58:25.942798 14586 caffe.cpp:313] Batch 559, loss = 1.78758
I0817 10:58:26.005395 14586 caffe.cpp:313] Batch 560, accuracy/top1 = 0.66
I0817 10:58:26.005417 14586 caffe.cpp:313] Batch 560, accuracy/top5 = 0.82
I0817 10:58:26.005421 14586 caffe.cpp:313] Batch 560, loss = 1.75887
I0817 10:58:26.068178 14586 caffe.cpp:313] Batch 561, accuracy/top1 = 0.6
I0817 10:58:26.068213 14586 caffe.cpp:313] Batch 561, accuracy/top5 = 0.74
I0817 10:58:26.068218 14586 caffe.cpp:313] Batch 561, loss = 1.9914
I0817 10:58:26.130949 14586 caffe.cpp:313] Batch 562, accuracy/top1 = 0.56
I0817 10:58:26.130971 14586 caffe.cpp:313] Batch 562, accuracy/top5 = 0.82
I0817 10:58:26.130975 14586 caffe.cpp:313] Batch 562, loss = 1.73365
I0817 10:58:26.193610 14586 caffe.cpp:313] Batch 563, accuracy/top1 = 0.62
I0817 10:58:26.193634 14586 caffe.cpp:313] Batch 563, accuracy/top5 = 0.82
I0817 10:58:26.193637 14586 caffe.cpp:313] Batch 563, loss = 1.83974
I0817 10:58:26.256202 14586 caffe.cpp:313] Batch 564, accuracy/top1 = 0.64
I0817 10:58:26.256224 14586 caffe.cpp:313] Batch 564, accuracy/top5 = 0.84
I0817 10:58:26.256228 14586 caffe.cpp:313] Batch 564, loss = 1.69459
I0817 10:58:26.318850 14586 caffe.cpp:313] Batch 565, accuracy/top1 = 0.58
I0817 10:58:26.318872 14586 caffe.cpp:313] Batch 565, accuracy/top5 = 0.88
I0817 10:58:26.318876 14586 caffe.cpp:313] Batch 565, loss = 1.53811
I0817 10:58:26.381618 14586 caffe.cpp:313] Batch 566, accuracy/top1 = 0.6
I0817 10:58:26.381640 14586 caffe.cpp:313] Batch 566, accuracy/top5 = 0.76
I0817 10:58:26.381644 14586 caffe.cpp:313] Batch 566, loss = 1.94914
I0817 10:58:26.444398 14586 caffe.cpp:313] Batch 567, accuracy/top1 = 0.52
I0817 10:58:26.444420 14586 caffe.cpp:313] Batch 567, accuracy/top5 = 0.78
I0817 10:58:26.444424 14586 caffe.cpp:313] Batch 567, loss = 1.8808
I0817 10:58:26.507176 14586 caffe.cpp:313] Batch 568, accuracy/top1 = 0.48
I0817 10:58:26.507199 14586 caffe.cpp:313] Batch 568, accuracy/top5 = 0.7
I0817 10:58:26.507203 14586 caffe.cpp:313] Batch 568, loss = 2.00517
I0817 10:58:26.569856 14586 caffe.cpp:313] Batch 569, accuracy/top1 = 0.44
I0817 10:58:26.569878 14586 caffe.cpp:313] Batch 569, accuracy/top5 = 0.72
I0817 10:58:26.569882 14586 caffe.cpp:313] Batch 569, loss = 2.67274
I0817 10:58:26.632623 14586 caffe.cpp:313] Batch 570, accuracy/top1 = 0.66
I0817 10:58:26.632647 14586 caffe.cpp:313] Batch 570, accuracy/top5 = 0.82
I0817 10:58:26.632652 14586 caffe.cpp:313] Batch 570, loss = 1.63267
I0817 10:58:26.695475 14586 caffe.cpp:313] Batch 571, accuracy/top1 = 0.48
I0817 10:58:26.695497 14586 caffe.cpp:313] Batch 571, accuracy/top5 = 0.66
I0817 10:58:26.695502 14586 caffe.cpp:313] Batch 571, loss = 2.45995
I0817 10:58:26.758198 14586 caffe.cpp:313] Batch 572, accuracy/top1 = 0.46
I0817 10:58:26.758219 14586 caffe.cpp:313] Batch 572, accuracy/top5 = 0.84
I0817 10:58:26.758224 14586 caffe.cpp:313] Batch 572, loss = 1.84296
I0817 10:58:26.820838 14586 caffe.cpp:313] Batch 573, accuracy/top1 = 0.64
I0817 10:58:26.820860 14586 caffe.cpp:313] Batch 573, accuracy/top5 = 0.84
I0817 10:58:26.820864 14586 caffe.cpp:313] Batch 573, loss = 1.93866
I0817 10:58:26.883728 14586 caffe.cpp:313] Batch 574, accuracy/top1 = 0.56
I0817 10:58:26.883750 14586 caffe.cpp:313] Batch 574, accuracy/top5 = 0.76
I0817 10:58:26.883754 14586 caffe.cpp:313] Batch 574, loss = 2.12251
I0817 10:58:26.946369 14586 caffe.cpp:313] Batch 575, accuracy/top1 = 0.54
I0817 10:58:26.946391 14586 caffe.cpp:313] Batch 575, accuracy/top5 = 0.76
I0817 10:58:26.946395 14586 caffe.cpp:313] Batch 575, loss = 1.97651
I0817 10:58:27.009021 14586 caffe.cpp:313] Batch 576, accuracy/top1 = 0.74
I0817 10:58:27.009043 14586 caffe.cpp:313] Batch 576, accuracy/top5 = 0.88
I0817 10:58:27.009047 14586 caffe.cpp:313] Batch 576, loss = 1.15774
I0817 10:58:27.072412 14586 caffe.cpp:313] Batch 577, accuracy/top1 = 0.56
I0817 10:58:27.072430 14586 caffe.cpp:313] Batch 577, accuracy/top5 = 0.84
I0817 10:58:27.072434 14586 caffe.cpp:313] Batch 577, loss = 1.65472
I0817 10:58:27.135685 14586 caffe.cpp:313] Batch 578, accuracy/top1 = 0.6
I0817 10:58:27.135709 14586 caffe.cpp:313] Batch 578, accuracy/top5 = 0.8
I0817 10:58:27.135712 14586 caffe.cpp:313] Batch 578, loss = 1.96098
I0817 10:58:27.198290 14586 caffe.cpp:313] Batch 579, accuracy/top1 = 0.66
I0817 10:58:27.198312 14586 caffe.cpp:313] Batch 579, accuracy/top5 = 0.78
I0817 10:58:27.198315 14586 caffe.cpp:313] Batch 579, loss = 2.02181
I0817 10:58:27.261080 14586 caffe.cpp:313] Batch 580, accuracy/top1 = 0.66
I0817 10:58:27.261103 14586 caffe.cpp:313] Batch 580, accuracy/top5 = 0.88
I0817 10:58:27.261107 14586 caffe.cpp:313] Batch 580, loss = 1.58029
I0817 10:58:27.323753 14586 caffe.cpp:313] Batch 581, accuracy/top1 = 0.58
I0817 10:58:27.323776 14586 caffe.cpp:313] Batch 581, accuracy/top5 = 0.86
I0817 10:58:27.323779 14586 caffe.cpp:313] Batch 581, loss = 1.83409
I0817 10:58:27.386438 14586 caffe.cpp:313] Batch 582, accuracy/top1 = 0.58
I0817 10:58:27.386461 14586 caffe.cpp:313] Batch 582, accuracy/top5 = 0.84
I0817 10:58:27.386464 14586 caffe.cpp:313] Batch 582, loss = 1.52987
I0817 10:58:27.449024 14586 caffe.cpp:313] Batch 583, accuracy/top1 = 0.68
I0817 10:58:27.449046 14586 caffe.cpp:313] Batch 583, accuracy/top5 = 0.82
I0817 10:58:27.449050 14586 caffe.cpp:313] Batch 583, loss = 1.65678
I0817 10:58:27.511745 14586 caffe.cpp:313] Batch 584, accuracy/top1 = 0.6
I0817 10:58:27.511766 14586 caffe.cpp:313] Batch 584, accuracy/top5 = 0.9
I0817 10:58:27.511771 14586 caffe.cpp:313] Batch 584, loss = 1.27142
I0817 10:58:27.574322 14586 caffe.cpp:313] Batch 585, accuracy/top1 = 0.68
I0817 10:58:27.574344 14586 caffe.cpp:313] Batch 585, accuracy/top5 = 0.86
I0817 10:58:27.574348 14586 caffe.cpp:313] Batch 585, loss = 1.41057
I0817 10:58:27.636916 14586 caffe.cpp:313] Batch 586, accuracy/top1 = 0.56
I0817 10:58:27.636939 14586 caffe.cpp:313] Batch 586, accuracy/top5 = 0.82
I0817 10:58:27.636942 14586 caffe.cpp:313] Batch 586, loss = 1.73486
I0817 10:58:27.699549 14586 caffe.cpp:313] Batch 587, accuracy/top1 = 0.64
I0817 10:58:27.699571 14586 caffe.cpp:313] Batch 587, accuracy/top5 = 0.82
I0817 10:58:27.699574 14586 caffe.cpp:313] Batch 587, loss = 1.35506
I0817 10:58:27.762437 14586 caffe.cpp:313] Batch 588, accuracy/top1 = 0.66
I0817 10:58:27.762460 14586 caffe.cpp:313] Batch 588, accuracy/top5 = 0.86
I0817 10:58:27.762465 14586 caffe.cpp:313] Batch 588, loss = 1.44068
I0817 10:58:27.825074 14586 caffe.cpp:313] Batch 589, accuracy/top1 = 0.62
I0817 10:58:27.825095 14586 caffe.cpp:313] Batch 589, accuracy/top5 = 0.84
I0817 10:58:27.825099 14586 caffe.cpp:313] Batch 589, loss = 1.79632
I0817 10:58:27.888123 14586 caffe.cpp:313] Batch 590, accuracy/top1 = 0.6
I0817 10:58:27.888146 14586 caffe.cpp:313] Batch 590, accuracy/top5 = 0.8
I0817 10:58:27.888150 14586 caffe.cpp:313] Batch 590, loss = 1.97084
I0817 10:58:27.951061 14586 caffe.cpp:313] Batch 591, accuracy/top1 = 0.6
I0817 10:58:27.951082 14586 caffe.cpp:313] Batch 591, accuracy/top5 = 0.76
I0817 10:58:27.951086 14586 caffe.cpp:313] Batch 591, loss = 1.69853
I0817 10:58:28.013923 14586 caffe.cpp:313] Batch 592, accuracy/top1 = 0.62
I0817 10:58:28.013947 14586 caffe.cpp:313] Batch 592, accuracy/top5 = 0.84
I0817 10:58:28.013949 14586 caffe.cpp:313] Batch 592, loss = 1.55484
I0817 10:58:28.076783 14586 caffe.cpp:313] Batch 593, accuracy/top1 = 0.48
I0817 10:58:28.076807 14586 caffe.cpp:313] Batch 593, accuracy/top5 = 0.76
I0817 10:58:28.076810 14586 caffe.cpp:313] Batch 593, loss = 2.31587
I0817 10:58:28.139503 14586 caffe.cpp:313] Batch 594, accuracy/top1 = 0.54
I0817 10:58:28.139524 14586 caffe.cpp:313] Batch 594, accuracy/top5 = 0.82
I0817 10:58:28.139529 14586 caffe.cpp:313] Batch 594, loss = 1.85398
I0817 10:58:28.202270 14586 caffe.cpp:313] Batch 595, accuracy/top1 = 0.68
I0817 10:58:28.202291 14586 caffe.cpp:313] Batch 595, accuracy/top5 = 0.9
I0817 10:58:28.202296 14586 caffe.cpp:313] Batch 595, loss = 1.51907
I0817 10:58:28.265072 14586 caffe.cpp:313] Batch 596, accuracy/top1 = 0.54
I0817 10:58:28.265095 14586 caffe.cpp:313] Batch 596, accuracy/top5 = 0.7
I0817 10:58:28.265100 14586 caffe.cpp:313] Batch 596, loss = 2.31156
I0817 10:58:28.327881 14586 caffe.cpp:313] Batch 597, accuracy/top1 = 0.72
I0817 10:58:28.327904 14586 caffe.cpp:313] Batch 597, accuracy/top5 = 0.88
I0817 10:58:28.327908 14586 caffe.cpp:313] Batch 597, loss = 1.22793
I0817 10:58:28.390630 14586 caffe.cpp:313] Batch 598, accuracy/top1 = 0.44
I0817 10:58:28.390653 14586 caffe.cpp:313] Batch 598, accuracy/top5 = 0.76
I0817 10:58:28.390674 14586 caffe.cpp:313] Batch 598, loss = 2.27008
I0817 10:58:28.453425 14586 caffe.cpp:313] Batch 599, accuracy/top1 = 0.6
I0817 10:58:28.453447 14586 caffe.cpp:313] Batch 599, accuracy/top5 = 0.76
I0817 10:58:28.453451 14586 caffe.cpp:313] Batch 599, loss = 1.9677
I0817 10:58:28.516160 14586 caffe.cpp:313] Batch 600, accuracy/top1 = 0.6
I0817 10:58:28.516183 14586 caffe.cpp:313] Batch 600, accuracy/top5 = 0.78
I0817 10:58:28.516187 14586 caffe.cpp:313] Batch 600, loss = 1.82956
I0817 10:58:28.578964 14586 caffe.cpp:313] Batch 601, accuracy/top1 = 0.56
I0817 10:58:28.578987 14586 caffe.cpp:313] Batch 601, accuracy/top5 = 0.9
I0817 10:58:28.578991 14586 caffe.cpp:313] Batch 601, loss = 1.47878
I0817 10:58:28.641772 14586 caffe.cpp:313] Batch 602, accuracy/top1 = 0.5
I0817 10:58:28.641794 14586 caffe.cpp:313] Batch 602, accuracy/top5 = 0.76
I0817 10:58:28.641798 14586 caffe.cpp:313] Batch 602, loss = 2.27757
I0817 10:58:28.704658 14586 caffe.cpp:313] Batch 603, accuracy/top1 = 0.66
I0817 10:58:28.704679 14586 caffe.cpp:313] Batch 603, accuracy/top5 = 0.8
I0817 10:58:28.704684 14586 caffe.cpp:313] Batch 603, loss = 1.3641
I0817 10:58:28.767469 14586 caffe.cpp:313] Batch 604, accuracy/top1 = 0.74
I0817 10:58:28.767490 14586 caffe.cpp:313] Batch 604, accuracy/top5 = 0.96
I0817 10:58:28.767494 14586 caffe.cpp:313] Batch 604, loss = 0.990387
I0817 10:58:28.830351 14586 caffe.cpp:313] Batch 605, accuracy/top1 = 0.52
I0817 10:58:28.830370 14586 caffe.cpp:313] Batch 605, accuracy/top5 = 0.7
I0817 10:58:28.830374 14586 caffe.cpp:313] Batch 605, loss = 2.18136
I0817 10:58:28.893249 14586 caffe.cpp:313] Batch 606, accuracy/top1 = 0.62
I0817 10:58:28.893270 14586 caffe.cpp:313] Batch 606, accuracy/top5 = 0.84
I0817 10:58:28.893273 14586 caffe.cpp:313] Batch 606, loss = 1.80077
I0817 10:58:28.955920 14586 caffe.cpp:313] Batch 607, accuracy/top1 = 0.52
I0817 10:58:28.955943 14586 caffe.cpp:313] Batch 607, accuracy/top5 = 0.78
I0817 10:58:28.955946 14586 caffe.cpp:313] Batch 607, loss = 1.8709
I0817 10:58:29.018703 14586 caffe.cpp:313] Batch 608, accuracy/top1 = 0.6
I0817 10:58:29.018725 14586 caffe.cpp:313] Batch 608, accuracy/top5 = 0.82
I0817 10:58:29.018729 14586 caffe.cpp:313] Batch 608, loss = 1.63041
I0817 10:58:29.081651 14586 caffe.cpp:313] Batch 609, accuracy/top1 = 0.62
I0817 10:58:29.081668 14586 caffe.cpp:313] Batch 609, accuracy/top5 = 0.8
I0817 10:58:29.081672 14586 caffe.cpp:313] Batch 609, loss = 1.9754
I0817 10:58:29.144412 14586 caffe.cpp:313] Batch 610, accuracy/top1 = 0.54
I0817 10:58:29.144434 14586 caffe.cpp:313] Batch 610, accuracy/top5 = 0.74
I0817 10:58:29.144438 14586 caffe.cpp:313] Batch 610, loss = 2.01296
I0817 10:58:29.207161 14586 caffe.cpp:313] Batch 611, accuracy/top1 = 0.64
I0817 10:58:29.207182 14586 caffe.cpp:313] Batch 611, accuracy/top5 = 0.8
I0817 10:58:29.207186 14586 caffe.cpp:313] Batch 611, loss = 1.7056
I0817 10:58:29.269942 14586 caffe.cpp:313] Batch 612, accuracy/top1 = 0.64
I0817 10:58:29.269965 14586 caffe.cpp:313] Batch 612, accuracy/top5 = 0.82
I0817 10:58:29.269969 14586 caffe.cpp:313] Batch 612, loss = 1.46691
I0817 10:58:29.332542 14586 caffe.cpp:313] Batch 613, accuracy/top1 = 0.58
I0817 10:58:29.332564 14586 caffe.cpp:313] Batch 613, accuracy/top5 = 0.76
I0817 10:58:29.332568 14586 caffe.cpp:313] Batch 613, loss = 2.11951
I0817 10:58:29.395156 14586 caffe.cpp:313] Batch 614, accuracy/top1 = 0.44
I0817 10:58:29.395179 14586 caffe.cpp:313] Batch 614, accuracy/top5 = 0.78
I0817 10:58:29.395182 14586 caffe.cpp:313] Batch 614, loss = 2.1755
I0817 10:58:29.457801 14586 caffe.cpp:313] Batch 615, accuracy/top1 = 0.42
I0817 10:58:29.457823 14586 caffe.cpp:313] Batch 615, accuracy/top5 = 0.78
I0817 10:58:29.457828 14586 caffe.cpp:313] Batch 615, loss = 2.21821
I0817 10:58:29.520546 14586 caffe.cpp:313] Batch 616, accuracy/top1 = 0.58
I0817 10:58:29.520570 14586 caffe.cpp:313] Batch 616, accuracy/top5 = 0.86
I0817 10:58:29.520573 14586 caffe.cpp:313] Batch 616, loss = 1.85001
I0817 10:58:29.583236 14586 caffe.cpp:313] Batch 617, accuracy/top1 = 0.58
I0817 10:58:29.583272 14586 caffe.cpp:313] Batch 617, accuracy/top5 = 0.78
I0817 10:58:29.583277 14586 caffe.cpp:313] Batch 617, loss = 1.93651
I0817 10:58:29.645922 14586 caffe.cpp:313] Batch 618, accuracy/top1 = 0.5
I0817 10:58:29.645946 14586 caffe.cpp:313] Batch 618, accuracy/top5 = 0.82
I0817 10:58:29.645951 14586 caffe.cpp:313] Batch 618, loss = 2.00655
I0817 10:58:29.708658 14586 caffe.cpp:313] Batch 619, accuracy/top1 = 0.7
I0817 10:58:29.708680 14586 caffe.cpp:313] Batch 619, accuracy/top5 = 0.84
I0817 10:58:29.708684 14586 caffe.cpp:313] Batch 619, loss = 1.29993
I0817 10:58:29.771409 14586 caffe.cpp:313] Batch 620, accuracy/top1 = 0.5
I0817 10:58:29.771431 14586 caffe.cpp:313] Batch 620, accuracy/top5 = 0.78
I0817 10:58:29.771435 14586 caffe.cpp:313] Batch 620, loss = 1.83845
I0817 10:58:29.834281 14586 caffe.cpp:313] Batch 621, accuracy/top1 = 0.56
I0817 10:58:29.834306 14586 caffe.cpp:313] Batch 621, accuracy/top5 = 0.84
I0817 10:58:29.834309 14586 caffe.cpp:313] Batch 621, loss = 1.89689
I0817 10:58:29.897140 14586 caffe.cpp:313] Batch 622, accuracy/top1 = 0.56
I0817 10:58:29.897161 14586 caffe.cpp:313] Batch 622, accuracy/top5 = 0.78
I0817 10:58:29.897164 14586 caffe.cpp:313] Batch 622, loss = 2.06681
I0817 10:58:29.959921 14586 caffe.cpp:313] Batch 623, accuracy/top1 = 0.52
I0817 10:58:29.959940 14586 caffe.cpp:313] Batch 623, accuracy/top5 = 0.82
I0817 10:58:29.959944 14586 caffe.cpp:313] Batch 623, loss = 1.98496
I0817 10:58:30.023025 14586 caffe.cpp:313] Batch 624, accuracy/top1 = 0.58
I0817 10:58:30.023043 14586 caffe.cpp:313] Batch 624, accuracy/top5 = 0.82
I0817 10:58:30.023047 14586 caffe.cpp:313] Batch 624, loss = 1.80896
I0817 10:58:30.085819 14586 caffe.cpp:313] Batch 625, accuracy/top1 = 0.5
I0817 10:58:30.085841 14586 caffe.cpp:313] Batch 625, accuracy/top5 = 0.78
I0817 10:58:30.085845 14586 caffe.cpp:313] Batch 625, loss = 2.3652
I0817 10:58:30.148617 14586 caffe.cpp:313] Batch 626, accuracy/top1 = 0.38
I0817 10:58:30.148638 14586 caffe.cpp:313] Batch 626, accuracy/top5 = 0.68
I0817 10:58:30.148643 14586 caffe.cpp:313] Batch 626, loss = 2.80917
I0817 10:58:30.211367 14586 caffe.cpp:313] Batch 627, accuracy/top1 = 0.46
I0817 10:58:30.211390 14586 caffe.cpp:313] Batch 627, accuracy/top5 = 0.78
I0817 10:58:30.211393 14586 caffe.cpp:313] Batch 627, loss = 2.26262
I0817 10:58:30.274076 14586 caffe.cpp:313] Batch 628, accuracy/top1 = 0.54
I0817 10:58:30.274099 14586 caffe.cpp:313] Batch 628, accuracy/top5 = 0.84
I0817 10:58:30.274103 14586 caffe.cpp:313] Batch 628, loss = 2.05987
I0817 10:58:30.336906 14586 caffe.cpp:313] Batch 629, accuracy/top1 = 0.56
I0817 10:58:30.336928 14586 caffe.cpp:313] Batch 629, accuracy/top5 = 0.72
I0817 10:58:30.336932 14586 caffe.cpp:313] Batch 629, loss = 2.06524
I0817 10:58:30.399657 14586 caffe.cpp:313] Batch 630, accuracy/top1 = 0.42
I0817 10:58:30.399679 14586 caffe.cpp:313] Batch 630, accuracy/top5 = 0.64
I0817 10:58:30.399683 14586 caffe.cpp:313] Batch 630, loss = 2.49964
I0817 10:58:30.462502 14586 caffe.cpp:313] Batch 631, accuracy/top1 = 0.7
I0817 10:58:30.462524 14586 caffe.cpp:313] Batch 631, accuracy/top5 = 0.88
I0817 10:58:30.462528 14586 caffe.cpp:313] Batch 631, loss = 1.25411
I0817 10:58:30.525362 14586 caffe.cpp:313] Batch 632, accuracy/top1 = 0.56
I0817 10:58:30.525385 14586 caffe.cpp:313] Batch 632, accuracy/top5 = 0.88
I0817 10:58:30.525389 14586 caffe.cpp:313] Batch 632, loss = 1.607
I0817 10:58:30.588162 14586 caffe.cpp:313] Batch 633, accuracy/top1 = 0.62
I0817 10:58:30.588186 14586 caffe.cpp:313] Batch 633, accuracy/top5 = 0.72
I0817 10:58:30.588189 14586 caffe.cpp:313] Batch 633, loss = 2.03886
I0817 10:58:30.650934 14586 caffe.cpp:313] Batch 634, accuracy/top1 = 0.58
I0817 10:58:30.650956 14586 caffe.cpp:313] Batch 634, accuracy/top5 = 0.72
I0817 10:58:30.650960 14586 caffe.cpp:313] Batch 634, loss = 2.28378
I0817 10:58:30.713716 14586 caffe.cpp:313] Batch 635, accuracy/top1 = 0.6
I0817 10:58:30.713738 14586 caffe.cpp:313] Batch 635, accuracy/top5 = 0.8
I0817 10:58:30.713742 14586 caffe.cpp:313] Batch 635, loss = 1.85911
I0817 10:58:30.776679 14586 caffe.cpp:313] Batch 636, accuracy/top1 = 0.56
I0817 10:58:30.776713 14586 caffe.cpp:313] Batch 636, accuracy/top5 = 0.78
I0817 10:58:30.776718 14586 caffe.cpp:313] Batch 636, loss = 2.09445
I0817 10:58:30.839733 14586 caffe.cpp:313] Batch 637, accuracy/top1 = 0.54
I0817 10:58:30.839753 14586 caffe.cpp:313] Batch 637, accuracy/top5 = 0.88
I0817 10:58:30.839757 14586 caffe.cpp:313] Batch 637, loss = 1.55365
I0817 10:58:30.902585 14586 caffe.cpp:313] Batch 638, accuracy/top1 = 0.58
I0817 10:58:30.902606 14586 caffe.cpp:313] Batch 638, accuracy/top5 = 0.82
I0817 10:58:30.902609 14586 caffe.cpp:313] Batch 638, loss = 1.69698
I0817 10:58:30.965324 14586 caffe.cpp:313] Batch 639, accuracy/top1 = 0.56
I0817 10:58:30.965345 14586 caffe.cpp:313] Batch 639, accuracy/top5 = 0.84
I0817 10:58:30.965349 14586 caffe.cpp:313] Batch 639, loss = 1.66447
I0817 10:58:31.028105 14586 caffe.cpp:313] Batch 640, accuracy/top1 = 0.52
I0817 10:58:31.028122 14586 caffe.cpp:313] Batch 640, accuracy/top5 = 0.82
I0817 10:58:31.028126 14586 caffe.cpp:313] Batch 640, loss = 1.86724
I0817 10:58:31.090746 14586 caffe.cpp:313] Batch 641, accuracy/top1 = 0.52
I0817 10:58:31.090768 14586 caffe.cpp:313] Batch 641, accuracy/top5 = 0.84
I0817 10:58:31.090771 14586 caffe.cpp:313] Batch 641, loss = 1.69844
I0817 10:58:31.154160 14586 caffe.cpp:313] Batch 642, accuracy/top1 = 0.64
I0817 10:58:31.154182 14586 caffe.cpp:313] Batch 642, accuracy/top5 = 0.78
I0817 10:58:31.154186 14586 caffe.cpp:313] Batch 642, loss = 1.85336
I0817 10:58:31.216852 14586 caffe.cpp:313] Batch 643, accuracy/top1 = 0.52
I0817 10:58:31.216874 14586 caffe.cpp:313] Batch 643, accuracy/top5 = 0.82
I0817 10:58:31.216878 14586 caffe.cpp:313] Batch 643, loss = 1.6813
I0817 10:58:31.279614 14586 caffe.cpp:313] Batch 644, accuracy/top1 = 0.54
I0817 10:58:31.279635 14586 caffe.cpp:313] Batch 644, accuracy/top5 = 0.72
I0817 10:58:31.279639 14586 caffe.cpp:313] Batch 644, loss = 1.99881
I0817 10:58:31.342324 14586 caffe.cpp:313] Batch 645, accuracy/top1 = 0.52
I0817 10:58:31.342346 14586 caffe.cpp:313] Batch 645, accuracy/top5 = 0.86
I0817 10:58:31.342350 14586 caffe.cpp:313] Batch 645, loss = 1.66467
I0817 10:58:31.405009 14586 caffe.cpp:313] Batch 646, accuracy/top1 = 0.6
I0817 10:58:31.405030 14586 caffe.cpp:313] Batch 646, accuracy/top5 = 0.86
I0817 10:58:31.405035 14586 caffe.cpp:313] Batch 646, loss = 1.54219
I0817 10:58:31.467686 14586 caffe.cpp:313] Batch 647, accuracy/top1 = 0.5
I0817 10:58:31.467710 14586 caffe.cpp:313] Batch 647, accuracy/top5 = 0.76
I0817 10:58:31.467713 14586 caffe.cpp:313] Batch 647, loss = 2.23332
I0817 10:58:31.530344 14586 caffe.cpp:313] Batch 648, accuracy/top1 = 0.6
I0817 10:58:31.530366 14586 caffe.cpp:313] Batch 648, accuracy/top5 = 0.82
I0817 10:58:31.530370 14586 caffe.cpp:313] Batch 648, loss = 1.4735
I0817 10:58:31.593092 14586 caffe.cpp:313] Batch 649, accuracy/top1 = 0.62
I0817 10:58:31.593116 14586 caffe.cpp:313] Batch 649, accuracy/top5 = 0.76
I0817 10:58:31.593119 14586 caffe.cpp:313] Batch 649, loss = 1.81963
I0817 10:58:31.655910 14586 caffe.cpp:313] Batch 650, accuracy/top1 = 0.56
I0817 10:58:31.655932 14586 caffe.cpp:313] Batch 650, accuracy/top5 = 0.74
I0817 10:58:31.655936 14586 caffe.cpp:313] Batch 650, loss = 1.67131
I0817 10:58:31.718688 14586 caffe.cpp:313] Batch 651, accuracy/top1 = 0.5
I0817 10:58:31.718711 14586 caffe.cpp:313] Batch 651, accuracy/top5 = 0.7
I0817 10:58:31.718715 14586 caffe.cpp:313] Batch 651, loss = 2.3341
I0817 10:58:31.781533 14586 caffe.cpp:313] Batch 652, accuracy/top1 = 0.54
I0817 10:58:31.781556 14586 caffe.cpp:313] Batch 652, accuracy/top5 = 0.74
I0817 10:58:31.781560 14586 caffe.cpp:313] Batch 652, loss = 2.18483
I0817 10:58:31.844959 14586 caffe.cpp:313] Batch 653, accuracy/top1 = 0.34
I0817 10:58:31.844983 14586 caffe.cpp:313] Batch 653, accuracy/top5 = 0.78
I0817 10:58:31.844987 14586 caffe.cpp:313] Batch 653, loss = 2.13586
I0817 10:58:31.907620 14586 caffe.cpp:313] Batch 654, accuracy/top1 = 0.54
I0817 10:58:31.907642 14586 caffe.cpp:313] Batch 654, accuracy/top5 = 0.86
I0817 10:58:31.907647 14586 caffe.cpp:313] Batch 654, loss = 1.87914
I0817 10:58:31.970286 14586 caffe.cpp:313] Batch 655, accuracy/top1 = 0.66
I0817 10:58:31.970309 14586 caffe.cpp:313] Batch 655, accuracy/top5 = 0.82
I0817 10:58:31.970312 14586 caffe.cpp:313] Batch 655, loss = 1.42597
I0817 10:58:32.032920 14586 caffe.cpp:313] Batch 656, accuracy/top1 = 0.62
I0817 10:58:32.032938 14586 caffe.cpp:313] Batch 656, accuracy/top5 = 0.9
I0817 10:58:32.032943 14586 caffe.cpp:313] Batch 656, loss = 1.51146
I0817 10:58:32.095767 14586 caffe.cpp:313] Batch 657, accuracy/top1 = 0.6
I0817 10:58:32.095787 14586 caffe.cpp:313] Batch 657, accuracy/top5 = 0.74
I0817 10:58:32.095790 14586 caffe.cpp:313] Batch 657, loss = 1.83087
I0817 10:58:32.158501 14586 caffe.cpp:313] Batch 658, accuracy/top1 = 0.48
I0817 10:58:32.158520 14586 caffe.cpp:313] Batch 658, accuracy/top5 = 0.78
I0817 10:58:32.158524 14586 caffe.cpp:313] Batch 658, loss = 1.93068
I0817 10:58:32.221310 14586 caffe.cpp:313] Batch 659, accuracy/top1 = 0.54
I0817 10:58:32.221333 14586 caffe.cpp:313] Batch 659, accuracy/top5 = 0.76
I0817 10:58:32.221336 14586 caffe.cpp:313] Batch 659, loss = 2.0243
I0817 10:58:32.284039 14586 caffe.cpp:313] Batch 660, accuracy/top1 = 0.7
I0817 10:58:32.284060 14586 caffe.cpp:313] Batch 660, accuracy/top5 = 0.78
I0817 10:58:32.284065 14586 caffe.cpp:313] Batch 660, loss = 1.59293
I0817 10:58:32.346666 14586 caffe.cpp:313] Batch 661, accuracy/top1 = 0.62
I0817 10:58:32.346689 14586 caffe.cpp:313] Batch 661, accuracy/top5 = 0.84
I0817 10:58:32.346693 14586 caffe.cpp:313] Batch 661, loss = 1.53237
I0817 10:58:32.409291 14586 caffe.cpp:313] Batch 662, accuracy/top1 = 0.48
I0817 10:58:32.409314 14586 caffe.cpp:313] Batch 662, accuracy/top5 = 0.74
I0817 10:58:32.409318 14586 caffe.cpp:313] Batch 662, loss = 2.05245
I0817 10:58:32.471837 14586 caffe.cpp:313] Batch 663, accuracy/top1 = 0.64
I0817 10:58:32.471858 14586 caffe.cpp:313] Batch 663, accuracy/top5 = 0.74
I0817 10:58:32.471863 14586 caffe.cpp:313] Batch 663, loss = 2.09809
I0817 10:58:32.534518 14586 caffe.cpp:313] Batch 664, accuracy/top1 = 0.5
I0817 10:58:32.534540 14586 caffe.cpp:313] Batch 664, accuracy/top5 = 0.8
I0817 10:58:32.534544 14586 caffe.cpp:313] Batch 664, loss = 1.95157
I0817 10:58:32.597240 14586 caffe.cpp:313] Batch 665, accuracy/top1 = 0.54
I0817 10:58:32.597262 14586 caffe.cpp:313] Batch 665, accuracy/top5 = 0.72
I0817 10:58:32.597266 14586 caffe.cpp:313] Batch 665, loss = 1.92019
I0817 10:58:32.660065 14586 caffe.cpp:313] Batch 666, accuracy/top1 = 0.62
I0817 10:58:32.660086 14586 caffe.cpp:313] Batch 666, accuracy/top5 = 0.82
I0817 10:58:32.660090 14586 caffe.cpp:313] Batch 666, loss = 1.70118
I0817 10:58:32.722771 14586 caffe.cpp:313] Batch 667, accuracy/top1 = 0.6
I0817 10:58:32.722795 14586 caffe.cpp:313] Batch 667, accuracy/top5 = 0.8
I0817 10:58:32.722800 14586 caffe.cpp:313] Batch 667, loss = 1.85198
I0817 10:58:32.785503 14586 caffe.cpp:313] Batch 668, accuracy/top1 = 0.52
I0817 10:58:32.785526 14586 caffe.cpp:313] Batch 668, accuracy/top5 = 0.76
I0817 10:58:32.785529 14586 caffe.cpp:313] Batch 668, loss = 2.01692
I0817 10:58:32.848457 14586 caffe.cpp:313] Batch 669, accuracy/top1 = 0.7
I0817 10:58:32.848475 14586 caffe.cpp:313] Batch 669, accuracy/top5 = 0.92
I0817 10:58:32.848479 14586 caffe.cpp:313] Batch 669, loss = 1.19673
I0817 10:58:32.911316 14586 caffe.cpp:313] Batch 670, accuracy/top1 = 0.6
I0817 10:58:32.911339 14586 caffe.cpp:313] Batch 670, accuracy/top5 = 0.78
I0817 10:58:32.911343 14586 caffe.cpp:313] Batch 670, loss = 1.87735
I0817 10:58:32.974017 14586 caffe.cpp:313] Batch 671, accuracy/top1 = 0.62
I0817 10:58:32.974040 14586 caffe.cpp:313] Batch 671, accuracy/top5 = 0.82
I0817 10:58:32.974043 14586 caffe.cpp:313] Batch 671, loss = 1.64302
I0817 10:58:33.036844 14586 caffe.cpp:313] Batch 672, accuracy/top1 = 0.48
I0817 10:58:33.036861 14586 caffe.cpp:313] Batch 672, accuracy/top5 = 0.86
I0817 10:58:33.036865 14586 caffe.cpp:313] Batch 672, loss = 2.0517
I0817 10:58:33.099575 14586 caffe.cpp:313] Batch 673, accuracy/top1 = 0.68
I0817 10:58:33.099597 14586 caffe.cpp:313] Batch 673, accuracy/top5 = 0.86
I0817 10:58:33.099618 14586 caffe.cpp:313] Batch 673, loss = 1.29738
I0817 10:58:33.162446 14586 caffe.cpp:313] Batch 674, accuracy/top1 = 0.7
I0817 10:58:33.162464 14586 caffe.cpp:313] Batch 674, accuracy/top5 = 0.92
I0817 10:58:33.162467 14586 caffe.cpp:313] Batch 674, loss = 1.09455
I0817 10:58:33.225029 14586 caffe.cpp:313] Batch 675, accuracy/top1 = 0.56
I0817 10:58:33.225051 14586 caffe.cpp:313] Batch 675, accuracy/top5 = 0.74
I0817 10:58:33.225055 14586 caffe.cpp:313] Batch 675, loss = 1.99795
I0817 10:58:33.287672 14586 caffe.cpp:313] Batch 676, accuracy/top1 = 0.48
I0817 10:58:33.287694 14586 caffe.cpp:313] Batch 676, accuracy/top5 = 0.78
I0817 10:58:33.287698 14586 caffe.cpp:313] Batch 676, loss = 1.86177
I0817 10:58:33.350322 14586 caffe.cpp:313] Batch 677, accuracy/top1 = 0.6
I0817 10:58:33.350345 14586 caffe.cpp:313] Batch 677, accuracy/top5 = 0.86
I0817 10:58:33.350349 14586 caffe.cpp:313] Batch 677, loss = 1.55427
I0817 10:58:33.413034 14586 caffe.cpp:313] Batch 678, accuracy/top1 = 0.5
I0817 10:58:33.413056 14586 caffe.cpp:313] Batch 678, accuracy/top5 = 0.82
I0817 10:58:33.413060 14586 caffe.cpp:313] Batch 678, loss = 2.00256
I0817 10:58:33.475847 14586 caffe.cpp:313] Batch 679, accuracy/top1 = 0.66
I0817 10:58:33.475870 14586 caffe.cpp:313] Batch 679, accuracy/top5 = 0.82
I0817 10:58:33.475874 14586 caffe.cpp:313] Batch 679, loss = 1.60097
I0817 10:58:33.538413 14586 caffe.cpp:313] Batch 680, accuracy/top1 = 0.58
I0817 10:58:33.538435 14586 caffe.cpp:313] Batch 680, accuracy/top5 = 0.88
I0817 10:58:33.538439 14586 caffe.cpp:313] Batch 680, loss = 1.47023
I0817 10:58:33.601060 14586 caffe.cpp:313] Batch 681, accuracy/top1 = 0.66
I0817 10:58:33.601083 14586 caffe.cpp:313] Batch 681, accuracy/top5 = 0.88
I0817 10:58:33.601086 14586 caffe.cpp:313] Batch 681, loss = 1.52075
I0817 10:58:33.663825 14586 caffe.cpp:313] Batch 682, accuracy/top1 = 0.54
I0817 10:58:33.663847 14586 caffe.cpp:313] Batch 682, accuracy/top5 = 0.84
I0817 10:58:33.663851 14586 caffe.cpp:313] Batch 682, loss = 1.95123
I0817 10:58:33.726523 14586 caffe.cpp:313] Batch 683, accuracy/top1 = 0.5
I0817 10:58:33.726546 14586 caffe.cpp:313] Batch 683, accuracy/top5 = 0.8
I0817 10:58:33.726549 14586 caffe.cpp:313] Batch 683, loss = 2.19013
I0817 10:58:33.789193 14586 caffe.cpp:313] Batch 684, accuracy/top1 = 0.52
I0817 10:58:33.789216 14586 caffe.cpp:313] Batch 684, accuracy/top5 = 0.76
I0817 10:58:33.789221 14586 caffe.cpp:313] Batch 684, loss = 2.00688
I0817 10:58:33.852056 14586 caffe.cpp:313] Batch 685, accuracy/top1 = 0.56
I0817 10:58:33.852074 14586 caffe.cpp:313] Batch 685, accuracy/top5 = 0.86
I0817 10:58:33.852078 14586 caffe.cpp:313] Batch 685, loss = 2.01172
I0817 10:58:33.914927 14586 caffe.cpp:313] Batch 686, accuracy/top1 = 0.5
I0817 10:58:33.914947 14586 caffe.cpp:313] Batch 686, accuracy/top5 = 0.72
I0817 10:58:33.914952 14586 caffe.cpp:313] Batch 686, loss = 2.05612
I0817 10:58:33.977780 14586 caffe.cpp:313] Batch 687, accuracy/top1 = 0.5
I0817 10:58:33.977803 14586 caffe.cpp:313] Batch 687, accuracy/top5 = 0.8
I0817 10:58:33.977807 14586 caffe.cpp:313] Batch 687, loss = 2.37729
I0817 10:58:34.040442 14586 caffe.cpp:313] Batch 688, accuracy/top1 = 0.56
I0817 10:58:34.040462 14586 caffe.cpp:313] Batch 688, accuracy/top5 = 0.86
I0817 10:58:34.040465 14586 caffe.cpp:313] Batch 688, loss = 1.78014
I0817 10:58:34.103332 14586 caffe.cpp:313] Batch 689, accuracy/top1 = 0.58
I0817 10:58:34.103353 14586 caffe.cpp:313] Batch 689, accuracy/top5 = 0.82
I0817 10:58:34.103358 14586 caffe.cpp:313] Batch 689, loss = 1.70153
I0817 10:58:34.165987 14586 caffe.cpp:313] Batch 690, accuracy/top1 = 0.48
I0817 10:58:34.166009 14586 caffe.cpp:313] Batch 690, accuracy/top5 = 0.76
I0817 10:58:34.166013 14586 caffe.cpp:313] Batch 690, loss = 1.99547
I0817 10:58:34.228726 14586 caffe.cpp:313] Batch 691, accuracy/top1 = 0.6
I0817 10:58:34.228746 14586 caffe.cpp:313] Batch 691, accuracy/top5 = 0.76
I0817 10:58:34.228751 14586 caffe.cpp:313] Batch 691, loss = 1.88191
I0817 10:58:34.291612 14586 caffe.cpp:313] Batch 692, accuracy/top1 = 0.58
I0817 10:58:34.291646 14586 caffe.cpp:313] Batch 692, accuracy/top5 = 0.78
I0817 10:58:34.291651 14586 caffe.cpp:313] Batch 692, loss = 2.05025
I0817 10:58:34.354424 14586 caffe.cpp:313] Batch 693, accuracy/top1 = 0.7
I0817 10:58:34.354447 14586 caffe.cpp:313] Batch 693, accuracy/top5 = 0.82
I0817 10:58:34.354451 14586 caffe.cpp:313] Batch 693, loss = 1.71301
I0817 10:58:34.417052 14586 caffe.cpp:313] Batch 694, accuracy/top1 = 0.56
I0817 10:58:34.417073 14586 caffe.cpp:313] Batch 694, accuracy/top5 = 0.78
I0817 10:58:34.417076 14586 caffe.cpp:313] Batch 694, loss = 1.75564
I0817 10:58:34.479686 14586 caffe.cpp:313] Batch 695, accuracy/top1 = 0.6
I0817 10:58:34.479709 14586 caffe.cpp:313] Batch 695, accuracy/top5 = 0.78
I0817 10:58:34.479713 14586 caffe.cpp:313] Batch 695, loss = 1.689
I0817 10:58:34.542397 14586 caffe.cpp:313] Batch 696, accuracy/top1 = 0.42
I0817 10:58:34.542418 14586 caffe.cpp:313] Batch 696, accuracy/top5 = 0.8
I0817 10:58:34.542423 14586 caffe.cpp:313] Batch 696, loss = 1.96428
I0817 10:58:34.605077 14586 caffe.cpp:313] Batch 697, accuracy/top1 = 0.68
I0817 10:58:34.605098 14586 caffe.cpp:313] Batch 697, accuracy/top5 = 0.88
I0817 10:58:34.605103 14586 caffe.cpp:313] Batch 697, loss = 1.36302
I0817 10:58:34.667510 14586 caffe.cpp:313] Batch 698, accuracy/top1 = 0.44
I0817 10:58:34.667531 14586 caffe.cpp:313] Batch 698, accuracy/top5 = 0.78
I0817 10:58:34.667536 14586 caffe.cpp:313] Batch 698, loss = 2.13963
I0817 10:58:34.730137 14586 caffe.cpp:313] Batch 699, accuracy/top1 = 0.72
I0817 10:58:34.730160 14586 caffe.cpp:313] Batch 699, accuracy/top5 = 0.88
I0817 10:58:34.730165 14586 caffe.cpp:313] Batch 699, loss = 1.27854
I0817 10:58:34.792860 14586 caffe.cpp:313] Batch 700, accuracy/top1 = 0.58
I0817 10:58:34.792883 14586 caffe.cpp:313] Batch 700, accuracy/top5 = 0.82
I0817 10:58:34.792887 14586 caffe.cpp:313] Batch 700, loss = 2.04326
I0817 10:58:34.856243 14586 caffe.cpp:313] Batch 701, accuracy/top1 = 0.48
I0817 10:58:34.856263 14586 caffe.cpp:313] Batch 701, accuracy/top5 = 0.68
I0817 10:58:34.856267 14586 caffe.cpp:313] Batch 701, loss = 2.43107
I0817 10:58:34.919039 14586 caffe.cpp:313] Batch 702, accuracy/top1 = 0.56
I0817 10:58:34.919061 14586 caffe.cpp:313] Batch 702, accuracy/top5 = 0.74
I0817 10:58:34.919065 14586 caffe.cpp:313] Batch 702, loss = 2.17195
I0817 10:58:34.981719 14586 caffe.cpp:313] Batch 703, accuracy/top1 = 0.64
I0817 10:58:34.981740 14586 caffe.cpp:313] Batch 703, accuracy/top5 = 0.9
I0817 10:58:34.981745 14586 caffe.cpp:313] Batch 703, loss = 1.36532
I0817 10:58:35.044517 14586 caffe.cpp:313] Batch 704, accuracy/top1 = 0.58
I0817 10:58:35.044535 14586 caffe.cpp:313] Batch 704, accuracy/top5 = 0.78
I0817 10:58:35.044539 14586 caffe.cpp:313] Batch 704, loss = 2.11531
I0817 10:58:35.107224 14586 caffe.cpp:313] Batch 705, accuracy/top1 = 0.68
I0817 10:58:35.107246 14586 caffe.cpp:313] Batch 705, accuracy/top5 = 0.82
I0817 10:58:35.107250 14586 caffe.cpp:313] Batch 705, loss = 1.36943
I0817 10:58:35.169973 14586 caffe.cpp:313] Batch 706, accuracy/top1 = 0.56
I0817 10:58:35.169991 14586 caffe.cpp:313] Batch 706, accuracy/top5 = 0.88
I0817 10:58:35.169996 14586 caffe.cpp:313] Batch 706, loss = 1.77899
I0817 10:58:35.232743 14586 caffe.cpp:313] Batch 707, accuracy/top1 = 0.62
I0817 10:58:35.232765 14586 caffe.cpp:313] Batch 707, accuracy/top5 = 0.84
I0817 10:58:35.232769 14586 caffe.cpp:313] Batch 707, loss = 1.56782
I0817 10:58:35.295473 14586 caffe.cpp:313] Batch 708, accuracy/top1 = 0.56
I0817 10:58:35.295496 14586 caffe.cpp:313] Batch 708, accuracy/top5 = 0.84
I0817 10:58:35.295500 14586 caffe.cpp:313] Batch 708, loss = 1.56141
I0817 10:58:35.358129 14586 caffe.cpp:313] Batch 709, accuracy/top1 = 0.58
I0817 10:58:35.358152 14586 caffe.cpp:313] Batch 709, accuracy/top5 = 0.82
I0817 10:58:35.358156 14586 caffe.cpp:313] Batch 709, loss = 2.02267
I0817 10:58:35.420898 14586 caffe.cpp:313] Batch 710, accuracy/top1 = 0.58
I0817 10:58:35.420922 14586 caffe.cpp:313] Batch 710, accuracy/top5 = 0.72
I0817 10:58:35.420925 14586 caffe.cpp:313] Batch 710, loss = 2.05953
I0817 10:58:35.483537 14586 caffe.cpp:313] Batch 711, accuracy/top1 = 0.56
I0817 10:58:35.483573 14586 caffe.cpp:313] Batch 711, accuracy/top5 = 0.8
I0817 10:58:35.483577 14586 caffe.cpp:313] Batch 711, loss = 1.89681
I0817 10:58:35.546306 14586 caffe.cpp:313] Batch 712, accuracy/top1 = 0.58
I0817 10:58:35.546329 14586 caffe.cpp:313] Batch 712, accuracy/top5 = 0.78
I0817 10:58:35.546332 14586 caffe.cpp:313] Batch 712, loss = 1.80079
I0817 10:58:35.608928 14586 caffe.cpp:313] Batch 713, accuracy/top1 = 0.46
I0817 10:58:35.608950 14586 caffe.cpp:313] Batch 713, accuracy/top5 = 0.72
I0817 10:58:35.608954 14586 caffe.cpp:313] Batch 713, loss = 2.41707
I0817 10:58:35.671489 14586 caffe.cpp:313] Batch 714, accuracy/top1 = 0.62
I0817 10:58:35.671511 14586 caffe.cpp:313] Batch 714, accuracy/top5 = 0.8
I0817 10:58:35.671515 14586 caffe.cpp:313] Batch 714, loss = 1.63596
I0817 10:58:35.734098 14586 caffe.cpp:313] Batch 715, accuracy/top1 = 0.52
I0817 10:58:35.734120 14586 caffe.cpp:313] Batch 715, accuracy/top5 = 0.84
I0817 10:58:35.734124 14586 caffe.cpp:313] Batch 715, loss = 1.95063
I0817 10:58:35.796849 14586 caffe.cpp:313] Batch 716, accuracy/top1 = 0.6
I0817 10:58:35.796871 14586 caffe.cpp:313] Batch 716, accuracy/top5 = 0.76
I0817 10:58:35.796875 14586 caffe.cpp:313] Batch 716, loss = 2.39696
I0817 10:58:35.860280 14586 caffe.cpp:313] Batch 717, accuracy/top1 = 0.72
I0817 10:58:35.860298 14586 caffe.cpp:313] Batch 717, accuracy/top5 = 0.86
I0817 10:58:35.860301 14586 caffe.cpp:313] Batch 717, loss = 1.44438
I0817 10:58:35.922988 14586 caffe.cpp:313] Batch 718, accuracy/top1 = 0.54
I0817 10:58:35.923012 14586 caffe.cpp:313] Batch 718, accuracy/top5 = 0.72
I0817 10:58:35.923014 14586 caffe.cpp:313] Batch 718, loss = 2.28741
I0817 10:58:35.985618 14586 caffe.cpp:313] Batch 719, accuracy/top1 = 0.62
I0817 10:58:35.985641 14586 caffe.cpp:313] Batch 719, accuracy/top5 = 0.76
I0817 10:58:35.985644 14586 caffe.cpp:313] Batch 719, loss = 1.84618
I0817 10:58:36.048434 14586 caffe.cpp:313] Batch 720, accuracy/top1 = 0.52
I0817 10:58:36.048454 14586 caffe.cpp:313] Batch 720, accuracy/top5 = 0.88
I0817 10:58:36.048456 14586 caffe.cpp:313] Batch 720, loss = 1.7433
I0817 10:58:36.111173 14586 caffe.cpp:313] Batch 721, accuracy/top1 = 0.62
I0817 10:58:36.111196 14586 caffe.cpp:313] Batch 721, accuracy/top5 = 0.82
I0817 10:58:36.111199 14586 caffe.cpp:313] Batch 721, loss = 1.82434
I0817 10:58:36.173805 14586 caffe.cpp:313] Batch 722, accuracy/top1 = 0.6
I0817 10:58:36.173827 14586 caffe.cpp:313] Batch 722, accuracy/top5 = 0.8
I0817 10:58:36.173831 14586 caffe.cpp:313] Batch 722, loss = 1.91808
I0817 10:58:36.236416 14586 caffe.cpp:313] Batch 723, accuracy/top1 = 0.54
I0817 10:58:36.236438 14586 caffe.cpp:313] Batch 723, accuracy/top5 = 0.82
I0817 10:58:36.236440 14586 caffe.cpp:313] Batch 723, loss = 1.78374
I0817 10:58:36.298997 14586 caffe.cpp:313] Batch 724, accuracy/top1 = 0.58
I0817 10:58:36.299019 14586 caffe.cpp:313] Batch 724, accuracy/top5 = 0.92
I0817 10:58:36.299022 14586 caffe.cpp:313] Batch 724, loss = 1.32983
I0817 10:58:36.361708 14586 caffe.cpp:313] Batch 725, accuracy/top1 = 0.64
I0817 10:58:36.361727 14586 caffe.cpp:313] Batch 725, accuracy/top5 = 0.92
I0817 10:58:36.361730 14586 caffe.cpp:313] Batch 725, loss = 1.39994
I0817 10:58:36.424386 14586 caffe.cpp:313] Batch 726, accuracy/top1 = 0.5
I0817 10:58:36.424405 14586 caffe.cpp:313] Batch 726, accuracy/top5 = 0.84
I0817 10:58:36.424407 14586 caffe.cpp:313] Batch 726, loss = 1.80786
I0817 10:58:36.487066 14586 caffe.cpp:313] Batch 727, accuracy/top1 = 0.58
I0817 10:58:36.487087 14586 caffe.cpp:313] Batch 727, accuracy/top5 = 0.84
I0817 10:58:36.487090 14586 caffe.cpp:313] Batch 727, loss = 1.82029
I0817 10:58:36.549607 14586 caffe.cpp:313] Batch 728, accuracy/top1 = 0.64
I0817 10:58:36.549628 14586 caffe.cpp:313] Batch 728, accuracy/top5 = 0.86
I0817 10:58:36.549631 14586 caffe.cpp:313] Batch 728, loss = 1.45874
I0817 10:58:36.612257 14586 caffe.cpp:313] Batch 729, accuracy/top1 = 0.66
I0817 10:58:36.612278 14586 caffe.cpp:313] Batch 729, accuracy/top5 = 0.86
I0817 10:58:36.612282 14586 caffe.cpp:313] Batch 729, loss = 1.32035
I0817 10:58:36.674849 14586 caffe.cpp:313] Batch 730, accuracy/top1 = 0.54
I0817 10:58:36.674871 14586 caffe.cpp:313] Batch 730, accuracy/top5 = 0.78
I0817 10:58:36.674875 14586 caffe.cpp:313] Batch 730, loss = 1.93782
I0817 10:58:36.737627 14586 caffe.cpp:313] Batch 731, accuracy/top1 = 0.48
I0817 10:58:36.737648 14586 caffe.cpp:313] Batch 731, accuracy/top5 = 0.74
I0817 10:58:36.737651 14586 caffe.cpp:313] Batch 731, loss = 2.06352
I0817 10:58:36.800256 14586 caffe.cpp:313] Batch 732, accuracy/top1 = 0.7
I0817 10:58:36.800277 14586 caffe.cpp:313] Batch 732, accuracy/top5 = 0.86
I0817 10:58:36.800281 14586 caffe.cpp:313] Batch 732, loss = 1.43173
I0817 10:58:36.863229 14586 caffe.cpp:313] Batch 733, accuracy/top1 = 0.4
I0817 10:58:36.863246 14586 caffe.cpp:313] Batch 733, accuracy/top5 = 0.78
I0817 10:58:36.863248 14586 caffe.cpp:313] Batch 733, loss = 2.33404
I0817 10:58:36.926017 14586 caffe.cpp:313] Batch 734, accuracy/top1 = 0.56
I0817 10:58:36.926039 14586 caffe.cpp:313] Batch 734, accuracy/top5 = 0.82
I0817 10:58:36.926043 14586 caffe.cpp:313] Batch 734, loss = 1.77074
I0817 10:58:36.988668 14586 caffe.cpp:313] Batch 735, accuracy/top1 = 0.58
I0817 10:58:36.988690 14586 caffe.cpp:313] Batch 735, accuracy/top5 = 0.74
I0817 10:58:36.988693 14586 caffe.cpp:313] Batch 735, loss = 1.95188
I0817 10:58:37.051412 14586 caffe.cpp:313] Batch 736, accuracy/top1 = 0.56
I0817 10:58:37.051429 14586 caffe.cpp:313] Batch 736, accuracy/top5 = 0.84
I0817 10:58:37.051432 14586 caffe.cpp:313] Batch 736, loss = 1.73738
I0817 10:58:37.114156 14586 caffe.cpp:313] Batch 737, accuracy/top1 = 0.62
I0817 10:58:37.114176 14586 caffe.cpp:313] Batch 737, accuracy/top5 = 0.86
I0817 10:58:37.114179 14586 caffe.cpp:313] Batch 737, loss = 1.77975
I0817 10:58:37.176892 14586 caffe.cpp:313] Batch 738, accuracy/top1 = 0.56
I0817 10:58:37.176913 14586 caffe.cpp:313] Batch 738, accuracy/top5 = 0.8
I0817 10:58:37.176915 14586 caffe.cpp:313] Batch 738, loss = 1.61858
I0817 10:58:37.239533 14586 caffe.cpp:313] Batch 739, accuracy/top1 = 0.62
I0817 10:58:37.239554 14586 caffe.cpp:313] Batch 739, accuracy/top5 = 0.86
I0817 10:58:37.239557 14586 caffe.cpp:313] Batch 739, loss = 1.59134
I0817 10:58:37.302109 14586 caffe.cpp:313] Batch 740, accuracy/top1 = 0.62
I0817 10:58:37.302131 14586 caffe.cpp:313] Batch 740, accuracy/top5 = 0.88
I0817 10:58:37.302134 14586 caffe.cpp:313] Batch 740, loss = 1.41245
I0817 10:58:37.364949 14586 caffe.cpp:313] Batch 741, accuracy/top1 = 0.66
I0817 10:58:37.364970 14586 caffe.cpp:313] Batch 741, accuracy/top5 = 0.86
I0817 10:58:37.364974 14586 caffe.cpp:313] Batch 741, loss = 1.53687
I0817 10:58:37.427758 14586 caffe.cpp:313] Batch 742, accuracy/top1 = 0.62
I0817 10:58:37.427780 14586 caffe.cpp:313] Batch 742, accuracy/top5 = 0.82
I0817 10:58:37.427783 14586 caffe.cpp:313] Batch 742, loss = 1.6981
I0817 10:58:37.490453 14586 caffe.cpp:313] Batch 743, accuracy/top1 = 0.56
I0817 10:58:37.490473 14586 caffe.cpp:313] Batch 743, accuracy/top5 = 0.84
I0817 10:58:37.490476 14586 caffe.cpp:313] Batch 743, loss = 1.95818
I0817 10:58:37.553176 14586 caffe.cpp:313] Batch 744, accuracy/top1 = 0.56
I0817 10:58:37.553198 14586 caffe.cpp:313] Batch 744, accuracy/top5 = 0.82
I0817 10:58:37.553201 14586 caffe.cpp:313] Batch 744, loss = 1.94269
I0817 10:58:37.615905 14586 caffe.cpp:313] Batch 745, accuracy/top1 = 0.66
I0817 10:58:37.615926 14586 caffe.cpp:313] Batch 745, accuracy/top5 = 0.88
I0817 10:58:37.615929 14586 caffe.cpp:313] Batch 745, loss = 1.40155
I0817 10:58:37.678719 14586 caffe.cpp:313] Batch 746, accuracy/top1 = 0.62
I0817 10:58:37.678741 14586 caffe.cpp:313] Batch 746, accuracy/top5 = 0.82
I0817 10:58:37.678745 14586 caffe.cpp:313] Batch 746, loss = 1.67903
I0817 10:58:37.741510 14586 caffe.cpp:313] Batch 747, accuracy/top1 = 0.58
I0817 10:58:37.741533 14586 caffe.cpp:313] Batch 747, accuracy/top5 = 0.82
I0817 10:58:37.741535 14586 caffe.cpp:313] Batch 747, loss = 1.68471
I0817 10:58:37.804234 14586 caffe.cpp:313] Batch 748, accuracy/top1 = 0.64
I0817 10:58:37.804256 14586 caffe.cpp:313] Batch 748, accuracy/top5 = 0.84
I0817 10:58:37.804273 14586 caffe.cpp:313] Batch 748, loss = 1.47584
I0817 10:58:37.867982 14586 caffe.cpp:313] Batch 749, accuracy/top1 = 0.62
I0817 10:58:37.868000 14586 caffe.cpp:313] Batch 749, accuracy/top5 = 0.86
I0817 10:58:37.868003 14586 caffe.cpp:313] Batch 749, loss = 1.32803
I0817 10:58:37.930815 14586 caffe.cpp:313] Batch 750, accuracy/top1 = 0.68
I0817 10:58:37.930837 14586 caffe.cpp:313] Batch 750, accuracy/top5 = 0.82
I0817 10:58:37.930840 14586 caffe.cpp:313] Batch 750, loss = 1.5588
I0817 10:58:37.993475 14586 caffe.cpp:313] Batch 751, accuracy/top1 = 0.58
I0817 10:58:37.993497 14586 caffe.cpp:313] Batch 751, accuracy/top5 = 0.84
I0817 10:58:37.993500 14586 caffe.cpp:313] Batch 751, loss = 1.72654
I0817 10:58:38.056068 14586 caffe.cpp:313] Batch 752, accuracy/top1 = 0.68
I0817 10:58:38.056087 14586 caffe.cpp:313] Batch 752, accuracy/top5 = 0.88
I0817 10:58:38.056089 14586 caffe.cpp:313] Batch 752, loss = 1.22801
I0817 10:58:38.118717 14586 caffe.cpp:313] Batch 753, accuracy/top1 = 0.5
I0817 10:58:38.118738 14586 caffe.cpp:313] Batch 753, accuracy/top5 = 0.78
I0817 10:58:38.118742 14586 caffe.cpp:313] Batch 753, loss = 2.15249
I0817 10:58:38.181429 14586 caffe.cpp:313] Batch 754, accuracy/top1 = 0.52
I0817 10:58:38.181452 14586 caffe.cpp:313] Batch 754, accuracy/top5 = 0.78
I0817 10:58:38.181454 14586 caffe.cpp:313] Batch 754, loss = 1.97909
I0817 10:58:38.244211 14586 caffe.cpp:313] Batch 755, accuracy/top1 = 0.5
I0817 10:58:38.244232 14586 caffe.cpp:313] Batch 755, accuracy/top5 = 0.76
I0817 10:58:38.244235 14586 caffe.cpp:313] Batch 755, loss = 2.28008
I0817 10:58:38.306962 14586 caffe.cpp:313] Batch 756, accuracy/top1 = 0.66
I0817 10:58:38.306982 14586 caffe.cpp:313] Batch 756, accuracy/top5 = 0.86
I0817 10:58:38.306985 14586 caffe.cpp:313] Batch 756, loss = 1.53873
I0817 10:58:38.369616 14586 caffe.cpp:313] Batch 757, accuracy/top1 = 0.6
I0817 10:58:38.369637 14586 caffe.cpp:313] Batch 757, accuracy/top5 = 0.82
I0817 10:58:38.369640 14586 caffe.cpp:313] Batch 757, loss = 1.44468
I0817 10:58:38.432132 14586 caffe.cpp:313] Batch 758, accuracy/top1 = 0.68
I0817 10:58:38.432153 14586 caffe.cpp:313] Batch 758, accuracy/top5 = 0.88
I0817 10:58:38.432157 14586 caffe.cpp:313] Batch 758, loss = 1.30009
I0817 10:58:38.494843 14586 caffe.cpp:313] Batch 759, accuracy/top1 = 0.62
I0817 10:58:38.494863 14586 caffe.cpp:313] Batch 759, accuracy/top5 = 0.82
I0817 10:58:38.494865 14586 caffe.cpp:313] Batch 759, loss = 1.77723
I0817 10:58:38.557835 14586 caffe.cpp:313] Batch 760, accuracy/top1 = 0.62
I0817 10:58:38.557854 14586 caffe.cpp:313] Batch 760, accuracy/top5 = 0.86
I0817 10:58:38.557857 14586 caffe.cpp:313] Batch 760, loss = 1.56792
I0817 10:58:38.620633 14586 caffe.cpp:313] Batch 761, accuracy/top1 = 0.68
I0817 10:58:38.620656 14586 caffe.cpp:313] Batch 761, accuracy/top5 = 0.86
I0817 10:58:38.620658 14586 caffe.cpp:313] Batch 761, loss = 1.66181
I0817 10:58:38.683267 14586 caffe.cpp:313] Batch 762, accuracy/top1 = 0.56
I0817 10:58:38.683290 14586 caffe.cpp:313] Batch 762, accuracy/top5 = 0.76
I0817 10:58:38.683292 14586 caffe.cpp:313] Batch 762, loss = 1.85766
I0817 10:58:38.746099 14586 caffe.cpp:313] Batch 763, accuracy/top1 = 0.58
I0817 10:58:38.746121 14586 caffe.cpp:313] Batch 763, accuracy/top5 = 0.82
I0817 10:58:38.746124 14586 caffe.cpp:313] Batch 763, loss = 1.64492
I0817 10:58:38.808974 14586 caffe.cpp:313] Batch 764, accuracy/top1 = 0.44
I0817 10:58:38.808995 14586 caffe.cpp:313] Batch 764, accuracy/top5 = 0.7
I0817 10:58:38.808998 14586 caffe.cpp:313] Batch 764, loss = 2.40351
I0817 10:58:38.872370 14586 caffe.cpp:313] Batch 765, accuracy/top1 = 0.64
I0817 10:58:38.872387 14586 caffe.cpp:313] Batch 765, accuracy/top5 = 0.88
I0817 10:58:38.872390 14586 caffe.cpp:313] Batch 765, loss = 1.55334
I0817 10:58:38.934903 14586 caffe.cpp:313] Batch 766, accuracy/top1 = 0.52
I0817 10:58:38.934926 14586 caffe.cpp:313] Batch 766, accuracy/top5 = 0.84
I0817 10:58:38.934928 14586 caffe.cpp:313] Batch 766, loss = 1.78284
I0817 10:58:38.997715 14586 caffe.cpp:313] Batch 767, accuracy/top1 = 0.6
I0817 10:58:38.997750 14586 caffe.cpp:313] Batch 767, accuracy/top5 = 0.82
I0817 10:58:38.997753 14586 caffe.cpp:313] Batch 767, loss = 1.75103
I0817 10:58:39.060493 14586 caffe.cpp:313] Batch 768, accuracy/top1 = 0.6
I0817 10:58:39.060514 14586 caffe.cpp:313] Batch 768, accuracy/top5 = 0.84
I0817 10:58:39.060518 14586 caffe.cpp:313] Batch 768, loss = 1.60283
I0817 10:58:39.123199 14586 caffe.cpp:313] Batch 769, accuracy/top1 = 0.62
I0817 10:58:39.123221 14586 caffe.cpp:313] Batch 769, accuracy/top5 = 0.84
I0817 10:58:39.123224 14586 caffe.cpp:313] Batch 769, loss = 1.48714
I0817 10:58:39.185971 14586 caffe.cpp:313] Batch 770, accuracy/top1 = 0.58
I0817 10:58:39.185992 14586 caffe.cpp:313] Batch 770, accuracy/top5 = 0.86
I0817 10:58:39.185995 14586 caffe.cpp:313] Batch 770, loss = 1.74671
I0817 10:58:39.248596 14586 caffe.cpp:313] Batch 771, accuracy/top1 = 0.6
I0817 10:58:39.248615 14586 caffe.cpp:313] Batch 771, accuracy/top5 = 0.78
I0817 10:58:39.248617 14586 caffe.cpp:313] Batch 771, loss = 2.28951
I0817 10:58:39.311271 14586 caffe.cpp:313] Batch 772, accuracy/top1 = 0.56
I0817 10:58:39.311292 14586 caffe.cpp:313] Batch 772, accuracy/top5 = 0.76
I0817 10:58:39.311295 14586 caffe.cpp:313] Batch 772, loss = 2.06076
I0817 10:58:39.373922 14586 caffe.cpp:313] Batch 773, accuracy/top1 = 0.46
I0817 10:58:39.373944 14586 caffe.cpp:313] Batch 773, accuracy/top5 = 0.8
I0817 10:58:39.373947 14586 caffe.cpp:313] Batch 773, loss = 2.33917
I0817 10:58:39.436561 14586 caffe.cpp:313] Batch 774, accuracy/top1 = 0.56
I0817 10:58:39.436583 14586 caffe.cpp:313] Batch 774, accuracy/top5 = 0.82
I0817 10:58:39.436585 14586 caffe.cpp:313] Batch 774, loss = 1.68879
I0817 10:58:39.499253 14586 caffe.cpp:313] Batch 775, accuracy/top1 = 0.56
I0817 10:58:39.499274 14586 caffe.cpp:313] Batch 775, accuracy/top5 = 0.76
I0817 10:58:39.499277 14586 caffe.cpp:313] Batch 775, loss = 1.76727
I0817 10:58:39.561959 14586 caffe.cpp:313] Batch 776, accuracy/top1 = 0.66
I0817 10:58:39.561980 14586 caffe.cpp:313] Batch 776, accuracy/top5 = 0.8
I0817 10:58:39.561982 14586 caffe.cpp:313] Batch 776, loss = 2.09092
I0817 10:58:39.624636 14586 caffe.cpp:313] Batch 777, accuracy/top1 = 0.52
I0817 10:58:39.624658 14586 caffe.cpp:313] Batch 777, accuracy/top5 = 0.8
I0817 10:58:39.624661 14586 caffe.cpp:313] Batch 777, loss = 1.87309
I0817 10:58:39.687355 14586 caffe.cpp:313] Batch 778, accuracy/top1 = 0.58
I0817 10:58:39.687377 14586 caffe.cpp:313] Batch 778, accuracy/top5 = 0.82
I0817 10:58:39.687381 14586 caffe.cpp:313] Batch 778, loss = 1.51792
I0817 10:58:39.750064 14586 caffe.cpp:313] Batch 779, accuracy/top1 = 0.58
I0817 10:58:39.750085 14586 caffe.cpp:313] Batch 779, accuracy/top5 = 0.78
I0817 10:58:39.750088 14586 caffe.cpp:313] Batch 779, loss = 1.94992
I0817 10:58:39.812793 14586 caffe.cpp:313] Batch 780, accuracy/top1 = 0.54
I0817 10:58:39.812813 14586 caffe.cpp:313] Batch 780, accuracy/top5 = 0.84
I0817 10:58:39.812816 14586 caffe.cpp:313] Batch 780, loss = 1.67008
I0817 10:58:39.876080 14586 caffe.cpp:313] Batch 781, accuracy/top1 = 0.62
I0817 10:58:39.876096 14586 caffe.cpp:313] Batch 781, accuracy/top5 = 0.86
I0817 10:58:39.876099 14586 caffe.cpp:313] Batch 781, loss = 1.4312
I0817 10:58:39.938732 14586 caffe.cpp:313] Batch 782, accuracy/top1 = 0.74
I0817 10:58:39.938753 14586 caffe.cpp:313] Batch 782, accuracy/top5 = 0.9
I0817 10:58:39.938756 14586 caffe.cpp:313] Batch 782, loss = 1.21415
I0817 10:58:40.001271 14586 caffe.cpp:313] Batch 783, accuracy/top1 = 0.6
I0817 10:58:40.001292 14586 caffe.cpp:313] Batch 783, accuracy/top5 = 0.84
I0817 10:58:40.001294 14586 caffe.cpp:313] Batch 783, loss = 1.60974
I0817 10:58:40.063990 14586 caffe.cpp:313] Batch 784, accuracy/top1 = 0.68
I0817 10:58:40.064009 14586 caffe.cpp:313] Batch 784, accuracy/top5 = 0.82
I0817 10:58:40.064013 14586 caffe.cpp:313] Batch 784, loss = 1.40279
I0817 10:58:40.126710 14586 caffe.cpp:313] Batch 785, accuracy/top1 = 0.6
I0817 10:58:40.126731 14586 caffe.cpp:313] Batch 785, accuracy/top5 = 0.82
I0817 10:58:40.126734 14586 caffe.cpp:313] Batch 785, loss = 1.95804
I0817 10:58:40.189422 14586 caffe.cpp:313] Batch 786, accuracy/top1 = 0.66
I0817 10:58:40.189445 14586 caffe.cpp:313] Batch 786, accuracy/top5 = 0.9
I0817 10:58:40.189447 14586 caffe.cpp:313] Batch 786, loss = 1.00867
I0817 10:58:40.252086 14586 caffe.cpp:313] Batch 787, accuracy/top1 = 0.52
I0817 10:58:40.252107 14586 caffe.cpp:313] Batch 787, accuracy/top5 = 0.88
I0817 10:58:40.252110 14586 caffe.cpp:313] Batch 787, loss = 1.58464
I0817 10:58:40.314919 14586 caffe.cpp:313] Batch 788, accuracy/top1 = 0.52
I0817 10:58:40.314939 14586 caffe.cpp:313] Batch 788, accuracy/top5 = 0.78
I0817 10:58:40.314942 14586 caffe.cpp:313] Batch 788, loss = 2.04148
I0817 10:58:40.377701 14586 caffe.cpp:313] Batch 789, accuracy/top1 = 0.64
I0817 10:58:40.377722 14586 caffe.cpp:313] Batch 789, accuracy/top5 = 0.84
I0817 10:58:40.377724 14586 caffe.cpp:313] Batch 789, loss = 1.48646
I0817 10:58:40.440316 14586 caffe.cpp:313] Batch 790, accuracy/top1 = 0.52
I0817 10:58:40.440337 14586 caffe.cpp:313] Batch 790, accuracy/top5 = 0.88
I0817 10:58:40.440340 14586 caffe.cpp:313] Batch 790, loss = 1.63673
I0817 10:58:40.502813 14586 caffe.cpp:313] Batch 791, accuracy/top1 = 0.44
I0817 10:58:40.502835 14586 caffe.cpp:313] Batch 791, accuracy/top5 = 0.74
I0817 10:58:40.502837 14586 caffe.cpp:313] Batch 791, loss = 2.25794
I0817 10:58:40.565407 14586 caffe.cpp:313] Batch 792, accuracy/top1 = 0.46
I0817 10:58:40.565429 14586 caffe.cpp:313] Batch 792, accuracy/top5 = 0.76
I0817 10:58:40.565433 14586 caffe.cpp:313] Batch 792, loss = 1.96168
I0817 10:58:40.628216 14586 caffe.cpp:313] Batch 793, accuracy/top1 = 0.6
I0817 10:58:40.628234 14586 caffe.cpp:313] Batch 793, accuracy/top5 = 0.8
I0817 10:58:40.628237 14586 caffe.cpp:313] Batch 793, loss = 1.78906
I0817 10:58:40.691076 14586 caffe.cpp:313] Batch 794, accuracy/top1 = 0.64
I0817 10:58:40.691094 14586 caffe.cpp:313] Batch 794, accuracy/top5 = 0.82
I0817 10:58:40.691097 14586 caffe.cpp:313] Batch 794, loss = 1.64781
I0817 10:58:40.753962 14586 caffe.cpp:313] Batch 795, accuracy/top1 = 0.54
I0817 10:58:40.753983 14586 caffe.cpp:313] Batch 795, accuracy/top5 = 0.8
I0817 10:58:40.753988 14586 caffe.cpp:313] Batch 795, loss = 2.00018
I0817 10:58:40.816628 14586 caffe.cpp:313] Batch 796, accuracy/top1 = 0.6
I0817 10:58:40.816649 14586 caffe.cpp:313] Batch 796, accuracy/top5 = 0.76
I0817 10:58:40.816653 14586 caffe.cpp:313] Batch 796, loss = 1.92911
I0817 10:58:40.879614 14586 caffe.cpp:313] Batch 797, accuracy/top1 = 0.56
I0817 10:58:40.879632 14586 caffe.cpp:313] Batch 797, accuracy/top5 = 0.76
I0817 10:58:40.879636 14586 caffe.cpp:313] Batch 797, loss = 2.01042
I0817 10:58:40.942401 14586 caffe.cpp:313] Batch 798, accuracy/top1 = 0.6
I0817 10:58:40.942425 14586 caffe.cpp:313] Batch 798, accuracy/top5 = 0.88
I0817 10:58:40.942427 14586 caffe.cpp:313] Batch 798, loss = 1.46247
I0817 10:58:41.005172 14586 caffe.cpp:313] Batch 799, accuracy/top1 = 0.66
I0817 10:58:41.005192 14586 caffe.cpp:313] Batch 799, accuracy/top5 = 0.86
I0817 10:58:41.005194 14586 caffe.cpp:313] Batch 799, loss = 1.66679
I0817 10:58:41.067719 14586 caffe.cpp:313] Batch 800, accuracy/top1 = 0.48
I0817 10:58:41.067741 14586 caffe.cpp:313] Batch 800, accuracy/top5 = 0.64
I0817 10:58:41.067744 14586 caffe.cpp:313] Batch 800, loss = 2.87637
I0817 10:58:41.130331 14586 caffe.cpp:313] Batch 801, accuracy/top1 = 0.62
I0817 10:58:41.130352 14586 caffe.cpp:313] Batch 801, accuracy/top5 = 0.82
I0817 10:58:41.130354 14586 caffe.cpp:313] Batch 801, loss = 1.7373
I0817 10:58:41.192960 14586 caffe.cpp:313] Batch 802, accuracy/top1 = 0.58
I0817 10:58:41.192981 14586 caffe.cpp:313] Batch 802, accuracy/top5 = 0.86
I0817 10:58:41.192983 14586 caffe.cpp:313] Batch 802, loss = 1.71056
I0817 10:58:41.255686 14586 caffe.cpp:313] Batch 803, accuracy/top1 = 0.64
I0817 10:58:41.255702 14586 caffe.cpp:313] Batch 803, accuracy/top5 = 0.8
I0817 10:58:41.255704 14586 caffe.cpp:313] Batch 803, loss = 1.66887
I0817 10:58:41.318433 14586 caffe.cpp:313] Batch 804, accuracy/top1 = 0.58
I0817 10:58:41.318454 14586 caffe.cpp:313] Batch 804, accuracy/top5 = 0.92
I0817 10:58:41.318457 14586 caffe.cpp:313] Batch 804, loss = 1.56513
I0817 10:58:41.381108 14586 caffe.cpp:313] Batch 805, accuracy/top1 = 0.68
I0817 10:58:41.381129 14586 caffe.cpp:313] Batch 805, accuracy/top5 = 0.84
I0817 10:58:41.381132 14586 caffe.cpp:313] Batch 805, loss = 1.74349
I0817 10:58:41.443668 14586 caffe.cpp:313] Batch 806, accuracy/top1 = 0.7
I0817 10:58:41.443689 14586 caffe.cpp:313] Batch 806, accuracy/top5 = 0.88
I0817 10:58:41.443692 14586 caffe.cpp:313] Batch 806, loss = 1.17083
I0817 10:58:41.506248 14586 caffe.cpp:313] Batch 807, accuracy/top1 = 0.56
I0817 10:58:41.506269 14586 caffe.cpp:313] Batch 807, accuracy/top5 = 0.86
I0817 10:58:41.506273 14586 caffe.cpp:313] Batch 807, loss = 1.97135
I0817 10:58:41.568920 14586 caffe.cpp:313] Batch 808, accuracy/top1 = 0.58
I0817 10:58:41.568941 14586 caffe.cpp:313] Batch 808, accuracy/top5 = 0.8
I0817 10:58:41.568944 14586 caffe.cpp:313] Batch 808, loss = 1.84357
I0817 10:58:41.631708 14586 caffe.cpp:313] Batch 809, accuracy/top1 = 0.48
I0817 10:58:41.631728 14586 caffe.cpp:313] Batch 809, accuracy/top5 = 0.72
I0817 10:58:41.631731 14586 caffe.cpp:313] Batch 809, loss = 2.45596
I0817 10:58:41.694553 14586 caffe.cpp:313] Batch 810, accuracy/top1 = 0.54
I0817 10:58:41.694576 14586 caffe.cpp:313] Batch 810, accuracy/top5 = 0.8
I0817 10:58:41.694578 14586 caffe.cpp:313] Batch 810, loss = 1.82238
I0817 10:58:41.757308 14586 caffe.cpp:313] Batch 811, accuracy/top1 = 0.6
I0817 10:58:41.757329 14586 caffe.cpp:313] Batch 811, accuracy/top5 = 0.82
I0817 10:58:41.757333 14586 caffe.cpp:313] Batch 811, loss = 2.11941
I0817 10:58:41.820086 14586 caffe.cpp:313] Batch 812, accuracy/top1 = 0.5
I0817 10:58:41.820107 14586 caffe.cpp:313] Batch 812, accuracy/top5 = 0.84
I0817 10:58:41.820111 14586 caffe.cpp:313] Batch 812, loss = 1.79512
I0817 10:58:41.883083 14586 caffe.cpp:313] Batch 813, accuracy/top1 = 0.52
I0817 10:58:41.883100 14586 caffe.cpp:313] Batch 813, accuracy/top5 = 0.78
I0817 10:58:41.883103 14586 caffe.cpp:313] Batch 813, loss = 1.96475
I0817 10:58:41.945804 14586 caffe.cpp:313] Batch 814, accuracy/top1 = 0.4
I0817 10:58:41.945827 14586 caffe.cpp:313] Batch 814, accuracy/top5 = 0.7
I0817 10:58:41.945830 14586 caffe.cpp:313] Batch 814, loss = 2.66598
I0817 10:58:42.008394 14586 caffe.cpp:313] Batch 815, accuracy/top1 = 0.54
I0817 10:58:42.008414 14586 caffe.cpp:313] Batch 815, accuracy/top5 = 0.9
I0817 10:58:42.008416 14586 caffe.cpp:313] Batch 815, loss = 1.38848
I0817 10:58:42.071030 14586 caffe.cpp:313] Batch 816, accuracy/top1 = 0.5
I0817 10:58:42.071051 14586 caffe.cpp:313] Batch 816, accuracy/top5 = 0.78
I0817 10:58:42.071054 14586 caffe.cpp:313] Batch 816, loss = 2.16032
I0817 10:58:42.133625 14586 caffe.cpp:313] Batch 817, accuracy/top1 = 0.62
I0817 10:58:42.133646 14586 caffe.cpp:313] Batch 817, accuracy/top5 = 0.86
I0817 10:58:42.133649 14586 caffe.cpp:313] Batch 817, loss = 1.68779
I0817 10:58:42.196419 14586 caffe.cpp:313] Batch 818, accuracy/top1 = 0.62
I0817 10:58:42.196440 14586 caffe.cpp:313] Batch 818, accuracy/top5 = 0.86
I0817 10:58:42.196444 14586 caffe.cpp:313] Batch 818, loss = 1.51624
I0817 10:58:42.259188 14586 caffe.cpp:313] Batch 819, accuracy/top1 = 0.72
I0817 10:58:42.259210 14586 caffe.cpp:313] Batch 819, accuracy/top5 = 0.82
I0817 10:58:42.259213 14586 caffe.cpp:313] Batch 819, loss = 1.55178
I0817 10:58:42.321784 14586 caffe.cpp:313] Batch 820, accuracy/top1 = 0.58
I0817 10:58:42.321805 14586 caffe.cpp:313] Batch 820, accuracy/top5 = 0.78
I0817 10:58:42.321808 14586 caffe.cpp:313] Batch 820, loss = 1.81725
I0817 10:58:42.384495 14586 caffe.cpp:313] Batch 821, accuracy/top1 = 0.56
I0817 10:58:42.384516 14586 caffe.cpp:313] Batch 821, accuracy/top5 = 0.82
I0817 10:58:42.384518 14586 caffe.cpp:313] Batch 821, loss = 1.80179
I0817 10:58:42.447262 14586 caffe.cpp:313] Batch 822, accuracy/top1 = 0.52
I0817 10:58:42.447283 14586 caffe.cpp:313] Batch 822, accuracy/top5 = 0.8
I0817 10:58:42.447286 14586 caffe.cpp:313] Batch 822, loss = 2.01632
I0817 10:58:42.509935 14586 caffe.cpp:313] Batch 823, accuracy/top1 = 0.62
I0817 10:58:42.509956 14586 caffe.cpp:313] Batch 823, accuracy/top5 = 0.9
I0817 10:58:42.509974 14586 caffe.cpp:313] Batch 823, loss = 1.45962
I0817 10:58:42.572744 14586 caffe.cpp:313] Batch 824, accuracy/top1 = 0.62
I0817 10:58:42.572765 14586 caffe.cpp:313] Batch 824, accuracy/top5 = 0.78
I0817 10:58:42.572768 14586 caffe.cpp:313] Batch 824, loss = 1.87111
I0817 10:58:42.635365 14586 caffe.cpp:313] Batch 825, accuracy/top1 = 0.68
I0817 10:58:42.635385 14586 caffe.cpp:313] Batch 825, accuracy/top5 = 0.84
I0817 10:58:42.635388 14586 caffe.cpp:313] Batch 825, loss = 1.46443
I0817 10:58:42.698168 14586 caffe.cpp:313] Batch 826, accuracy/top1 = 0.56
I0817 10:58:42.698191 14586 caffe.cpp:313] Batch 826, accuracy/top5 = 0.84
I0817 10:58:42.698194 14586 caffe.cpp:313] Batch 826, loss = 1.73309
I0817 10:58:42.761010 14586 caffe.cpp:313] Batch 827, accuracy/top1 = 0.66
I0817 10:58:42.761030 14586 caffe.cpp:313] Batch 827, accuracy/top5 = 0.92
I0817 10:58:42.761032 14586 caffe.cpp:313] Batch 827, loss = 1.29515
I0817 10:58:42.823928 14586 caffe.cpp:313] Batch 828, accuracy/top1 = 0.54
I0817 10:58:42.823948 14586 caffe.cpp:313] Batch 828, accuracy/top5 = 0.74
I0817 10:58:42.823951 14586 caffe.cpp:313] Batch 828, loss = 2.06656
I0817 10:58:42.886934 14586 caffe.cpp:313] Batch 829, accuracy/top1 = 0.52
I0817 10:58:42.886955 14586 caffe.cpp:313] Batch 829, accuracy/top5 = 0.76
I0817 10:58:42.886957 14586 caffe.cpp:313] Batch 829, loss = 2.56524
I0817 10:58:42.949690 14586 caffe.cpp:313] Batch 830, accuracy/top1 = 0.68
I0817 10:58:42.949713 14586 caffe.cpp:313] Batch 830, accuracy/top5 = 0.84
I0817 10:58:42.949717 14586 caffe.cpp:313] Batch 830, loss = 1.49087
I0817 10:58:43.012537 14586 caffe.cpp:313] Batch 831, accuracy/top1 = 0.6
I0817 10:58:43.012557 14586 caffe.cpp:313] Batch 831, accuracy/top5 = 0.86
I0817 10:58:43.012560 14586 caffe.cpp:313] Batch 831, loss = 1.60146
I0817 10:58:43.075242 14586 caffe.cpp:313] Batch 832, accuracy/top1 = 0.44
I0817 10:58:43.075263 14586 caffe.cpp:313] Batch 832, accuracy/top5 = 0.72
I0817 10:58:43.075266 14586 caffe.cpp:313] Batch 832, loss = 2.21319
I0817 10:58:43.137991 14586 caffe.cpp:313] Batch 833, accuracy/top1 = 0.62
I0817 10:58:43.138012 14586 caffe.cpp:313] Batch 833, accuracy/top5 = 0.82
I0817 10:58:43.138015 14586 caffe.cpp:313] Batch 833, loss = 1.66249
I0817 10:58:43.200850 14586 caffe.cpp:313] Batch 834, accuracy/top1 = 0.62
I0817 10:58:43.200871 14586 caffe.cpp:313] Batch 834, accuracy/top5 = 0.8
I0817 10:58:43.200875 14586 caffe.cpp:313] Batch 834, loss = 2.0657
I0817 10:58:43.263762 14586 caffe.cpp:313] Batch 835, accuracy/top1 = 0.58
I0817 10:58:43.263778 14586 caffe.cpp:313] Batch 835, accuracy/top5 = 0.8
I0817 10:58:43.263782 14586 caffe.cpp:313] Batch 835, loss = 2.01355
I0817 10:58:43.326638 14586 caffe.cpp:313] Batch 836, accuracy/top1 = 0.66
I0817 10:58:43.326659 14586 caffe.cpp:313] Batch 836, accuracy/top5 = 0.84
I0817 10:58:43.326663 14586 caffe.cpp:313] Batch 836, loss = 1.48448
I0817 10:58:43.389466 14586 caffe.cpp:313] Batch 837, accuracy/top1 = 0.6
I0817 10:58:43.389487 14586 caffe.cpp:313] Batch 837, accuracy/top5 = 0.8
I0817 10:58:43.389489 14586 caffe.cpp:313] Batch 837, loss = 1.51126
I0817 10:58:43.452255 14586 caffe.cpp:313] Batch 838, accuracy/top1 = 0.48
I0817 10:58:43.452275 14586 caffe.cpp:313] Batch 838, accuracy/top5 = 0.82
I0817 10:58:43.452278 14586 caffe.cpp:313] Batch 838, loss = 1.8193
I0817 10:58:43.515120 14586 caffe.cpp:313] Batch 839, accuracy/top1 = 0.6
I0817 10:58:43.515141 14586 caffe.cpp:313] Batch 839, accuracy/top5 = 0.78
I0817 10:58:43.515143 14586 caffe.cpp:313] Batch 839, loss = 1.81161
I0817 10:58:43.577992 14586 caffe.cpp:313] Batch 840, accuracy/top1 = 0.56
I0817 10:58:43.578014 14586 caffe.cpp:313] Batch 840, accuracy/top5 = 0.76
I0817 10:58:43.578017 14586 caffe.cpp:313] Batch 840, loss = 2.00253
I0817 10:58:43.640707 14586 caffe.cpp:313] Batch 841, accuracy/top1 = 0.54
I0817 10:58:43.640728 14586 caffe.cpp:313] Batch 841, accuracy/top5 = 0.78
I0817 10:58:43.640732 14586 caffe.cpp:313] Batch 841, loss = 1.92266
I0817 10:58:43.703356 14586 caffe.cpp:313] Batch 842, accuracy/top1 = 0.6
I0817 10:58:43.703392 14586 caffe.cpp:313] Batch 842, accuracy/top5 = 0.86
I0817 10:58:43.703395 14586 caffe.cpp:313] Batch 842, loss = 1.54264
I0817 10:58:43.766037 14586 caffe.cpp:313] Batch 843, accuracy/top1 = 0.5
I0817 10:58:43.766059 14586 caffe.cpp:313] Batch 843, accuracy/top5 = 0.86
I0817 10:58:43.766062 14586 caffe.cpp:313] Batch 843, loss = 1.83261
I0817 10:58:43.828920 14586 caffe.cpp:313] Batch 844, accuracy/top1 = 0.62
I0817 10:58:43.828943 14586 caffe.cpp:313] Batch 844, accuracy/top5 = 0.86
I0817 10:58:43.828944 14586 caffe.cpp:313] Batch 844, loss = 1.5894
I0817 10:58:43.891877 14586 caffe.cpp:313] Batch 845, accuracy/top1 = 0.6
I0817 10:58:43.891898 14586 caffe.cpp:313] Batch 845, accuracy/top5 = 0.76
I0817 10:58:43.891901 14586 caffe.cpp:313] Batch 845, loss = 1.93434
I0817 10:58:43.954632 14586 caffe.cpp:313] Batch 846, accuracy/top1 = 0.54
I0817 10:58:43.954654 14586 caffe.cpp:313] Batch 846, accuracy/top5 = 0.82
I0817 10:58:43.954658 14586 caffe.cpp:313] Batch 846, loss = 1.77095
I0817 10:58:44.017315 14586 caffe.cpp:313] Batch 847, accuracy/top1 = 0.54
I0817 10:58:44.017338 14586 caffe.cpp:313] Batch 847, accuracy/top5 = 0.7
I0817 10:58:44.017340 14586 caffe.cpp:313] Batch 847, loss = 2.21639
I0817 10:58:44.080147 14586 caffe.cpp:313] Batch 848, accuracy/top1 = 0.58
I0817 10:58:44.080168 14586 caffe.cpp:313] Batch 848, accuracy/top5 = 0.84
I0817 10:58:44.080171 14586 caffe.cpp:313] Batch 848, loss = 1.74887
I0817 10:58:44.143007 14586 caffe.cpp:313] Batch 849, accuracy/top1 = 0.62
I0817 10:58:44.143029 14586 caffe.cpp:313] Batch 849, accuracy/top5 = 0.9
I0817 10:58:44.143033 14586 caffe.cpp:313] Batch 849, loss = 1.55699
I0817 10:58:44.205756 14586 caffe.cpp:313] Batch 850, accuracy/top1 = 0.6
I0817 10:58:44.205778 14586 caffe.cpp:313] Batch 850, accuracy/top5 = 0.82
I0817 10:58:44.205781 14586 caffe.cpp:313] Batch 850, loss = 1.7837
I0817 10:58:44.268467 14586 caffe.cpp:313] Batch 851, accuracy/top1 = 0.54
I0817 10:58:44.268491 14586 caffe.cpp:313] Batch 851, accuracy/top5 = 0.82
I0817 10:58:44.268493 14586 caffe.cpp:313] Batch 851, loss = 2.04098
I0817 10:58:44.331233 14586 caffe.cpp:313] Batch 852, accuracy/top1 = 0.54
I0817 10:58:44.331255 14586 caffe.cpp:313] Batch 852, accuracy/top5 = 0.78
I0817 10:58:44.331259 14586 caffe.cpp:313] Batch 852, loss = 2.03534
I0817 10:58:44.393941 14586 caffe.cpp:313] Batch 853, accuracy/top1 = 0.7
I0817 10:58:44.393962 14586 caffe.cpp:313] Batch 853, accuracy/top5 = 0.86
I0817 10:58:44.393965 14586 caffe.cpp:313] Batch 853, loss = 1.37425
I0817 10:58:44.456670 14586 caffe.cpp:313] Batch 854, accuracy/top1 = 0.66
I0817 10:58:44.456692 14586 caffe.cpp:313] Batch 854, accuracy/top5 = 0.82
I0817 10:58:44.456696 14586 caffe.cpp:313] Batch 854, loss = 1.58463
I0817 10:58:44.519395 14586 caffe.cpp:313] Batch 855, accuracy/top1 = 0.44
I0817 10:58:44.519417 14586 caffe.cpp:313] Batch 855, accuracy/top5 = 0.74
I0817 10:58:44.519419 14586 caffe.cpp:313] Batch 855, loss = 2.28102
I0817 10:58:44.582165 14586 caffe.cpp:313] Batch 856, accuracy/top1 = 0.5
I0817 10:58:44.582186 14586 caffe.cpp:313] Batch 856, accuracy/top5 = 0.8
I0817 10:58:44.582190 14586 caffe.cpp:313] Batch 856, loss = 2.01556
I0817 10:58:44.645056 14586 caffe.cpp:313] Batch 857, accuracy/top1 = 0.68
I0817 10:58:44.645078 14586 caffe.cpp:313] Batch 857, accuracy/top5 = 0.88
I0817 10:58:44.645081 14586 caffe.cpp:313] Batch 857, loss = 1.36542
I0817 10:58:44.707851 14586 caffe.cpp:313] Batch 858, accuracy/top1 = 0.56
I0817 10:58:44.707873 14586 caffe.cpp:313] Batch 858, accuracy/top5 = 0.9
I0817 10:58:44.707876 14586 caffe.cpp:313] Batch 858, loss = 1.42018
I0817 10:58:44.770617 14586 caffe.cpp:313] Batch 859, accuracy/top1 = 0.58
I0817 10:58:44.770638 14586 caffe.cpp:313] Batch 859, accuracy/top5 = 0.8
I0817 10:58:44.770642 14586 caffe.cpp:313] Batch 859, loss = 1.94232
I0817 10:58:44.833379 14586 caffe.cpp:313] Batch 860, accuracy/top1 = 0.6
I0817 10:58:44.833400 14586 caffe.cpp:313] Batch 860, accuracy/top5 = 0.78
I0817 10:58:44.833403 14586 caffe.cpp:313] Batch 860, loss = 1.68544
I0817 10:58:44.896545 14586 caffe.cpp:313] Batch 861, accuracy/top1 = 0.7
I0817 10:58:44.896566 14586 caffe.cpp:313] Batch 861, accuracy/top5 = 0.82
I0817 10:58:44.896569 14586 caffe.cpp:313] Batch 861, loss = 1.5526
I0817 10:58:44.959529 14586 caffe.cpp:313] Batch 862, accuracy/top1 = 0.56
I0817 10:58:44.959548 14586 caffe.cpp:313] Batch 862, accuracy/top5 = 0.88
I0817 10:58:44.959553 14586 caffe.cpp:313] Batch 862, loss = 1.63075
I0817 10:58:45.022363 14586 caffe.cpp:313] Batch 863, accuracy/top1 = 0.6
I0817 10:58:45.022382 14586 caffe.cpp:313] Batch 863, accuracy/top5 = 0.74
I0817 10:58:45.022385 14586 caffe.cpp:313] Batch 863, loss = 1.80999
I0817 10:58:45.085041 14586 caffe.cpp:313] Batch 864, accuracy/top1 = 0.64
I0817 10:58:45.085062 14586 caffe.cpp:313] Batch 864, accuracy/top5 = 0.88
I0817 10:58:45.085065 14586 caffe.cpp:313] Batch 864, loss = 1.48218
I0817 10:58:45.147840 14586 caffe.cpp:313] Batch 865, accuracy/top1 = 0.62
I0817 10:58:45.147862 14586 caffe.cpp:313] Batch 865, accuracy/top5 = 0.78
I0817 10:58:45.147866 14586 caffe.cpp:313] Batch 865, loss = 1.93583
I0817 10:58:45.210660 14586 caffe.cpp:313] Batch 866, accuracy/top1 = 0.6
I0817 10:58:45.210681 14586 caffe.cpp:313] Batch 866, accuracy/top5 = 0.86
I0817 10:58:45.210685 14586 caffe.cpp:313] Batch 866, loss = 1.46979
I0817 10:58:45.273437 14586 caffe.cpp:313] Batch 867, accuracy/top1 = 0.6
I0817 10:58:45.273459 14586 caffe.cpp:313] Batch 867, accuracy/top5 = 0.72
I0817 10:58:45.273463 14586 caffe.cpp:313] Batch 867, loss = 2.13258
I0817 10:58:45.336112 14586 caffe.cpp:313] Batch 868, accuracy/top1 = 0.56
I0817 10:58:45.336134 14586 caffe.cpp:313] Batch 868, accuracy/top5 = 0.78
I0817 10:58:45.336138 14586 caffe.cpp:313] Batch 868, loss = 1.98842
I0817 10:58:45.398766 14586 caffe.cpp:313] Batch 869, accuracy/top1 = 0.48
I0817 10:58:45.398788 14586 caffe.cpp:313] Batch 869, accuracy/top5 = 0.8
I0817 10:58:45.398792 14586 caffe.cpp:313] Batch 869, loss = 2.04823
I0817 10:58:45.461390 14586 caffe.cpp:313] Batch 870, accuracy/top1 = 0.52
I0817 10:58:45.461412 14586 caffe.cpp:313] Batch 870, accuracy/top5 = 0.72
I0817 10:58:45.461416 14586 caffe.cpp:313] Batch 870, loss = 2.2458
I0817 10:58:45.524160 14586 caffe.cpp:313] Batch 871, accuracy/top1 = 0.6
I0817 10:58:45.524183 14586 caffe.cpp:313] Batch 871, accuracy/top5 = 0.84
I0817 10:58:45.524185 14586 caffe.cpp:313] Batch 871, loss = 1.59699
I0817 10:58:45.587009 14586 caffe.cpp:313] Batch 872, accuracy/top1 = 0.54
I0817 10:58:45.587030 14586 caffe.cpp:313] Batch 872, accuracy/top5 = 0.8
I0817 10:58:45.587033 14586 caffe.cpp:313] Batch 872, loss = 1.78343
I0817 10:58:45.649739 14586 caffe.cpp:313] Batch 873, accuracy/top1 = 0.62
I0817 10:58:45.649761 14586 caffe.cpp:313] Batch 873, accuracy/top5 = 0.86
I0817 10:58:45.649765 14586 caffe.cpp:313] Batch 873, loss = 1.71179
I0817 10:58:45.712538 14586 caffe.cpp:313] Batch 874, accuracy/top1 = 0.68
I0817 10:58:45.712560 14586 caffe.cpp:313] Batch 874, accuracy/top5 = 0.82
I0817 10:58:45.712563 14586 caffe.cpp:313] Batch 874, loss = 1.60826
I0817 10:58:45.775305 14586 caffe.cpp:313] Batch 875, accuracy/top1 = 0.64
I0817 10:58:45.775326 14586 caffe.cpp:313] Batch 875, accuracy/top5 = 0.8
I0817 10:58:45.775329 14586 caffe.cpp:313] Batch 875, loss = 1.66203
I0817 10:58:45.838155 14586 caffe.cpp:313] Batch 876, accuracy/top1 = 0.64
I0817 10:58:45.838176 14586 caffe.cpp:313] Batch 876, accuracy/top5 = 0.84
I0817 10:58:45.838179 14586 caffe.cpp:313] Batch 876, loss = 1.40685
I0817 10:58:45.901198 14586 caffe.cpp:313] Batch 877, accuracy/top1 = 0.6
I0817 10:58:45.901217 14586 caffe.cpp:313] Batch 877, accuracy/top5 = 0.8
I0817 10:58:45.901221 14586 caffe.cpp:313] Batch 877, loss = 1.68611
I0817 10:58:45.963999 14586 caffe.cpp:313] Batch 878, accuracy/top1 = 0.52
I0817 10:58:45.964021 14586 caffe.cpp:313] Batch 878, accuracy/top5 = 0.72
I0817 10:58:45.964025 14586 caffe.cpp:313] Batch 878, loss = 2.19636
I0817 10:58:46.026785 14586 caffe.cpp:313] Batch 879, accuracy/top1 = 0.6
I0817 10:58:46.026803 14586 caffe.cpp:313] Batch 879, accuracy/top5 = 0.82
I0817 10:58:46.026806 14586 caffe.cpp:313] Batch 879, loss = 1.49428
I0817 10:58:46.089640 14586 caffe.cpp:313] Batch 880, accuracy/top1 = 0.66
I0817 10:58:46.089661 14586 caffe.cpp:313] Batch 880, accuracy/top5 = 0.84
I0817 10:58:46.089664 14586 caffe.cpp:313] Batch 880, loss = 1.4512
I0817 10:58:46.152462 14586 caffe.cpp:313] Batch 881, accuracy/top1 = 0.6
I0817 10:58:46.152482 14586 caffe.cpp:313] Batch 881, accuracy/top5 = 0.88
I0817 10:58:46.152485 14586 caffe.cpp:313] Batch 881, loss = 1.70183
I0817 10:58:46.215306 14586 caffe.cpp:313] Batch 882, accuracy/top1 = 0.62
I0817 10:58:46.215327 14586 caffe.cpp:313] Batch 882, accuracy/top5 = 0.82
I0817 10:58:46.215330 14586 caffe.cpp:313] Batch 882, loss = 1.37572
I0817 10:58:46.278184 14586 caffe.cpp:313] Batch 883, accuracy/top1 = 0.62
I0817 10:58:46.278205 14586 caffe.cpp:313] Batch 883, accuracy/top5 = 0.76
I0817 10:58:46.278208 14586 caffe.cpp:313] Batch 883, loss = 1.93394
I0817 10:58:46.341002 14586 caffe.cpp:313] Batch 884, accuracy/top1 = 0.44
I0817 10:58:46.341023 14586 caffe.cpp:313] Batch 884, accuracy/top5 = 0.8
I0817 10:58:46.341027 14586 caffe.cpp:313] Batch 884, loss = 1.97852
I0817 10:58:46.403738 14586 caffe.cpp:313] Batch 885, accuracy/top1 = 0.5
I0817 10:58:46.403759 14586 caffe.cpp:313] Batch 885, accuracy/top5 = 0.9
I0817 10:58:46.403761 14586 caffe.cpp:313] Batch 885, loss = 1.65015
I0817 10:58:46.466372 14586 caffe.cpp:313] Batch 886, accuracy/top1 = 0.62
I0817 10:58:46.466393 14586 caffe.cpp:313] Batch 886, accuracy/top5 = 0.84
I0817 10:58:46.466397 14586 caffe.cpp:313] Batch 886, loss = 1.71988
I0817 10:58:46.529228 14586 caffe.cpp:313] Batch 887, accuracy/top1 = 0.54
I0817 10:58:46.529250 14586 caffe.cpp:313] Batch 887, accuracy/top5 = 0.84
I0817 10:58:46.529253 14586 caffe.cpp:313] Batch 887, loss = 1.75883
I0817 10:58:46.592000 14586 caffe.cpp:313] Batch 888, accuracy/top1 = 0.58
I0817 10:58:46.592022 14586 caffe.cpp:313] Batch 888, accuracy/top5 = 0.82
I0817 10:58:46.592025 14586 caffe.cpp:313] Batch 888, loss = 1.90841
I0817 10:58:46.654731 14586 caffe.cpp:313] Batch 889, accuracy/top1 = 0.56
I0817 10:58:46.654753 14586 caffe.cpp:313] Batch 889, accuracy/top5 = 0.86
I0817 10:58:46.654757 14586 caffe.cpp:313] Batch 889, loss = 1.90385
I0817 10:58:46.717344 14586 caffe.cpp:313] Batch 890, accuracy/top1 = 0.5
I0817 10:58:46.717366 14586 caffe.cpp:313] Batch 890, accuracy/top5 = 0.76
I0817 10:58:46.717370 14586 caffe.cpp:313] Batch 890, loss = 2.09204
I0817 10:58:46.780232 14586 caffe.cpp:313] Batch 891, accuracy/top1 = 0.6
I0817 10:58:46.780254 14586 caffe.cpp:313] Batch 891, accuracy/top5 = 0.74
I0817 10:58:46.780257 14586 caffe.cpp:313] Batch 891, loss = 1.84054
I0817 10:58:46.843098 14586 caffe.cpp:313] Batch 892, accuracy/top1 = 0.54
I0817 10:58:46.843122 14586 caffe.cpp:313] Batch 892, accuracy/top5 = 0.8
I0817 10:58:46.843124 14586 caffe.cpp:313] Batch 892, loss = 1.82861
I0817 10:58:46.906033 14586 caffe.cpp:313] Batch 893, accuracy/top1 = 0.6
I0817 10:58:46.906054 14586 caffe.cpp:313] Batch 893, accuracy/top5 = 0.84
I0817 10:58:46.906057 14586 caffe.cpp:313] Batch 893, loss = 1.72719
I0817 10:58:46.968811 14586 caffe.cpp:313] Batch 894, accuracy/top1 = 0.46
I0817 10:58:46.968832 14586 caffe.cpp:313] Batch 894, accuracy/top5 = 0.72
I0817 10:58:46.968835 14586 caffe.cpp:313] Batch 894, loss = 2.27506
I0817 10:58:47.031672 14586 caffe.cpp:313] Batch 895, accuracy/top1 = 0.72
I0817 10:58:47.031690 14586 caffe.cpp:313] Batch 895, accuracy/top5 = 0.92
I0817 10:58:47.031693 14586 caffe.cpp:313] Batch 895, loss = 1.11229
I0817 10:58:47.094647 14586 caffe.cpp:313] Batch 896, accuracy/top1 = 0.52
I0817 10:58:47.094667 14586 caffe.cpp:313] Batch 896, accuracy/top5 = 0.74
I0817 10:58:47.094671 14586 caffe.cpp:313] Batch 896, loss = 1.79996
I0817 10:58:47.157531 14586 caffe.cpp:313] Batch 897, accuracy/top1 = 0.68
I0817 10:58:47.157553 14586 caffe.cpp:313] Batch 897, accuracy/top5 = 0.78
I0817 10:58:47.157557 14586 caffe.cpp:313] Batch 897, loss = 1.60354
I0817 10:58:47.220360 14586 caffe.cpp:313] Batch 898, accuracy/top1 = 0.64
I0817 10:58:47.220381 14586 caffe.cpp:313] Batch 898, accuracy/top5 = 0.86
I0817 10:58:47.220399 14586 caffe.cpp:313] Batch 898, loss = 1.50716
I0817 10:58:47.283228 14586 caffe.cpp:313] Batch 899, accuracy/top1 = 0.68
I0817 10:58:47.283249 14586 caffe.cpp:313] Batch 899, accuracy/top5 = 0.86
I0817 10:58:47.283252 14586 caffe.cpp:313] Batch 899, loss = 1.55591
I0817 10:58:47.346129 14586 caffe.cpp:313] Batch 900, accuracy/top1 = 0.62
I0817 10:58:47.346148 14586 caffe.cpp:313] Batch 900, accuracy/top5 = 0.86
I0817 10:58:47.346150 14586 caffe.cpp:313] Batch 900, loss = 1.34523
I0817 10:58:47.408812 14586 caffe.cpp:313] Batch 901, accuracy/top1 = 0.54
I0817 10:58:47.408833 14586 caffe.cpp:313] Batch 901, accuracy/top5 = 0.78
I0817 10:58:47.408836 14586 caffe.cpp:313] Batch 901, loss = 1.93475
I0817 10:58:47.471640 14586 caffe.cpp:313] Batch 902, accuracy/top1 = 0.56
I0817 10:58:47.471663 14586 caffe.cpp:313] Batch 902, accuracy/top5 = 0.86
I0817 10:58:47.471665 14586 caffe.cpp:313] Batch 902, loss = 1.67275
I0817 10:58:47.534413 14586 caffe.cpp:313] Batch 903, accuracy/top1 = 0.68
I0817 10:58:47.534435 14586 caffe.cpp:313] Batch 903, accuracy/top5 = 0.8
I0817 10:58:47.534438 14586 caffe.cpp:313] Batch 903, loss = 1.52738
I0817 10:58:47.597298 14586 caffe.cpp:313] Batch 904, accuracy/top1 = 0.7
I0817 10:58:47.597321 14586 caffe.cpp:313] Batch 904, accuracy/top5 = 0.86
I0817 10:58:47.597324 14586 caffe.cpp:313] Batch 904, loss = 1.56109
I0817 10:58:47.660179 14586 caffe.cpp:313] Batch 905, accuracy/top1 = 0.54
I0817 10:58:47.660202 14586 caffe.cpp:313] Batch 905, accuracy/top5 = 0.82
I0817 10:58:47.660204 14586 caffe.cpp:313] Batch 905, loss = 1.87972
I0817 10:58:47.723021 14586 caffe.cpp:313] Batch 906, accuracy/top1 = 0.6
I0817 10:58:47.723042 14586 caffe.cpp:313] Batch 906, accuracy/top5 = 0.8
I0817 10:58:47.723045 14586 caffe.cpp:313] Batch 906, loss = 1.65915
I0817 10:58:47.785797 14586 caffe.cpp:313] Batch 907, accuracy/top1 = 0.52
I0817 10:58:47.785820 14586 caffe.cpp:313] Batch 907, accuracy/top5 = 0.86
I0817 10:58:47.785822 14586 caffe.cpp:313] Batch 907, loss = 1.67052
I0817 10:58:47.848887 14586 caffe.cpp:313] Batch 908, accuracy/top1 = 0.6
I0817 10:58:47.848909 14586 caffe.cpp:313] Batch 908, accuracy/top5 = 0.86
I0817 10:58:47.848912 14586 caffe.cpp:313] Batch 908, loss = 1.53281
I0817 10:58:47.911906 14586 caffe.cpp:313] Batch 909, accuracy/top1 = 0.64
I0817 10:58:47.911926 14586 caffe.cpp:313] Batch 909, accuracy/top5 = 0.84
I0817 10:58:47.911929 14586 caffe.cpp:313] Batch 909, loss = 1.5357
I0817 10:58:47.974680 14586 caffe.cpp:313] Batch 910, accuracy/top1 = 0.54
I0817 10:58:47.974702 14586 caffe.cpp:313] Batch 910, accuracy/top5 = 0.76
I0817 10:58:47.974705 14586 caffe.cpp:313] Batch 910, loss = 2.21006
I0817 10:58:48.037390 14586 caffe.cpp:313] Batch 911, accuracy/top1 = 0.54
I0817 10:58:48.037408 14586 caffe.cpp:313] Batch 911, accuracy/top5 = 0.82
I0817 10:58:48.037411 14586 caffe.cpp:313] Batch 911, loss = 1.84634
I0817 10:58:48.100028 14586 caffe.cpp:313] Batch 912, accuracy/top1 = 0.6
I0817 10:58:48.100049 14586 caffe.cpp:313] Batch 912, accuracy/top5 = 0.76
I0817 10:58:48.100052 14586 caffe.cpp:313] Batch 912, loss = 1.72205
I0817 10:58:48.162778 14586 caffe.cpp:313] Batch 913, accuracy/top1 = 0.58
I0817 10:58:48.162799 14586 caffe.cpp:313] Batch 913, accuracy/top5 = 0.8
I0817 10:58:48.162802 14586 caffe.cpp:313] Batch 913, loss = 1.79877
I0817 10:58:48.225483 14586 caffe.cpp:313] Batch 914, accuracy/top1 = 0.64
I0817 10:58:48.225505 14586 caffe.cpp:313] Batch 914, accuracy/top5 = 0.8
I0817 10:58:48.225508 14586 caffe.cpp:313] Batch 914, loss = 1.87108
I0817 10:58:48.288164 14586 caffe.cpp:313] Batch 915, accuracy/top1 = 0.66
I0817 10:58:48.288187 14586 caffe.cpp:313] Batch 915, accuracy/top5 = 0.8
I0817 10:58:48.288189 14586 caffe.cpp:313] Batch 915, loss = 1.73449
I0817 10:58:48.353308 14586 caffe.cpp:313] Batch 916, accuracy/top1 = 0.68
I0817 10:58:48.353365 14586 caffe.cpp:313] Batch 916, accuracy/top5 = 0.86
I0817 10:58:48.353379 14586 caffe.cpp:313] Batch 916, loss = 1.52044
I0817 10:58:48.417798 14586 caffe.cpp:313] Batch 917, accuracy/top1 = 0.66
I0817 10:58:48.417861 14586 caffe.cpp:313] Batch 917, accuracy/top5 = 0.82
I0817 10:58:48.417874 14586 caffe.cpp:313] Batch 917, loss = 1.62252
I0817 10:58:48.480779 14586 caffe.cpp:313] Batch 918, accuracy/top1 = 0.58
I0817 10:58:48.480801 14586 caffe.cpp:313] Batch 918, accuracy/top5 = 0.76
I0817 10:58:48.480804 14586 caffe.cpp:313] Batch 918, loss = 1.84145
I0817 10:58:48.543618 14586 caffe.cpp:313] Batch 919, accuracy/top1 = 0.62
I0817 10:58:48.543642 14586 caffe.cpp:313] Batch 919, accuracy/top5 = 0.78
I0817 10:58:48.543643 14586 caffe.cpp:313] Batch 919, loss = 1.99044
I0817 10:58:48.606518 14586 caffe.cpp:313] Batch 920, accuracy/top1 = 0.64
I0817 10:58:48.606539 14586 caffe.cpp:313] Batch 920, accuracy/top5 = 0.8
I0817 10:58:48.606542 14586 caffe.cpp:313] Batch 920, loss = 1.81201
I0817 10:58:48.669291 14586 caffe.cpp:313] Batch 921, accuracy/top1 = 0.58
I0817 10:58:48.669312 14586 caffe.cpp:313] Batch 921, accuracy/top5 = 0.78
I0817 10:58:48.669315 14586 caffe.cpp:313] Batch 921, loss = 1.8894
I0817 10:58:48.732111 14586 caffe.cpp:313] Batch 922, accuracy/top1 = 0.72
I0817 10:58:48.732136 14586 caffe.cpp:313] Batch 922, accuracy/top5 = 0.9
I0817 10:58:48.732138 14586 caffe.cpp:313] Batch 922, loss = 1.17538
I0817 10:58:48.795006 14586 caffe.cpp:313] Batch 923, accuracy/top1 = 0.68
I0817 10:58:48.795027 14586 caffe.cpp:313] Batch 923, accuracy/top5 = 0.8
I0817 10:58:48.795030 14586 caffe.cpp:313] Batch 923, loss = 2.01277
I0817 10:58:48.858806 14586 caffe.cpp:313] Batch 924, accuracy/top1 = 0.6
I0817 10:58:48.858824 14586 caffe.cpp:313] Batch 924, accuracy/top5 = 0.88
I0817 10:58:48.858826 14586 caffe.cpp:313] Batch 924, loss = 1.56412
I0817 10:58:48.921655 14586 caffe.cpp:313] Batch 925, accuracy/top1 = 0.58
I0817 10:58:48.921676 14586 caffe.cpp:313] Batch 925, accuracy/top5 = 0.8
I0817 10:58:48.921679 14586 caffe.cpp:313] Batch 925, loss = 1.88526
I0817 10:58:48.984372 14586 caffe.cpp:313] Batch 926, accuracy/top1 = 0.66
I0817 10:58:48.984395 14586 caffe.cpp:313] Batch 926, accuracy/top5 = 0.8
I0817 10:58:48.984398 14586 caffe.cpp:313] Batch 926, loss = 1.99104
I0817 10:58:49.047222 14586 caffe.cpp:313] Batch 927, accuracy/top1 = 0.54
I0817 10:58:49.047240 14586 caffe.cpp:313] Batch 927, accuracy/top5 = 0.72
I0817 10:58:49.047242 14586 caffe.cpp:313] Batch 927, loss = 2.15183
I0817 10:58:49.110072 14586 caffe.cpp:313] Batch 928, accuracy/top1 = 0.7
I0817 10:58:49.110091 14586 caffe.cpp:313] Batch 928, accuracy/top5 = 0.88
I0817 10:58:49.110095 14586 caffe.cpp:313] Batch 928, loss = 1.35411
I0817 10:58:49.173070 14586 caffe.cpp:313] Batch 929, accuracy/top1 = 0.54
I0817 10:58:49.173091 14586 caffe.cpp:313] Batch 929, accuracy/top5 = 0.84
I0817 10:58:49.173094 14586 caffe.cpp:313] Batch 929, loss = 1.59755
I0817 10:58:49.236062 14586 caffe.cpp:313] Batch 930, accuracy/top1 = 0.64
I0817 10:58:49.236081 14586 caffe.cpp:313] Batch 930, accuracy/top5 = 0.78
I0817 10:58:49.236084 14586 caffe.cpp:313] Batch 930, loss = 1.75479
I0817 10:58:49.298992 14586 caffe.cpp:313] Batch 931, accuracy/top1 = 0.54
I0817 10:58:49.299015 14586 caffe.cpp:313] Batch 931, accuracy/top5 = 0.82
I0817 10:58:49.299017 14586 caffe.cpp:313] Batch 931, loss = 2.24914
I0817 10:58:49.361852 14586 caffe.cpp:313] Batch 932, accuracy/top1 = 0.64
I0817 10:58:49.361870 14586 caffe.cpp:313] Batch 932, accuracy/top5 = 0.82
I0817 10:58:49.361872 14586 caffe.cpp:313] Batch 932, loss = 1.59453
I0817 10:58:49.424556 14586 caffe.cpp:313] Batch 933, accuracy/top1 = 0.64
I0817 10:58:49.424578 14586 caffe.cpp:313] Batch 933, accuracy/top5 = 0.8
I0817 10:58:49.424582 14586 caffe.cpp:313] Batch 933, loss = 2.06305
I0817 10:58:49.487350 14586 caffe.cpp:313] Batch 934, accuracy/top1 = 0.6
I0817 10:58:49.487372 14586 caffe.cpp:313] Batch 934, accuracy/top5 = 0.74
I0817 10:58:49.487375 14586 caffe.cpp:313] Batch 934, loss = 1.94022
I0817 10:58:49.550235 14586 caffe.cpp:313] Batch 935, accuracy/top1 = 0.64
I0817 10:58:49.550256 14586 caffe.cpp:313] Batch 935, accuracy/top5 = 0.78
I0817 10:58:49.550259 14586 caffe.cpp:313] Batch 935, loss = 2.09436
I0817 10:58:49.613138 14586 caffe.cpp:313] Batch 936, accuracy/top1 = 0.64
I0817 10:58:49.613159 14586 caffe.cpp:313] Batch 936, accuracy/top5 = 0.86
I0817 10:58:49.613162 14586 caffe.cpp:313] Batch 936, loss = 1.61705
I0817 10:58:49.675952 14586 caffe.cpp:313] Batch 937, accuracy/top1 = 0.58
I0817 10:58:49.675974 14586 caffe.cpp:313] Batch 937, accuracy/top5 = 0.84
I0817 10:58:49.675977 14586 caffe.cpp:313] Batch 937, loss = 1.77043
I0817 10:58:49.738811 14586 caffe.cpp:313] Batch 938, accuracy/top1 = 0.54
I0817 10:58:49.738833 14586 caffe.cpp:313] Batch 938, accuracy/top5 = 0.8
I0817 10:58:49.738837 14586 caffe.cpp:313] Batch 938, loss = 1.99112
I0817 10:58:49.801579 14586 caffe.cpp:313] Batch 939, accuracy/top1 = 0.56
I0817 10:58:49.801601 14586 caffe.cpp:313] Batch 939, accuracy/top5 = 0.78
I0817 10:58:49.801604 14586 caffe.cpp:313] Batch 939, loss = 1.88508
I0817 10:58:49.865396 14586 caffe.cpp:313] Batch 940, accuracy/top1 = 0.56
I0817 10:58:49.865413 14586 caffe.cpp:313] Batch 940, accuracy/top5 = 0.8
I0817 10:58:49.865417 14586 caffe.cpp:313] Batch 940, loss = 1.85787
I0817 10:58:49.928237 14586 caffe.cpp:313] Batch 941, accuracy/top1 = 0.58
I0817 10:58:49.928257 14586 caffe.cpp:313] Batch 941, accuracy/top5 = 0.78
I0817 10:58:49.928261 14586 caffe.cpp:313] Batch 941, loss = 1.94919
I0817 10:58:49.991107 14586 caffe.cpp:313] Batch 942, accuracy/top1 = 0.52
I0817 10:58:49.991130 14586 caffe.cpp:313] Batch 942, accuracy/top5 = 0.76
I0817 10:58:49.991132 14586 caffe.cpp:313] Batch 942, loss = 2.12031
I0817 10:58:50.053767 14586 caffe.cpp:313] Batch 943, accuracy/top1 = 0.58
I0817 10:58:50.053786 14586 caffe.cpp:313] Batch 943, accuracy/top5 = 0.82
I0817 10:58:50.053789 14586 caffe.cpp:313] Batch 943, loss = 1.62223
I0817 10:58:50.116482 14586 caffe.cpp:313] Batch 944, accuracy/top1 = 0.58
I0817 10:58:50.116503 14586 caffe.cpp:313] Batch 944, accuracy/top5 = 0.84
I0817 10:58:50.116506 14586 caffe.cpp:313] Batch 944, loss = 1.50576
I0817 10:58:50.179129 14586 caffe.cpp:313] Batch 945, accuracy/top1 = 0.58
I0817 10:58:50.179221 14586 caffe.cpp:313] Batch 945, accuracy/top5 = 0.84
I0817 10:58:50.179226 14586 caffe.cpp:313] Batch 945, loss = 1.99001
I0817 10:58:50.241988 14586 caffe.cpp:313] Batch 946, accuracy/top1 = 0.62
I0817 10:58:50.242008 14586 caffe.cpp:313] Batch 946, accuracy/top5 = 0.78
I0817 10:58:50.242012 14586 caffe.cpp:313] Batch 946, loss = 1.91883
I0817 10:58:50.304903 14586 caffe.cpp:313] Batch 947, accuracy/top1 = 0.64
I0817 10:58:50.304925 14586 caffe.cpp:313] Batch 947, accuracy/top5 = 0.88
I0817 10:58:50.304929 14586 caffe.cpp:313] Batch 947, loss = 1.29706
I0817 10:58:50.367653 14586 caffe.cpp:313] Batch 948, accuracy/top1 = 0.5
I0817 10:58:50.367676 14586 caffe.cpp:313] Batch 948, accuracy/top5 = 0.74
I0817 10:58:50.367679 14586 caffe.cpp:313] Batch 948, loss = 2.28255
I0817 10:58:50.430518 14586 caffe.cpp:313] Batch 949, accuracy/top1 = 0.66
I0817 10:58:50.430541 14586 caffe.cpp:313] Batch 949, accuracy/top5 = 0.86
I0817 10:58:50.430542 14586 caffe.cpp:313] Batch 949, loss = 1.34075
I0817 10:58:50.493412 14586 caffe.cpp:313] Batch 950, accuracy/top1 = 0.64
I0817 10:58:50.493433 14586 caffe.cpp:313] Batch 950, accuracy/top5 = 0.76
I0817 10:58:50.493436 14586 caffe.cpp:313] Batch 950, loss = 2.05328
I0817 10:58:50.556176 14586 caffe.cpp:313] Batch 951, accuracy/top1 = 0.72
I0817 10:58:50.556198 14586 caffe.cpp:313] Batch 951, accuracy/top5 = 0.88
I0817 10:58:50.556201 14586 caffe.cpp:313] Batch 951, loss = 1.21205
I0817 10:58:50.618787 14586 caffe.cpp:313] Batch 952, accuracy/top1 = 0.7
I0817 10:58:50.618808 14586 caffe.cpp:313] Batch 952, accuracy/top5 = 0.82
I0817 10:58:50.618811 14586 caffe.cpp:313] Batch 952, loss = 2.07632
I0817 10:58:50.681668 14586 caffe.cpp:313] Batch 953, accuracy/top1 = 0.52
I0817 10:58:50.681689 14586 caffe.cpp:313] Batch 953, accuracy/top5 = 0.74
I0817 10:58:50.681692 14586 caffe.cpp:313] Batch 953, loss = 2.23308
I0817 10:58:50.744520 14586 caffe.cpp:313] Batch 954, accuracy/top1 = 0.5
I0817 10:58:50.744544 14586 caffe.cpp:313] Batch 954, accuracy/top5 = 0.74
I0817 10:58:50.744546 14586 caffe.cpp:313] Batch 954, loss = 2.27375
I0817 10:58:50.807327 14586 caffe.cpp:313] Batch 955, accuracy/top1 = 0.54
I0817 10:58:50.807349 14586 caffe.cpp:313] Batch 955, accuracy/top5 = 0.76
I0817 10:58:50.807353 14586 caffe.cpp:313] Batch 955, loss = 2.15143
I0817 10:58:50.870318 14586 caffe.cpp:313] Batch 956, accuracy/top1 = 0.56
I0817 10:58:50.870335 14586 caffe.cpp:313] Batch 956, accuracy/top5 = 0.84
I0817 10:58:50.870338 14586 caffe.cpp:313] Batch 956, loss = 1.92893
I0817 10:58:50.933143 14586 caffe.cpp:313] Batch 957, accuracy/top1 = 0.52
I0817 10:58:50.933164 14586 caffe.cpp:313] Batch 957, accuracy/top5 = 0.8
I0817 10:58:50.933167 14586 caffe.cpp:313] Batch 957, loss = 2.10975
I0817 10:58:50.995998 14586 caffe.cpp:313] Batch 958, accuracy/top1 = 0.6
I0817 10:58:50.996021 14586 caffe.cpp:313] Batch 958, accuracy/top5 = 0.88
I0817 10:58:50.996023 14586 caffe.cpp:313] Batch 958, loss = 1.62717
I0817 10:58:51.058786 14586 caffe.cpp:313] Batch 959, accuracy/top1 = 0.54
I0817 10:58:51.058806 14586 caffe.cpp:313] Batch 959, accuracy/top5 = 0.82
I0817 10:58:51.058809 14586 caffe.cpp:313] Batch 959, loss = 1.95643
I0817 10:58:51.121409 14586 caffe.cpp:313] Batch 960, accuracy/top1 = 0.64
I0817 10:58:51.121430 14586 caffe.cpp:313] Batch 960, accuracy/top5 = 0.84
I0817 10:58:51.121433 14586 caffe.cpp:313] Batch 960, loss = 1.75115
I0817 10:58:51.184216 14586 caffe.cpp:313] Batch 961, accuracy/top1 = 0.5
I0817 10:58:51.184237 14586 caffe.cpp:313] Batch 961, accuracy/top5 = 0.74
I0817 10:58:51.184240 14586 caffe.cpp:313] Batch 961, loss = 2.35058
I0817 10:58:51.247143 14586 caffe.cpp:313] Batch 962, accuracy/top1 = 0.6
I0817 10:58:51.247160 14586 caffe.cpp:313] Batch 962, accuracy/top5 = 0.88
I0817 10:58:51.247164 14586 caffe.cpp:313] Batch 962, loss = 1.39396
I0817 10:58:51.310148 14586 caffe.cpp:313] Batch 963, accuracy/top1 = 0.48
I0817 10:58:51.310168 14586 caffe.cpp:313] Batch 963, accuracy/top5 = 0.76
I0817 10:58:51.310170 14586 caffe.cpp:313] Batch 963, loss = 2.02668
I0817 10:58:51.373121 14586 caffe.cpp:313] Batch 964, accuracy/top1 = 0.6
I0817 10:58:51.373148 14586 caffe.cpp:313] Batch 964, accuracy/top5 = 0.8
I0817 10:58:51.373152 14586 caffe.cpp:313] Batch 964, loss = 1.82539
I0817 10:58:51.435856 14586 caffe.cpp:313] Batch 965, accuracy/top1 = 0.52
I0817 10:58:51.435875 14586 caffe.cpp:313] Batch 965, accuracy/top5 = 0.76
I0817 10:58:51.435878 14586 caffe.cpp:313] Batch 965, loss = 2.0132
I0817 10:58:51.498615 14586 caffe.cpp:313] Batch 966, accuracy/top1 = 0.54
I0817 10:58:51.498636 14586 caffe.cpp:313] Batch 966, accuracy/top5 = 0.82
I0817 10:58:51.498639 14586 caffe.cpp:313] Batch 966, loss = 2.0503
I0817 10:58:51.561522 14586 caffe.cpp:313] Batch 967, accuracy/top1 = 0.52
I0817 10:58:51.561542 14586 caffe.cpp:313] Batch 967, accuracy/top5 = 0.82
I0817 10:58:51.561545 14586 caffe.cpp:313] Batch 967, loss = 1.89443
I0817 10:58:51.624234 14586 caffe.cpp:313] Batch 968, accuracy/top1 = 0.58
I0817 10:58:51.624255 14586 caffe.cpp:313] Batch 968, accuracy/top5 = 0.8
I0817 10:58:51.624258 14586 caffe.cpp:313] Batch 968, loss = 2.10617
I0817 10:58:51.687011 14586 caffe.cpp:313] Batch 969, accuracy/top1 = 0.52
I0817 10:58:51.687031 14586 caffe.cpp:313] Batch 969, accuracy/top5 = 0.8
I0817 10:58:51.687034 14586 caffe.cpp:313] Batch 969, loss = 1.73607
I0817 10:58:51.749780 14586 caffe.cpp:313] Batch 970, accuracy/top1 = 0.64
I0817 10:58:51.749802 14586 caffe.cpp:313] Batch 970, accuracy/top5 = 0.82
I0817 10:58:51.749805 14586 caffe.cpp:313] Batch 970, loss = 1.60022
I0817 10:58:51.812567 14586 caffe.cpp:313] Batch 971, accuracy/top1 = 0.64
I0817 10:58:51.812588 14586 caffe.cpp:313] Batch 971, accuracy/top5 = 0.9
I0817 10:58:51.812592 14586 caffe.cpp:313] Batch 971, loss = 1.50805
I0817 10:58:51.875651 14586 caffe.cpp:313] Batch 972, accuracy/top1 = 0.62
I0817 10:58:51.875669 14586 caffe.cpp:313] Batch 972, accuracy/top5 = 0.9
I0817 10:58:51.875671 14586 caffe.cpp:313] Batch 972, loss = 1.49859
I0817 10:58:51.938303 14586 caffe.cpp:313] Batch 973, accuracy/top1 = 0.44
I0817 10:58:51.938325 14586 caffe.cpp:313] Batch 973, accuracy/top5 = 0.74
I0817 10:58:51.938328 14586 caffe.cpp:313] Batch 973, loss = 2.37363
I0817 10:58:52.000998 14586 caffe.cpp:313] Batch 974, accuracy/top1 = 0.56
I0817 10:58:52.001019 14586 caffe.cpp:313] Batch 974, accuracy/top5 = 0.82
I0817 10:58:52.001022 14586 caffe.cpp:313] Batch 974, loss = 1.78066
I0817 10:58:52.063673 14586 caffe.cpp:313] Batch 975, accuracy/top1 = 0.66
I0817 10:58:52.063693 14586 caffe.cpp:313] Batch 975, accuracy/top5 = 0.8
I0817 10:58:52.063696 14586 caffe.cpp:313] Batch 975, loss = 1.65167
I0817 10:58:52.126536 14586 caffe.cpp:313] Batch 976, accuracy/top1 = 0.54
I0817 10:58:52.126557 14586 caffe.cpp:313] Batch 976, accuracy/top5 = 0.76
I0817 10:58:52.126560 14586 caffe.cpp:313] Batch 976, loss = 1.98399
I0817 10:58:52.189338 14586 caffe.cpp:313] Batch 977, accuracy/top1 = 0.56
I0817 10:58:52.189359 14586 caffe.cpp:313] Batch 977, accuracy/top5 = 0.8
I0817 10:58:52.189363 14586 caffe.cpp:313] Batch 977, loss = 1.86095
I0817 10:58:52.252046 14586 caffe.cpp:313] Batch 978, accuracy/top1 = 0.54
I0817 10:58:52.252068 14586 caffe.cpp:313] Batch 978, accuracy/top5 = 0.76
I0817 10:58:52.252071 14586 caffe.cpp:313] Batch 978, loss = 2.17502
I0817 10:58:52.314723 14586 caffe.cpp:313] Batch 979, accuracy/top1 = 0.6
I0817 10:58:52.314744 14586 caffe.cpp:313] Batch 979, accuracy/top5 = 0.82
I0817 10:58:52.314748 14586 caffe.cpp:313] Batch 979, loss = 1.75383
I0817 10:58:52.377467 14586 caffe.cpp:313] Batch 980, accuracy/top1 = 0.54
I0817 10:58:52.377490 14586 caffe.cpp:313] Batch 980, accuracy/top5 = 0.74
I0817 10:58:52.377491 14586 caffe.cpp:313] Batch 980, loss = 2.23879
I0817 10:58:52.440052 14586 caffe.cpp:313] Batch 981, accuracy/top1 = 0.52
I0817 10:58:52.440075 14586 caffe.cpp:313] Batch 981, accuracy/top5 = 0.74
I0817 10:58:52.440078 14586 caffe.cpp:313] Batch 981, loss = 2.11889
I0817 10:58:52.502715 14586 caffe.cpp:313] Batch 982, accuracy/top1 = 0.56
I0817 10:58:52.502737 14586 caffe.cpp:313] Batch 982, accuracy/top5 = 0.68
I0817 10:58:52.502740 14586 caffe.cpp:313] Batch 982, loss = 2.62231
I0817 10:58:52.565426 14586 caffe.cpp:313] Batch 983, accuracy/top1 = 0.48
I0817 10:58:52.565449 14586 caffe.cpp:313] Batch 983, accuracy/top5 = 0.78
I0817 10:58:52.565452 14586 caffe.cpp:313] Batch 983, loss = 2.38975
I0817 10:58:52.628254 14586 caffe.cpp:313] Batch 984, accuracy/top1 = 0.7
I0817 10:58:52.628275 14586 caffe.cpp:313] Batch 984, accuracy/top5 = 0.9
I0817 10:58:52.628278 14586 caffe.cpp:313] Batch 984, loss = 1.36328
I0817 10:58:52.691151 14586 caffe.cpp:313] Batch 985, accuracy/top1 = 0.62
I0817 10:58:52.691174 14586 caffe.cpp:313] Batch 985, accuracy/top5 = 0.86
I0817 10:58:52.691176 14586 caffe.cpp:313] Batch 985, loss = 1.56022
I0817 10:58:52.753976 14586 caffe.cpp:313] Batch 986, accuracy/top1 = 0.56
I0817 10:58:52.753998 14586 caffe.cpp:313] Batch 986, accuracy/top5 = 0.78
I0817 10:58:52.754001 14586 caffe.cpp:313] Batch 986, loss = 2.09855
I0817 10:58:52.816705 14586 caffe.cpp:313] Batch 987, accuracy/top1 = 0.52
I0817 10:58:52.816727 14586 caffe.cpp:313] Batch 987, accuracy/top5 = 0.78
I0817 10:58:52.816730 14586 caffe.cpp:313] Batch 987, loss = 1.89621
I0817 10:58:52.879941 14586 caffe.cpp:313] Batch 988, accuracy/top1 = 0.54
I0817 10:58:52.879958 14586 caffe.cpp:313] Batch 988, accuracy/top5 = 0.84
I0817 10:58:52.879961 14586 caffe.cpp:313] Batch 988, loss = 1.77873
I0817 10:58:52.942739 14586 caffe.cpp:313] Batch 989, accuracy/top1 = 0.46
I0817 10:58:52.942761 14586 caffe.cpp:313] Batch 989, accuracy/top5 = 0.76
I0817 10:58:52.942764 14586 caffe.cpp:313] Batch 989, loss = 1.94277
I0817 10:58:53.005568 14586 caffe.cpp:313] Batch 990, accuracy/top1 = 0.62
I0817 10:58:53.005590 14586 caffe.cpp:313] Batch 990, accuracy/top5 = 0.86
I0817 10:58:53.005594 14586 caffe.cpp:313] Batch 990, loss = 1.31043
I0817 10:58:53.068217 14586 caffe.cpp:313] Batch 991, accuracy/top1 = 0.66
I0817 10:58:53.068239 14586 caffe.cpp:313] Batch 991, accuracy/top5 = 0.86
I0817 10:58:53.068243 14586 caffe.cpp:313] Batch 991, loss = 1.64038
I0817 10:58:53.130904 14586 caffe.cpp:313] Batch 992, accuracy/top1 = 0.62
I0817 10:58:53.130928 14586 caffe.cpp:313] Batch 992, accuracy/top5 = 0.82
I0817 10:58:53.130929 14586 caffe.cpp:313] Batch 992, loss = 1.3797
I0817 10:58:53.193645 14586 caffe.cpp:313] Batch 993, accuracy/top1 = 0.68
I0817 10:58:53.193667 14586 caffe.cpp:313] Batch 993, accuracy/top5 = 0.86
I0817 10:58:53.193670 14586 caffe.cpp:313] Batch 993, loss = 1.32431
I0817 10:58:53.256427 14586 caffe.cpp:313] Batch 994, accuracy/top1 = 0.68
I0817 10:58:53.256445 14586 caffe.cpp:313] Batch 994, accuracy/top5 = 0.92
I0817 10:58:53.256448 14586 caffe.cpp:313] Batch 994, loss = 1.3607
I0817 10:58:53.319211 14586 caffe.cpp:313] Batch 995, accuracy/top1 = 0.64
I0817 10:58:53.319233 14586 caffe.cpp:313] Batch 995, accuracy/top5 = 0.82
I0817 10:58:53.319236 14586 caffe.cpp:313] Batch 995, loss = 1.73583
I0817 10:58:53.382035 14586 caffe.cpp:313] Batch 996, accuracy/top1 = 0.56
I0817 10:58:53.382055 14586 caffe.cpp:313] Batch 996, accuracy/top5 = 0.8
I0817 10:58:53.382057 14586 caffe.cpp:313] Batch 996, loss = 1.73074
I0817 10:58:53.432507 14619 data_reader.cpp:288] Starting prefetch of epoch 1
I0817 10:58:53.447413 14586 caffe.cpp:313] Batch 997, accuracy/top1 = 0.64
I0817 10:58:53.447438 14586 caffe.cpp:313] Batch 997, accuracy/top5 = 0.86
I0817 10:58:53.447443 14586 caffe.cpp:313] Batch 997, loss = 1.71568
I0817 10:58:53.510890 14586 caffe.cpp:313] Batch 998, accuracy/top1 = 0.48
I0817 10:58:53.510910 14586 caffe.cpp:313] Batch 998, accuracy/top5 = 0.7
I0817 10:58:53.510912 14586 caffe.cpp:313] Batch 998, loss = 2.13354
I0817 10:58:53.574291 14586 caffe.cpp:313] Batch 999, accuracy/top1 = 0.74
I0817 10:58:53.574313 14586 caffe.cpp:313] Batch 999, accuracy/top5 = 0.88
I0817 10:58:53.574316 14586 caffe.cpp:313] Batch 999, loss = 1.38884
I0817 10:58:53.574319 14586 caffe.cpp:318] Loss: 1.79763
I0817 10:58:53.574327 14586 caffe.cpp:330] accuracy/top1 = 0.582959
I0817 10:58:53.574331 14586 caffe.cpp:330] accuracy/top5 = 0.812541
I0817 10:58:53.574335 14586 caffe.cpp:330] loss = 1.79763 (* 1 = 1.79763 loss)
