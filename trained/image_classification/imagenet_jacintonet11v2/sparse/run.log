I0815 09:50:27.824497  8671 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 09:50:27 2017
I0815 09:50:27.824926  8671 caffe.cpp:611] CuDNN version: 6021
I0815 09:50:27.824932  8671 caffe.cpp:612] CuBLAS version: 8000
I0815 09:50:27.824934  8671 caffe.cpp:613] CUDA version: 8000
I0815 09:50:27.824935  8671 caffe.cpp:614] CUDA driver version: 8000
I0815 09:50:28.095011  8671 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 09:50:28.095633  8671 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 09:50:28.096201  8671 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 09:50:28.096774  8671 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 09:50:28.096791  8671 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 09:50:28.097122  8671 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 09:50:28.097453  8671 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 09:50:28.097784  8671 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 09:50:28.098271  8671 solver.cpp:42] Solver data type: FLOAT
I0815 09:50:28.098320  8671 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/train.prototxt"
test_net: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt"
test_iter: 1000
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 160000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 2
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 20000
sparsity_start_factor: 0
I0815 09:50:28.117274  8671 solver.cpp:77] Creating training net from train_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/train.prototxt
I0815 09:50:28.117895  8671 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 09:50:28.117902  8671 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 09:50:28.117930  8671 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:28.118649  8671 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_train_lmdb"
    batch_size: 43
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0815 09:50:28.118782  8671 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:28.118788  8671 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:28.118791  8671 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0815 09:50:28.118796  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.134377  8671 net.cpp:184] Created Layer data (0)
I0815 09:50:28.134393  8671 net.cpp:530] data -> data
I0815 09:50:28.134407  8671 net.cpp:530] data -> label
I0815 09:50:28.134431  8671 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:28.134454  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.175618  8726 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:28.177450  8671 data_layer.cpp:185] [0] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:28.177513  8671 data_layer.cpp:209] [0] Output data size: 43, 3, 224, 224
I0815 09:50:28.177520  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.177542  8671 net.cpp:245] Setting up data
I0815 09:50:28.177551  8671 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 3 224 224 (6472704)
I0815 09:50:28.177556  8671 net.cpp:252] TRAIN Top shape for layer 0 'data' 43 (43)
I0815 09:50:28.177564  8671 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 09:50:28.177570  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.177588  8671 net.cpp:184] Created Layer data/bias (1)
I0815 09:50:28.177593  8671 net.cpp:561] data/bias <- data
I0815 09:50:28.177603  8671 net.cpp:530] data/bias -> data/bias
I0815 09:50:28.179690  8671 net.cpp:245] Setting up data/bias
I0815 09:50:28.179702  8671 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 43 3 224 224 (6472704)
I0815 09:50:28.179713  8671 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 09:50:28.179718  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.179733  8671 net.cpp:184] Created Layer conv1a (2)
I0815 09:50:28.179736  8671 net.cpp:561] conv1a <- data/bias
I0815 09:50:28.179741  8671 net.cpp:530] conv1a -> conv1a
I0815 09:50:28.764436  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.97G, req 0G)
I0815 09:50:28.764458  8671 net.cpp:245] Setting up conv1a
I0815 09:50:28.764467  8671 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 43 32 112 112 (17260544)
I0815 09:50:28.764477  8671 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 09:50:28.764482  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.764495  8671 net.cpp:184] Created Layer conv1a/bn (3)
I0815 09:50:28.764500  8671 net.cpp:561] conv1a/bn <- conv1a
I0815 09:50:28.764506  8671 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 09:50:28.765156  8671 net.cpp:245] Setting up conv1a/bn
I0815 09:50:28.765166  8671 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 43 32 112 112 (17260544)
I0815 09:50:28.765175  8671 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 09:50:28.765179  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.765187  8671 net.cpp:184] Created Layer conv1a/relu (4)
I0815 09:50:28.765190  8671 net.cpp:561] conv1a/relu <- conv1a
I0815 09:50:28.765194  8671 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 09:50:28.765209  8671 net.cpp:245] Setting up conv1a/relu
I0815 09:50:28.765214  8671 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 43 32 112 112 (17260544)
I0815 09:50:28.765218  8671 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 09:50:28.765223  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.765233  8671 net.cpp:184] Created Layer conv1b (5)
I0815 09:50:28.765235  8671 net.cpp:561] conv1b <- conv1a
I0815 09:50:28.765240  8671 net.cpp:530] conv1b -> conv1b
I0815 09:50:28.790783  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 09:50:28.790796  8671 net.cpp:245] Setting up conv1b
I0815 09:50:28.790807  8671 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 43 32 112 112 (17260544)
I0815 09:50:28.790814  8671 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 09:50:28.790827  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.790835  8671 net.cpp:184] Created Layer conv1b/bn (6)
I0815 09:50:28.790839  8671 net.cpp:561] conv1b/bn <- conv1b
I0815 09:50:28.790843  8671 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 09:50:28.791451  8671 net.cpp:245] Setting up conv1b/bn
I0815 09:50:28.791460  8671 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 43 32 112 112 (17260544)
I0815 09:50:28.791468  8671 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 09:50:28.791472  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791477  8671 net.cpp:184] Created Layer conv1b/relu (7)
I0815 09:50:28.791481  8671 net.cpp:561] conv1b/relu <- conv1b
I0815 09:50:28.791486  8671 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 09:50:28.791491  8671 net.cpp:245] Setting up conv1b/relu
I0815 09:50:28.791496  8671 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 43 32 112 112 (17260544)
I0815 09:50:28.791501  8671 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 09:50:28.791504  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791512  8671 net.cpp:184] Created Layer pool1 (8)
I0815 09:50:28.791517  8671 net.cpp:561] pool1 <- conv1b
I0815 09:50:28.791520  8671 net.cpp:530] pool1 -> pool1
I0815 09:50:28.791592  8671 net.cpp:245] Setting up pool1
I0815 09:50:28.791599  8671 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 43 32 56 56 (4315136)
I0815 09:50:28.791602  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 09:50:28.791607  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.791617  8671 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 09:50:28.791620  8671 net.cpp:561] res2a_branch2a <- pool1
I0815 09:50:28.791625  8671 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 09:50:28.816090  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.71G, req 0G)
I0815 09:50:28.816108  8671 net.cpp:245] Setting up res2a_branch2a
I0815 09:50:28.816115  8671 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 43 64 56 56 (8630272)
I0815 09:50:28.816126  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.816138  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816148  8671 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 09:50:28.816153  8671 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 09:50:28.816157  8671 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 09:50:28.816810  8671 net.cpp:245] Setting up res2a_branch2a/bn
I0815 09:50:28.816818  8671 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 43 64 56 56 (8630272)
I0815 09:50:28.816828  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.816831  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816838  8671 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 09:50:28.816840  8671 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 09:50:28.816844  8671 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 09:50:28.816851  8671 net.cpp:245] Setting up res2a_branch2a/relu
I0815 09:50:28.816856  8671 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 43 64 56 56 (8630272)
I0815 09:50:28.816860  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 09:50:28.816865  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.816875  8671 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 09:50:28.816879  8671 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 09:50:28.816882  8671 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 09:50:28.828797  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.64G, req 0G)
I0815 09:50:28.828821  8671 net.cpp:245] Setting up res2a_branch2b
I0815 09:50:28.828829  8671 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 43 64 56 56 (8630272)
I0815 09:50:28.828837  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.828842  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.828855  8671 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 09:50:28.828860  8671 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 09:50:28.828866  8671 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 09:50:28.830344  8671 net.cpp:245] Setting up res2a_branch2b/bn
I0815 09:50:28.830355  8671 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 43 64 56 56 (8630272)
I0815 09:50:28.830364  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.830368  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830374  8671 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 09:50:28.830377  8671 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 09:50:28.830381  8671 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 09:50:28.830387  8671 net.cpp:245] Setting up res2a_branch2b/relu
I0815 09:50:28.830394  8671 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 43 64 56 56 (8630272)
I0815 09:50:28.830397  8671 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 09:50:28.830401  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830410  8671 net.cpp:184] Created Layer pool2 (15)
I0815 09:50:28.830415  8671 net.cpp:561] pool2 <- res2a_branch2b
I0815 09:50:28.830418  8671 net.cpp:530] pool2 -> pool2
I0815 09:50:28.830500  8671 net.cpp:245] Setting up pool2
I0815 09:50:28.830507  8671 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 43 64 28 28 (2157568)
I0815 09:50:28.830512  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 09:50:28.830516  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.830528  8671 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 09:50:28.830531  8671 net.cpp:561] res3a_branch2a <- pool2
I0815 09:50:28.830535  8671 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 09:50:28.852579  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.58G, req 0G)
I0815 09:50:28.852594  8671 net.cpp:245] Setting up res3a_branch2a
I0815 09:50:28.852599  8671 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 43 128 28 28 (4315136)
I0815 09:50:28.852604  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.852607  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.852612  8671 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 09:50:28.852615  8671 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 09:50:28.852618  8671 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 09:50:28.853263  8671 net.cpp:245] Setting up res3a_branch2a/bn
I0815 09:50:28.853271  8671 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 43 128 28 28 (4315136)
I0815 09:50:28.853279  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.853283  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.853286  8671 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 09:50:28.853289  8671 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 09:50:28.853291  8671 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 09:50:28.853307  8671 net.cpp:245] Setting up res3a_branch2a/relu
I0815 09:50:28.853310  8671 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 43 128 28 28 (4315136)
I0815 09:50:28.853312  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 09:50:28.853314  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.853338  8671 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 09:50:28.853340  8671 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 09:50:28.853343  8671 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 09:50:28.862071  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.54G, req 0G)
I0815 09:50:28.862093  8671 net.cpp:245] Setting up res3a_branch2b
I0815 09:50:28.862102  8671 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 43 128 28 28 (4315136)
I0815 09:50:28.862112  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.862118  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.862128  8671 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 09:50:28.862131  8671 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 09:50:28.862136  8671 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 09:50:28.863054  8671 net.cpp:245] Setting up res3a_branch2b/bn
I0815 09:50:28.863066  8671 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 43 128 28 28 (4315136)
I0815 09:50:28.863078  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.863085  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863090  8671 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 09:50:28.863095  8671 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 09:50:28.863099  8671 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 09:50:28.863106  8671 net.cpp:245] Setting up res3a_branch2b/relu
I0815 09:50:28.863111  8671 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 43 128 28 28 (4315136)
I0815 09:50:28.863114  8671 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 09:50:28.863118  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863126  8671 net.cpp:184] Created Layer pool3 (22)
I0815 09:50:28.863131  8671 net.cpp:561] pool3 <- res3a_branch2b
I0815 09:50:28.863135  8671 net.cpp:530] pool3 -> pool3
I0815 09:50:28.863234  8671 net.cpp:245] Setting up pool3
I0815 09:50:28.863241  8671 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 43 128 14 14 (1078784)
I0815 09:50:28.863246  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 09:50:28.863252  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.863268  8671 net.cpp:184] Created Layer res4a_branch2a (23)
I0815 09:50:28.863275  8671 net.cpp:561] res4a_branch2a <- pool3
I0815 09:50:28.863279  8671 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 09:50:28.889317  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.51G, req 0G)
I0815 09:50:28.889338  8671 net.cpp:245] Setting up res4a_branch2a
I0815 09:50:28.889344  8671 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 43 256 14 14 (2157568)
I0815 09:50:28.889351  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.889355  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.889364  8671 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0815 09:50:28.889367  8671 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 09:50:28.889371  8671 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 09:50:28.890071  8671 net.cpp:245] Setting up res4a_branch2a/bn
I0815 09:50:28.890089  8671 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 43 256 14 14 (2157568)
I0815 09:50:28.890095  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.890099  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.890102  8671 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0815 09:50:28.890105  8671 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 09:50:28.890107  8671 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 09:50:28.890111  8671 net.cpp:245] Setting up res4a_branch2a/relu
I0815 09:50:28.890113  8671 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 43 256 14 14 (2157568)
I0815 09:50:28.890115  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 09:50:28.890117  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.890125  8671 net.cpp:184] Created Layer res4a_branch2b (26)
I0815 09:50:28.890132  8671 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 09:50:28.890135  8671 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 09:50:28.899150  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.48G, req 0G)
I0815 09:50:28.899163  8671 net.cpp:245] Setting up res4a_branch2b
I0815 09:50:28.899168  8671 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 43 256 14 14 (2157568)
I0815 09:50:28.899173  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.899176  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899180  8671 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0815 09:50:28.899183  8671 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 09:50:28.899186  8671 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 09:50:28.899842  8671 net.cpp:245] Setting up res4a_branch2b/bn
I0815 09:50:28.899852  8671 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 43 256 14 14 (2157568)
I0815 09:50:28.899857  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.899860  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899869  8671 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0815 09:50:28.899873  8671 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 09:50:28.899874  8671 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 09:50:28.899879  8671 net.cpp:245] Setting up res4a_branch2b/relu
I0815 09:50:28.899883  8671 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 43 256 14 14 (2157568)
I0815 09:50:28.899884  8671 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 09:50:28.899886  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899890  8671 net.cpp:184] Created Layer pool4 (29)
I0815 09:50:28.899893  8671 net.cpp:561] pool4 <- res4a_branch2b
I0815 09:50:28.899895  8671 net.cpp:530] pool4 -> pool4
I0815 09:50:28.899958  8671 net.cpp:245] Setting up pool4
I0815 09:50:28.899963  8671 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 43 256 7 7 (539392)
I0815 09:50:28.899966  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 09:50:28.899968  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.899974  8671 net.cpp:184] Created Layer res5a_branch2a (30)
I0815 09:50:28.899977  8671 net.cpp:561] res5a_branch2a <- pool4
I0815 09:50:28.899979  8671 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 09:50:28.948348  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 7.45G, req 0G)
I0815 09:50:28.948365  8671 net.cpp:245] Setting up res5a_branch2a
I0815 09:50:28.948370  8671 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 43 512 7 7 (1078784)
I0815 09:50:28.948388  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:28.948392  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.948405  8671 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0815 09:50:28.948410  8671 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 09:50:28.948413  8671 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 09:50:28.949066  8671 net.cpp:245] Setting up res5a_branch2a/bn
I0815 09:50:28.949074  8671 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 43 512 7 7 (1078784)
I0815 09:50:28.949080  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 09:50:28.949084  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.949087  8671 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0815 09:50:28.949090  8671 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 09:50:28.949092  8671 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 09:50:28.949096  8671 net.cpp:245] Setting up res5a_branch2a/relu
I0815 09:50:28.949098  8671 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 43 512 7 7 (1078784)
I0815 09:50:28.949100  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 09:50:28.949103  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.949113  8671 net.cpp:184] Created Layer res5a_branch2b (33)
I0815 09:50:28.949116  8671 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 09:50:28.949120  8671 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 09:50:28.967604  8671 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.44G, req 0G)
I0815 09:50:28.967617  8671 net.cpp:245] Setting up res5a_branch2b
I0815 09:50:28.967622  8671 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 43 512 7 7 (1078784)
I0815 09:50:28.967629  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:28.967633  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.967639  8671 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0815 09:50:28.967643  8671 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 09:50:28.967644  8671 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 09:50:28.968312  8671 net.cpp:245] Setting up res5a_branch2b/bn
I0815 09:50:28.968322  8671 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 43 512 7 7 (1078784)
I0815 09:50:28.968327  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 09:50:28.968329  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968333  8671 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0815 09:50:28.968335  8671 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 09:50:28.968338  8671 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 09:50:28.968343  8671 net.cpp:245] Setting up res5a_branch2b/relu
I0815 09:50:28.968345  8671 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 43 512 7 7 (1078784)
I0815 09:50:28.968348  8671 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0815 09:50:28.968349  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968354  8671 net.cpp:184] Created Layer pool5 (36)
I0815 09:50:28.968356  8671 net.cpp:561] pool5 <- res5a_branch2b
I0815 09:50:28.968359  8671 net.cpp:530] pool5 -> pool5
I0815 09:50:28.968384  8671 net.cpp:245] Setting up pool5
I0815 09:50:28.968389  8671 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 43 512 1 1 (22016)
I0815 09:50:28.968390  8671 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0815 09:50:28.968401  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.968406  8671 net.cpp:184] Created Layer fc1000 (37)
I0815 09:50:28.968410  8671 net.cpp:561] fc1000 <- pool5
I0815 09:50:28.968411  8671 net.cpp:530] fc1000 -> fc1000
I0815 09:50:28.979735  8671 net.cpp:245] Setting up fc1000
I0815 09:50:28.979745  8671 net.cpp:252] TRAIN Top shape for layer 37 'fc1000' 43 1000 (43000)
I0815 09:50:28.979751  8671 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 09:50:28.979755  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.979765  8671 net.cpp:184] Created Layer loss (38)
I0815 09:50:28.979768  8671 net.cpp:561] loss <- fc1000
I0815 09:50:28.979771  8671 net.cpp:561] loss <- label
I0815 09:50:28.979775  8671 net.cpp:530] loss -> loss
I0815 09:50:28.980206  8671 net.cpp:245] Setting up loss
I0815 09:50:28.980214  8671 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0815 09:50:28.980216  8671 net.cpp:256]     with loss weight 1
I0815 09:50:28.980222  8671 net.cpp:323] loss needs backward computation.
I0815 09:50:28.980224  8671 net.cpp:323] fc1000 needs backward computation.
I0815 09:50:28.980227  8671 net.cpp:323] pool5 needs backward computation.
I0815 09:50:28.980229  8671 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 09:50:28.980232  8671 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 09:50:28.980233  8671 net.cpp:323] res5a_branch2b needs backward computation.
I0815 09:50:28.980235  8671 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 09:50:28.980237  8671 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 09:50:28.980239  8671 net.cpp:323] res5a_branch2a needs backward computation.
I0815 09:50:28.980242  8671 net.cpp:323] pool4 needs backward computation.
I0815 09:50:28.980245  8671 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 09:50:28.980247  8671 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 09:50:28.980248  8671 net.cpp:323] res4a_branch2b needs backward computation.
I0815 09:50:28.980250  8671 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 09:50:28.980253  8671 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 09:50:28.980255  8671 net.cpp:323] res4a_branch2a needs backward computation.
I0815 09:50:28.980257  8671 net.cpp:323] pool3 needs backward computation.
I0815 09:50:28.980259  8671 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 09:50:28.980262  8671 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 09:50:28.980264  8671 net.cpp:323] res3a_branch2b needs backward computation.
I0815 09:50:28.980267  8671 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 09:50:28.980268  8671 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 09:50:28.980270  8671 net.cpp:323] res3a_branch2a needs backward computation.
I0815 09:50:28.980273  8671 net.cpp:323] pool2 needs backward computation.
I0815 09:50:28.980275  8671 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 09:50:28.980278  8671 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 09:50:28.980280  8671 net.cpp:323] res2a_branch2b needs backward computation.
I0815 09:50:28.980281  8671 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 09:50:28.980283  8671 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 09:50:28.980286  8671 net.cpp:323] res2a_branch2a needs backward computation.
I0815 09:50:28.980288  8671 net.cpp:323] pool1 needs backward computation.
I0815 09:50:28.980290  8671 net.cpp:323] conv1b/relu needs backward computation.
I0815 09:50:28.980293  8671 net.cpp:323] conv1b/bn needs backward computation.
I0815 09:50:28.980294  8671 net.cpp:323] conv1b needs backward computation.
I0815 09:50:28.980296  8671 net.cpp:323] conv1a/relu needs backward computation.
I0815 09:50:28.980299  8671 net.cpp:323] conv1a/bn needs backward computation.
I0815 09:50:28.980311  8671 net.cpp:323] conv1a needs backward computation.
I0815 09:50:28.980314  8671 net.cpp:325] data/bias does not need backward computation.
I0815 09:50:28.980317  8671 net.cpp:325] data does not need backward computation.
I0815 09:50:28.980319  8671 net.cpp:367] This network produces output loss
I0815 09:50:28.980355  8671 net.cpp:389] Top memory (TRAIN) required for data: 802615296 diff: 802615304
I0815 09:50:28.980358  8671 net.cpp:392] Bottom memory (TRAIN) required for data: 802615296 diff: 802615296
I0815 09:50:28.980360  8671 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 535076864 diff: 535076864
I0815 09:50:28.980362  8671 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0815 09:50:28.980363  8671 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 09:50:28.980366  8671 net.cpp:407] Network initialization done.
I0815 09:50:28.980885  8671 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:28.980942  8671 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:28.981070  8671 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0815 09:50:28.981165  8671 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:28.981170  8671 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:28.981173  8671 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0815 09:50:28.981175  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.981187  8671 net.cpp:184] Created Layer data (0)
I0815 09:50:28.981191  8671 net.cpp:530] data -> data
I0815 09:50:28.981196  8671 net.cpp:530] data -> label
I0815 09:50:28.981205  8671 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:28.981221  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.982070  8747 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:28.982638  8671 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:28.982722  8671 data_layer.cpp:209] (0) Output data size: 17, 3, 224, 224
I0815 09:50:28.982728  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:28.982748  8671 net.cpp:245] Setting up data
I0815 09:50:28.982755  8671 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 224 224 (2558976)
I0815 09:50:28.982762  8671 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0815 09:50:28.982767  8671 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 09:50:28.982779  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.982787  8671 net.cpp:184] Created Layer label_data_1_split (1)
I0815 09:50:28.982792  8671 net.cpp:561] label_data_1_split <- label
I0815 09:50:28.982797  8671 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 09:50:28.982803  8671 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 09:50:28.982808  8671 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 09:50:28.982875  8671 net.cpp:245] Setting up label_data_1_split
I0815 09:50:28.982882  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982885  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982890  8671 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0815 09:50:28.982894  8671 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 09:50:28.982899  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.982906  8671 net.cpp:184] Created Layer data/bias (2)
I0815 09:50:28.982911  8671 net.cpp:561] data/bias <- data
I0815 09:50:28.982916  8671 net.cpp:530] data/bias -> data/bias
I0815 09:50:28.983103  8671 net.cpp:245] Setting up data/bias
I0815 09:50:28.983109  8671 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 224 224 (2558976)
I0815 09:50:28.983117  8671 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 09:50:28.983122  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.983134  8671 net.cpp:184] Created Layer conv1a (3)
I0815 09:50:28.983139  8671 net.cpp:561] conv1a <- data/bias
I0815 09:50:28.983142  8671 net.cpp:530] conv1a -> conv1a
I0815 09:50:28.984009  8748 data_layer.cpp:97] (0) Parser threads: 1
I0815 09:50:28.984017  8748 data_layer.cpp:99] (0) Transformer threads: 1
I0815 09:50:28.989540  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.4G, req 0G)
I0815 09:50:28.989560  8671 net.cpp:245] Setting up conv1a
I0815 09:50:28.989567  8671 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 112 112 (6823936)
I0815 09:50:28.989579  8671 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 09:50:28.989584  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.989598  8671 net.cpp:184] Created Layer conv1a/bn (4)
I0815 09:50:28.989604  8671 net.cpp:561] conv1a/bn <- conv1a
I0815 09:50:28.989609  8671 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 09:50:28.990360  8671 net.cpp:245] Setting up conv1a/bn
I0815 09:50:28.990368  8671 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 112 112 (6823936)
I0815 09:50:28.990377  8671 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 09:50:28.990381  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.990388  8671 net.cpp:184] Created Layer conv1a/relu (5)
I0815 09:50:28.990392  8671 net.cpp:561] conv1a/relu <- conv1a
I0815 09:50:28.990396  8671 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 09:50:28.990406  8671 net.cpp:245] Setting up conv1a/relu
I0815 09:50:28.990411  8671 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 112 112 (6823936)
I0815 09:50:28.990416  8671 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 09:50:28.990420  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.990432  8671 net.cpp:184] Created Layer conv1b (6)
I0815 09:50:28.990435  8671 net.cpp:561] conv1b <- conv1a
I0815 09:50:28.990439  8671 net.cpp:530] conv1b -> conv1b
I0815 09:50:28.995615  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:28.995625  8671 net.cpp:245] Setting up conv1b
I0815 09:50:28.995631  8671 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 112 112 (6823936)
I0815 09:50:28.995648  8671 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 09:50:28.995652  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.995661  8671 net.cpp:184] Created Layer conv1b/bn (7)
I0815 09:50:28.995666  8671 net.cpp:561] conv1b/bn <- conv1b
I0815 09:50:28.995671  8671 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 09:50:28.996361  8671 net.cpp:245] Setting up conv1b/bn
I0815 09:50:28.996368  8671 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 112 112 (6823936)
I0815 09:50:28.996377  8671 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 09:50:28.996381  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996387  8671 net.cpp:184] Created Layer conv1b/relu (8)
I0815 09:50:28.996392  8671 net.cpp:561] conv1b/relu <- conv1b
I0815 09:50:28.996395  8671 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 09:50:28.996402  8671 net.cpp:245] Setting up conv1b/relu
I0815 09:50:28.996407  8671 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 112 112 (6823936)
I0815 09:50:28.996410  8671 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 09:50:28.996414  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996420  8671 net.cpp:184] Created Layer pool1 (9)
I0815 09:50:28.996424  8671 net.cpp:561] pool1 <- conv1b
I0815 09:50:28.996428  8671 net.cpp:530] pool1 -> pool1
I0815 09:50:28.996492  8671 net.cpp:245] Setting up pool1
I0815 09:50:28.996498  8671 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 56 56 (1705984)
I0815 09:50:28.996502  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 09:50:28.996506  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:28.996515  8671 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 09:50:28.996518  8671 net.cpp:561] res2a_branch2a <- pool1
I0815 09:50:28.996523  8671 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 09:50:29.001720  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.34G, req 0G)
I0815 09:50:29.001732  8671 net.cpp:245] Setting up res2a_branch2a
I0815 09:50:29.001739  8671 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 56 56 (3411968)
I0815 09:50:29.001747  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.001752  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.001760  8671 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 09:50:29.001765  8671 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 09:50:29.001770  8671 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 09:50:29.002534  8671 net.cpp:245] Setting up res2a_branch2a/bn
I0815 09:50:29.002543  8671 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 56 56 (3411968)
I0815 09:50:29.002552  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.002557  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.002562  8671 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 09:50:29.002565  8671 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 09:50:29.002569  8671 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 09:50:29.002578  8671 net.cpp:245] Setting up res2a_branch2a/relu
I0815 09:50:29.002583  8671 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 56 56 (3411968)
I0815 09:50:29.002588  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 09:50:29.002593  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.002604  8671 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 09:50:29.002614  8671 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 09:50:29.002619  8671 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 09:50:29.007417  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0815 09:50:29.007429  8671 net.cpp:245] Setting up res2a_branch2b
I0815 09:50:29.007436  8671 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 56 56 (3411968)
I0815 09:50:29.007443  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.007447  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.007454  8671 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 09:50:29.007459  8671 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 09:50:29.007462  8671 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 09:50:29.008152  8671 net.cpp:245] Setting up res2a_branch2b/bn
I0815 09:50:29.008172  8671 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 56 56 (3411968)
I0815 09:50:29.008183  8671 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.008186  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008190  8671 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 09:50:29.008194  8671 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 09:50:29.008198  8671 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 09:50:29.008203  8671 net.cpp:245] Setting up res2a_branch2b/relu
I0815 09:50:29.008206  8671 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 56 56 (3411968)
I0815 09:50:29.008210  8671 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 09:50:29.008213  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008219  8671 net.cpp:184] Created Layer pool2 (16)
I0815 09:50:29.008224  8671 net.cpp:561] pool2 <- res2a_branch2b
I0815 09:50:29.008229  8671 net.cpp:530] pool2 -> pool2
I0815 09:50:29.008298  8671 net.cpp:245] Setting up pool2
I0815 09:50:29.008304  8671 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 28 28 (852992)
I0815 09:50:29.008306  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 09:50:29.008311  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.008317  8671 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 09:50:29.008321  8671 net.cpp:561] res3a_branch2a <- pool2
I0815 09:50:29.008322  8671 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 09:50:29.015259  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.31G, req 0G)
I0815 09:50:29.015271  8671 net.cpp:245] Setting up res3a_branch2a
I0815 09:50:29.015275  8671 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 28 28 (1705984)
I0815 09:50:29.015280  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.015283  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.015288  8671 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 09:50:29.015291  8671 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 09:50:29.015295  8671 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 09:50:29.015980  8671 net.cpp:245] Setting up res3a_branch2a/bn
I0815 09:50:29.015987  8671 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 28 28 (1705984)
I0815 09:50:29.015995  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.015997  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.016000  8671 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 09:50:29.016002  8671 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 09:50:29.016005  8671 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 09:50:29.016017  8671 net.cpp:245] Setting up res3a_branch2a/relu
I0815 09:50:29.016021  8671 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 28 28 (1705984)
I0815 09:50:29.016023  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 09:50:29.016028  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.016038  8671 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 09:50:29.016042  8671 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 09:50:29.016044  8671 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 09:50:29.019454  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.3G, req 0G)
I0815 09:50:29.019464  8671 net.cpp:245] Setting up res3a_branch2b
I0815 09:50:29.019469  8671 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 28 28 (1705984)
I0815 09:50:29.019474  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.019480  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.019484  8671 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 09:50:29.019487  8671 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 09:50:29.019490  8671 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 09:50:29.020169  8671 net.cpp:245] Setting up res3a_branch2b/bn
I0815 09:50:29.020177  8671 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 28 28 (1705984)
I0815 09:50:29.020184  8671 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.020186  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020190  8671 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 09:50:29.020191  8671 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 09:50:29.020193  8671 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 09:50:29.020196  8671 net.cpp:245] Setting up res3a_branch2b/relu
I0815 09:50:29.020200  8671 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 28 28 (1705984)
I0815 09:50:29.020201  8671 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 09:50:29.020203  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020206  8671 net.cpp:184] Created Layer pool3 (23)
I0815 09:50:29.020208  8671 net.cpp:561] pool3 <- res3a_branch2b
I0815 09:50:29.020211  8671 net.cpp:530] pool3 -> pool3
I0815 09:50:29.020273  8671 net.cpp:245] Setting up pool3
I0815 09:50:29.020278  8671 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 14 14 (426496)
I0815 09:50:29.020280  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 09:50:29.020282  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.020287  8671 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 09:50:29.020290  8671 net.cpp:561] res4a_branch2a <- pool3
I0815 09:50:29.020292  8671 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 09:50:29.031576  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.29G, req 0G)
I0815 09:50:29.031586  8671 net.cpp:245] Setting up res4a_branch2a
I0815 09:50:29.031590  8671 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 14 14 (852992)
I0815 09:50:29.031595  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.031597  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.031602  8671 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 09:50:29.031605  8671 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 09:50:29.031607  8671 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 09:50:29.032284  8671 net.cpp:245] Setting up res4a_branch2a/bn
I0815 09:50:29.032299  8671 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 14 14 (852992)
I0815 09:50:29.032305  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.032308  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.032311  8671 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 09:50:29.032313  8671 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 09:50:29.032316  8671 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 09:50:29.032320  8671 net.cpp:245] Setting up res4a_branch2a/relu
I0815 09:50:29.032322  8671 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 14 14 (852992)
I0815 09:50:29.032325  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 09:50:29.032326  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.032336  8671 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 09:50:29.032341  8671 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 09:50:29.032342  8671 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 09:50:29.038236  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.28G, req 0G)
I0815 09:50:29.038246  8671 net.cpp:245] Setting up res4a_branch2b
I0815 09:50:29.038250  8671 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 14 14 (852992)
I0815 09:50:29.038254  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.038257  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038266  8671 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 09:50:29.038269  8671 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 09:50:29.038272  8671 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 09:50:29.038949  8671 net.cpp:245] Setting up res4a_branch2b/bn
I0815 09:50:29.038956  8671 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 14 14 (852992)
I0815 09:50:29.038962  8671 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.038964  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038967  8671 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 09:50:29.038970  8671 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 09:50:29.038972  8671 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 09:50:29.038975  8671 net.cpp:245] Setting up res4a_branch2b/relu
I0815 09:50:29.038978  8671 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 14 14 (852992)
I0815 09:50:29.038980  8671 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 09:50:29.038982  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.038985  8671 net.cpp:184] Created Layer pool4 (30)
I0815 09:50:29.038987  8671 net.cpp:561] pool4 <- res4a_branch2b
I0815 09:50:29.038990  8671 net.cpp:530] pool4 -> pool4
I0815 09:50:29.039057  8671 net.cpp:245] Setting up pool4
I0815 09:50:29.039062  8671 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 7 7 (213248)
I0815 09:50:29.039064  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 09:50:29.039067  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.039075  8671 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 09:50:29.039079  8671 net.cpp:561] res5a_branch2a <- pool4
I0815 09:50:29.039083  8671 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 09:50:29.069139  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.28G, req 0G)
I0815 09:50:29.069155  8671 net.cpp:245] Setting up res5a_branch2a
I0815 09:50:29.069160  8671 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 7 7 (426496)
I0815 09:50:29.069177  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 09:50:29.069181  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069188  8671 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 09:50:29.069191  8671 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 09:50:29.069196  8671 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 09:50:29.069898  8671 net.cpp:245] Setting up res5a_branch2a/bn
I0815 09:50:29.069906  8671 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 7 7 (426496)
I0815 09:50:29.069912  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 09:50:29.069914  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069921  8671 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 09:50:29.069923  8671 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 09:50:29.069926  8671 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 09:50:29.069931  8671 net.cpp:245] Setting up res5a_branch2a/relu
I0815 09:50:29.069932  8671 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 7 7 (426496)
I0815 09:50:29.069934  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 09:50:29.069937  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.069947  8671 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 09:50:29.069949  8671 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 09:50:29.069952  8671 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 09:50:29.086246  8671 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.27G, req 0G)
I0815 09:50:29.086258  8671 net.cpp:245] Setting up res5a_branch2b
I0815 09:50:29.086262  8671 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 7 7 (426496)
I0815 09:50:29.086271  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 09:50:29.086273  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.086277  8671 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 09:50:29.086280  8671 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 09:50:29.086282  8671 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 09:50:29.086974  8671 net.cpp:245] Setting up res5a_branch2b/bn
I0815 09:50:29.086982  8671 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 7 7 (426496)
I0815 09:50:29.086987  8671 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 09:50:29.086990  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.086993  8671 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 09:50:29.086995  8671 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 09:50:29.086998  8671 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 09:50:29.087002  8671 net.cpp:245] Setting up res5a_branch2b/relu
I0815 09:50:29.087004  8671 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 7 7 (426496)
I0815 09:50:29.087007  8671 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0815 09:50:29.087008  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.087011  8671 net.cpp:184] Created Layer pool5 (37)
I0815 09:50:29.087014  8671 net.cpp:561] pool5 <- res5a_branch2b
I0815 09:50:29.087018  8671 net.cpp:530] pool5 -> pool5
I0815 09:50:29.087044  8671 net.cpp:245] Setting up pool5
I0815 09:50:29.087049  8671 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0815 09:50:29.087050  8671 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0815 09:50:29.087052  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.087064  8671 net.cpp:184] Created Layer fc1000 (38)
I0815 09:50:29.087066  8671 net.cpp:561] fc1000 <- pool5
I0815 09:50:29.087069  8671 net.cpp:530] fc1000 -> fc1000
I0815 09:50:29.098291  8671 net.cpp:245] Setting up fc1000
I0815 09:50:29.098315  8671 net.cpp:252] TEST Top shape for layer 38 'fc1000' 17 1000 (17000)
I0815 09:50:29.098323  8671 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0815 09:50:29.098327  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098333  8671 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0815 09:50:29.098348  8671 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0815 09:50:29.098354  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0815 09:50:29.098359  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0815 09:50:29.098364  8671 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0815 09:50:29.098457  8671 net.cpp:245] Setting up fc1000_fc1000_0_split
I0815 09:50:29.098464  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098469  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098475  8671 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 17 1000 (17000)
I0815 09:50:29.098480  8671 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 09:50:29.098484  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098491  8671 net.cpp:184] Created Layer loss (40)
I0815 09:50:29.098495  8671 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0815 09:50:29.098500  8671 net.cpp:561] loss <- label_data_1_split_0
I0815 09:50:29.098505  8671 net.cpp:530] loss -> loss
I0815 09:50:29.098696  8671 net.cpp:245] Setting up loss
I0815 09:50:29.098704  8671 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0815 09:50:29.098707  8671 net.cpp:256]     with loss weight 1
I0815 09:50:29.098716  8671 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 09:50:29.098721  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098734  8671 net.cpp:184] Created Layer accuracy/top1 (41)
I0815 09:50:29.098737  8671 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0815 09:50:29.098742  8671 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 09:50:29.098747  8671 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 09:50:29.098754  8671 net.cpp:245] Setting up accuracy/top1
I0815 09:50:29.098758  8671 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0815 09:50:29.098763  8671 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 09:50:29.098767  8671 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 09:50:29.098773  8671 net.cpp:184] Created Layer accuracy/top5 (42)
I0815 09:50:29.098776  8671 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0815 09:50:29.098780  8671 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 09:50:29.098785  8671 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 09:50:29.098793  8671 net.cpp:245] Setting up accuracy/top5
I0815 09:50:29.098798  8671 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0815 09:50:29.098801  8671 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 09:50:29.098805  8671 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 09:50:29.098809  8671 net.cpp:323] loss needs backward computation.
I0815 09:50:29.098814  8671 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0815 09:50:29.098819  8671 net.cpp:323] fc1000 needs backward computation.
I0815 09:50:29.098822  8671 net.cpp:323] pool5 needs backward computation.
I0815 09:50:29.098826  8671 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 09:50:29.098839  8671 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 09:50:29.098842  8671 net.cpp:323] res5a_branch2b needs backward computation.
I0815 09:50:29.098846  8671 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 09:50:29.098850  8671 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 09:50:29.098853  8671 net.cpp:323] res5a_branch2a needs backward computation.
I0815 09:50:29.098858  8671 net.cpp:323] pool4 needs backward computation.
I0815 09:50:29.098862  8671 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 09:50:29.098865  8671 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 09:50:29.098870  8671 net.cpp:323] res4a_branch2b needs backward computation.
I0815 09:50:29.098873  8671 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 09:50:29.098877  8671 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 09:50:29.098881  8671 net.cpp:323] res4a_branch2a needs backward computation.
I0815 09:50:29.098886  8671 net.cpp:323] pool3 needs backward computation.
I0815 09:50:29.098889  8671 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 09:50:29.098893  8671 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 09:50:29.098897  8671 net.cpp:323] res3a_branch2b needs backward computation.
I0815 09:50:29.098901  8671 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 09:50:29.098904  8671 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 09:50:29.098907  8671 net.cpp:323] res3a_branch2a needs backward computation.
I0815 09:50:29.098912  8671 net.cpp:323] pool2 needs backward computation.
I0815 09:50:29.098915  8671 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 09:50:29.098919  8671 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 09:50:29.098923  8671 net.cpp:323] res2a_branch2b needs backward computation.
I0815 09:50:29.098927  8671 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 09:50:29.098932  8671 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 09:50:29.098934  8671 net.cpp:323] res2a_branch2a needs backward computation.
I0815 09:50:29.098938  8671 net.cpp:323] pool1 needs backward computation.
I0815 09:50:29.098942  8671 net.cpp:323] conv1b/relu needs backward computation.
I0815 09:50:29.098947  8671 net.cpp:323] conv1b/bn needs backward computation.
I0815 09:50:29.098950  8671 net.cpp:323] conv1b needs backward computation.
I0815 09:50:29.098954  8671 net.cpp:323] conv1a/relu needs backward computation.
I0815 09:50:29.098958  8671 net.cpp:323] conv1a/bn needs backward computation.
I0815 09:50:29.098963  8671 net.cpp:323] conv1a needs backward computation.
I0815 09:50:29.098966  8671 net.cpp:325] data/bias does not need backward computation.
I0815 09:50:29.098971  8671 net.cpp:325] label_data_1_split does not need backward computation.
I0815 09:50:29.098975  8671 net.cpp:325] data does not need backward computation.
I0815 09:50:29.098978  8671 net.cpp:367] This network produces output accuracy/top1
I0815 09:50:29.098983  8671 net.cpp:367] This network produces output accuracy/top5
I0815 09:50:29.098986  8671 net.cpp:367] This network produces output loss
I0815 09:50:29.099017  8671 net.cpp:389] Top memory (TEST) required for data: 317313024 diff: 8
I0815 09:50:29.099021  8671 net.cpp:392] Bottom memory (TEST) required for data: 317313024 diff: 317313024
I0815 09:50:29.099025  8671 net.cpp:395] Shared (in-place) memory (TEST) by data: 211542016 diff: 211542016
I0815 09:50:29.099028  8671 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0815 09:50:29.099031  8671 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 09:50:29.099035  8671 net.cpp:407] Network initialization done.
I0815 09:50:29.099092  8671 solver.cpp:56] Solver scaffolding done.
I0815 09:50:29.103351  8671 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_2017-08-14_19-50-34/initial/imagenet_jacintonet11v2_iter_320000.caffemodel
I0815 09:50:29.109087  8671 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 09:50:29.109117  8671 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 09:50:29.109182  8671 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 09:50:29.109206  8671 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.109616  8671 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 09:50:29.109625  8671 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 09:50:29.109637  8671 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.109848  8671 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 09:50:29.109858  8671 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 09:50:29.109860  8671 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.109884  8671 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110081  8671 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110088  8671 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110106  8671 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110291  8671 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.110298  8671 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 09:50:29.110301  8671 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.110340  8671 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110471  8671 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110477  8671 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110499  8671 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110621  8671 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.110626  8671 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 09:50:29.110628  8671 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.110749  8671 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.110877  8671 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.110882  8671 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.110949  8671 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111070  8671 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.111075  8671 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 09:50:29.111078  8671 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.111495  8671 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111631  8671 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.111637  8671 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.111840  8671 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.111963  8671 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.111968  8671 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0815 09:50:29.111971  8671 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0815 09:50:29.112162  8671 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 09:50:29.116477  8671 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 09:50:29.116492  8671 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 09:50:29.116524  8671 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 09:50:29.116546  8671 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.116806  8671 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 09:50:29.116812  8671 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 09:50:29.116821  8671 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.116969  8671 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 09:50:29.116974  8671 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 09:50:29.116976  8671 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.116991  8671 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117146  8671 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117151  8671 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117163  8671 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117307  8671 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.117312  8671 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 09:50:29.117314  8671 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.117370  8671 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117509  8671 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117514  8671 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117535  8671 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117656  8671 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.117660  8671 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 09:50:29.117663  8671 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.117791  8671 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.117920  8671 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.117925  8671 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.117990  8671 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.118111  8671 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.118116  8671 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 09:50:29.118119  8671 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 09:50:29.118551  8671 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 09:50:29.118677  8671 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 09:50:29.118682  8671 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 09:50:29.118887  8671 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 09:50:29.119009  8671 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 09:50:29.119014  8671 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0815 09:50:29.119019  8671 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0815 09:50:29.119189  8671 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 09:50:29.119256  8671 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 09:50:29.119259  8671 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 09:50:29.119262  8671 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 09:50:29.119263  8671 parallel.cpp:59] Starting Optimization
I0815 09:50:29.119266  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 09:50:29.119284  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.119310  8671 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.119946  8764 device_alternate.hpp:116] NVML initialized on thread 140247644952320
I0815 09:50:29.137429  8764 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 09:50:29.137454  8765 device_alternate.hpp:116] NVML initialized on thread 140247636559616
I0815 09:50:29.138136  8765 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 09:50:29.138190  8766 device_alternate.hpp:116] NVML initialized on thread 140247628166912
I0815 09:50:29.139082  8766 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 09:50:29.142688  8765 solver.cpp:42] Solver data type: FLOAT
W0815 09:50:29.143046  8765 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:29.143148  8765 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.143155  8765 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.143187  8765 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:29.143200  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.146636  8766 solver.cpp:42] Solver data type: FLOAT
W0815 09:50:29.146980  8766 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 128 to 129
I0815 09:50:29.147043  8766 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.147047  8766 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.147068  8766 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 43
I0815 09:50:29.147078  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.147315  8767 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:29.148075  8768 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_train_lmdb
I0815 09:50:29.149785  8765 data_layer.cpp:185] [1] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:29.150135  8766 data_layer.cpp:185] [2] ReshapePrefetch 43, 3, 224, 224
I0815 09:50:29.150398  8765 data_layer.cpp:209] [1] Output data size: 43, 3, 224, 224
I0815 09:50:29.150408  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.150449  8766 data_layer.cpp:209] [2] Output data size: 43, 3, 224, 224
I0815 09:50:29.150457  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.642834  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0815 09:50:29.659576  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.06G, req 0G)
I0815 09:50:29.670234  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0815 09:50:29.686518  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.92G, req 0G)
I0815 09:50:29.695930  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0815 09:50:29.709318  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0815 09:50:29.714108  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0G)
I0815 09:50:29.728184  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0G)
I0815 09:50:29.734447  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 7.68G, req 0G)
I0815 09:50:29.743526  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0815 09:50:29.751754  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.68G, req 0G)
I0815 09:50:29.761013  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.64G, req 0G)
I0815 09:50:29.771639  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0815 09:50:29.782881  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0815 09:50:29.789307  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.6G, req 0G)
I0815 09:50:29.799487  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.58G, req 0G)
I0815 09:50:29.834940  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0815 09:50:29.852519  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 7.55G, req 0G)
I0815 09:50:29.855957  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 7.53G, req 0G)
I0815 09:50:29.868577  8765 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:29.868638  8765 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:29.868731  8765 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.868736  8765 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.868752  8765 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:29.868760  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.869946  8771 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:29.871270  8765 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:29.871418  8765 data_layer.cpp:209] (1) Output data size: 17, 3, 224, 224
I0815 09:50:29.871428  8765 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 09:50:29.872181  8772 data_layer.cpp:97] (1) Parser threads: 1
I0815 09:50:29.872190  8772 data_layer.cpp:99] (1) Transformer threads: 1
I0815 09:50:29.874037  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 1 1 3  (limit 7.53G, req 0G)
I0815 09:50:29.878528  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0815 09:50:29.885160  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0815 09:50:29.890936  8766 solver.cpp:176] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/test.prototxt
W0815 09:50:29.890998  8766 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0815 09:50:29.891126  8766 net.cpp:104] Using FLOAT as default forward math type
I0815 09:50:29.891134  8766 net.cpp:110] Using FLOAT as default backward math type
I0815 09:50:29.891154  8766 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0815 09:50:29.891163  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.891903  8773 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0815 09:50:29.892587  8766 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 224, 224
I0815 09:50:29.892735  8766 data_layer.cpp:209] (2) Output data size: 17, 3, 224, 224
I0815 09:50:29.892741  8766 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 09:50:29.894080  8774 data_layer.cpp:97] (2) Parser threads: 1
I0815 09:50:29.894090  8774 data_layer.cpp:99] (2) Transformer threads: 1
I0815 09:50:29.894641  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.43G, req 0G)
I0815 09:50:29.901062  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.49G, req 0G)
I0815 09:50:29.904008  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0815 09:50:29.908491  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.46G, req 0G)
I0815 09:50:29.912778  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0815 09:50:29.916293  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.43G, req 0G)
I0815 09:50:29.919464  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0815 09:50:29.922703  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.42G, req 0G)
I0815 09:50:29.931401  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.4G, req 0G)
I0815 09:50:29.935210  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0815 09:50:29.935860  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.39G, req 0G)
I0815 09:50:29.942101  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0815 09:50:29.950568  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.39G, req 0G)
I0815 09:50:29.957403  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.38G, req 0G)
I0815 09:50:29.978293  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0815 09:50:29.988998  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.37G, req 0G)
I0815 09:50:29.998667  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:30.006583  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.37G, req 0G)
I0815 09:50:30.013453  8765 solver.cpp:56] Solver scaffolding done.
I0815 09:50:30.019537  8766 solver.cpp:56] Solver scaffolding done.
I0815 09:50:30.062592  8766 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 09:50:30.062592  8765 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 09:50:30.062618  8764 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 09:50:30.255599  8764 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 09:50:30.263182  8765 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.263200  8765 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.263219  8766 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.263229  8766 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.269048  8764 solver.cpp:438] Solving jacintonet11v2_train
I0815 09:50:30.269058  8764 solver.cpp:439] Learning Rate Policy: poly
I0815 09:50:30.278180  8766 solver.cpp:227] Starting Optimization on GPU 2
I0815 09:50:30.278182  8764 solver.cpp:227] Starting Optimization on GPU 0
I0815 09:50:30.278180  8765 solver.cpp:227] Starting Optimization on GPU 1
I0815 09:50:30.278375  8764 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 09:50:30.278388  8792 device_alternate.hpp:116] NVML initialized on thread 140105801520896
I0815 09:50:30.278409  8792 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 09:50:30.278421  8794 device_alternate.hpp:116] NVML initialized on thread 140105793128192
I0815 09:50:30.278435  8794 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 09:50:30.278548  8793 device_alternate.hpp:116] NVML initialized on thread 140105809913600
I0815 09:50:30.278563  8793 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 09:50:30.286167  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.3G, req 0G)
I0815 09:50:30.295697  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.24G, req 0G)
I0815 09:50:30.296815  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.3G, req 0G)
I0815 09:50:30.307793  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.24G, req 0G)
I0815 09:50:30.309789  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.17G, req 0G)
I0815 09:50:30.311377  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.24G, req 0G)
I0815 09:50:30.317014  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.14G, req 0G)
I0815 09:50:30.319846  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.17G, req 0G)
I0815 09:50:30.323022  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.17G, req 0G)
I0815 09:50:30.324574  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.11G, req 0G)
I0815 09:50:30.326530  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.14G, req 0G)
I0815 09:50:30.330193  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 09:50:30.333767  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 09:50:30.334270  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.11G, req 0G)
I0815 09:50:30.337064  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0815 09:50:30.339977  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 09:50:30.340641  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.341563  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.347712  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0815 09:50:30.349736  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.04G, req 0G)
I0815 09:50:30.350428  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.06G, req 0G)
I0815 09:50:30.353463  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 09:50:30.356401  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 09:50:30.357349  8765 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 09:50:30.360445  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.06G, req 0G)
I0815 09:50:30.364109  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 09:50:30.364617  8766 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 09:50:30.368089  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 09:50:30.373881  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 6.99G, req 0G)
I0815 09:50:30.377813  8764 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 6.99G, req 0G)
I0815 09:50:30.380987  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.411765
I0815 09:50:30.380998  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.647059
I0815 09:50:30.381002  8764 solver.cpp:594]     Test net output #2: loss = 2.59704 (* 1 = 2.59704 loss)
I0815 09:50:30.381017  8764 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 09:50:30.381031  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 09:50:30.472728  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 1  (limit 7G, req 0G)
I0815 09:50:30.479776  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.93G, req 0G)
I0815 09:50:30.480700  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7G, req 0G)
I0815 09:50:30.503252  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.86G, req 0G)
I0815 09:50:30.509383  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.79G, req 0G)
I0815 09:50:30.515101  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.86G, req 0G)
I0815 09:50:30.536636  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 6.7G, req 0G)
I0815 09:50:30.541236  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 6.63G, req 0G)
I0815 09:50:30.547163  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 6.7G, req 0G)
I0815 09:50:30.551407  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.63G, req 0G)
I0815 09:50:30.560731  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.56G, req 0G)
I0815 09:50:30.563598  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 0  (limit 6.63G, req 0G)
I0815 09:50:30.581929  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 6.55G, req 0G)
I0815 09:50:30.583058  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.48G, req 0G)
I0815 09:50:30.587162  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 6.55G, req 0G)
I0815 09:50:30.592979  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.51G, req 0G)
I0815 09:50:30.593278  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.44G, req 0G)
I0815 09:50:30.597041  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 6.51G, req 0G)
I0815 09:50:30.614681  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.47G, req 0G)
I0815 09:50:30.615087  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 6.39G, req 0G)
I0815 09:50:30.618422  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 6.47G, req 0G)
I0815 09:50:30.623517  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.45G, req 0G)
I0815 09:50:30.623908  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.37G, req 0G)
I0815 09:50:30.627024  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 6.45G, req 0G)
I0815 09:50:30.649219  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 6.42G, req 0G)
I0815 09:50:30.650141  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 3  (limit 6.34G, req 0G)
I0815 09:50:30.653628  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 1 1 1  (limit 6.42G, req 0G)
I0815 09:50:30.658583  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.41G, req 0G)
I0815 09:50:30.658764  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.33G, req 0G)
I0815 09:50:30.661267  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 1 3  (limit 6.41G, req 0G)
I0815 09:50:30.718473  8729 data_layer.cpp:97] [0] Parser threads: 1
I0815 09:50:30.718488  8729 data_layer.cpp:99] [0] Transformer threads: 1
I0815 09:50:30.720818  8769 data_layer.cpp:97] [1] Parser threads: 1
I0815 09:50:30.720831  8769 data_layer.cpp:99] [1] Transformer threads: 1
I0815 09:50:30.722805  8770 data_layer.cpp:97] [2] Parser threads: 1
I0815 09:50:30.722815  8770 data_layer.cpp:99] [2] Transformer threads: 1
I0815 09:50:30.881630  8764 solver.cpp:317] Iteration 0 (0.500577 s), loss = 1.12189
I0815 09:50:30.881651  8764 solver.cpp:334]     Train net output #0: loss = 1.07472 (* 1 = 1.07472 loss)
I0815 09:50:30.881655  8764 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0815 09:50:30.909463  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.4G, req 0G)
I0815 09:50:30.909730  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.4G, req 0G)
I0815 09:50:30.917467  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.71G/1 1 0 3  (limit 5.31G, req 0G)
I0815 09:50:30.962762  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:30.963400  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:30.965306  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.006168  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.010534  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.012485  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.43G/1 6 4 3  (limit 4.69G, req 0G)
I0815 09:50:31.023758  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.029104  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.69G, req 0G)
I0815 09:50:31.031196  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.43G/2 6 4 0  (limit 4.69G, req 0G)
I0815 09:50:31.050061  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 1  (limit 4.6G, req 0G)
I0815 09:50:31.058642  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.062106  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0G)
I0815 09:50:31.062432  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.070116  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.073978  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.088513  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.096323  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.6G, req 0.04G)
I0815 09:50:31.099578  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.103124  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.43G/1 6 4 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.108814  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.112413  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.43G/2 6 4 3  (limit 4.69G, req 0.06G)
I0815 09:50:31.144206  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.155078  8764 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 1 1 5  (limit 4.6G, req 0.04G)
I0815 09:50:31.158326  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.162051  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.43G/1 7 5 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.169030  8766 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 7 1 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.172641  8765 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.43G/2 7 1 5  (limit 4.69G, req 0.06G)
I0815 09:50:31.196440  8764 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.43G -> 0.09G
I0815 09:50:31.212311  8766 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0815 09:50:31.217344  8765 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.43G -> 0.12G
I0815 09:50:31.308323  8764 solver.cpp:317] Iteration 1 (0.426673 s), loss = 0.943738
I0815 09:50:31.308393  8764 solver.cpp:334]     Train net output #0: loss = 0.965158 (* 1 = 0.965158 loss)
I0815 09:50:31.463210  8764 solver.cpp:317] Iteration 2 (0.154872 s), loss = 1.35809
I0815 09:50:31.463241  8764 solver.cpp:334]     Train net output #0: loss = 1.45321 (* 1 = 1.45321 loss)
I0815 09:50:45.508155  8764 solver.cpp:312] Iteration 100 (6.97779 iter/s, 14.0446s/98 iter), loss = 1.45671
I0815 09:50:45.508206  8764 solver.cpp:334]     Train net output #0: loss = 1.44152 (* 1 = 1.44152 loss)
I0815 09:50:45.508219  8764 sgd_solver.cpp:136] Iteration 100, lr = 0.00999375, m = 0.9
I0815 09:51:00.027153  8764 solver.cpp:312] Iteration 200 (6.88772 iter/s, 14.5186s/100 iter), loss = 1.53172
I0815 09:51:00.027304  8764 solver.cpp:334]     Train net output #0: loss = 1.39721 (* 1 = 1.39721 loss)
I0815 09:51:00.027323  8764 sgd_solver.cpp:136] Iteration 200, lr = 0.0099875, m = 0.9
I0815 09:51:14.657567  8764 solver.cpp:312] Iteration 300 (6.83527 iter/s, 14.63s/100 iter), loss = 1.73236
I0815 09:51:14.657625  8764 solver.cpp:334]     Train net output #0: loss = 1.48686 (* 1 = 1.48686 loss)
I0815 09:51:14.657639  8764 sgd_solver.cpp:136] Iteration 300, lr = 0.00998125, m = 0.9
I0815 09:51:29.081573  8764 solver.cpp:312] Iteration 400 (6.93308 iter/s, 14.4236s/100 iter), loss = 1.49868
I0815 09:51:29.081600  8764 solver.cpp:334]     Train net output #0: loss = 1.79943 (* 1 = 1.79943 loss)
I0815 09:51:29.081606  8764 sgd_solver.cpp:136] Iteration 400, lr = 0.009975, m = 0.9
I0815 09:51:43.539085  8764 solver.cpp:312] Iteration 500 (6.91701 iter/s, 14.4571s/100 iter), loss = 1.23975
I0815 09:51:43.539156  8764 solver.cpp:334]     Train net output #0: loss = 1.15057 (* 1 = 1.15057 loss)
I0815 09:51:43.539163  8764 sgd_solver.cpp:136] Iteration 500, lr = 0.00996875, m = 0.9
I0815 09:51:57.878718  8764 solver.cpp:312] Iteration 600 (6.97387 iter/s, 14.3392s/100 iter), loss = 1.24526
I0815 09:51:57.878739  8764 solver.cpp:334]     Train net output #0: loss = 1.32154 (* 1 = 1.32154 loss)
I0815 09:51:57.878829  8764 sgd_solver.cpp:136] Iteration 600, lr = 0.0099625, m = 0.9
I0815 09:52:12.227082  8764 solver.cpp:312] Iteration 700 (6.96963 iter/s, 14.348s/100 iter), loss = 1.25718
I0815 09:52:12.227110  8764 solver.cpp:334]     Train net output #0: loss = 1.42179 (* 1 = 1.42179 loss)
I0815 09:52:12.227118  8764 sgd_solver.cpp:136] Iteration 700, lr = 0.00995625, m = 0.9
I0815 09:52:26.558117  8764 solver.cpp:312] Iteration 800 (6.97806 iter/s, 14.3306s/100 iter), loss = 1.49093
I0815 09:52:26.558198  8764 solver.cpp:334]     Train net output #0: loss = 1.46195 (* 1 = 1.46195 loss)
I0815 09:52:26.558205  8764 sgd_solver.cpp:136] Iteration 800, lr = 0.00995, m = 0.9
I0815 09:52:41.116451  8764 solver.cpp:312] Iteration 900 (6.86911 iter/s, 14.5579s/100 iter), loss = 1.35589
I0815 09:52:41.116514  8764 solver.cpp:334]     Train net output #0: loss = 1.64769 (* 1 = 1.64769 loss)
I0815 09:52:41.116531  8764 sgd_solver.cpp:136] Iteration 900, lr = 0.00994375, m = 0.9
I0815 09:52:55.512455  8764 solver.cpp:363] Sparsity after update:
I0815 09:52:55.527741  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:52:55.527756  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:52:55.527770  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:52:55.527776  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:52:55.527778  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:52:55.527781  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:52:55.527784  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:52:55.527788  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:52:55.527792  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:52:55.527796  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:52:55.527798  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:52:55.527802  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:52:55.527806  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:52:55.663664  8764 solver.cpp:312] Iteration 1000 (6.87436 iter/s, 14.5468s/100 iter), loss = 1.42743
I0815 09:52:55.663724  8764 solver.cpp:334]     Train net output #0: loss = 1.28583 (* 1 = 1.28583 loss)
I0815 09:52:55.663741  8764 sgd_solver.cpp:136] Iteration 1000, lr = 0.0099375, m = 0.9
I0815 09:53:10.044531  8764 solver.cpp:312] Iteration 1100 (6.95388 iter/s, 14.3805s/100 iter), loss = 1.4919
I0815 09:53:10.044610  8764 solver.cpp:334]     Train net output #0: loss = 1.48686 (* 1 = 1.48686 loss)
I0815 09:53:10.044620  8764 sgd_solver.cpp:136] Iteration 1100, lr = 0.00993125, m = 0.9
I0815 09:53:24.620254  8764 solver.cpp:312] Iteration 1200 (6.86091 iter/s, 14.5753s/100 iter), loss = 1.38441
I0815 09:53:24.620278  8764 solver.cpp:334]     Train net output #0: loss = 1.45225 (* 1 = 1.45225 loss)
I0815 09:53:24.620283  8764 sgd_solver.cpp:136] Iteration 1200, lr = 0.009925, m = 0.9
I0815 09:53:39.148172  8764 solver.cpp:312] Iteration 1300 (6.88349 iter/s, 14.5275s/100 iter), loss = 1.19057
I0815 09:53:39.148201  8764 solver.cpp:334]     Train net output #0: loss = 0.978157 (* 1 = 0.978157 loss)
I0815 09:53:39.148208  8764 sgd_solver.cpp:136] Iteration 1300, lr = 0.00991875, m = 0.9
I0815 09:53:53.726241  8764 solver.cpp:312] Iteration 1400 (6.85981 iter/s, 14.5777s/100 iter), loss = 1.3075
I0815 09:53:53.726337  8764 solver.cpp:334]     Train net output #0: loss = 1.17399 (* 1 = 1.17399 loss)
I0815 09:53:53.726357  8764 sgd_solver.cpp:136] Iteration 1400, lr = 0.0099125, m = 0.9
I0815 09:54:08.560456  8764 solver.cpp:312] Iteration 1500 (6.74136 iter/s, 14.8338s/100 iter), loss = 1.5068
I0815 09:54:08.560479  8764 solver.cpp:334]     Train net output #0: loss = 1.54028 (* 1 = 1.54028 loss)
I0815 09:54:08.560483  8764 sgd_solver.cpp:136] Iteration 1500, lr = 0.00990625, m = 0.9
I0815 09:54:23.292912  8764 solver.cpp:312] Iteration 1600 (6.78792 iter/s, 14.7321s/100 iter), loss = 1.69717
I0815 09:54:23.292937  8764 solver.cpp:334]     Train net output #0: loss = 1.74796 (* 1 = 1.74796 loss)
I0815 09:54:23.292940  8764 sgd_solver.cpp:136] Iteration 1600, lr = 0.0099, m = 0.9
I0815 09:54:37.766944  8764 solver.cpp:312] Iteration 1700 (6.90912 iter/s, 14.4736s/100 iter), loss = 1.3926
I0815 09:54:37.767074  8764 solver.cpp:334]     Train net output #0: loss = 1.51627 (* 1 = 1.51627 loss)
I0815 09:54:37.767093  8764 sgd_solver.cpp:136] Iteration 1700, lr = 0.00989375, m = 0.9
I0815 09:54:52.323420  8764 solver.cpp:312] Iteration 1800 (6.86998 iter/s, 14.5561s/100 iter), loss = 1.56322
I0815 09:54:52.323560  8764 solver.cpp:334]     Train net output #0: loss = 1.73 (* 1 = 1.73 loss)
I0815 09:54:52.323575  8764 sgd_solver.cpp:136] Iteration 1800, lr = 0.0098875, m = 0.9
I0815 09:55:07.145094  8764 solver.cpp:312] Iteration 1900 (6.74706 iter/s, 14.8213s/100 iter), loss = 1.43671
I0815 09:55:07.145241  8764 solver.cpp:334]     Train net output #0: loss = 1.74979 (* 1 = 1.74979 loss)
I0815 09:55:07.145309  8764 sgd_solver.cpp:136] Iteration 1900, lr = 0.00988125, m = 0.9
I0815 09:55:21.824743  8764 solver.cpp:363] Sparsity after update:
I0815 09:55:21.835436  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:55:21.835455  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:55:21.835461  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:55:21.835464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:55:21.835466  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:55:21.835469  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:55:21.835472  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:55:21.835475  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:55:21.835479  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:55:21.835490  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:55:21.835494  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:55:21.835499  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:55:21.835501  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:55:21.835513  8764 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 09:55:41.159482  8747 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 09:55:42.320375  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.532588
I0815 09:55:42.320402  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.771704
I0815 09:55:42.320410  8764 solver.cpp:594]     Test net output #2: loss = 2.08463 (* 1 = 2.08463 loss)
I0815 09:55:42.320430  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.4844s
I0815 09:55:42.489234  8764 solver.cpp:312] Iteration 2000 (2.8294 iter/s, 35.3432s/100 iter), loss = 1.57945
I0815 09:55:42.489411  8764 solver.cpp:334]     Train net output #0: loss = 1.76152 (* 1 = 1.76152 loss)
I0815 09:55:42.489500  8764 sgd_solver.cpp:136] Iteration 2000, lr = 0.009875, m = 0.9
I0815 09:55:56.977505  8764 solver.cpp:312] Iteration 2100 (6.90233 iter/s, 14.4879s/100 iter), loss = 1.34038
I0815 09:55:56.977557  8764 solver.cpp:334]     Train net output #0: loss = 1.43644 (* 1 = 1.43644 loss)
I0815 09:55:56.977563  8764 sgd_solver.cpp:136] Iteration 2100, lr = 0.00986875, m = 0.9
I0815 09:56:11.428660  8764 solver.cpp:312] Iteration 2200 (6.92005 iter/s, 14.4508s/100 iter), loss = 1.61807
I0815 09:56:11.428686  8764 solver.cpp:334]     Train net output #0: loss = 1.85548 (* 1 = 1.85548 loss)
I0815 09:56:11.428691  8764 sgd_solver.cpp:136] Iteration 2200, lr = 0.0098625, m = 0.9
I0815 09:56:26.226137  8764 solver.cpp:312] Iteration 2300 (6.7581 iter/s, 14.7971s/100 iter), loss = 1.40816
I0815 09:56:26.226209  8764 solver.cpp:334]     Train net output #0: loss = 1.26492 (* 1 = 1.26492 loss)
I0815 09:56:26.226225  8764 sgd_solver.cpp:136] Iteration 2300, lr = 0.00985625, m = 0.9
I0815 09:56:40.952759  8764 solver.cpp:312] Iteration 2400 (6.79061 iter/s, 14.7262s/100 iter), loss = 1.54934
I0815 09:56:40.952890  8764 solver.cpp:334]     Train net output #0: loss = 1.50371 (* 1 = 1.50371 loss)
I0815 09:56:40.952913  8764 sgd_solver.cpp:136] Iteration 2400, lr = 0.00985, m = 0.9
I0815 09:56:55.548861  8764 solver.cpp:312] Iteration 2500 (6.85133 iter/s, 14.5957s/100 iter), loss = 1.19602
I0815 09:56:55.548889  8764 solver.cpp:334]     Train net output #0: loss = 1.01414 (* 1 = 1.01414 loss)
I0815 09:56:55.548895  8764 sgd_solver.cpp:136] Iteration 2500, lr = 0.00984375, m = 0.9
I0815 09:57:10.292160  8764 solver.cpp:312] Iteration 2600 (6.78293 iter/s, 14.7429s/100 iter), loss = 1.76389
I0815 09:57:10.292227  8764 solver.cpp:334]     Train net output #0: loss = 1.90808 (* 1 = 1.90808 loss)
I0815 09:57:10.292249  8764 sgd_solver.cpp:136] Iteration 2600, lr = 0.0098375, m = 0.9
I0815 09:57:24.831439  8764 solver.cpp:312] Iteration 2700 (6.87811 iter/s, 14.5389s/100 iter), loss = 1.26027
I0815 09:57:24.831634  8764 solver.cpp:334]     Train net output #0: loss = 1.29005 (* 1 = 1.29005 loss)
I0815 09:57:24.831653  8764 sgd_solver.cpp:136] Iteration 2700, lr = 0.00983125, m = 0.9
I0815 09:57:39.398829  8764 solver.cpp:312] Iteration 2800 (6.86484 iter/s, 14.567s/100 iter), loss = 1.44491
I0815 09:57:39.399049  8764 solver.cpp:334]     Train net output #0: loss = 1.67055 (* 1 = 1.67055 loss)
I0815 09:57:39.399157  8764 sgd_solver.cpp:136] Iteration 2800, lr = 0.009825, m = 0.9
I0815 09:57:54.184928  8764 solver.cpp:312] Iteration 2900 (6.7633 iter/s, 14.7857s/100 iter), loss = 1.4977
I0815 09:57:54.184957  8764 solver.cpp:334]     Train net output #0: loss = 1.46464 (* 1 = 1.46464 loss)
I0815 09:57:54.184963  8764 sgd_solver.cpp:136] Iteration 2900, lr = 0.00981875, m = 0.9
I0815 09:58:08.600488  8764 solver.cpp:363] Sparsity after update:
I0815 09:58:08.613029  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 09:58:08.613046  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 09:58:08.613054  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 09:58:08.613057  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 09:58:08.613061  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 09:58:08.613065  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 09:58:08.613067  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 09:58:08.613070  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 09:58:08.613073  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 09:58:08.613076  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 09:58:08.613080  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 09:58:08.613082  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 09:58:08.613085  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 09:58:08.747040  8764 solver.cpp:312] Iteration 3000 (6.86732 iter/s, 14.5617s/100 iter), loss = 1.48742
I0815 09:58:08.747104  8764 solver.cpp:334]     Train net output #0: loss = 1.4992 (* 1 = 1.4992 loss)
I0815 09:58:08.747122  8764 sgd_solver.cpp:136] Iteration 3000, lr = 0.0098125, m = 0.9
I0815 09:58:23.285490  8764 solver.cpp:312] Iteration 3100 (6.8785 iter/s, 14.5381s/100 iter), loss = 1.64661
I0815 09:58:23.285518  8764 solver.cpp:334]     Train net output #0: loss = 1.38105 (* 1 = 1.38105 loss)
I0815 09:58:23.285523  8764 sgd_solver.cpp:136] Iteration 3100, lr = 0.00980625, m = 0.9
I0815 09:58:37.973593  8764 solver.cpp:312] Iteration 3200 (6.80842 iter/s, 14.6877s/100 iter), loss = 1.45851
I0815 09:58:37.973623  8764 solver.cpp:334]     Train net output #0: loss = 1.53664 (* 1 = 1.53664 loss)
I0815 09:58:37.973630  8764 sgd_solver.cpp:136] Iteration 3200, lr = 0.0098, m = 0.9
I0815 09:58:52.581856  8764 solver.cpp:312] Iteration 3300 (6.84563 iter/s, 14.6079s/100 iter), loss = 1.24967
I0815 09:58:52.581924  8764 solver.cpp:334]     Train net output #0: loss = 1.31604 (* 1 = 1.31604 loss)
I0815 09:58:52.581931  8764 sgd_solver.cpp:136] Iteration 3300, lr = 0.00979375, m = 0.9
I0815 09:59:07.334108  8764 solver.cpp:312] Iteration 3400 (6.77881 iter/s, 14.7518s/100 iter), loss = 1.58056
I0815 09:59:07.334136  8764 solver.cpp:334]     Train net output #0: loss = 1.60852 (* 1 = 1.60852 loss)
I0815 09:59:07.334142  8764 sgd_solver.cpp:136] Iteration 3400, lr = 0.0097875, m = 0.9
I0815 09:59:21.718279  8764 solver.cpp:312] Iteration 3500 (6.95228 iter/s, 14.3838s/100 iter), loss = 1.92395
I0815 09:59:21.718350  8764 solver.cpp:334]     Train net output #0: loss = 1.60401 (* 1 = 1.60401 loss)
I0815 09:59:21.718369  8764 sgd_solver.cpp:136] Iteration 3500, lr = 0.00978125, m = 0.9
I0815 09:59:35.966784  8764 solver.cpp:312] Iteration 3600 (7.01847 iter/s, 14.2481s/100 iter), loss = 1.61741
I0815 09:59:35.966866  8764 solver.cpp:334]     Train net output #0: loss = 1.76983 (* 1 = 1.76983 loss)
I0815 09:59:35.966873  8764 sgd_solver.cpp:136] Iteration 3600, lr = 0.009775, m = 0.9
I0815 09:59:50.886339  8764 solver.cpp:312] Iteration 3700 (6.7028 iter/s, 14.9191s/100 iter), loss = 1.64132
I0815 09:59:50.886409  8764 solver.cpp:334]     Train net output #0: loss = 2.37452 (* 1 = 2.37452 loss)
I0815 09:59:50.886428  8764 sgd_solver.cpp:136] Iteration 3700, lr = 0.00976875, m = 0.9
I0815 10:00:05.344192  8764 solver.cpp:312] Iteration 3800 (6.91685 iter/s, 14.4575s/100 iter), loss = 1.88628
I0815 10:00:05.344220  8764 solver.cpp:334]     Train net output #0: loss = 2.00306 (* 1 = 2.00306 loss)
I0815 10:00:05.344226  8764 sgd_solver.cpp:136] Iteration 3800, lr = 0.0097625, m = 0.9
I0815 10:00:19.785884  8764 solver.cpp:312] Iteration 3900 (6.92459 iter/s, 14.4413s/100 iter), loss = 1.61876
I0815 10:00:19.788166  8764 solver.cpp:334]     Train net output #0: loss = 1.84803 (* 1 = 1.84803 loss)
I0815 10:00:19.788179  8764 sgd_solver.cpp:136] Iteration 3900, lr = 0.00975625, m = 0.9
I0815 10:00:34.244745  8764 solver.cpp:363] Sparsity after update:
I0815 10:00:34.248956  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:00:34.248980  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:00:34.248993  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:00:34.249001  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:00:34.249009  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:00:34.249017  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:00:34.249025  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:00:34.249033  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:00:34.249042  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:00:34.249048  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:00:34.249056  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:00:34.249064  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:00:34.249073  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:00:34.249088  8764 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 10:00:55.193785  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.552471
I0815 10:00:55.193835  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.78788
I0815 10:00:55.193840  8764 solver.cpp:594]     Test net output #2: loss = 1.98977 (* 1 = 1.98977 loss)
I0815 10:00:55.193859  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.9442s
I0815 10:00:55.342311  8764 solver.cpp:312] Iteration 4000 (2.81251 iter/s, 35.5555s/100 iter), loss = 1.44905
I0815 10:00:55.342339  8764 solver.cpp:334]     Train net output #0: loss = 1.55423 (* 1 = 1.55423 loss)
I0815 10:00:55.342345  8764 sgd_solver.cpp:136] Iteration 4000, lr = 0.00975, m = 0.9
I0815 10:01:09.781297  8764 solver.cpp:312] Iteration 4100 (6.92589 iter/s, 14.4386s/100 iter), loss = 1.39839
I0815 10:01:09.781325  8764 solver.cpp:334]     Train net output #0: loss = 1.49331 (* 1 = 1.49331 loss)
I0815 10:01:09.781330  8764 sgd_solver.cpp:136] Iteration 4100, lr = 0.00974375, m = 0.9
I0815 10:01:24.324801  8764 solver.cpp:312] Iteration 4200 (6.87611 iter/s, 14.5431s/100 iter), loss = 1.31043
I0815 10:01:24.324873  8764 solver.cpp:334]     Train net output #0: loss = 1.32433 (* 1 = 1.32433 loss)
I0815 10:01:24.324893  8764 sgd_solver.cpp:136] Iteration 4200, lr = 0.0097375, m = 0.9
I0815 10:01:39.014941  8764 solver.cpp:312] Iteration 4300 (6.80748 iter/s, 14.6897s/100 iter), loss = 1.7333
I0815 10:01:39.015000  8764 solver.cpp:334]     Train net output #0: loss = 1.90661 (* 1 = 1.90661 loss)
I0815 10:01:39.015007  8764 sgd_solver.cpp:136] Iteration 4300, lr = 0.00973125, m = 0.9
I0815 10:01:53.663494  8764 solver.cpp:312] Iteration 4400 (6.8268 iter/s, 14.6481s/100 iter), loss = 1.60305
I0815 10:01:53.663570  8764 solver.cpp:334]     Train net output #0: loss = 1.74804 (* 1 = 1.74804 loss)
I0815 10:01:53.663590  8764 sgd_solver.cpp:136] Iteration 4400, lr = 0.009725, m = 0.9
I0815 10:02:08.045459  8764 solver.cpp:312] Iteration 4500 (6.95334 iter/s, 14.3816s/100 iter), loss = 1.75029
I0815 10:02:08.045526  8764 solver.cpp:334]     Train net output #0: loss = 1.97829 (* 1 = 1.97829 loss)
I0815 10:02:08.045545  8764 sgd_solver.cpp:136] Iteration 4500, lr = 0.00971875, m = 0.9
I0815 10:02:22.421499  8764 solver.cpp:312] Iteration 4600 (6.95621 iter/s, 14.3756s/100 iter), loss = 1.55973
I0815 10:02:22.421572  8764 solver.cpp:334]     Train net output #0: loss = 1.50551 (* 1 = 1.50551 loss)
I0815 10:02:22.421579  8764 sgd_solver.cpp:136] Iteration 4600, lr = 0.0097125, m = 0.9
I0815 10:02:37.009093  8764 solver.cpp:312] Iteration 4700 (6.85533 iter/s, 14.5872s/100 iter), loss = 1.67851
I0815 10:02:37.009117  8764 solver.cpp:334]     Train net output #0: loss = 1.56117 (* 1 = 1.56117 loss)
I0815 10:02:37.009124  8764 sgd_solver.cpp:136] Iteration 4700, lr = 0.00970625, m = 0.9
I0815 10:02:51.685168  8764 solver.cpp:312] Iteration 4800 (6.814 iter/s, 14.6757s/100 iter), loss = 1.47144
I0815 10:02:51.685197  8764 solver.cpp:334]     Train net output #0: loss = 2.09591 (* 1 = 2.09591 loss)
I0815 10:02:51.685204  8764 sgd_solver.cpp:136] Iteration 4800, lr = 0.0097, m = 0.9
I0815 10:03:06.424181  8764 solver.cpp:312] Iteration 4900 (6.7849 iter/s, 14.7386s/100 iter), loss = 1.56856
I0815 10:03:06.424263  8764 solver.cpp:334]     Train net output #0: loss = 1.57836 (* 1 = 1.57836 loss)
I0815 10:03:06.424273  8764 sgd_solver.cpp:136] Iteration 4900, lr = 0.00969375, m = 0.9
I0815 10:03:20.692587  8764 solver.cpp:363] Sparsity after update:
I0815 10:03:20.704169  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:03:20.704185  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:03:20.704192  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:03:20.704195  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:03:20.704200  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:03:20.704215  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:03:20.704226  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:03:20.704236  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:03:20.704244  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:03:20.704253  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:03:20.704262  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:03:20.704272  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:03:20.704279  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:03:20.842365  8764 solver.cpp:312] Iteration 5000 (6.93588 iter/s, 14.4178s/100 iter), loss = 1.38993
I0815 10:03:20.842388  8764 solver.cpp:334]     Train net output #0: loss = 1.76336 (* 1 = 1.76336 loss)
I0815 10:03:20.842393  8764 sgd_solver.cpp:136] Iteration 5000, lr = 0.0096875, m = 0.9
I0815 10:03:35.227777  8764 solver.cpp:312] Iteration 5100 (6.95168 iter/s, 14.385s/100 iter), loss = 1.41983
I0815 10:03:35.227847  8764 solver.cpp:334]     Train net output #0: loss = 1.44808 (* 1 = 1.44808 loss)
I0815 10:03:35.227866  8764 sgd_solver.cpp:136] Iteration 5100, lr = 0.00968125, m = 0.9
I0815 10:03:49.668843  8764 solver.cpp:312] Iteration 5200 (6.92489 iter/s, 14.4407s/100 iter), loss = 1.40807
I0815 10:03:49.674190  8764 solver.cpp:334]     Train net output #0: loss = 1.40904 (* 1 = 1.40904 loss)
I0815 10:03:49.674201  8764 sgd_solver.cpp:136] Iteration 5200, lr = 0.009675, m = 0.9
I0815 10:04:04.184106  8764 solver.cpp:312] Iteration 5300 (6.88949 iter/s, 14.5149s/100 iter), loss = 1.64604
I0815 10:04:04.184134  8764 solver.cpp:334]     Train net output #0: loss = 1.57986 (* 1 = 1.57986 loss)
I0815 10:04:04.184140  8764 sgd_solver.cpp:136] Iteration 5300, lr = 0.00966875, m = 0.9
I0815 10:04:18.856452  8764 solver.cpp:312] Iteration 5400 (6.81573 iter/s, 14.6719s/100 iter), loss = 1.47968
I0815 10:04:18.856609  8764 solver.cpp:334]     Train net output #0: loss = 1.59342 (* 1 = 1.59342 loss)
I0815 10:04:18.856631  8764 sgd_solver.cpp:136] Iteration 5400, lr = 0.0096625, m = 0.9
I0815 10:04:33.564191  8764 solver.cpp:312] Iteration 5500 (6.79933 iter/s, 14.7073s/100 iter), loss = 1.34736
I0815 10:04:33.568195  8764 solver.cpp:334]     Train net output #0: loss = 1.30237 (* 1 = 1.30237 loss)
I0815 10:04:33.568217  8764 sgd_solver.cpp:136] Iteration 5500, lr = 0.00965625, m = 0.9
I0815 10:04:48.212049  8764 solver.cpp:312] Iteration 5600 (6.82713 iter/s, 14.6475s/100 iter), loss = 1.51493
I0815 10:04:48.212077  8764 solver.cpp:334]     Train net output #0: loss = 1.55066 (* 1 = 1.55066 loss)
I0815 10:04:48.212083  8764 sgd_solver.cpp:136] Iteration 5600, lr = 0.00965, m = 0.9
I0815 10:05:02.726135  8764 solver.cpp:312] Iteration 5700 (6.89005 iter/s, 14.5137s/100 iter), loss = 1.66847
I0815 10:05:02.726162  8764 solver.cpp:334]     Train net output #0: loss = 1.77811 (* 1 = 1.77811 loss)
I0815 10:05:02.726168  8764 sgd_solver.cpp:136] Iteration 5700, lr = 0.00964375, m = 0.9
I0815 10:05:17.023833  8764 solver.cpp:312] Iteration 5800 (6.99433 iter/s, 14.2973s/100 iter), loss = 1.42647
I0815 10:05:17.023937  8764 solver.cpp:334]     Train net output #0: loss = 1.56034 (* 1 = 1.56034 loss)
I0815 10:05:17.023954  8764 sgd_solver.cpp:136] Iteration 5800, lr = 0.0096375, m = 0.9
I0815 10:05:31.520084  8764 solver.cpp:312] Iteration 5900 (6.89852 iter/s, 14.4959s/100 iter), loss = 1.28274
I0815 10:05:31.520110  8764 solver.cpp:334]     Train net output #0: loss = 1.21825 (* 1 = 1.21825 loss)
I0815 10:05:31.520117  8764 sgd_solver.cpp:136] Iteration 5900, lr = 0.00963125, m = 0.9
I0815 10:05:45.834892  8764 solver.cpp:363] Sparsity after update:
I0815 10:05:45.839905  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:05:45.839943  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:05:45.839962  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:05:45.839974  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:05:45.839987  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:05:45.839999  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:05:45.840013  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:05:45.840026  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:05:45.840039  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:05:45.840051  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:05:45.840064  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:05:45.840075  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:05:45.840086  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:05:45.840106  8764 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 10:06:04.378435  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:06:06.680872  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.555765
I0815 10:06:06.680894  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790644
I0815 10:06:06.680901  8764 solver.cpp:594]     Test net output #2: loss = 1.99858 (* 1 = 1.99858 loss)
I0815 10:06:06.680917  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8402s
I0815 10:06:06.828153  8764 solver.cpp:312] Iteration 6000 (2.83229 iter/s, 35.3071s/100 iter), loss = 1.44797
I0815 10:06:06.828181  8764 solver.cpp:334]     Train net output #0: loss = 1.55907 (* 1 = 1.55907 loss)
I0815 10:06:06.828187  8764 sgd_solver.cpp:136] Iteration 6000, lr = 0.009625, m = 0.9
I0815 10:06:21.621286  8764 solver.cpp:312] Iteration 6100 (6.76008 iter/s, 14.7927s/100 iter), loss = 1.54992
I0815 10:06:21.621314  8764 solver.cpp:334]     Train net output #0: loss = 1.49461 (* 1 = 1.49461 loss)
I0815 10:06:21.621318  8764 sgd_solver.cpp:136] Iteration 6100, lr = 0.00961875, m = 0.9
I0815 10:06:36.717360  8764 solver.cpp:312] Iteration 6200 (6.62442 iter/s, 15.0957s/100 iter), loss = 1.8655
I0815 10:06:36.717408  8764 solver.cpp:334]     Train net output #0: loss = 1.47909 (* 1 = 1.47909 loss)
I0815 10:06:36.717416  8764 sgd_solver.cpp:136] Iteration 6200, lr = 0.0096125, m = 0.9
I0815 10:06:51.205816  8764 solver.cpp:312] Iteration 6300 (6.90224 iter/s, 14.4881s/100 iter), loss = 1.33149
I0815 10:06:51.205843  8764 solver.cpp:334]     Train net output #0: loss = 1.20779 (* 1 = 1.20779 loss)
I0815 10:06:51.205849  8764 sgd_solver.cpp:136] Iteration 6300, lr = 0.00960625, m = 0.9
I0815 10:07:05.798765  8764 solver.cpp:312] Iteration 6400 (6.85281 iter/s, 14.5925s/100 iter), loss = 1.28445
I0815 10:07:05.798792  8764 solver.cpp:334]     Train net output #0: loss = 1.50755 (* 1 = 1.50755 loss)
I0815 10:07:05.798797  8764 sgd_solver.cpp:136] Iteration 6400, lr = 0.0096, m = 0.9
I0815 10:07:20.356020  8764 solver.cpp:312] Iteration 6500 (6.86962 iter/s, 14.5569s/100 iter), loss = 1.51377
I0815 10:07:20.356122  8764 solver.cpp:334]     Train net output #0: loss = 1.51061 (* 1 = 1.51061 loss)
I0815 10:07:20.356143  8764 sgd_solver.cpp:136] Iteration 6500, lr = 0.00959375, m = 0.9
I0815 10:07:34.969367  8764 solver.cpp:312] Iteration 6600 (6.84325 iter/s, 14.6129s/100 iter), loss = 1.25744
I0815 10:07:34.969394  8764 solver.cpp:334]     Train net output #0: loss = 0.886921 (* 1 = 0.886921 loss)
I0815 10:07:34.969399  8764 sgd_solver.cpp:136] Iteration 6600, lr = 0.0095875, m = 0.9
I0815 10:07:49.511700  8764 solver.cpp:312] Iteration 6700 (6.87667 iter/s, 14.5419s/100 iter), loss = 1.59163
I0815 10:07:49.511729  8764 solver.cpp:334]     Train net output #0: loss = 1.86037 (* 1 = 1.86037 loss)
I0815 10:07:49.511735  8764 sgd_solver.cpp:136] Iteration 6700, lr = 0.00958125, m = 0.9
I0815 10:08:03.758765  8764 solver.cpp:312] Iteration 6800 (7.01918 iter/s, 14.2467s/100 iter), loss = 1.26957
I0815 10:08:03.759009  8764 solver.cpp:334]     Train net output #0: loss = 1.62755 (* 1 = 1.62755 loss)
I0815 10:08:03.759120  8764 sgd_solver.cpp:136] Iteration 6800, lr = 0.009575, m = 0.9
I0815 10:08:18.052237  8764 solver.cpp:312] Iteration 6900 (6.99639 iter/s, 14.2931s/100 iter), loss = 1.4879
I0815 10:08:18.052443  8764 solver.cpp:334]     Train net output #0: loss = 1.23992 (* 1 = 1.23992 loss)
I0815 10:08:18.052551  8764 sgd_solver.cpp:136] Iteration 6900, lr = 0.00956875, m = 0.9
I0815 10:08:32.328367  8764 solver.cpp:363] Sparsity after update:
I0815 10:08:32.340636  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:08:32.340678  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:08:32.340692  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:08:32.340700  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:08:32.340708  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:08:32.340716  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:08:32.340724  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:08:32.340731  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:08:32.340739  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:08:32.340747  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:08:32.340755  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:08:32.340762  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:08:32.340770  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:08:32.481984  8764 solver.cpp:312] Iteration 7000 (6.93032 iter/s, 14.4293s/100 iter), loss = 1.48696
I0815 10:08:32.482012  8764 solver.cpp:334]     Train net output #0: loss = 1.2886 (* 1 = 1.2886 loss)
I0815 10:08:32.482018  8764 sgd_solver.cpp:136] Iteration 7000, lr = 0.0095625, m = 0.9
I0815 10:08:46.978690  8764 solver.cpp:312] Iteration 7100 (6.89831 iter/s, 14.4963s/100 iter), loss = 1.78903
I0815 10:08:46.978775  8764 solver.cpp:334]     Train net output #0: loss = 1.85591 (* 1 = 1.85591 loss)
I0815 10:08:46.978788  8764 sgd_solver.cpp:136] Iteration 7100, lr = 0.00955625, m = 0.9
I0815 10:09:01.387362  8764 solver.cpp:312] Iteration 7200 (6.94046 iter/s, 14.4083s/100 iter), loss = 1.9599
I0815 10:09:01.387392  8764 solver.cpp:334]     Train net output #0: loss = 1.90156 (* 1 = 1.90156 loss)
I0815 10:09:01.387399  8764 sgd_solver.cpp:136] Iteration 7200, lr = 0.00955, m = 0.9
I0815 10:09:15.900842  8764 solver.cpp:312] Iteration 7300 (6.89033 iter/s, 14.5131s/100 iter), loss = 1.26201
I0815 10:09:15.900877  8764 solver.cpp:334]     Train net output #0: loss = 1.13923 (* 1 = 1.13923 loss)
I0815 10:09:15.900883  8764 sgd_solver.cpp:136] Iteration 7300, lr = 0.00954375, m = 0.9
I0815 10:09:30.452217  8764 solver.cpp:312] Iteration 7400 (6.87239 iter/s, 14.551s/100 iter), loss = 1.56688
I0815 10:09:30.452335  8764 solver.cpp:334]     Train net output #0: loss = 1.65461 (* 1 = 1.65461 loss)
I0815 10:09:30.452353  8764 sgd_solver.cpp:136] Iteration 7400, lr = 0.0095375, m = 0.9
I0815 10:09:45.231451  8764 solver.cpp:312] Iteration 7500 (6.76644 iter/s, 14.7788s/100 iter), loss = 1.65908
I0815 10:09:45.231475  8764 solver.cpp:334]     Train net output #0: loss = 1.94755 (* 1 = 1.94755 loss)
I0815 10:09:45.231479  8764 sgd_solver.cpp:136] Iteration 7500, lr = 0.00953125, m = 0.9
I0815 10:09:59.848242  8764 solver.cpp:312] Iteration 7600 (6.84163 iter/s, 14.6164s/100 iter), loss = 1.23027
I0815 10:09:59.848317  8764 solver.cpp:334]     Train net output #0: loss = 1.13527 (* 1 = 1.13527 loss)
I0815 10:09:59.848336  8764 sgd_solver.cpp:136] Iteration 7600, lr = 0.009525, m = 0.9
I0815 10:10:14.572152  8764 solver.cpp:312] Iteration 7700 (6.79186 iter/s, 14.7235s/100 iter), loss = 1.23974
I0815 10:10:14.572391  8764 solver.cpp:334]     Train net output #0: loss = 1.25319 (* 1 = 1.25319 loss)
I0815 10:10:14.572412  8764 sgd_solver.cpp:136] Iteration 7700, lr = 0.00951875, m = 0.9
I0815 10:10:28.965420  8764 solver.cpp:312] Iteration 7800 (6.94788 iter/s, 14.3929s/100 iter), loss = 1.4441
I0815 10:10:28.965446  8764 solver.cpp:334]     Train net output #0: loss = 1.59873 (* 1 = 1.59873 loss)
I0815 10:10:28.965452  8764 sgd_solver.cpp:136] Iteration 7800, lr = 0.0095125, m = 0.9
I0815 10:10:43.292306  8764 solver.cpp:312] Iteration 7900 (6.98008 iter/s, 14.3265s/100 iter), loss = 1.12198
I0815 10:10:43.292335  8764 solver.cpp:334]     Train net output #0: loss = 1.30991 (* 1 = 1.30991 loss)
I0815 10:10:43.292340  8764 sgd_solver.cpp:136] Iteration 7900, lr = 0.00950625, m = 0.9
I0815 10:10:57.863756  8764 solver.cpp:363] Sparsity after update:
I0815 10:10:57.867698  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:10:57.867735  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:10:57.867753  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:10:57.867765  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:10:57.867777  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:10:57.867789  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:10:57.867801  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:10:57.867812  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:10:57.867825  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:10:57.867835  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:10:57.867852  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:10:57.867864  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:10:57.867877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:10:57.867897  8764 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 10:11:00.857771  8748 blocking_queue.cpp:40] Waiting for datum
I0815 10:11:18.963187  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.539059
I0815 10:11:18.963217  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.782704
I0815 10:11:18.963224  8764 solver.cpp:594]     Test net output #2: loss = 2.05523 (* 1 = 2.05523 loss)
I0815 10:11:18.963301  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0948s
I0815 10:11:19.112325  8764 solver.cpp:312] Iteration 8000 (2.79181 iter/s, 35.819s/100 iter), loss = 1.34738
I0815 10:11:19.112349  8764 solver.cpp:334]     Train net output #0: loss = 1.4008 (* 1 = 1.4008 loss)
I0815 10:11:19.112354  8764 sgd_solver.cpp:136] Iteration 8000, lr = 0.0095, m = 0.9
I0815 10:11:33.472110  8764 solver.cpp:312] Iteration 8100 (6.96409 iter/s, 14.3594s/100 iter), loss = 1.68998
I0815 10:11:33.472194  8764 solver.cpp:334]     Train net output #0: loss = 1.89889 (* 1 = 1.89889 loss)
I0815 10:11:33.472208  8764 sgd_solver.cpp:136] Iteration 8100, lr = 0.00949375, m = 0.9
I0815 10:11:47.854619  8764 solver.cpp:312] Iteration 8200 (6.95308 iter/s, 14.3821s/100 iter), loss = 1.45251
I0815 10:11:47.854645  8764 solver.cpp:334]     Train net output #0: loss = 1.55172 (* 1 = 1.55172 loss)
I0815 10:11:47.854648  8764 sgd_solver.cpp:136] Iteration 8200, lr = 0.0094875, m = 0.9
I0815 10:12:02.431308  8764 solver.cpp:312] Iteration 8300 (6.86046 iter/s, 14.5763s/100 iter), loss = 1.16528
I0815 10:12:02.431372  8764 solver.cpp:334]     Train net output #0: loss = 1.31213 (* 1 = 1.31213 loss)
I0815 10:12:02.431390  8764 sgd_solver.cpp:136] Iteration 8300, lr = 0.00948125, m = 0.9
I0815 10:12:17.024359  8764 solver.cpp:312] Iteration 8400 (6.85277 iter/s, 14.5926s/100 iter), loss = 1.32743
I0815 10:12:17.024468  8764 solver.cpp:334]     Train net output #0: loss = 1.56869 (* 1 = 1.56869 loss)
I0815 10:12:17.024487  8764 sgd_solver.cpp:136] Iteration 8400, lr = 0.009475, m = 0.9
I0815 10:12:31.603361  8764 solver.cpp:312] Iteration 8500 (6.85937 iter/s, 14.5786s/100 iter), loss = 1.61987
I0815 10:12:31.603428  8764 solver.cpp:334]     Train net output #0: loss = 1.6365 (* 1 = 1.6365 loss)
I0815 10:12:31.603437  8764 sgd_solver.cpp:136] Iteration 8500, lr = 0.00946875, m = 0.9
I0815 10:12:46.108028  8764 solver.cpp:312] Iteration 8600 (6.89452 iter/s, 14.5043s/100 iter), loss = 1.66105
I0815 10:12:46.108057  8764 solver.cpp:334]     Train net output #0: loss = 1.62928 (* 1 = 1.62928 loss)
I0815 10:12:46.108062  8764 sgd_solver.cpp:136] Iteration 8600, lr = 0.0094625, m = 0.9
I0815 10:13:00.783509  8764 solver.cpp:312] Iteration 8700 (6.81428 iter/s, 14.6751s/100 iter), loss = 1.109
I0815 10:13:00.783748  8764 solver.cpp:334]     Train net output #0: loss = 1.08699 (* 1 = 1.08699 loss)
I0815 10:13:00.783856  8764 sgd_solver.cpp:136] Iteration 8700, lr = 0.00945625, m = 0.9
I0815 10:13:15.143515  8764 solver.cpp:312] Iteration 8800 (6.96398 iter/s, 14.3596s/100 iter), loss = 1.33345
I0815 10:13:15.143576  8764 solver.cpp:334]     Train net output #0: loss = 1.6936 (* 1 = 1.6936 loss)
I0815 10:13:15.143594  8764 sgd_solver.cpp:136] Iteration 8800, lr = 0.00945, m = 0.9
I0815 10:13:29.932106  8764 solver.cpp:312] Iteration 8900 (6.76216 iter/s, 14.7882s/100 iter), loss = 1.36181
I0815 10:13:29.932142  8764 solver.cpp:334]     Train net output #0: loss = 1.24685 (* 1 = 1.24685 loss)
I0815 10:13:29.932148  8764 sgd_solver.cpp:136] Iteration 8900, lr = 0.00944375, m = 0.9
I0815 10:13:44.070381  8764 solver.cpp:363] Sparsity after update:
I0815 10:13:44.080818  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:13:44.080857  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:13:44.080875  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:13:44.080888  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:13:44.080904  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:13:44.080916  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:13:44.080929  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:13:44.080940  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:13:44.080951  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:13:44.080962  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:13:44.080973  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:13:44.080984  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:13:44.080996  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:13:44.230854  8764 solver.cpp:312] Iteration 9000 (6.99381 iter/s, 14.2984s/100 iter), loss = 1.46253
I0815 10:13:44.230880  8764 solver.cpp:334]     Train net output #0: loss = 1.52149 (* 1 = 1.52149 loss)
I0815 10:13:44.230885  8764 sgd_solver.cpp:136] Iteration 9000, lr = 0.0094375, m = 0.9
I0815 10:13:58.656586  8764 solver.cpp:312] Iteration 9100 (6.93225 iter/s, 14.4253s/100 iter), loss = 1.40323
I0815 10:13:58.656637  8764 solver.cpp:334]     Train net output #0: loss = 1.06087 (* 1 = 1.06087 loss)
I0815 10:13:58.656651  8764 sgd_solver.cpp:136] Iteration 9100, lr = 0.00943125, m = 0.9
I0815 10:14:13.218739  8764 solver.cpp:312] Iteration 9200 (6.86731 iter/s, 14.5617s/100 iter), loss = 1.55577
I0815 10:14:13.218770  8764 solver.cpp:334]     Train net output #0: loss = 1.69355 (* 1 = 1.69355 loss)
I0815 10:14:13.218775  8764 sgd_solver.cpp:136] Iteration 9200, lr = 0.009425, m = 0.9
I0815 10:14:27.995007  8764 solver.cpp:312] Iteration 9300 (6.7678 iter/s, 14.7759s/100 iter), loss = 1.74798
I0815 10:14:27.995268  8764 solver.cpp:334]     Train net output #0: loss = 2.06613 (* 1 = 2.06613 loss)
I0815 10:14:27.995383  8764 sgd_solver.cpp:136] Iteration 9300, lr = 0.00941875, m = 0.9
I0815 10:14:42.596657  8764 solver.cpp:312] Iteration 9400 (6.84873 iter/s, 14.6012s/100 iter), loss = 1.24374
I0815 10:14:42.596689  8764 solver.cpp:334]     Train net output #0: loss = 0.800043 (* 1 = 0.800043 loss)
I0815 10:14:42.596694  8764 sgd_solver.cpp:136] Iteration 9400, lr = 0.0094125, m = 0.9
I0815 10:14:57.065856  8764 solver.cpp:312] Iteration 9500 (6.91142 iter/s, 14.4688s/100 iter), loss = 1.72969
I0815 10:14:57.065883  8764 solver.cpp:334]     Train net output #0: loss = 1.68036 (* 1 = 1.68036 loss)
I0815 10:14:57.065891  8764 sgd_solver.cpp:136] Iteration 9500, lr = 0.00940625, m = 0.9
I0815 10:15:11.534775  8764 solver.cpp:312] Iteration 9600 (6.91156 iter/s, 14.4685s/100 iter), loss = 1.70527
I0815 10:15:11.535009  8764 solver.cpp:334]     Train net output #0: loss = 1.89232 (* 1 = 1.89232 loss)
I0815 10:15:11.535092  8764 sgd_solver.cpp:136] Iteration 9600, lr = 0.0094, m = 0.9
I0815 10:15:25.859203  8764 solver.cpp:312] Iteration 9700 (6.98127 iter/s, 14.324s/100 iter), loss = 1.5291
I0815 10:15:25.859232  8764 solver.cpp:334]     Train net output #0: loss = 1.56542 (* 1 = 1.56542 loss)
I0815 10:15:25.859239  8764 sgd_solver.cpp:136] Iteration 9700, lr = 0.00939375, m = 0.9
I0815 10:15:40.385224  8764 solver.cpp:312] Iteration 9800 (6.88438 iter/s, 14.5256s/100 iter), loss = 1.23557
I0815 10:15:40.385251  8764 solver.cpp:334]     Train net output #0: loss = 1.42344 (* 1 = 1.42344 loss)
I0815 10:15:40.385257  8764 sgd_solver.cpp:136] Iteration 9800, lr = 0.0093875, m = 0.9
I0815 10:15:54.808027  8764 solver.cpp:312] Iteration 9900 (6.93366 iter/s, 14.4224s/100 iter), loss = 1.09155
I0815 10:15:54.808845  8764 solver.cpp:334]     Train net output #0: loss = 1.02268 (* 1 = 1.02268 loss)
I0815 10:15:54.808866  8764 sgd_solver.cpp:136] Iteration 9900, lr = 0.00938125, m = 0.9
I0815 10:16:09.261303  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_10000.caffemodel
I0815 10:16:09.311563  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_10000.solverstate
I0815 10:16:09.321283  8764 solver.cpp:363] Sparsity after update:
I0815 10:16:09.335827  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:16:09.335850  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:16:09.335855  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:16:09.335860  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:16:09.335862  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:16:09.335865  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:16:09.335868  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:16:09.335871  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:16:09.335875  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:16:09.335880  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:16:09.335883  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:16:09.335887  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:16:09.335891  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:16:09.335903  8764 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 10:16:29.889214  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.537
I0815 10:16:29.889273  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.776292
I0815 10:16:29.889282  8764 solver.cpp:594]     Test net output #2: loss = 2.05574 (* 1 = 2.05574 loss)
I0815 10:16:29.889303  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.5528s
I0815 10:16:30.062683  8764 solver.cpp:312] Iteration 10000 (2.83658 iter/s, 35.2537s/100 iter), loss = 1.57708
I0815 10:16:30.062749  8764 solver.cpp:334]     Train net output #0: loss = 1.36222 (* 1 = 1.36222 loss)
I0815 10:16:30.062769  8764 sgd_solver.cpp:136] Iteration 10000, lr = 0.009375, m = 0.9
I0815 10:16:44.423110  8764 solver.cpp:312] Iteration 10100 (6.96377 iter/s, 14.36s/100 iter), loss = 1.08106
I0815 10:16:44.423136  8764 solver.cpp:334]     Train net output #0: loss = 1.28649 (* 1 = 1.28649 loss)
I0815 10:16:44.423142  8764 sgd_solver.cpp:136] Iteration 10100, lr = 0.00936875, m = 0.9
I0815 10:16:58.842099  8764 solver.cpp:312] Iteration 10200 (6.93549 iter/s, 14.4186s/100 iter), loss = 1.46343
I0815 10:16:58.842173  8764 solver.cpp:334]     Train net output #0: loss = 1.43318 (* 1 = 1.43318 loss)
I0815 10:16:58.842192  8764 sgd_solver.cpp:136] Iteration 10200, lr = 0.0093625, m = 0.9
I0815 10:17:13.284518  8764 solver.cpp:312] Iteration 10300 (6.92424 iter/s, 14.442s/100 iter), loss = 1.35174
I0815 10:17:13.284637  8764 solver.cpp:334]     Train net output #0: loss = 1.64232 (* 1 = 1.64232 loss)
I0815 10:17:13.284657  8764 sgd_solver.cpp:136] Iteration 10300, lr = 0.00935625, m = 0.9
I0815 10:17:27.795475  8764 solver.cpp:312] Iteration 10400 (6.89153 iter/s, 14.5106s/100 iter), loss = 1.17509
I0815 10:17:27.795497  8764 solver.cpp:334]     Train net output #0: loss = 0.878668 (* 1 = 0.878668 loss)
I0815 10:17:27.795501  8764 sgd_solver.cpp:136] Iteration 10400, lr = 0.00935, m = 0.9
I0815 10:17:42.286389  8764 solver.cpp:312] Iteration 10500 (6.90107 iter/s, 14.4905s/100 iter), loss = 1.08584
I0815 10:17:42.286440  8764 solver.cpp:334]     Train net output #0: loss = 0.902722 (* 1 = 0.902722 loss)
I0815 10:17:42.286453  8764 sgd_solver.cpp:136] Iteration 10500, lr = 0.00934375, m = 0.9
I0815 10:17:56.922474  8764 solver.cpp:312] Iteration 10600 (6.83262 iter/s, 14.6357s/100 iter), loss = 1.47749
I0815 10:17:56.922554  8764 solver.cpp:334]     Train net output #0: loss = 1.4628 (* 1 = 1.4628 loss)
I0815 10:17:56.922562  8764 sgd_solver.cpp:136] Iteration 10600, lr = 0.0093375, m = 0.9
I0815 10:18:11.526206  8764 solver.cpp:312] Iteration 10700 (6.84775 iter/s, 14.6033s/100 iter), loss = 1.72734
I0815 10:18:11.526232  8764 solver.cpp:334]     Train net output #0: loss = 1.36186 (* 1 = 1.36186 loss)
I0815 10:18:11.526239  8764 sgd_solver.cpp:136] Iteration 10700, lr = 0.00933125, m = 0.9
I0815 10:18:25.887877  8764 solver.cpp:312] Iteration 10800 (6.96317 iter/s, 14.3613s/100 iter), loss = 1.27609
I0815 10:18:25.887903  8764 solver.cpp:334]     Train net output #0: loss = 1.13368 (* 1 = 1.13368 loss)
I0815 10:18:25.887907  8764 sgd_solver.cpp:136] Iteration 10800, lr = 0.009325, m = 0.9
I0815 10:18:40.281808  8764 solver.cpp:312] Iteration 10900 (6.94756 iter/s, 14.3935s/100 iter), loss = 1.36548
I0815 10:18:40.281898  8764 solver.cpp:334]     Train net output #0: loss = 1.3521 (* 1 = 1.3521 loss)
I0815 10:18:40.281915  8764 sgd_solver.cpp:136] Iteration 10900, lr = 0.00931875, m = 0.9
I0815 10:18:54.623863  8764 solver.cpp:363] Sparsity after update:
I0815 10:18:54.636503  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:18:54.636543  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:18:54.636562  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:18:54.636574  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:18:54.636585  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:18:54.636597  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:18:54.636610  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:18:54.636620  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:18:54.636632  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:18:54.636643  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:18:54.636654  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:18:54.636667  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:18:54.636677  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:18:54.771378  8764 solver.cpp:312] Iteration 11000 (6.90171 iter/s, 14.4892s/100 iter), loss = 1.12952
I0815 10:18:54.771445  8764 solver.cpp:334]     Train net output #0: loss = 1.15626 (* 1 = 1.15626 loss)
I0815 10:18:54.771462  8764 sgd_solver.cpp:136] Iteration 11000, lr = 0.0093125, m = 0.9
I0815 10:19:09.100778  8764 solver.cpp:312] Iteration 11100 (6.97885 iter/s, 14.329s/100 iter), loss = 1.46508
I0815 10:19:09.100800  8764 solver.cpp:334]     Train net output #0: loss = 1.29209 (* 1 = 1.29209 loss)
I0815 10:19:09.100805  8764 sgd_solver.cpp:136] Iteration 11100, lr = 0.00930625, m = 0.9
I0815 10:19:23.618122  8764 solver.cpp:312] Iteration 11200 (6.8885 iter/s, 14.5169s/100 iter), loss = 1.26725
I0815 10:19:23.618191  8764 solver.cpp:334]     Train net output #0: loss = 1.16406 (* 1 = 1.16406 loss)
I0815 10:19:23.618197  8764 sgd_solver.cpp:136] Iteration 11200, lr = 0.0093, m = 0.9
I0815 10:19:38.304913  8764 solver.cpp:312] Iteration 11300 (6.80903 iter/s, 14.6864s/100 iter), loss = 1.43913
I0815 10:19:38.304977  8764 solver.cpp:334]     Train net output #0: loss = 1.46051 (* 1 = 1.46051 loss)
I0815 10:19:38.304996  8764 sgd_solver.cpp:136] Iteration 11300, lr = 0.00929375, m = 0.9
I0815 10:19:52.677225  8764 solver.cpp:312] Iteration 11400 (6.95801 iter/s, 14.3719s/100 iter), loss = 1.7589
I0815 10:19:52.677253  8764 solver.cpp:334]     Train net output #0: loss = 1.6674 (* 1 = 1.6674 loss)
I0815 10:19:52.677259  8764 sgd_solver.cpp:136] Iteration 11400, lr = 0.0092875, m = 0.9
I0815 10:20:07.105329  8764 solver.cpp:312] Iteration 11500 (6.93111 iter/s, 14.4277s/100 iter), loss = 2.04712
I0815 10:20:07.105429  8764 solver.cpp:334]     Train net output #0: loss = 1.78589 (* 1 = 1.78589 loss)
I0815 10:20:07.105448  8764 sgd_solver.cpp:136] Iteration 11500, lr = 0.00928125, m = 0.9
I0815 10:20:21.500174  8764 solver.cpp:312] Iteration 11600 (6.94712 iter/s, 14.3944s/100 iter), loss = 1.10706
I0815 10:20:21.500201  8764 solver.cpp:334]     Train net output #0: loss = 1.12198 (* 1 = 1.12198 loss)
I0815 10:20:21.500206  8764 sgd_solver.cpp:136] Iteration 11600, lr = 0.009275, m = 0.9
I0815 10:20:35.868270  8764 solver.cpp:312] Iteration 11700 (6.96006 iter/s, 14.3677s/100 iter), loss = 1.30817
I0815 10:20:35.868297  8764 solver.cpp:334]     Train net output #0: loss = 1.3164 (* 1 = 1.3164 loss)
I0815 10:20:35.868304  8764 sgd_solver.cpp:136] Iteration 11700, lr = 0.00926875, m = 0.9
I0815 10:20:50.505903  8764 solver.cpp:312] Iteration 11800 (6.83189 iter/s, 14.6372s/100 iter), loss = 1.44771
I0815 10:20:50.506688  8764 solver.cpp:334]     Train net output #0: loss = 1.05362 (* 1 = 1.05362 loss)
I0815 10:20:50.506697  8764 sgd_solver.cpp:136] Iteration 11800, lr = 0.0092625, m = 0.9
I0815 10:21:04.811462  8764 solver.cpp:312] Iteration 11900 (6.99048 iter/s, 14.3052s/100 iter), loss = 1.71835
I0815 10:21:04.811486  8764 solver.cpp:334]     Train net output #0: loss = 2.11933 (* 1 = 2.11933 loss)
I0815 10:21:04.811491  8764 sgd_solver.cpp:136] Iteration 11900, lr = 0.00925625, m = 0.9
I0815 10:21:19.192898  8764 solver.cpp:363] Sparsity after update:
I0815 10:21:19.197028  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:21:19.197149  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:21:19.197227  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:21:19.197309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:21:19.197382  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:21:19.197459  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:21:19.197525  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:21:19.197592  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:21:19.197674  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:21:19.197751  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:21:19.197824  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:21:19.197891  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:21:19.197969  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:21:19.198057  8764 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 10:21:35.636730  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:21:40.306510  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.541177
I0815 10:21:40.306530  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.777645
I0815 10:21:40.306535  8764 solver.cpp:594]     Test net output #2: loss = 2.05769 (* 1 = 2.05769 loss)
I0815 10:21:40.306557  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1079s
I0815 10:21:40.494673  8764 solver.cpp:312] Iteration 12000 (2.80251 iter/s, 35.6822s/100 iter), loss = 1.54142
I0815 10:21:40.494698  8764 solver.cpp:334]     Train net output #0: loss = 1.42048 (* 1 = 1.42048 loss)
I0815 10:21:40.494701  8764 sgd_solver.cpp:136] Iteration 12000, lr = 0.00925, m = 0.9
I0815 10:21:55.053822  8764 solver.cpp:312] Iteration 12100 (6.86872 iter/s, 14.5587s/100 iter), loss = 1.41261
I0815 10:21:55.053892  8764 solver.cpp:334]     Train net output #0: loss = 1.28694 (* 1 = 1.28694 loss)
I0815 10:21:55.053911  8764 sgd_solver.cpp:136] Iteration 12100, lr = 0.00924375, m = 0.9
I0815 10:22:09.424939  8764 solver.cpp:312] Iteration 12200 (6.95859 iter/s, 14.3707s/100 iter), loss = 1.26473
I0815 10:22:09.428192  8764 solver.cpp:334]     Train net output #0: loss = 0.998983 (* 1 = 0.998983 loss)
I0815 10:22:09.428211  8764 sgd_solver.cpp:136] Iteration 12200, lr = 0.0092375, m = 0.9
I0815 10:22:23.701004  8764 solver.cpp:312] Iteration 12300 (7.00493 iter/s, 14.2757s/100 iter), loss = 1.0436
I0815 10:22:23.701026  8764 solver.cpp:334]     Train net output #0: loss = 0.823622 (* 1 = 0.823622 loss)
I0815 10:22:23.701030  8764 sgd_solver.cpp:136] Iteration 12300, lr = 0.00923125, m = 0.9
I0815 10:22:38.187963  8764 solver.cpp:312] Iteration 12400 (6.90295 iter/s, 14.4866s/100 iter), loss = 1.31849
I0815 10:22:38.188026  8764 solver.cpp:334]     Train net output #0: loss = 1.09411 (* 1 = 1.09411 loss)
I0815 10:22:38.188045  8764 sgd_solver.cpp:136] Iteration 12400, lr = 0.009225, m = 0.9
I0815 10:22:52.550243  8764 solver.cpp:312] Iteration 12500 (6.96287 iter/s, 14.3619s/100 iter), loss = 1.49624
I0815 10:22:52.550307  8764 solver.cpp:334]     Train net output #0: loss = 1.57458 (* 1 = 1.57458 loss)
I0815 10:22:52.550314  8764 sgd_solver.cpp:136] Iteration 12500, lr = 0.00921875, m = 0.9
I0815 10:23:07.005527  8764 solver.cpp:312] Iteration 12600 (6.91808 iter/s, 14.4549s/100 iter), loss = 1.82941
I0815 10:23:07.005554  8764 solver.cpp:334]     Train net output #0: loss = 1.6125 (* 1 = 1.6125 loss)
I0815 10:23:07.005560  8764 sgd_solver.cpp:136] Iteration 12600, lr = 0.0092125, m = 0.9
I0815 10:23:21.522992  8764 solver.cpp:312] Iteration 12700 (6.88844 iter/s, 14.5171s/100 iter), loss = 1.35146
I0815 10:23:21.523025  8764 solver.cpp:334]     Train net output #0: loss = 1.38287 (* 1 = 1.38287 loss)
I0815 10:23:21.523030  8764 sgd_solver.cpp:136] Iteration 12700, lr = 0.00920625, m = 0.9
I0815 10:23:35.972972  8764 solver.cpp:312] Iteration 12800 (6.92062 iter/s, 14.4496s/100 iter), loss = 1.93914
I0815 10:23:35.973076  8764 solver.cpp:334]     Train net output #0: loss = 2.23588 (* 1 = 2.23588 loss)
I0815 10:23:35.973094  8764 sgd_solver.cpp:136] Iteration 12800, lr = 0.0092, m = 0.9
I0815 10:23:50.555377  8764 solver.cpp:312] Iteration 12900 (6.85777 iter/s, 14.582s/100 iter), loss = 1.25388
I0815 10:23:50.555404  8764 solver.cpp:334]     Train net output #0: loss = 1.27162 (* 1 = 1.27162 loss)
I0815 10:23:50.555410  8764 sgd_solver.cpp:136] Iteration 12900, lr = 0.00919375, m = 0.9
I0815 10:24:05.218369  8764 solver.cpp:363] Sparsity after update:
I0815 10:24:05.228261  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:24:05.228286  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:24:05.228301  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:24:05.228309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:24:05.228317  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:24:05.228325  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:24:05.228332  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:24:05.228340  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:24:05.228354  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:24:05.228363  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:24:05.228370  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:24:05.228379  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:24:05.228386  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:24:05.371446  8764 solver.cpp:312] Iteration 13000 (6.74961 iter/s, 14.8157s/100 iter), loss = 1.4975
I0815 10:24:05.371474  8764 solver.cpp:334]     Train net output #0: loss = 1.5766 (* 1 = 1.5766 loss)
I0815 10:24:05.371479  8764 sgd_solver.cpp:136] Iteration 13000, lr = 0.0091875, m = 0.9
I0815 10:24:19.705018  8764 solver.cpp:312] Iteration 13100 (6.97682 iter/s, 14.3332s/100 iter), loss = 1.5332
I0815 10:24:19.705121  8764 solver.cpp:334]     Train net output #0: loss = 1.43757 (* 1 = 1.43757 loss)
I0815 10:24:19.705139  8764 sgd_solver.cpp:136] Iteration 13100, lr = 0.00918125, m = 0.9
I0815 10:24:34.122544  8764 solver.cpp:312] Iteration 13200 (6.93619 iter/s, 14.4171s/100 iter), loss = 1.72298
I0815 10:24:34.122570  8764 solver.cpp:334]     Train net output #0: loss = 1.63294 (* 1 = 1.63294 loss)
I0815 10:24:34.122575  8764 sgd_solver.cpp:136] Iteration 13200, lr = 0.009175, m = 0.9
I0815 10:24:48.621631  8764 solver.cpp:312] Iteration 13300 (6.89718 iter/s, 14.4987s/100 iter), loss = 1.14293
I0815 10:24:48.621665  8764 solver.cpp:334]     Train net output #0: loss = 1.0525 (* 1 = 1.0525 loss)
I0815 10:24:48.621671  8764 sgd_solver.cpp:136] Iteration 13300, lr = 0.00916875, m = 0.9
I0815 10:25:03.396958  8764 solver.cpp:312] Iteration 13400 (6.76822 iter/s, 14.7749s/100 iter), loss = 1.57674
I0815 10:25:03.398630  8764 solver.cpp:334]     Train net output #0: loss = 1.63493 (* 1 = 1.63493 loss)
I0815 10:25:03.398648  8764 sgd_solver.cpp:136] Iteration 13400, lr = 0.0091625, m = 0.9
I0815 10:25:17.840298  8764 solver.cpp:312] Iteration 13500 (6.9238 iter/s, 14.4429s/100 iter), loss = 1.26778
I0815 10:25:17.840324  8764 solver.cpp:334]     Train net output #0: loss = 1.28192 (* 1 = 1.28192 loss)
I0815 10:25:17.840328  8764 sgd_solver.cpp:136] Iteration 13500, lr = 0.00915625, m = 0.9
I0815 10:25:32.309550  8764 solver.cpp:312] Iteration 13600 (6.9114 iter/s, 14.4689s/100 iter), loss = 1.26609
I0815 10:25:32.309577  8764 solver.cpp:334]     Train net output #0: loss = 1.33708 (* 1 = 1.33708 loss)
I0815 10:25:32.309583  8764 sgd_solver.cpp:136] Iteration 13600, lr = 0.00915, m = 0.9
I0815 10:25:47.076370  8764 solver.cpp:312] Iteration 13700 (6.77213 iter/s, 14.7664s/100 iter), loss = 1.25149
I0815 10:25:47.076469  8764 solver.cpp:334]     Train net output #0: loss = 1.21438 (* 1 = 1.21438 loss)
I0815 10:25:47.076488  8764 sgd_solver.cpp:136] Iteration 13700, lr = 0.00914375, m = 0.9
I0815 10:26:01.888300  8764 solver.cpp:312] Iteration 13800 (6.7515 iter/s, 14.8115s/100 iter), loss = 1.25259
I0815 10:26:01.888355  8764 solver.cpp:334]     Train net output #0: loss = 0.99163 (* 1 = 0.99163 loss)
I0815 10:26:01.888368  8764 sgd_solver.cpp:136] Iteration 13800, lr = 0.0091375, m = 0.9
I0815 10:26:16.768690  8764 solver.cpp:312] Iteration 13900 (6.72044 iter/s, 14.88s/100 iter), loss = 1.43786
I0815 10:26:16.768717  8764 solver.cpp:334]     Train net output #0: loss = 1.8209 (* 1 = 1.8209 loss)
I0815 10:26:16.768723  8764 sgd_solver.cpp:136] Iteration 13900, lr = 0.00913125, m = 0.9
I0815 10:26:31.630465  8764 solver.cpp:363] Sparsity after update:
I0815 10:26:31.636924  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:26:31.636940  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:26:31.636948  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:26:31.636951  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:26:31.636955  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:26:31.636960  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:26:31.636962  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:26:31.636965  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:26:31.636968  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:26:31.636971  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:26:31.636976  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:26:31.636978  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:26:31.636981  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:26:31.636992  8764 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 10:26:52.686982  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.537294
I0815 10:26:52.687005  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.778703
I0815 10:26:52.687011  8764 solver.cpp:594]     Test net output #2: loss = 2.07471 (* 1 = 2.07471 loss)
I0815 10:26:52.687036  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0495s
I0815 10:26:52.847307  8764 solver.cpp:312] Iteration 14000 (2.7718 iter/s, 36.0776s/100 iter), loss = 1.46188
I0815 10:26:52.847334  8764 solver.cpp:334]     Train net output #0: loss = 1.80588 (* 1 = 1.80588 loss)
I0815 10:26:52.847340  8764 sgd_solver.cpp:136] Iteration 14000, lr = 0.009125, m = 0.9
I0815 10:27:07.156455  8764 solver.cpp:312] Iteration 14100 (6.98873 iter/s, 14.3088s/100 iter), loss = 1.65773
I0815 10:27:07.156558  8764 solver.cpp:334]     Train net output #0: loss = 1.90813 (* 1 = 1.90813 loss)
I0815 10:27:07.156575  8764 sgd_solver.cpp:136] Iteration 14100, lr = 0.00911875, m = 0.9
I0815 10:27:21.398648  8764 solver.cpp:312] Iteration 14200 (7.02159 iter/s, 14.2418s/100 iter), loss = 1.56751
I0815 10:27:21.398718  8764 solver.cpp:334]     Train net output #0: loss = 1.78187 (* 1 = 1.78187 loss)
I0815 10:27:21.398738  8764 sgd_solver.cpp:136] Iteration 14200, lr = 0.0091125, m = 0.9
I0815 10:27:35.849916  8764 solver.cpp:312] Iteration 14300 (6.92 iter/s, 14.4509s/100 iter), loss = 1.32475
I0815 10:27:35.849982  8764 solver.cpp:334]     Train net output #0: loss = 1.28208 (* 1 = 1.28208 loss)
I0815 10:27:35.849999  8764 sgd_solver.cpp:136] Iteration 14300, lr = 0.00910625, m = 0.9
I0815 10:27:50.475085  8764 solver.cpp:312] Iteration 14400 (6.83772 iter/s, 14.6248s/100 iter), loss = 1.70034
I0815 10:27:50.475147  8764 solver.cpp:334]     Train net output #0: loss = 1.84253 (* 1 = 1.84253 loss)
I0815 10:27:50.475153  8764 sgd_solver.cpp:136] Iteration 14400, lr = 0.0091, m = 0.9
I0815 10:28:04.959123  8764 solver.cpp:312] Iteration 14500 (6.90435 iter/s, 14.4836s/100 iter), loss = 1.52315
I0815 10:28:04.959146  8764 solver.cpp:334]     Train net output #0: loss = 1.57122 (* 1 = 1.57122 loss)
I0815 10:28:04.959151  8764 sgd_solver.cpp:136] Iteration 14500, lr = 0.00909375, m = 0.9
I0815 10:28:19.381454  8764 solver.cpp:312] Iteration 14600 (6.93389 iter/s, 14.4219s/100 iter), loss = 1.55892
I0815 10:28:19.381479  8764 solver.cpp:334]     Train net output #0: loss = 1.20012 (* 1 = 1.20012 loss)
I0815 10:28:19.381484  8764 sgd_solver.cpp:136] Iteration 14600, lr = 0.0090875, m = 0.9
I0815 10:28:33.770452  8764 solver.cpp:312] Iteration 14700 (6.94995 iter/s, 14.3886s/100 iter), loss = 1.74884
I0815 10:28:33.772538  8764 solver.cpp:334]     Train net output #0: loss = 1.69962 (* 1 = 1.69962 loss)
I0815 10:28:33.772553  8764 sgd_solver.cpp:136] Iteration 14700, lr = 0.00908125, m = 0.9
I0815 10:28:48.161681  8764 solver.cpp:312] Iteration 14800 (6.94887 iter/s, 14.3908s/100 iter), loss = 1.31492
I0815 10:28:48.161708  8764 solver.cpp:334]     Train net output #0: loss = 1.31315 (* 1 = 1.31315 loss)
I0815 10:28:48.161715  8764 sgd_solver.cpp:136] Iteration 14800, lr = 0.009075, m = 0.9
I0815 10:29:02.629848  8764 solver.cpp:312] Iteration 14900 (6.91192 iter/s, 14.4678s/100 iter), loss = 1.46861
I0815 10:29:02.629910  8764 solver.cpp:334]     Train net output #0: loss = 1.56518 (* 1 = 1.56518 loss)
I0815 10:29:02.629930  8764 sgd_solver.cpp:136] Iteration 14900, lr = 0.00906875, m = 0.9
I0815 10:29:16.729456  8764 solver.cpp:363] Sparsity after update:
I0815 10:29:16.744709  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:29:16.744747  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:29:16.744765  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:29:16.744784  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:29:16.744796  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:29:16.744807  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:29:16.744819  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:29:16.744830  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:29:16.744841  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:29:16.744853  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:29:16.744864  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:29:16.744875  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:29:16.744886  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:29:16.884160  8764 solver.cpp:312] Iteration 15000 (7.01562 iter/s, 14.2539s/100 iter), loss = 1.55413
I0815 10:29:16.884182  8764 solver.cpp:334]     Train net output #0: loss = 1.5552 (* 1 = 1.5552 loss)
I0815 10:29:16.884187  8764 sgd_solver.cpp:136] Iteration 15000, lr = 0.0090625, m = 0.9
I0815 10:29:31.199621  8764 solver.cpp:312] Iteration 15100 (6.98565 iter/s, 14.3151s/100 iter), loss = 1.24791
I0815 10:29:31.199652  8764 solver.cpp:334]     Train net output #0: loss = 1.1021 (* 1 = 1.1021 loss)
I0815 10:29:31.199657  8764 sgd_solver.cpp:136] Iteration 15100, lr = 0.00905625, m = 0.9
I0815 10:29:45.591223  8764 solver.cpp:312] Iteration 15200 (6.94869 iter/s, 14.3912s/100 iter), loss = 1.80903
I0815 10:29:45.591291  8764 solver.cpp:334]     Train net output #0: loss = 1.83317 (* 1 = 1.83317 loss)
I0815 10:29:45.591308  8764 sgd_solver.cpp:136] Iteration 15200, lr = 0.00905, m = 0.9
I0815 10:29:59.970324  8764 solver.cpp:312] Iteration 15300 (6.95473 iter/s, 14.3787s/100 iter), loss = 1.58119
I0815 10:29:59.970432  8764 solver.cpp:334]     Train net output #0: loss = 1.80936 (* 1 = 1.80936 loss)
I0815 10:29:59.970451  8764 sgd_solver.cpp:136] Iteration 15300, lr = 0.00904375, m = 0.9
I0815 10:30:14.311681  8764 solver.cpp:312] Iteration 15400 (6.97304 iter/s, 14.341s/100 iter), loss = 1.62458
I0815 10:30:14.311749  8764 solver.cpp:334]     Train net output #0: loss = 1.72081 (* 1 = 1.72081 loss)
I0815 10:30:14.311769  8764 sgd_solver.cpp:136] Iteration 15400, lr = 0.0090375, m = 0.9
I0815 10:30:28.667563  8764 solver.cpp:312] Iteration 15500 (6.96598 iter/s, 14.3555s/100 iter), loss = 1.69418
I0815 10:30:28.667589  8764 solver.cpp:334]     Train net output #0: loss = 1.72651 (* 1 = 1.72651 loss)
I0815 10:30:28.667593  8764 sgd_solver.cpp:136] Iteration 15500, lr = 0.00903125, m = 0.9
I0815 10:30:43.277721  8764 solver.cpp:312] Iteration 15600 (6.84474 iter/s, 14.6098s/100 iter), loss = 1.73383
I0815 10:30:43.277819  8764 solver.cpp:334]     Train net output #0: loss = 1.79138 (* 1 = 1.79138 loss)
I0815 10:30:43.277837  8764 sgd_solver.cpp:136] Iteration 15600, lr = 0.009025, m = 0.9
I0815 10:30:57.750053  8764 solver.cpp:312] Iteration 15700 (6.90993 iter/s, 14.4719s/100 iter), loss = 1.58815
I0815 10:30:57.750123  8764 solver.cpp:334]     Train net output #0: loss = 1.33573 (* 1 = 1.33573 loss)
I0815 10:30:57.750144  8764 sgd_solver.cpp:136] Iteration 15700, lr = 0.00901875, m = 0.9
I0815 10:31:12.043956  8764 solver.cpp:312] Iteration 15800 (6.99619 iter/s, 14.2935s/100 iter), loss = 1.35074
I0815 10:31:12.044023  8764 solver.cpp:334]     Train net output #0: loss = 1.36072 (* 1 = 1.36072 loss)
I0815 10:31:12.044042  8764 sgd_solver.cpp:136] Iteration 15800, lr = 0.0090125, m = 0.9
I0815 10:31:26.731420  8764 solver.cpp:312] Iteration 15900 (6.80872 iter/s, 14.6871s/100 iter), loss = 1.51744
I0815 10:31:26.736160  8764 solver.cpp:334]     Train net output #0: loss = 1.39367 (* 1 = 1.39367 loss)
I0815 10:31:26.736171  8764 sgd_solver.cpp:136] Iteration 15900, lr = 0.00900625, m = 0.9
I0815 10:31:40.925514  8764 solver.cpp:363] Sparsity after update:
I0815 10:31:40.931443  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:31:40.931457  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:31:40.931463  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:31:40.931464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:31:40.931466  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:31:40.931468  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:31:40.931470  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:31:40.931471  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:31:40.931473  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:31:40.931475  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:31:40.931478  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:31:40.931479  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:31:40.931483  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:31:40.931493  8764 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 10:31:57.976362  8747 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 10:32:02.031059  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548471
I0815 10:32:02.031080  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.784527
I0815 10:32:02.031086  8764 solver.cpp:594]     Test net output #2: loss = 2.03109 (* 1 = 2.03109 loss)
I0815 10:32:02.031112  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.099s
I0815 10:32:02.195773  8764 solver.cpp:312] Iteration 16000 (2.81981 iter/s, 35.4634s/100 iter), loss = 1.20465
I0815 10:32:02.195801  8764 solver.cpp:334]     Train net output #0: loss = 1.09559 (* 1 = 1.09559 loss)
I0815 10:32:02.195807  8764 sgd_solver.cpp:136] Iteration 16000, lr = 0.009, m = 0.9
I0815 10:32:16.579713  8764 solver.cpp:312] Iteration 16100 (6.95239 iter/s, 14.3835s/100 iter), loss = 1.44112
I0815 10:32:16.579777  8764 solver.cpp:334]     Train net output #0: loss = 1.24157 (* 1 = 1.24157 loss)
I0815 10:32:16.579793  8764 sgd_solver.cpp:136] Iteration 16100, lr = 0.00899375, m = 0.9
I0815 10:32:30.942487  8764 solver.cpp:312] Iteration 16200 (6.96264 iter/s, 14.3624s/100 iter), loss = 1.31002
I0815 10:32:30.942548  8764 solver.cpp:334]     Train net output #0: loss = 1.18965 (* 1 = 1.18965 loss)
I0815 10:32:30.942555  8764 sgd_solver.cpp:136] Iteration 16200, lr = 0.0089875, m = 0.9
I0815 10:32:45.259392  8764 solver.cpp:312] Iteration 16300 (6.98495 iter/s, 14.3165s/100 iter), loss = 1.19198
I0815 10:32:45.259419  8764 solver.cpp:334]     Train net output #0: loss = 1.1025 (* 1 = 1.1025 loss)
I0815 10:32:45.259424  8764 sgd_solver.cpp:136] Iteration 16300, lr = 0.00898125, m = 0.9
I0815 10:32:59.728626  8764 solver.cpp:312] Iteration 16400 (6.91141 iter/s, 14.4688s/100 iter), loss = 1.46634
I0815 10:32:59.728654  8764 solver.cpp:334]     Train net output #0: loss = 1.0785 (* 1 = 1.0785 loss)
I0815 10:32:59.728660  8764 sgd_solver.cpp:136] Iteration 16400, lr = 0.008975, m = 0.9
I0815 10:33:14.081918  8764 solver.cpp:312] Iteration 16500 (6.96724 iter/s, 14.3529s/100 iter), loss = 1.54892
I0815 10:33:14.081982  8764 solver.cpp:334]     Train net output #0: loss = 1.94599 (* 1 = 1.94599 loss)
I0815 10:33:14.081991  8764 sgd_solver.cpp:136] Iteration 16500, lr = 0.00896875, m = 0.9
I0815 10:33:28.505082  8764 solver.cpp:312] Iteration 16600 (6.93348 iter/s, 14.4228s/100 iter), loss = 1.60668
I0815 10:33:28.505103  8764 solver.cpp:334]     Train net output #0: loss = 1.76218 (* 1 = 1.76218 loss)
I0815 10:33:28.505107  8764 sgd_solver.cpp:136] Iteration 16600, lr = 0.0089625, m = 0.9
I0815 10:33:42.984658  8764 solver.cpp:312] Iteration 16700 (6.90647 iter/s, 14.4792s/100 iter), loss = 1.43564
I0815 10:33:42.984690  8764 solver.cpp:334]     Train net output #0: loss = 1.53533 (* 1 = 1.53533 loss)
I0815 10:33:42.984696  8764 sgd_solver.cpp:136] Iteration 16700, lr = 0.00895625, m = 0.9
I0815 10:33:57.689990  8764 solver.cpp:312] Iteration 16800 (6.80044 iter/s, 14.7049s/100 iter), loss = 1.10436
I0815 10:33:57.690083  8764 solver.cpp:334]     Train net output #0: loss = 1.05811 (* 1 = 1.05811 loss)
I0815 10:33:57.690104  8764 sgd_solver.cpp:136] Iteration 16800, lr = 0.00895, m = 0.9
I0815 10:34:12.127557  8764 solver.cpp:312] Iteration 16900 (6.92657 iter/s, 14.4372s/100 iter), loss = 1.47177
I0815 10:34:12.127588  8764 solver.cpp:334]     Train net output #0: loss = 1.53715 (* 1 = 1.53715 loss)
I0815 10:34:12.127593  8764 sgd_solver.cpp:136] Iteration 16900, lr = 0.00894375, m = 0.9
I0815 10:34:26.619760  8764 solver.cpp:363] Sparsity after update:
I0815 10:34:26.632753  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:34:26.632802  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:34:26.632819  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:34:26.632832  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:34:26.632843  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:34:26.632854  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:34:26.632865  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:34:26.632877  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:34:26.632889  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:34:26.632900  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:34:26.632912  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:34:26.632923  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:34:26.632935  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:34:26.775313  8764 solver.cpp:312] Iteration 17000 (6.82718 iter/s, 14.6473s/100 iter), loss = 1.6301
I0815 10:34:26.775380  8764 solver.cpp:334]     Train net output #0: loss = 1.75171 (* 1 = 1.75171 loss)
I0815 10:34:26.775401  8764 sgd_solver.cpp:136] Iteration 17000, lr = 0.0089375, m = 0.9
I0815 10:34:41.195123  8764 solver.cpp:312] Iteration 17100 (6.9351 iter/s, 14.4194s/100 iter), loss = 1.38845
I0815 10:34:41.196164  8764 solver.cpp:334]     Train net output #0: loss = 1.4746 (* 1 = 1.4746 loss)
I0815 10:34:41.196174  8764 sgd_solver.cpp:136] Iteration 17100, lr = 0.00893125, m = 0.9
I0815 10:34:55.639494  8764 solver.cpp:312] Iteration 17200 (6.9233 iter/s, 14.444s/100 iter), loss = 1.94359
I0815 10:34:55.639521  8764 solver.cpp:334]     Train net output #0: loss = 2.11574 (* 1 = 2.11574 loss)
I0815 10:34:55.639528  8764 sgd_solver.cpp:136] Iteration 17200, lr = 0.008925, m = 0.9
I0815 10:35:09.988303  8764 solver.cpp:312] Iteration 17300 (6.96941 iter/s, 14.3484s/100 iter), loss = 1.31806
I0815 10:35:09.988332  8764 solver.cpp:334]     Train net output #0: loss = 0.945189 (* 1 = 0.945189 loss)
I0815 10:35:09.988338  8764 sgd_solver.cpp:136] Iteration 17300, lr = 0.00891875, m = 0.9
I0815 10:35:24.355796  8764 solver.cpp:312] Iteration 17400 (6.96035 iter/s, 14.3671s/100 iter), loss = 1.4887
I0815 10:35:24.355860  8764 solver.cpp:334]     Train net output #0: loss = 1.87435 (* 1 = 1.87435 loss)
I0815 10:35:24.355867  8764 sgd_solver.cpp:136] Iteration 17400, lr = 0.0089125, m = 0.9
I0815 10:35:38.926161  8764 solver.cpp:312] Iteration 17500 (6.86344 iter/s, 14.57s/100 iter), loss = 1.51671
I0815 10:35:38.926187  8764 solver.cpp:334]     Train net output #0: loss = 1.37766 (* 1 = 1.37766 loss)
I0815 10:35:38.926193  8764 sgd_solver.cpp:136] Iteration 17500, lr = 0.00890625, m = 0.9
I0815 10:35:53.224124  8764 solver.cpp:312] Iteration 17600 (6.9942 iter/s, 14.2976s/100 iter), loss = 1.28457
I0815 10:35:53.224161  8764 solver.cpp:334]     Train net output #0: loss = 1.407 (* 1 = 1.407 loss)
I0815 10:35:53.224167  8764 sgd_solver.cpp:136] Iteration 17600, lr = 0.0089, m = 0.9
I0815 10:36:07.869613  8764 solver.cpp:312] Iteration 17700 (6.82823 iter/s, 14.6451s/100 iter), loss = 1.65691
I0815 10:36:07.869701  8764 solver.cpp:334]     Train net output #0: loss = 1.86985 (* 1 = 1.86985 loss)
I0815 10:36:07.869711  8764 sgd_solver.cpp:136] Iteration 17700, lr = 0.00889375, m = 0.9
I0815 10:36:22.580610  8764 solver.cpp:312] Iteration 17800 (6.79782 iter/s, 14.7106s/100 iter), loss = 1.6942
I0815 10:36:22.580672  8764 solver.cpp:334]     Train net output #0: loss = 1.49174 (* 1 = 1.49174 loss)
I0815 10:36:22.580689  8764 sgd_solver.cpp:136] Iteration 17800, lr = 0.0088875, m = 0.9
I0815 10:36:37.274873  8764 solver.cpp:312] Iteration 17900 (6.80557 iter/s, 14.6939s/100 iter), loss = 1.08556
I0815 10:36:37.274902  8764 solver.cpp:334]     Train net output #0: loss = 0.612818 (* 1 = 0.612818 loss)
I0815 10:36:37.274909  8764 sgd_solver.cpp:136] Iteration 17900, lr = 0.00888125, m = 0.9
I0815 10:36:51.753264  8764 solver.cpp:363] Sparsity after update:
I0815 10:36:51.758139  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:36:51.758170  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:36:51.758188  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:36:51.758199  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:36:51.758210  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:36:51.758221  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:36:51.758234  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:36:51.758245  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:36:51.758256  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:36:51.758267  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:36:51.758278  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:36:51.758289  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:36:51.758301  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:36:51.758321  8764 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 10:37:06.472256  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:37:12.789474  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547353
I0815 10:37:12.789499  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.788056
I0815 10:37:12.789505  8764 solver.cpp:594]     Test net output #2: loss = 2.02225 (* 1 = 2.02225 loss)
I0815 10:37:12.789526  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0306s
I0815 10:37:12.930544  8764 solver.cpp:312] Iteration 18000 (2.80468 iter/s, 35.6547s/100 iter), loss = 1.52627
I0815 10:37:12.930567  8764 solver.cpp:334]     Train net output #0: loss = 1.38381 (* 1 = 1.38381 loss)
I0815 10:37:12.930572  8764 sgd_solver.cpp:136] Iteration 18000, lr = 0.008875, m = 0.9
I0815 10:37:27.439707  8764 solver.cpp:312] Iteration 18100 (6.89239 iter/s, 14.5088s/100 iter), loss = 1.73037
I0815 10:37:27.439795  8764 solver.cpp:334]     Train net output #0: loss = 1.9933 (* 1 = 1.9933 loss)
I0815 10:37:27.439811  8764 sgd_solver.cpp:136] Iteration 18100, lr = 0.00886875, m = 0.9
I0815 10:37:41.986382  8764 solver.cpp:312] Iteration 18200 (6.87462 iter/s, 14.5463s/100 iter), loss = 1.75411
I0815 10:37:41.986459  8764 solver.cpp:334]     Train net output #0: loss = 1.4956 (* 1 = 1.4956 loss)
I0815 10:37:41.986479  8764 sgd_solver.cpp:136] Iteration 18200, lr = 0.0088625, m = 0.9
I0815 10:37:56.513049  8764 solver.cpp:312] Iteration 18300 (6.88409 iter/s, 14.5263s/100 iter), loss = 1.52293
I0815 10:37:56.513101  8764 solver.cpp:334]     Train net output #0: loss = 1.42972 (* 1 = 1.42972 loss)
I0815 10:37:56.513113  8764 sgd_solver.cpp:136] Iteration 18300, lr = 0.00885625, m = 0.9
I0815 10:38:10.970475  8764 solver.cpp:312] Iteration 18400 (6.91706 iter/s, 14.457s/100 iter), loss = 1.17809
I0815 10:38:10.970535  8764 solver.cpp:334]     Train net output #0: loss = 1.40938 (* 1 = 1.40938 loss)
I0815 10:38:10.970541  8764 sgd_solver.cpp:136] Iteration 18400, lr = 0.00885, m = 0.9
I0815 10:38:25.490820  8764 solver.cpp:312] Iteration 18500 (6.88708 iter/s, 14.5199s/100 iter), loss = 1.31463
I0815 10:38:25.490845  8764 solver.cpp:334]     Train net output #0: loss = 1.6196 (* 1 = 1.6196 loss)
I0815 10:38:25.490851  8764 sgd_solver.cpp:136] Iteration 18500, lr = 0.00884375, m = 0.9
I0815 10:38:39.900907  8764 solver.cpp:312] Iteration 18600 (6.93978 iter/s, 14.4097s/100 iter), loss = 1.52249
I0815 10:38:39.900935  8764 solver.cpp:334]     Train net output #0: loss = 1.31894 (* 1 = 1.31894 loss)
I0815 10:38:39.900941  8764 sgd_solver.cpp:136] Iteration 18600, lr = 0.0088375, m = 0.9
I0815 10:38:54.243531  8764 solver.cpp:312] Iteration 18700 (6.97242 iter/s, 14.3422s/100 iter), loss = 1.4283
I0815 10:38:54.243628  8764 solver.cpp:334]     Train net output #0: loss = 1.59023 (* 1 = 1.59023 loss)
I0815 10:38:54.243645  8764 sgd_solver.cpp:136] Iteration 18700, lr = 0.00883125, m = 0.9
I0815 10:39:08.565724  8764 solver.cpp:312] Iteration 18800 (6.98236 iter/s, 14.3218s/100 iter), loss = 1.49904
I0815 10:39:08.565747  8764 solver.cpp:334]     Train net output #0: loss = 1.77952 (* 1 = 1.77952 loss)
I0815 10:39:08.565752  8764 sgd_solver.cpp:136] Iteration 18800, lr = 0.008825, m = 0.9
I0815 10:39:22.891083  8764 solver.cpp:312] Iteration 18900 (6.98082 iter/s, 14.325s/100 iter), loss = 1.89046
I0815 10:39:22.891134  8764 solver.cpp:334]     Train net output #0: loss = 1.83585 (* 1 = 1.83585 loss)
I0815 10:39:22.891146  8764 sgd_solver.cpp:136] Iteration 18900, lr = 0.00881875, m = 0.9
I0815 10:39:37.478921  8764 solver.cpp:363] Sparsity after update:
I0815 10:39:37.489289  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:39:37.489300  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:39:37.489308  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:39:37.489312  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:39:37.489317  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:39:37.489320  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:39:37.489325  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:39:37.489327  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:39:37.489329  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:39:37.489332  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:39:37.489334  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:39:37.489337  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:39:37.489341  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:39:37.639734  8764 solver.cpp:312] Iteration 19000 (6.78047 iter/s, 14.7482s/100 iter), loss = 1.88524
I0815 10:39:37.639758  8764 solver.cpp:334]     Train net output #0: loss = 1.87795 (* 1 = 1.87795 loss)
I0815 10:39:37.639763  8764 sgd_solver.cpp:136] Iteration 19000, lr = 0.0088125, m = 0.9
I0815 10:39:52.123793  8764 solver.cpp:312] Iteration 19100 (6.90433 iter/s, 14.4837s/100 iter), loss = 1.21713
I0815 10:39:52.123858  8764 solver.cpp:334]     Train net output #0: loss = 0.896025 (* 1 = 0.896025 loss)
I0815 10:39:52.123875  8764 sgd_solver.cpp:136] Iteration 19100, lr = 0.00880625, m = 0.9
I0815 10:40:06.750233  8764 solver.cpp:312] Iteration 19200 (6.83713 iter/s, 14.626s/100 iter), loss = 1.45944
I0815 10:40:06.750258  8764 solver.cpp:334]     Train net output #0: loss = 1.8879 (* 1 = 1.8879 loss)
I0815 10:40:06.750262  8764 sgd_solver.cpp:136] Iteration 19200, lr = 0.0088, m = 0.9
I0815 10:40:21.113793  8764 solver.cpp:312] Iteration 19300 (6.96226 iter/s, 14.3632s/100 iter), loss = 1.70222
I0815 10:40:21.113893  8764 solver.cpp:334]     Train net output #0: loss = 1.78798 (* 1 = 1.78798 loss)
I0815 10:40:21.113917  8764 sgd_solver.cpp:136] Iteration 19300, lr = 0.00879375, m = 0.9
I0815 10:40:35.346855  8764 solver.cpp:312] Iteration 19400 (7.02609 iter/s, 14.2327s/100 iter), loss = 1.61483
I0815 10:40:35.346927  8764 solver.cpp:334]     Train net output #0: loss = 1.53809 (* 1 = 1.53809 loss)
I0815 10:40:35.346947  8764 sgd_solver.cpp:136] Iteration 19400, lr = 0.0087875, m = 0.9
I0815 10:40:50.025851  8764 solver.cpp:312] Iteration 19500 (6.81264 iter/s, 14.6786s/100 iter), loss = 1.68126
I0815 10:40:50.025872  8764 solver.cpp:334]     Train net output #0: loss = 1.62817 (* 1 = 1.62817 loss)
I0815 10:40:50.025876  8764 sgd_solver.cpp:136] Iteration 19500, lr = 0.00878125, m = 0.9
I0815 10:41:04.491503  8764 solver.cpp:312] Iteration 19600 (6.91312 iter/s, 14.4652s/100 iter), loss = 1.18645
I0815 10:41:04.491574  8764 solver.cpp:334]     Train net output #0: loss = 1.15002 (* 1 = 1.15002 loss)
I0815 10:41:04.491580  8764 sgd_solver.cpp:136] Iteration 19600, lr = 0.008775, m = 0.9
I0815 10:41:19.184689  8764 solver.cpp:312] Iteration 19700 (6.80606 iter/s, 14.6928s/100 iter), loss = 1.40411
I0815 10:41:19.184713  8764 solver.cpp:334]     Train net output #0: loss = 1.49794 (* 1 = 1.49794 loss)
I0815 10:41:19.184718  8764 sgd_solver.cpp:136] Iteration 19700, lr = 0.00876875, m = 0.9
I0815 10:41:33.767304  8764 solver.cpp:312] Iteration 19800 (6.85767 iter/s, 14.5822s/100 iter), loss = 1.9287
I0815 10:41:33.767330  8764 solver.cpp:334]     Train net output #0: loss = 1.53796 (* 1 = 1.53796 loss)
I0815 10:41:33.767338  8764 sgd_solver.cpp:136] Iteration 19800, lr = 0.0087625, m = 0.9
I0815 10:41:48.149811  8764 solver.cpp:312] Iteration 19900 (6.95308 iter/s, 14.3821s/100 iter), loss = 1.53357
I0815 10:41:48.149880  8764 solver.cpp:334]     Train net output #0: loss = 1.29975 (* 1 = 1.29975 loss)
I0815 10:41:48.149888  8764 sgd_solver.cpp:136] Iteration 19900, lr = 0.00875625, m = 0.9
I0815 10:42:02.653903  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_20000.caffemodel
I0815 10:42:02.666936  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_20000.solverstate
I0815 10:42:02.673233  8764 solver.cpp:363] Sparsity after update:
I0815 10:42:02.676901  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:42:02.676919  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:42:02.676928  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:42:02.676931  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:42:02.676935  8764 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 10:42:02.676939  8764 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 10:42:02.676949  8764 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 10:42:02.676954  8764 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 10:42:02.676959  8764 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 10:42:02.676965  8764 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 10:42:02.676970  8764 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 10:42:02.676975  8764 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 10:42:02.676978  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.86678e+06) 0
I0815 10:42:02.676992  8764 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 10:42:23.819118  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548824
I0815 10:42:23.819169  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.785468
I0815 10:42:23.819175  8764 solver.cpp:594]     Test net output #2: loss = 2.0085 (* 1 = 2.0085 loss)
I0815 10:42:23.819193  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1416s
I0815 10:42:23.971807  8794 solver.cpp:409] Finding and applying sparsity: 0.01
I0815 10:42:42.599304  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:42:42.601398  8764 solver.cpp:312] Iteration 20000 (1.83654 iter/s, 54.4501s/100 iter), loss = 1.59951
I0815 10:42:42.601418  8764 solver.cpp:334]     Train net output #0: loss = 1.54208 (* 1 = 1.54208 loss)
I0815 10:42:42.601430  8764 sgd_solver.cpp:136] Iteration 20000, lr = 0.00875, m = 0.9
I0815 10:42:57.491704  8764 solver.cpp:312] Iteration 20100 (6.71597 iter/s, 14.8899s/100 iter), loss = 1.5301
I0815 10:42:57.491760  8764 solver.cpp:334]     Train net output #0: loss = 1.81347 (* 1 = 1.81347 loss)
I0815 10:42:57.491766  8764 sgd_solver.cpp:136] Iteration 20100, lr = 0.00874375, m = 0.9
I0815 10:43:12.357650  8764 solver.cpp:312] Iteration 20200 (6.72697 iter/s, 14.8655s/100 iter), loss = 1.74039
I0815 10:43:12.357681  8764 solver.cpp:334]     Train net output #0: loss = 1.58709 (* 1 = 1.58709 loss)
I0815 10:43:12.357686  8764 sgd_solver.cpp:136] Iteration 20200, lr = 0.0087375, m = 0.9
I0815 10:43:27.331311  8764 solver.cpp:312] Iteration 20300 (6.67858 iter/s, 14.9732s/100 iter), loss = 1.37512
I0815 10:43:27.331456  8764 solver.cpp:334]     Train net output #0: loss = 1.60547 (* 1 = 1.60547 loss)
I0815 10:43:27.331478  8764 sgd_solver.cpp:136] Iteration 20300, lr = 0.00873125, m = 0.9
I0815 10:43:42.289146  8764 solver.cpp:312] Iteration 20400 (6.68565 iter/s, 14.9574s/100 iter), loss = 1.40209
I0815 10:43:42.289197  8764 solver.cpp:334]     Train net output #0: loss = 1.40282 (* 1 = 1.40282 loss)
I0815 10:43:42.289204  8764 sgd_solver.cpp:136] Iteration 20400, lr = 0.008725, m = 0.9
I0815 10:43:57.213552  8764 solver.cpp:312] Iteration 20500 (6.70062 iter/s, 14.924s/100 iter), loss = 1.45482
I0815 10:43:57.213578  8764 solver.cpp:334]     Train net output #0: loss = 1.2293 (* 1 = 1.2293 loss)
I0815 10:43:57.213583  8764 sgd_solver.cpp:136] Iteration 20500, lr = 0.00871875, m = 0.9
I0815 10:44:11.939987  8764 solver.cpp:312] Iteration 20600 (6.7907 iter/s, 14.726s/100 iter), loss = 1.39389
I0815 10:44:11.940014  8764 solver.cpp:334]     Train net output #0: loss = 0.971006 (* 1 = 0.971006 loss)
I0815 10:44:11.940018  8764 sgd_solver.cpp:136] Iteration 20600, lr = 0.0087125, m = 0.9
I0815 10:44:26.356204  8764 solver.cpp:312] Iteration 20700 (6.93683 iter/s, 14.4158s/100 iter), loss = 1.77832
I0815 10:44:26.356287  8764 solver.cpp:334]     Train net output #0: loss = 1.51747 (* 1 = 1.51747 loss)
I0815 10:44:26.356293  8764 sgd_solver.cpp:136] Iteration 20700, lr = 0.00870625, m = 0.9
I0815 10:44:41.048907  8764 solver.cpp:312] Iteration 20800 (6.80629 iter/s, 14.6923s/100 iter), loss = 1.31137
I0815 10:44:41.048931  8764 solver.cpp:334]     Train net output #0: loss = 1.5965 (* 1 = 1.5965 loss)
I0815 10:44:41.048935  8764 sgd_solver.cpp:136] Iteration 20800, lr = 0.0087, m = 0.9
I0815 10:44:55.628990  8764 solver.cpp:312] Iteration 20900 (6.85886 iter/s, 14.5797s/100 iter), loss = 1.28849
I0815 10:44:55.629019  8764 solver.cpp:334]     Train net output #0: loss = 1.28038 (* 1 = 1.28038 loss)
I0815 10:44:55.629024  8764 sgd_solver.cpp:136] Iteration 20900, lr = 0.00869375, m = 0.9
I0815 10:45:10.153106  8764 solver.cpp:363] Sparsity after update:
I0815 10:45:10.168390  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:45:10.168432  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:45:10.168452  8764 net.cpp:2192] conv1b_param_0(0) 
I0815 10:45:10.168463  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:45:10.168474  8764 net.cpp:2192] res2a_branch2a_param_0(0.00694) 
I0815 10:45:10.168488  8764 net.cpp:2192] res2a_branch2b_param_0(0.00694) 
I0815 10:45:10.168500  8764 net.cpp:2192] res3a_branch2a_param_0(0.00868) 
I0815 10:45:10.168512  8764 net.cpp:2192] res3a_branch2b_param_0(0.00694) 
I0815 10:45:10.168524  8764 net.cpp:2192] res4a_branch2a_param_0(0.00954) 
I0815 10:45:10.168536  8764 net.cpp:2192] res4a_branch2b_param_0(0.00867) 
I0815 10:45:10.168548  8764 net.cpp:2192] res5a_branch2a_param_0(0.00997) 
I0815 10:45:10.168560  8764 net.cpp:2192] res5a_branch2b_param_0(0.00952) 
I0815 10:45:10.168571  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (22558/2.86678e+06) 0.00787
I0815 10:45:10.306285  8794 solver.cpp:409] Finding and applying sparsity: 0.02
I0815 10:45:29.175987  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:45:29.177980  8764 solver.cpp:312] Iteration 21000 (2.9808 iter/s, 33.5481s/100 iter), loss = 1.59313
I0815 10:45:29.177999  8764 solver.cpp:334]     Train net output #0: loss = 1.34017 (* 1 = 1.34017 loss)
I0815 10:45:29.178006  8764 sgd_solver.cpp:136] Iteration 21000, lr = 0.0086875, m = 0.9
I0815 10:45:44.513276  8764 solver.cpp:312] Iteration 21100 (6.52109 iter/s, 15.3349s/100 iter), loss = 1.80665
I0815 10:45:44.513381  8764 solver.cpp:334]     Train net output #0: loss = 1.3402 (* 1 = 1.3402 loss)
I0815 10:45:44.513403  8764 sgd_solver.cpp:136] Iteration 21100, lr = 0.00868125, m = 0.9
I0815 10:45:59.201431  8764 solver.cpp:312] Iteration 21200 (6.8084 iter/s, 14.6877s/100 iter), loss = 1.67857
I0815 10:45:59.201455  8764 solver.cpp:334]     Train net output #0: loss = 2.1459 (* 1 = 2.1459 loss)
I0815 10:45:59.201460  8764 sgd_solver.cpp:136] Iteration 21200, lr = 0.008675, m = 0.9
I0815 10:46:13.636041  8764 solver.cpp:312] Iteration 21300 (6.92799 iter/s, 14.4342s/100 iter), loss = 1.47882
I0815 10:46:13.636106  8764 solver.cpp:334]     Train net output #0: loss = 1.98458 (* 1 = 1.98458 loss)
I0815 10:46:13.636123  8764 sgd_solver.cpp:136] Iteration 21300, lr = 0.00866875, m = 0.9
I0815 10:46:28.163369  8764 solver.cpp:312] Iteration 21400 (6.88377 iter/s, 14.5269s/100 iter), loss = 1.43872
I0815 10:46:28.163446  8764 solver.cpp:334]     Train net output #0: loss = 1.23638 (* 1 = 1.23638 loss)
I0815 10:46:28.163455  8764 sgd_solver.cpp:136] Iteration 21400, lr = 0.0086625, m = 0.9
I0815 10:46:42.565021  8764 solver.cpp:312] Iteration 21500 (6.94384 iter/s, 14.4012s/100 iter), loss = 1.53981
I0815 10:46:42.565048  8764 solver.cpp:334]     Train net output #0: loss = 1.59704 (* 1 = 1.59704 loss)
I0815 10:46:42.565053  8764 sgd_solver.cpp:136] Iteration 21500, lr = 0.00865625, m = 0.9
I0815 10:46:57.042495  8764 solver.cpp:312] Iteration 21600 (6.90747 iter/s, 14.4771s/100 iter), loss = 1.87825
I0815 10:46:57.042695  8764 solver.cpp:334]     Train net output #0: loss = 1.69784 (* 1 = 1.69784 loss)
I0815 10:46:57.042809  8764 sgd_solver.cpp:136] Iteration 21600, lr = 0.00865, m = 0.9
I0815 10:47:11.419450  8764 solver.cpp:312] Iteration 21700 (6.95577 iter/s, 14.3766s/100 iter), loss = 1.68366
I0815 10:47:11.419523  8764 solver.cpp:334]     Train net output #0: loss = 1.72389 (* 1 = 1.72389 loss)
I0815 10:47:11.419535  8764 sgd_solver.cpp:136] Iteration 21700, lr = 0.00864375, m = 0.9
I0815 10:47:25.902451  8764 solver.cpp:312] Iteration 21800 (6.90484 iter/s, 14.4826s/100 iter), loss = 1.25637
I0815 10:47:25.902482  8764 solver.cpp:334]     Train net output #0: loss = 1.20337 (* 1 = 1.20337 loss)
I0815 10:47:25.902490  8764 sgd_solver.cpp:136] Iteration 21800, lr = 0.0086375, m = 0.9
I0815 10:47:40.603759  8764 solver.cpp:312] Iteration 21900 (6.80231 iter/s, 14.7009s/100 iter), loss = 1.32153
I0815 10:47:40.603786  8764 solver.cpp:334]     Train net output #0: loss = 1.65279 (* 1 = 1.65279 loss)
I0815 10:47:40.603791  8764 sgd_solver.cpp:136] Iteration 21900, lr = 0.00863125, m = 0.9
I0815 10:47:54.824087  8764 solver.cpp:363] Sparsity after update:
I0815 10:47:54.828191  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:47:54.828200  8764 net.cpp:2192] conv1a_param_0(0) 
I0815 10:47:54.828208  8764 net.cpp:2192] conv1b_param_0(0.00694) 
I0815 10:47:54.828212  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:47:54.828223  8764 net.cpp:2192] res2a_branch2a_param_0(0.0174) 
I0815 10:47:54.828233  8764 net.cpp:2192] res2a_branch2b_param_0(0.0139) 
I0815 10:47:54.828241  8764 net.cpp:2192] res3a_branch2a_param_0(0.0191) 
I0815 10:47:54.828250  8764 net.cpp:2192] res3a_branch2b_param_0(0.0174) 
I0815 10:47:54.828259  8764 net.cpp:2192] res4a_branch2a_param_0(0.02) 
I0815 10:47:54.828268  8764 net.cpp:2192] res4a_branch2b_param_0(0.0191) 
I0815 10:47:54.828275  8764 net.cpp:2192] res5a_branch2a_param_0(0.02) 
I0815 10:47:54.828285  8764 net.cpp:2192] res5a_branch2b_param_0(0.0199) 
I0815 10:47:54.828294  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (46520/2.86678e+06) 0.0162
I0815 10:47:54.828311  8764 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 10:48:16.009193  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.552236
I0815 10:48:16.009217  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.79335
I0815 10:48:16.009222  8764 solver.cpp:594]     Test net output #2: loss = 1.98977 (* 1 = 1.98977 loss)
I0815 10:48:16.009241  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.1803s
I0815 10:48:16.168021  8794 solver.cpp:409] Finding and applying sparsity: 0.03
I0815 10:48:34.965389  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:48:34.967432  8764 solver.cpp:312] Iteration 22000 (1.83951 iter/s, 54.3622s/100 iter), loss = 1.91454
I0815 10:48:34.967455  8764 solver.cpp:334]     Train net output #0: loss = 1.64977 (* 1 = 1.64977 loss)
I0815 10:48:34.967465  8764 sgd_solver.cpp:136] Iteration 22000, lr = 0.008625, m = 0.9
I0815 10:48:49.813133  8764 solver.cpp:312] Iteration 22100 (6.73615 iter/s, 14.8453s/100 iter), loss = 1.60787
I0815 10:48:49.813159  8764 solver.cpp:334]     Train net output #0: loss = 1.62387 (* 1 = 1.62387 loss)
I0815 10:48:49.813164  8764 sgd_solver.cpp:136] Iteration 22100, lr = 0.00861875, m = 0.9
I0815 10:49:04.275087  8764 solver.cpp:312] Iteration 22200 (6.91489 iter/s, 14.4615s/100 iter), loss = 1.15288
I0815 10:49:04.275144  8764 solver.cpp:334]     Train net output #0: loss = 0.914332 (* 1 = 0.914332 loss)
I0815 10:49:04.275156  8764 sgd_solver.cpp:136] Iteration 22200, lr = 0.0086125, m = 0.9
I0815 10:49:18.648142  8764 solver.cpp:312] Iteration 22300 (6.95766 iter/s, 14.3726s/100 iter), loss = 1.14478
I0815 10:49:18.648213  8764 solver.cpp:334]     Train net output #0: loss = 1.12705 (* 1 = 1.12705 loss)
I0815 10:49:18.648221  8764 sgd_solver.cpp:136] Iteration 22300, lr = 0.00860625, m = 0.9
I0815 10:49:33.046385  8764 solver.cpp:312] Iteration 22400 (6.94549 iter/s, 14.3978s/100 iter), loss = 1.61693
I0815 10:49:33.046407  8764 solver.cpp:334]     Train net output #0: loss = 1.56965 (* 1 = 1.56965 loss)
I0815 10:49:33.046412  8764 sgd_solver.cpp:136] Iteration 22400, lr = 0.0086, m = 0.9
I0815 10:49:47.557471  8764 solver.cpp:312] Iteration 22500 (6.89148 iter/s, 14.5107s/100 iter), loss = 1.28368
I0815 10:49:47.557497  8764 solver.cpp:334]     Train net output #0: loss = 1.29777 (* 1 = 1.29777 loss)
I0815 10:49:47.557503  8764 sgd_solver.cpp:136] Iteration 22500, lr = 0.00859375, m = 0.9
I0815 10:50:01.948277  8764 solver.cpp:312] Iteration 22600 (6.94908 iter/s, 14.3904s/100 iter), loss = 1.45606
I0815 10:50:01.948354  8764 solver.cpp:334]     Train net output #0: loss = 0.793708 (* 1 = 0.793708 loss)
I0815 10:50:01.948361  8764 sgd_solver.cpp:136] Iteration 22600, lr = 0.0085875, m = 0.9
I0815 10:50:16.412655  8764 solver.cpp:312] Iteration 22700 (6.91373 iter/s, 14.464s/100 iter), loss = 1.37543
I0815 10:50:16.412708  8764 solver.cpp:334]     Train net output #0: loss = 1.70279 (* 1 = 1.70279 loss)
I0815 10:50:16.412720  8764 sgd_solver.cpp:136] Iteration 22700, lr = 0.00858125, m = 0.9
I0815 10:50:31.364444  8764 solver.cpp:312] Iteration 22800 (6.68835 iter/s, 14.9514s/100 iter), loss = 1.49565
I0815 10:50:31.364472  8764 solver.cpp:334]     Train net output #0: loss = 1.71238 (* 1 = 1.71238 loss)
I0815 10:50:31.364478  8764 sgd_solver.cpp:136] Iteration 22800, lr = 0.008575, m = 0.9
I0815 10:50:45.899433  8764 solver.cpp:312] Iteration 22900 (6.88014 iter/s, 14.5346s/100 iter), loss = 1.57657
I0815 10:50:45.899657  8764 solver.cpp:334]     Train net output #0: loss = 1.47793 (* 1 = 1.47793 loss)
I0815 10:50:45.899670  8764 sgd_solver.cpp:136] Iteration 22900, lr = 0.00856875, m = 0.9
I0815 10:51:00.159179  8764 solver.cpp:363] Sparsity after update:
I0815 10:51:00.170892  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:51:00.170928  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:51:00.170948  8764 net.cpp:2192] conv1b_param_0(0.0139) 
I0815 10:51:00.170960  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:51:00.170979  8764 net.cpp:2192] res2a_branch2a_param_0(0.0278) 
I0815 10:51:00.170991  8764 net.cpp:2192] res2a_branch2b_param_0(0.0278) 
I0815 10:51:00.171002  8764 net.cpp:2192] res3a_branch2a_param_0(0.0295) 
I0815 10:51:00.171015  8764 net.cpp:2192] res3a_branch2b_param_0(0.0278) 
I0815 10:51:00.171025  8764 net.cpp:2192] res4a_branch2a_param_0(0.0295) 
I0815 10:51:00.171037  8764 net.cpp:2192] res4a_branch2b_param_0(0.0295) 
I0815 10:51:00.171048  8764 net.cpp:2192] res5a_branch2a_param_0(0.0299) 
I0815 10:51:00.171061  8764 net.cpp:2192] res5a_branch2b_param_0(0.0295) 
I0815 10:51:00.171072  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (69782/2.86678e+06) 0.0243
I0815 10:51:00.305153  8794 solver.cpp:409] Finding and applying sparsity: 0.04
I0815 10:51:19.085427  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:51:19.087574  8764 solver.cpp:312] Iteration 23000 (3.01321 iter/s, 33.1872s/100 iter), loss = 1.58943
I0815 10:51:19.087594  8764 solver.cpp:334]     Train net output #0: loss = 1.69372 (* 1 = 1.69372 loss)
I0815 10:51:19.087600  8764 sgd_solver.cpp:136] Iteration 23000, lr = 0.0085625, m = 0.9
I0815 10:51:33.884841  8764 solver.cpp:312] Iteration 23100 (6.7582 iter/s, 14.7968s/100 iter), loss = 1.42651
I0815 10:51:33.884912  8764 solver.cpp:334]     Train net output #0: loss = 1.34256 (* 1 = 1.34256 loss)
I0815 10:51:33.884930  8764 sgd_solver.cpp:136] Iteration 23100, lr = 0.00855625, m = 0.9
I0815 10:51:48.259126  8764 solver.cpp:312] Iteration 23200 (6.95706 iter/s, 14.3739s/100 iter), loss = 1.42097
I0815 10:51:48.259150  8764 solver.cpp:334]     Train net output #0: loss = 1.15643 (* 1 = 1.15643 loss)
I0815 10:51:48.259153  8764 sgd_solver.cpp:136] Iteration 23200, lr = 0.00855, m = 0.9
I0815 10:52:02.593327  8764 solver.cpp:312] Iteration 23300 (6.97652 iter/s, 14.3338s/100 iter), loss = 1.47046
I0815 10:52:02.593406  8764 solver.cpp:334]     Train net output #0: loss = 1.39842 (* 1 = 1.39842 loss)
I0815 10:52:02.593412  8764 sgd_solver.cpp:136] Iteration 23300, lr = 0.00854375, m = 0.9
I0815 10:52:16.977816  8764 solver.cpp:312] Iteration 23400 (6.95213 iter/s, 14.3841s/100 iter), loss = 1.37197
I0815 10:52:16.977843  8764 solver.cpp:334]     Train net output #0: loss = 1.44074 (* 1 = 1.44074 loss)
I0815 10:52:16.977849  8764 sgd_solver.cpp:136] Iteration 23400, lr = 0.0085375, m = 0.9
I0815 10:52:31.329288  8764 solver.cpp:312] Iteration 23500 (6.96812 iter/s, 14.3511s/100 iter), loss = 1.3692
I0815 10:52:31.329316  8764 solver.cpp:334]     Train net output #0: loss = 1.40021 (* 1 = 1.40021 loss)
I0815 10:52:31.329321  8764 sgd_solver.cpp:136] Iteration 23500, lr = 0.00853125, m = 0.9
I0815 10:52:45.866128  8764 solver.cpp:312] Iteration 23600 (6.87926 iter/s, 14.5364s/100 iter), loss = 1.69229
I0815 10:52:45.866183  8764 solver.cpp:334]     Train net output #0: loss = 1.79838 (* 1 = 1.79838 loss)
I0815 10:52:45.866189  8764 sgd_solver.cpp:136] Iteration 23600, lr = 0.008525, m = 0.9
I0815 10:53:00.178336  8764 solver.cpp:312] Iteration 23700 (6.98724 iter/s, 14.3118s/100 iter), loss = 1.28587
I0815 10:53:00.178364  8764 solver.cpp:334]     Train net output #0: loss = 1.26745 (* 1 = 1.26745 loss)
I0815 10:53:00.178369  8764 sgd_solver.cpp:136] Iteration 23700, lr = 0.00851875, m = 0.9
I0815 10:53:15.022892  8764 solver.cpp:312] Iteration 23800 (6.73666 iter/s, 14.8441s/100 iter), loss = 1.5245
I0815 10:53:15.022914  8764 solver.cpp:334]     Train net output #0: loss = 1.86026 (* 1 = 1.86026 loss)
I0815 10:53:15.022919  8764 sgd_solver.cpp:136] Iteration 23800, lr = 0.0085125, m = 0.9
I0815 10:53:29.333326  8764 solver.cpp:312] Iteration 23900 (6.9881 iter/s, 14.31s/100 iter), loss = 1.37899
I0815 10:53:29.333431  8764 solver.cpp:334]     Train net output #0: loss = 1.11413 (* 1 = 1.11413 loss)
I0815 10:53:29.333451  8764 sgd_solver.cpp:136] Iteration 23900, lr = 0.00850625, m = 0.9
I0815 10:53:43.467234  8764 solver.cpp:363] Sparsity after update:
I0815 10:53:43.472162  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:53:43.472174  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:53:43.472183  8764 net.cpp:2192] conv1b_param_0(0.0139) 
I0815 10:53:43.472187  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:53:43.472190  8764 net.cpp:2192] res2a_branch2a_param_0(0.0382) 
I0815 10:53:43.472204  8764 net.cpp:2192] res2a_branch2b_param_0(0.0347) 
I0815 10:53:43.472213  8764 net.cpp:2192] res3a_branch2a_param_0(0.0399) 
I0815 10:53:43.472221  8764 net.cpp:2192] res3a_branch2b_param_0(0.0382) 
I0815 10:53:43.472229  8764 net.cpp:2192] res4a_branch2a_param_0(0.0399) 
I0815 10:53:43.472237  8764 net.cpp:2192] res4a_branch2b_param_0(0.0399) 
I0815 10:53:43.472245  8764 net.cpp:2192] res5a_branch2a_param_0(0.0399) 
I0815 10:53:43.472252  8764 net.cpp:2192] res5a_branch2b_param_0(0.0399) 
I0815 10:53:43.472260  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (93730/2.86678e+06) 0.0327
I0815 10:53:43.472276  8764 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 10:53:56.082759  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 10:54:04.336007  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.548529
I0815 10:54:04.336067  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.786939
I0815 10:54:04.336076  8764 solver.cpp:594]     Test net output #2: loss = 1.98828 (* 1 = 1.98828 loss)
I0815 10:54:04.336098  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8632s
I0815 10:54:04.485455  8794 solver.cpp:409] Finding and applying sparsity: 0.05
I0815 10:54:23.374076  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:54:23.376071  8764 solver.cpp:312] Iteration 24000 (1.85044 iter/s, 54.0413s/100 iter), loss = 1.41579
I0815 10:54:23.376091  8764 solver.cpp:334]     Train net output #0: loss = 1.24893 (* 1 = 1.24893 loss)
I0815 10:54:23.376097  8764 sgd_solver.cpp:136] Iteration 24000, lr = 0.0085, m = 0.9
I0815 10:54:38.314175  8764 solver.cpp:312] Iteration 24100 (6.69448 iter/s, 14.9377s/100 iter), loss = 1.31769
I0815 10:54:38.314244  8764 solver.cpp:334]     Train net output #0: loss = 1.08207 (* 1 = 1.08207 loss)
I0815 10:54:38.314249  8764 sgd_solver.cpp:136] Iteration 24100, lr = 0.00849375, m = 0.9
I0815 10:54:53.318390  8764 solver.cpp:312] Iteration 24200 (6.66498 iter/s, 15.0038s/100 iter), loss = 1.31259
I0815 10:54:53.318418  8764 solver.cpp:334]     Train net output #0: loss = 0.99761 (* 1 = 0.99761 loss)
I0815 10:54:53.318423  8764 sgd_solver.cpp:136] Iteration 24200, lr = 0.0084875, m = 0.9
I0815 10:55:08.461946  8764 solver.cpp:312] Iteration 24300 (6.60365 iter/s, 15.1431s/100 iter), loss = 1.09978
I0815 10:55:08.462036  8764 solver.cpp:334]     Train net output #0: loss = 1.16197 (* 1 = 1.16197 loss)
I0815 10:55:08.462054  8764 sgd_solver.cpp:136] Iteration 24300, lr = 0.00848125, m = 0.9
I0815 10:55:22.966084  8764 solver.cpp:312] Iteration 24400 (6.89478 iter/s, 14.5037s/100 iter), loss = 1.36673
I0815 10:55:22.966110  8764 solver.cpp:334]     Train net output #0: loss = 1.41052 (* 1 = 1.41052 loss)
I0815 10:55:22.966115  8764 sgd_solver.cpp:136] Iteration 24400, lr = 0.008475, m = 0.9
I0815 10:55:37.594830  8764 solver.cpp:312] Iteration 24500 (6.83605 iter/s, 14.6283s/100 iter), loss = 1.45214
I0815 10:55:37.594882  8764 solver.cpp:334]     Train net output #0: loss = 1.78618 (* 1 = 1.78618 loss)
I0815 10:55:37.594895  8764 sgd_solver.cpp:136] Iteration 24500, lr = 0.00846875, m = 0.9
I0815 10:55:52.225976  8764 solver.cpp:312] Iteration 24600 (6.83493 iter/s, 14.6307s/100 iter), loss = 1.60348
I0815 10:55:52.226032  8764 solver.cpp:334]     Train net output #0: loss = 1.33022 (* 1 = 1.33022 loss)
I0815 10:55:52.226074  8764 sgd_solver.cpp:136] Iteration 24600, lr = 0.0084625, m = 0.9
I0815 10:56:06.634874  8764 solver.cpp:312] Iteration 24700 (6.94035 iter/s, 14.4085s/100 iter), loss = 1.36035
I0815 10:56:06.634902  8764 solver.cpp:334]     Train net output #0: loss = 1.55896 (* 1 = 1.55896 loss)
I0815 10:56:06.634905  8764 sgd_solver.cpp:136] Iteration 24700, lr = 0.00845625, m = 0.9
I0815 10:56:21.255949  8764 solver.cpp:312] Iteration 24800 (6.83963 iter/s, 14.6207s/100 iter), loss = 1.26099
I0815 10:56:21.255976  8764 solver.cpp:334]     Train net output #0: loss = 1.11093 (* 1 = 1.11093 loss)
I0815 10:56:21.255982  8764 sgd_solver.cpp:136] Iteration 24800, lr = 0.00845, m = 0.9
I0815 10:56:35.643162  8764 solver.cpp:312] Iteration 24900 (6.95081 iter/s, 14.3868s/100 iter), loss = 1.26161
I0815 10:56:35.643267  8764 solver.cpp:334]     Train net output #0: loss = 1.19712 (* 1 = 1.19712 loss)
I0815 10:56:35.643286  8764 sgd_solver.cpp:136] Iteration 24900, lr = 0.00844375, m = 0.9
I0815 10:56:50.086872  8764 solver.cpp:363] Sparsity after update:
I0815 10:56:50.100687  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:56:50.100702  8764 net.cpp:2192] conv1a_param_0(0.0133) 
I0815 10:56:50.100710  8764 net.cpp:2192] conv1b_param_0(0.0208) 
I0815 10:56:50.100713  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:56:50.100726  8764 net.cpp:2192] res2a_branch2a_param_0(0.0486) 
I0815 10:56:50.100736  8764 net.cpp:2192] res2a_branch2b_param_0(0.0486) 
I0815 10:56:50.100744  8764 net.cpp:2192] res3a_branch2a_param_0(0.0486) 
I0815 10:56:50.100752  8764 net.cpp:2192] res3a_branch2b_param_0(0.0486) 
I0815 10:56:50.100760  8764 net.cpp:2192] res4a_branch2a_param_0(0.0495) 
I0815 10:56:50.100769  8764 net.cpp:2192] res4a_branch2b_param_0(0.0486) 
I0815 10:56:50.100776  8764 net.cpp:2192] res5a_branch2a_param_0(0.0499) 
I0815 10:56:50.100790  8764 net.cpp:2192] res5a_branch2b_param_0(0.0494) 
I0815 10:56:50.100798  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (116588/2.86678e+06) 0.0407
I0815 10:56:50.229231  8794 solver.cpp:409] Finding and applying sparsity: 0.06
I0815 10:57:09.534209  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 10:57:09.536237  8764 solver.cpp:312] Iteration 25000 (2.95054 iter/s, 33.8921s/100 iter), loss = 1.55955
I0815 10:57:09.536262  8764 solver.cpp:334]     Train net output #0: loss = 1.4423 (* 1 = 1.4423 loss)
I0815 10:57:09.536272  8764 sgd_solver.cpp:136] Iteration 25000, lr = 0.0084375, m = 0.9
I0815 10:57:24.376633  8764 solver.cpp:312] Iteration 25100 (6.73856 iter/s, 14.84s/100 iter), loss = 1.63713
I0815 10:57:24.376660  8764 solver.cpp:334]     Train net output #0: loss = 1.84988 (* 1 = 1.84988 loss)
I0815 10:57:24.376664  8764 sgd_solver.cpp:136] Iteration 25100, lr = 0.00843125, m = 0.9
I0815 10:57:38.670490  8764 solver.cpp:312] Iteration 25200 (6.99621 iter/s, 14.2935s/100 iter), loss = 1.41189
I0815 10:57:38.670642  8764 solver.cpp:334]     Train net output #0: loss = 1.50523 (* 1 = 1.50523 loss)
I0815 10:57:38.670661  8764 sgd_solver.cpp:136] Iteration 25200, lr = 0.008425, m = 0.9
I0815 10:57:53.120515  8764 solver.cpp:312] Iteration 25300 (6.9206 iter/s, 14.4496s/100 iter), loss = 1.90378
I0815 10:57:53.120581  8764 solver.cpp:334]     Train net output #0: loss = 1.95791 (* 1 = 1.95791 loss)
I0815 10:57:53.120587  8764 sgd_solver.cpp:136] Iteration 25300, lr = 0.00841875, m = 0.9
I0815 10:58:07.585923  8764 solver.cpp:312] Iteration 25400 (6.91324 iter/s, 14.465s/100 iter), loss = 1.58621
I0815 10:58:07.585988  8764 solver.cpp:334]     Train net output #0: loss = 1.55782 (* 1 = 1.55782 loss)
I0815 10:58:07.586007  8764 sgd_solver.cpp:136] Iteration 25400, lr = 0.0084125, m = 0.9
I0815 10:58:22.094128  8764 solver.cpp:312] Iteration 25500 (6.89284 iter/s, 14.5078s/100 iter), loss = 1.72325
I0815 10:58:22.094158  8764 solver.cpp:334]     Train net output #0: loss = 1.57287 (* 1 = 1.57287 loss)
I0815 10:58:22.094163  8764 sgd_solver.cpp:136] Iteration 25500, lr = 0.00840625, m = 0.9
I0815 10:58:36.995476  8764 solver.cpp:312] Iteration 25600 (6.71099 iter/s, 14.9009s/100 iter), loss = 1.63977
I0815 10:58:36.995568  8764 solver.cpp:334]     Train net output #0: loss = 1.48799 (* 1 = 1.48799 loss)
I0815 10:58:36.995586  8764 sgd_solver.cpp:136] Iteration 25600, lr = 0.0084, m = 0.9
I0815 10:58:51.653141  8764 solver.cpp:312] Iteration 25700 (6.82256 iter/s, 14.6573s/100 iter), loss = 1.50664
I0815 10:58:51.653188  8764 solver.cpp:334]     Train net output #0: loss = 1.16109 (* 1 = 1.16109 loss)
I0815 10:58:51.653201  8764 sgd_solver.cpp:136] Iteration 25700, lr = 0.00839375, m = 0.9
I0815 10:59:06.007891  8764 solver.cpp:312] Iteration 25800 (6.96653 iter/s, 14.3544s/100 iter), loss = 1.38817
I0815 10:59:06.008035  8764 solver.cpp:334]     Train net output #0: loss = 1.17633 (* 1 = 1.17633 loss)
I0815 10:59:06.008052  8764 sgd_solver.cpp:136] Iteration 25800, lr = 0.0083875, m = 0.9
I0815 10:59:20.672155  8764 solver.cpp:312] Iteration 25900 (6.81949 iter/s, 14.6639s/100 iter), loss = 1.67179
I0815 10:59:20.672209  8764 solver.cpp:334]     Train net output #0: loss = 2.23788 (* 1 = 2.23788 loss)
I0815 10:59:20.672214  8764 sgd_solver.cpp:136] Iteration 25900, lr = 0.00838125, m = 0.9
I0815 10:59:35.450229  8764 solver.cpp:363] Sparsity after update:
I0815 10:59:35.454587  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 10:59:35.460379  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 10:59:35.460500  8764 net.cpp:2192] conv1b_param_0(0.0278) 
I0815 10:59:35.460590  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 10:59:35.460677  8764 net.cpp:2192] res2a_branch2a_param_0(0.059) 
I0815 10:59:35.460770  8764 net.cpp:2192] res2a_branch2b_param_0(0.0556) 
I0815 10:59:35.461071  8764 net.cpp:2192] res3a_branch2a_param_0(0.059) 
I0815 10:59:35.461170  8764 net.cpp:2192] res3a_branch2b_param_0(0.059) 
I0815 10:59:35.461263  8764 net.cpp:2192] res4a_branch2a_param_0(0.0599) 
I0815 10:59:35.461364  8764 net.cpp:2192] res4a_branch2b_param_0(0.059) 
I0815 10:59:35.461454  8764 net.cpp:2192] res5a_branch2a_param_0(0.0599) 
I0815 10:59:35.461545  8764 net.cpp:2192] res5a_branch2b_param_0(0.0599) 
I0815 10:59:35.461634  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (140569/2.86678e+06) 0.049
I0815 10:59:35.461747  8764 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 10:59:56.352982  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547941
I0815 10:59:56.353091  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.786115
I0815 10:59:56.353102  8764 solver.cpp:594]     Test net output #2: loss = 2.0518 (* 1 = 2.0518 loss)
I0815 10:59:56.353121  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8908s
I0815 10:59:56.528174  8794 solver.cpp:409] Finding and applying sparsity: 0.07
I0815 11:00:15.796306  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:00:15.798300  8764 solver.cpp:312] Iteration 26000 (1.81407 iter/s, 55.1246s/100 iter), loss = 1.20655
I0815 11:00:15.798321  8764 solver.cpp:334]     Train net output #0: loss = 1.09252 (* 1 = 1.09252 loss)
I0815 11:00:15.798329  8764 sgd_solver.cpp:136] Iteration 26000, lr = 0.008375, m = 0.9
I0815 11:00:30.950533  8764 solver.cpp:312] Iteration 26100 (6.59987 iter/s, 15.1518s/100 iter), loss = 1.55173
I0815 11:00:30.950588  8764 solver.cpp:334]     Train net output #0: loss = 1.63927 (* 1 = 1.63927 loss)
I0815 11:00:30.950594  8764 sgd_solver.cpp:136] Iteration 26100, lr = 0.00836875, m = 0.9
I0815 11:00:45.405405  8764 solver.cpp:312] Iteration 26200 (6.91828 iter/s, 14.4545s/100 iter), loss = 1.48566
I0815 11:00:45.405627  8764 solver.cpp:334]     Train net output #0: loss = 1.84958 (* 1 = 1.84958 loss)
I0815 11:00:45.405737  8764 sgd_solver.cpp:136] Iteration 26200, lr = 0.0083625, m = 0.9
I0815 11:01:00.221204  8764 solver.cpp:312] Iteration 26300 (6.74974 iter/s, 14.8154s/100 iter), loss = 1.81562
I0815 11:01:00.221230  8764 solver.cpp:334]     Train net output #0: loss = 2.20279 (* 1 = 2.20279 loss)
I0815 11:01:00.221235  8764 sgd_solver.cpp:136] Iteration 26300, lr = 0.00835625, m = 0.9
I0815 11:01:14.674392  8764 solver.cpp:312] Iteration 26400 (6.91908 iter/s, 14.4528s/100 iter), loss = 1.33417
I0815 11:01:14.674453  8764 solver.cpp:334]     Train net output #0: loss = 1.06237 (* 1 = 1.06237 loss)
I0815 11:01:14.674458  8764 sgd_solver.cpp:136] Iteration 26400, lr = 0.00835, m = 0.9
I0815 11:01:29.357813  8764 solver.cpp:312] Iteration 26500 (6.81059 iter/s, 14.683s/100 iter), loss = 1.41424
I0815 11:01:29.357880  8764 solver.cpp:334]     Train net output #0: loss = 1.55952 (* 1 = 1.55952 loss)
I0815 11:01:29.357905  8764 sgd_solver.cpp:136] Iteration 26500, lr = 0.00834375, m = 0.9
I0815 11:01:43.896121  8764 solver.cpp:312] Iteration 26600 (6.87857 iter/s, 14.5379s/100 iter), loss = 1.37624
I0815 11:01:43.896193  8764 solver.cpp:334]     Train net output #0: loss = 1.20808 (* 1 = 1.20808 loss)
I0815 11:01:43.896209  8764 sgd_solver.cpp:136] Iteration 26600, lr = 0.0083375, m = 0.9
I0815 11:01:58.587353  8764 solver.cpp:312] Iteration 26700 (6.80697 iter/s, 14.6908s/100 iter), loss = 1.32699
I0815 11:01:58.587558  8764 solver.cpp:334]     Train net output #0: loss = 1.26393 (* 1 = 1.26393 loss)
I0815 11:01:58.587565  8764 sgd_solver.cpp:136] Iteration 26700, lr = 0.00833125, m = 0.9
I0815 11:02:13.227515  8764 solver.cpp:312] Iteration 26800 (6.83071 iter/s, 14.6398s/100 iter), loss = 1.55041
I0815 11:02:13.227572  8764 solver.cpp:334]     Train net output #0: loss = 1.44544 (* 1 = 1.44544 loss)
I0815 11:02:13.227584  8764 sgd_solver.cpp:136] Iteration 26800, lr = 0.008325, m = 0.9
I0815 11:02:27.887164  8764 solver.cpp:312] Iteration 26900 (6.82164 iter/s, 14.6592s/100 iter), loss = 1.54581
I0815 11:02:27.887189  8764 solver.cpp:334]     Train net output #0: loss = 1.52541 (* 1 = 1.52541 loss)
I0815 11:02:27.887193  8764 sgd_solver.cpp:136] Iteration 26900, lr = 0.00831875, m = 0.9
I0815 11:02:42.205195  8764 solver.cpp:363] Sparsity after update:
I0815 11:02:42.220921  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:02:42.220935  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 11:02:42.220944  8764 net.cpp:2192] conv1b_param_0(0.0347) 
I0815 11:02:42.220947  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:02:42.220953  8764 net.cpp:2192] res2a_branch2a_param_0(0.0694) 
I0815 11:02:42.220957  8764 net.cpp:2192] res2a_branch2b_param_0(0.0694) 
I0815 11:02:42.220959  8764 net.cpp:2192] res3a_branch2a_param_0(0.0694) 
I0815 11:02:42.220963  8764 net.cpp:2192] res3a_branch2b_param_0(0.0694) 
I0815 11:02:42.220965  8764 net.cpp:2192] res4a_branch2a_param_0(0.0694) 
I0815 11:02:42.220969  8764 net.cpp:2192] res4a_branch2b_param_0(0.0694) 
I0815 11:02:42.220973  8764 net.cpp:2192] res5a_branch2a_param_0(0.0699) 
I0815 11:02:42.220975  8764 net.cpp:2192] res5a_branch2b_param_0(0.0694) 
I0815 11:02:42.220978  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (163810/2.86678e+06) 0.0571
I0815 11:02:42.360739  8794 solver.cpp:409] Finding and applying sparsity: 0.08
I0815 11:03:01.619261  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:03:01.621296  8764 solver.cpp:312] Iteration 27000 (2.96444 iter/s, 33.7332s/100 iter), loss = 1.27561
I0815 11:03:01.621320  8764 solver.cpp:334]     Train net output #0: loss = 1.21934 (* 1 = 1.21934 loss)
I0815 11:03:01.621326  8764 sgd_solver.cpp:136] Iteration 27000, lr = 0.0083125, m = 0.9
I0815 11:03:16.509910  8764 solver.cpp:312] Iteration 27100 (6.71673 iter/s, 14.8882s/100 iter), loss = 1.41786
I0815 11:03:16.512210  8764 solver.cpp:334]     Train net output #0: loss = 1.6917 (* 1 = 1.6917 loss)
I0815 11:03:16.512225  8764 sgd_solver.cpp:136] Iteration 27100, lr = 0.00830625, m = 0.9
I0815 11:03:30.781731  8764 solver.cpp:312] Iteration 27200 (7.00701 iter/s, 14.2714s/100 iter), loss = 1.44426
I0815 11:03:30.781802  8764 solver.cpp:334]     Train net output #0: loss = 1.45973 (* 1 = 1.45973 loss)
I0815 11:03:30.781822  8764 sgd_solver.cpp:136] Iteration 27200, lr = 0.0083, m = 0.9
I0815 11:03:45.766923  8764 solver.cpp:312] Iteration 27300 (6.67344 iter/s, 14.9848s/100 iter), loss = 1.68348
I0815 11:03:45.766997  8764 solver.cpp:334]     Train net output #0: loss = 1.87922 (* 1 = 1.87922 loss)
I0815 11:03:45.767016  8764 sgd_solver.cpp:136] Iteration 27300, lr = 0.00829375, m = 0.9
I0815 11:03:59.921541  8764 solver.cpp:312] Iteration 27400 (7.06503 iter/s, 14.1542s/100 iter), loss = 1.0589
I0815 11:03:59.921638  8764 solver.cpp:334]     Train net output #0: loss = 0.918475 (* 1 = 0.918475 loss)
I0815 11:03:59.921654  8764 sgd_solver.cpp:136] Iteration 27400, lr = 0.0082875, m = 0.9
I0815 11:04:14.227741  8764 solver.cpp:312] Iteration 27500 (6.99017 iter/s, 14.3058s/100 iter), loss = 1.22823
I0815 11:04:14.227767  8764 solver.cpp:334]     Train net output #0: loss = 1.34658 (* 1 = 1.34658 loss)
I0815 11:04:14.227774  8764 sgd_solver.cpp:136] Iteration 27500, lr = 0.00828125, m = 0.9
I0815 11:04:28.500653  8764 solver.cpp:312] Iteration 27600 (7.00648 iter/s, 14.2725s/100 iter), loss = 1.57171
I0815 11:04:28.500679  8764 solver.cpp:334]     Train net output #0: loss = 1.78909 (* 1 = 1.78909 loss)
I0815 11:04:28.500684  8764 sgd_solver.cpp:136] Iteration 27600, lr = 0.008275, m = 0.9
I0815 11:04:43.009814  8764 solver.cpp:312] Iteration 27700 (6.89239 iter/s, 14.5088s/100 iter), loss = 1.37735
I0815 11:04:43.009912  8764 solver.cpp:334]     Train net output #0: loss = 1.13237 (* 1 = 1.13237 loss)
I0815 11:04:43.009932  8764 sgd_solver.cpp:136] Iteration 27700, lr = 0.00826875, m = 0.9
I0815 11:04:57.670859  8764 solver.cpp:312] Iteration 27800 (6.82099 iter/s, 14.6606s/100 iter), loss = 1.35543
I0815 11:04:57.670887  8764 solver.cpp:334]     Train net output #0: loss = 1.58264 (* 1 = 1.58264 loss)
I0815 11:04:57.670893  8764 sgd_solver.cpp:136] Iteration 27800, lr = 0.0082625, m = 0.9
I0815 11:05:12.274839  8764 solver.cpp:312] Iteration 27900 (6.84764 iter/s, 14.6036s/100 iter), loss = 1.18147
I0815 11:05:12.274914  8764 solver.cpp:334]     Train net output #0: loss = 0.997273 (* 1 = 0.997273 loss)
I0815 11:05:12.274942  8764 sgd_solver.cpp:136] Iteration 27900, lr = 0.00825625, m = 0.9
I0815 11:05:26.634584  8764 solver.cpp:363] Sparsity after update:
I0815 11:05:26.640009  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:05:26.640020  8764 net.cpp:2192] conv1a_param_0(0.0267) 
I0815 11:05:26.640028  8764 net.cpp:2192] conv1b_param_0(0.0347) 
I0815 11:05:26.640041  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:05:26.640053  8764 net.cpp:2192] res2a_branch2a_param_0(0.0799) 
I0815 11:05:26.640058  8764 net.cpp:2192] res2a_branch2b_param_0(0.0764) 
I0815 11:05:26.640063  8764 net.cpp:2192] res3a_branch2a_param_0(0.0799) 
I0815 11:05:26.640070  8764 net.cpp:2192] res3a_branch2b_param_0(0.0798) 
I0815 11:05:26.640075  8764 net.cpp:2192] res4a_branch2a_param_0(0.0799) 
I0815 11:05:26.640079  8764 net.cpp:2192] res4a_branch2b_param_0(0.0799) 
I0815 11:05:26.640086  8764 net.cpp:2192] res5a_branch2a_param_0(0.0799) 
I0815 11:05:26.640090  8764 net.cpp:2192] res5a_branch2b_param_0(0.0798) 
I0815 11:05:26.640094  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (187748/2.86678e+06) 0.0655
I0815 11:05:26.640106  8764 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 11:05:47.695675  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549177
I0815 11:05:47.695699  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791644
I0815 11:05:47.695704  8764 solver.cpp:594]     Test net output #2: loss = 1.99878 (* 1 = 1.99878 loss)
I0815 11:05:47.695772  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.0551s
I0815 11:05:47.867705  8794 solver.cpp:409] Finding and applying sparsity: 0.09
I0815 11:06:07.140550  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:06:07.142609  8764 solver.cpp:312] Iteration 28000 (1.82261 iter/s, 54.8663s/100 iter), loss = 1.45269
I0815 11:06:07.142632  8764 solver.cpp:334]     Train net output #0: loss = 1.3289 (* 1 = 1.3289 loss)
I0815 11:06:07.142642  8764 sgd_solver.cpp:136] Iteration 28000, lr = 0.00825, m = 0.9
I0815 11:06:21.876559  8764 solver.cpp:312] Iteration 28100 (6.78724 iter/s, 14.7335s/100 iter), loss = 1.51348
I0815 11:06:21.876622  8764 solver.cpp:334]     Train net output #0: loss = 1.25018 (* 1 = 1.25018 loss)
I0815 11:06:21.876641  8764 sgd_solver.cpp:136] Iteration 28100, lr = 0.00824375, m = 0.9
I0815 11:06:36.373471  8764 solver.cpp:312] Iteration 28200 (6.89821 iter/s, 14.4965s/100 iter), loss = 1.40581
I0815 11:06:36.373536  8764 solver.cpp:334]     Train net output #0: loss = 1.70909 (* 1 = 1.70909 loss)
I0815 11:06:36.373561  8764 sgd_solver.cpp:136] Iteration 28200, lr = 0.0082375, m = 0.9
I0815 11:06:50.755679  8764 solver.cpp:312] Iteration 28300 (6.95323 iter/s, 14.3818s/100 iter), loss = 1.24752
I0815 11:06:50.755779  8764 solver.cpp:334]     Train net output #0: loss = 0.946841 (* 1 = 0.946841 loss)
I0815 11:06:50.755796  8764 sgd_solver.cpp:136] Iteration 28300, lr = 0.00823125, m = 0.9
I0815 11:07:05.520364  8764 solver.cpp:312] Iteration 28400 (6.77311 iter/s, 14.7643s/100 iter), loss = 1.32987
I0815 11:07:05.520437  8764 solver.cpp:334]     Train net output #0: loss = 1.1723 (* 1 = 1.1723 loss)
I0815 11:07:05.520455  8764 sgd_solver.cpp:136] Iteration 28400, lr = 0.008225, m = 0.9
I0815 11:07:20.037627  8764 solver.cpp:312] Iteration 28500 (6.88854 iter/s, 14.5169s/100 iter), loss = 1.53403
I0815 11:07:20.037695  8764 solver.cpp:334]     Train net output #0: loss = 1.18039 (* 1 = 1.18039 loss)
I0815 11:07:20.037714  8764 sgd_solver.cpp:136] Iteration 28500, lr = 0.00821875, m = 0.9
I0815 11:07:34.903842  8764 solver.cpp:312] Iteration 28600 (6.72685 iter/s, 14.8658s/100 iter), loss = 1.46091
I0815 11:07:34.903904  8764 solver.cpp:334]     Train net output #0: loss = 1.26492 (* 1 = 1.26492 loss)
I0815 11:07:34.903911  8764 sgd_solver.cpp:136] Iteration 28600, lr = 0.0082125, m = 0.9
I0815 11:07:49.626127  8764 solver.cpp:312] Iteration 28700 (6.79261 iter/s, 14.7219s/100 iter), loss = 1.62631
I0815 11:07:49.626157  8764 solver.cpp:334]     Train net output #0: loss = 1.60162 (* 1 = 1.60162 loss)
I0815 11:07:49.626163  8764 sgd_solver.cpp:136] Iteration 28700, lr = 0.00820625, m = 0.9
I0815 11:08:05.696491  8764 solver.cpp:312] Iteration 28800 (6.22282 iter/s, 16.0699s/100 iter), loss = 1.48598
I0815 11:08:05.696847  8764 solver.cpp:334]     Train net output #0: loss = 1.60172 (* 1 = 1.60172 loss)
I0815 11:08:05.696970  8764 sgd_solver.cpp:136] Iteration 28800, lr = 0.0082, m = 0.9
I0815 11:08:20.970950  8764 solver.cpp:312] Iteration 28900 (6.54706 iter/s, 15.274s/100 iter), loss = 1.16453
I0815 11:08:20.971004  8764 solver.cpp:334]     Train net output #0: loss = 1.09054 (* 1 = 1.09054 loss)
I0815 11:08:20.971016  8764 sgd_solver.cpp:136] Iteration 28900, lr = 0.00819375, m = 0.9
I0815 11:08:35.569126  8764 solver.cpp:363] Sparsity after update:
I0815 11:08:35.582509  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:08:35.582551  8764 net.cpp:2192] conv1a_param_0(0.04) 
I0815 11:08:35.582571  8764 net.cpp:2192] conv1b_param_0(0.0417) 
I0815 11:08:35.582584  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:08:35.582597  8764 net.cpp:2192] res2a_branch2a_param_0(0.0868) 
I0815 11:08:35.582610  8764 net.cpp:2192] res2a_branch2b_param_0(0.0833) 
I0815 11:08:35.582623  8764 net.cpp:2192] res3a_branch2a_param_0(0.0885) 
I0815 11:08:35.582635  8764 net.cpp:2192] res3a_branch2b_param_0(0.0868) 
I0815 11:08:35.582648  8764 net.cpp:2192] res4a_branch2a_param_0(0.0894) 
I0815 11:08:35.582660  8764 net.cpp:2192] res4a_branch2b_param_0(0.0885) 
I0815 11:08:35.582672  8764 net.cpp:2192] res5a_branch2a_param_0(0.0898) 
I0815 11:08:35.582684  8764 net.cpp:2192] res5a_branch2b_param_0(0.0893) 
I0815 11:08:35.582697  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (210376/2.86678e+06) 0.0734
I0815 11:08:35.712347  8794 solver.cpp:409] Finding and applying sparsity: 0.1
I0815 11:08:55.406098  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:08:55.408097  8764 solver.cpp:312] Iteration 29000 (2.90392 iter/s, 34.4362s/100 iter), loss = 1.08967
I0815 11:08:55.408119  8764 solver.cpp:334]     Train net output #0: loss = 0.927995 (* 1 = 0.927995 loss)
I0815 11:08:55.408128  8764 sgd_solver.cpp:136] Iteration 29000, lr = 0.0081875, m = 0.9
I0815 11:09:10.263370  8764 solver.cpp:312] Iteration 29100 (6.73181 iter/s, 14.8549s/100 iter), loss = 1.32076
I0815 11:09:10.263464  8764 solver.cpp:334]     Train net output #0: loss = 1.10945 (* 1 = 1.10945 loss)
I0815 11:09:10.263481  8764 sgd_solver.cpp:136] Iteration 29100, lr = 0.00818125, m = 0.9
I0815 11:09:25.449616  8764 solver.cpp:312] Iteration 29200 (6.58509 iter/s, 15.1858s/100 iter), loss = 1.41317
I0815 11:09:25.449643  8764 solver.cpp:334]     Train net output #0: loss = 0.977146 (* 1 = 0.977146 loss)
I0815 11:09:25.449650  8764 sgd_solver.cpp:136] Iteration 29200, lr = 0.008175, m = 0.9
I0815 11:09:40.511469  8764 solver.cpp:312] Iteration 29300 (6.63947 iter/s, 15.0614s/100 iter), loss = 1.9192
I0815 11:09:40.511687  8764 solver.cpp:334]     Train net output #0: loss = 1.51714 (* 1 = 1.51714 loss)
I0815 11:09:40.511693  8764 sgd_solver.cpp:136] Iteration 29300, lr = 0.00816875, m = 0.9
I0815 11:09:55.044852  8764 solver.cpp:312] Iteration 29400 (6.8809 iter/s, 14.533s/100 iter), loss = 1.45987
I0815 11:09:55.044884  8764 solver.cpp:334]     Train net output #0: loss = 1.27953 (* 1 = 1.27953 loss)
I0815 11:09:55.044890  8764 sgd_solver.cpp:136] Iteration 29400, lr = 0.0081625, m = 0.9
I0815 11:10:09.582815  8764 solver.cpp:312] Iteration 29500 (6.87873 iter/s, 14.5376s/100 iter), loss = 1.07128
I0815 11:10:09.583035  8764 solver.cpp:334]     Train net output #0: loss = 1.04673 (* 1 = 1.04673 loss)
I0815 11:10:09.583145  8764 sgd_solver.cpp:136] Iteration 29500, lr = 0.00815625, m = 0.9
I0815 11:10:24.397931  8764 solver.cpp:312] Iteration 29600 (6.75005 iter/s, 14.8147s/100 iter), loss = 1.28777
I0815 11:10:24.398026  8764 solver.cpp:334]     Train net output #0: loss = 1.36537 (* 1 = 1.36537 loss)
I0815 11:10:24.398041  8764 sgd_solver.cpp:136] Iteration 29600, lr = 0.00815, m = 0.9
I0815 11:10:40.151679  8764 solver.cpp:312] Iteration 29700 (6.34787 iter/s, 15.7533s/100 iter), loss = 1.31191
I0815 11:10:40.151752  8764 solver.cpp:334]     Train net output #0: loss = 0.798737 (* 1 = 0.798737 loss)
I0815 11:10:40.151772  8764 sgd_solver.cpp:136] Iteration 29700, lr = 0.00814375, m = 0.9
I0815 11:10:54.784852  8764 solver.cpp:312] Iteration 29800 (6.83398 iter/s, 14.6328s/100 iter), loss = 1.22954
I0815 11:10:54.785019  8764 solver.cpp:334]     Train net output #0: loss = 1.32441 (* 1 = 1.32441 loss)
I0815 11:10:54.785037  8764 sgd_solver.cpp:136] Iteration 29800, lr = 0.0081375, m = 0.9
I0815 11:11:09.551268  8764 solver.cpp:312] Iteration 29900 (6.77231 iter/s, 14.766s/100 iter), loss = 1.20236
I0815 11:11:09.551293  8764 solver.cpp:334]     Train net output #0: loss = 1.31346 (* 1 = 1.31346 loss)
I0815 11:11:09.551300  8764 sgd_solver.cpp:136] Iteration 29900, lr = 0.00813125, m = 0.9
I0815 11:11:23.944622  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_30000.caffemodel
I0815 11:11:23.971768  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_30000.solverstate
I0815 11:11:23.977174  8764 solver.cpp:363] Sparsity after update:
I0815 11:11:23.978258  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:11:23.978267  8764 net.cpp:2192] conv1a_param_0(0.04) 
I0815 11:11:23.978276  8764 net.cpp:2192] conv1b_param_0(0.0486) 
I0815 11:11:23.978281  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:11:23.978284  8764 net.cpp:2192] res2a_branch2a_param_0(0.0972) 
I0815 11:11:23.978291  8764 net.cpp:2192] res2a_branch2b_param_0(0.0972) 
I0815 11:11:23.978296  8764 net.cpp:2192] res3a_branch2a_param_0(0.099) 
I0815 11:11:23.978302  8764 net.cpp:2192] res3a_branch2b_param_0(0.0972) 
I0815 11:11:23.978307  8764 net.cpp:2192] res4a_branch2a_param_0(0.0998) 
I0815 11:11:23.978309  8764 net.cpp:2192] res4a_branch2b_param_0(0.099) 
I0815 11:11:23.978314  8764 net.cpp:2192] res5a_branch2a_param_0(0.0998) 
I0815 11:11:23.978317  8764 net.cpp:2192] res5a_branch2b_param_0(0.0998) 
I0815 11:11:23.978322  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (234405/2.86678e+06) 0.0818
I0815 11:11:23.978332  8764 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 11:11:33.009713  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:11:38.409981  8747 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 11:11:45.811498  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.545176
I0815 11:11:45.811520  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.785115
I0815 11:11:45.811528  8764 solver.cpp:594]     Test net output #2: loss = 2.02438 (* 1 = 2.02438 loss)
I0815 11:11:45.811550  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.8326s
I0815 11:11:45.975399  8794 solver.cpp:409] Finding and applying sparsity: 0.11
I0815 11:12:05.725106  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:12:05.727202  8764 solver.cpp:312] Iteration 30000 (1.78017 iter/s, 56.1744s/100 iter), loss = 1.5899
I0815 11:12:05.727221  8764 solver.cpp:334]     Train net output #0: loss = 1.76908 (* 1 = 1.76908 loss)
I0815 11:12:05.727229  8764 sgd_solver.cpp:136] Iteration 30000, lr = 0.008125, m = 0.9
I0815 11:12:20.711761  8764 solver.cpp:312] Iteration 30100 (6.67373 iter/s, 14.9841s/100 iter), loss = 1.2876
I0815 11:12:20.711788  8764 solver.cpp:334]     Train net output #0: loss = 1.54309 (* 1 = 1.54309 loss)
I0815 11:12:20.711794  8764 sgd_solver.cpp:136] Iteration 30100, lr = 0.00811875, m = 0.9
I0815 11:12:35.447245  8764 solver.cpp:312] Iteration 30200 (6.78653 iter/s, 14.7351s/100 iter), loss = 1.61887
I0815 11:12:35.447312  8764 solver.cpp:334]     Train net output #0: loss = 1.47352 (* 1 = 1.47352 loss)
I0815 11:12:35.447329  8764 sgd_solver.cpp:136] Iteration 30200, lr = 0.0081125, m = 0.9
I0815 11:12:51.923940  8764 solver.cpp:312] Iteration 30300 (6.06936 iter/s, 16.4762s/100 iter), loss = 1.4051
I0815 11:12:51.924093  8764 solver.cpp:334]     Train net output #0: loss = 1.42913 (* 1 = 1.42913 loss)
I0815 11:12:51.924120  8764 sgd_solver.cpp:136] Iteration 30300, lr = 0.00810625, m = 0.9
I0815 11:13:09.554170  8764 solver.cpp:312] Iteration 30400 (5.67223 iter/s, 17.6297s/100 iter), loss = 1.70585
I0815 11:13:09.554196  8764 solver.cpp:334]     Train net output #0: loss = 1.48485 (* 1 = 1.48485 loss)
I0815 11:13:09.554200  8764 sgd_solver.cpp:136] Iteration 30400, lr = 0.0081, m = 0.9
I0815 11:13:25.444195  8764 solver.cpp:312] Iteration 30500 (6.29343 iter/s, 15.8896s/100 iter), loss = 1.36114
I0815 11:13:25.444272  8764 solver.cpp:334]     Train net output #0: loss = 1.19018 (* 1 = 1.19018 loss)
I0815 11:13:25.444284  8764 sgd_solver.cpp:136] Iteration 30500, lr = 0.00809375, m = 0.9
I0815 11:13:40.242146  8764 solver.cpp:312] Iteration 30600 (6.75788 iter/s, 14.7975s/100 iter), loss = 1.82426
I0815 11:13:40.242172  8764 solver.cpp:334]     Train net output #0: loss = 1.71371 (* 1 = 1.71371 loss)
I0815 11:13:40.242177  8764 sgd_solver.cpp:136] Iteration 30600, lr = 0.0080875, m = 0.9
I0815 11:13:54.712312  8764 solver.cpp:312] Iteration 30700 (6.91096 iter/s, 14.4698s/100 iter), loss = 1.48349
I0815 11:13:54.712339  8764 solver.cpp:334]     Train net output #0: loss = 1.59649 (* 1 = 1.59649 loss)
I0815 11:13:54.712343  8764 sgd_solver.cpp:136] Iteration 30700, lr = 0.00808125, m = 0.9
I0815 11:14:09.810112  8764 solver.cpp:312] Iteration 30800 (6.62367 iter/s, 15.0974s/100 iter), loss = 1.77905
I0815 11:14:09.810174  8764 solver.cpp:334]     Train net output #0: loss = 1.88324 (* 1 = 1.88324 loss)
I0815 11:14:09.810180  8764 sgd_solver.cpp:136] Iteration 30800, lr = 0.008075, m = 0.9
I0815 11:14:25.199748  8764 solver.cpp:312] Iteration 30900 (6.49806 iter/s, 15.3892s/100 iter), loss = 1.54822
I0815 11:14:25.199931  8764 solver.cpp:334]     Train net output #0: loss = 1.51528 (* 1 = 1.51528 loss)
I0815 11:14:25.200021  8764 sgd_solver.cpp:136] Iteration 30900, lr = 0.00806875, m = 0.9
I0815 11:14:41.291348  8764 solver.cpp:363] Sparsity after update:
I0815 11:14:41.302917  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:14:41.303059  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:14:41.303153  8764 net.cpp:2192] conv1b_param_0(0.0486) 
I0815 11:14:41.303241  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:14:41.303330  8764 net.cpp:2192] res2a_branch2a_param_0(0.108) 
I0815 11:14:41.303422  8764 net.cpp:2192] res2a_branch2b_param_0(0.104) 
I0815 11:14:41.303516  8764 net.cpp:2192] res3a_branch2a_param_0(0.109) 
I0815 11:14:41.303611  8764 net.cpp:2192] res3a_branch2b_param_0(0.108) 
I0815 11:14:41.303700  8764 net.cpp:2192] res4a_branch2a_param_0(0.109) 
I0815 11:14:41.303791  8764 net.cpp:2192] res4a_branch2b_param_0(0.109) 
I0815 11:14:41.303879  8764 net.cpp:2192] res5a_branch2a_param_0(0.11) 
I0815 11:14:41.303967  8764 net.cpp:2192] res5a_branch2b_param_0(0.109) 
I0815 11:14:41.304055  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (257610/2.86678e+06) 0.0899
I0815 11:14:41.462749  8794 solver.cpp:409] Finding and applying sparsity: 0.12
I0815 11:15:02.577852  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:15:02.579936  8764 solver.cpp:312] Iteration 31000 (2.67529 iter/s, 37.3792s/100 iter), loss = 1.56124
I0815 11:15:02.579953  8764 solver.cpp:334]     Train net output #0: loss = 1.43548 (* 1 = 1.43548 loss)
I0815 11:15:02.579958  8764 sgd_solver.cpp:136] Iteration 31000, lr = 0.0080625, m = 0.9
I0815 11:15:18.166894  8764 solver.cpp:312] Iteration 31100 (6.4158 iter/s, 15.5865s/100 iter), loss = 1.17394
I0815 11:15:18.166965  8764 solver.cpp:334]     Train net output #0: loss = 1.12161 (* 1 = 1.12161 loss)
I0815 11:15:18.166972  8764 sgd_solver.cpp:136] Iteration 31100, lr = 0.00805625, m = 0.9
I0815 11:15:34.653153  8764 solver.cpp:312] Iteration 31200 (6.06583 iter/s, 16.4858s/100 iter), loss = 1.52235
I0815 11:15:34.653329  8764 solver.cpp:334]     Train net output #0: loss = 1.31434 (* 1 = 1.31434 loss)
I0815 11:15:34.653417  8764 sgd_solver.cpp:136] Iteration 31200, lr = 0.00805, m = 0.9
I0815 11:15:50.398641  8764 solver.cpp:312] Iteration 31300 (6.3512 iter/s, 15.745s/100 iter), loss = 1.76115
I0815 11:15:50.398721  8764 solver.cpp:334]     Train net output #0: loss = 1.87971 (* 1 = 1.87971 loss)
I0815 11:15:50.398728  8764 sgd_solver.cpp:136] Iteration 31300, lr = 0.00804375, m = 0.9
I0815 11:16:05.529326  8764 solver.cpp:312] Iteration 31400 (6.60927 iter/s, 15.1303s/100 iter), loss = 1.21216
I0815 11:16:05.529353  8764 solver.cpp:334]     Train net output #0: loss = 1.0918 (* 1 = 1.0918 loss)
I0815 11:16:05.529359  8764 sgd_solver.cpp:136] Iteration 31400, lr = 0.0080375, m = 0.9
I0815 11:16:22.730139  8764 solver.cpp:312] Iteration 31500 (5.81384 iter/s, 17.2003s/100 iter), loss = 1.62102
I0815 11:16:22.730232  8764 solver.cpp:334]     Train net output #0: loss = 1.52572 (* 1 = 1.52572 loss)
I0815 11:16:22.730250  8764 sgd_solver.cpp:136] Iteration 31500, lr = 0.00803125, m = 0.9
I0815 11:16:37.267717  8764 solver.cpp:312] Iteration 31600 (6.87892 iter/s, 14.5372s/100 iter), loss = 0.999727
I0815 11:16:37.267791  8764 solver.cpp:334]     Train net output #0: loss = 1.12834 (* 1 = 1.12834 loss)
I0815 11:16:37.267810  8764 sgd_solver.cpp:136] Iteration 31600, lr = 0.008025, m = 0.9
I0815 11:16:51.829358  8764 solver.cpp:312] Iteration 31700 (6.86755 iter/s, 14.5612s/100 iter), loss = 1.52216
I0815 11:16:51.829385  8764 solver.cpp:334]     Train net output #0: loss = 1.38804 (* 1 = 1.38804 loss)
I0815 11:16:51.829390  8764 sgd_solver.cpp:136] Iteration 31700, lr = 0.00801875, m = 0.9
I0815 11:17:07.508738  8764 solver.cpp:312] Iteration 31800 (6.37798 iter/s, 15.6789s/100 iter), loss = 1.3094
I0815 11:17:07.508941  8764 solver.cpp:334]     Train net output #0: loss = 1.31021 (* 1 = 1.31021 loss)
I0815 11:17:07.508958  8764 sgd_solver.cpp:136] Iteration 31800, lr = 0.0080125, m = 0.9
I0815 11:17:23.549547  8764 solver.cpp:312] Iteration 31900 (6.23427 iter/s, 16.0404s/100 iter), loss = 1.54923
I0815 11:17:23.549619  8764 solver.cpp:334]     Train net output #0: loss = 1.49676 (* 1 = 1.49676 loss)
I0815 11:17:23.549638  8764 sgd_solver.cpp:136] Iteration 31900, lr = 0.00800625, m = 0.9
I0815 11:17:40.074251  8764 solver.cpp:363] Sparsity after update:
I0815 11:17:40.078821  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:17:40.078832  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:17:40.078840  8764 net.cpp:2192] conv1b_param_0(0.0556) 
I0815 11:17:40.078843  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:17:40.078850  8764 net.cpp:2192] res2a_branch2a_param_0(0.118) 
I0815 11:17:40.078853  8764 net.cpp:2192] res2a_branch2b_param_0(0.118) 
I0815 11:17:40.078856  8764 net.cpp:2192] res3a_branch2a_param_0(0.12) 
I0815 11:17:40.078860  8764 net.cpp:2192] res3a_branch2b_param_0(0.118) 
I0815 11:17:40.078863  8764 net.cpp:2192] res4a_branch2a_param_0(0.12) 
I0815 11:17:40.078867  8764 net.cpp:2192] res4a_branch2b_param_0(0.12) 
I0815 11:17:40.078871  8764 net.cpp:2192] res5a_branch2a_param_0(0.12) 
I0815 11:17:40.078874  8764 net.cpp:2192] res5a_branch2b_param_0(0.12) 
I0815 11:17:40.078877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (281617/2.86678e+06) 0.0982
I0815 11:17:40.078889  8764 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 11:18:05.524494  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.547882
I0815 11:18:05.524520  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.789822
I0815 11:18:05.524528  8764 solver.cpp:594]     Test net output #2: loss = 2.00044 (* 1 = 2.00044 loss)
I0815 11:18:05.524549  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.445s
I0815 11:18:05.672992  8794 solver.cpp:409] Finding and applying sparsity: 0.13
I0815 11:18:25.010236  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:18:25.012337  8764 solver.cpp:312] Iteration 32000 (1.62705 iter/s, 61.4611s/100 iter), loss = 1.4308
I0815 11:18:25.012356  8764 solver.cpp:334]     Train net output #0: loss = 1.13015 (* 1 = 1.13015 loss)
I0815 11:18:25.012362  8764 sgd_solver.cpp:136] Iteration 32000, lr = 0.008, m = 0.9
I0815 11:18:41.392751  8764 solver.cpp:312] Iteration 32100 (6.10503 iter/s, 16.3799s/100 iter), loss = 1.32199
I0815 11:18:41.392779  8764 solver.cpp:334]     Train net output #0: loss = 1.40445 (* 1 = 1.40445 loss)
I0815 11:18:41.392786  8764 sgd_solver.cpp:136] Iteration 32100, lr = 0.00799375, m = 0.9
I0815 11:18:55.968163  8764 solver.cpp:312] Iteration 32200 (6.86106 iter/s, 14.575s/100 iter), loss = 1.65492
I0815 11:18:55.968243  8764 solver.cpp:334]     Train net output #0: loss = 1.71042 (* 1 = 1.71042 loss)
I0815 11:18:55.968250  8764 sgd_solver.cpp:136] Iteration 32200, lr = 0.0079875, m = 0.9
I0815 11:19:11.882033  8764 solver.cpp:312] Iteration 32300 (6.28401 iter/s, 15.9134s/100 iter), loss = 1.4625
I0815 11:19:11.882092  8764 solver.cpp:334]     Train net output #0: loss = 1.22742 (* 1 = 1.22742 loss)
I0815 11:19:11.882107  8764 sgd_solver.cpp:136] Iteration 32300, lr = 0.00798125, m = 0.9
I0815 11:19:26.753446  8764 solver.cpp:312] Iteration 32400 (6.7245 iter/s, 14.871s/100 iter), loss = 1.4643
I0815 11:19:26.753516  8764 solver.cpp:334]     Train net output #0: loss = 1.35998 (* 1 = 1.35998 loss)
I0815 11:19:26.753525  8764 sgd_solver.cpp:136] Iteration 32400, lr = 0.007975, m = 0.9
I0815 11:19:41.259615  8764 solver.cpp:312] Iteration 32500 (6.89381 iter/s, 14.5058s/100 iter), loss = 1.36085
I0815 11:19:41.259645  8764 solver.cpp:334]     Train net output #0: loss = 1.44033 (* 1 = 1.44033 loss)
I0815 11:19:41.259654  8764 sgd_solver.cpp:136] Iteration 32500, lr = 0.00796875, m = 0.9
I0815 11:19:56.219529  8764 solver.cpp:312] Iteration 32600 (6.68472 iter/s, 14.9595s/100 iter), loss = 1.66874
I0815 11:19:56.219552  8764 solver.cpp:334]     Train net output #0: loss = 2.0117 (* 1 = 2.0117 loss)
I0815 11:19:56.219555  8764 sgd_solver.cpp:136] Iteration 32600, lr = 0.0079625, m = 0.9
I0815 11:20:13.282013  8764 solver.cpp:312] Iteration 32700 (5.86097 iter/s, 17.062s/100 iter), loss = 1.20602
I0815 11:20:13.282099  8764 solver.cpp:334]     Train net output #0: loss = 1.33977 (* 1 = 1.33977 loss)
I0815 11:20:13.282112  8764 sgd_solver.cpp:136] Iteration 32700, lr = 0.00795625, m = 0.9
I0815 11:20:30.988759  8764 solver.cpp:312] Iteration 32800 (5.64772 iter/s, 17.7063s/100 iter), loss = 1.21403
I0815 11:20:30.988903  8764 solver.cpp:334]     Train net output #0: loss = 1.16877 (* 1 = 1.16877 loss)
I0815 11:20:30.988920  8764 sgd_solver.cpp:136] Iteration 32800, lr = 0.00795, m = 0.9
I0815 11:20:46.554407  8764 solver.cpp:312] Iteration 32900 (6.42458 iter/s, 15.5652s/100 iter), loss = 1.80834
I0815 11:20:46.554471  8764 solver.cpp:334]     Train net output #0: loss = 1.93065 (* 1 = 1.93065 loss)
I0815 11:20:46.554478  8764 sgd_solver.cpp:136] Iteration 32900, lr = 0.00794375, m = 0.9
I0815 11:21:01.552922  8764 solver.cpp:363] Sparsity after update:
I0815 11:21:01.566452  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:21:01.566483  8764 net.cpp:2192] conv1a_param_0(0.0533) 
I0815 11:21:01.566496  8764 net.cpp:2192] conv1b_param_0(0.0938) 
I0815 11:21:01.566506  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:21:01.566515  8764 net.cpp:2192] res2a_branch2a_param_0(0.128) 
I0815 11:21:01.566524  8764 net.cpp:2192] res2a_branch2b_param_0(0.125) 
I0815 11:21:01.566532  8764 net.cpp:2192] res3a_branch2a_param_0(0.128) 
I0815 11:21:01.566540  8764 net.cpp:2192] res3a_branch2b_param_0(0.128) 
I0815 11:21:01.566550  8764 net.cpp:2192] res4a_branch2a_param_0(0.129) 
I0815 11:21:01.566560  8764 net.cpp:2192] res4a_branch2b_param_0(0.128) 
I0815 11:21:01.566568  8764 net.cpp:2192] res5a_branch2a_param_0(0.13) 
I0815 11:21:01.566577  8764 net.cpp:2192] res5a_branch2b_param_0(0.129) 
I0815 11:21:01.566587  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (304492/2.86678e+06) 0.106
I0815 11:21:01.705672  8794 solver.cpp:409] Finding and applying sparsity: 0.14
I0815 11:21:21.776078  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:21:21.778131  8764 solver.cpp:312] Iteration 33000 (2.83907 iter/s, 35.2228s/100 iter), loss = 1.53488
I0815 11:21:21.778153  8764 solver.cpp:334]     Train net output #0: loss = 1.46084 (* 1 = 1.46084 loss)
I0815 11:21:21.778161  8764 sgd_solver.cpp:136] Iteration 33000, lr = 0.0079375, m = 0.9
I0815 11:21:37.674592  8764 solver.cpp:312] Iteration 33100 (6.29089 iter/s, 15.896s/100 iter), loss = 1.68687
I0815 11:21:37.674662  8764 solver.cpp:334]     Train net output #0: loss = 1.8109 (* 1 = 1.8109 loss)
I0815 11:21:37.674682  8764 sgd_solver.cpp:136] Iteration 33100, lr = 0.00793125, m = 0.9
I0815 11:21:53.345787  8764 solver.cpp:312] Iteration 33200 (6.38131 iter/s, 15.6708s/100 iter), loss = 1.4617
I0815 11:21:53.345860  8764 solver.cpp:334]     Train net output #0: loss = 1.21956 (* 1 = 1.21956 loss)
I0815 11:21:53.345866  8764 sgd_solver.cpp:136] Iteration 33200, lr = 0.007925, m = 0.9
I0815 11:22:08.656699  8764 solver.cpp:312] Iteration 33300 (6.53147 iter/s, 15.3105s/100 iter), loss = 1.59192
I0815 11:22:08.656728  8764 solver.cpp:334]     Train net output #0: loss = 1.31211 (* 1 = 1.31211 loss)
I0815 11:22:08.656734  8764 sgd_solver.cpp:136] Iteration 33300, lr = 0.00791875, m = 0.9
I0815 11:22:24.034198  8764 solver.cpp:312] Iteration 33400 (6.50319 iter/s, 15.3771s/100 iter), loss = 1.54802
I0815 11:22:24.034251  8764 solver.cpp:334]     Train net output #0: loss = 1.33934 (* 1 = 1.33934 loss)
I0815 11:22:24.034258  8764 sgd_solver.cpp:136] Iteration 33400, lr = 0.0079125, m = 0.9
I0815 11:22:40.046442  8764 solver.cpp:312] Iteration 33500 (6.2454 iter/s, 16.0118s/100 iter), loss = 1.67432
I0815 11:22:40.046510  8764 solver.cpp:334]     Train net output #0: loss = 1.51729 (* 1 = 1.51729 loss)
I0815 11:22:40.046530  8764 sgd_solver.cpp:136] Iteration 33500, lr = 0.00790625, m = 0.9
I0815 11:22:57.076661  8764 solver.cpp:312] Iteration 33600 (5.87208 iter/s, 17.0297s/100 iter), loss = 1.59763
I0815 11:22:57.076769  8764 solver.cpp:334]     Train net output #0: loss = 1.81875 (* 1 = 1.81875 loss)
I0815 11:22:57.076792  8764 sgd_solver.cpp:136] Iteration 33600, lr = 0.0079, m = 0.9
I0815 11:23:12.250653  8764 solver.cpp:312] Iteration 33700 (6.5904 iter/s, 15.1736s/100 iter), loss = 1.34758
I0815 11:23:12.250675  8764 solver.cpp:334]     Train net output #0: loss = 1.60743 (* 1 = 1.60743 loss)
I0815 11:23:12.250681  8764 sgd_solver.cpp:136] Iteration 33700, lr = 0.00789375, m = 0.9
I0815 11:23:27.083969  8764 solver.cpp:312] Iteration 33800 (6.74177 iter/s, 14.8329s/100 iter), loss = 1.55141
I0815 11:23:27.084023  8764 solver.cpp:334]     Train net output #0: loss = 1.90215 (* 1 = 1.90215 loss)
I0815 11:23:27.084030  8764 sgd_solver.cpp:136] Iteration 33800, lr = 0.0078875, m = 0.9
I0815 11:23:43.764436  8764 solver.cpp:312] Iteration 33900 (5.9952 iter/s, 16.68s/100 iter), loss = 1.4318
I0815 11:23:43.764462  8764 solver.cpp:334]     Train net output #0: loss = 1.52724 (* 1 = 1.52724 loss)
I0815 11:23:43.764466  8764 sgd_solver.cpp:136] Iteration 33900, lr = 0.00788125, m = 0.9
I0815 11:23:59.943383  8764 solver.cpp:363] Sparsity after update:
I0815 11:23:59.947299  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:23:59.947329  8764 net.cpp:2192] conv1a_param_0(0.0667) 
I0815 11:23:59.947345  8764 net.cpp:2192] conv1b_param_0(0.104) 
I0815 11:23:59.947355  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:23:59.947365  8764 net.cpp:2192] res2a_branch2a_param_0(0.139) 
I0815 11:23:59.947374  8764 net.cpp:2192] res2a_branch2b_param_0(0.139) 
I0815 11:23:59.947384  8764 net.cpp:2192] res3a_branch2a_param_0(0.139) 
I0815 11:23:59.947393  8764 net.cpp:2192] res3a_branch2b_param_0(0.139) 
I0815 11:23:59.947402  8764 net.cpp:2192] res4a_branch2a_param_0(0.14) 
I0815 11:23:59.947412  8764 net.cpp:2192] res4a_branch2b_param_0(0.139) 
I0815 11:23:59.947420  8764 net.cpp:2192] res5a_branch2a_param_0(0.14) 
I0815 11:23:59.947429  8764 net.cpp:2192] res5a_branch2b_param_0(0.14) 
I0815 11:23:59.947439  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (328550/2.86678e+06) 0.115
I0815 11:23:59.947463  8764 solver.cpp:509] Iteration 34000, Testing net (#0)
I0815 11:24:24.419677  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:24:24.785504  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.557
I0815 11:24:24.785531  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791939
I0815 11:24:24.785537  8764 solver.cpp:594]     Test net output #2: loss = 1.95703 (* 1 = 1.95703 loss)
I0815 11:24:24.785559  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.8374s
I0815 11:24:24.922410  8794 solver.cpp:409] Finding and applying sparsity: 0.15
I0815 11:24:44.920830  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:24:44.922870  8764 solver.cpp:312] Iteration 34000 (1.63514 iter/s, 61.1568s/100 iter), loss = 1.80419
I0815 11:24:44.922891  8764 solver.cpp:334]     Train net output #0: loss = 1.36294 (* 1 = 1.36294 loss)
I0815 11:24:44.922899  8764 sgd_solver.cpp:136] Iteration 34000, lr = 0.007875, m = 0.9
I0815 11:25:00.186173  8764 solver.cpp:312] Iteration 34100 (6.55185 iter/s, 15.2629s/100 iter), loss = 1.54083
I0815 11:25:00.186200  8764 solver.cpp:334]     Train net output #0: loss = 1.98953 (* 1 = 1.98953 loss)
I0815 11:25:00.186206  8764 sgd_solver.cpp:136] Iteration 34100, lr = 0.00786875, m = 0.9
I0815 11:25:15.190654  8764 solver.cpp:312] Iteration 34200 (6.66486 iter/s, 15.0041s/100 iter), loss = 1.47853
I0815 11:25:15.190752  8764 solver.cpp:334]     Train net output #0: loss = 1.33805 (* 1 = 1.33805 loss)
I0815 11:25:15.190760  8764 sgd_solver.cpp:136] Iteration 34200, lr = 0.0078625, m = 0.9
I0815 11:25:31.300882  8764 solver.cpp:312] Iteration 34300 (6.20741 iter/s, 16.1098s/100 iter), loss = 1.49382
I0815 11:25:31.300921  8764 solver.cpp:334]     Train net output #0: loss = 1.41662 (* 1 = 1.41662 loss)
I0815 11:25:31.300930  8764 sgd_solver.cpp:136] Iteration 34300, lr = 0.00785625, m = 0.9
I0815 11:25:47.740947  8764 solver.cpp:312] Iteration 34400 (6.08287 iter/s, 16.4396s/100 iter), loss = 1.41608
I0815 11:25:47.741032  8764 solver.cpp:334]     Train net output #0: loss = 1.23243 (* 1 = 1.23243 loss)
I0815 11:25:47.741045  8764 sgd_solver.cpp:136] Iteration 34400, lr = 0.00785, m = 0.9
I0815 11:26:05.341651  8764 solver.cpp:312] Iteration 34500 (5.68175 iter/s, 17.6002s/100 iter), loss = 1.09213
I0815 11:26:05.341673  8764 solver.cpp:334]     Train net output #0: loss = 1.24418 (* 1 = 1.24418 loss)
I0815 11:26:05.341677  8764 sgd_solver.cpp:136] Iteration 34500, lr = 0.00784375, m = 0.9
I0815 11:26:20.984057  8764 solver.cpp:312] Iteration 34600 (6.39306 iter/s, 15.642s/100 iter), loss = 1.17766
I0815 11:26:20.984140  8764 solver.cpp:334]     Train net output #0: loss = 1.11194 (* 1 = 1.11194 loss)
I0815 11:26:20.984148  8764 sgd_solver.cpp:136] Iteration 34600, lr = 0.0078375, m = 0.9
I0815 11:26:36.007835  8764 solver.cpp:312] Iteration 34700 (6.6563 iter/s, 15.0234s/100 iter), loss = 1.21683
I0815 11:26:36.007915  8764 solver.cpp:334]     Train net output #0: loss = 1.35786 (* 1 = 1.35786 loss)
I0815 11:26:36.007935  8764 sgd_solver.cpp:136] Iteration 34700, lr = 0.00783125, m = 0.9
I0815 11:26:50.811862  8764 solver.cpp:312] Iteration 34800 (6.75511 iter/s, 14.8036s/100 iter), loss = 1.47487
I0815 11:26:50.811887  8764 solver.cpp:334]     Train net output #0: loss = 1.82908 (* 1 = 1.82908 loss)
I0815 11:26:50.811892  8764 sgd_solver.cpp:136] Iteration 34800, lr = 0.007825, m = 0.9
I0815 11:27:07.234835  8764 solver.cpp:312] Iteration 34900 (6.0892 iter/s, 16.4225s/100 iter), loss = 1.36259
I0815 11:27:07.234895  8764 solver.cpp:334]     Train net output #0: loss = 1.49973 (* 1 = 1.49973 loss)
I0815 11:27:07.234901  8764 sgd_solver.cpp:136] Iteration 34900, lr = 0.00781875, m = 0.9
I0815 11:27:21.564394  8764 solver.cpp:363] Sparsity after update:
I0815 11:27:21.574854  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:27:21.574867  8764 net.cpp:2192] conv1a_param_0(0.0662) 
I0815 11:27:21.574875  8764 net.cpp:2192] conv1b_param_0(0.104) 
I0815 11:27:21.574880  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:27:21.575011  8764 net.cpp:2192] res2a_branch2a_param_0(0.149) 
I0815 11:27:21.575016  8764 net.cpp:2192] res2a_branch2b_param_0(0.146) 
I0815 11:27:21.575018  8764 net.cpp:2192] res3a_branch2a_param_0(0.149) 
I0815 11:27:21.575021  8764 net.cpp:2192] res3a_branch2b_param_0(0.149) 
I0815 11:27:21.575139  8764 net.cpp:2192] res4a_branch2a_param_0(0.149) 
I0815 11:27:21.575142  8764 net.cpp:2192] res4a_branch2b_param_0(0.149) 
I0815 11:27:21.575146  8764 net.cpp:2192] res5a_branch2a_param_0(0.15) 
I0815 11:27:21.575150  8764 net.cpp:2192] res5a_branch2b_param_0(0.149) 
I0815 11:27:21.575268  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (351719/2.86678e+06) 0.123
I0815 11:27:21.707368  8794 solver.cpp:409] Finding and applying sparsity: 0.16
I0815 11:27:42.163173  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:27:42.165204  8764 solver.cpp:312] Iteration 35000 (2.86292 iter/s, 34.9294s/100 iter), loss = 1.19298
I0815 11:27:42.165222  8764 solver.cpp:334]     Train net output #0: loss = 0.96306 (* 1 = 0.96306 loss)
I0815 11:27:42.165227  8764 sgd_solver.cpp:136] Iteration 35000, lr = 0.0078125, m = 0.9
I0815 11:27:57.014886  8764 solver.cpp:312] Iteration 35100 (6.73434 iter/s, 14.8493s/100 iter), loss = 1.25358
I0815 11:27:57.015107  8764 solver.cpp:334]     Train net output #0: loss = 1.65726 (* 1 = 1.65726 loss)
I0815 11:27:57.015218  8764 sgd_solver.cpp:136] Iteration 35100, lr = 0.00780625, m = 0.9
I0815 11:28:11.910329  8764 solver.cpp:312] Iteration 35200 (6.71365 iter/s, 14.895s/100 iter), loss = 1.64793
I0815 11:28:11.910353  8764 solver.cpp:334]     Train net output #0: loss = 1.838 (* 1 = 1.838 loss)
I0815 11:28:11.910359  8764 sgd_solver.cpp:136] Iteration 35200, lr = 0.0078, m = 0.9
I0815 11:28:29.028208  8764 solver.cpp:312] Iteration 35300 (5.84201 iter/s, 17.1174s/100 iter), loss = 1.52537
I0815 11:28:29.028307  8764 solver.cpp:334]     Train net output #0: loss = 0.974179 (* 1 = 0.974179 loss)
I0815 11:28:29.028319  8764 sgd_solver.cpp:136] Iteration 35300, lr = 0.00779375, m = 0.9
I0815 11:28:44.439420  8764 solver.cpp:312] Iteration 35400 (6.48896 iter/s, 15.4108s/100 iter), loss = 1.42437
I0815 11:28:44.439448  8764 solver.cpp:334]     Train net output #0: loss = 1.31148 (* 1 = 1.31148 loss)
I0815 11:28:44.439455  8764 sgd_solver.cpp:136] Iteration 35400, lr = 0.0077875, m = 0.9
I0815 11:29:01.591969  8764 solver.cpp:312] Iteration 35500 (5.8302 iter/s, 17.1521s/100 iter), loss = 1.59699
I0815 11:29:01.592020  8764 solver.cpp:334]     Train net output #0: loss = 1.45553 (* 1 = 1.45553 loss)
I0815 11:29:01.592026  8764 sgd_solver.cpp:136] Iteration 35500, lr = 0.00778125, m = 0.9
I0815 11:29:19.469418  8764 solver.cpp:312] Iteration 35600 (5.59379 iter/s, 17.877s/100 iter), loss = 1.89033
I0815 11:29:19.469596  8764 solver.cpp:334]     Train net output #0: loss = 1.87403 (* 1 = 1.87403 loss)
I0815 11:29:19.469686  8764 sgd_solver.cpp:136] Iteration 35600, lr = 0.007775, m = 0.9
I0815 11:29:36.423794  8764 solver.cpp:312] Iteration 35700 (5.89834 iter/s, 16.9539s/100 iter), loss = 1.39617
I0815 11:29:36.423857  8764 solver.cpp:334]     Train net output #0: loss = 0.967375 (* 1 = 0.967375 loss)
I0815 11:29:36.423864  8764 sgd_solver.cpp:136] Iteration 35700, lr = 0.00776875, m = 0.9
I0815 11:29:53.480577  8764 solver.cpp:312] Iteration 35800 (5.86294 iter/s, 17.0563s/100 iter), loss = 1.39828
I0815 11:29:53.480634  8764 solver.cpp:334]     Train net output #0: loss = 1.41542 (* 1 = 1.41542 loss)
I0815 11:29:53.480650  8764 sgd_solver.cpp:136] Iteration 35800, lr = 0.0077625, m = 0.9
I0815 11:30:10.723016  8764 solver.cpp:312] Iteration 35900 (5.7998 iter/s, 17.242s/100 iter), loss = 1.47997
I0815 11:30:10.723117  8764 solver.cpp:334]     Train net output #0: loss = 1.29137 (* 1 = 1.29137 loss)
I0815 11:30:10.723136  8764 sgd_solver.cpp:136] Iteration 35900, lr = 0.00775625, m = 0.9
I0815 11:30:27.144430  8764 solver.cpp:363] Sparsity after update:
I0815 11:30:27.157223  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:30:27.157285  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:30:27.157315  8764 net.cpp:2192] conv1b_param_0(0.115) 
I0815 11:30:27.157335  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:30:27.157387  8764 net.cpp:2192] res2a_branch2a_param_0(0.16) 
I0815 11:30:27.157424  8764 net.cpp:2192] res2a_branch2b_param_0(0.16) 
I0815 11:30:27.157444  8764 net.cpp:2192] res3a_branch2a_param_0(0.16) 
I0815 11:30:27.157461  8764 net.cpp:2192] res3a_branch2b_param_0(0.16) 
I0815 11:30:27.157495  8764 net.cpp:2192] res4a_branch2a_param_0(0.16) 
I0815 11:30:27.157512  8764 net.cpp:2192] res4a_branch2b_param_0(0.16) 
I0815 11:30:27.157553  8764 net.cpp:2192] res5a_branch2a_param_0(0.16) 
I0815 11:30:27.157570  8764 net.cpp:2192] res5a_branch2b_param_0(0.16) 
I0815 11:30:27.157589  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (375789/2.86678e+06) 0.131
I0815 11:30:27.157622  8764 solver.cpp:509] Iteration 36000, Testing net (#0)
I0815 11:30:51.102052  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.556824
I0815 11:30:51.102121  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.791586
I0815 11:30:51.102128  8764 solver.cpp:594]     Test net output #2: loss = 1.97146 (* 1 = 1.97146 loss)
I0815 11:30:51.102147  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.9439s
I0815 11:30:51.260190  8794 solver.cpp:409] Finding and applying sparsity: 0.17
I0815 11:31:11.242786  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:31:11.244763  8764 solver.cpp:312] Iteration 36000 (1.65234 iter/s, 60.5201s/100 iter), loss = 1.55046
I0815 11:31:11.244786  8764 solver.cpp:334]     Train net output #0: loss = 1.4654 (* 1 = 1.4654 loss)
I0815 11:31:11.244794  8764 sgd_solver.cpp:136] Iteration 36000, lr = 0.00775, m = 0.9
I0815 11:31:28.195852  8764 solver.cpp:312] Iteration 36100 (5.89949 iter/s, 16.9506s/100 iter), loss = 1.37475
I0815 11:31:28.196061  8764 solver.cpp:334]     Train net output #0: loss = 1.34243 (* 1 = 1.34243 loss)
I0815 11:31:28.196152  8764 sgd_solver.cpp:136] Iteration 36100, lr = 0.00774375, m = 0.9
I0815 11:31:44.183331  8764 solver.cpp:312] Iteration 36200 (6.25507 iter/s, 15.987s/100 iter), loss = 1.87017
I0815 11:31:44.183357  8764 solver.cpp:334]     Train net output #0: loss = 1.61033 (* 1 = 1.61033 loss)
I0815 11:31:44.183363  8764 sgd_solver.cpp:136] Iteration 36200, lr = 0.0077375, m = 0.9
I0815 11:31:59.514802  8764 solver.cpp:312] Iteration 36300 (6.52271 iter/s, 15.331s/100 iter), loss = 1.33007
I0815 11:31:59.514858  8764 solver.cpp:334]     Train net output #0: loss = 1.4809 (* 1 = 1.4809 loss)
I0815 11:31:59.514864  8764 sgd_solver.cpp:136] Iteration 36300, lr = 0.00773125, m = 0.9
I0815 11:32:15.762256  8764 solver.cpp:312] Iteration 36400 (6.15498 iter/s, 16.247s/100 iter), loss = 1.25349
I0815 11:32:15.762284  8764 solver.cpp:334]     Train net output #0: loss = 1.64154 (* 1 = 1.64154 loss)
I0815 11:32:15.762287  8764 sgd_solver.cpp:136] Iteration 36400, lr = 0.007725, m = 0.9
I0815 11:32:32.052091  8764 solver.cpp:312] Iteration 36500 (6.13897 iter/s, 16.2894s/100 iter), loss = 1.45727
I0815 11:32:32.052160  8764 solver.cpp:334]     Train net output #0: loss = 1.44405 (* 1 = 1.44405 loss)
I0815 11:32:32.052168  8764 sgd_solver.cpp:136] Iteration 36500, lr = 0.00771875, m = 0.9
I0815 11:32:47.590975  8764 solver.cpp:312] Iteration 36600 (6.43565 iter/s, 15.5385s/100 iter), loss = 1.46538
I0815 11:32:47.591006  8764 solver.cpp:334]     Train net output #0: loss = 1.22474 (* 1 = 1.22474 loss)
I0815 11:32:47.591011  8764 sgd_solver.cpp:136] Iteration 36600, lr = 0.0077125, m = 0.9
I0815 11:33:02.649451  8764 solver.cpp:312] Iteration 36700 (6.64096 iter/s, 15.0581s/100 iter), loss = 1.34385
I0815 11:33:02.649695  8764 solver.cpp:334]     Train net output #0: loss = 1.06285 (* 1 = 1.06285 loss)
I0815 11:33:02.649719  8764 sgd_solver.cpp:136] Iteration 36700, lr = 0.00770625, m = 0.9
I0815 11:33:17.599063  8764 solver.cpp:312] Iteration 36800 (6.68932 iter/s, 14.9492s/100 iter), loss = 1.14324
I0815 11:33:17.599088  8764 solver.cpp:334]     Train net output #0: loss = 1.43592 (* 1 = 1.43592 loss)
I0815 11:33:17.599094  8764 sgd_solver.cpp:136] Iteration 36800, lr = 0.0077, m = 0.9
I0815 11:33:35.190912  8764 solver.cpp:312] Iteration 36900 (5.68461 iter/s, 17.5914s/100 iter), loss = 1.47882
I0815 11:33:35.191040  8764 solver.cpp:334]     Train net output #0: loss = 1.55108 (* 1 = 1.55108 loss)
I0815 11:33:35.191058  8764 sgd_solver.cpp:136] Iteration 36900, lr = 0.00769375, m = 0.9
I0815 11:33:51.420459  8764 solver.cpp:363] Sparsity after update:
I0815 11:33:51.432106  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:33:51.432258  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:33:51.432353  8764 net.cpp:2192] conv1b_param_0(0.125) 
I0815 11:33:51.432441  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:33:51.432526  8764 net.cpp:2192] res2a_branch2a_param_0(0.167) 
I0815 11:33:51.432615  8764 net.cpp:2192] res2a_branch2b_param_0(0.167) 
I0815 11:33:51.432703  8764 net.cpp:2192] res3a_branch2a_param_0(0.168) 
I0815 11:33:51.432792  8764 net.cpp:2192] res3a_branch2b_param_0(0.167) 
I0815 11:33:51.432883  8764 net.cpp:2192] res4a_branch2a_param_0(0.169) 
I0815 11:33:51.432974  8764 net.cpp:2192] res4a_branch2b_param_0(0.168) 
I0815 11:33:51.433063  8764 net.cpp:2192] res5a_branch2a_param_0(0.17) 
I0815 11:33:51.433151  8764 net.cpp:2192] res5a_branch2b_param_0(0.169) 
I0815 11:33:51.433245  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (398388/2.86678e+06) 0.139
I0815 11:33:51.567394  8794 solver.cpp:409] Finding and applying sparsity: 0.18
I0815 11:34:12.200837  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:34:12.204712  8764 solver.cpp:312] Iteration 37000 (2.70177 iter/s, 37.0128s/100 iter), loss = 1.34163
I0815 11:34:12.204741  8764 solver.cpp:334]     Train net output #0: loss = 1.03597 (* 1 = 1.03597 loss)
I0815 11:34:12.204748  8764 sgd_solver.cpp:136] Iteration 37000, lr = 0.0076875, m = 0.9
I0815 11:34:29.559839  8764 solver.cpp:312] Iteration 37100 (5.76215 iter/s, 17.3546s/100 iter), loss = 1.10127
I0815 11:34:29.559867  8764 solver.cpp:334]     Train net output #0: loss = 0.903144 (* 1 = 0.903144 loss)
I0815 11:34:29.559872  8764 sgd_solver.cpp:136] Iteration 37100, lr = 0.00768125, m = 0.9
I0815 11:34:44.139724  8764 solver.cpp:312] Iteration 37200 (6.85896 iter/s, 14.5795s/100 iter), loss = 1.6586
I0815 11:34:44.139827  8764 solver.cpp:334]     Train net output #0: loss = 1.73729 (* 1 = 1.73729 loss)
I0815 11:34:44.139848  8764 sgd_solver.cpp:136] Iteration 37200, lr = 0.007675, m = 0.9
I0815 11:34:58.815135  8764 solver.cpp:312] Iteration 37300 (6.81431 iter/s, 14.675s/100 iter), loss = 1.84164
I0815 11:34:58.815189  8764 solver.cpp:334]     Train net output #0: loss = 1.8634 (* 1 = 1.8634 loss)
I0815 11:34:58.815198  8764 sgd_solver.cpp:136] Iteration 37300, lr = 0.00766875, m = 0.9
I0815 11:35:15.880841  8764 solver.cpp:312] Iteration 37400 (5.85987 iter/s, 17.0652s/100 iter), loss = 1.53415
I0815 11:35:15.880905  8764 solver.cpp:334]     Train net output #0: loss = 1.55228 (* 1 = 1.55228 loss)
I0815 11:35:15.880913  8764 sgd_solver.cpp:136] Iteration 37400, lr = 0.0076625, m = 0.9
I0815 11:35:33.582965  8764 solver.cpp:312] Iteration 37500 (5.6492 iter/s, 17.7016s/100 iter), loss = 1.60261
I0815 11:35:33.583029  8764 solver.cpp:334]     Train net output #0: loss = 1.87063 (* 1 = 1.87063 loss)
I0815 11:35:33.583047  8764 sgd_solver.cpp:136] Iteration 37500, lr = 0.00765625, m = 0.9
I0815 11:35:51.310339  8764 solver.cpp:312] Iteration 37600 (5.64115 iter/s, 17.7269s/100 iter), loss = 1.25786
I0815 11:35:51.310598  8764 solver.cpp:334]     Train net output #0: loss = 1.4747 (* 1 = 1.4747 loss)
I0815 11:35:51.310627  8764 sgd_solver.cpp:136] Iteration 37600, lr = 0.00765, m = 0.9
I0815 11:36:09.663689  8764 solver.cpp:312] Iteration 37700 (5.44875 iter/s, 18.3528s/100 iter), loss = 1.58454
I0815 11:36:09.663743  8764 solver.cpp:334]     Train net output #0: loss = 1.38431 (* 1 = 1.38431 loss)
I0815 11:36:09.663754  8764 sgd_solver.cpp:136] Iteration 37700, lr = 0.00764375, m = 0.9
I0815 11:36:27.896723  8764 solver.cpp:312] Iteration 37800 (5.4847 iter/s, 18.2325s/100 iter), loss = 1.58842
I0815 11:36:27.896790  8764 solver.cpp:334]     Train net output #0: loss = 1.40674 (* 1 = 1.40674 loss)
I0815 11:36:27.896795  8764 sgd_solver.cpp:136] Iteration 37800, lr = 0.0076375, m = 0.9
I0815 11:36:44.533901  8764 solver.cpp:312] Iteration 37900 (6.0108 iter/s, 16.6367s/100 iter), loss = 1.62509
I0815 11:36:44.533924  8764 solver.cpp:334]     Train net output #0: loss = 2.20072 (* 1 = 2.20072 loss)
I0815 11:36:44.533928  8764 sgd_solver.cpp:136] Iteration 37900, lr = 0.00763125, m = 0.9
I0815 11:36:59.702090  8764 solver.cpp:363] Sparsity after update:
I0815 11:36:59.706921  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:36:59.706933  8764 net.cpp:2192] conv1a_param_0(0.08) 
I0815 11:36:59.706941  8764 net.cpp:2192] conv1b_param_0(0.125) 
I0815 11:36:59.706945  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:36:59.707118  8764 net.cpp:2192] res2a_branch2a_param_0(0.177) 
I0815 11:36:59.707130  8764 net.cpp:2192] res2a_branch2b_param_0(0.174) 
I0815 11:36:59.707249  8764 net.cpp:2192] res3a_branch2a_param_0(0.179) 
I0815 11:36:59.707314  8764 net.cpp:2192] res3a_branch2b_param_0(0.177) 
I0815 11:36:59.707378  8764 net.cpp:2192] res4a_branch2a_param_0(0.18) 
I0815 11:36:59.707440  8764 net.cpp:2192] res4a_branch2b_param_0(0.179) 
I0815 11:36:59.707504  8764 net.cpp:2192] res5a_branch2a_param_0(0.18) 
I0815 11:36:59.707566  8764 net.cpp:2192] res5a_branch2b_param_0(0.18) 
I0815 11:36:59.707628  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (422333/2.86678e+06) 0.147
I0815 11:36:59.707711  8764 solver.cpp:509] Iteration 38000, Testing net (#0)
I0815 11:37:20.342571  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549118
I0815 11:37:20.342598  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.793233
I0815 11:37:20.342607  8764 solver.cpp:594]     Test net output #2: loss = 1.97336 (* 1 = 1.97336 loss)
I0815 11:37:20.342628  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.6343s
I0815 11:37:20.512686  8794 solver.cpp:409] Finding and applying sparsity: 0.19
I0815 11:37:40.956804  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:37:40.958834  8764 solver.cpp:312] Iteration 38000 (1.77232 iter/s, 56.4234s/100 iter), loss = 1.49895
I0815 11:37:40.958855  8764 solver.cpp:334]     Train net output #0: loss = 1.65869 (* 1 = 1.65869 loss)
I0815 11:37:40.958864  8764 sgd_solver.cpp:136] Iteration 38000, lr = 0.007625, m = 0.9
I0815 11:38:00.798552  8764 solver.cpp:312] Iteration 38100 (5.04054 iter/s, 19.8392s/100 iter), loss = 1.76343
I0815 11:38:00.798580  8764 solver.cpp:334]     Train net output #0: loss = 2.04413 (* 1 = 2.04413 loss)
I0815 11:38:00.798584  8764 sgd_solver.cpp:136] Iteration 38100, lr = 0.00761875, m = 0.9
I0815 11:38:15.247417  8764 solver.cpp:312] Iteration 38200 (6.92115 iter/s, 14.4485s/100 iter), loss = 1.21291
I0815 11:38:15.247563  8764 solver.cpp:334]     Train net output #0: loss = 1.48143 (* 1 = 1.48143 loss)
I0815 11:38:15.247582  8764 sgd_solver.cpp:136] Iteration 38200, lr = 0.0076125, m = 0.9
I0815 11:38:29.973095  8764 solver.cpp:312] Iteration 38300 (6.79105 iter/s, 14.7253s/100 iter), loss = 1.411
I0815 11:38:29.973163  8764 solver.cpp:334]     Train net output #0: loss = 1.4162 (* 1 = 1.4162 loss)
I0815 11:38:29.973181  8764 sgd_solver.cpp:136] Iteration 38300, lr = 0.00760625, m = 0.9
I0815 11:38:45.308043  8764 solver.cpp:312] Iteration 38400 (6.52123 iter/s, 15.3345s/100 iter), loss = 1.26071
I0815 11:38:45.312181  8764 solver.cpp:334]     Train net output #0: loss = 1.44127 (* 1 = 1.44127 loss)
I0815 11:38:45.312201  8764 sgd_solver.cpp:136] Iteration 38400, lr = 0.0076, m = 0.9
I0815 11:39:02.956161  8764 solver.cpp:312] Iteration 38500 (5.66648 iter/s, 17.6476s/100 iter), loss = 1.44182
I0815 11:39:02.956192  8764 solver.cpp:334]     Train net output #0: loss = 1.27267 (* 1 = 1.27267 loss)
I0815 11:39:02.956197  8764 sgd_solver.cpp:136] Iteration 38500, lr = 0.00759375, m = 0.9
I0815 11:39:19.250962  8764 solver.cpp:312] Iteration 38600 (6.1371 iter/s, 16.2943s/100 iter), loss = 1.45062
I0815 11:39:19.251072  8764 solver.cpp:334]     Train net output #0: loss = 1.38586 (* 1 = 1.38586 loss)
I0815 11:39:19.251092  8764 sgd_solver.cpp:136] Iteration 38600, lr = 0.0075875, m = 0.9
I0815 11:39:33.939545  8764 solver.cpp:312] Iteration 38700 (6.8082 iter/s, 14.6882s/100 iter), loss = 1.32119
I0815 11:39:33.939574  8764 solver.cpp:334]     Train net output #0: loss = 1.23376 (* 1 = 1.23376 loss)
I0815 11:39:33.939579  8764 sgd_solver.cpp:136] Iteration 38700, lr = 0.00758125, m = 0.9
I0815 11:39:49.186115  8764 solver.cpp:312] Iteration 38800 (6.55904 iter/s, 15.2461s/100 iter), loss = 1.07166
I0815 11:39:49.186143  8764 solver.cpp:334]     Train net output #0: loss = 1.33458 (* 1 = 1.33458 loss)
I0815 11:39:49.186151  8764 sgd_solver.cpp:136] Iteration 38800, lr = 0.007575, m = 0.9
I0815 11:40:06.023881  8764 solver.cpp:312] Iteration 38900 (5.9392 iter/s, 16.8373s/100 iter), loss = 1.64579
I0815 11:40:06.023985  8764 solver.cpp:334]     Train net output #0: loss = 1.47681 (* 1 = 1.47681 loss)
I0815 11:40:06.024003  8764 sgd_solver.cpp:136] Iteration 38900, lr = 0.00756875, m = 0.9
I0815 11:40:24.416576  8764 solver.cpp:363] Sparsity after update:
I0815 11:40:24.422780  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:40:24.424140  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:40:24.424165  8764 net.cpp:2192] conv1b_param_0(0.135) 
I0815 11:40:24.424180  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:40:24.424192  8764 net.cpp:2192] res2a_branch2a_param_0(0.188) 
I0815 11:40:24.424203  8764 net.cpp:2192] res2a_branch2b_param_0(0.187) 
I0815 11:40:24.424216  8764 net.cpp:2192] res3a_branch2a_param_0(0.189) 
I0815 11:40:24.424226  8764 net.cpp:2192] res3a_branch2b_param_0(0.188) 
I0815 11:40:24.424237  8764 net.cpp:2192] res4a_branch2a_param_0(0.189) 
I0815 11:40:24.424248  8764 net.cpp:2192] res4a_branch2b_param_0(0.189) 
I0815 11:40:24.424262  8764 net.cpp:2192] res5a_branch2a_param_0(0.19) 
I0815 11:40:24.424271  8764 net.cpp:2192] res5a_branch2b_param_0(0.189) 
I0815 11:40:24.424283  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (445609/2.86678e+06) 0.155
I0815 11:40:24.715215  8794 solver.cpp:409] Finding and applying sparsity: 0.2
I0815 11:40:45.977237  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:40:45.979388  8764 solver.cpp:312] Iteration 39000 (2.50285 iter/s, 39.9544s/100 iter), loss = 1.38248
I0815 11:40:45.979413  8764 solver.cpp:334]     Train net output #0: loss = 1.44988 (* 1 = 1.44988 loss)
I0815 11:40:45.979420  8764 sgd_solver.cpp:136] Iteration 39000, lr = 0.0075625, m = 0.9
I0815 11:41:03.810811  8764 solver.cpp:312] Iteration 39100 (5.60824 iter/s, 17.8309s/100 iter), loss = 1.51247
I0815 11:41:03.811035  8764 solver.cpp:334]     Train net output #0: loss = 1.42328 (* 1 = 1.42328 loss)
I0815 11:41:03.811146  8764 sgd_solver.cpp:136] Iteration 39100, lr = 0.00755625, m = 0.9
I0815 11:41:20.162564  8764 solver.cpp:312] Iteration 39200 (6.11572 iter/s, 16.3513s/100 iter), loss = 1.32689
I0815 11:41:20.162648  8764 solver.cpp:334]     Train net output #0: loss = 1.35685 (* 1 = 1.35685 loss)
I0815 11:41:20.162658  8764 sgd_solver.cpp:136] Iteration 39200, lr = 0.00755, m = 0.9
I0815 11:41:35.761705  8764 solver.cpp:312] Iteration 39300 (6.41079 iter/s, 15.5987s/100 iter), loss = 1.64366
I0815 11:41:35.761731  8764 solver.cpp:334]     Train net output #0: loss = 1.68788 (* 1 = 1.68788 loss)
I0815 11:41:35.761737  8764 sgd_solver.cpp:136] Iteration 39300, lr = 0.00754375, m = 0.9
I0815 11:41:52.704222  8764 solver.cpp:312] Iteration 39400 (5.90248 iter/s, 16.942s/100 iter), loss = 1.37474
I0815 11:41:52.706936  8764 solver.cpp:334]     Train net output #0: loss = 1.34989 (* 1 = 1.34989 loss)
I0815 11:41:52.706959  8764 sgd_solver.cpp:136] Iteration 39400, lr = 0.0075375, m = 0.9
I0815 11:42:09.371809  8764 solver.cpp:312] Iteration 39500 (5.99984 iter/s, 16.6671s/100 iter), loss = 1.14548
I0815 11:42:09.371836  8764 solver.cpp:334]     Train net output #0: loss = 1.116 (* 1 = 1.116 loss)
I0815 11:42:09.371842  8764 sgd_solver.cpp:136] Iteration 39500, lr = 0.00753125, m = 0.9
I0815 11:42:25.625625  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:42:26.828435  8764 solver.cpp:312] Iteration 39600 (5.72864 iter/s, 17.4561s/100 iter), loss = 1.21043
I0815 11:42:26.828498  8764 solver.cpp:334]     Train net output #0: loss = 1.23909 (* 1 = 1.23909 loss)
I0815 11:42:26.828516  8764 sgd_solver.cpp:136] Iteration 39600, lr = 0.007525, m = 0.9
I0815 11:42:41.835108  8764 solver.cpp:312] Iteration 39700 (6.66389 iter/s, 15.0063s/100 iter), loss = 1.74436
I0815 11:42:41.835132  8764 solver.cpp:334]     Train net output #0: loss = 1.63773 (* 1 = 1.63773 loss)
I0815 11:42:41.835137  8764 sgd_solver.cpp:136] Iteration 39700, lr = 0.00751875, m = 0.9
I0815 11:42:56.206392  8764 solver.cpp:312] Iteration 39800 (6.95851 iter/s, 14.3709s/100 iter), loss = 1.44734
I0815 11:42:56.206467  8764 solver.cpp:334]     Train net output #0: loss = 1.3769 (* 1 = 1.3769 loss)
I0815 11:42:56.206480  8764 sgd_solver.cpp:136] Iteration 39800, lr = 0.0075125, m = 0.9
I0815 11:43:10.735148  8764 solver.cpp:312] Iteration 39900 (6.88309 iter/s, 14.5284s/100 iter), loss = 1.47683
I0815 11:43:10.735199  8764 solver.cpp:334]     Train net output #0: loss = 1.70602 (* 1 = 1.70602 loss)
I0815 11:43:10.735213  8764 sgd_solver.cpp:136] Iteration 39900, lr = 0.00750625, m = 0.9
I0815 11:43:25.970715  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_40000.caffemodel
I0815 11:43:25.985014  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_40000.solverstate
I0815 11:43:25.991058  8764 solver.cpp:363] Sparsity after update:
I0815 11:43:25.993029  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:43:25.993041  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:43:25.993049  8764 net.cpp:2192] conv1b_param_0(0.146) 
I0815 11:43:25.993053  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:43:25.993057  8764 net.cpp:2192] res2a_branch2a_param_0(0.198) 
I0815 11:43:25.993060  8764 net.cpp:2192] res2a_branch2b_param_0(0.194) 
I0815 11:43:25.993063  8764 net.cpp:2192] res3a_branch2a_param_0(0.2) 
I0815 11:43:25.993067  8764 net.cpp:2192] res3a_branch2b_param_0(0.198) 
I0815 11:43:25.993070  8764 net.cpp:2192] res4a_branch2a_param_0(0.2) 
I0815 11:43:25.993074  8764 net.cpp:2192] res4a_branch2b_param_0(0.2) 
I0815 11:43:25.993077  8764 net.cpp:2192] res5a_branch2a_param_0(0.2) 
I0815 11:43:25.993098  8764 net.cpp:2192] res5a_branch2b_param_0(0.2) 
I0815 11:43:25.993113  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (469572/2.86678e+06) 0.164
I0815 11:43:25.993132  8764 solver.cpp:509] Iteration 40000, Testing net (#0)
I0815 11:43:46.576879  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.549118
I0815 11:43:46.576926  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.787704
I0815 11:43:46.576931  8764 solver.cpp:594]     Test net output #2: loss = 2.00016 (* 1 = 2.00016 loss)
I0815 11:43:46.576951  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.5833s
I0815 11:43:46.724166  8794 solver.cpp:409] Finding and applying sparsity: 0.21
I0815 11:44:07.559698  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:44:07.561921  8764 solver.cpp:312] Iteration 40000 (1.75978 iter/s, 56.8252s/100 iter), loss = 1.33954
I0815 11:44:07.561945  8764 solver.cpp:334]     Train net output #0: loss = 1.25362 (* 1 = 1.25362 loss)
I0815 11:44:07.561951  8764 sgd_solver.cpp:136] Iteration 40000, lr = 0.0075, m = 0.9
I0815 11:44:22.564882  8764 solver.cpp:312] Iteration 40100 (6.66554 iter/s, 15.0025s/100 iter), loss = 1.58491
I0815 11:44:22.564996  8764 solver.cpp:334]     Train net output #0: loss = 1.26917 (* 1 = 1.26917 loss)
I0815 11:44:22.565004  8764 sgd_solver.cpp:136] Iteration 40100, lr = 0.00749375, m = 0.9
I0815 11:44:37.559574  8764 solver.cpp:312] Iteration 40200 (6.66921 iter/s, 14.9943s/100 iter), loss = 1.27357
I0815 11:44:37.559638  8764 solver.cpp:334]     Train net output #0: loss = 1.39762 (* 1 = 1.39762 loss)
I0815 11:44:37.559655  8764 sgd_solver.cpp:136] Iteration 40200, lr = 0.0074875, m = 0.9
I0815 11:44:54.653398  8764 solver.cpp:312] Iteration 40300 (5.85023 iter/s, 17.0933s/100 iter), loss = 1.4494
I0815 11:44:54.653604  8764 solver.cpp:334]     Train net output #0: loss = 1.4241 (* 1 = 1.4241 loss)
I0815 11:44:54.653688  8764 sgd_solver.cpp:136] Iteration 40300, lr = 0.00748125, m = 0.9
I0815 11:45:10.723640  8764 solver.cpp:312] Iteration 40400 (6.22286 iter/s, 16.0698s/100 iter), loss = 1.17721
I0815 11:45:10.723667  8764 solver.cpp:334]     Train net output #0: loss = 1.13213 (* 1 = 1.13213 loss)
I0815 11:45:10.723671  8764 sgd_solver.cpp:136] Iteration 40400, lr = 0.007475, m = 0.9
I0815 11:45:26.757740  8764 solver.cpp:312] Iteration 40500 (6.23688 iter/s, 16.0337s/100 iter), loss = 1.39355
I0815 11:45:26.764202  8764 solver.cpp:334]     Train net output #0: loss = 1.34223 (* 1 = 1.34223 loss)
I0815 11:45:26.764226  8764 sgd_solver.cpp:136] Iteration 40500, lr = 0.00746875, m = 0.9
I0815 11:45:44.359653  8764 solver.cpp:312] Iteration 40600 (5.68136 iter/s, 17.6014s/100 iter), loss = 1.64247
I0815 11:45:44.359696  8764 solver.cpp:334]     Train net output #0: loss = 1.77967 (* 1 = 1.77967 loss)
I0815 11:45:44.359705  8764 sgd_solver.cpp:136] Iteration 40600, lr = 0.0074625, m = 0.9
I0815 11:46:01.303287  8764 solver.cpp:312] Iteration 40700 (5.90209 iter/s, 16.9432s/100 iter), loss = 1.07761
I0815 11:46:01.303391  8764 solver.cpp:334]     Train net output #0: loss = 1.14018 (* 1 = 1.14018 loss)
I0815 11:46:01.303406  8764 sgd_solver.cpp:136] Iteration 40700, lr = 0.00745625, m = 0.9
I0815 11:46:19.363677  8764 solver.cpp:312] Iteration 40800 (5.53713 iter/s, 18.0599s/100 iter), loss = 1.55574
I0815 11:46:19.363756  8764 solver.cpp:334]     Train net output #0: loss = 1.55386 (* 1 = 1.55386 loss)
I0815 11:46:19.363776  8764 sgd_solver.cpp:136] Iteration 40800, lr = 0.00745, m = 0.9
I0815 11:46:36.668980  8764 solver.cpp:312] Iteration 40900 (5.77875 iter/s, 17.3048s/100 iter), loss = 1.55835
I0815 11:46:36.669160  8764 solver.cpp:334]     Train net output #0: loss = 1.52459 (* 1 = 1.52459 loss)
I0815 11:46:36.669201  8764 sgd_solver.cpp:136] Iteration 40900, lr = 0.00744375, m = 0.9
I0815 11:46:52.674320  8764 solver.cpp:363] Sparsity after update:
I0815 11:46:52.688282  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:46:52.688297  8764 net.cpp:2192] conv1a_param_0(0.0933) 
I0815 11:46:52.688305  8764 net.cpp:2192] conv1b_param_0(0.156) 
I0815 11:46:52.688309  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:46:52.688325  8764 net.cpp:2192] res2a_branch2a_param_0(0.208) 
I0815 11:46:52.688336  8764 net.cpp:2192] res2a_branch2b_param_0(0.208) 
I0815 11:46:52.688344  8764 net.cpp:2192] res3a_branch2a_param_0(0.208) 
I0815 11:46:52.688354  8764 net.cpp:2192] res3a_branch2b_param_0(0.208) 
I0815 11:46:52.688361  8764 net.cpp:2192] res4a_branch2a_param_0(0.209) 
I0815 11:46:52.688369  8764 net.cpp:2192] res4a_branch2b_param_0(0.208) 
I0815 11:46:52.688376  8764 net.cpp:2192] res5a_branch2a_param_0(0.21) 
I0815 11:46:52.688385  8764 net.cpp:2192] res5a_branch2b_param_0(0.209) 
I0815 11:46:52.688392  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (492445/2.86678e+06) 0.172
I0815 11:46:52.815456  8794 solver.cpp:409] Finding and applying sparsity: 0.22
I0815 11:47:13.745668  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:47:13.747722  8764 solver.cpp:312] Iteration 41000 (2.69704 iter/s, 37.0777s/100 iter), loss = 0.992317
I0815 11:47:13.747745  8764 solver.cpp:334]     Train net output #0: loss = 1.01455 (* 1 = 1.01455 loss)
I0815 11:47:13.747753  8764 sgd_solver.cpp:136] Iteration 41000, lr = 0.0074375, m = 0.9
I0815 11:47:30.309731  8764 solver.cpp:312] Iteration 41100 (6.03809 iter/s, 16.5615s/100 iter), loss = 1.89137
I0815 11:47:30.309757  8764 solver.cpp:334]     Train net output #0: loss = 1.83254 (* 1 = 1.83254 loss)
I0815 11:47:30.309762  8764 sgd_solver.cpp:136] Iteration 41100, lr = 0.00743125, m = 0.9
I0815 11:47:47.053587  8764 solver.cpp:312] Iteration 41200 (5.97251 iter/s, 16.7434s/100 iter), loss = 1.5478
I0815 11:47:47.053683  8764 solver.cpp:334]     Train net output #0: loss = 1.43836 (* 1 = 1.43836 loss)
I0815 11:47:47.053700  8764 sgd_solver.cpp:136] Iteration 41200, lr = 0.007425, m = 0.9
I0815 11:48:04.258460  8764 solver.cpp:312] Iteration 41300 (5.81249 iter/s, 17.2043s/100 iter), loss = 1.29145
I0815 11:48:04.258544  8764 solver.cpp:334]     Train net output #0: loss = 1.4247 (* 1 = 1.4247 loss)
I0815 11:48:04.258563  8764 sgd_solver.cpp:136] Iteration 41300, lr = 0.00741875, m = 0.9
I0815 11:48:22.533788  8764 solver.cpp:312] Iteration 41400 (5.472 iter/s, 18.2748s/100 iter), loss = 1.51453
I0815 11:48:22.533840  8764 solver.cpp:334]     Train net output #0: loss = 1.44247 (* 1 = 1.44247 loss)
I0815 11:48:22.533848  8764 sgd_solver.cpp:136] Iteration 41400, lr = 0.0074125, m = 0.9
I0815 11:48:41.749469  8764 solver.cpp:312] Iteration 41500 (5.20423 iter/s, 19.2151s/100 iter), loss = 1.3972
I0815 11:48:41.749532  8764 solver.cpp:334]     Train net output #0: loss = 1.1877 (* 1 = 1.1877 loss)
I0815 11:48:41.749547  8764 sgd_solver.cpp:136] Iteration 41500, lr = 0.00740625, m = 0.9
I0815 11:49:03.237642  8764 solver.cpp:312] Iteration 41600 (4.65386 iter/s, 21.4875s/100 iter), loss = 1.49586
I0815 11:49:03.237778  8764 solver.cpp:334]     Train net output #0: loss = 1.11073 (* 1 = 1.11073 loss)
I0815 11:49:03.237793  8764 sgd_solver.cpp:136] Iteration 41600, lr = 0.0074, m = 0.9
I0815 11:49:19.571549  8764 solver.cpp:312] Iteration 41700 (6.1224 iter/s, 16.3335s/100 iter), loss = 1.2191
I0815 11:49:19.571614  8764 solver.cpp:334]     Train net output #0: loss = 1.11921 (* 1 = 1.11921 loss)
I0815 11:49:19.571635  8764 sgd_solver.cpp:136] Iteration 41700, lr = 0.00739375, m = 0.9
I0815 11:49:36.373278  8764 solver.cpp:312] Iteration 41800 (5.95193 iter/s, 16.8013s/100 iter), loss = 1.35527
I0815 11:49:36.373332  8764 solver.cpp:334]     Train net output #0: loss = 1.04712 (* 1 = 1.04712 loss)
I0815 11:49:36.373338  8764 sgd_solver.cpp:136] Iteration 41800, lr = 0.0073875, m = 0.9
I0815 11:49:52.429368  8764 solver.cpp:312] Iteration 41900 (6.22834 iter/s, 16.0556s/100 iter), loss = 1.98607
I0815 11:49:52.429395  8764 solver.cpp:334]     Train net output #0: loss = 2.54703 (* 1 = 2.54703 loss)
I0815 11:49:52.429400  8764 sgd_solver.cpp:136] Iteration 41900, lr = 0.00738125, m = 0.9
I0815 11:50:07.508283  8764 solver.cpp:363] Sparsity after update:
I0815 11:50:07.522851  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:50:07.522902  8764 net.cpp:2192] conv1a_param_0(0.107) 
I0815 11:50:07.522933  8764 net.cpp:2192] conv1b_param_0(0.156) 
I0815 11:50:07.522946  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:50:07.522958  8764 net.cpp:2192] res2a_branch2a_param_0(0.219) 
I0815 11:50:07.522970  8764 net.cpp:2192] res2a_branch2b_param_0(0.215) 
I0815 11:50:07.522984  8764 net.cpp:2192] res3a_branch2a_param_0(0.219) 
I0815 11:50:07.522996  8764 net.cpp:2192] res3a_branch2b_param_0(0.219) 
I0815 11:50:07.523005  8764 net.cpp:2192] res4a_branch2a_param_0(0.22) 
I0815 11:50:07.523017  8764 net.cpp:2192] res4a_branch2b_param_0(0.219) 
I0815 11:50:07.523026  8764 net.cpp:2192] res5a_branch2a_param_0(0.22) 
I0815 11:50:07.523041  8764 net.cpp:2192] res5a_branch2b_param_0(0.22) 
I0815 11:50:07.523048  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (516422/2.86678e+06) 0.18
I0815 11:50:07.523088  8764 solver.cpp:509] Iteration 42000, Testing net (#0)
I0815 11:50:31.705016  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571
I0815 11:50:31.705042  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805703
I0815 11:50:31.705049  8764 solver.cpp:594]     Test net output #2: loss = 1.8802 (* 1 = 1.8802 loss)
I0815 11:50:31.705096  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.1813s
I0815 11:50:31.853641  8794 solver.cpp:409] Finding and applying sparsity: 0.23
I0815 11:50:52.637635  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:50:52.639650  8764 solver.cpp:312] Iteration 42000 (1.66089 iter/s, 60.2086s/100 iter), loss = 1.27541
I0815 11:50:52.639673  8764 solver.cpp:334]     Train net output #0: loss = 1.28959 (* 1 = 1.28959 loss)
I0815 11:50:52.639683  8764 sgd_solver.cpp:136] Iteration 42000, lr = 0.007375, m = 0.9
I0815 11:51:07.841305  8764 solver.cpp:312] Iteration 42100 (6.57842 iter/s, 15.2012s/100 iter), loss = 1.00871
I0815 11:51:07.841332  8764 solver.cpp:334]     Train net output #0: loss = 1.25629 (* 1 = 1.25629 loss)
I0815 11:51:07.841338  8764 sgd_solver.cpp:136] Iteration 42100, lr = 0.00736875, m = 0.9
I0815 11:51:23.127192  8764 solver.cpp:312] Iteration 42200 (6.54217 iter/s, 15.2855s/100 iter), loss = 1.60824
I0815 11:51:23.127244  8764 solver.cpp:334]     Train net output #0: loss = 1.32156 (* 1 = 1.32156 loss)
I0815 11:51:23.127250  8764 sgd_solver.cpp:136] Iteration 42200, lr = 0.0073625, m = 0.9
I0815 11:51:39.580008  8764 solver.cpp:312] Iteration 42300 (6.07816 iter/s, 16.4524s/100 iter), loss = 1.12307
I0815 11:51:39.580041  8764 solver.cpp:334]     Train net output #0: loss = 1.27288 (* 1 = 1.27288 loss)
I0815 11:51:39.580047  8764 sgd_solver.cpp:136] Iteration 42300, lr = 0.00735625, m = 0.9
I0815 11:51:57.247968  8764 solver.cpp:312] Iteration 42400 (5.66012 iter/s, 17.6675s/100 iter), loss = 1.77322
I0815 11:51:57.248028  8764 solver.cpp:334]     Train net output #0: loss = 1.85288 (* 1 = 1.85288 loss)
I0815 11:51:57.248036  8764 sgd_solver.cpp:136] Iteration 42400, lr = 0.00735, m = 0.9
I0815 11:52:13.072741  8764 solver.cpp:312] Iteration 42500 (6.31938 iter/s, 15.8243s/100 iter), loss = 1.26093
I0815 11:52:13.072818  8764 solver.cpp:334]     Train net output #0: loss = 1.0852 (* 1 = 1.0852 loss)
I0815 11:52:13.072837  8764 sgd_solver.cpp:136] Iteration 42500, lr = 0.00734375, m = 0.9
I0815 11:52:27.530305  8764 solver.cpp:312] Iteration 42600 (6.91699 iter/s, 14.4572s/100 iter), loss = 1.31173
I0815 11:52:27.530365  8764 solver.cpp:334]     Train net output #0: loss = 1.06282 (* 1 = 1.06282 loss)
I0815 11:52:27.530371  8764 sgd_solver.cpp:136] Iteration 42600, lr = 0.0073375, m = 0.9
I0815 11:52:42.235016  8764 solver.cpp:312] Iteration 42700 (6.80073 iter/s, 14.7043s/100 iter), loss = 1.56785
I0815 11:52:42.235041  8764 solver.cpp:334]     Train net output #0: loss = 1.66233 (* 1 = 1.66233 loss)
I0815 11:52:42.235047  8764 sgd_solver.cpp:136] Iteration 42700, lr = 0.00733125, m = 0.9
I0815 11:52:59.723587  8764 solver.cpp:312] Iteration 42800 (5.71818 iter/s, 17.4881s/100 iter), loss = 1.04003
I0815 11:52:59.723685  8764 solver.cpp:334]     Train net output #0: loss = 0.698877 (* 1 = 0.698877 loss)
I0815 11:52:59.723704  8764 sgd_solver.cpp:136] Iteration 42800, lr = 0.007325, m = 0.9
I0815 11:53:15.506523  8764 solver.cpp:312] Iteration 42900 (6.33613 iter/s, 15.7825s/100 iter), loss = 1.32275
I0815 11:53:15.506585  8764 solver.cpp:334]     Train net output #0: loss = 1.43373 (* 1 = 1.43373 loss)
I0815 11:53:15.506608  8764 sgd_solver.cpp:136] Iteration 42900, lr = 0.00731875, m = 0.9
I0815 11:53:31.004928  8764 solver.cpp:363] Sparsity after update:
I0815 11:53:31.018941  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:53:31.018988  8764 net.cpp:2192] conv1a_param_0(0.107) 
I0815 11:53:31.019006  8764 net.cpp:2192] conv1b_param_0(0.167) 
I0815 11:53:31.019019  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:53:31.019032  8764 net.cpp:2192] res2a_branch2a_param_0(0.229) 
I0815 11:53:31.019044  8764 net.cpp:2192] res2a_branch2b_param_0(0.229) 
I0815 11:53:31.019057  8764 net.cpp:2192] res3a_branch2a_param_0(0.229) 
I0815 11:53:31.019068  8764 net.cpp:2192] res3a_branch2b_param_0(0.229) 
I0815 11:53:31.019080  8764 net.cpp:2192] res4a_branch2a_param_0(0.229) 
I0815 11:53:31.019091  8764 net.cpp:2192] res4a_branch2b_param_0(0.229) 
I0815 11:53:31.019109  8764 net.cpp:2192] res5a_branch2a_param_0(0.23) 
I0815 11:53:31.019122  8764 net.cpp:2192] res5a_branch2b_param_0(0.229) 
I0815 11:53:31.019134  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (539669/2.86678e+06) 0.188
I0815 11:53:31.152777  8794 solver.cpp:409] Finding and applying sparsity: 0.24
I0815 11:53:53.399744  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:53:53.401758  8764 solver.cpp:312] Iteration 43000 (2.63893 iter/s, 37.8942s/100 iter), loss = 1.28148
I0815 11:53:53.401777  8764 solver.cpp:334]     Train net output #0: loss = 1.01363 (* 1 = 1.01363 loss)
I0815 11:53:53.401783  8764 sgd_solver.cpp:136] Iteration 43000, lr = 0.0073125, m = 0.9
I0815 11:54:09.859926  8764 solver.cpp:312] Iteration 43100 (6.07618 iter/s, 16.4577s/100 iter), loss = 1.09728
I0815 11:54:09.859997  8764 solver.cpp:334]     Train net output #0: loss = 1.28057 (* 1 = 1.28057 loss)
I0815 11:54:09.860007  8764 sgd_solver.cpp:136] Iteration 43100, lr = 0.00730625, m = 0.9
I0815 11:54:26.898460  8764 solver.cpp:312] Iteration 43200 (5.86922 iter/s, 17.0381s/100 iter), loss = 1.57557
I0815 11:54:26.898504  8764 solver.cpp:334]     Train net output #0: loss = 1.90334 (* 1 = 1.90334 loss)
I0815 11:54:26.898512  8764 sgd_solver.cpp:136] Iteration 43200, lr = 0.0073, m = 0.9
I0815 11:54:44.432389  8764 solver.cpp:312] Iteration 43300 (5.70339 iter/s, 17.5334s/100 iter), loss = 1.02392
I0815 11:54:44.432456  8764 solver.cpp:334]     Train net output #0: loss = 1.20143 (* 1 = 1.20143 loss)
I0815 11:54:44.432462  8764 sgd_solver.cpp:136] Iteration 43300, lr = 0.00729375, m = 0.9
I0815 11:54:59.163769  8764 solver.cpp:312] Iteration 43400 (6.78842 iter/s, 14.731s/100 iter), loss = 1.28839
I0815 11:54:59.163803  8764 solver.cpp:334]     Train net output #0: loss = 1.4176 (* 1 = 1.4176 loss)
I0815 11:54:59.163810  8764 sgd_solver.cpp:136] Iteration 43400, lr = 0.0072875, m = 0.9
I0815 11:55:16.026682  8764 solver.cpp:312] Iteration 43500 (5.93034 iter/s, 16.8624s/100 iter), loss = 1.52607
I0815 11:55:16.026780  8764 solver.cpp:334]     Train net output #0: loss = 1.57361 (* 1 = 1.57361 loss)
I0815 11:55:16.026799  8764 sgd_solver.cpp:136] Iteration 43500, lr = 0.00728125, m = 0.9
I0815 11:55:34.197878  8764 solver.cpp:312] Iteration 43600 (5.50337 iter/s, 18.1707s/100 iter), loss = 1.59554
I0815 11:55:34.197971  8764 solver.cpp:334]     Train net output #0: loss = 1.33383 (* 1 = 1.33383 loss)
I0815 11:55:34.198009  8764 sgd_solver.cpp:136] Iteration 43600, lr = 0.007275, m = 0.9
I0815 11:55:50.861923  8764 solver.cpp:312] Iteration 43700 (6.00111 iter/s, 16.6636s/100 iter), loss = 1.92091
I0815 11:55:50.862023  8764 solver.cpp:334]     Train net output #0: loss = 1.59046 (* 1 = 1.59046 loss)
I0815 11:55:50.862041  8764 sgd_solver.cpp:136] Iteration 43700, lr = 0.00726875, m = 0.9
I0815 11:56:07.861461  8764 solver.cpp:312] Iteration 43800 (5.88268 iter/s, 16.9991s/100 iter), loss = 1.42534
I0815 11:56:07.861505  8764 solver.cpp:334]     Train net output #0: loss = 1.89759 (* 1 = 1.89759 loss)
I0815 11:56:07.861519  8764 sgd_solver.cpp:136] Iteration 43800, lr = 0.0072625, m = 0.9
I0815 11:56:24.440541  8764 solver.cpp:312] Iteration 43900 (6.03186 iter/s, 16.5786s/100 iter), loss = 1.72971
I0815 11:56:24.440632  8764 solver.cpp:334]     Train net output #0: loss = 2.16681 (* 1 = 2.16681 loss)
I0815 11:56:24.440652  8764 sgd_solver.cpp:136] Iteration 43900, lr = 0.00725625, m = 0.9
I0815 11:56:39.350481  8764 solver.cpp:363] Sparsity after update:
I0815 11:56:39.354569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 11:56:39.354600  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 11:56:39.354620  8764 net.cpp:2192] conv1b_param_0(0.177) 
I0815 11:56:39.354632  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 11:56:39.354645  8764 net.cpp:2192] res2a_branch2a_param_0(0.24) 
I0815 11:56:39.354657  8764 net.cpp:2192] res2a_branch2b_param_0(0.236) 
I0815 11:56:39.354668  8764 net.cpp:2192] res3a_branch2a_param_0(0.24) 
I0815 11:56:39.354681  8764 net.cpp:2192] res3a_branch2b_param_0(0.24) 
I0815 11:56:39.354697  8764 net.cpp:2192] res4a_branch2a_param_0(0.24) 
I0815 11:56:39.354709  8764 net.cpp:2192] res4a_branch2b_param_0(0.24) 
I0815 11:56:39.354720  8764 net.cpp:2192] res5a_branch2a_param_0(0.24) 
I0815 11:56:39.354732  8764 net.cpp:2192] res5a_branch2b_param_0(0.24) 
I0815 11:56:39.354744  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (563662/2.86678e+06) 0.197
I0815 11:56:39.354763  8764 solver.cpp:509] Iteration 44000, Testing net (#0)
I0815 11:56:44.103063  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 11:57:00.994954  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.555882
I0815 11:57:00.995079  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.790762
I0815 11:57:00.995087  8764 solver.cpp:594]     Test net output #2: loss = 1.99448 (* 1 = 1.99448 loss)
I0815 11:57:00.995108  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.6397s
I0815 11:57:01.143477  8794 solver.cpp:409] Finding and applying sparsity: 0.25
I0815 11:57:22.724108  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 11:57:22.726094  8764 solver.cpp:312] Iteration 44000 (1.71574 iter/s, 58.284s/100 iter), loss = 1.45532
I0815 11:57:22.726109  8764 solver.cpp:334]     Train net output #0: loss = 1.69752 (* 1 = 1.69752 loss)
I0815 11:57:22.726114  8764 sgd_solver.cpp:136] Iteration 44000, lr = 0.00725, m = 0.9
I0815 11:57:38.666327  8764 solver.cpp:312] Iteration 44100 (6.27361 iter/s, 15.9398s/100 iter), loss = 1.26984
I0815 11:57:38.666404  8764 solver.cpp:334]     Train net output #0: loss = 1.48498 (* 1 = 1.48498 loss)
I0815 11:57:38.666422  8764 sgd_solver.cpp:136] Iteration 44100, lr = 0.00724375, m = 0.9
I0815 11:57:53.387658  8764 solver.cpp:312] Iteration 44200 (6.79305 iter/s, 14.7209s/100 iter), loss = 1.3449
I0815 11:57:53.387686  8764 solver.cpp:334]     Train net output #0: loss = 1.54304 (* 1 = 1.54304 loss)
I0815 11:57:53.387692  8764 sgd_solver.cpp:136] Iteration 44200, lr = 0.0072375, m = 0.9
I0815 11:58:07.885869  8764 solver.cpp:312] Iteration 44300 (6.8976 iter/s, 14.4978s/100 iter), loss = 1.61303
I0815 11:58:07.885939  8764 solver.cpp:334]     Train net output #0: loss = 1.28602 (* 1 = 1.28602 loss)
I0815 11:58:07.885957  8764 sgd_solver.cpp:136] Iteration 44300, lr = 0.00723125, m = 0.9
I0815 11:58:25.124315  8764 solver.cpp:312] Iteration 44400 (5.80115 iter/s, 17.238s/100 iter), loss = 1.42898
I0815 11:58:25.124402  8764 solver.cpp:334]     Train net output #0: loss = 1.55798 (* 1 = 1.55798 loss)
I0815 11:58:25.124415  8764 sgd_solver.cpp:136] Iteration 44400, lr = 0.007225, m = 0.9
I0815 11:58:42.120970  8764 solver.cpp:312] Iteration 44500 (5.88367 iter/s, 16.9962s/100 iter), loss = 1.67365
I0815 11:58:42.120999  8764 solver.cpp:334]     Train net output #0: loss = 1.4465 (* 1 = 1.4465 loss)
I0815 11:58:42.121006  8764 sgd_solver.cpp:136] Iteration 44500, lr = 0.00721875, m = 0.9
I0815 11:58:59.736941  8764 solver.cpp:312] Iteration 44600 (5.67682 iter/s, 17.6155s/100 iter), loss = 1.72893
I0815 11:58:59.737001  8764 solver.cpp:334]     Train net output #0: loss = 1.53836 (* 1 = 1.53836 loss)
I0815 11:58:59.737007  8764 sgd_solver.cpp:136] Iteration 44600, lr = 0.0072125, m = 0.9
I0815 11:59:14.823428  8726 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 11:59:16.414969  8764 solver.cpp:312] Iteration 44700 (5.99608 iter/s, 16.6776s/100 iter), loss = 1.29547
I0815 11:59:16.414995  8764 solver.cpp:334]     Train net output #0: loss = 1.35022 (* 1 = 1.35022 loss)
I0815 11:59:16.414999  8764 sgd_solver.cpp:136] Iteration 44700, lr = 0.00720625, m = 0.9
I0815 11:59:31.003136  8764 solver.cpp:312] Iteration 44800 (6.85506 iter/s, 14.5878s/100 iter), loss = 1.78236
I0815 11:59:31.003309  8764 solver.cpp:334]     Train net output #0: loss = 1.87248 (* 1 = 1.87248 loss)
I0815 11:59:31.003330  8764 sgd_solver.cpp:136] Iteration 44800, lr = 0.0072, m = 0.9
I0815 11:59:46.616648  8764 solver.cpp:312] Iteration 44900 (6.40489 iter/s, 15.6131s/100 iter), loss = 1.4337
I0815 11:59:46.616677  8764 solver.cpp:334]     Train net output #0: loss = 1.78207 (* 1 = 1.78207 loss)
I0815 11:59:46.616683  8764 sgd_solver.cpp:136] Iteration 44900, lr = 0.00719375, m = 0.9
I0815 12:00:01.080303  8764 solver.cpp:363] Sparsity after update:
I0815 12:00:01.094779  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:00:01.094938  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 12:00:01.095036  8764 net.cpp:2192] conv1b_param_0(0.188) 
I0815 12:00:01.095130  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:00:01.095221  8764 net.cpp:2192] res2a_branch2a_param_0(0.25) 
I0815 12:00:01.095310  8764 net.cpp:2192] res2a_branch2b_param_0(0.25) 
I0815 12:00:01.095401  8764 net.cpp:2192] res3a_branch2a_param_0(0.25) 
I0815 12:00:01.095489  8764 net.cpp:2192] res3a_branch2b_param_0(0.25) 
I0815 12:00:01.095577  8764 net.cpp:2192] res4a_branch2a_param_0(0.25) 
I0815 12:00:01.095665  8764 net.cpp:2192] res4a_branch2b_param_0(0.25) 
I0815 12:00:01.095755  8764 net.cpp:2192] res5a_branch2a_param_0(0.25) 
I0815 12:00:01.095844  8764 net.cpp:2192] res5a_branch2b_param_0(0.25) 
I0815 12:00:01.095934  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (588214/2.86678e+06) 0.205
I0815 12:00:01.251045  8794 solver.cpp:409] Finding and applying sparsity: 0.26
I0815 12:00:22.645915  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:00:22.647920  8764 solver.cpp:312] Iteration 45000 (2.77544 iter/s, 36.0303s/100 iter), loss = 1.56589
I0815 12:00:22.647946  8764 solver.cpp:334]     Train net output #0: loss = 1.8113 (* 1 = 1.8113 loss)
I0815 12:00:22.647956  8764 sgd_solver.cpp:136] Iteration 45000, lr = 0.0071875, m = 0.9
I0815 12:00:37.819586  8764 solver.cpp:312] Iteration 45100 (6.59142 iter/s, 15.1712s/100 iter), loss = 1.5246
I0815 12:00:37.819845  8764 solver.cpp:334]     Train net output #0: loss = 1.83433 (* 1 = 1.83433 loss)
I0815 12:00:37.819953  8764 sgd_solver.cpp:136] Iteration 45100, lr = 0.00718125, m = 0.9
I0815 12:00:53.750051  8764 solver.cpp:312] Iteration 45200 (6.27746 iter/s, 15.93s/100 iter), loss = 1.55649
I0815 12:00:53.750102  8764 solver.cpp:334]     Train net output #0: loss = 1.27667 (* 1 = 1.27667 loss)
I0815 12:00:53.750115  8764 sgd_solver.cpp:136] Iteration 45200, lr = 0.007175, m = 0.9
I0815 12:01:09.099642  8764 solver.cpp:312] Iteration 45300 (6.51501 iter/s, 15.3492s/100 iter), loss = 1.46779
I0815 12:01:09.099716  8764 solver.cpp:334]     Train net output #0: loss = 1.38662 (* 1 = 1.38662 loss)
I0815 12:01:09.099728  8764 sgd_solver.cpp:136] Iteration 45300, lr = 0.00716875, m = 0.9
I0815 12:01:24.613761  8764 solver.cpp:312] Iteration 45400 (6.44592 iter/s, 15.5137s/100 iter), loss = 1.49828
I0815 12:01:24.613790  8764 solver.cpp:334]     Train net output #0: loss = 1.49396 (* 1 = 1.49396 loss)
I0815 12:01:24.613793  8764 sgd_solver.cpp:136] Iteration 45400, lr = 0.0071625, m = 0.9
I0815 12:01:40.074525  8764 solver.cpp:312] Iteration 45500 (6.46816 iter/s, 15.4603s/100 iter), loss = 1.60593
I0815 12:01:40.074600  8764 solver.cpp:334]     Train net output #0: loss = 1.9962 (* 1 = 1.9962 loss)
I0815 12:01:40.074614  8764 sgd_solver.cpp:136] Iteration 45500, lr = 0.00715625, m = 0.9
I0815 12:01:55.151181  8764 solver.cpp:312] Iteration 45600 (6.63295 iter/s, 15.0762s/100 iter), loss = 1.1566
I0815 12:01:55.151209  8764 solver.cpp:334]     Train net output #0: loss = 1.07997 (* 1 = 1.07997 loss)
I0815 12:01:55.151214  8764 sgd_solver.cpp:136] Iteration 45600, lr = 0.00715, m = 0.9
I0815 12:02:10.114506  8764 solver.cpp:312] Iteration 45700 (6.68319 iter/s, 14.9629s/100 iter), loss = 1.15881
I0815 12:02:10.116245  8764 solver.cpp:334]     Train net output #0: loss = 1.461 (* 1 = 1.461 loss)
I0815 12:02:10.116261  8764 sgd_solver.cpp:136] Iteration 45700, lr = 0.00714375, m = 0.9
I0815 12:02:27.248343  8764 solver.cpp:312] Iteration 45800 (5.83657 iter/s, 17.1334s/100 iter), loss = 1.54467
I0815 12:02:27.248591  8764 solver.cpp:334]     Train net output #0: loss = 1.6908 (* 1 = 1.6908 loss)
I0815 12:02:27.248716  8764 sgd_solver.cpp:136] Iteration 45800, lr = 0.0071375, m = 0.9
I0815 12:02:44.970896  8764 solver.cpp:312] Iteration 45900 (5.64268 iter/s, 17.7221s/100 iter), loss = 1.25274
I0815 12:02:44.971011  8764 solver.cpp:334]     Train net output #0: loss = 1.21828 (* 1 = 1.21828 loss)
I0815 12:02:44.971031  8764 sgd_solver.cpp:136] Iteration 45900, lr = 0.00713125, m = 0.9
I0815 12:03:03.341490  8764 solver.cpp:363] Sparsity after update:
I0815 12:03:03.348592  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:03:03.348608  8764 net.cpp:2192] conv1a_param_0(0.12) 
I0815 12:03:03.348615  8764 net.cpp:2192] conv1b_param_0(0.188) 
I0815 12:03:03.348619  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:03:03.348623  8764 net.cpp:2192] res2a_branch2a_param_0(0.257) 
I0815 12:03:03.348625  8764 net.cpp:2192] res2a_branch2b_param_0(0.257) 
I0815 12:03:03.348628  8764 net.cpp:2192] res3a_branch2a_param_0(0.259) 
I0815 12:03:03.348634  8764 net.cpp:2192] res3a_branch2b_param_0(0.257) 
I0815 12:03:03.348636  8764 net.cpp:2192] res4a_branch2a_param_0(0.26) 
I0815 12:03:03.348639  8764 net.cpp:2192] res4a_branch2b_param_0(0.259) 
I0815 12:03:03.348642  8764 net.cpp:2192] res5a_branch2a_param_0(0.26) 
I0815 12:03:03.348646  8764 net.cpp:2192] res5a_branch2b_param_0(0.26) 
I0815 12:03:03.348649  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (610795/2.86678e+06) 0.213
I0815 12:03:03.348686  8764 solver.cpp:509] Iteration 46000, Testing net (#0)
I0815 12:03:25.667194  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.563118
I0815 12:03:25.667282  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.797527
I0815 12:03:25.667291  8764 solver.cpp:594]     Test net output #2: loss = 1.92302 (* 1 = 1.92302 loss)
I0815 12:03:25.667312  8764 solver.cpp:264] [MultiGPU] Tests completed in 22.318s
I0815 12:03:25.824658  8794 solver.cpp:409] Finding and applying sparsity: 0.27
I0815 12:03:47.631114  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:03:47.633178  8764 solver.cpp:312] Iteration 46000 (1.5959 iter/s, 62.6606s/100 iter), loss = 1.62186
I0815 12:03:47.633196  8764 solver.cpp:334]     Train net output #0: loss = 1.45711 (* 1 = 1.45711 loss)
I0815 12:03:47.633205  8764 sgd_solver.cpp:136] Iteration 46000, lr = 0.007125, m = 0.9
I0815 12:04:05.362973  8764 solver.cpp:312] Iteration 46100 (5.64038 iter/s, 17.7293s/100 iter), loss = 1.52139
I0815 12:04:05.363056  8764 solver.cpp:334]     Train net output #0: loss = 1.42839 (* 1 = 1.42839 loss)
I0815 12:04:05.363071  8764 sgd_solver.cpp:136] Iteration 46100, lr = 0.00711875, m = 0.9
I0815 12:04:22.494807  8764 solver.cpp:312] Iteration 46200 (5.83725 iter/s, 17.1313s/100 iter), loss = 0.994209
I0815 12:04:22.494833  8764 solver.cpp:334]     Train net output #0: loss = 0.684631 (* 1 = 0.684631 loss)
I0815 12:04:22.494839  8764 sgd_solver.cpp:136] Iteration 46200, lr = 0.0071125, m = 0.9
I0815 12:04:37.270679  8764 solver.cpp:312] Iteration 46300 (6.76798 iter/s, 14.7755s/100 iter), loss = 1.18022
I0815 12:04:37.270730  8764 solver.cpp:334]     Train net output #0: loss = 1.06926 (* 1 = 1.06926 loss)
I0815 12:04:37.270736  8764 sgd_solver.cpp:136] Iteration 46300, lr = 0.00710625, m = 0.9
I0815 12:04:52.719944  8764 solver.cpp:312] Iteration 46400 (6.47298 iter/s, 15.4488s/100 iter), loss = 1.37574
I0815 12:04:52.719996  8764 solver.cpp:334]     Train net output #0: loss = 1.06927 (* 1 = 1.06927 loss)
I0815 12:04:52.720016  8764 sgd_solver.cpp:136] Iteration 46400, lr = 0.0071, m = 0.9
I0815 12:05:08.542891  8764 solver.cpp:312] Iteration 46500 (6.32011 iter/s, 15.8225s/100 iter), loss = 1.55071
I0815 12:05:08.542992  8764 solver.cpp:334]     Train net output #0: loss = 1.73957 (* 1 = 1.73957 loss)
I0815 12:05:08.543007  8764 sgd_solver.cpp:136] Iteration 46500, lr = 0.00709375, m = 0.9
I0815 12:05:26.504575  8764 solver.cpp:312] Iteration 46600 (5.56756 iter/s, 17.9612s/100 iter), loss = 1.24983
I0815 12:05:26.504602  8764 solver.cpp:334]     Train net output #0: loss = 0.944877 (* 1 = 0.944877 loss)
I0815 12:05:26.504609  8764 sgd_solver.cpp:136] Iteration 46600, lr = 0.0070875, m = 0.9
I0815 12:05:41.608597  8764 solver.cpp:312] Iteration 46700 (6.62094 iter/s, 15.1036s/100 iter), loss = 1.85067
I0815 12:05:41.608657  8764 solver.cpp:334]     Train net output #0: loss = 1.5561 (* 1 = 1.5561 loss)
I0815 12:05:41.608664  8764 sgd_solver.cpp:136] Iteration 46700, lr = 0.00708125, m = 0.9
I0815 12:05:56.106071  8764 solver.cpp:312] Iteration 46800 (6.89794 iter/s, 14.4971s/100 iter), loss = 1.5578
I0815 12:05:56.106099  8764 solver.cpp:334]     Train net output #0: loss = 1.83319 (* 1 = 1.83319 loss)
I0815 12:05:56.106106  8764 sgd_solver.cpp:136] Iteration 46800, lr = 0.007075, m = 0.9
I0815 12:06:10.689889  8764 solver.cpp:312] Iteration 46900 (6.85711 iter/s, 14.5834s/100 iter), loss = 1.61098
I0815 12:06:10.689914  8764 solver.cpp:334]     Train net output #0: loss = 1.77921 (* 1 = 1.77921 loss)
I0815 12:06:10.689919  8764 sgd_solver.cpp:136] Iteration 46900, lr = 0.00706875, m = 0.9
I0815 12:06:26.741771  8764 solver.cpp:363] Sparsity after update:
I0815 12:06:26.754218  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:06:26.754250  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:06:26.754266  8764 net.cpp:2192] conv1b_param_0(0.198) 
I0815 12:06:26.754274  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:06:26.754284  8764 net.cpp:2192] res2a_branch2a_param_0(0.267) 
I0815 12:06:26.754293  8764 net.cpp:2192] res2a_branch2b_param_0(0.264) 
I0815 12:06:26.754302  8764 net.cpp:2192] res3a_branch2a_param_0(0.269) 
I0815 12:06:26.754312  8764 net.cpp:2192] res3a_branch2b_param_0(0.267) 
I0815 12:06:26.754320  8764 net.cpp:2192] res4a_branch2a_param_0(0.27) 
I0815 12:06:26.754329  8764 net.cpp:2192] res4a_branch2b_param_0(0.269) 
I0815 12:06:26.754338  8764 net.cpp:2192] res5a_branch2a_param_0(0.27) 
I0815 12:06:26.754346  8764 net.cpp:2192] res5a_branch2b_param_0(0.27) 
I0815 12:06:26.754356  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (634803/2.86678e+06) 0.221
I0815 12:06:26.899572  8794 solver.cpp:409] Finding and applying sparsity: 0.28
I0815 12:06:48.919200  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:06:48.921195  8764 solver.cpp:312] Iteration 47000 (2.61573 iter/s, 38.2303s/100 iter), loss = 1.54641
I0815 12:06:48.921217  8764 solver.cpp:334]     Train net output #0: loss = 1.16724 (* 1 = 1.16724 loss)
I0815 12:06:48.921226  8764 sgd_solver.cpp:136] Iteration 47000, lr = 0.0070625, m = 0.9
I0815 12:07:03.850061  8764 solver.cpp:312] Iteration 47100 (6.69862 iter/s, 14.9284s/100 iter), loss = 1.71577
I0815 12:07:03.852161  8764 solver.cpp:334]     Train net output #0: loss = 1.92475 (* 1 = 1.92475 loss)
I0815 12:07:03.852169  8764 sgd_solver.cpp:136] Iteration 47100, lr = 0.00705625, m = 0.9
I0815 12:07:20.646836  8764 solver.cpp:312] Iteration 47200 (5.95369 iter/s, 16.7963s/100 iter), loss = 1.53251
I0815 12:07:20.646862  8764 solver.cpp:334]     Train net output #0: loss = 1.37507 (* 1 = 1.37507 loss)
I0815 12:07:20.646867  8764 sgd_solver.cpp:136] Iteration 47200, lr = 0.00705, m = 0.9
I0815 12:07:37.345934  8764 solver.cpp:312] Iteration 47300 (5.98851 iter/s, 16.6986s/100 iter), loss = 1.16392
I0815 12:07:37.346020  8764 solver.cpp:334]     Train net output #0: loss = 1.206 (* 1 = 1.206 loss)
I0815 12:07:37.346035  8764 sgd_solver.cpp:136] Iteration 47300, lr = 0.00704375, m = 0.9
I0815 12:07:55.026607  8764 solver.cpp:312] Iteration 47400 (5.65605 iter/s, 17.6802s/100 iter), loss = 1.21349
I0815 12:07:55.026630  8764 solver.cpp:334]     Train net output #0: loss = 1.22781 (* 1 = 1.22781 loss)
I0815 12:07:55.026636  8764 sgd_solver.cpp:136] Iteration 47400, lr = 0.0070375, m = 0.9
I0815 12:08:11.181463  8764 solver.cpp:312] Iteration 47500 (6.19026 iter/s, 16.1544s/100 iter), loss = 1.30624
I0815 12:08:11.181545  8764 solver.cpp:334]     Train net output #0: loss = 1.41948 (* 1 = 1.41948 loss)
I0815 12:08:11.181550  8764 sgd_solver.cpp:136] Iteration 47500, lr = 0.00703125, m = 0.9
I0815 12:08:27.219866  8764 solver.cpp:312] Iteration 47600 (6.23521 iter/s, 16.038s/100 iter), loss = 1.82261
I0815 12:08:27.219894  8764 solver.cpp:334]     Train net output #0: loss = 1.90741 (* 1 = 1.90741 loss)
I0815 12:08:27.219898  8764 sgd_solver.cpp:136] Iteration 47600, lr = 0.007025, m = 0.9
I0815 12:08:44.710924  8764 solver.cpp:312] Iteration 47700 (5.71737 iter/s, 17.4906s/100 iter), loss = 1.38151
I0815 12:08:44.711052  8764 solver.cpp:334]     Train net output #0: loss = 1.37571 (* 1 = 1.37571 loss)
I0815 12:08:44.711067  8764 sgd_solver.cpp:136] Iteration 47700, lr = 0.00701875, m = 0.9
I0815 12:09:00.969638  8764 solver.cpp:312] Iteration 47800 (6.15072 iter/s, 16.2583s/100 iter), loss = 1.02217
I0815 12:09:00.969704  8764 solver.cpp:334]     Train net output #0: loss = 0.909984 (* 1 = 0.909984 loss)
I0815 12:09:00.969722  8764 sgd_solver.cpp:136] Iteration 47800, lr = 0.0070125, m = 0.9
I0815 12:09:17.128612  8764 solver.cpp:312] Iteration 47900 (6.18868 iter/s, 16.1585s/100 iter), loss = 1.53592
I0815 12:09:17.128672  8764 solver.cpp:334]     Train net output #0: loss = 2.12751 (* 1 = 2.12751 loss)
I0815 12:09:17.128679  8764 sgd_solver.cpp:136] Iteration 47900, lr = 0.00700625, m = 0.9
I0815 12:09:34.932883  8764 solver.cpp:363] Sparsity after update:
I0815 12:09:34.936831  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:09:34.936841  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:09:34.936849  8764 net.cpp:2192] conv1b_param_0(0.208) 
I0815 12:09:34.936853  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:09:34.936858  8764 net.cpp:2192] res2a_branch2a_param_0(0.278) 
I0815 12:09:34.936862  8764 net.cpp:2192] res2a_branch2b_param_0(0.278) 
I0815 12:09:34.936866  8764 net.cpp:2192] res3a_branch2a_param_0(0.28) 
I0815 12:09:34.936872  8764 net.cpp:2192] res3a_branch2b_param_0(0.278) 
I0815 12:09:34.936875  8764 net.cpp:2192] res4a_branch2a_param_0(0.28) 
I0815 12:09:34.936880  8764 net.cpp:2192] res4a_branch2b_param_0(0.28) 
I0815 12:09:34.936884  8764 net.cpp:2192] res5a_branch2a_param_0(0.28) 
I0815 12:09:34.936888  8764 net.cpp:2192] res5a_branch2b_param_0(0.279) 
I0815 12:09:34.936892  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (658049/2.86678e+06) 0.23
I0815 12:09:34.936903  8764 solver.cpp:509] Iteration 48000, Testing net (#0)
I0815 12:09:51.351804  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:09:58.606039  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.564765
I0815 12:09:58.606058  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.799233
I0815 12:09:58.606063  8764 solver.cpp:594]     Test net output #2: loss = 1.92158 (* 1 = 1.92158 loss)
I0815 12:09:58.606128  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.6686s
I0815 12:09:58.741977  8794 solver.cpp:409] Finding and applying sparsity: 0.29
I0815 12:10:20.740388  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:10:20.742483  8764 solver.cpp:312] Iteration 48000 (1.57203 iter/s, 63.6121s/100 iter), loss = 1.49832
I0815 12:10:20.742501  8764 solver.cpp:334]     Train net output #0: loss = 1.65855 (* 1 = 1.65855 loss)
I0815 12:10:20.742506  8764 sgd_solver.cpp:136] Iteration 48000, lr = 0.007, m = 0.9
I0815 12:10:39.106744  8764 solver.cpp:312] Iteration 48100 (5.44551 iter/s, 18.3637s/100 iter), loss = 1.57247
I0815 12:10:39.106819  8764 solver.cpp:334]     Train net output #0: loss = 2.05396 (* 1 = 2.05396 loss)
I0815 12:10:39.106832  8764 sgd_solver.cpp:136] Iteration 48100, lr = 0.00699375, m = 0.9
I0815 12:10:54.151089  8764 solver.cpp:312] Iteration 48200 (6.6472 iter/s, 15.0439s/100 iter), loss = 1.5505
I0815 12:10:54.151141  8764 solver.cpp:334]     Train net output #0: loss = 1.82996 (* 1 = 1.82996 loss)
I0815 12:10:54.151154  8764 sgd_solver.cpp:136] Iteration 48200, lr = 0.0069875, m = 0.9
I0815 12:11:10.475033  8764 solver.cpp:312] Iteration 48300 (6.12614 iter/s, 16.3235s/100 iter), loss = 1.365
I0815 12:11:10.475114  8764 solver.cpp:334]     Train net output #0: loss = 1.4657 (* 1 = 1.4657 loss)
I0815 12:11:10.475131  8764 sgd_solver.cpp:136] Iteration 48300, lr = 0.00698125, m = 0.9
I0815 12:11:26.591536  8764 solver.cpp:312] Iteration 48400 (6.205 iter/s, 16.116s/100 iter), loss = 1.64312
I0815 12:11:26.591594  8764 solver.cpp:334]     Train net output #0: loss = 1.29514 (* 1 = 1.29514 loss)
I0815 12:11:26.591708  8764 sgd_solver.cpp:136] Iteration 48400, lr = 0.006975, m = 0.9
I0815 12:11:41.945718  8764 solver.cpp:312] Iteration 48500 (6.51306 iter/s, 15.3538s/100 iter), loss = 1.38458
I0815 12:11:41.945825  8764 solver.cpp:334]     Train net output #0: loss = 1.26828 (* 1 = 1.26828 loss)
I0815 12:11:41.945844  8764 sgd_solver.cpp:136] Iteration 48500, lr = 0.00696875, m = 0.9
I0815 12:11:56.485824  8764 solver.cpp:312] Iteration 48600 (6.87772 iter/s, 14.5397s/100 iter), loss = 1.25453
I0815 12:11:56.485852  8764 solver.cpp:334]     Train net output #0: loss = 1.40803 (* 1 = 1.40803 loss)
I0815 12:11:56.485855  8764 sgd_solver.cpp:136] Iteration 48600, lr = 0.0069625, m = 0.9
I0815 12:12:11.100008  8764 solver.cpp:312] Iteration 48700 (6.84286 iter/s, 14.6138s/100 iter), loss = 1.53765
I0815 12:12:11.100039  8764 solver.cpp:334]     Train net output #0: loss = 1.34266 (* 1 = 1.34266 loss)
I0815 12:12:11.100078  8764 sgd_solver.cpp:136] Iteration 48700, lr = 0.00695625, m = 0.9
I0815 12:12:26.160913  8764 solver.cpp:312] Iteration 48800 (6.63989 iter/s, 15.0605s/100 iter), loss = 1.53279
I0815 12:12:26.160969  8764 solver.cpp:334]     Train net output #0: loss = 1.73425 (* 1 = 1.73425 loss)
I0815 12:12:26.160975  8764 sgd_solver.cpp:136] Iteration 48800, lr = 0.00695, m = 0.9
I0815 12:12:42.156818  8764 solver.cpp:312] Iteration 48900 (6.25177 iter/s, 15.9955s/100 iter), loss = 1.48683
I0815 12:12:42.156844  8764 solver.cpp:334]     Train net output #0: loss = 1.38383 (* 1 = 1.38383 loss)
I0815 12:12:42.156850  8764 sgd_solver.cpp:136] Iteration 48900, lr = 0.00694375, m = 0.9
I0815 12:12:58.637769  8764 solver.cpp:363] Sparsity after update:
I0815 12:12:58.649293  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:12:58.649441  8764 net.cpp:2192] conv1a_param_0(0.133) 
I0815 12:12:58.649540  8764 net.cpp:2192] conv1b_param_0(0.208) 
I0815 12:12:58.649611  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:12:58.649682  8764 net.cpp:2192] res2a_branch2a_param_0(0.288) 
I0815 12:12:58.649752  8764 net.cpp:2192] res2a_branch2b_param_0(0.285) 
I0815 12:12:58.649821  8764 net.cpp:2192] res3a_branch2a_param_0(0.29) 
I0815 12:12:58.650117  8764 net.cpp:2192] res3a_branch2b_param_0(0.288) 
I0815 12:12:58.650198  8764 net.cpp:2192] res4a_branch2a_param_0(0.29) 
I0815 12:12:58.650274  8764 net.cpp:2192] res4a_branch2b_param_0(0.29) 
I0815 12:12:58.650342  8764 net.cpp:2192] res5a_branch2a_param_0(0.29) 
I0815 12:12:58.650418  8764 net.cpp:2192] res5a_branch2b_param_0(0.29) 
I0815 12:12:58.650486  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (681993/2.86678e+06) 0.238
I0815 12:12:58.802037  8794 solver.cpp:409] Finding and applying sparsity: 0.3
I0815 12:13:23.737083  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:13:23.739070  8764 solver.cpp:312] Iteration 49000 (2.40494 iter/s, 41.5811s/100 iter), loss = 1.5265
I0815 12:13:23.739092  8764 solver.cpp:334]     Train net output #0: loss = 1.39039 (* 1 = 1.39039 loss)
I0815 12:13:23.739104  8764 sgd_solver.cpp:136] Iteration 49000, lr = 0.0069375, m = 0.9
I0815 12:13:39.736353  8764 solver.cpp:312] Iteration 49100 (6.25124 iter/s, 15.9968s/100 iter), loss = 1.4934
I0815 12:13:39.736440  8764 solver.cpp:334]     Train net output #0: loss = 1.75301 (* 1 = 1.75301 loss)
I0815 12:13:39.736469  8764 sgd_solver.cpp:136] Iteration 49100, lr = 0.00693125, m = 0.9
I0815 12:13:56.096521  8764 solver.cpp:312] Iteration 49200 (6.11258 iter/s, 16.3597s/100 iter), loss = 1.66964
I0815 12:13:56.096547  8764 solver.cpp:334]     Train net output #0: loss = 2.03676 (* 1 = 2.03676 loss)
I0815 12:13:56.096552  8764 sgd_solver.cpp:136] Iteration 49200, lr = 0.006925, m = 0.9
I0815 12:14:11.917856  8764 solver.cpp:312] Iteration 49300 (6.32076 iter/s, 15.8209s/100 iter), loss = 1.41552
I0815 12:14:11.917951  8764 solver.cpp:334]     Train net output #0: loss = 1.56721 (* 1 = 1.56721 loss)
I0815 12:14:11.917976  8764 sgd_solver.cpp:136] Iteration 49300, lr = 0.00691875, m = 0.9
I0815 12:14:28.976078  8764 solver.cpp:312] Iteration 49400 (5.86244 iter/s, 17.0577s/100 iter), loss = 1.13411
I0815 12:14:28.976106  8764 solver.cpp:334]     Train net output #0: loss = 0.897654 (* 1 = 0.897654 loss)
I0815 12:14:28.976114  8764 sgd_solver.cpp:136] Iteration 49400, lr = 0.0069125, m = 0.9
I0815 12:14:44.171216  8764 solver.cpp:312] Iteration 49500 (6.58124 iter/s, 15.1947s/100 iter), loss = 1.23731
I0815 12:14:44.171285  8764 solver.cpp:334]     Train net output #0: loss = 1.09128 (* 1 = 1.09128 loss)
I0815 12:14:44.171293  8764 sgd_solver.cpp:136] Iteration 49500, lr = 0.00690625, m = 0.9
I0815 12:15:02.557997  8764 solver.cpp:312] Iteration 49600 (5.43884 iter/s, 18.3863s/100 iter), loss = 1.61531
I0815 12:15:02.558024  8764 solver.cpp:334]     Train net output #0: loss = 1.66046 (* 1 = 1.66046 loss)
I0815 12:15:02.558029  8764 sgd_solver.cpp:136] Iteration 49600, lr = 0.0069, m = 0.9
I0815 12:15:19.412220  8764 solver.cpp:312] Iteration 49700 (5.9334 iter/s, 16.8538s/100 iter), loss = 1.66179
I0815 12:15:19.412327  8764 solver.cpp:334]     Train net output #0: loss = 1.62524 (* 1 = 1.62524 loss)
I0815 12:15:19.412338  8764 sgd_solver.cpp:136] Iteration 49700, lr = 0.00689375, m = 0.9
I0815 12:15:35.781685  8764 solver.cpp:312] Iteration 49800 (6.1091 iter/s, 16.369s/100 iter), loss = 1.70197
I0815 12:15:35.781708  8764 solver.cpp:334]     Train net output #0: loss = 2.06953 (* 1 = 2.06953 loss)
I0815 12:15:35.781713  8764 sgd_solver.cpp:136] Iteration 49800, lr = 0.0068875, m = 0.9
I0815 12:15:51.608513  8764 solver.cpp:312] Iteration 49900 (6.31856 iter/s, 15.8264s/100 iter), loss = 1.24262
I0815 12:15:51.608564  8764 solver.cpp:334]     Train net output #0: loss = 1.27901 (* 1 = 1.27901 loss)
I0815 12:15:51.608570  8764 sgd_solver.cpp:136] Iteration 49900, lr = 0.00688125, m = 0.9
I0815 12:16:08.060145  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_50000.caffemodel
I0815 12:16:08.105482  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_50000.solverstate
I0815 12:16:08.111462  8764 solver.cpp:363] Sparsity after update:
I0815 12:16:08.115473  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:16:08.115516  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:16:08.115535  8764 net.cpp:2192] conv1b_param_0(0.219) 
I0815 12:16:08.115551  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:16:08.115566  8764 net.cpp:2192] res2a_branch2a_param_0(0.299) 
I0815 12:16:08.115577  8764 net.cpp:2192] res2a_branch2b_param_0(0.299) 
I0815 12:16:08.115589  8764 net.cpp:2192] res3a_branch2a_param_0(0.299) 
I0815 12:16:08.115600  8764 net.cpp:2192] res3a_branch2b_param_0(0.299) 
I0815 12:16:08.115612  8764 net.cpp:2192] res4a_branch2a_param_0(0.299) 
I0815 12:16:08.115624  8764 net.cpp:2192] res4a_branch2b_param_0(0.299) 
I0815 12:16:08.115635  8764 net.cpp:2192] res5a_branch2a_param_0(0.3) 
I0815 12:16:08.115648  8764 net.cpp:2192] res5a_branch2b_param_0(0.299) 
I0815 12:16:08.115659  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (704892/2.86678e+06) 0.246
I0815 12:16:08.115682  8764 solver.cpp:509] Iteration 50000, Testing net (#0)
I0815 12:16:33.626886  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.564
I0815 12:16:33.626967  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.798351
I0815 12:16:33.626976  8764 solver.cpp:594]     Test net output #2: loss = 1.93075 (* 1 = 1.93075 loss)
I0815 12:16:33.626996  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.5106s
I0815 12:16:33.792814  8794 solver.cpp:409] Finding and applying sparsity: 0.31
I0815 12:16:56.202391  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:16:56.204380  8764 solver.cpp:312] Iteration 50000 (1.54813 iter/s, 64.5941s/100 iter), loss = 1.69561
I0815 12:16:56.204399  8764 solver.cpp:334]     Train net output #0: loss = 1.61734 (* 1 = 1.61734 loss)
I0815 12:16:56.204406  8764 sgd_solver.cpp:136] Iteration 50000, lr = 0.006875, m = 0.9
I0815 12:17:12.486609  8764 solver.cpp:312] Iteration 50100 (6.14184 iter/s, 16.2818s/100 iter), loss = 1.32587
I0815 12:17:12.486711  8764 solver.cpp:334]     Train net output #0: loss = 1.17205 (* 1 = 1.17205 loss)
I0815 12:17:12.486726  8764 sgd_solver.cpp:136] Iteration 50100, lr = 0.00686875, m = 0.9
I0815 12:17:30.793222  8764 solver.cpp:312] Iteration 50200 (5.46266 iter/s, 18.3061s/100 iter), loss = 1.60456
I0815 12:17:30.793315  8764 solver.cpp:334]     Train net output #0: loss = 1.48049 (* 1 = 1.48049 loss)
I0815 12:17:30.793344  8764 sgd_solver.cpp:136] Iteration 50200, lr = 0.0068625, m = 0.9
I0815 12:17:47.591727  8764 solver.cpp:312] Iteration 50300 (5.95308 iter/s, 16.798s/100 iter), loss = 1.29112
I0815 12:17:47.592340  8764 solver.cpp:334]     Train net output #0: loss = 1.20653 (* 1 = 1.20653 loss)
I0815 12:17:47.592347  8764 sgd_solver.cpp:136] Iteration 50300, lr = 0.00685625, m = 0.9
I0815 12:18:06.452807  8764 solver.cpp:312] Iteration 50400 (5.30207 iter/s, 18.8606s/100 iter), loss = 1.59051
I0815 12:18:06.452836  8764 solver.cpp:334]     Train net output #0: loss = 1.94822 (* 1 = 1.94822 loss)
I0815 12:18:06.452842  8764 sgd_solver.cpp:136] Iteration 50400, lr = 0.00685, m = 0.9
I0815 12:18:21.024885  8764 solver.cpp:312] Iteration 50500 (6.86263 iter/s, 14.5717s/100 iter), loss = 1.42025
I0815 12:18:21.024940  8764 solver.cpp:334]     Train net output #0: loss = 1.37511 (* 1 = 1.37511 loss)
I0815 12:18:21.024946  8764 sgd_solver.cpp:136] Iteration 50500, lr = 0.00684375, m = 0.9
I0815 12:18:35.660806  8764 solver.cpp:312] Iteration 50600 (6.8327 iter/s, 14.6355s/100 iter), loss = 1.26979
I0815 12:18:35.660856  8764 solver.cpp:334]     Train net output #0: loss = 1.59741 (* 1 = 1.59741 loss)
I0815 12:18:35.660869  8764 sgd_solver.cpp:136] Iteration 50600, lr = 0.0068375, m = 0.9
I0815 12:18:50.164551  8764 solver.cpp:312] Iteration 50700 (6.89497 iter/s, 14.5033s/100 iter), loss = 1.48423
I0815 12:18:50.164613  8764 solver.cpp:334]     Train net output #0: loss = 1.75006 (* 1 = 1.75006 loss)
I0815 12:18:50.164630  8764 sgd_solver.cpp:136] Iteration 50700, lr = 0.00683125, m = 0.9
I0815 12:19:07.139281  8764 solver.cpp:312] Iteration 50800 (5.89128 iter/s, 16.9742s/100 iter), loss = 1.65512
I0815 12:19:07.139431  8764 solver.cpp:334]     Train net output #0: loss = 1.61348 (* 1 = 1.61348 loss)
I0815 12:19:07.139456  8764 sgd_solver.cpp:136] Iteration 50800, lr = 0.006825, m = 0.9
I0815 12:19:22.577517  8764 solver.cpp:312] Iteration 50900 (6.4776 iter/s, 15.4378s/100 iter), loss = 1.26324
I0815 12:19:22.577567  8764 solver.cpp:334]     Train net output #0: loss = 1.17853 (* 1 = 1.17853 loss)
I0815 12:19:22.577580  8764 sgd_solver.cpp:136] Iteration 50900, lr = 0.00681875, m = 0.9
I0815 12:19:36.982797  8764 solver.cpp:363] Sparsity after update:
I0815 12:19:36.995421  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:19:36.995455  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:19:36.995470  8764 net.cpp:2192] conv1b_param_0(0.229) 
I0815 12:19:36.995479  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:19:36.995488  8764 net.cpp:2192] res2a_branch2a_param_0(0.309) 
I0815 12:19:36.995498  8764 net.cpp:2192] res2a_branch2b_param_0(0.306) 
I0815 12:19:36.995507  8764 net.cpp:2192] res3a_branch2a_param_0(0.309) 
I0815 12:19:36.995515  8764 net.cpp:2192] res3a_branch2b_param_0(0.309) 
I0815 12:19:36.995524  8764 net.cpp:2192] res4a_branch2a_param_0(0.31) 
I0815 12:19:36.995533  8764 net.cpp:2192] res4a_branch2b_param_0(0.309) 
I0815 12:19:36.995542  8764 net.cpp:2192] res5a_branch2a_param_0(0.31) 
I0815 12:19:36.995550  8764 net.cpp:2192] res5a_branch2b_param_0(0.31) 
I0815 12:19:36.995559  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (728858/2.86678e+06) 0.254
I0815 12:19:37.140291  8794 solver.cpp:409] Finding and applying sparsity: 0.32
I0815 12:20:00.293084  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:20:00.295086  8764 solver.cpp:312] Iteration 51000 (2.65136 iter/s, 37.7165s/100 iter), loss = 1.34622
I0815 12:20:00.295109  8764 solver.cpp:334]     Train net output #0: loss = 1.54464 (* 1 = 1.54464 loss)
I0815 12:20:00.295119  8764 sgd_solver.cpp:136] Iteration 51000, lr = 0.0068125, m = 0.9
I0815 12:20:15.524338  8764 solver.cpp:312] Iteration 51100 (6.5665 iter/s, 15.2288s/100 iter), loss = 1.54826
I0815 12:20:15.524415  8764 solver.cpp:334]     Train net output #0: loss = 1.56856 (* 1 = 1.56856 loss)
I0815 12:20:15.524421  8764 sgd_solver.cpp:136] Iteration 51100, lr = 0.00680625, m = 0.9
I0815 12:20:32.963016  8764 solver.cpp:312] Iteration 51200 (5.73454 iter/s, 17.4382s/100 iter), loss = 1.13849
I0815 12:20:32.963160  8764 solver.cpp:334]     Train net output #0: loss = 1.0593 (* 1 = 1.0593 loss)
I0815 12:20:32.963176  8764 sgd_solver.cpp:136] Iteration 51200, lr = 0.0068, m = 0.9
I0815 12:20:48.011862  8764 solver.cpp:312] Iteration 51300 (6.64521 iter/s, 15.0484s/100 iter), loss = 1.29535
I0815 12:20:48.011955  8764 solver.cpp:334]     Train net output #0: loss = 1.50254 (* 1 = 1.50254 loss)
I0815 12:20:48.011972  8764 sgd_solver.cpp:136] Iteration 51300, lr = 0.00679375, m = 0.9
I0815 12:21:04.133414  8764 solver.cpp:312] Iteration 51400 (6.20305 iter/s, 16.1211s/100 iter), loss = 1.69135
I0815 12:21:04.133440  8764 solver.cpp:334]     Train net output #0: loss = 1.70859 (* 1 = 1.70859 loss)
I0815 12:21:04.133445  8764 sgd_solver.cpp:136] Iteration 51400, lr = 0.0067875, m = 0.9
I0815 12:21:18.824151  8764 solver.cpp:312] Iteration 51500 (6.8072 iter/s, 14.6903s/100 iter), loss = 1.4026
I0815 12:21:18.824210  8764 solver.cpp:334]     Train net output #0: loss = 1.30629 (* 1 = 1.30629 loss)
I0815 12:21:18.824218  8764 sgd_solver.cpp:136] Iteration 51500, lr = 0.00678125, m = 0.9
I0815 12:21:34.819317  8764 solver.cpp:312] Iteration 51600 (6.25206 iter/s, 15.9947s/100 iter), loss = 1.31866
I0815 12:21:34.819341  8764 solver.cpp:334]     Train net output #0: loss = 1.2085 (* 1 = 1.2085 loss)
I0815 12:21:34.819345  8764 sgd_solver.cpp:136] Iteration 51600, lr = 0.006775, m = 0.9
I0815 12:21:51.974074  8764 solver.cpp:312] Iteration 51700 (5.82945 iter/s, 17.1543s/100 iter), loss = 1.35794
I0815 12:21:51.974141  8764 solver.cpp:334]     Train net output #0: loss = 1.40449 (* 1 = 1.40449 loss)
I0815 12:21:51.974148  8764 sgd_solver.cpp:136] Iteration 51700, lr = 0.00676875, m = 0.9
I0815 12:22:09.210952  8764 solver.cpp:312] Iteration 51800 (5.80167 iter/s, 17.2364s/100 iter), loss = 1.1651
I0815 12:22:09.210978  8764 solver.cpp:334]     Train net output #0: loss = 0.786118 (* 1 = 0.786118 loss)
I0815 12:22:09.210981  8764 sgd_solver.cpp:136] Iteration 51800, lr = 0.0067625, m = 0.9
I0815 12:22:26.383611  8764 solver.cpp:312] Iteration 51900 (5.82337 iter/s, 17.1722s/100 iter), loss = 1.74761
I0815 12:22:26.383716  8764 solver.cpp:334]     Train net output #0: loss = 1.54096 (* 1 = 1.54096 loss)
I0815 12:22:26.383735  8764 sgd_solver.cpp:136] Iteration 51900, lr = 0.00675625, m = 0.9
I0815 12:22:43.319113  8764 solver.cpp:363] Sparsity after update:
I0815 12:22:43.323026  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:22:43.323143  8764 net.cpp:2192] conv1a_param_0(0.147) 
I0815 12:22:43.323225  8764 net.cpp:2192] conv1b_param_0(0.24) 
I0815 12:22:43.323295  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:22:43.323362  8764 net.cpp:2192] res2a_branch2a_param_0(0.319) 
I0815 12:22:43.323432  8764 net.cpp:2192] res2a_branch2b_param_0(0.319) 
I0815 12:22:43.323501  8764 net.cpp:2192] res3a_branch2a_param_0(0.319) 
I0815 12:22:43.323570  8764 net.cpp:2192] res3a_branch2b_param_0(0.319) 
I0815 12:22:43.323635  8764 net.cpp:2192] res4a_branch2a_param_0(0.319) 
I0815 12:22:43.323704  8764 net.cpp:2192] res4a_branch2b_param_0(0.319) 
I0815 12:22:43.323770  8764 net.cpp:2192] res5a_branch2a_param_0(0.32) 
I0815 12:22:43.323839  8764 net.cpp:2192] res5a_branch2b_param_0(0.319) 
I0815 12:22:43.323906  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (752114/2.86678e+06) 0.262
I0815 12:22:43.324003  8764 solver.cpp:509] Iteration 52000, Testing net (#0)
I0815 12:23:05.993937  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.57053
I0815 12:23:05.994060  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805998
I0815 12:23:05.994068  8764 solver.cpp:594]     Test net output #2: loss = 1.87023 (* 1 = 1.87023 loss)
I0815 12:23:05.994086  8764 solver.cpp:264] [MultiGPU] Tests completed in 22.6695s
I0815 12:23:06.140754  8794 solver.cpp:409] Finding and applying sparsity: 0.33
I0815 12:23:30.798746  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:23:30.800775  8764 solver.cpp:312] Iteration 52000 (1.55242 iter/s, 64.4154s/100 iter), loss = 1.43277
I0815 12:23:30.800792  8764 solver.cpp:334]     Train net output #0: loss = 0.852598 (* 1 = 0.852598 loss)
I0815 12:23:30.800798  8764 sgd_solver.cpp:136] Iteration 52000, lr = 0.00675, m = 0.9
I0815 12:23:46.086879  8764 solver.cpp:312] Iteration 52100 (6.54207 iter/s, 15.2857s/100 iter), loss = 1.37018
I0815 12:23:46.086930  8764 solver.cpp:334]     Train net output #0: loss = 1.23538 (* 1 = 1.23538 loss)
I0815 12:23:46.086937  8764 sgd_solver.cpp:136] Iteration 52100, lr = 0.00674375, m = 0.9
I0815 12:24:00.021019  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:24:04.037585  8764 solver.cpp:312] Iteration 52200 (5.57097 iter/s, 17.9502s/100 iter), loss = 1.14367
I0815 12:24:04.037609  8764 solver.cpp:334]     Train net output #0: loss = 1.29614 (* 1 = 1.29614 loss)
I0815 12:24:04.037616  8764 sgd_solver.cpp:136] Iteration 52200, lr = 0.0067375, m = 0.9
I0815 12:24:21.742593  8764 solver.cpp:312] Iteration 52300 (5.64828 iter/s, 17.7045s/100 iter), loss = 1.54924
I0815 12:24:21.742883  8764 solver.cpp:334]     Train net output #0: loss = 1.55209 (* 1 = 1.55209 loss)
I0815 12:24:21.743000  8764 sgd_solver.cpp:136] Iteration 52300, lr = 0.00673125, m = 0.9
I0815 12:24:37.832157  8764 solver.cpp:312] Iteration 52400 (6.21538 iter/s, 16.0891s/100 iter), loss = 1.28388
I0815 12:24:37.832221  8764 solver.cpp:334]     Train net output #0: loss = 1.2624 (* 1 = 1.2624 loss)
I0815 12:24:37.832237  8764 sgd_solver.cpp:136] Iteration 52400, lr = 0.006725, m = 0.9
I0815 12:24:52.658485  8764 solver.cpp:312] Iteration 52500 (6.74495 iter/s, 14.8259s/100 iter), loss = 1.44685
I0815 12:24:52.658565  8764 solver.cpp:334]     Train net output #0: loss = 1.2415 (* 1 = 1.2415 loss)
I0815 12:24:52.658579  8764 sgd_solver.cpp:136] Iteration 52500, lr = 0.00671875, m = 0.9
I0815 12:25:07.287089  8764 solver.cpp:312] Iteration 52600 (6.83611 iter/s, 14.6282s/100 iter), loss = 1.31348
I0815 12:25:07.287142  8764 solver.cpp:334]     Train net output #0: loss = 1.47019 (* 1 = 1.47019 loss)
I0815 12:25:07.287153  8764 sgd_solver.cpp:136] Iteration 52600, lr = 0.0067125, m = 0.9
I0815 12:25:24.308967  8764 solver.cpp:312] Iteration 52700 (5.87496 iter/s, 17.0214s/100 iter), loss = 1.29145
I0815 12:25:24.309082  8764 solver.cpp:334]     Train net output #0: loss = 1.02401 (* 1 = 1.02401 loss)
I0815 12:25:24.309108  8764 sgd_solver.cpp:136] Iteration 52700, lr = 0.00670625, m = 0.9
I0815 12:25:42.467923  8764 solver.cpp:312] Iteration 52800 (5.50708 iter/s, 18.1585s/100 iter), loss = 1.5578
I0815 12:25:42.467950  8764 solver.cpp:334]     Train net output #0: loss = 1.19745 (* 1 = 1.19745 loss)
I0815 12:25:42.467954  8764 sgd_solver.cpp:136] Iteration 52800, lr = 0.0067, m = 0.9
I0815 12:25:58.958534  8764 solver.cpp:312] Iteration 52900 (6.06423 iter/s, 16.4902s/100 iter), loss = 1.40384
I0815 12:25:58.958631  8764 solver.cpp:334]     Train net output #0: loss = 1.3108 (* 1 = 1.3108 loss)
I0815 12:25:58.958648  8764 sgd_solver.cpp:136] Iteration 52900, lr = 0.00669375, m = 0.9
I0815 12:26:15.865139  8764 solver.cpp:363] Sparsity after update:
I0815 12:26:15.880775  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:26:15.880791  8764 net.cpp:2192] conv1a_param_0(0.16) 
I0815 12:26:15.880801  8764 net.cpp:2192] conv1b_param_0(0.24) 
I0815 12:26:15.880805  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:26:15.880808  8764 net.cpp:2192] res2a_branch2a_param_0(0.33) 
I0815 12:26:15.880812  8764 net.cpp:2192] res2a_branch2b_param_0(0.326) 
I0815 12:26:15.880827  8764 net.cpp:2192] res3a_branch2a_param_0(0.33) 
I0815 12:26:15.880837  8764 net.cpp:2192] res3a_branch2b_param_0(0.33) 
I0815 12:26:15.880846  8764 net.cpp:2192] res4a_branch2a_param_0(0.33) 
I0815 12:26:15.880853  8764 net.cpp:2192] res4a_branch2b_param_0(0.33) 
I0815 12:26:15.880861  8764 net.cpp:2192] res5a_branch2a_param_0(0.33) 
I0815 12:26:15.880872  8764 net.cpp:2192] res5a_branch2b_param_0(0.33) 
I0815 12:26:15.880880  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (776078/2.86678e+06) 0.271
I0815 12:26:16.031514  8794 solver.cpp:409] Finding and applying sparsity: 0.34
I0815 12:26:39.381769  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:26:39.383831  8764 solver.cpp:312] Iteration 53000 (2.47377 iter/s, 40.4242s/100 iter), loss = 1.32734
I0815 12:26:39.383854  8764 solver.cpp:334]     Train net output #0: loss = 1.19055 (* 1 = 1.19055 loss)
I0815 12:26:39.383868  8764 sgd_solver.cpp:136] Iteration 53000, lr = 0.0066875, m = 0.9
I0815 12:26:54.614543  8764 solver.cpp:312] Iteration 53100 (6.56587 iter/s, 15.2303s/100 iter), loss = 1.34362
I0815 12:26:54.614568  8764 solver.cpp:334]     Train net output #0: loss = 1.36786 (* 1 = 1.36786 loss)
I0815 12:26:54.614574  8764 sgd_solver.cpp:136] Iteration 53100, lr = 0.00668125, m = 0.9
I0815 12:27:11.461181  8764 solver.cpp:312] Iteration 53200 (5.93607 iter/s, 16.8462s/100 iter), loss = 1.24478
I0815 12:27:11.461241  8764 solver.cpp:334]     Train net output #0: loss = 1.33558 (* 1 = 1.33558 loss)
I0815 12:27:11.461246  8764 sgd_solver.cpp:136] Iteration 53200, lr = 0.006675, m = 0.9
I0815 12:27:29.170125  8764 solver.cpp:312] Iteration 53300 (5.64702 iter/s, 17.7084s/100 iter), loss = 1.09232
I0815 12:27:29.170151  8764 solver.cpp:334]     Train net output #0: loss = 1.01427 (* 1 = 1.01427 loss)
I0815 12:27:29.170156  8764 sgd_solver.cpp:136] Iteration 53300, lr = 0.00666875, m = 0.9
I0815 12:27:43.754289  8764 solver.cpp:312] Iteration 53400 (6.85694 iter/s, 14.5838s/100 iter), loss = 1.57764
I0815 12:27:43.754380  8764 solver.cpp:334]     Train net output #0: loss = 1.74569 (* 1 = 1.74569 loss)
I0815 12:27:43.754398  8764 sgd_solver.cpp:136] Iteration 53400, lr = 0.0066625, m = 0.9
I0815 12:28:02.056634  8764 solver.cpp:312] Iteration 53500 (5.46393 iter/s, 18.3018s/100 iter), loss = 1.25891
I0815 12:28:02.058202  8764 solver.cpp:334]     Train net output #0: loss = 1.44068 (* 1 = 1.44068 loss)
I0815 12:28:02.058231  8764 sgd_solver.cpp:136] Iteration 53500, lr = 0.00665625, m = 0.9
I0815 12:28:17.404484  8764 solver.cpp:312] Iteration 53600 (6.51575 iter/s, 15.3474s/100 iter), loss = 1.46637
I0815 12:28:17.405243  8764 solver.cpp:334]     Train net output #0: loss = 1.46788 (* 1 = 1.46788 loss)
I0815 12:28:17.405261  8764 sgd_solver.cpp:136] Iteration 53600, lr = 0.00665, m = 0.9
I0815 12:28:34.735841  8764 solver.cpp:312] Iteration 53700 (5.77005 iter/s, 17.3309s/100 iter), loss = 1.31344
I0815 12:28:34.735868  8764 solver.cpp:334]     Train net output #0: loss = 1.39106 (* 1 = 1.39106 loss)
I0815 12:28:34.735874  8764 sgd_solver.cpp:136] Iteration 53700, lr = 0.00664375, m = 0.9
I0815 12:28:51.607003  8764 solver.cpp:312] Iteration 53800 (5.92744 iter/s, 16.8707s/100 iter), loss = 1.55178
I0815 12:28:51.607219  8764 solver.cpp:334]     Train net output #0: loss = 1.94969 (* 1 = 1.94969 loss)
I0815 12:28:51.607234  8764 sgd_solver.cpp:136] Iteration 53800, lr = 0.0066375, m = 0.9
I0815 12:29:07.852169  8764 solver.cpp:312] Iteration 53900 (6.15585 iter/s, 16.2447s/100 iter), loss = 1.37435
I0815 12:29:07.852222  8764 solver.cpp:334]     Train net output #0: loss = 1.30394 (* 1 = 1.30394 loss)
I0815 12:29:07.852236  8764 sgd_solver.cpp:136] Iteration 53900, lr = 0.00663125, m = 0.9
I0815 12:29:23.992152  8764 solver.cpp:363] Sparsity after update:
I0815 12:29:23.994640  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:29:23.994660  8764 net.cpp:2192] conv1a_param_0(0.16) 
I0815 12:29:23.994678  8764 net.cpp:2192] conv1b_param_0(0.25) 
I0815 12:29:23.994688  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:29:23.994694  8764 net.cpp:2192] res2a_branch2a_param_0(0.337) 
I0815 12:29:23.994700  8764 net.cpp:2192] res2a_branch2b_param_0(0.333) 
I0815 12:29:23.994707  8764 net.cpp:2192] res3a_branch2a_param_0(0.339) 
I0815 12:29:23.994714  8764 net.cpp:2192] res3a_branch2b_param_0(0.337) 
I0815 12:29:23.994720  8764 net.cpp:2192] res4a_branch2a_param_0(0.339) 
I0815 12:29:23.994729  8764 net.cpp:2192] res4a_branch2b_param_0(0.339) 
I0815 12:29:23.994735  8764 net.cpp:2192] res5a_branch2a_param_0(0.34) 
I0815 12:29:23.994741  8764 net.cpp:2192] res5a_branch2b_param_0(0.339) 
I0815 12:29:23.994748  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (798708/2.86678e+06) 0.279
I0815 12:29:23.994774  8764 solver.cpp:509] Iteration 54000, Testing net (#0)
I0815 12:29:47.303164  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.558471
I0815 12:29:47.303184  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.799644
I0815 12:29:47.303191  8764 solver.cpp:594]     Test net output #2: loss = 1.9315 (* 1 = 1.9315 loss)
I0815 12:29:47.303211  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.3078s
I0815 12:29:47.457026  8794 solver.cpp:409] Finding and applying sparsity: 0.35
I0815 12:30:11.527894  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:30:11.529909  8764 solver.cpp:312] Iteration 54000 (1.57045 iter/s, 63.676s/100 iter), loss = 1.30581
I0815 12:30:11.529930  8764 solver.cpp:334]     Train net output #0: loss = 1.56092 (* 1 = 1.56092 loss)
I0815 12:30:11.529940  8764 sgd_solver.cpp:136] Iteration 54000, lr = 0.006625, m = 0.9
I0815 12:30:27.924468  8764 solver.cpp:312] Iteration 54100 (6.09976 iter/s, 16.3941s/100 iter), loss = 1.20821
I0815 12:30:27.924495  8764 solver.cpp:334]     Train net output #0: loss = 1.53579 (* 1 = 1.53579 loss)
I0815 12:30:27.924499  8764 sgd_solver.cpp:136] Iteration 54100, lr = 0.00661875, m = 0.9
I0815 12:30:44.457183  8764 solver.cpp:312] Iteration 54200 (6.04878 iter/s, 16.5323s/100 iter), loss = 1.76486
I0815 12:30:44.457247  8764 solver.cpp:334]     Train net output #0: loss = 1.33795 (* 1 = 1.33795 loss)
I0815 12:30:44.457254  8764 sgd_solver.cpp:136] Iteration 54200, lr = 0.0066125, m = 0.9
I0815 12:31:04.651316  8764 solver.cpp:312] Iteration 54300 (4.95207 iter/s, 20.1936s/100 iter), loss = 1.38692
I0815 12:31:04.651376  8764 solver.cpp:334]     Train net output #0: loss = 1.07039 (* 1 = 1.07039 loss)
I0815 12:31:04.651398  8764 sgd_solver.cpp:136] Iteration 54300, lr = 0.00660625, m = 0.9
I0815 12:31:20.684638  8764 solver.cpp:312] Iteration 54400 (6.23719 iter/s, 16.0329s/100 iter), loss = 1.35246
I0815 12:31:20.684692  8764 solver.cpp:334]     Train net output #0: loss = 1.41178 (* 1 = 1.41178 loss)
I0815 12:31:20.684700  8764 sgd_solver.cpp:136] Iteration 54400, lr = 0.0066, m = 0.9
I0815 12:31:37.337643  8764 solver.cpp:312] Iteration 54500 (6.00509 iter/s, 16.6525s/100 iter), loss = 1.63038
I0815 12:31:37.337668  8764 solver.cpp:334]     Train net output #0: loss = 1.91332 (* 1 = 1.91332 loss)
I0815 12:31:37.337673  8764 sgd_solver.cpp:136] Iteration 54500, lr = 0.00659375, m = 0.9
I0815 12:31:53.990952  8764 solver.cpp:312] Iteration 54600 (6.00498 iter/s, 16.6528s/100 iter), loss = 1.28982
I0815 12:31:53.991034  8764 solver.cpp:334]     Train net output #0: loss = 1.4631 (* 1 = 1.4631 loss)
I0815 12:31:53.991123  8764 sgd_solver.cpp:136] Iteration 54600, lr = 0.0065875, m = 0.9
I0815 12:32:10.561812  8764 solver.cpp:312] Iteration 54700 (6.03485 iter/s, 16.5704s/100 iter), loss = 1.6286
I0815 12:32:10.561841  8764 solver.cpp:334]     Train net output #0: loss = 1.84617 (* 1 = 1.84617 loss)
I0815 12:32:10.561846  8764 sgd_solver.cpp:136] Iteration 54700, lr = 0.00658125, m = 0.9
I0815 12:32:27.624892  8764 solver.cpp:312] Iteration 54800 (5.86077 iter/s, 17.0626s/100 iter), loss = 1.11796
I0815 12:32:27.624963  8764 solver.cpp:334]     Train net output #0: loss = 1.20446 (* 1 = 1.20446 loss)
I0815 12:32:27.624971  8764 sgd_solver.cpp:136] Iteration 54800, lr = 0.006575, m = 0.9
I0815 12:32:44.277544  8764 solver.cpp:312] Iteration 54900 (6.00521 iter/s, 16.6522s/100 iter), loss = 1.67474
I0815 12:32:44.277572  8764 solver.cpp:334]     Train net output #0: loss = 1.60701 (* 1 = 1.60701 loss)
I0815 12:32:44.277578  8764 sgd_solver.cpp:136] Iteration 54900, lr = 0.00656875, m = 0.9
I0815 12:32:59.088043  8764 solver.cpp:363] Sparsity after update:
I0815 12:32:59.098551  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:32:59.098589  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:32:59.098603  8764 net.cpp:2192] conv1b_param_0(0.26) 
I0815 12:32:59.098613  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:32:59.098620  8764 net.cpp:2192] res2a_branch2a_param_0(0.347) 
I0815 12:32:59.098628  8764 net.cpp:2192] res2a_branch2b_param_0(0.347) 
I0815 12:32:59.098636  8764 net.cpp:2192] res3a_branch2a_param_0(0.349) 
I0815 12:32:59.098644  8764 net.cpp:2192] res3a_branch2b_param_0(0.347) 
I0815 12:32:59.098651  8764 net.cpp:2192] res4a_branch2a_param_0(0.35) 
I0815 12:32:59.098659  8764 net.cpp:2192] res4a_branch2b_param_0(0.349) 
I0815 12:32:59.098667  8764 net.cpp:2192] res5a_branch2a_param_0(0.35) 
I0815 12:32:59.098675  8764 net.cpp:2192] res5a_branch2b_param_0(0.35) 
I0815 12:32:59.098682  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (822758/2.86678e+06) 0.287
I0815 12:32:59.252229  8794 solver.cpp:409] Finding and applying sparsity: 0.36
I0815 12:33:23.255916  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:33:23.258029  8764 solver.cpp:312] Iteration 55000 (2.56546 iter/s, 38.9794s/100 iter), loss = 1.17539
I0815 12:33:23.258049  8764 solver.cpp:334]     Train net output #0: loss = 1.35282 (* 1 = 1.35282 loss)
I0815 12:33:23.258057  8764 sgd_solver.cpp:136] Iteration 55000, lr = 0.0065625, m = 0.9
I0815 12:33:38.066442  8764 solver.cpp:312] Iteration 55100 (6.75311 iter/s, 14.808s/100 iter), loss = 1.30555
I0815 12:33:38.066493  8764 solver.cpp:334]     Train net output #0: loss = 1.05288 (* 1 = 1.05288 loss)
I0815 12:33:38.066499  8764 sgd_solver.cpp:136] Iteration 55100, lr = 0.00655625, m = 0.9
I0815 12:33:56.071745  8764 solver.cpp:312] Iteration 55200 (5.55407 iter/s, 18.0048s/100 iter), loss = 1.22635
I0815 12:33:56.071784  8764 solver.cpp:334]     Train net output #0: loss = 1.52938 (* 1 = 1.52938 loss)
I0815 12:33:56.071792  8764 sgd_solver.cpp:136] Iteration 55200, lr = 0.00655, m = 0.9
I0815 12:34:12.673099  8764 solver.cpp:312] Iteration 55300 (6.02377 iter/s, 16.6009s/100 iter), loss = 1.30569
I0815 12:34:12.673158  8764 solver.cpp:334]     Train net output #0: loss = 1.39365 (* 1 = 1.39365 loss)
I0815 12:34:12.673166  8764 sgd_solver.cpp:136] Iteration 55300, lr = 0.00654375, m = 0.9
I0815 12:34:27.010190  8764 solver.cpp:312] Iteration 55400 (6.97511 iter/s, 14.3367s/100 iter), loss = 1.49972
I0815 12:34:27.010215  8764 solver.cpp:334]     Train net output #0: loss = 1.42432 (* 1 = 1.42432 loss)
I0815 12:34:27.010241  8764 sgd_solver.cpp:136] Iteration 55400, lr = 0.0065375, m = 0.9
I0815 12:34:44.191239  8764 solver.cpp:312] Iteration 55500 (5.82053 iter/s, 17.1806s/100 iter), loss = 1.29205
I0815 12:34:44.191388  8764 solver.cpp:334]     Train net output #0: loss = 1.10008 (* 1 = 1.10008 loss)
I0815 12:34:44.191411  8764 sgd_solver.cpp:136] Iteration 55500, lr = 0.00653125, m = 0.9
I0815 12:35:02.660953  8764 solver.cpp:312] Iteration 55600 (5.41442 iter/s, 18.4692s/100 iter), loss = 1.60754
I0815 12:35:02.660979  8764 solver.cpp:334]     Train net output #0: loss = 1.74925 (* 1 = 1.74925 loss)
I0815 12:35:02.660984  8764 sgd_solver.cpp:136] Iteration 55600, lr = 0.006525, m = 0.9
I0815 12:35:21.712406  8764 solver.cpp:312] Iteration 55700 (5.24909 iter/s, 19.0509s/100 iter), loss = 1.47047
I0815 12:35:21.712528  8764 solver.cpp:334]     Train net output #0: loss = 1.54156 (* 1 = 1.54156 loss)
I0815 12:35:21.712543  8764 sgd_solver.cpp:136] Iteration 55700, lr = 0.00651875, m = 0.9
I0815 12:35:40.867506  8764 solver.cpp:312] Iteration 55800 (5.22069 iter/s, 19.1546s/100 iter), loss = 1.47894
I0815 12:35:40.869690  8764 solver.cpp:334]     Train net output #0: loss = 1.85913 (* 1 = 1.85913 loss)
I0815 12:35:40.869712  8764 sgd_solver.cpp:136] Iteration 55800, lr = 0.0065125, m = 0.9
I0815 12:35:55.736387  8764 solver.cpp:312] Iteration 55900 (6.72564 iter/s, 14.8685s/100 iter), loss = 1.33017
I0815 12:35:55.736481  8764 solver.cpp:334]     Train net output #0: loss = 1.42967 (* 1 = 1.42967 loss)
I0815 12:35:55.736498  8764 sgd_solver.cpp:136] Iteration 55900, lr = 0.00650625, m = 0.9
I0815 12:36:10.227483  8764 solver.cpp:363] Sparsity after update:
I0815 12:36:10.231364  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:36:10.231374  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:36:10.231382  8764 net.cpp:2192] conv1b_param_0(0.26) 
I0815 12:36:10.231385  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:36:10.231398  8764 net.cpp:2192] res2a_branch2a_param_0(0.358) 
I0815 12:36:10.231412  8764 net.cpp:2192] res2a_branch2b_param_0(0.354) 
I0815 12:36:10.231418  8764 net.cpp:2192] res3a_branch2a_param_0(0.359) 
I0815 12:36:10.231421  8764 net.cpp:2192] res3a_branch2b_param_0(0.358) 
I0815 12:36:10.231429  8764 net.cpp:2192] res4a_branch2a_param_0(0.359) 
I0815 12:36:10.231434  8764 net.cpp:2192] res4a_branch2b_param_0(0.359) 
I0815 12:36:10.231443  8764 net.cpp:2192] res5a_branch2a_param_0(0.36) 
I0815 12:36:10.231448  8764 net.cpp:2192] res5a_branch2b_param_0(0.359) 
I0815 12:36:10.231451  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (845930/2.86678e+06) 0.295
I0815 12:36:10.231470  8764 solver.cpp:509] Iteration 56000, Testing net (#0)
I0815 12:36:31.056681  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571294
I0815 12:36:31.056728  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804939
I0815 12:36:31.056735  8764 solver.cpp:594]     Test net output #2: loss = 1.8944 (* 1 = 1.8944 loss)
I0815 12:36:31.059449  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.8274s
I0815 12:36:31.206226  8794 solver.cpp:409] Finding and applying sparsity: 0.37
I0815 12:36:55.348986  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:36:55.351032  8764 solver.cpp:312] Iteration 56000 (1.67749 iter/s, 59.613s/100 iter), loss = 1.28572
I0815 12:36:55.351054  8764 solver.cpp:334]     Train net output #0: loss = 1.41576 (* 1 = 1.41576 loss)
I0815 12:36:55.351061  8764 sgd_solver.cpp:136] Iteration 56000, lr = 0.0065, m = 0.9
I0815 12:37:12.334408  8764 solver.cpp:312] Iteration 56100 (5.88828 iter/s, 16.9829s/100 iter), loss = 1.43881
I0815 12:37:12.334498  8764 solver.cpp:334]     Train net output #0: loss = 1.6393 (* 1 = 1.6393 loss)
I0815 12:37:12.334512  8764 sgd_solver.cpp:136] Iteration 56100, lr = 0.00649375, m = 0.9
I0815 12:37:28.215925  8764 solver.cpp:312] Iteration 56200 (6.2968 iter/s, 15.8811s/100 iter), loss = 1.37647
I0815 12:37:28.216150  8764 solver.cpp:334]     Train net output #0: loss = 1.62677 (* 1 = 1.62677 loss)
I0815 12:37:28.216264  8764 sgd_solver.cpp:136] Iteration 56200, lr = 0.0064875, m = 0.9
I0815 12:37:45.912179  8764 solver.cpp:312] Iteration 56300 (5.65107 iter/s, 17.6958s/100 iter), loss = 1.59454
I0815 12:37:45.912227  8764 solver.cpp:334]     Train net output #0: loss = 1.70409 (* 1 = 1.70409 loss)
I0815 12:37:45.912232  8764 sgd_solver.cpp:136] Iteration 56300, lr = 0.00648125, m = 0.9
I0815 12:38:01.295022  8764 solver.cpp:312] Iteration 56400 (6.50093 iter/s, 15.3824s/100 iter), loss = 1.72164
I0815 12:38:01.295047  8764 solver.cpp:334]     Train net output #0: loss = 1.33969 (* 1 = 1.33969 loss)
I0815 12:38:01.295053  8764 sgd_solver.cpp:136] Iteration 56400, lr = 0.006475, m = 0.9
I0815 12:38:17.987663  8764 solver.cpp:312] Iteration 56500 (5.99083 iter/s, 16.6922s/100 iter), loss = 1.46486
I0815 12:38:17.987721  8764 solver.cpp:334]     Train net output #0: loss = 1.67182 (* 1 = 1.67182 loss)
I0815 12:38:17.987726  8764 sgd_solver.cpp:136] Iteration 56500, lr = 0.00646875, m = 0.9
I0815 12:38:34.220371  8764 solver.cpp:312] Iteration 56600 (6.16057 iter/s, 16.2323s/100 iter), loss = 1.10914
I0815 12:38:34.220448  8764 solver.cpp:334]     Train net output #0: loss = 0.975896 (* 1 = 0.975896 loss)
I0815 12:38:34.220468  8764 sgd_solver.cpp:136] Iteration 56600, lr = 0.0064625, m = 0.9
I0815 12:38:50.128127  8764 solver.cpp:312] Iteration 56700 (6.28642 iter/s, 15.9073s/100 iter), loss = 1.48183
I0815 12:38:50.128204  8764 solver.cpp:334]     Train net output #0: loss = 2.02349 (* 1 = 2.02349 loss)
I0815 12:38:50.128209  8764 sgd_solver.cpp:136] Iteration 56700, lr = 0.00645625, m = 0.9
I0815 12:39:06.162526  8764 solver.cpp:312] Iteration 56800 (6.23676 iter/s, 16.034s/100 iter), loss = 1.30046
I0815 12:39:06.162557  8764 solver.cpp:334]     Train net output #0: loss = 1.50873 (* 1 = 1.50873 loss)
I0815 12:39:06.162564  8764 sgd_solver.cpp:136] Iteration 56800, lr = 0.00645, m = 0.9
I0815 12:39:21.399505  8764 solver.cpp:312] Iteration 56900 (6.56316 iter/s, 15.2366s/100 iter), loss = 1.11307
I0815 12:39:21.399598  8764 solver.cpp:334]     Train net output #0: loss = 1.10894 (* 1 = 1.10894 loss)
I0815 12:39:21.399616  8764 sgd_solver.cpp:136] Iteration 56900, lr = 0.00644375, m = 0.9
I0815 12:39:37.939755  8764 solver.cpp:363] Sparsity after update:
I0815 12:39:37.952587  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:39:37.952602  8764 net.cpp:2192] conv1a_param_0(0.173) 
I0815 12:39:37.952611  8764 net.cpp:2192] conv1b_param_0(0.271) 
I0815 12:39:37.952615  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:39:37.952621  8764 net.cpp:2192] res2a_branch2a_param_0(0.368) 
I0815 12:39:37.952625  8764 net.cpp:2192] res2a_branch2b_param_0(0.368) 
I0815 12:39:37.952627  8764 net.cpp:2192] res3a_branch2a_param_0(0.37) 
I0815 12:39:37.952630  8764 net.cpp:2192] res3a_branch2b_param_0(0.368) 
I0815 12:39:37.952635  8764 net.cpp:2192] res4a_branch2a_param_0(0.37) 
I0815 12:39:37.952637  8764 net.cpp:2192] res4a_branch2b_param_0(0.37) 
I0815 12:39:37.952641  8764 net.cpp:2192] res5a_branch2a_param_0(0.37) 
I0815 12:39:37.952643  8764 net.cpp:2192] res5a_branch2b_param_0(0.37) 
I0815 12:39:37.952646  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (869947/2.86678e+06) 0.303
I0815 12:39:38.092272  8794 solver.cpp:409] Finding and applying sparsity: 0.38
I0815 12:40:07.145928  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:40:07.148059  8764 solver.cpp:312] Iteration 57000 (2.18592 iter/s, 45.7473s/100 iter), loss = 1.39197
I0815 12:40:07.148078  8764 solver.cpp:334]     Train net output #0: loss = 1.45955 (* 1 = 1.45955 loss)
I0815 12:40:07.148087  8764 sgd_solver.cpp:136] Iteration 57000, lr = 0.0064375, m = 0.9
I0815 12:40:23.704613  8764 solver.cpp:312] Iteration 57100 (6.04007 iter/s, 16.5561s/100 iter), loss = 1.42926
I0815 12:40:23.704640  8764 solver.cpp:334]     Train net output #0: loss = 1.3874 (* 1 = 1.3874 loss)
I0815 12:40:23.704645  8764 sgd_solver.cpp:136] Iteration 57100, lr = 0.00643125, m = 0.9
I0815 12:40:41.731842  8764 solver.cpp:312] Iteration 57200 (5.54732 iter/s, 18.0267s/100 iter), loss = 1.35756
I0815 12:40:41.731945  8764 solver.cpp:334]     Train net output #0: loss = 1.25896 (* 1 = 1.25896 loss)
I0815 12:40:41.731958  8764 sgd_solver.cpp:136] Iteration 57200, lr = 0.006425, m = 0.9
I0815 12:40:58.606662  8764 solver.cpp:312] Iteration 57300 (5.92615 iter/s, 16.8743s/100 iter), loss = 1.24083
I0815 12:40:58.606709  8764 solver.cpp:334]     Train net output #0: loss = 1.33497 (* 1 = 1.33497 loss)
I0815 12:40:58.606720  8764 sgd_solver.cpp:136] Iteration 57300, lr = 0.00641875, m = 0.9
I0815 12:40:59.534304  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:41:15.535609  8764 solver.cpp:312] Iteration 57400 (5.9072 iter/s, 16.9285s/100 iter), loss = 1.65923
I0815 12:41:15.535691  8764 solver.cpp:334]     Train net output #0: loss = 1.61933 (* 1 = 1.61933 loss)
I0815 12:41:15.535697  8764 sgd_solver.cpp:136] Iteration 57400, lr = 0.0064125, m = 0.9
I0815 12:41:31.025511  8764 solver.cpp:312] Iteration 57500 (6.456 iter/s, 15.4895s/100 iter), loss = 0.931446
I0815 12:41:31.025534  8764 solver.cpp:334]     Train net output #0: loss = 0.634437 (* 1 = 0.634437 loss)
I0815 12:41:31.025538  8764 sgd_solver.cpp:136] Iteration 57500, lr = 0.00640625, m = 0.9
I0815 12:41:46.862406  8764 solver.cpp:312] Iteration 57600 (6.31454 iter/s, 15.8365s/100 iter), loss = 1.4606
I0815 12:41:46.864157  8764 solver.cpp:334]     Train net output #0: loss = 1.70401 (* 1 = 1.70401 loss)
I0815 12:41:46.864166  8764 sgd_solver.cpp:136] Iteration 57600, lr = 0.0064, m = 0.9
I0815 12:42:03.397094  8764 solver.cpp:312] Iteration 57700 (6.04806 iter/s, 16.5342s/100 iter), loss = 1.4333
I0815 12:42:03.397120  8764 solver.cpp:334]     Train net output #0: loss = 1.41328 (* 1 = 1.41328 loss)
I0815 12:42:03.397125  8764 sgd_solver.cpp:136] Iteration 57700, lr = 0.00639375, m = 0.9
I0815 12:42:20.062961  8764 solver.cpp:312] Iteration 57800 (6.00045 iter/s, 16.6654s/100 iter), loss = 1.80786
I0815 12:42:20.063201  8764 solver.cpp:334]     Train net output #0: loss = 1.81814 (* 1 = 1.81814 loss)
I0815 12:42:20.063313  8764 sgd_solver.cpp:136] Iteration 57800, lr = 0.0063875, m = 0.9
I0815 12:42:36.194933  8764 solver.cpp:312] Iteration 57900 (6.19904 iter/s, 16.1315s/100 iter), loss = 1.58255
I0815 12:42:36.194962  8764 solver.cpp:334]     Train net output #0: loss = 1.51307 (* 1 = 1.51307 loss)
I0815 12:42:36.194967  8764 sgd_solver.cpp:136] Iteration 57900, lr = 0.00638125, m = 0.9
I0815 12:42:51.517217  8764 solver.cpp:363] Sparsity after update:
I0815 12:42:51.521083  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:42:51.521093  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:42:51.521103  8764 net.cpp:2192] conv1b_param_0(0.375) 
I0815 12:42:51.521117  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:42:51.521127  8764 net.cpp:2192] res2a_branch2a_param_0(0.378) 
I0815 12:42:51.521139  8764 net.cpp:2192] res2a_branch2b_param_0(0.375) 
I0815 12:42:51.521148  8764 net.cpp:2192] res3a_branch2a_param_0(0.378) 
I0815 12:42:51.521157  8764 net.cpp:2192] res3a_branch2b_param_0(0.378) 
I0815 12:42:51.521165  8764 net.cpp:2192] res4a_branch2a_param_0(0.379) 
I0815 12:42:51.521174  8764 net.cpp:2192] res4a_branch2b_param_0(0.378) 
I0815 12:42:51.521183  8764 net.cpp:2192] res5a_branch2a_param_0(0.38) 
I0815 12:42:51.521193  8764 net.cpp:2192] res5a_branch2b_param_0(0.379) 
I0815 12:42:51.521200  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (893005/2.86678e+06) 0.312
I0815 12:42:51.521217  8764 solver.cpp:509] Iteration 58000, Testing net (#0)
I0815 12:43:14.737154  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.574824
I0815 12:43:14.737175  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804174
I0815 12:43:14.737182  8764 solver.cpp:594]     Test net output #2: loss = 1.86589 (* 1 = 1.86589 loss)
I0815 12:43:14.737258  8764 solver.cpp:264] [MultiGPU] Tests completed in 23.2154s
I0815 12:43:14.894122  8794 solver.cpp:409] Finding and applying sparsity: 0.39
I0815 12:43:39.942683  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:43:39.944768  8764 solver.cpp:312] Iteration 58000 (1.56867 iter/s, 63.7481s/100 iter), loss = 1.83487
I0815 12:43:39.944790  8764 solver.cpp:334]     Train net output #0: loss = 1.5769 (* 1 = 1.5769 loss)
I0815 12:43:39.944798  8764 sgd_solver.cpp:136] Iteration 58000, lr = 0.006375, m = 0.9
I0815 12:43:56.628553  8764 solver.cpp:312] Iteration 58100 (5.99401 iter/s, 16.6833s/100 iter), loss = 1.54222
I0815 12:43:56.628578  8764 solver.cpp:334]     Train net output #0: loss = 1.99252 (* 1 = 1.99252 loss)
I0815 12:43:56.628582  8764 sgd_solver.cpp:136] Iteration 58100, lr = 0.00636875, m = 0.9
I0815 12:44:11.225312  8764 solver.cpp:312] Iteration 58200 (6.85103 iter/s, 14.5964s/100 iter), loss = 1.51371
I0815 12:44:11.225375  8764 solver.cpp:334]     Train net output #0: loss = 1.38955 (* 1 = 1.38955 loss)
I0815 12:44:11.225380  8764 sgd_solver.cpp:136] Iteration 58200, lr = 0.0063625, m = 0.9
I0815 12:44:26.733448  8764 solver.cpp:312] Iteration 58300 (6.44841 iter/s, 15.5077s/100 iter), loss = 1.78242
I0815 12:44:26.733517  8764 solver.cpp:334]     Train net output #0: loss = 1.77128 (* 1 = 1.77128 loss)
I0815 12:44:26.733536  8764 sgd_solver.cpp:136] Iteration 58300, lr = 0.00635625, m = 0.9
I0815 12:44:43.791076  8764 solver.cpp:312] Iteration 58400 (5.86264 iter/s, 17.0572s/100 iter), loss = 1.63353
I0815 12:44:43.791136  8764 solver.cpp:334]     Train net output #0: loss = 1.73331 (* 1 = 1.73331 loss)
I0815 12:44:43.791141  8764 sgd_solver.cpp:136] Iteration 58400, lr = 0.00635, m = 0.9
I0815 12:45:00.440737  8764 solver.cpp:312] Iteration 58500 (6.0063 iter/s, 16.6492s/100 iter), loss = 1.66732
I0815 12:45:00.440794  8764 solver.cpp:334]     Train net output #0: loss = 1.35997 (* 1 = 1.35997 loss)
I0815 12:45:00.440805  8764 sgd_solver.cpp:136] Iteration 58500, lr = 0.00634375, m = 0.9
I0815 12:45:16.168184  8764 solver.cpp:312] Iteration 58600 (6.35848 iter/s, 15.727s/100 iter), loss = 1.5193
I0815 12:45:16.168231  8764 solver.cpp:334]     Train net output #0: loss = 1.2942 (* 1 = 1.2942 loss)
I0815 12:45:16.168236  8764 sgd_solver.cpp:136] Iteration 58600, lr = 0.0063375, m = 0.9
I0815 12:45:32.196089  8764 solver.cpp:312] Iteration 58700 (6.23929 iter/s, 16.0275s/100 iter), loss = 1.25862
I0815 12:45:32.196117  8764 solver.cpp:334]     Train net output #0: loss = 1.23083 (* 1 = 1.23083 loss)
I0815 12:45:32.196123  8764 sgd_solver.cpp:136] Iteration 58700, lr = 0.00633125, m = 0.9
I0815 12:45:50.465425  8764 solver.cpp:312] Iteration 58800 (5.4738 iter/s, 18.2688s/100 iter), loss = 1.91853
I0815 12:45:50.468240  8764 solver.cpp:334]     Train net output #0: loss = 1.98158 (* 1 = 1.98158 loss)
I0815 12:45:50.468271  8764 sgd_solver.cpp:136] Iteration 58800, lr = 0.006325, m = 0.9
I0815 12:46:07.476353  8764 solver.cpp:312] Iteration 58900 (5.87873 iter/s, 17.0105s/100 iter), loss = 1.29631
I0815 12:46:07.476382  8764 solver.cpp:334]     Train net output #0: loss = 1.33704 (* 1 = 1.33704 loss)
I0815 12:46:07.476388  8764 sgd_solver.cpp:136] Iteration 58900, lr = 0.00631875, m = 0.9
I0815 12:46:23.682009  8764 solver.cpp:363] Sparsity after update:
I0815 12:46:23.695515  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:46:23.695530  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:46:23.695539  8764 net.cpp:2192] conv1b_param_0(0.389) 
I0815 12:46:23.695543  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:46:23.695547  8764 net.cpp:2192] res2a_branch2a_param_0(0.389) 
I0815 12:46:23.695551  8764 net.cpp:2192] res2a_branch2b_param_0(0.389) 
I0815 12:46:23.695554  8764 net.cpp:2192] res3a_branch2a_param_0(0.389) 
I0815 12:46:23.695557  8764 net.cpp:2192] res3a_branch2b_param_0(0.389) 
I0815 12:46:23.695560  8764 net.cpp:2192] res4a_branch2a_param_0(0.39) 
I0815 12:46:23.695564  8764 net.cpp:2192] res4a_branch2b_param_0(0.389) 
I0815 12:46:23.695566  8764 net.cpp:2192] res5a_branch2a_param_0(0.39) 
I0815 12:46:23.695570  8764 net.cpp:2192] res5a_branch2b_param_0(0.39) 
I0815 12:46:23.695574  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (917042/2.86678e+06) 0.32
I0815 12:46:23.843201  8794 solver.cpp:409] Finding and applying sparsity: 0.4
I0815 12:46:51.063246  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:46:51.065251  8764 solver.cpp:312] Iteration 59000 (2.29422 iter/s, 43.5877s/100 iter), loss = 1.61565
I0815 12:46:51.065270  8764 solver.cpp:334]     Train net output #0: loss = 2.1244 (* 1 = 2.1244 loss)
I0815 12:46:51.065275  8764 sgd_solver.cpp:136] Iteration 59000, lr = 0.0063125, m = 0.9
I0815 12:47:08.078419  8764 solver.cpp:312] Iteration 59100 (5.87796 iter/s, 17.0127s/100 iter), loss = 1.72105
I0815 12:47:08.078474  8764 solver.cpp:334]     Train net output #0: loss = 1.20753 (* 1 = 1.20753 loss)
I0815 12:47:08.078477  8764 sgd_solver.cpp:136] Iteration 59100, lr = 0.00630625, m = 0.9
I0815 12:47:25.024884  8764 solver.cpp:312] Iteration 59200 (5.9011 iter/s, 16.946s/100 iter), loss = 1.49584
I0815 12:47:25.024952  8764 solver.cpp:334]     Train net output #0: loss = 1.56826 (* 1 = 1.56826 loss)
I0815 12:47:25.024969  8764 sgd_solver.cpp:136] Iteration 59200, lr = 0.0063, m = 0.9
I0815 12:47:41.528509  8764 solver.cpp:312] Iteration 59300 (6.05944 iter/s, 16.5032s/100 iter), loss = 1.45367
I0815 12:47:41.528568  8764 solver.cpp:334]     Train net output #0: loss = 1.20018 (* 1 = 1.20018 loss)
I0815 12:47:41.528576  8764 sgd_solver.cpp:136] Iteration 59300, lr = 0.00629375, m = 0.9
I0815 12:47:57.735184  8764 solver.cpp:312] Iteration 59400 (6.17047 iter/s, 16.2062s/100 iter), loss = 1.72684
I0815 12:47:57.735211  8764 solver.cpp:334]     Train net output #0: loss = 1.84737 (* 1 = 1.84737 loss)
I0815 12:47:57.735219  8764 sgd_solver.cpp:136] Iteration 59400, lr = 0.0062875, m = 0.9
I0815 12:48:14.983206  8764 solver.cpp:312] Iteration 59500 (5.79793 iter/s, 17.2475s/100 iter), loss = 1.42758
I0815 12:48:14.984213  8764 solver.cpp:334]     Train net output #0: loss = 1.35373 (* 1 = 1.35373 loss)
I0815 12:48:14.984221  8764 sgd_solver.cpp:136] Iteration 59500, lr = 0.00628125, m = 0.9
I0815 12:48:34.005475  8726 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 12:48:35.896302  8764 solver.cpp:312] Iteration 59600 (4.78183 iter/s, 20.9125s/100 iter), loss = 1.50134
I0815 12:48:35.896332  8764 solver.cpp:334]     Train net output #0: loss = 1.35289 (* 1 = 1.35289 loss)
I0815 12:48:35.896338  8764 sgd_solver.cpp:136] Iteration 59600, lr = 0.006275, m = 0.9
I0815 12:48:53.346122  8764 solver.cpp:312] Iteration 59700 (5.73088 iter/s, 17.4493s/100 iter), loss = 1.59109
I0815 12:48:53.346215  8764 solver.cpp:334]     Train net output #0: loss = 1.75092 (* 1 = 1.75092 loss)
I0815 12:48:53.346232  8764 sgd_solver.cpp:136] Iteration 59700, lr = 0.00626875, m = 0.9
I0815 12:49:08.829792  8764 solver.cpp:312] Iteration 59800 (6.4586 iter/s, 15.4832s/100 iter), loss = 1.44818
I0815 12:49:08.829815  8764 solver.cpp:334]     Train net output #0: loss = 1.35366 (* 1 = 1.35366 loss)
I0815 12:49:08.829819  8764 sgd_solver.cpp:136] Iteration 59800, lr = 0.0062625, m = 0.9
I0815 12:49:27.431583  8764 solver.cpp:312] Iteration 59900 (5.37597 iter/s, 18.6013s/100 iter), loss = 1.26812
I0815 12:49:27.431674  8764 solver.cpp:334]     Train net output #0: loss = 1.33322 (* 1 = 1.33322 loss)
I0815 12:49:27.431694  8764 sgd_solver.cpp:136] Iteration 59900, lr = 0.00625625, m = 0.9
I0815 12:49:48.257220  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_60000.caffemodel
I0815 12:49:48.282263  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_60000.solverstate
I0815 12:49:48.286793  8764 solver.cpp:363] Sparsity after update:
I0815 12:49:48.289248  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:49:48.289258  8764 net.cpp:2192] conv1a_param_0(0.187) 
I0815 12:49:48.289268  8764 net.cpp:2192] conv1b_param_0(0.389) 
I0815 12:49:48.289278  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:49:48.289288  8764 net.cpp:2192] res2a_branch2a_param_0(0.399) 
I0815 12:49:48.289300  8764 net.cpp:2192] res2a_branch2b_param_0(0.396) 
I0815 12:49:48.289306  8764 net.cpp:2192] res3a_branch2a_param_0(0.399) 
I0815 12:49:48.289310  8764 net.cpp:2192] res3a_branch2b_param_0(0.399) 
I0815 12:49:48.289319  8764 net.cpp:2192] res4a_branch2a_param_0(0.399) 
I0815 12:49:48.289327  8764 net.cpp:2192] res4a_branch2b_param_0(0.399) 
I0815 12:49:48.289336  8764 net.cpp:2192] res5a_branch2a_param_0(0.4) 
I0815 12:49:48.289342  8764 net.cpp:2192] res5a_branch2b_param_0(0.399) 
I0815 12:49:48.289345  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (940215/2.86678e+06) 0.328
I0815 12:49:48.289362  8764 solver.cpp:509] Iteration 60000, Testing net (#0)
I0815 12:50:09.246868  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.565647
I0815 12:50:09.246948  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804528
I0815 12:50:09.246956  8764 solver.cpp:594]     Test net output #2: loss = 1.90823 (* 1 = 1.90823 loss)
I0815 12:50:09.246978  8764 solver.cpp:264] [MultiGPU] Tests completed in 20.957s
I0815 12:50:09.393518  8794 solver.cpp:409] Finding and applying sparsity: 0.41
I0815 12:50:35.388639  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:50:35.390632  8764 solver.cpp:312] Iteration 60000 (1.47151 iter/s, 67.9572s/100 iter), loss = 2.00603
I0815 12:50:35.390655  8764 solver.cpp:334]     Train net output #0: loss = 2.71763 (* 1 = 2.71763 loss)
I0815 12:50:35.390664  8764 sgd_solver.cpp:136] Iteration 60000, lr = 0.00625, m = 0.9
I0815 12:50:50.319118  8764 solver.cpp:312] Iteration 60100 (6.69879 iter/s, 14.9281s/100 iter), loss = 1.4814
I0815 12:50:50.319197  8764 solver.cpp:334]     Train net output #0: loss = 1.32399 (* 1 = 1.32399 loss)
I0815 12:50:50.319205  8764 sgd_solver.cpp:136] Iteration 60100, lr = 0.00624375, m = 0.9
I0815 12:51:04.589813  8764 solver.cpp:312] Iteration 60200 (7.00756 iter/s, 14.2703s/100 iter), loss = 1.34191
I0815 12:51:04.589839  8764 solver.cpp:334]     Train net output #0: loss = 1.08615 (* 1 = 1.08615 loss)
I0815 12:51:04.589845  8764 sgd_solver.cpp:136] Iteration 60200, lr = 0.0062375, m = 0.9
I0815 12:51:20.933516  8764 solver.cpp:312] Iteration 60300 (6.11873 iter/s, 16.3433s/100 iter), loss = 1.36182
I0815 12:51:20.933568  8764 solver.cpp:334]     Train net output #0: loss = 1.24709 (* 1 = 1.24709 loss)
I0815 12:51:20.933574  8764 sgd_solver.cpp:136] Iteration 60300, lr = 0.00623125, m = 0.9
I0815 12:51:39.237521  8764 solver.cpp:312] Iteration 60400 (5.46344 iter/s, 18.3035s/100 iter), loss = 1.36529
I0815 12:51:39.237593  8764 solver.cpp:334]     Train net output #0: loss = 1.41465 (* 1 = 1.41465 loss)
I0815 12:51:39.237614  8764 sgd_solver.cpp:136] Iteration 60400, lr = 0.006225, m = 0.9
I0815 12:51:57.480775  8764 solver.cpp:312] Iteration 60500 (5.48163 iter/s, 18.2428s/100 iter), loss = 1.17489
I0815 12:51:57.480880  8764 solver.cpp:334]     Train net output #0: loss = 0.977305 (* 1 = 0.977305 loss)
I0815 12:51:57.480897  8764 sgd_solver.cpp:136] Iteration 60500, lr = 0.00621875, m = 0.9
I0815 12:52:13.574512  8764 solver.cpp:312] Iteration 60600 (6.21377 iter/s, 16.0933s/100 iter), loss = 1.57528
I0815 12:52:13.574535  8764 solver.cpp:334]     Train net output #0: loss = 1.82204 (* 1 = 1.82204 loss)
I0815 12:52:13.574539  8764 sgd_solver.cpp:136] Iteration 60600, lr = 0.0062125, m = 0.9
I0815 12:52:31.176517  8764 solver.cpp:312] Iteration 60700 (5.68133 iter/s, 17.6015s/100 iter), loss = 1.6933
I0815 12:52:31.176568  8764 solver.cpp:334]     Train net output #0: loss = 1.5932 (* 1 = 1.5932 loss)
I0815 12:52:31.176574  8764 sgd_solver.cpp:136] Iteration 60700, lr = 0.00620625, m = 0.9
I0815 12:52:47.179056  8764 solver.cpp:312] Iteration 60800 (6.24918 iter/s, 16.0021s/100 iter), loss = 1.47919
I0815 12:52:47.179110  8764 solver.cpp:334]     Train net output #0: loss = 1.57729 (* 1 = 1.57729 loss)
I0815 12:52:47.179122  8764 sgd_solver.cpp:136] Iteration 60800, lr = 0.0062, m = 0.9
I0815 12:53:04.719851  8764 solver.cpp:312] Iteration 60900 (5.70116 iter/s, 17.5403s/100 iter), loss = 1.77734
I0815 12:53:04.719944  8764 solver.cpp:334]     Train net output #0: loss = 1.49476 (* 1 = 1.49476 loss)
I0815 12:53:04.719960  8764 sgd_solver.cpp:136] Iteration 60900, lr = 0.00619375, m = 0.9
I0815 12:53:24.871227  8764 solver.cpp:363] Sparsity after update:
I0815 12:53:24.881078  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:53:24.881089  8764 net.cpp:2192] conv1a_param_0(0.2) 
I0815 12:53:24.881098  8764 net.cpp:2192] conv1b_param_0(0.403) 
I0815 12:53:24.881101  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:53:24.881116  8764 net.cpp:2192] res2a_branch2a_param_0(0.41) 
I0815 12:53:24.881125  8764 net.cpp:2192] res2a_branch2b_param_0(0.41) 
I0815 12:53:24.881134  8764 net.cpp:2192] res3a_branch2a_param_0(0.41) 
I0815 12:53:24.881141  8764 net.cpp:2192] res3a_branch2b_param_0(0.41) 
I0815 12:53:24.881155  8764 net.cpp:2192] res4a_branch2a_param_0(0.41) 
I0815 12:53:24.881163  8764 net.cpp:2192] res4a_branch2b_param_0(0.41) 
I0815 12:53:24.881171  8764 net.cpp:2192] res5a_branch2a_param_0(0.41) 
I0815 12:53:24.881178  8764 net.cpp:2192] res5a_branch2b_param_0(0.41) 
I0815 12:53:24.881186  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (964276/2.86678e+06) 0.336
I0815 12:53:25.011606  8794 solver.cpp:409] Finding and applying sparsity: 0.42
I0815 12:54:04.278414  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:54:04.280534  8764 solver.cpp:312] Iteration 61000 (1.67901 iter/s, 59.5591s/100 iter), loss = 1.64483
I0815 12:54:04.280553  8764 solver.cpp:334]     Train net output #0: loss = 1.63877 (* 1 = 1.63877 loss)
I0815 12:54:04.280561  8764 sgd_solver.cpp:136] Iteration 61000, lr = 0.0061875, m = 0.9
I0815 12:54:19.228817  8764 solver.cpp:312] Iteration 61100 (6.68992 iter/s, 14.9479s/100 iter), loss = 1.374
I0815 12:54:19.228870  8764 solver.cpp:334]     Train net output #0: loss = 1.26007 (* 1 = 1.26007 loss)
I0815 12:54:19.228894  8764 sgd_solver.cpp:136] Iteration 61100, lr = 0.00618125, m = 0.9
I0815 12:54:35.333278  8764 solver.cpp:312] Iteration 61200 (6.20963 iter/s, 16.104s/100 iter), loss = 1.53721
I0815 12:54:35.333328  8764 solver.cpp:334]     Train net output #0: loss = 1.97372 (* 1 = 1.97372 loss)
I0815 12:54:35.333333  8764 sgd_solver.cpp:136] Iteration 61200, lr = 0.006175, m = 0.9
I0815 12:54:52.515410  8764 solver.cpp:312] Iteration 61300 (5.82016 iter/s, 17.1817s/100 iter), loss = 1.85532
I0815 12:54:52.515471  8764 solver.cpp:334]     Train net output #0: loss = 1.62121 (* 1 = 1.62121 loss)
I0815 12:54:52.515488  8764 sgd_solver.cpp:136] Iteration 61300, lr = 0.00616875, m = 0.9
I0815 12:55:09.773262  8764 solver.cpp:312] Iteration 61400 (5.79463 iter/s, 17.2574s/100 iter), loss = 1.39158
I0815 12:55:09.773346  8764 solver.cpp:334]     Train net output #0: loss = 1.40008 (* 1 = 1.40008 loss)
I0815 12:55:09.773353  8764 sgd_solver.cpp:136] Iteration 61400, lr = 0.0061625, m = 0.9
I0815 12:55:24.842167  8764 solver.cpp:312] Iteration 61500 (6.63637 iter/s, 15.0685s/100 iter), loss = 1.1842
I0815 12:55:24.842222  8764 solver.cpp:334]     Train net output #0: loss = 1.09671 (* 1 = 1.09671 loss)
I0815 12:55:24.842236  8764 sgd_solver.cpp:136] Iteration 61500, lr = 0.00615625, m = 0.9
I0815 12:55:43.032631  8764 solver.cpp:312] Iteration 61600 (5.49754 iter/s, 18.19s/100 iter), loss = 1.15654
I0815 12:55:43.032721  8764 solver.cpp:334]     Train net output #0: loss = 1.08556 (* 1 = 1.08556 loss)
I0815 12:55:43.032739  8764 sgd_solver.cpp:136] Iteration 61600, lr = 0.00615, m = 0.9
I0815 12:55:59.008615  8764 solver.cpp:312] Iteration 61700 (6.25957 iter/s, 15.9755s/100 iter), loss = 1.33313
I0815 12:55:59.008680  8764 solver.cpp:334]     Train net output #0: loss = 1.40643 (* 1 = 1.40643 loss)
I0815 12:55:59.008697  8764 sgd_solver.cpp:136] Iteration 61700, lr = 0.00614375, m = 0.9
I0815 12:56:16.761963  8764 solver.cpp:312] Iteration 61800 (5.6329 iter/s, 17.7529s/100 iter), loss = 1.2476
I0815 12:56:16.762058  8764 solver.cpp:334]     Train net output #0: loss = 1.24123 (* 1 = 1.24123 loss)
I0815 12:56:16.762075  8764 sgd_solver.cpp:136] Iteration 61800, lr = 0.0061375, m = 0.9
I0815 12:56:31.726631  8764 solver.cpp:312] Iteration 61900 (6.68259 iter/s, 14.9643s/100 iter), loss = 1.70402
I0815 12:56:31.726699  8764 solver.cpp:334]     Train net output #0: loss = 1.52929 (* 1 = 1.52929 loss)
I0815 12:56:31.726716  8764 sgd_solver.cpp:136] Iteration 61900, lr = 0.00613125, m = 0.9
I0815 12:56:33.307108  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 12:56:46.225857  8764 solver.cpp:363] Sparsity after update:
I0815 12:56:46.234266  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 12:56:46.234277  8764 net.cpp:2192] conv1a_param_0(0.2) 
I0815 12:56:46.234284  8764 net.cpp:2192] conv1b_param_0(0.417) 
I0815 12:56:46.234288  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 12:56:46.234293  8764 net.cpp:2192] res2a_branch2a_param_0(0.417) 
I0815 12:56:46.234297  8764 net.cpp:2192] res2a_branch2b_param_0(0.417) 
I0815 12:56:46.234300  8764 net.cpp:2192] res3a_branch2a_param_0(0.418) 
I0815 12:56:46.234302  8764 net.cpp:2192] res3a_branch2b_param_0(0.417) 
I0815 12:56:46.234305  8764 net.cpp:2192] res4a_branch2a_param_0(0.419) 
I0815 12:56:46.234308  8764 net.cpp:2192] res4a_branch2b_param_0(0.418) 
I0815 12:56:46.234311  8764 net.cpp:2192] res5a_branch2a_param_0(0.42) 
I0815 12:56:46.234314  8764 net.cpp:2192] res5a_branch2b_param_0(0.419) 
I0815 12:56:46.234318  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (986901/2.86678e+06) 0.344
I0815 12:56:46.234329  8764 solver.cpp:509] Iteration 62000, Testing net (#0)
I0815 12:57:07.583370  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.578294
I0815 12:57:07.583442  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.804409
I0815 12:57:07.583449  8764 solver.cpp:594]     Test net output #2: loss = 1.87143 (* 1 = 1.87143 loss)
I0815 12:57:07.583472  8764 solver.cpp:264] [MultiGPU] Tests completed in 21.3486s
I0815 12:57:07.732343  8794 solver.cpp:409] Finding and applying sparsity: 0.43
I0815 12:57:34.147594  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 12:57:34.149576  8764 solver.cpp:312] Iteration 62000 (1.60202 iter/s, 62.4212s/100 iter), loss = 1.35888
I0815 12:57:34.149598  8764 solver.cpp:334]     Train net output #0: loss = 1.30671 (* 1 = 1.30671 loss)
I0815 12:57:34.149606  8764 sgd_solver.cpp:136] Iteration 62000, lr = 0.006125, m = 0.9
I0815 12:57:49.134184  8764 solver.cpp:312] Iteration 62100 (6.6737 iter/s, 14.9842s/100 iter), loss = 1.68422
I0815 12:57:49.134286  8764 solver.cpp:334]     Train net output #0: loss = 1.87793 (* 1 = 1.87793 loss)
I0815 12:57:49.134305  8764 sgd_solver.cpp:136] Iteration 62100, lr = 0.00611875, m = 0.9
I0815 12:58:05.782207  8764 solver.cpp:312] Iteration 62200 (6.00689 iter/s, 16.6476s/100 iter), loss = 1.27317
I0815 12:58:05.782246  8764 solver.cpp:334]     Train net output #0: loss = 1.30325 (* 1 = 1.30325 loss)
I0815 12:58:05.782255  8764 sgd_solver.cpp:136] Iteration 62200, lr = 0.0061125, m = 0.9
I0815 12:58:23.413094  8764 solver.cpp:312] Iteration 62300 (5.67202 iter/s, 17.6304s/100 iter), loss = 1.72359
I0815 12:58:23.413146  8764 solver.cpp:334]     Train net output #0: loss = 2.19357 (* 1 = 2.19357 loss)
I0815 12:58:23.413153  8764 sgd_solver.cpp:136] Iteration 62300, lr = 0.00610625, m = 0.9
I0815 12:58:39.460616  8764 solver.cpp:312] Iteration 62400 (6.23166 iter/s, 16.0471s/100 iter), loss = 1.65933
I0815 12:58:39.460680  8764 solver.cpp:334]     Train net output #0: loss = 1.5394 (* 1 = 1.5394 loss)
I0815 12:58:39.460703  8764 sgd_solver.cpp:136] Iteration 62400, lr = 0.0061, m = 0.9
I0815 12:58:58.322710  8764 solver.cpp:312] Iteration 62500 (5.30179 iter/s, 18.8616s/100 iter), loss = 1.3287
I0815 12:58:58.322789  8764 solver.cpp:334]     Train net output #0: loss = 1.06271 (* 1 = 1.06271 loss)
I0815 12:58:58.322801  8764 sgd_solver.cpp:136] Iteration 62500, lr = 0.00609375, m = 0.9
I0815 12:59:17.239631  8764 solver.cpp:312] Iteration 62600 (5.28642 iter/s, 18.9164s/100 iter), loss = 1.2398
I0815 12:59:17.239677  8764 solver.cpp:334]     Train net output #0: loss = 1.03079 (* 1 = 1.03079 loss)
I0815 12:59:17.239686  8764 sgd_solver.cpp:136] Iteration 62600, lr = 0.0060875, m = 0.9
I0815 12:59:34.666725  8764 solver.cpp:312] Iteration 62700 (5.73835 iter/s, 17.4266s/100 iter), loss = 1.37029
I0815 12:59:34.666805  8764 solver.cpp:334]     Train net output #0: loss = 1.68098 (* 1 = 1.68098 loss)
I0815 12:59:34.666811  8764 sgd_solver.cpp:136] Iteration 62700, lr = 0.00608125, m = 0.9
I0815 12:59:50.721254  8764 solver.cpp:312] Iteration 62800 (6.22894 iter/s, 16.0541s/100 iter), loss = 1.42268
I0815 12:59:50.721282  8764 solver.cpp:334]     Train net output #0: loss = 1.46988 (* 1 = 1.46988 loss)
I0815 12:59:50.721287  8764 sgd_solver.cpp:136] Iteration 62800, lr = 0.006075, m = 0.9
I0815 13:00:07.722420  8764 solver.cpp:312] Iteration 62900 (5.88211 iter/s, 17.0007s/100 iter), loss = 1.57324
I0815 13:00:07.736161  8764 solver.cpp:334]     Train net output #0: loss = 1.46042 (* 1 = 1.46042 loss)
I0815 13:00:07.736181  8764 sgd_solver.cpp:136] Iteration 62900, lr = 0.00606875, m = 0.9
I0815 13:00:25.893345  8764 solver.cpp:363] Sparsity after update:
I0815 13:00:25.901809  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:00:25.901832  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:00:25.901849  8764 net.cpp:2192] conv1b_param_0(0.417) 
I0815 13:00:25.901859  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:00:25.901868  8764 net.cpp:2192] res2a_branch2a_param_0(0.427) 
I0815 13:00:25.901877  8764 net.cpp:2192] res2a_branch2b_param_0(0.424) 
I0815 13:00:25.901887  8764 net.cpp:2192] res3a_branch2a_param_0(0.429) 
I0815 13:00:25.901895  8764 net.cpp:2192] res3a_branch2b_param_0(0.427) 
I0815 13:00:25.901904  8764 net.cpp:2192] res4a_branch2a_param_0(0.43) 
I0815 13:00:25.901913  8764 net.cpp:2192] res4a_branch2b_param_0(0.429) 
I0815 13:00:25.901922  8764 net.cpp:2192] res5a_branch2a_param_0(0.43) 
I0815 13:00:25.901932  8764 net.cpp:2192] res5a_branch2b_param_0(0.43) 
I0815 13:00:25.901939  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.01087e+06/2.86678e+06) 0.353
I0815 13:00:26.065733  8794 solver.cpp:409] Finding and applying sparsity: 0.44
I0815 13:01:09.699960  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:01:09.702064  8764 solver.cpp:312] Iteration 63000 (1.61348 iter/s, 61.9779s/100 iter), loss = 1.26998
I0815 13:01:09.702080  8764 solver.cpp:334]     Train net output #0: loss = 1.04654 (* 1 = 1.04654 loss)
I0815 13:01:09.702086  8764 sgd_solver.cpp:136] Iteration 63000, lr = 0.0060625, m = 0.9
I0815 13:01:27.374088  8764 solver.cpp:312] Iteration 63100 (5.65882 iter/s, 17.6715s/100 iter), loss = 1.80403
I0815 13:01:27.374135  8764 solver.cpp:334]     Train net output #0: loss = 1.5742 (* 1 = 1.5742 loss)
I0815 13:01:27.374143  8764 sgd_solver.cpp:136] Iteration 63100, lr = 0.00605625, m = 0.9
I0815 13:01:43.115913  8764 solver.cpp:312] Iteration 63200 (6.35268 iter/s, 15.7414s/100 iter), loss = 1.6208
I0815 13:01:43.120846  8764 solver.cpp:334]     Train net output #0: loss = 1.52821 (* 1 = 1.52821 loss)
I0815 13:01:43.120852  8764 sgd_solver.cpp:136] Iteration 63200, lr = 0.00605, m = 0.9
I0815 13:01:59.462496  8764 solver.cpp:312] Iteration 63300 (6.11766 iter/s, 16.3461s/100 iter), loss = 1.13974
I0815 13:01:59.462538  8764 solver.cpp:334]     Train net output #0: loss = 1.06235 (* 1 = 1.06235 loss)
I0815 13:01:59.462554  8764 sgd_solver.cpp:136] Iteration 63300, lr = 0.00604375, m = 0.9
I0815 13:02:17.056931  8764 solver.cpp:312] Iteration 63400 (5.68377 iter/s, 17.594s/100 iter), loss = 1.20662
I0815 13:02:17.056995  8764 solver.cpp:334]     Train net output #0: loss = 1.47672 (* 1 = 1.47672 loss)
I0815 13:02:17.057000  8764 sgd_solver.cpp:136] Iteration 63400, lr = 0.0060375, m = 0.9
I0815 13:02:33.684144  8764 solver.cpp:312] Iteration 63500 (6.01441 iter/s, 16.6267s/100 iter), loss = 1.25218
I0815 13:02:33.684171  8764 solver.cpp:334]     Train net output #0: loss = 1.31544 (* 1 = 1.31544 loss)
I0815 13:02:33.684176  8764 sgd_solver.cpp:136] Iteration 63500, lr = 0.00603125, m = 0.9
I0815 13:02:52.544231  8764 solver.cpp:312] Iteration 63600 (5.30235 iter/s, 18.8596s/100 iter), loss = 1.59148
I0815 13:02:52.544293  8764 solver.cpp:334]     Train net output #0: loss = 1.51833 (* 1 = 1.51833 loss)
I0815 13:02:52.544299  8764 sgd_solver.cpp:136] Iteration 63600, lr = 0.006025, m = 0.9
I0815 13:03:09.138936  8764 solver.cpp:312] Iteration 63700 (6.02619 iter/s, 16.5942s/100 iter), loss = 1.23551
I0815 13:03:09.138964  8764 solver.cpp:334]     Train net output #0: loss = 1.29027 (* 1 = 1.29027 loss)
I0815 13:03:09.138969  8764 sgd_solver.cpp:136] Iteration 63700, lr = 0.00601875, m = 0.9
I0815 13:03:27.566093  8764 solver.cpp:312] Iteration 63800 (5.42692 iter/s, 18.4267s/100 iter), loss = 1.33243
I0815 13:03:27.566195  8764 solver.cpp:334]     Train net output #0: loss = 1.22377 (* 1 = 1.22377 loss)
I0815 13:03:27.566217  8764 sgd_solver.cpp:136] Iteration 63800, lr = 0.0060125, m = 0.9
I0815 13:03:43.835019  8764 solver.cpp:312] Iteration 63900 (6.14686 iter/s, 16.2685s/100 iter), loss = 1.70912
I0815 13:03:43.835047  8764 solver.cpp:334]     Train net output #0: loss = 2.01047 (* 1 = 2.01047 loss)
I0815 13:03:43.835054  8764 sgd_solver.cpp:136] Iteration 63900, lr = 0.00600625, m = 0.9
I0815 13:03:59.483263  8764 solver.cpp:363] Sparsity after update:
I0815 13:03:59.488739  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:03:59.488752  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:03:59.488761  8764 net.cpp:2192] conv1b_param_0(0.431) 
I0815 13:03:59.488765  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:03:59.488772  8764 net.cpp:2192] res2a_branch2a_param_0(0.438) 
I0815 13:03:59.488775  8764 net.cpp:2192] res2a_branch2b_param_0(0.438) 
I0815 13:03:59.488780  8764 net.cpp:2192] res3a_branch2a_param_0(0.439) 
I0815 13:03:59.488785  8764 net.cpp:2192] res3a_branch2b_param_0(0.438) 
I0815 13:03:59.488790  8764 net.cpp:2192] res4a_branch2a_param_0(0.439) 
I0815 13:03:59.488793  8764 net.cpp:2192] res4a_branch2b_param_0(0.439) 
I0815 13:03:59.488796  8764 net.cpp:2192] res5a_branch2a_param_0(0.44) 
I0815 13:03:59.488801  8764 net.cpp:2192] res5a_branch2b_param_0(0.439) 
I0815 13:03:59.488803  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.03413e+06/2.86678e+06) 0.361
I0815 13:03:59.488814  8764 solver.cpp:509] Iteration 64000, Testing net (#0)
I0815 13:04:26.445896  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.573353
I0815 13:04:26.445919  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.805527
I0815 13:04:26.445924  8764 solver.cpp:594]     Test net output #2: loss = 1.87058 (* 1 = 1.87058 loss)
I0815 13:04:26.445947  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9564s
I0815 13:04:26.606147  8794 solver.cpp:409] Finding and applying sparsity: 0.45
I0815 13:04:54.008700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:04:54.010720  8764 solver.cpp:312] Iteration 64000 (1.42503 iter/s, 70.1738s/100 iter), loss = 1.5287
I0815 13:04:54.010745  8764 solver.cpp:334]     Train net output #0: loss = 2.04011 (* 1 = 2.04011 loss)
I0815 13:04:54.010752  8764 sgd_solver.cpp:136] Iteration 64000, lr = 0.006, m = 0.9
I0815 13:05:10.551008  8764 solver.cpp:312] Iteration 64100 (6.04601 iter/s, 16.5398s/100 iter), loss = 1.55201
I0815 13:05:10.551059  8764 solver.cpp:334]     Train net output #0: loss = 1.23312 (* 1 = 1.23312 loss)
I0815 13:05:10.551071  8764 sgd_solver.cpp:136] Iteration 64100, lr = 0.00599375, m = 0.9
I0815 13:05:27.030252  8764 solver.cpp:312] Iteration 64200 (6.06841 iter/s, 16.4788s/100 iter), loss = 1.12914
I0815 13:05:27.030345  8764 solver.cpp:334]     Train net output #0: loss = 1.41669 (* 1 = 1.41669 loss)
I0815 13:05:27.030369  8764 sgd_solver.cpp:136] Iteration 64200, lr = 0.0059875, m = 0.9
I0815 13:05:45.112574  8764 solver.cpp:312] Iteration 64300 (5.53042 iter/s, 18.0818s/100 iter), loss = 1.76796
I0815 13:05:45.112645  8764 solver.cpp:334]     Train net output #0: loss = 1.83705 (* 1 = 1.83705 loss)
I0815 13:05:45.112665  8764 sgd_solver.cpp:136] Iteration 64300, lr = 0.00598125, m = 0.9
I0815 13:06:02.738343  8764 solver.cpp:312] Iteration 64400 (5.67367 iter/s, 17.6253s/100 iter), loss = 1.58658
I0815 13:06:02.738524  8764 solver.cpp:334]     Train net output #0: loss = 1.5808 (* 1 = 1.5808 loss)
I0815 13:06:02.738544  8764 sgd_solver.cpp:136] Iteration 64400, lr = 0.005975, m = 0.9
I0815 13:06:20.789386  8764 solver.cpp:312] Iteration 64500 (5.54 iter/s, 18.0505s/100 iter), loss = 1.18877
I0815 13:06:20.789425  8764 solver.cpp:334]     Train net output #0: loss = 1.639 (* 1 = 1.639 loss)
I0815 13:06:20.789435  8764 sgd_solver.cpp:136] Iteration 64500, lr = 0.00596875, m = 0.9
I0815 13:06:38.402802  8764 solver.cpp:312] Iteration 64600 (5.67765 iter/s, 17.6129s/100 iter), loss = 1.7929
I0815 13:06:38.403008  8764 solver.cpp:334]     Train net output #0: loss = 1.73411 (* 1 = 1.73411 loss)
I0815 13:06:38.403086  8764 sgd_solver.cpp:136] Iteration 64600, lr = 0.0059625, m = 0.9
I0815 13:06:54.630553  8764 solver.cpp:312] Iteration 64700 (6.16245 iter/s, 16.2273s/100 iter), loss = 1.7429
I0815 13:06:54.630580  8764 solver.cpp:334]     Train net output #0: loss = 1.84382 (* 1 = 1.84382 loss)
I0815 13:06:54.630585  8764 sgd_solver.cpp:136] Iteration 64700, lr = 0.00595625, m = 0.9
I0815 13:07:10.981813  8764 solver.cpp:312] Iteration 64800 (6.11591 iter/s, 16.3508s/100 iter), loss = 1.68711
I0815 13:07:10.981878  8764 solver.cpp:334]     Train net output #0: loss = 1.77009 (* 1 = 1.77009 loss)
I0815 13:07:10.981885  8764 sgd_solver.cpp:136] Iteration 64800, lr = 0.00595, m = 0.9
I0815 13:07:25.416648  8764 solver.cpp:312] Iteration 64900 (6.92787 iter/s, 14.4344s/100 iter), loss = 1.4653
I0815 13:07:25.416713  8764 solver.cpp:334]     Train net output #0: loss = 1.47136 (* 1 = 1.47136 loss)
I0815 13:07:25.416730  8764 sgd_solver.cpp:136] Iteration 64900, lr = 0.00594375, m = 0.9
I0815 13:07:40.959288  8764 solver.cpp:363] Sparsity after update:
I0815 13:07:40.977955  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:07:40.978016  8764 net.cpp:2192] conv1a_param_0(0.213) 
I0815 13:07:40.978042  8764 net.cpp:2192] conv1b_param_0(0.444) 
I0815 13:07:40.978056  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:07:40.978070  8764 net.cpp:2192] res2a_branch2a_param_0(0.448) 
I0815 13:07:40.978083  8764 net.cpp:2192] res2a_branch2b_param_0(0.444) 
I0815 13:07:40.978096  8764 net.cpp:2192] res3a_branch2a_param_0(0.45) 
I0815 13:07:40.978109  8764 net.cpp:2192] res3a_branch2b_param_0(0.448) 
I0815 13:07:40.978121  8764 net.cpp:2192] res4a_branch2a_param_0(0.45) 
I0815 13:07:40.978133  8764 net.cpp:2192] res4a_branch2b_param_0(0.45) 
I0815 13:07:40.978147  8764 net.cpp:2192] res5a_branch2a_param_0(0.45) 
I0815 13:07:40.978159  8764 net.cpp:2192] res5a_branch2b_param_0(0.45) 
I0815 13:07:40.978173  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.0581e+06/2.86678e+06) 0.369
I0815 13:07:41.145735  8794 solver.cpp:409] Finding and applying sparsity: 0.46
I0815 13:08:17.089399  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:08:17.091718  8764 solver.cpp:312] Iteration 65000 (1.93522 iter/s, 51.6737s/100 iter), loss = 1.17349
I0815 13:08:17.091755  8764 solver.cpp:334]     Train net output #0: loss = 1.41889 (* 1 = 1.41889 loss)
I0815 13:08:17.091773  8764 sgd_solver.cpp:136] Iteration 65000, lr = 0.0059375, m = 0.9
I0815 13:08:37.717114  8764 solver.cpp:312] Iteration 65100 (4.84853 iter/s, 20.6248s/100 iter), loss = 1.3322
I0815 13:08:37.717187  8764 solver.cpp:334]     Train net output #0: loss = 1.46155 (* 1 = 1.46155 loss)
I0815 13:08:37.717213  8764 sgd_solver.cpp:136] Iteration 65100, lr = 0.00593125, m = 0.9
I0815 13:08:58.872285  8764 solver.cpp:312] Iteration 65200 (4.72711 iter/s, 21.1546s/100 iter), loss = 1.58034
I0815 13:08:58.872376  8764 solver.cpp:334]     Train net output #0: loss = 1.53174 (* 1 = 1.53174 loss)
I0815 13:08:58.872390  8764 sgd_solver.cpp:136] Iteration 65200, lr = 0.005925, m = 0.9
I0815 13:09:16.897444  8764 solver.cpp:312] Iteration 65300 (5.54795 iter/s, 18.0247s/100 iter), loss = 1.49843
I0815 13:09:16.897585  8764 solver.cpp:334]     Train net output #0: loss = 1.68334 (* 1 = 1.68334 loss)
I0815 13:09:16.897601  8764 sgd_solver.cpp:136] Iteration 65300, lr = 0.00591875, m = 0.9
I0815 13:09:36.164700  8764 solver.cpp:312] Iteration 65400 (5.1903 iter/s, 19.2667s/100 iter), loss = 1.37127
I0815 13:09:36.164757  8764 solver.cpp:334]     Train net output #0: loss = 1.62407 (* 1 = 1.62407 loss)
I0815 13:09:36.164763  8764 sgd_solver.cpp:136] Iteration 65400, lr = 0.0059125, m = 0.9
I0815 13:09:57.621767  8764 solver.cpp:312] Iteration 65500 (4.66061 iter/s, 21.4564s/100 iter), loss = 1.28434
I0815 13:09:57.622195  8764 solver.cpp:334]     Train net output #0: loss = 1.13793 (* 1 = 1.13793 loss)
I0815 13:09:57.622387  8764 sgd_solver.cpp:136] Iteration 65500, lr = 0.00590625, m = 0.9
I0815 13:10:15.129700  8764 solver.cpp:312] Iteration 65600 (5.71185 iter/s, 17.5075s/100 iter), loss = 1.33897
I0815 13:10:15.129811  8764 solver.cpp:334]     Train net output #0: loss = 1.06932 (* 1 = 1.06932 loss)
I0815 13:10:15.129828  8764 sgd_solver.cpp:136] Iteration 65600, lr = 0.0059, m = 0.9
I0815 13:10:32.135716  8764 solver.cpp:312] Iteration 65700 (5.88044 iter/s, 17.0055s/100 iter), loss = 1.16395
I0815 13:10:32.135764  8764 solver.cpp:334]     Train net output #0: loss = 1.08777 (* 1 = 1.08777 loss)
I0815 13:10:32.135776  8764 sgd_solver.cpp:136] Iteration 65700, lr = 0.00589375, m = 0.9
I0815 13:10:40.084488  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:10:56.500113  8764 solver.cpp:312] Iteration 65800 (4.10447 iter/s, 24.3637s/100 iter), loss = 1.42356
I0815 13:10:56.500423  8764 solver.cpp:334]     Train net output #0: loss = 1.32615 (* 1 = 1.32615 loss)
I0815 13:10:56.500437  8764 sgd_solver.cpp:136] Iteration 65800, lr = 0.0058875, m = 0.9
I0815 13:11:17.671625  8764 solver.cpp:312] Iteration 65900 (4.72346 iter/s, 21.1709s/100 iter), loss = 1.60577
I0815 13:11:17.671679  8764 solver.cpp:334]     Train net output #0: loss = 2.11977 (* 1 = 2.11977 loss)
I0815 13:11:17.671689  8764 sgd_solver.cpp:136] Iteration 65900, lr = 0.00588125, m = 0.9
I0815 13:11:35.522290  8764 solver.cpp:363] Sparsity after update:
I0815 13:11:35.525642  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:11:35.525657  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:11:35.525662  8764 net.cpp:2192] conv1b_param_0(0.458) 
I0815 13:11:35.525665  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:11:35.525667  8764 net.cpp:2192] res2a_branch2a_param_0(0.458) 
I0815 13:11:35.525669  8764 net.cpp:2192] res2a_branch2b_param_0(0.458) 
I0815 13:11:35.525671  8764 net.cpp:2192] res3a_branch2a_param_0(0.458) 
I0815 13:11:35.525673  8764 net.cpp:2192] res3a_branch2b_param_0(0.458) 
I0815 13:11:35.525676  8764 net.cpp:2192] res4a_branch2a_param_0(0.459) 
I0815 13:11:35.525678  8764 net.cpp:2192] res4a_branch2b_param_0(0.458) 
I0815 13:11:35.525681  8764 net.cpp:2192] res5a_branch2a_param_0(0.46) 
I0815 13:11:35.525682  8764 net.cpp:2192] res5a_branch2b_param_0(0.459) 
I0815 13:11:35.525684  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.08101e+06/2.86678e+06) 0.377
I0815 13:11:35.525693  8764 solver.cpp:509] Iteration 66000, Testing net (#0)
I0815 13:12:06.323820  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571529
I0815 13:12:06.323870  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.80588
I0815 13:12:06.323876  8764 solver.cpp:594]     Test net output #2: loss = 1.86551 (* 1 = 1.86551 loss)
I0815 13:12:06.336163  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.8096s
I0815 13:12:06.471952  8794 solver.cpp:409] Finding and applying sparsity: 0.47
I0815 13:12:52.640643  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:12:52.643153  8764 solver.cpp:312] Iteration 66000 (1.05298 iter/s, 94.9689s/100 iter), loss = 1.38213
I0815 13:12:52.643193  8764 solver.cpp:334]     Train net output #0: loss = 1.51744 (* 1 = 1.51744 loss)
I0815 13:12:52.643211  8764 sgd_solver.cpp:136] Iteration 66000, lr = 0.005875, m = 0.9
I0815 13:13:13.029465  8764 solver.cpp:312] Iteration 66100 (4.90539 iter/s, 20.3857s/100 iter), loss = 1.11908
I0815 13:13:13.029531  8764 solver.cpp:334]     Train net output #0: loss = 0.744224 (* 1 = 0.744224 loss)
I0815 13:13:13.029552  8764 sgd_solver.cpp:136] Iteration 66100, lr = 0.00586875, m = 0.9
I0815 13:13:30.236754  8764 solver.cpp:312] Iteration 66200 (5.81165 iter/s, 17.2068s/100 iter), loss = 1.29443
I0815 13:13:30.236821  8764 solver.cpp:334]     Train net output #0: loss = 1.67036 (* 1 = 1.67036 loss)
I0815 13:13:30.236827  8764 sgd_solver.cpp:136] Iteration 66200, lr = 0.0058625, m = 0.9
I0815 13:13:47.692559  8764 solver.cpp:312] Iteration 66300 (5.72891 iter/s, 17.4553s/100 iter), loss = 1.4839
I0815 13:13:47.692612  8764 solver.cpp:334]     Train net output #0: loss = 1.37712 (* 1 = 1.37712 loss)
I0815 13:13:47.692627  8764 sgd_solver.cpp:136] Iteration 66300, lr = 0.00585625, m = 0.9
I0815 13:14:05.624788  8764 solver.cpp:312] Iteration 66400 (5.57671 iter/s, 17.9317s/100 iter), loss = 1.1872
I0815 13:14:05.624923  8764 solver.cpp:334]     Train net output #0: loss = 1.14941 (* 1 = 1.14941 loss)
I0815 13:14:05.624938  8764 sgd_solver.cpp:136] Iteration 66400, lr = 0.00585, m = 0.9
I0815 13:14:27.483467  8764 solver.cpp:312] Iteration 66500 (4.57497 iter/s, 21.8581s/100 iter), loss = 1.39026
I0815 13:14:27.483690  8764 solver.cpp:334]     Train net output #0: loss = 1.47278 (* 1 = 1.47278 loss)
I0815 13:14:27.483800  8764 sgd_solver.cpp:136] Iteration 66500, lr = 0.00584375, m = 0.9
I0815 13:14:47.518101  8764 solver.cpp:312] Iteration 66600 (4.9915 iter/s, 20.0341s/100 iter), loss = 1.17274
I0815 13:14:47.518221  8764 solver.cpp:334]     Train net output #0: loss = 1.20311 (* 1 = 1.20311 loss)
I0815 13:14:47.518239  8764 sgd_solver.cpp:136] Iteration 66600, lr = 0.0058375, m = 0.9
I0815 13:15:03.512670  8764 solver.cpp:312] Iteration 66700 (6.25229 iter/s, 15.9941s/100 iter), loss = 1.51075
I0815 13:15:03.512734  8764 solver.cpp:334]     Train net output #0: loss = 1.62757 (* 1 = 1.62757 loss)
I0815 13:15:03.512758  8764 sgd_solver.cpp:136] Iteration 66700, lr = 0.00583125, m = 0.9
I0815 13:15:21.602460  8764 solver.cpp:312] Iteration 66800 (5.52813 iter/s, 18.0893s/100 iter), loss = 1.21686
I0815 13:15:21.602588  8764 solver.cpp:334]     Train net output #0: loss = 1.0644 (* 1 = 1.0644 loss)
I0815 13:15:21.602614  8764 sgd_solver.cpp:136] Iteration 66800, lr = 0.005825, m = 0.9
I0815 13:15:38.718693  8764 solver.cpp:312] Iteration 66900 (5.84257 iter/s, 17.1158s/100 iter), loss = 1.37457
I0815 13:15:38.718729  8764 solver.cpp:334]     Train net output #0: loss = 1.30464 (* 1 = 1.30464 loss)
I0815 13:15:38.718735  8764 sgd_solver.cpp:136] Iteration 66900, lr = 0.00581875, m = 0.9
I0815 13:16:00.345567  8764 solver.cpp:363] Sparsity after update:
I0815 13:16:00.356160  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:16:00.356178  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:16:00.356204  8764 net.cpp:2192] conv1b_param_0(0.458) 
I0815 13:16:00.356214  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:16:00.356230  8764 net.cpp:2192] res2a_branch2a_param_0(0.469) 
I0815 13:16:00.356236  8764 net.cpp:2192] res2a_branch2b_param_0(0.465) 
I0815 13:16:00.356250  8764 net.cpp:2192] res3a_branch2a_param_0(0.469) 
I0815 13:16:00.356257  8764 net.cpp:2192] res3a_branch2b_param_0(0.469) 
I0815 13:16:00.356271  8764 net.cpp:2192] res4a_branch2a_param_0(0.47) 
I0815 13:16:00.356277  8764 net.cpp:2192] res4a_branch2b_param_0(0.469) 
I0815 13:16:00.356286  8764 net.cpp:2192] res5a_branch2a_param_0(0.47) 
I0815 13:16:00.356298  8764 net.cpp:2192] res5a_branch2b_param_0(0.47) 
I0815 13:16:00.356305  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.10495e+06/2.86678e+06) 0.385
I0815 13:16:00.571846  8794 solver.cpp:409] Finding and applying sparsity: 0.48
I0815 13:16:48.113127  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:16:48.115517  8764 solver.cpp:312] Iteration 67000 (1.44103 iter/s, 69.3949s/100 iter), loss = 1.52261
I0815 13:16:48.115556  8764 solver.cpp:334]     Train net output #0: loss = 1.69908 (* 1 = 1.69908 loss)
I0815 13:16:48.115569  8764 sgd_solver.cpp:136] Iteration 67000, lr = 0.0058125, m = 0.9
I0815 13:17:06.513216  8764 solver.cpp:312] Iteration 67100 (5.43561 iter/s, 18.3972s/100 iter), loss = 1.20898
I0815 13:17:06.513243  8764 solver.cpp:334]     Train net output #0: loss = 1.11067 (* 1 = 1.11067 loss)
I0815 13:17:06.513247  8764 sgd_solver.cpp:136] Iteration 67100, lr = 0.00580625, m = 0.9
I0815 13:17:23.922999  8764 solver.cpp:312] Iteration 67200 (5.74406 iter/s, 17.4093s/100 iter), loss = 1.42368
I0815 13:17:23.923099  8764 solver.cpp:334]     Train net output #0: loss = 1.52245 (* 1 = 1.52245 loss)
I0815 13:17:23.923115  8764 sgd_solver.cpp:136] Iteration 67200, lr = 0.0058, m = 0.9
I0815 13:17:40.079193  8764 solver.cpp:312] Iteration 67300 (6.18975 iter/s, 16.1557s/100 iter), loss = 1.31919
I0815 13:17:40.079252  8764 solver.cpp:334]     Train net output #0: loss = 1.21525 (* 1 = 1.21525 loss)
I0815 13:17:40.079262  8764 sgd_solver.cpp:136] Iteration 67300, lr = 0.00579375, m = 0.9
I0815 13:17:57.210067  8764 solver.cpp:312] Iteration 67400 (5.83758 iter/s, 17.1304s/100 iter), loss = 1.18411
I0815 13:17:57.210155  8764 solver.cpp:334]     Train net output #0: loss = 1.04356 (* 1 = 1.04356 loss)
I0815 13:17:57.210176  8764 sgd_solver.cpp:136] Iteration 67400, lr = 0.0057875, m = 0.9
I0815 13:18:14.100121  8764 solver.cpp:312] Iteration 67500 (5.92081 iter/s, 16.8896s/100 iter), loss = 1.30163
I0815 13:18:14.100157  8764 solver.cpp:334]     Train net output #0: loss = 1.12928 (* 1 = 1.12928 loss)
I0815 13:18:14.100162  8764 sgd_solver.cpp:136] Iteration 67500, lr = 0.00578125, m = 0.9
I0815 13:18:31.593940  8764 solver.cpp:312] Iteration 67600 (5.71646 iter/s, 17.4933s/100 iter), loss = 1.19478
I0815 13:18:31.594002  8764 solver.cpp:334]     Train net output #0: loss = 1.10683 (* 1 = 1.10683 loss)
I0815 13:18:31.594008  8764 sgd_solver.cpp:136] Iteration 67600, lr = 0.005775, m = 0.9
I0815 13:18:46.327811  8764 solver.cpp:312] Iteration 67700 (6.78727 iter/s, 14.7335s/100 iter), loss = 1.16353
I0815 13:18:46.327875  8764 solver.cpp:334]     Train net output #0: loss = 1.07391 (* 1 = 1.07391 loss)
I0815 13:18:46.327893  8764 sgd_solver.cpp:136] Iteration 67700, lr = 0.00576875, m = 0.9
I0815 13:19:03.612557  8764 solver.cpp:312] Iteration 67800 (5.78561 iter/s, 17.2843s/100 iter), loss = 1.3854
I0815 13:19:03.612644  8764 solver.cpp:334]     Train net output #0: loss = 1.33172 (* 1 = 1.33172 loss)
I0815 13:19:03.612653  8764 sgd_solver.cpp:136] Iteration 67800, lr = 0.0057625, m = 0.9
I0815 13:19:22.164736  8764 solver.cpp:312] Iteration 67900 (5.39035 iter/s, 18.5517s/100 iter), loss = 1.54934
I0815 13:19:22.164779  8764 solver.cpp:334]     Train net output #0: loss = 1.60223 (* 1 = 1.60223 loss)
I0815 13:19:22.164786  8764 sgd_solver.cpp:136] Iteration 67900, lr = 0.00575625, m = 0.9
I0815 13:19:40.792330  8764 solver.cpp:363] Sparsity after update:
I0815 13:19:40.797329  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:19:40.797341  8764 net.cpp:2192] conv1a_param_0(0.226) 
I0815 13:19:40.797349  8764 net.cpp:2192] conv1b_param_0(0.472) 
I0815 13:19:40.797353  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:19:40.797358  8764 net.cpp:2192] res2a_branch2a_param_0(0.479) 
I0815 13:19:40.797361  8764 net.cpp:2192] res2a_branch2b_param_0(0.479) 
I0815 13:19:40.797364  8764 net.cpp:2192] res3a_branch2a_param_0(0.479) 
I0815 13:19:40.797368  8764 net.cpp:2192] res3a_branch2b_param_0(0.479) 
I0815 13:19:40.797370  8764 net.cpp:2192] res4a_branch2a_param_0(0.479) 
I0815 13:19:40.797374  8764 net.cpp:2192] res4a_branch2b_param_0(0.479) 
I0815 13:19:40.797375  8764 net.cpp:2192] res5a_branch2a_param_0(0.48) 
I0815 13:19:40.797379  8764 net.cpp:2192] res5a_branch2b_param_0(0.479) 
I0815 13:19:40.797382  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.12821e+06/2.86678e+06) 0.394
I0815 13:19:40.797392  8764 solver.cpp:509] Iteration 68000, Testing net (#0)
I0815 13:20:05.065351  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575059
I0815 13:20:05.065399  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.806057
I0815 13:20:05.065418  8764 solver.cpp:594]     Test net output #2: loss = 1.88469 (* 1 = 1.88469 loss)
I0815 13:20:05.065474  8764 solver.cpp:264] [MultiGPU] Tests completed in 24.2674s
I0815 13:20:05.208216  8794 solver.cpp:409] Finding and applying sparsity: 0.49
I0815 13:20:56.183220  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:20:56.185338  8764 solver.cpp:312] Iteration 68000 (1.06363 iter/s, 94.018s/100 iter), loss = 1.6056
I0815 13:20:56.185355  8764 solver.cpp:334]     Train net output #0: loss = 1.29496 (* 1 = 1.29496 loss)
I0815 13:20:56.185360  8764 sgd_solver.cpp:136] Iteration 68000, lr = 0.00575, m = 0.9
I0815 13:21:12.376251  8764 solver.cpp:312] Iteration 68100 (6.17649 iter/s, 16.1904s/100 iter), loss = 1.3345
I0815 13:21:12.376441  8764 solver.cpp:334]     Train net output #0: loss = 0.970617 (* 1 = 0.970617 loss)
I0815 13:21:12.376514  8764 sgd_solver.cpp:136] Iteration 68100, lr = 0.00574375, m = 0.9
I0815 13:21:32.906891  8764 solver.cpp:312] Iteration 68200 (4.8709 iter/s, 20.5301s/100 iter), loss = 1.40848
I0815 13:21:32.906962  8764 solver.cpp:334]     Train net output #0: loss = 1.40567 (* 1 = 1.40567 loss)
I0815 13:21:32.906970  8764 sgd_solver.cpp:136] Iteration 68200, lr = 0.0057375, m = 0.9
I0815 13:21:58.998153  8764 solver.cpp:312] Iteration 68300 (3.83281 iter/s, 26.0905s/100 iter), loss = 1.91848
I0815 13:21:58.998200  8764 solver.cpp:334]     Train net output #0: loss = 1.72673 (* 1 = 1.72673 loss)
I0815 13:21:58.998211  8764 sgd_solver.cpp:136] Iteration 68300, lr = 0.00573125, m = 0.9
I0815 13:22:18.028627  8764 solver.cpp:312] Iteration 68400 (5.25488 iter/s, 19.0299s/100 iter), loss = 1.71025
I0815 13:22:18.028690  8764 solver.cpp:334]     Train net output #0: loss = 1.89361 (* 1 = 1.89361 loss)
I0815 13:22:18.028699  8764 sgd_solver.cpp:136] Iteration 68400, lr = 0.005725, m = 0.9
I0815 13:22:34.064322  8764 solver.cpp:312] Iteration 68500 (6.23626 iter/s, 16.0352s/100 iter), loss = 1.30958
I0815 13:22:34.064391  8764 solver.cpp:334]     Train net output #0: loss = 1.53588 (* 1 = 1.53588 loss)
I0815 13:22:34.064410  8764 sgd_solver.cpp:136] Iteration 68500, lr = 0.00571875, m = 0.9
I0815 13:22:55.090487  8764 solver.cpp:312] Iteration 68600 (4.75611 iter/s, 21.0256s/100 iter), loss = 1.16677
I0815 13:22:55.090581  8764 solver.cpp:334]     Train net output #0: loss = 1.44158 (* 1 = 1.44158 loss)
I0815 13:22:55.090600  8764 sgd_solver.cpp:136] Iteration 68600, lr = 0.0057125, m = 0.9
I0815 13:23:03.740913  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:23:10.270617  8764 solver.cpp:312] Iteration 68700 (6.58774 iter/s, 15.1797s/100 iter), loss = 1.34469
I0815 13:23:10.270690  8764 solver.cpp:334]     Train net output #0: loss = 1.2625 (* 1 = 1.2625 loss)
I0815 13:23:10.270709  8764 sgd_solver.cpp:136] Iteration 68700, lr = 0.00570625, m = 0.9
I0815 13:23:25.967900  8764 solver.cpp:312] Iteration 68800 (6.3707 iter/s, 15.6969s/100 iter), loss = 1.2449
I0815 13:23:25.967957  8764 solver.cpp:334]     Train net output #0: loss = 1.40576 (* 1 = 1.40576 loss)
I0815 13:23:25.967964  8764 sgd_solver.cpp:136] Iteration 68800, lr = 0.0057, m = 0.9
I0815 13:23:42.088192  8764 solver.cpp:312] Iteration 68900 (6.20353 iter/s, 16.1198s/100 iter), loss = 0.934832
I0815 13:23:42.088259  8764 solver.cpp:334]     Train net output #0: loss = 1.09363 (* 1 = 1.09363 loss)
I0815 13:23:42.088284  8764 sgd_solver.cpp:136] Iteration 68900, lr = 0.00569375, m = 0.9
I0815 13:24:00.063720  8764 solver.cpp:363] Sparsity after update:
I0815 13:24:00.077392  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:24:00.077435  8764 net.cpp:2192] conv1a_param_0(0.239) 
I0815 13:24:00.077448  8764 net.cpp:2192] conv1b_param_0(0.486) 
I0815 13:24:00.077457  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:24:00.077466  8764 net.cpp:2192] res2a_branch2a_param_0(0.49) 
I0815 13:24:00.077476  8764 net.cpp:2192] res2a_branch2b_param_0(0.486) 
I0815 13:24:00.077483  8764 net.cpp:2192] res3a_branch2a_param_0(0.49) 
I0815 13:24:00.077491  8764 net.cpp:2192] res3a_branch2b_param_0(0.49) 
I0815 13:24:00.077499  8764 net.cpp:2192] res4a_branch2a_param_0(0.49) 
I0815 13:24:00.077508  8764 net.cpp:2192] res4a_branch2b_param_0(0.49) 
I0815 13:24:00.077517  8764 net.cpp:2192] res5a_branch2a_param_0(0.49) 
I0815 13:24:00.077527  8764 net.cpp:2192] res5a_branch2b_param_0(0.49) 
I0815 13:24:00.077535  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.15221e+06/2.86678e+06) 0.402
I0815 13:24:00.269779  8794 solver.cpp:409] Finding and applying sparsity: 0.5
I0815 13:24:32.477629  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:24:32.479812  8764 solver.cpp:312] Iteration 69000 (1.98451 iter/s, 50.3902s/100 iter), loss = 1.68543
I0815 13:24:32.479835  8764 solver.cpp:334]     Train net output #0: loss = 1.56553 (* 1 = 1.56553 loss)
I0815 13:24:32.479842  8764 sgd_solver.cpp:136] Iteration 69000, lr = 0.0056875, m = 0.9
I0815 13:24:49.495286  8764 solver.cpp:312] Iteration 69100 (5.87717 iter/s, 17.015s/100 iter), loss = 1.28258
I0815 13:24:49.495528  8764 solver.cpp:334]     Train net output #0: loss = 1.45268 (* 1 = 1.45268 loss)
I0815 13:24:49.495647  8764 sgd_solver.cpp:136] Iteration 69100, lr = 0.00568125, m = 0.9
I0815 13:25:06.594962  8764 solver.cpp:312] Iteration 69200 (5.84823 iter/s, 17.0992s/100 iter), loss = 1.47579
I0815 13:25:06.595021  8764 solver.cpp:334]     Train net output #0: loss = 1.24272 (* 1 = 1.24272 loss)
I0815 13:25:06.595026  8764 sgd_solver.cpp:136] Iteration 69200, lr = 0.005675, m = 0.9
I0815 13:25:23.462337  8764 solver.cpp:312] Iteration 69300 (5.92878 iter/s, 16.8669s/100 iter), loss = 1.49861
I0815 13:25:23.462399  8764 solver.cpp:334]     Train net output #0: loss = 1.01725 (* 1 = 1.01725 loss)
I0815 13:25:23.462412  8764 sgd_solver.cpp:136] Iteration 69300, lr = 0.00566875, m = 0.9
I0815 13:25:39.891876  8764 solver.cpp:312] Iteration 69400 (6.08676 iter/s, 16.4291s/100 iter), loss = 1.55036
I0815 13:25:39.891960  8764 solver.cpp:334]     Train net output #0: loss = 1.14888 (* 1 = 1.14888 loss)
I0815 13:25:39.891973  8764 sgd_solver.cpp:136] Iteration 69400, lr = 0.0056625, m = 0.9
I0815 13:25:55.466994  8764 solver.cpp:312] Iteration 69500 (6.42067 iter/s, 15.5747s/100 iter), loss = 1.39307
I0815 13:25:55.467022  8764 solver.cpp:334]     Train net output #0: loss = 1.41923 (* 1 = 1.41923 loss)
I0815 13:25:55.467028  8764 sgd_solver.cpp:136] Iteration 69500, lr = 0.00565625, m = 0.9
I0815 13:26:10.572741  8764 solver.cpp:312] Iteration 69600 (6.62018 iter/s, 15.1053s/100 iter), loss = 1.2397
I0815 13:26:10.572796  8764 solver.cpp:334]     Train net output #0: loss = 1.42685 (* 1 = 1.42685 loss)
I0815 13:26:10.572803  8764 sgd_solver.cpp:136] Iteration 69600, lr = 0.00565, m = 0.9
I0815 13:26:26.268477  8764 solver.cpp:312] Iteration 69700 (6.37133 iter/s, 15.6953s/100 iter), loss = 1.53504
I0815 13:26:26.268551  8764 solver.cpp:334]     Train net output #0: loss = 1.40105 (* 1 = 1.40105 loss)
I0815 13:26:26.268570  8764 sgd_solver.cpp:136] Iteration 69700, lr = 0.00564375, m = 0.9
I0815 13:26:42.499006  8764 solver.cpp:312] Iteration 69800 (6.1614 iter/s, 16.2301s/100 iter), loss = 1.57601
I0815 13:26:42.499073  8764 solver.cpp:334]     Train net output #0: loss = 1.7385 (* 1 = 1.7385 loss)
I0815 13:26:42.499081  8764 sgd_solver.cpp:136] Iteration 69800, lr = 0.0056375, m = 0.9
I0815 13:27:00.303617  8764 solver.cpp:312] Iteration 69900 (5.61667 iter/s, 17.8041s/100 iter), loss = 1.18314
I0815 13:27:00.303639  8764 solver.cpp:334]     Train net output #0: loss = 1.41551 (* 1 = 1.41551 loss)
I0815 13:27:00.303642  8764 sgd_solver.cpp:136] Iteration 69900, lr = 0.00563125, m = 0.9
I0815 13:27:18.280887  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_70000.caffemodel
I0815 13:27:18.460563  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_70000.solverstate
I0815 13:27:18.468539  8764 solver.cpp:363] Sparsity after update:
I0815 13:27:18.471418  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:27:18.472347  8764 net.cpp:2192] conv1a_param_0(0.239) 
I0815 13:27:18.472383  8764 net.cpp:2192] conv1b_param_0(0.486) 
I0815 13:27:18.472400  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:27:18.472417  8764 net.cpp:2192] res2a_branch2a_param_0(0.497) 
I0815 13:27:18.472432  8764 net.cpp:2192] res2a_branch2b_param_0(0.493) 
I0815 13:27:18.472446  8764 net.cpp:2192] res3a_branch2a_param_0(0.498) 
I0815 13:27:18.472460  8764 net.cpp:2192] res3a_branch2b_param_0(0.497) 
I0815 13:27:18.472472  8764 net.cpp:2192] res4a_branch2a_param_0(0.499) 
I0815 13:27:18.472486  8764 net.cpp:2192] res4a_branch2b_param_0(0.498) 
I0815 13:27:18.472507  8764 net.cpp:2192] res5a_branch2a_param_0(0.5) 
I0815 13:27:18.472519  8764 net.cpp:2192] res5a_branch2b_param_0(0.499) 
I0815 13:27:18.472530  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.1748e+06/2.86678e+06) 0.41
I0815 13:27:18.472554  8764 solver.cpp:509] Iteration 70000, Testing net (#0)
I0815 13:27:43.539031  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58347
I0815 13:27:43.539049  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815115
I0815 13:27:43.539057  8764 solver.cpp:594]     Test net output #2: loss = 1.83643 (* 1 = 1.83643 loss)
I0815 13:27:43.539078  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.0658s
I0815 13:27:43.697067  8794 solver.cpp:409] Finding and applying sparsity: 0.51
I0815 13:28:13.921357  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:28:13.923425  8764 solver.cpp:312] Iteration 70000 (1.35837 iter/s, 73.6178s/100 iter), loss = 1.4378
I0815 13:28:13.923449  8764 solver.cpp:334]     Train net output #0: loss = 1.44726 (* 1 = 1.44726 loss)
I0815 13:28:13.923460  8764 sgd_solver.cpp:136] Iteration 70000, lr = 0.005625, m = 0.9
I0815 13:28:30.126468  8764 solver.cpp:312] Iteration 70100 (6.17185 iter/s, 16.2026s/100 iter), loss = 1.2762
I0815 13:28:30.126518  8764 solver.cpp:334]     Train net output #0: loss = 1.21528 (* 1 = 1.21528 loss)
I0815 13:28:30.126530  8764 sgd_solver.cpp:136] Iteration 70100, lr = 0.00561875, m = 0.9
I0815 13:28:47.920068  8764 solver.cpp:312] Iteration 70200 (5.62015 iter/s, 17.7931s/100 iter), loss = 1.20405
I0815 13:28:47.920132  8764 solver.cpp:334]     Train net output #0: loss = 1.3407 (* 1 = 1.3407 loss)
I0815 13:28:47.920140  8764 sgd_solver.cpp:136] Iteration 70200, lr = 0.0056125, m = 0.9
I0815 13:29:03.682466  8764 solver.cpp:312] Iteration 70300 (6.34439 iter/s, 15.762s/100 iter), loss = 1.63355
I0815 13:29:03.682519  8764 solver.cpp:334]     Train net output #0: loss = 1.10683 (* 1 = 1.10683 loss)
I0815 13:29:03.682533  8764 sgd_solver.cpp:136] Iteration 70300, lr = 0.00560625, m = 0.9
I0815 13:29:21.226047  8764 solver.cpp:312] Iteration 70400 (5.70025 iter/s, 17.5431s/100 iter), loss = 1.32081
I0815 13:29:21.226124  8764 solver.cpp:334]     Train net output #0: loss = 1.13253 (* 1 = 1.13253 loss)
I0815 13:29:21.226136  8764 sgd_solver.cpp:136] Iteration 70400, lr = 0.0056, m = 0.9
I0815 13:29:38.273231  8764 solver.cpp:312] Iteration 70500 (5.86623 iter/s, 17.0467s/100 iter), loss = 1.186
I0815 13:29:38.273260  8764 solver.cpp:334]     Train net output #0: loss = 1.30868 (* 1 = 1.30868 loss)
I0815 13:29:38.273267  8764 sgd_solver.cpp:136] Iteration 70500, lr = 0.00559375, m = 0.9
I0815 13:29:56.238643  8764 solver.cpp:312] Iteration 70600 (5.5664 iter/s, 17.9649s/100 iter), loss = 1.66847
I0815 13:29:56.238699  8764 solver.cpp:334]     Train net output #0: loss = 1.51678 (* 1 = 1.51678 loss)
I0815 13:29:56.238706  8764 sgd_solver.cpp:136] Iteration 70600, lr = 0.0055875, m = 0.9
I0815 13:30:12.549562  8764 solver.cpp:312] Iteration 70700 (6.13103 iter/s, 16.3105s/100 iter), loss = 1.59683
I0815 13:30:12.549592  8764 solver.cpp:334]     Train net output #0: loss = 1.52847 (* 1 = 1.52847 loss)
I0815 13:30:12.549598  8764 sgd_solver.cpp:136] Iteration 70700, lr = 0.00558125, m = 0.9
I0815 13:30:31.078505  8764 solver.cpp:312] Iteration 70800 (5.39711 iter/s, 18.5284s/100 iter), loss = 1.34794
I0815 13:30:31.078598  8764 solver.cpp:334]     Train net output #0: loss = 1.03813 (* 1 = 1.03813 loss)
I0815 13:30:31.078626  8764 sgd_solver.cpp:136] Iteration 70800, lr = 0.005575, m = 0.9
I0815 13:30:48.912989  8764 solver.cpp:312] Iteration 70900 (5.60727 iter/s, 17.834s/100 iter), loss = 1.06901
I0815 13:30:48.913013  8764 solver.cpp:334]     Train net output #0: loss = 0.934914 (* 1 = 0.934914 loss)
I0815 13:30:48.913019  8764 sgd_solver.cpp:136] Iteration 70900, lr = 0.00556875, m = 0.9
I0815 13:31:04.770673  8764 solver.cpp:363] Sparsity after update:
I0815 13:31:04.788504  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:31:04.788539  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:31:04.788558  8764 net.cpp:2192] conv1b_param_0(0.499) 
I0815 13:31:04.788568  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:31:04.788575  8764 net.cpp:2192] res2a_branch2a_param_0(0.507) 
I0815 13:31:04.788583  8764 net.cpp:2192] res2a_branch2b_param_0(0.506) 
I0815 13:31:04.788589  8764 net.cpp:2192] res3a_branch2a_param_0(0.509) 
I0815 13:31:04.788596  8764 net.cpp:2192] res3a_branch2b_param_0(0.507) 
I0815 13:31:04.788602  8764 net.cpp:2192] res4a_branch2a_param_0(0.51) 
I0815 13:31:04.788609  8764 net.cpp:2192] res4a_branch2b_param_0(0.509) 
I0815 13:31:04.788616  8764 net.cpp:2192] res5a_branch2a_param_0(0.51) 
I0815 13:31:04.788624  8764 net.cpp:2192] res5a_branch2b_param_0(0.51) 
I0815 13:31:04.788630  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.19937e+06/2.86678e+06) 0.418
I0815 13:31:04.963099  8794 solver.cpp:409] Finding and applying sparsity: 0.52
I0815 13:31:36.401479  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:31:36.403611  8764 solver.cpp:312] Iteration 71000 (2.10574 iter/s, 47.4893s/100 iter), loss = 1.11723
I0815 13:31:36.403635  8764 solver.cpp:334]     Train net output #0: loss = 1.19103 (* 1 = 1.19103 loss)
I0815 13:31:36.403641  8764 sgd_solver.cpp:136] Iteration 71000, lr = 0.0055625, m = 0.9
I0815 13:31:52.622756  8764 solver.cpp:312] Iteration 71100 (6.16573 iter/s, 16.2187s/100 iter), loss = 1.00766
I0815 13:31:52.622781  8764 solver.cpp:334]     Train net output #0: loss = 1.16084 (* 1 = 1.16084 loss)
I0815 13:31:52.622786  8764 sgd_solver.cpp:136] Iteration 71100, lr = 0.00555625, m = 0.9
I0815 13:32:10.270056  8764 solver.cpp:312] Iteration 71200 (5.66675 iter/s, 17.6468s/100 iter), loss = 1.27051
I0815 13:32:10.270120  8764 solver.cpp:334]     Train net output #0: loss = 1.14007 (* 1 = 1.14007 loss)
I0815 13:32:10.270128  8764 sgd_solver.cpp:136] Iteration 71200, lr = 0.00555, m = 0.9
I0815 13:32:28.771471  8764 solver.cpp:312] Iteration 71300 (5.40514 iter/s, 18.5009s/100 iter), loss = 1.4299
I0815 13:32:28.771495  8764 solver.cpp:334]     Train net output #0: loss = 1.30661 (* 1 = 1.30661 loss)
I0815 13:32:28.771499  8764 sgd_solver.cpp:136] Iteration 71300, lr = 0.00554375, m = 0.9
I0815 13:32:44.302573  8764 solver.cpp:312] Iteration 71400 (6.43887 iter/s, 15.5307s/100 iter), loss = 1.26389
I0815 13:32:44.302633  8764 solver.cpp:334]     Train net output #0: loss = 1.72833 (* 1 = 1.72833 loss)
I0815 13:32:44.302639  8764 sgd_solver.cpp:136] Iteration 71400, lr = 0.0055375, m = 0.9
I0815 13:33:00.330325  8764 solver.cpp:312] Iteration 71500 (6.23935 iter/s, 16.0273s/100 iter), loss = 1.08291
I0815 13:33:00.330395  8764 solver.cpp:334]     Train net output #0: loss = 1.22494 (* 1 = 1.22494 loss)
I0815 13:33:00.330413  8764 sgd_solver.cpp:136] Iteration 71500, lr = 0.00553125, m = 0.9
I0815 13:33:16.644997  8764 solver.cpp:312] Iteration 71600 (6.12962 iter/s, 16.3142s/100 iter), loss = 1.13526
I0815 13:33:16.648159  8764 solver.cpp:334]     Train net output #0: loss = 0.955377 (* 1 = 0.955377 loss)
I0815 13:33:16.648167  8764 sgd_solver.cpp:136] Iteration 71600, lr = 0.005525, m = 0.9
I0815 13:33:33.573279  8764 solver.cpp:312] Iteration 71700 (5.90744 iter/s, 16.9278s/100 iter), loss = 1.49086
I0815 13:33:33.573314  8764 solver.cpp:334]     Train net output #0: loss = 1.47299 (* 1 = 1.47299 loss)
I0815 13:33:33.573320  8764 sgd_solver.cpp:136] Iteration 71700, lr = 0.00551875, m = 0.9
I0815 13:33:52.425997  8764 solver.cpp:312] Iteration 71800 (5.30442 iter/s, 18.8522s/100 iter), loss = 1.61794
I0815 13:33:52.426082  8764 solver.cpp:334]     Train net output #0: loss = 1.56014 (* 1 = 1.56014 loss)
I0815 13:33:52.426100  8764 sgd_solver.cpp:136] Iteration 71800, lr = 0.0055125, m = 0.9
I0815 13:34:07.371035  8764 solver.cpp:312] Iteration 71900 (6.69137 iter/s, 14.9446s/100 iter), loss = 1.47572
I0815 13:34:07.371062  8764 solver.cpp:334]     Train net output #0: loss = 1.3653 (* 1 = 1.3653 loss)
I0815 13:34:07.371068  8764 sgd_solver.cpp:136] Iteration 71900, lr = 0.00550625, m = 0.9
I0815 13:34:25.176157  8764 solver.cpp:363] Sparsity after update:
I0815 13:34:25.194059  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:34:25.194228  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:34:25.194325  8764 net.cpp:2192] conv1b_param_0(0.513) 
I0815 13:34:25.194411  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:34:25.194499  8764 net.cpp:2192] res2a_branch2a_param_0(0.517) 
I0815 13:34:25.194586  8764 net.cpp:2192] res2a_branch2b_param_0(0.513) 
I0815 13:34:25.194672  8764 net.cpp:2192] res3a_branch2a_param_0(0.519) 
I0815 13:34:25.194759  8764 net.cpp:2192] res3a_branch2b_param_0(0.517) 
I0815 13:34:25.194845  8764 net.cpp:2192] res4a_branch2a_param_0(0.52) 
I0815 13:34:25.194932  8764 net.cpp:2192] res4a_branch2b_param_0(0.519) 
I0815 13:34:25.195020  8764 net.cpp:2192] res5a_branch2a_param_0(0.52) 
I0815 13:34:25.195106  8764 net.cpp:2192] res5a_branch2b_param_0(0.52) 
I0815 13:34:25.195194  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.22334e+06/2.86678e+06) 0.427
I0815 13:34:25.195307  8764 solver.cpp:509] Iteration 72000, Testing net (#0)
I0815 13:34:58.880463  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.577647
I0815 13:34:58.880555  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807705
I0815 13:34:58.880564  8764 solver.cpp:594]     Test net output #2: loss = 1.868 (* 1 = 1.868 loss)
I0815 13:34:58.880584  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.6844s
I0815 13:34:59.074308  8794 solver.cpp:409] Finding and applying sparsity: 0.53
I0815 13:35:43.485175  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:35:43.487687  8764 solver.cpp:312] Iteration 72000 (1.04043 iter/s, 96.114s/100 iter), loss = 1.31492
I0815 13:35:43.487726  8764 solver.cpp:334]     Train net output #0: loss = 1.27071 (* 1 = 1.27071 loss)
I0815 13:35:43.487740  8764 sgd_solver.cpp:136] Iteration 72000, lr = 0.0055, m = 0.9
I0815 13:36:02.486325  8764 solver.cpp:312] Iteration 72100 (5.26368 iter/s, 18.9981s/100 iter), loss = 1.45444
I0815 13:36:02.486377  8764 solver.cpp:334]     Train net output #0: loss = 1.23593 (* 1 = 1.23593 loss)
I0815 13:36:02.486480  8764 sgd_solver.cpp:136] Iteration 72100, lr = 0.00549375, m = 0.9
I0815 13:36:18.491150  8764 solver.cpp:312] Iteration 72200 (6.24829 iter/s, 16.0044s/100 iter), loss = 1.26378
I0815 13:36:18.491243  8764 solver.cpp:334]     Train net output #0: loss = 1.4246 (* 1 = 1.4246 loss)
I0815 13:36:18.491261  8764 sgd_solver.cpp:136] Iteration 72200, lr = 0.0054875, m = 0.9
I0815 13:36:32.940659  8764 solver.cpp:312] Iteration 72300 (6.92084 iter/s, 14.4491s/100 iter), loss = 1.3406
I0815 13:36:32.940682  8764 solver.cpp:334]     Train net output #0: loss = 1.26069 (* 1 = 1.26069 loss)
I0815 13:36:32.940688  8764 sgd_solver.cpp:136] Iteration 72300, lr = 0.00548125, m = 0.9
I0815 13:36:49.161640  8764 solver.cpp:312] Iteration 72400 (6.16502 iter/s, 16.2205s/100 iter), loss = 1.47459
I0815 13:36:49.161700  8764 solver.cpp:334]     Train net output #0: loss = 1.54757 (* 1 = 1.54757 loss)
I0815 13:36:49.161707  8764 sgd_solver.cpp:136] Iteration 72400, lr = 0.005475, m = 0.9
I0815 13:37:08.864179  8764 solver.cpp:312] Iteration 72500 (5.07563 iter/s, 19.702s/100 iter), loss = 1.20936
I0815 13:37:08.864260  8764 solver.cpp:334]     Train net output #0: loss = 1.39287 (* 1 = 1.39287 loss)
I0815 13:37:08.864284  8764 sgd_solver.cpp:136] Iteration 72500, lr = 0.00546875, m = 0.9
I0815 13:37:28.907801  8764 solver.cpp:312] Iteration 72600 (4.98926 iter/s, 20.0431s/100 iter), loss = 1.16458
I0815 13:37:28.907871  8764 solver.cpp:334]     Train net output #0: loss = 1.34948 (* 1 = 1.34948 loss)
I0815 13:37:28.907878  8764 sgd_solver.cpp:136] Iteration 72600, lr = 0.0054625, m = 0.9
I0815 13:37:47.116658  8764 solver.cpp:312] Iteration 72700 (5.492 iter/s, 18.2083s/100 iter), loss = 1.38161
I0815 13:37:47.116791  8764 solver.cpp:334]     Train net output #0: loss = 1.34025 (* 1 = 1.34025 loss)
I0815 13:37:47.116824  8764 sgd_solver.cpp:136] Iteration 72700, lr = 0.00545625, m = 0.9
I0815 13:38:07.187736  8764 solver.cpp:312] Iteration 72800 (4.98243 iter/s, 20.0705s/100 iter), loss = 1.38824
I0815 13:38:07.187865  8764 solver.cpp:334]     Train net output #0: loss = 1.62624 (* 1 = 1.62624 loss)
I0815 13:38:07.187891  8764 sgd_solver.cpp:136] Iteration 72800, lr = 0.00545, m = 0.9
I0815 13:38:14.876085  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:38:30.647259  8764 solver.cpp:312] Iteration 72900 (4.26278 iter/s, 23.4589s/100 iter), loss = 1.38717
I0815 13:38:30.647286  8764 solver.cpp:334]     Train net output #0: loss = 1.28371 (* 1 = 1.28371 loss)
I0815 13:38:30.647292  8764 sgd_solver.cpp:136] Iteration 72900, lr = 0.00544375, m = 0.9
I0815 13:38:47.435343  8764 solver.cpp:363] Sparsity after update:
I0815 13:38:47.448081  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:38:47.448141  8764 net.cpp:2192] conv1a_param_0(0.252) 
I0815 13:38:47.448160  8764 net.cpp:2192] conv1b_param_0(0.526) 
I0815 13:38:47.448173  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:38:47.448184  8764 net.cpp:2192] res2a_branch2a_param_0(0.528) 
I0815 13:38:47.448195  8764 net.cpp:2192] res2a_branch2b_param_0(0.526) 
I0815 13:38:47.448226  8764 net.cpp:2192] res3a_branch2a_param_0(0.53) 
I0815 13:38:47.448242  8764 net.cpp:2192] res3a_branch2b_param_0(0.528) 
I0815 13:38:47.448256  8764 net.cpp:2192] res4a_branch2a_param_0(0.53) 
I0815 13:38:47.448269  8764 net.cpp:2192] res4a_branch2b_param_0(0.53) 
I0815 13:38:47.448283  8764 net.cpp:2192] res5a_branch2a_param_0(0.53) 
I0815 13:38:47.448297  8764 net.cpp:2192] res5a_branch2b_param_0(0.53) 
I0815 13:38:47.448310  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.24659e+06/2.86678e+06) 0.435
I0815 13:38:47.605795  8794 solver.cpp:409] Finding and applying sparsity: 0.54
I0815 13:39:52.968119  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:39:52.970613  8764 solver.cpp:312] Iteration 73000 (1.21476 iter/s, 82.3211s/100 iter), loss = 1.40478
I0815 13:39:52.970661  8764 solver.cpp:334]     Train net output #0: loss = 1.60809 (* 1 = 1.60809 loss)
I0815 13:39:52.970679  8764 sgd_solver.cpp:136] Iteration 73000, lr = 0.0054375, m = 0.9
I0815 13:40:11.232157  8764 solver.cpp:312] Iteration 73100 (5.47614 iter/s, 18.261s/100 iter), loss = 1.46007
I0815 13:40:11.232183  8764 solver.cpp:334]     Train net output #0: loss = 1.29588 (* 1 = 1.29588 loss)
I0815 13:40:11.232189  8764 sgd_solver.cpp:136] Iteration 73100, lr = 0.00543125, m = 0.9
I0815 13:40:32.977454  8764 solver.cpp:312] Iteration 73200 (4.59882 iter/s, 21.7447s/100 iter), loss = 1.3195
I0815 13:40:32.977519  8764 solver.cpp:334]     Train net output #0: loss = 1.48863 (* 1 = 1.48863 loss)
I0815 13:40:32.977525  8764 sgd_solver.cpp:136] Iteration 73200, lr = 0.005425, m = 0.9
I0815 13:40:53.051101  8764 solver.cpp:312] Iteration 73300 (4.98181 iter/s, 20.073s/100 iter), loss = 1.25838
I0815 13:40:53.051271  8764 solver.cpp:334]     Train net output #0: loss = 1.2955 (* 1 = 1.2955 loss)
I0815 13:40:53.051360  8764 sgd_solver.cpp:136] Iteration 73300, lr = 0.00541875, m = 0.9
I0815 13:41:11.478224  8764 solver.cpp:312] Iteration 73400 (5.42693 iter/s, 18.4266s/100 iter), loss = 1.17954
I0815 13:41:11.478469  8764 solver.cpp:334]     Train net output #0: loss = 0.989542 (* 1 = 0.989542 loss)
I0815 13:41:11.478582  8764 sgd_solver.cpp:136] Iteration 73400, lr = 0.0054125, m = 0.9
I0815 13:41:29.673141  8764 solver.cpp:312] Iteration 73500 (5.49619 iter/s, 18.1944s/100 iter), loss = 1.27165
I0815 13:41:29.673238  8764 solver.cpp:334]     Train net output #0: loss = 0.93066 (* 1 = 0.93066 loss)
I0815 13:41:29.673269  8764 sgd_solver.cpp:136] Iteration 73500, lr = 0.00540625, m = 0.9
I0815 13:41:56.620193  8764 solver.cpp:312] Iteration 73600 (3.71109 iter/s, 26.9463s/100 iter), loss = 1.13744
I0815 13:41:56.620327  8764 solver.cpp:334]     Train net output #0: loss = 0.891401 (* 1 = 0.891401 loss)
I0815 13:41:56.620368  8764 sgd_solver.cpp:136] Iteration 73600, lr = 0.0054, m = 0.9
I0815 13:42:17.149361  8764 solver.cpp:312] Iteration 73700 (4.87125 iter/s, 20.5286s/100 iter), loss = 1.46148
I0815 13:42:17.149574  8764 solver.cpp:334]     Train net output #0: loss = 1.8041 (* 1 = 1.8041 loss)
I0815 13:42:17.149657  8764 sgd_solver.cpp:136] Iteration 73700, lr = 0.00539375, m = 0.9
I0815 13:42:34.899082  8764 solver.cpp:312] Iteration 73800 (5.63405 iter/s, 17.7492s/100 iter), loss = 1.46362
I0815 13:42:34.899204  8764 solver.cpp:334]     Train net output #0: loss = 1.72779 (* 1 = 1.72779 loss)
I0815 13:42:34.899222  8764 sgd_solver.cpp:136] Iteration 73800, lr = 0.0053875, m = 0.9
I0815 13:42:56.789317  8764 solver.cpp:312] Iteration 73900 (4.56837 iter/s, 21.8896s/100 iter), loss = 1.47106
I0815 13:42:56.789347  8764 solver.cpp:334]     Train net output #0: loss = 1.50359 (* 1 = 1.50359 loss)
I0815 13:42:56.789353  8764 sgd_solver.cpp:136] Iteration 73900, lr = 0.00538125, m = 0.9
I0815 13:43:15.124948  8764 solver.cpp:363] Sparsity after update:
I0815 13:43:15.132913  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:43:15.132927  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:43:15.132936  8764 net.cpp:2192] conv1b_param_0(0.526) 
I0815 13:43:15.132939  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:43:15.132942  8764 net.cpp:2192] res2a_branch2a_param_0(0.538) 
I0815 13:43:15.132946  8764 net.cpp:2192] res2a_branch2b_param_0(0.533) 
I0815 13:43:15.132949  8764 net.cpp:2192] res3a_branch2a_param_0(0.54) 
I0815 13:43:15.132952  8764 net.cpp:2192] res3a_branch2b_param_0(0.538) 
I0815 13:43:15.132956  8764 net.cpp:2192] res4a_branch2a_param_0(0.54) 
I0815 13:43:15.132958  8764 net.cpp:2192] res4a_branch2b_param_0(0.54) 
I0815 13:43:15.132961  8764 net.cpp:2192] res5a_branch2a_param_0(0.54) 
I0815 13:43:15.132964  8764 net.cpp:2192] res5a_branch2b_param_0(0.54) 
I0815 13:43:15.132967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.27056e+06/2.86678e+06) 0.443
I0815 13:43:15.132977  8764 solver.cpp:509] Iteration 74000, Testing net (#0)
I0815 13:43:23.207036  8747 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 13:43:48.081229  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583176
I0815 13:43:48.081272  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815409
I0815 13:43:48.081277  8764 solver.cpp:594]     Test net output #2: loss = 1.79888 (* 1 = 1.79888 loss)
I0815 13:43:48.081298  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.9474s
I0815 13:43:48.249557  8794 solver.cpp:409] Finding and applying sparsity: 0.55
I0815 13:44:45.648016  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:44:45.650044  8764 solver.cpp:312] Iteration 74000 (0.91863 iter/s, 108.858s/100 iter), loss = 1.1336
I0815 13:44:45.650068  8764 solver.cpp:334]     Train net output #0: loss = 1.01767 (* 1 = 1.01767 loss)
I0815 13:44:45.650141  8764 sgd_solver.cpp:136] Iteration 74000, lr = 0.005375, m = 0.9
I0815 13:45:05.060369  8764 solver.cpp:312] Iteration 74100 (5.15204 iter/s, 19.4098s/100 iter), loss = 1.44655
I0815 13:45:05.060433  8764 solver.cpp:334]     Train net output #0: loss = 1.4243 (* 1 = 1.4243 loss)
I0815 13:45:05.060451  8764 sgd_solver.cpp:136] Iteration 74100, lr = 0.00536875, m = 0.9
I0815 13:45:27.321580  8764 solver.cpp:312] Iteration 74200 (4.49224 iter/s, 22.2606s/100 iter), loss = 1.65701
I0815 13:45:27.321665  8764 solver.cpp:334]     Train net output #0: loss = 2.02465 (* 1 = 2.02465 loss)
I0815 13:45:27.321679  8764 sgd_solver.cpp:136] Iteration 74200, lr = 0.0053625, m = 0.9
I0815 13:45:44.048034  8764 solver.cpp:312] Iteration 74300 (5.97872 iter/s, 16.726s/100 iter), loss = 1.21866
I0815 13:45:44.048096  8764 solver.cpp:334]     Train net output #0: loss = 1.51586 (* 1 = 1.51586 loss)
I0815 13:45:44.048113  8764 sgd_solver.cpp:136] Iteration 74300, lr = 0.00535625, m = 0.9
I0815 13:46:02.544136  8764 solver.cpp:312] Iteration 74400 (5.4067 iter/s, 18.4956s/100 iter), loss = 1.03548
I0815 13:46:02.544245  8764 solver.cpp:334]     Train net output #0: loss = 0.978426 (* 1 = 0.978426 loss)
I0815 13:46:02.544268  8764 sgd_solver.cpp:136] Iteration 74400, lr = 0.00535, m = 0.9
I0815 13:46:23.794894  8764 solver.cpp:312] Iteration 74500 (4.70584 iter/s, 21.2502s/100 iter), loss = 1.66779
I0815 13:46:23.794982  8764 solver.cpp:334]     Train net output #0: loss = 1.81796 (* 1 = 1.81796 loss)
I0815 13:46:23.795003  8764 sgd_solver.cpp:136] Iteration 74500, lr = 0.00534375, m = 0.9
I0815 13:46:41.537704  8764 solver.cpp:312] Iteration 74600 (5.63624 iter/s, 17.7423s/100 iter), loss = 1.33991
I0815 13:46:41.537811  8764 solver.cpp:334]     Train net output #0: loss = 1.07503 (* 1 = 1.07503 loss)
I0815 13:46:41.537829  8764 sgd_solver.cpp:136] Iteration 74600, lr = 0.0053375, m = 0.9
I0815 13:46:57.841248  8764 solver.cpp:312] Iteration 74700 (6.13381 iter/s, 16.3031s/100 iter), loss = 1.30291
I0815 13:46:57.841295  8764 solver.cpp:334]     Train net output #0: loss = 1.30166 (* 1 = 1.30166 loss)
I0815 13:46:57.841305  8764 sgd_solver.cpp:136] Iteration 74700, lr = 0.00533125, m = 0.9
I0815 13:47:14.239966  8764 solver.cpp:312] Iteration 74800 (6.0982 iter/s, 16.3983s/100 iter), loss = 1.66095
I0815 13:47:14.240020  8764 solver.cpp:334]     Train net output #0: loss = 1.90864 (* 1 = 1.90864 loss)
I0815 13:47:14.240025  8764 sgd_solver.cpp:136] Iteration 74800, lr = 0.005325, m = 0.9
I0815 13:47:32.650331  8764 solver.cpp:312] Iteration 74900 (5.43188 iter/s, 18.4098s/100 iter), loss = 1.50802
I0815 13:47:32.650413  8764 solver.cpp:334]     Train net output #0: loss = 1.94347 (* 1 = 1.94347 loss)
I0815 13:47:32.650430  8764 sgd_solver.cpp:136] Iteration 74900, lr = 0.00531875, m = 0.9
I0815 13:47:52.884798  8764 solver.cpp:363] Sparsity after update:
I0815 13:47:52.897260  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:47:52.897285  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:47:52.897310  8764 net.cpp:2192] conv1b_param_0(0.539) 
I0815 13:47:52.897323  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:47:52.897334  8764 net.cpp:2192] res2a_branch2a_param_0(0.549) 
I0815 13:47:52.897346  8764 net.cpp:2192] res2a_branch2b_param_0(0.546) 
I0815 13:47:52.897356  8764 net.cpp:2192] res3a_branch2a_param_0(0.549) 
I0815 13:47:52.897367  8764 net.cpp:2192] res3a_branch2b_param_0(0.549) 
I0815 13:47:52.897377  8764 net.cpp:2192] res4a_branch2a_param_0(0.549) 
I0815 13:47:52.897388  8764 net.cpp:2192] res4a_branch2b_param_0(0.549) 
I0815 13:47:52.897398  8764 net.cpp:2192] res5a_branch2a_param_0(0.55) 
I0815 13:47:52.897409  8764 net.cpp:2192] res5a_branch2b_param_0(0.549) 
I0815 13:47:52.897419  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.29342e+06/2.86678e+06) 0.451
I0815 13:47:53.084859  8794 solver.cpp:409] Finding and applying sparsity: 0.56
I0815 13:48:41.573949  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:48:41.576445  8764 solver.cpp:312] Iteration 75000 (1.45087 iter/s, 68.9242s/100 iter), loss = 1.00258
I0815 13:48:41.576495  8764 solver.cpp:334]     Train net output #0: loss = 1.11989 (* 1 = 1.11989 loss)
I0815 13:48:41.576512  8764 sgd_solver.cpp:136] Iteration 75000, lr = 0.0053125, m = 0.9
I0815 13:48:58.256510  8764 solver.cpp:312] Iteration 75100 (5.99535 iter/s, 16.6796s/100 iter), loss = 0.913202
I0815 13:48:58.256561  8764 solver.cpp:334]     Train net output #0: loss = 1.00748 (* 1 = 1.00748 loss)
I0815 13:48:58.256573  8764 sgd_solver.cpp:136] Iteration 75100, lr = 0.00530625, m = 0.9
I0815 13:49:14.516646  8764 solver.cpp:312] Iteration 75200 (6.15018 iter/s, 16.2597s/100 iter), loss = 1.26299
I0815 13:49:14.516728  8764 solver.cpp:334]     Train net output #0: loss = 1.2991 (* 1 = 1.2991 loss)
I0815 13:49:14.516741  8764 sgd_solver.cpp:136] Iteration 75200, lr = 0.0053, m = 0.9
I0815 13:49:31.712338  8764 solver.cpp:312] Iteration 75300 (5.81557 iter/s, 17.1952s/100 iter), loss = 1.50106
I0815 13:49:31.712365  8764 solver.cpp:334]     Train net output #0: loss = 1.93389 (* 1 = 1.93389 loss)
I0815 13:49:31.712370  8764 sgd_solver.cpp:136] Iteration 75300, lr = 0.00529375, m = 0.9
I0815 13:49:52.210034  8764 solver.cpp:312] Iteration 75400 (4.87873 iter/s, 20.4971s/100 iter), loss = 1.70027
I0815 13:49:52.210175  8764 solver.cpp:334]     Train net output #0: loss = 1.67273 (* 1 = 1.67273 loss)
I0815 13:49:52.210183  8764 sgd_solver.cpp:136] Iteration 75400, lr = 0.0052875, m = 0.9
I0815 13:50:10.703825  8764 solver.cpp:312] Iteration 75500 (5.40737 iter/s, 18.4933s/100 iter), loss = 1.38705
I0815 13:50:10.712151  8764 solver.cpp:334]     Train net output #0: loss = 1.46105 (* 1 = 1.46105 loss)
I0815 13:50:10.712185  8764 sgd_solver.cpp:136] Iteration 75500, lr = 0.00528125, m = 0.9
I0815 13:50:27.676693  8764 solver.cpp:312] Iteration 75600 (5.89192 iter/s, 16.9724s/100 iter), loss = 1.32964
I0815 13:50:27.676785  8764 solver.cpp:334]     Train net output #0: loss = 1.50343 (* 1 = 1.50343 loss)
I0815 13:50:27.676802  8764 sgd_solver.cpp:136] Iteration 75600, lr = 0.005275, m = 0.9
I0815 13:50:43.767388  8764 solver.cpp:312] Iteration 75700 (6.21494 iter/s, 16.0903s/100 iter), loss = 1.30666
I0815 13:50:43.767417  8764 solver.cpp:334]     Train net output #0: loss = 1.48023 (* 1 = 1.48023 loss)
I0815 13:50:43.767423  8764 sgd_solver.cpp:136] Iteration 75700, lr = 0.00526875, m = 0.9
I0815 13:51:01.914274  8764 solver.cpp:312] Iteration 75800 (5.51074 iter/s, 18.1464s/100 iter), loss = 1.19163
I0815 13:51:01.916179  8764 solver.cpp:334]     Train net output #0: loss = 1.11732 (* 1 = 1.11732 loss)
I0815 13:51:01.916216  8764 sgd_solver.cpp:136] Iteration 75800, lr = 0.0052625, m = 0.9
I0815 13:51:21.665263  8764 solver.cpp:312] Iteration 75900 (5.06317 iter/s, 19.7505s/100 iter), loss = 1.14395
I0815 13:51:21.665290  8764 solver.cpp:334]     Train net output #0: loss = 1.02167 (* 1 = 1.02167 loss)
I0815 13:51:21.665297  8764 sgd_solver.cpp:136] Iteration 75900, lr = 0.00525625, m = 0.9
I0815 13:51:38.824400  8764 solver.cpp:363] Sparsity after update:
I0815 13:51:38.830765  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:51:38.830788  8764 net.cpp:2192] conv1a_param_0(0.265) 
I0815 13:51:38.830803  8764 net.cpp:2192] conv1b_param_0(0.552) 
I0815 13:51:38.830812  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:51:38.830821  8764 net.cpp:2192] res2a_branch2a_param_0(0.559) 
I0815 13:51:38.830831  8764 net.cpp:2192] res2a_branch2b_param_0(0.553) 
I0815 13:51:38.830839  8764 net.cpp:2192] res3a_branch2a_param_0(0.559) 
I0815 13:51:38.830848  8764 net.cpp:2192] res3a_branch2b_param_0(0.559) 
I0815 13:51:38.830857  8764 net.cpp:2192] res4a_branch2a_param_0(0.56) 
I0815 13:51:38.830865  8764 net.cpp:2192] res4a_branch2b_param_0(0.559) 
I0815 13:51:38.830873  8764 net.cpp:2192] res5a_branch2a_param_0(0.56) 
I0815 13:51:38.830883  8764 net.cpp:2192] res5a_branch2b_param_0(0.56) 
I0815 13:51:38.830890  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.31739e+06/2.86678e+06) 0.46
I0815 13:51:38.830907  8764 solver.cpp:509] Iteration 76000, Testing net (#0)
I0815 13:52:01.261704  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 13:52:06.131361  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.580706
I0815 13:52:06.131381  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.808821
I0815 13:52:06.131386  8764 solver.cpp:594]     Test net output #2: loss = 1.85827 (* 1 = 1.85827 loss)
I0815 13:52:06.131409  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.2998s
I0815 13:52:06.304857  8794 solver.cpp:409] Finding and applying sparsity: 0.57
I0815 13:52:41.283316  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:52:41.285377  8764 solver.cpp:312] Iteration 76000 (1.256 iter/s, 79.618s/100 iter), loss = 1.43638
I0815 13:52:41.285398  8764 solver.cpp:334]     Train net output #0: loss = 1.43316 (* 1 = 1.43316 loss)
I0815 13:52:41.285405  8764 sgd_solver.cpp:136] Iteration 76000, lr = 0.00525, m = 0.9
I0815 13:52:59.660406  8764 solver.cpp:312] Iteration 76100 (5.44232 iter/s, 18.3745s/100 iter), loss = 1.19768
I0815 13:52:59.660447  8764 solver.cpp:334]     Train net output #0: loss = 1.0534 (* 1 = 1.0534 loss)
I0815 13:52:59.660455  8764 sgd_solver.cpp:136] Iteration 76100, lr = 0.00524375, m = 0.9
I0815 13:53:16.070694  8764 solver.cpp:312] Iteration 76200 (6.09391 iter/s, 16.4098s/100 iter), loss = 1.11379
I0815 13:53:16.070762  8764 solver.cpp:334]     Train net output #0: loss = 1.41549 (* 1 = 1.41549 loss)
I0815 13:53:16.070770  8764 sgd_solver.cpp:136] Iteration 76200, lr = 0.0052375, m = 0.9
I0815 13:53:35.267165  8764 solver.cpp:312] Iteration 76300 (5.20944 iter/s, 19.1959s/100 iter), loss = 1.24614
I0815 13:53:35.267266  8764 solver.cpp:334]     Train net output #0: loss = 1.33311 (* 1 = 1.33311 loss)
I0815 13:53:35.267293  8764 sgd_solver.cpp:136] Iteration 76300, lr = 0.00523125, m = 0.9
I0815 13:53:51.545001  8764 solver.cpp:312] Iteration 76400 (6.14349 iter/s, 16.2774s/100 iter), loss = 1.05348
I0815 13:53:51.545701  8764 solver.cpp:334]     Train net output #0: loss = 0.957156 (* 1 = 0.957156 loss)
I0815 13:53:51.545809  8764 sgd_solver.cpp:136] Iteration 76400, lr = 0.005225, m = 0.9
I0815 13:54:08.844421  8764 solver.cpp:312] Iteration 76500 (5.7807 iter/s, 17.2989s/100 iter), loss = 1.42427
I0815 13:54:08.844477  8764 solver.cpp:334]     Train net output #0: loss = 1.4424 (* 1 = 1.4424 loss)
I0815 13:54:08.844491  8764 sgd_solver.cpp:136] Iteration 76500, lr = 0.00521875, m = 0.9
I0815 13:54:25.813961  8764 solver.cpp:312] Iteration 76600 (5.89307 iter/s, 16.9691s/100 iter), loss = 1.16146
I0815 13:54:25.814167  8764 solver.cpp:334]     Train net output #0: loss = 1.12659 (* 1 = 1.12659 loss)
I0815 13:54:25.814252  8764 sgd_solver.cpp:136] Iteration 76600, lr = 0.0052125, m = 0.9
I0815 13:54:41.397620  8764 solver.cpp:312] Iteration 76700 (6.41715 iter/s, 15.5832s/100 iter), loss = 1.17507
I0815 13:54:41.397644  8764 solver.cpp:334]     Train net output #0: loss = 1.19324 (* 1 = 1.19324 loss)
I0815 13:54:41.397650  8764 sgd_solver.cpp:136] Iteration 76700, lr = 0.00520625, m = 0.9
I0815 13:54:55.938654  8764 solver.cpp:312] Iteration 76800 (6.87728 iter/s, 14.5406s/100 iter), loss = 1.22019
I0815 13:54:55.938714  8764 solver.cpp:334]     Train net output #0: loss = 1.19016 (* 1 = 1.19016 loss)
I0815 13:54:55.938719  8764 sgd_solver.cpp:136] Iteration 76800, lr = 0.0052, m = 0.9
I0815 13:55:12.773144  8764 solver.cpp:312] Iteration 76900 (5.94035 iter/s, 16.834s/100 iter), loss = 1.47172
I0815 13:55:12.773171  8764 solver.cpp:334]     Train net output #0: loss = 1.53934 (* 1 = 1.53934 loss)
I0815 13:55:12.773176  8764 sgd_solver.cpp:136] Iteration 76900, lr = 0.00519375, m = 0.9
I0815 13:55:28.463145  8764 solver.cpp:363] Sparsity after update:
I0815 13:55:28.473598  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:55:28.473610  8764 net.cpp:2192] conv1a_param_0(0.278) 
I0815 13:55:28.473620  8764 net.cpp:2192] conv1b_param_0(0.563) 
I0815 13:55:28.473631  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:55:28.473636  8764 net.cpp:2192] res2a_branch2a_param_0(0.569) 
I0815 13:55:28.473640  8764 net.cpp:2192] res2a_branch2b_param_0(0.564) 
I0815 13:55:28.473644  8764 net.cpp:2192] res3a_branch2a_param_0(0.569) 
I0815 13:55:28.473646  8764 net.cpp:2192] res3a_branch2b_param_0(0.569) 
I0815 13:55:28.473650  8764 net.cpp:2192] res4a_branch2a_param_0(0.569) 
I0815 13:55:28.473654  8764 net.cpp:2192] res4a_branch2b_param_0(0.569) 
I0815 13:55:28.473657  8764 net.cpp:2192] res5a_branch2a_param_0(0.57) 
I0815 13:55:28.473662  8764 net.cpp:2192] res5a_branch2b_param_0(0.569) 
I0815 13:55:28.473666  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.34065e+06/2.86678e+06) 0.468
I0815 13:55:28.632561  8794 solver.cpp:409] Finding and applying sparsity: 0.58
I0815 13:56:03.837258  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 13:56:03.839395  8764 solver.cpp:312] Iteration 77000 (1.95829 iter/s, 51.0648s/100 iter), loss = 1.31031
I0815 13:56:03.839413  8764 solver.cpp:334]     Train net output #0: loss = 1.42772 (* 1 = 1.42772 loss)
I0815 13:56:03.839423  8764 sgd_solver.cpp:136] Iteration 77000, lr = 0.0051875, m = 0.9
I0815 13:56:21.304471  8764 solver.cpp:312] Iteration 77100 (5.72588 iter/s, 17.4646s/100 iter), loss = 1.19945
I0815 13:56:21.304595  8764 solver.cpp:334]     Train net output #0: loss = 1.11596 (* 1 = 1.11596 loss)
I0815 13:56:21.304643  8764 sgd_solver.cpp:136] Iteration 77100, lr = 0.00518125, m = 0.9
I0815 13:56:38.923822  8764 solver.cpp:312] Iteration 77200 (5.67573 iter/s, 17.6189s/100 iter), loss = 1.25527
I0815 13:56:38.923893  8764 solver.cpp:334]     Train net output #0: loss = 1.03412 (* 1 = 1.03412 loss)
I0815 13:56:38.923900  8764 sgd_solver.cpp:136] Iteration 77200, lr = 0.005175, m = 0.9
I0815 13:56:56.083168  8764 solver.cpp:312] Iteration 77300 (5.82789 iter/s, 17.1589s/100 iter), loss = 1.40372
I0815 13:56:56.083341  8764 solver.cpp:334]     Train net output #0: loss = 1.16774 (* 1 = 1.16774 loss)
I0815 13:56:56.083428  8764 sgd_solver.cpp:136] Iteration 77300, lr = 0.00516875, m = 0.9
I0815 13:57:12.802129  8764 solver.cpp:312] Iteration 77400 (5.9814 iter/s, 16.7185s/100 iter), loss = 1.33528
I0815 13:57:12.802189  8764 solver.cpp:334]     Train net output #0: loss = 0.996885 (* 1 = 0.996885 loss)
I0815 13:57:12.802196  8764 sgd_solver.cpp:136] Iteration 77400, lr = 0.0051625, m = 0.9
I0815 13:57:29.360671  8764 solver.cpp:312] Iteration 77500 (6.03934 iter/s, 16.5581s/100 iter), loss = 1.65802
I0815 13:57:29.360694  8764 solver.cpp:334]     Train net output #0: loss = 1.73316 (* 1 = 1.73316 loss)
I0815 13:57:29.360699  8764 sgd_solver.cpp:136] Iteration 77500, lr = 0.00515625, m = 0.9
I0815 13:57:47.074259  8764 solver.cpp:312] Iteration 77600 (5.64554 iter/s, 17.7131s/100 iter), loss = 1.17913
I0815 13:57:47.074358  8764 solver.cpp:334]     Train net output #0: loss = 1.21175 (* 1 = 1.21175 loss)
I0815 13:57:47.074378  8764 sgd_solver.cpp:136] Iteration 77600, lr = 0.00515, m = 0.9
I0815 13:58:06.216951  8764 solver.cpp:312] Iteration 77700 (5.22407 iter/s, 19.1422s/100 iter), loss = 1.50315
I0815 13:58:06.216979  8764 solver.cpp:334]     Train net output #0: loss = 1.62973 (* 1 = 1.62973 loss)
I0815 13:58:06.216984  8764 sgd_solver.cpp:136] Iteration 77700, lr = 0.00514375, m = 0.9
I0815 13:58:23.965375  8764 solver.cpp:312] Iteration 77800 (5.63446 iter/s, 17.7479s/100 iter), loss = 1.09306
I0815 13:58:23.965610  8764 solver.cpp:334]     Train net output #0: loss = 1.28899 (* 1 = 1.28899 loss)
I0815 13:58:23.965623  8764 sgd_solver.cpp:136] Iteration 77800, lr = 0.0051375, m = 0.9
I0815 13:58:41.033462  8764 solver.cpp:312] Iteration 77900 (5.85905 iter/s, 17.0676s/100 iter), loss = 1.12937
I0815 13:58:41.033483  8764 solver.cpp:334]     Train net output #0: loss = 1.00155 (* 1 = 1.00155 loss)
I0815 13:58:41.033486  8764 sgd_solver.cpp:136] Iteration 77900, lr = 0.00513125, m = 0.9
I0815 13:58:59.328941  8764 solver.cpp:363] Sparsity after update:
I0815 13:58:59.332854  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 13:58:59.332866  8764 net.cpp:2192] conv1a_param_0(0.279) 
I0815 13:58:59.332875  8764 net.cpp:2192] conv1b_param_0(0.564) 
I0815 13:58:59.332878  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 13:58:59.332892  8764 net.cpp:2192] res2a_branch2a_param_0(0.579) 
I0815 13:58:59.332902  8764 net.cpp:2192] res2a_branch2b_param_0(0.571) 
I0815 13:58:59.332911  8764 net.cpp:2192] res3a_branch2a_param_0(0.58) 
I0815 13:58:59.332918  8764 net.cpp:2192] res3a_branch2b_param_0(0.58) 
I0815 13:58:59.332926  8764 net.cpp:2192] res4a_branch2a_param_0(0.58) 
I0815 13:58:59.332934  8764 net.cpp:2192] res4a_branch2b_param_0(0.58) 
I0815 13:58:59.332942  8764 net.cpp:2192] res5a_branch2a_param_0(0.58) 
I0815 13:58:59.332950  8764 net.cpp:2192] res5a_branch2b_param_0(0.58) 
I0815 13:58:59.332958  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.36458e+06/2.86678e+06) 0.476
I0815 13:58:59.332973  8764 solver.cpp:509] Iteration 78000, Testing net (#0)
I0815 13:59:29.671066  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583529
I0815 13:59:29.671135  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810881
I0815 13:59:29.671159  8764 solver.cpp:594]     Test net output #2: loss = 1.81208 (* 1 = 1.81208 loss)
I0815 13:59:29.671195  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.3374s
I0815 13:59:29.894726  8794 solver.cpp:409] Finding and applying sparsity: 0.59
I0815 14:00:08.257700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:00:08.259752  8764 solver.cpp:312] Iteration 78000 (1.14647 iter/s, 87.2239s/100 iter), loss = 1.21672
I0815 14:00:08.259768  8764 solver.cpp:334]     Train net output #0: loss = 1.19087 (* 1 = 1.19087 loss)
I0815 14:00:08.259774  8764 sgd_solver.cpp:136] Iteration 78000, lr = 0.005125, m = 0.9
I0815 14:00:24.420838  8764 solver.cpp:312] Iteration 78100 (6.18788 iter/s, 16.1606s/100 iter), loss = 1.32997
I0815 14:00:24.420866  8764 solver.cpp:334]     Train net output #0: loss = 1.43266 (* 1 = 1.43266 loss)
I0815 14:00:24.420872  8764 sgd_solver.cpp:136] Iteration 78100, lr = 0.00511875, m = 0.9
I0815 14:00:44.617434  8764 solver.cpp:312] Iteration 78200 (4.95147 iter/s, 20.196s/100 iter), loss = 1.42135
I0815 14:00:44.617532  8764 solver.cpp:334]     Train net output #0: loss = 1.52396 (* 1 = 1.52396 loss)
I0815 14:00:44.617550  8764 sgd_solver.cpp:136] Iteration 78200, lr = 0.0051125, m = 0.9
I0815 14:00:59.295253  8764 solver.cpp:312] Iteration 78300 (6.81319 iter/s, 14.6774s/100 iter), loss = 1.47947
I0815 14:00:59.295280  8764 solver.cpp:334]     Train net output #0: loss = 1.26338 (* 1 = 1.26338 loss)
I0815 14:00:59.295286  8764 sgd_solver.cpp:136] Iteration 78300, lr = 0.00510625, m = 0.9
I0815 14:01:17.996472  8764 solver.cpp:312] Iteration 78400 (5.34739 iter/s, 18.7007s/100 iter), loss = 1.52452
I0815 14:01:17.996527  8764 solver.cpp:334]     Train net output #0: loss = 1.53519 (* 1 = 1.53519 loss)
I0815 14:01:17.996533  8764 sgd_solver.cpp:136] Iteration 78400, lr = 0.0051, m = 0.9
I0815 14:01:32.643646  8764 solver.cpp:312] Iteration 78500 (6.82744 iter/s, 14.6468s/100 iter), loss = 1.0836
I0815 14:01:32.643715  8764 solver.cpp:334]     Train net output #0: loss = 1.12205 (* 1 = 1.12205 loss)
I0815 14:01:32.643735  8764 sgd_solver.cpp:136] Iteration 78500, lr = 0.00509375, m = 0.9
I0815 14:01:50.075871  8764 solver.cpp:312] Iteration 78600 (5.73666 iter/s, 17.4317s/100 iter), loss = 1.33488
I0815 14:01:50.075930  8764 solver.cpp:334]     Train net output #0: loss = 1.63349 (* 1 = 1.63349 loss)
I0815 14:01:50.075937  8764 sgd_solver.cpp:136] Iteration 78600, lr = 0.0050875, m = 0.9
I0815 14:02:08.149205  8764 solver.cpp:312] Iteration 78700 (5.53316 iter/s, 18.0728s/100 iter), loss = 1.46113
I0815 14:02:08.149233  8764 solver.cpp:334]     Train net output #0: loss = 1.38657 (* 1 = 1.38657 loss)
I0815 14:02:08.149237  8764 sgd_solver.cpp:136] Iteration 78700, lr = 0.00508125, m = 0.9
I0815 14:02:25.777597  8764 solver.cpp:312] Iteration 78800 (5.67282 iter/s, 17.6279s/100 iter), loss = 1.37533
I0815 14:02:25.780159  8764 solver.cpp:334]     Train net output #0: loss = 1.14315 (* 1 = 1.14315 loss)
I0815 14:02:25.780169  8764 sgd_solver.cpp:136] Iteration 78800, lr = 0.005075, m = 0.9
I0815 14:02:43.186267  8764 solver.cpp:312] Iteration 78900 (5.74442 iter/s, 17.4082s/100 iter), loss = 1.08301
I0815 14:02:43.186295  8764 solver.cpp:334]     Train net output #0: loss = 1.1251 (* 1 = 1.1251 loss)
I0815 14:02:43.186300  8764 sgd_solver.cpp:136] Iteration 78900, lr = 0.00506875, m = 0.9
I0815 14:02:57.866828  8764 solver.cpp:363] Sparsity after update:
I0815 14:02:57.877261  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:02:57.877298  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:02:57.877317  8764 net.cpp:2192] conv1b_param_0(0.575) 
I0815 14:02:57.877329  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:02:57.877341  8764 net.cpp:2192] res2a_branch2a_param_0(0.586) 
I0815 14:02:57.877353  8764 net.cpp:2192] res2a_branch2b_param_0(0.576) 
I0815 14:02:57.877365  8764 net.cpp:2192] res3a_branch2a_param_0(0.589) 
I0815 14:02:57.877377  8764 net.cpp:2192] res3a_branch2b_param_0(0.587) 
I0815 14:02:57.877388  8764 net.cpp:2192] res4a_branch2a_param_0(0.589) 
I0815 14:02:57.877400  8764 net.cpp:2192] res4a_branch2b_param_0(0.589) 
I0815 14:02:57.877418  8764 net.cpp:2192] res5a_branch2a_param_0(0.59) 
I0815 14:02:57.877429  8764 net.cpp:2192] res5a_branch2b_param_0(0.589) 
I0815 14:02:57.877440  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.38722e+06/2.86678e+06) 0.484
I0815 14:02:58.015166  8794 solver.cpp:409] Finding and applying sparsity: 0.6
I0815 14:03:34.802429  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:03:34.804463  8764 solver.cpp:312] Iteration 79000 (1.93735 iter/s, 51.6168s/100 iter), loss = 1.22992
I0815 14:03:34.804486  8764 solver.cpp:334]     Train net output #0: loss = 1.26349 (* 1 = 1.26349 loss)
I0815 14:03:34.804494  8764 sgd_solver.cpp:136] Iteration 79000, lr = 0.0050625, m = 0.9
I0815 14:03:51.758153  8764 solver.cpp:312] Iteration 79100 (5.89859 iter/s, 16.9532s/100 iter), loss = 1.29853
I0815 14:03:51.758178  8764 solver.cpp:334]     Train net output #0: loss = 1.04669 (* 1 = 1.04669 loss)
I0815 14:03:51.758183  8764 sgd_solver.cpp:136] Iteration 79100, lr = 0.00505625, m = 0.9
I0815 14:04:09.609027  8764 solver.cpp:312] Iteration 79200 (5.60212 iter/s, 17.8504s/100 iter), loss = 1.42978
I0815 14:04:09.609091  8764 solver.cpp:334]     Train net output #0: loss = 1.30263 (* 1 = 1.30263 loss)
I0815 14:04:09.609098  8764 sgd_solver.cpp:136] Iteration 79200, lr = 0.00505, m = 0.9
I0815 14:04:27.152170  8764 solver.cpp:312] Iteration 79300 (5.70039 iter/s, 17.5427s/100 iter), loss = 0.961555
I0815 14:04:27.152209  8764 solver.cpp:334]     Train net output #0: loss = 1.09247 (* 1 = 1.09247 loss)
I0815 14:04:27.152216  8764 sgd_solver.cpp:136] Iteration 79300, lr = 0.00504375, m = 0.9
I0815 14:04:44.642822  8764 solver.cpp:312] Iteration 79400 (5.7175 iter/s, 17.4902s/100 iter), loss = 1.43284
I0815 14:04:44.642922  8764 solver.cpp:334]     Train net output #0: loss = 1.51546 (* 1 = 1.51546 loss)
I0815 14:04:44.642936  8764 sgd_solver.cpp:136] Iteration 79400, lr = 0.0050375, m = 0.9
I0815 14:05:02.518226  8764 solver.cpp:312] Iteration 79500 (5.59443 iter/s, 17.8749s/100 iter), loss = 1.41846
I0815 14:05:02.518266  8764 solver.cpp:334]     Train net output #0: loss = 1.48044 (* 1 = 1.48044 loss)
I0815 14:05:02.518275  8764 sgd_solver.cpp:136] Iteration 79500, lr = 0.00503125, m = 0.9
I0815 14:05:23.172166  8764 solver.cpp:312] Iteration 79600 (4.84183 iter/s, 20.6534s/100 iter), loss = 0.87254
I0815 14:05:23.172266  8764 solver.cpp:334]     Train net output #0: loss = 0.940715 (* 1 = 0.940715 loss)
I0815 14:05:23.172282  8764 sgd_solver.cpp:136] Iteration 79600, lr = 0.005025, m = 0.9
I0815 14:05:40.602203  8764 solver.cpp:312] Iteration 79700 (5.73738 iter/s, 17.4296s/100 iter), loss = 1.41946
I0815 14:05:40.602233  8764 solver.cpp:334]     Train net output #0: loss = 1.68366 (* 1 = 1.68366 loss)
I0815 14:05:40.602241  8764 sgd_solver.cpp:136] Iteration 79700, lr = 0.00501875, m = 0.9
I0815 14:05:57.411633  8764 solver.cpp:312] Iteration 79800 (5.94921 iter/s, 16.809s/100 iter), loss = 1.32533
I0815 14:05:57.411728  8764 solver.cpp:334]     Train net output #0: loss = 1.01095 (* 1 = 1.01095 loss)
I0815 14:05:57.411742  8764 sgd_solver.cpp:136] Iteration 79800, lr = 0.0050125, m = 0.9
I0815 14:06:15.473908  8764 solver.cpp:312] Iteration 79900 (5.53655 iter/s, 18.0618s/100 iter), loss = 1.6585
I0815 14:06:15.473968  8764 solver.cpp:334]     Train net output #0: loss = 1.63224 (* 1 = 1.63224 loss)
I0815 14:06:15.473978  8764 sgd_solver.cpp:136] Iteration 79900, lr = 0.00500625, m = 0.9
I0815 14:06:32.495507  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_80000.caffemodel
I0815 14:06:32.525611  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_80000.solverstate
I0815 14:06:32.531903  8764 solver.cpp:363] Sparsity after update:
I0815 14:06:32.533181  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:06:32.533192  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:06:32.533200  8764 net.cpp:2192] conv1b_param_0(0.586) 
I0815 14:06:32.533205  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:06:32.533207  8764 net.cpp:2192] res2a_branch2a_param_0(0.597) 
I0815 14:06:32.533210  8764 net.cpp:2192] res2a_branch2b_param_0(0.588) 
I0815 14:06:32.533215  8764 net.cpp:2192] res3a_branch2a_param_0(0.599) 
I0815 14:06:32.533218  8764 net.cpp:2192] res3a_branch2b_param_0(0.597) 
I0815 14:06:32.533221  8764 net.cpp:2192] res4a_branch2a_param_0(0.6) 
I0815 14:06:32.533224  8764 net.cpp:2192] res4a_branch2b_param_0(0.599) 
I0815 14:06:32.533228  8764 net.cpp:2192] res5a_branch2a_param_0(0.6) 
I0815 14:06:32.533231  8764 net.cpp:2192] res5a_branch2b_param_0(0.6) 
I0815 14:06:32.533234  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.41122e+06/2.86678e+06) 0.492
I0815 14:06:32.533244  8764 solver.cpp:509] Iteration 80000, Testing net (#0)
I0815 14:07:00.605337  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:07:05.642781  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583941
I0815 14:07:05.642839  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814232
I0815 14:07:05.642846  8764 solver.cpp:594]     Test net output #2: loss = 1.82412 (* 1 = 1.82412 loss)
I0815 14:07:05.642864  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.1087s
I0815 14:07:05.793496  8794 solver.cpp:409] Finding and applying sparsity: 0.61
I0815 14:07:44.492436  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:07:44.494474  8764 solver.cpp:312] Iteration 80000 (1.12337 iter/s, 89.0181s/100 iter), loss = 1.22296
I0815 14:07:44.494496  8764 solver.cpp:334]     Train net output #0: loss = 1.18691 (* 1 = 1.18691 loss)
I0815 14:07:44.494504  8764 sgd_solver.cpp:136] Iteration 80000, lr = 0.005, m = 0.9
I0815 14:07:59.976236  8764 solver.cpp:312] Iteration 80100 (6.45939 iter/s, 15.4813s/100 iter), loss = 1.41535
I0815 14:07:59.976260  8764 solver.cpp:334]     Train net output #0: loss = 1.46901 (* 1 = 1.46901 loss)
I0815 14:07:59.976264  8764 sgd_solver.cpp:136] Iteration 80100, lr = 0.00499375, m = 0.9
I0815 14:08:16.844087  8764 solver.cpp:312] Iteration 80200 (5.9286 iter/s, 16.8674s/100 iter), loss = 1.30288
I0815 14:08:16.844185  8764 solver.cpp:334]     Train net output #0: loss = 1.45557 (* 1 = 1.45557 loss)
I0815 14:08:16.844203  8764 sgd_solver.cpp:136] Iteration 80200, lr = 0.0049875, m = 0.9
I0815 14:08:33.728485  8764 solver.cpp:312] Iteration 80300 (5.92279 iter/s, 16.8839s/100 iter), loss = 1.06484
I0815 14:08:33.728555  8764 solver.cpp:334]     Train net output #0: loss = 1.05947 (* 1 = 1.05947 loss)
I0815 14:08:33.728581  8764 sgd_solver.cpp:136] Iteration 80300, lr = 0.00498125, m = 0.9
I0815 14:08:51.143457  8764 solver.cpp:312] Iteration 80400 (5.74234 iter/s, 17.4145s/100 iter), loss = 1.23376
I0815 14:08:51.143512  8764 solver.cpp:334]     Train net output #0: loss = 1.28657 (* 1 = 1.28657 loss)
I0815 14:08:51.143517  8764 sgd_solver.cpp:136] Iteration 80400, lr = 0.004975, m = 0.9
I0815 14:09:07.059029  8764 solver.cpp:312] Iteration 80500 (6.28333 iter/s, 15.9151s/100 iter), loss = 1.43968
I0815 14:09:07.059053  8764 solver.cpp:334]     Train net output #0: loss = 1.13297 (* 1 = 1.13297 loss)
I0815 14:09:07.059059  8764 sgd_solver.cpp:136] Iteration 80500, lr = 0.00496875, m = 0.9
I0815 14:09:22.680541  8764 solver.cpp:312] Iteration 80600 (6.40161 iter/s, 15.6211s/100 iter), loss = 1.65056
I0815 14:09:22.680824  8764 solver.cpp:334]     Train net output #0: loss = 2.00595 (* 1 = 2.00595 loss)
I0815 14:09:22.680940  8764 sgd_solver.cpp:136] Iteration 80600, lr = 0.0049625, m = 0.9
I0815 14:09:41.309136  8764 solver.cpp:312] Iteration 80700 (5.36824 iter/s, 18.6281s/100 iter), loss = 1.35408
I0815 14:09:41.309214  8764 solver.cpp:334]     Train net output #0: loss = 1.40364 (* 1 = 1.40364 loss)
I0815 14:09:41.309237  8764 sgd_solver.cpp:136] Iteration 80700, lr = 0.00495625, m = 0.9
I0815 14:09:58.922379  8764 solver.cpp:312] Iteration 80800 (5.6777 iter/s, 17.6128s/100 iter), loss = 1.19097
I0815 14:09:58.930796  8764 solver.cpp:334]     Train net output #0: loss = 1.41116 (* 1 = 1.41116 loss)
I0815 14:09:58.930810  8764 sgd_solver.cpp:136] Iteration 80800, lr = 0.00495, m = 0.9
I0815 14:10:16.094542  8764 solver.cpp:312] Iteration 80900 (5.82354 iter/s, 17.1717s/100 iter), loss = 1.12402
I0815 14:10:16.094573  8764 solver.cpp:334]     Train net output #0: loss = 1.10179 (* 1 = 1.10179 loss)
I0815 14:10:16.094578  8764 sgd_solver.cpp:136] Iteration 80900, lr = 0.00494375, m = 0.9
I0815 14:10:34.417428  8764 solver.cpp:363] Sparsity after update:
I0815 14:10:34.433223  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:10:34.433262  8764 net.cpp:2192] conv1a_param_0(0.292) 
I0815 14:10:34.433297  8764 net.cpp:2192] conv1b_param_0(0.587) 
I0815 14:10:34.433315  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:10:34.433331  8764 net.cpp:2192] res2a_branch2a_param_0(0.607) 
I0815 14:10:34.433341  8764 net.cpp:2192] res2a_branch2b_param_0(0.593) 
I0815 14:10:34.433349  8764 net.cpp:2192] res3a_branch2a_param_0(0.609) 
I0815 14:10:34.433362  8764 net.cpp:2192] res3a_branch2b_param_0(0.607) 
I0815 14:10:34.433375  8764 net.cpp:2192] res4a_branch2a_param_0(0.609) 
I0815 14:10:34.433393  8764 net.cpp:2192] res4a_branch2b_param_0(0.609) 
I0815 14:10:34.433410  8764 net.cpp:2192] res5a_branch2a_param_0(0.61) 
I0815 14:10:34.433421  8764 net.cpp:2192] res5a_branch2b_param_0(0.609) 
I0815 14:10:34.433436  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.43438e+06/2.86678e+06) 0.5
I0815 14:10:34.808419  8794 solver.cpp:409] Finding and applying sparsity: 0.62
I0815 14:11:16.458614  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:11:16.460680  8764 solver.cpp:312] Iteration 81000 (1.6566 iter/s, 60.3645s/100 iter), loss = 1.33992
I0815 14:11:16.460703  8764 solver.cpp:334]     Train net output #0: loss = 1.13687 (* 1 = 1.13687 loss)
I0815 14:11:16.460712  8764 sgd_solver.cpp:136] Iteration 81000, lr = 0.0049375, m = 0.9
I0815 14:11:36.149379  8764 solver.cpp:312] Iteration 81100 (5.0792 iter/s, 19.6881s/100 iter), loss = 1.40127
I0815 14:11:36.149405  8764 solver.cpp:334]     Train net output #0: loss = 1.35297 (* 1 = 1.35297 loss)
I0815 14:11:36.149440  8764 sgd_solver.cpp:136] Iteration 81100, lr = 0.00493125, m = 0.9
I0815 14:11:52.610958  8764 solver.cpp:312] Iteration 81200 (6.07492 iter/s, 16.4611s/100 iter), loss = 1.05522
I0815 14:11:52.611045  8764 solver.cpp:334]     Train net output #0: loss = 1.18069 (* 1 = 1.18069 loss)
I0815 14:11:52.611063  8764 sgd_solver.cpp:136] Iteration 81200, lr = 0.004925, m = 0.9
I0815 14:12:10.117558  8764 solver.cpp:312] Iteration 81300 (5.71229 iter/s, 17.5061s/100 iter), loss = 1.45076
I0815 14:12:10.117584  8764 solver.cpp:334]     Train net output #0: loss = 0.944007 (* 1 = 0.944007 loss)
I0815 14:12:10.117591  8764 sgd_solver.cpp:136] Iteration 81300, lr = 0.00491875, m = 0.9
I0815 14:12:29.918900  8764 solver.cpp:312] Iteration 81400 (5.0503 iter/s, 19.8008s/100 iter), loss = 1.34157
I0815 14:12:29.918977  8764 solver.cpp:334]     Train net output #0: loss = 1.32597 (* 1 = 1.32597 loss)
I0815 14:12:29.918989  8764 sgd_solver.cpp:136] Iteration 81400, lr = 0.0049125, m = 0.9
I0815 14:12:45.455142  8764 solver.cpp:312] Iteration 81500 (6.43674 iter/s, 15.5358s/100 iter), loss = 1.62228
I0815 14:12:45.455171  8764 solver.cpp:334]     Train net output #0: loss = 1.61507 (* 1 = 1.61507 loss)
I0815 14:12:45.455178  8764 sgd_solver.cpp:136] Iteration 81500, lr = 0.00490625, m = 0.9
I0815 14:13:04.163959  8764 solver.cpp:312] Iteration 81600 (5.34522 iter/s, 18.7083s/100 iter), loss = 1.26786
I0815 14:13:04.164036  8764 solver.cpp:334]     Train net output #0: loss = 1.18083 (* 1 = 1.18083 loss)
I0815 14:13:04.164049  8764 sgd_solver.cpp:136] Iteration 81600, lr = 0.0049, m = 0.9
I0815 14:13:20.646679  8764 solver.cpp:312] Iteration 81700 (6.06713 iter/s, 16.4823s/100 iter), loss = 1.14376
I0815 14:13:20.646708  8764 solver.cpp:334]     Train net output #0: loss = 1.09733 (* 1 = 1.09733 loss)
I0815 14:13:20.646714  8764 sgd_solver.cpp:136] Iteration 81700, lr = 0.00489375, m = 0.9
I0815 14:13:35.895846  8764 solver.cpp:312] Iteration 81800 (6.55791 iter/s, 15.2487s/100 iter), loss = 1.60374
I0815 14:13:35.895923  8764 solver.cpp:334]     Train net output #0: loss = 1.30075 (* 1 = 1.30075 loss)
I0815 14:13:35.895932  8764 sgd_solver.cpp:136] Iteration 81800, lr = 0.0048875, m = 0.9
I0815 14:13:50.791602  8764 solver.cpp:312] Iteration 81900 (6.7135 iter/s, 14.8953s/100 iter), loss = 1.19126
I0815 14:13:50.791672  8764 solver.cpp:334]     Train net output #0: loss = 1.23163 (* 1 = 1.23163 loss)
I0815 14:13:50.791692  8764 sgd_solver.cpp:136] Iteration 81900, lr = 0.00488125, m = 0.9
I0815 14:14:06.921975  8764 solver.cpp:363] Sparsity after update:
I0815 14:14:06.926610  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:14:06.926743  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:14:06.926836  8764 net.cpp:2192] conv1b_param_0(0.597) 
I0815 14:14:06.926923  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:14:06.927006  8764 net.cpp:2192] res2a_branch2a_param_0(0.618) 
I0815 14:14:06.927098  8764 net.cpp:2192] res2a_branch2b_param_0(0.604) 
I0815 14:14:06.927188  8764 net.cpp:2192] res3a_branch2a_param_0(0.62) 
I0815 14:14:06.927278  8764 net.cpp:2192] res3a_branch2b_param_0(0.617) 
I0815 14:14:06.927364  8764 net.cpp:2192] res4a_branch2a_param_0(0.62) 
I0815 14:14:06.927454  8764 net.cpp:2192] res4a_branch2b_param_0(0.62) 
I0815 14:14:06.927542  8764 net.cpp:2192] res5a_branch2a_param_0(0.62) 
I0815 14:14:06.927630  8764 net.cpp:2192] res5a_branch2b_param_0(0.62) 
I0815 14:14:06.927719  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.45839e+06/2.86678e+06) 0.509
I0815 14:14:06.927827  8764 solver.cpp:509] Iteration 82000, Testing net (#0)
I0815 14:14:41.069046  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576882
I0815 14:14:41.069139  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.811586
I0815 14:14:41.069159  8764 solver.cpp:594]     Test net output #2: loss = 1.84846 (* 1 = 1.84846 loss)
I0815 14:14:41.069201  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.1404s
I0815 14:14:41.257091  8794 solver.cpp:409] Finding and applying sparsity: 0.63
I0815 14:15:53.922154  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:15:53.924533  8764 solver.cpp:312] Iteration 82000 (0.812153 iter/s, 123.13s/100 iter), loss = 1.23752
I0815 14:15:53.924582  8764 solver.cpp:334]     Train net output #0: loss = 1.146 (* 1 = 1.146 loss)
I0815 14:15:53.924597  8764 sgd_solver.cpp:136] Iteration 82000, lr = 0.004875, m = 0.9
I0815 14:16:13.142514  8764 solver.cpp:312] Iteration 82100 (5.20361 iter/s, 19.2174s/100 iter), loss = 1.23006
I0815 14:16:13.142604  8764 solver.cpp:334]     Train net output #0: loss = 1.32661 (* 1 = 1.32661 loss)
I0815 14:16:13.142629  8764 sgd_solver.cpp:136] Iteration 82100, lr = 0.00486875, m = 0.9
I0815 14:16:29.358317  8764 solver.cpp:312] Iteration 82200 (6.167 iter/s, 16.2153s/100 iter), loss = 1.39961
I0815 14:16:29.358379  8764 solver.cpp:334]     Train net output #0: loss = 1.10819 (* 1 = 1.10819 loss)
I0815 14:16:29.358386  8764 sgd_solver.cpp:136] Iteration 82200, lr = 0.0048625, m = 0.9
I0815 14:16:45.544492  8764 solver.cpp:312] Iteration 82300 (6.17829 iter/s, 16.1857s/100 iter), loss = 1.35569
I0815 14:16:45.544535  8764 solver.cpp:334]     Train net output #0: loss = 1.03428 (* 1 = 1.03428 loss)
I0815 14:16:45.544548  8764 sgd_solver.cpp:136] Iteration 82300, lr = 0.00485625, m = 0.9
I0815 14:17:04.436385  8764 solver.cpp:312] Iteration 82400 (5.29342 iter/s, 18.8914s/100 iter), loss = 1.21874
I0815 14:17:04.436650  8764 solver.cpp:334]     Train net output #0: loss = 1.36399 (* 1 = 1.36399 loss)
I0815 14:17:04.436755  8764 sgd_solver.cpp:136] Iteration 82400, lr = 0.00485, m = 0.9
I0815 14:17:21.093336  8764 solver.cpp:312] Iteration 82500 (6.00366 iter/s, 16.6565s/100 iter), loss = 1.17384
I0815 14:17:21.093361  8764 solver.cpp:334]     Train net output #0: loss = 0.924864 (* 1 = 0.924864 loss)
I0815 14:17:21.093364  8764 sgd_solver.cpp:136] Iteration 82500, lr = 0.00484375, m = 0.9
I0815 14:17:37.215070  8764 solver.cpp:312] Iteration 82600 (6.20298 iter/s, 16.1213s/100 iter), loss = 1.32248
I0815 14:17:37.215134  8764 solver.cpp:334]     Train net output #0: loss = 1.47368 (* 1 = 1.47368 loss)
I0815 14:17:37.215142  8764 sgd_solver.cpp:136] Iteration 82600, lr = 0.0048375, m = 0.9
I0815 14:17:56.606057  8764 solver.cpp:312] Iteration 82700 (5.15718 iter/s, 19.3904s/100 iter), loss = 1.46028
I0815 14:17:56.613450  8764 solver.cpp:334]     Train net output #0: loss = 1.43146 (* 1 = 1.43146 loss)
I0815 14:17:56.613471  8764 sgd_solver.cpp:136] Iteration 82700, lr = 0.00483125, m = 0.9
I0815 14:18:14.912348  8764 solver.cpp:312] Iteration 82800 (5.46277 iter/s, 18.3057s/100 iter), loss = 1.29521
I0815 14:18:14.912564  8764 solver.cpp:334]     Train net output #0: loss = 1.44741 (* 1 = 1.44741 loss)
I0815 14:18:14.912643  8764 sgd_solver.cpp:136] Iteration 82800, lr = 0.004825, m = 0.9
I0815 14:18:32.626485  8764 solver.cpp:312] Iteration 82900 (5.64536 iter/s, 17.7137s/100 iter), loss = 1.29959
I0815 14:18:32.626543  8764 solver.cpp:334]     Train net output #0: loss = 1.47281 (* 1 = 1.47281 loss)
I0815 14:18:32.626552  8764 sgd_solver.cpp:136] Iteration 82900, lr = 0.00481875, m = 0.9
I0815 14:18:49.641587  8764 solver.cpp:363] Sparsity after update:
I0815 14:18:49.654044  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:18:49.654153  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:18:49.654225  8764 net.cpp:2192] conv1b_param_0(0.606) 
I0815 14:18:49.654291  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:18:49.654353  8764 net.cpp:2192] res2a_branch2a_param_0(0.628) 
I0815 14:18:49.654425  8764 net.cpp:2192] res2a_branch2b_param_0(0.61) 
I0815 14:18:49.654491  8764 net.cpp:2192] res3a_branch2a_param_0(0.628) 
I0815 14:18:49.654557  8764 net.cpp:2192] res3a_branch2b_param_0(0.628) 
I0815 14:18:49.654623  8764 net.cpp:2192] res4a_branch2a_param_0(0.629) 
I0815 14:18:49.654686  8764 net.cpp:2192] res4a_branch2b_param_0(0.628) 
I0815 14:18:49.654752  8764 net.cpp:2192] res5a_branch2a_param_0(0.63) 
I0815 14:18:49.654816  8764 net.cpp:2192] res5a_branch2b_param_0(0.629) 
I0815 14:18:49.654881  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.48118e+06/2.86678e+06) 0.517
I0815 14:18:49.786146  8794 solver.cpp:409] Finding and applying sparsity: 0.64
I0815 14:20:01.378927  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:20:01.380923  8764 solver.cpp:312] Iteration 83000 (1.12673 iter/s, 88.752s/100 iter), loss = 1.29777
I0815 14:20:01.380946  8764 solver.cpp:334]     Train net output #0: loss = 1.32254 (* 1 = 1.32254 loss)
I0815 14:20:01.380954  8764 sgd_solver.cpp:136] Iteration 83000, lr = 0.0048125, m = 0.9
I0815 14:20:19.042560  8764 solver.cpp:312] Iteration 83100 (5.66215 iter/s, 17.6611s/100 iter), loss = 1.58361
I0815 14:20:19.042620  8764 solver.cpp:334]     Train net output #0: loss = 1.62628 (* 1 = 1.62628 loss)
I0815 14:20:19.042637  8764 sgd_solver.cpp:136] Iteration 83100, lr = 0.00480625, m = 0.9
I0815 14:20:37.817981  8764 solver.cpp:312] Iteration 83200 (5.32626 iter/s, 18.7749s/100 iter), loss = 1.2578
I0815 14:20:37.818233  8764 solver.cpp:334]     Train net output #0: loss = 1.5599 (* 1 = 1.5599 loss)
I0815 14:20:37.818325  8764 sgd_solver.cpp:136] Iteration 83200, lr = 0.0048, m = 0.9
I0815 14:20:54.776161  8764 solver.cpp:312] Iteration 83300 (5.89702 iter/s, 16.9577s/100 iter), loss = 1.18968
I0815 14:20:54.776233  8764 solver.cpp:334]     Train net output #0: loss = 1.06591 (* 1 = 1.06591 loss)
I0815 14:20:54.776252  8764 sgd_solver.cpp:136] Iteration 83300, lr = 0.00479375, m = 0.9
I0815 14:21:12.345999  8764 solver.cpp:312] Iteration 83400 (5.69173 iter/s, 17.5694s/100 iter), loss = 1.83297
I0815 14:21:12.346055  8764 solver.cpp:334]     Train net output #0: loss = 2.17298 (* 1 = 2.17298 loss)
I0815 14:21:12.346062  8764 sgd_solver.cpp:136] Iteration 83400, lr = 0.0047875, m = 0.9
I0815 14:21:28.733192  8764 solver.cpp:312] Iteration 83500 (6.1025 iter/s, 16.3867s/100 iter), loss = 1.23815
I0815 14:21:28.733239  8764 solver.cpp:334]     Train net output #0: loss = 1.39546 (* 1 = 1.39546 loss)
I0815 14:21:28.733250  8764 sgd_solver.cpp:136] Iteration 83500, lr = 0.00478125, m = 0.9
I0815 14:21:46.220757  8764 solver.cpp:312] Iteration 83600 (5.71851 iter/s, 17.4871s/100 iter), loss = 1.7465
I0815 14:21:46.220861  8764 solver.cpp:334]     Train net output #0: loss = 1.59385 (* 1 = 1.59385 loss)
I0815 14:21:46.220877  8764 sgd_solver.cpp:136] Iteration 83600, lr = 0.004775, m = 0.9
I0815 14:22:05.888160  8764 solver.cpp:312] Iteration 83700 (5.0847 iter/s, 19.6669s/100 iter), loss = 1.38969
I0815 14:22:05.888204  8764 solver.cpp:334]     Train net output #0: loss = 1.53208 (* 1 = 1.53208 loss)
I0815 14:22:05.888214  8764 sgd_solver.cpp:136] Iteration 83700, lr = 0.00476875, m = 0.9
I0815 14:22:24.355553  8764 solver.cpp:312] Iteration 83800 (5.4151 iter/s, 18.4669s/100 iter), loss = 1.46024
I0815 14:22:24.355629  8764 solver.cpp:334]     Train net output #0: loss = 1.7669 (* 1 = 1.7669 loss)
I0815 14:22:24.355641  8764 sgd_solver.cpp:136] Iteration 83800, lr = 0.0047625, m = 0.9
I0815 14:22:42.120383  8764 solver.cpp:312] Iteration 83900 (5.62925 iter/s, 17.7643s/100 iter), loss = 1.46721
I0815 14:22:42.120410  8764 solver.cpp:334]     Train net output #0: loss = 1.37755 (* 1 = 1.37755 loss)
I0815 14:22:42.120414  8764 sgd_solver.cpp:136] Iteration 83900, lr = 0.00475625, m = 0.9
I0815 14:22:58.909368  8764 solver.cpp:363] Sparsity after update:
I0815 14:22:58.914085  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:22:58.914109  8764 net.cpp:2192] conv1a_param_0(0.305) 
I0815 14:22:58.914124  8764 net.cpp:2192] conv1b_param_0(0.614) 
I0815 14:22:58.914134  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:22:58.914144  8764 net.cpp:2192] res2a_branch2a_param_0(0.638) 
I0815 14:22:58.914151  8764 net.cpp:2192] res2a_branch2b_param_0(0.62) 
I0815 14:22:58.914160  8764 net.cpp:2192] res3a_branch2a_param_0(0.639) 
I0815 14:22:58.914170  8764 net.cpp:2192] res3a_branch2b_param_0(0.637) 
I0815 14:22:58.914178  8764 net.cpp:2192] res4a_branch2a_param_0(0.64) 
I0815 14:22:58.914187  8764 net.cpp:2192] res4a_branch2b_param_0(0.639) 
I0815 14:22:58.914196  8764 net.cpp:2192] res5a_branch2a_param_0(0.64) 
I0815 14:22:58.914204  8764 net.cpp:2192] res5a_branch2b_param_0(0.64) 
I0815 14:22:58.914213  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.50512e+06/2.86678e+06) 0.525
I0815 14:22:58.914230  8764 solver.cpp:509] Iteration 84000, Testing net (#0)
I0815 14:23:24.587138  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:23:31.684767  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585294
I0815 14:23:31.684844  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812585
I0815 14:23:31.684854  8764 solver.cpp:594]     Test net output #2: loss = 1.8141 (* 1 = 1.8141 loss)
I0815 14:23:31.684875  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.7697s
I0815 14:23:31.822388  8794 solver.cpp:409] Finding and applying sparsity: 0.65
I0815 14:24:16.233182  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:24:16.235266  8764 solver.cpp:312] Iteration 84000 (1.06256 iter/s, 94.1123s/100 iter), loss = 1.32017
I0815 14:24:16.235286  8764 solver.cpp:334]     Train net output #0: loss = 1.59795 (* 1 = 1.59795 loss)
I0815 14:24:16.235296  8764 sgd_solver.cpp:136] Iteration 84000, lr = 0.00475, m = 0.9
I0815 14:24:33.632000  8764 solver.cpp:312] Iteration 84100 (5.74837 iter/s, 17.3962s/100 iter), loss = 1.49362
I0815 14:24:33.632277  8764 solver.cpp:334]     Train net output #0: loss = 1.42017 (* 1 = 1.42017 loss)
I0815 14:24:33.632416  8764 sgd_solver.cpp:136] Iteration 84100, lr = 0.00474375, m = 0.9
I0815 14:24:49.715581  8764 solver.cpp:312] Iteration 84200 (6.21769 iter/s, 16.0831s/100 iter), loss = 1.60583
I0815 14:24:49.715637  8764 solver.cpp:334]     Train net output #0: loss = 1.85842 (* 1 = 1.85842 loss)
I0815 14:24:49.715644  8764 sgd_solver.cpp:136] Iteration 84200, lr = 0.0047375, m = 0.9
I0815 14:25:06.675746  8764 solver.cpp:312] Iteration 84300 (5.89633 iter/s, 16.9597s/100 iter), loss = 1.50066
I0815 14:25:06.675772  8764 solver.cpp:334]     Train net output #0: loss = 1.44543 (* 1 = 1.44543 loss)
I0815 14:25:06.675778  8764 sgd_solver.cpp:136] Iteration 84300, lr = 0.00473125, m = 0.9
I0815 14:25:23.736083  8764 solver.cpp:312] Iteration 84400 (5.86171 iter/s, 17.0599s/100 iter), loss = 1.22322
I0815 14:25:23.736217  8764 solver.cpp:334]     Train net output #0: loss = 1.22081 (* 1 = 1.22081 loss)
I0815 14:25:23.736233  8764 sgd_solver.cpp:136] Iteration 84400, lr = 0.004725, m = 0.9
I0815 14:25:40.505393  8764 solver.cpp:312] Iteration 84500 (5.96344 iter/s, 16.7688s/100 iter), loss = 1.42346
I0815 14:25:40.505417  8764 solver.cpp:334]     Train net output #0: loss = 1.27846 (* 1 = 1.27846 loss)
I0815 14:25:40.505422  8764 sgd_solver.cpp:136] Iteration 84500, lr = 0.00471875, m = 0.9
I0815 14:25:57.772173  8764 solver.cpp:312] Iteration 84600 (5.79163 iter/s, 17.2663s/100 iter), loss = 1.26689
I0815 14:25:57.772235  8764 solver.cpp:334]     Train net output #0: loss = 1.10734 (* 1 = 1.10734 loss)
I0815 14:25:57.772243  8764 sgd_solver.cpp:136] Iteration 84600, lr = 0.0047125, m = 0.9
I0815 14:26:19.764334  8764 solver.cpp:312] Iteration 84700 (4.5472 iter/s, 21.9916s/100 iter), loss = 1.30703
I0815 14:26:19.764369  8764 solver.cpp:334]     Train net output #0: loss = 1.08755 (* 1 = 1.08755 loss)
I0815 14:26:19.764377  8764 sgd_solver.cpp:136] Iteration 84700, lr = 0.00470625, m = 0.9
I0815 14:26:40.983780  8764 solver.cpp:312] Iteration 84800 (4.71279 iter/s, 21.2189s/100 iter), loss = 1.36915
I0815 14:26:40.983831  8764 solver.cpp:334]     Train net output #0: loss = 0.98797 (* 1 = 0.98797 loss)
I0815 14:26:40.983839  8764 sgd_solver.cpp:136] Iteration 84800, lr = 0.0047, m = 0.9
I0815 14:26:58.317769  8764 solver.cpp:312] Iteration 84900 (5.76917 iter/s, 17.3335s/100 iter), loss = 1.39134
I0815 14:26:58.317793  8764 solver.cpp:334]     Train net output #0: loss = 1.1584 (* 1 = 1.1584 loss)
I0815 14:26:58.317798  8764 sgd_solver.cpp:136] Iteration 84900, lr = 0.00469375, m = 0.9
I0815 14:27:18.091308  8764 solver.cpp:363] Sparsity after update:
I0815 14:27:18.101887  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:27:18.101992  8764 net.cpp:2192] conv1a_param_0(0.318) 
I0815 14:27:18.102059  8764 net.cpp:2192] conv1b_param_0(0.615) 
I0815 14:27:18.102120  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:27:18.102180  8764 net.cpp:2192] res2a_branch2a_param_0(0.648) 
I0815 14:27:18.102247  8764 net.cpp:2192] res2a_branch2b_param_0(0.625) 
I0815 14:27:18.102310  8764 net.cpp:2192] res3a_branch2a_param_0(0.649) 
I0815 14:27:18.102371  8764 net.cpp:2192] res3a_branch2b_param_0(0.647) 
I0815 14:27:18.102435  8764 net.cpp:2192] res4a_branch2a_param_0(0.649) 
I0815 14:27:18.102497  8764 net.cpp:2192] res4a_branch2b_param_0(0.649) 
I0815 14:27:18.102558  8764 net.cpp:2192] res5a_branch2a_param_0(0.65) 
I0815 14:27:18.102618  8764 net.cpp:2192] res5a_branch2b_param_0(0.649) 
I0815 14:27:18.102680  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.52828e+06/2.86678e+06) 0.533
I0815 14:27:18.337580  8794 solver.cpp:409] Finding and applying sparsity: 0.66
I0815 14:28:37.698923  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:28:37.701458  8764 solver.cpp:312] Iteration 85000 (1.00623 iter/s, 99.381s/100 iter), loss = 1.29193
I0815 14:28:37.701498  8764 solver.cpp:334]     Train net output #0: loss = 0.953374 (* 1 = 0.953374 loss)
I0815 14:28:37.701510  8764 sgd_solver.cpp:136] Iteration 85000, lr = 0.0046875, m = 0.9
I0815 14:28:57.880601  8764 solver.cpp:312] Iteration 85100 (4.95576 iter/s, 20.1786s/100 iter), loss = 1.56708
I0815 14:28:57.880874  8764 solver.cpp:334]     Train net output #0: loss = 1.70273 (* 1 = 1.70273 loss)
I0815 14:28:57.881005  8764 sgd_solver.cpp:136] Iteration 85100, lr = 0.00468125, m = 0.9
I0815 14:29:15.844235  8764 solver.cpp:312] Iteration 85200 (5.56696 iter/s, 17.9631s/100 iter), loss = 1.14244
I0815 14:29:15.844297  8764 solver.cpp:334]     Train net output #0: loss = 1.09223 (* 1 = 1.09223 loss)
I0815 14:29:15.844302  8764 sgd_solver.cpp:136] Iteration 85200, lr = 0.004675, m = 0.9
I0815 14:29:35.047286  8764 solver.cpp:312] Iteration 85300 (5.20765 iter/s, 19.2025s/100 iter), loss = 1.60098
I0815 14:29:35.047355  8764 solver.cpp:334]     Train net output #0: loss = 1.41373 (* 1 = 1.41373 loss)
I0815 14:29:35.047371  8764 sgd_solver.cpp:136] Iteration 85300, lr = 0.00466875, m = 0.9
I0815 14:29:54.085996  8764 solver.cpp:312] Iteration 85400 (5.2526 iter/s, 19.0382s/100 iter), loss = 1.4755
I0815 14:29:54.086127  8764 solver.cpp:334]     Train net output #0: loss = 1.72883 (* 1 = 1.72883 loss)
I0815 14:29:54.086149  8764 sgd_solver.cpp:136] Iteration 85400, lr = 0.0046625, m = 0.9
I0815 14:30:13.460446  8764 solver.cpp:312] Iteration 85500 (5.1616 iter/s, 19.3738s/100 iter), loss = 1.224
I0815 14:30:13.460631  8764 solver.cpp:334]     Train net output #0: loss = 1.26732 (* 1 = 1.26732 loss)
I0815 14:30:13.460698  8764 sgd_solver.cpp:136] Iteration 85500, lr = 0.00465625, m = 0.9
I0815 14:30:33.171718  8764 solver.cpp:312] Iteration 85600 (5.07337 iter/s, 19.7108s/100 iter), loss = 1.60804
I0815 14:30:33.171823  8764 solver.cpp:334]     Train net output #0: loss = 1.50811 (* 1 = 1.50811 loss)
I0815 14:30:33.171841  8764 sgd_solver.cpp:136] Iteration 85600, lr = 0.00465, m = 0.9
I0815 14:30:51.553186  8764 solver.cpp:312] Iteration 85700 (5.44041 iter/s, 18.381s/100 iter), loss = 1.12261
I0815 14:30:51.553231  8764 solver.cpp:334]     Train net output #0: loss = 1.42923 (* 1 = 1.42923 loss)
I0815 14:30:51.553241  8764 sgd_solver.cpp:136] Iteration 85700, lr = 0.00464375, m = 0.9
I0815 14:31:11.297709  8764 solver.cpp:312] Iteration 85800 (5.06483 iter/s, 19.744s/100 iter), loss = 1.1753
I0815 14:31:11.297760  8764 solver.cpp:334]     Train net output #0: loss = 1.37679 (* 1 = 1.37679 loss)
I0815 14:31:11.297765  8764 sgd_solver.cpp:136] Iteration 85800, lr = 0.0046375, m = 0.9
I0815 14:31:29.877710  8764 solver.cpp:312] Iteration 85900 (5.38228 iter/s, 18.5795s/100 iter), loss = 1.32522
I0815 14:31:29.877741  8764 solver.cpp:334]     Train net output #0: loss = 1.28554 (* 1 = 1.28554 loss)
I0815 14:31:29.877748  8764 sgd_solver.cpp:136] Iteration 85900, lr = 0.00463125, m = 0.9
I0815 14:31:46.920151  8764 solver.cpp:363] Sparsity after update:
I0815 14:31:46.925767  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:31:46.925869  8764 net.cpp:2192] conv1a_param_0(0.318) 
I0815 14:31:46.925940  8764 net.cpp:2192] conv1b_param_0(0.624) 
I0815 14:31:46.926004  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:31:46.926067  8764 net.cpp:2192] res2a_branch2a_param_0(0.658) 
I0815 14:31:46.926134  8764 net.cpp:2192] res2a_branch2b_param_0(0.635) 
I0815 14:31:46.926198  8764 net.cpp:2192] res3a_branch2a_param_0(0.66) 
I0815 14:31:46.926261  8764 net.cpp:2192] res3a_branch2b_param_0(0.657) 
I0815 14:31:46.926326  8764 net.cpp:2192] res4a_branch2a_param_0(0.66) 
I0815 14:31:46.926388  8764 net.cpp:2192] res4a_branch2b_param_0(0.66) 
I0815 14:31:46.926452  8764 net.cpp:2192] res5a_branch2a_param_0(0.66) 
I0815 14:31:46.926517  8764 net.cpp:2192] res5a_branch2b_param_0(0.66) 
I0815 14:31:46.926580  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.55222e+06/2.86678e+06) 0.541
I0815 14:31:46.926666  8764 solver.cpp:509] Iteration 86000, Testing net (#0)
I0815 14:32:18.983095  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576588
I0815 14:32:18.983142  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810351
I0815 14:32:18.983148  8764 solver.cpp:594]     Test net output #2: loss = 1.83458 (* 1 = 1.83458 loss)
I0815 14:32:18.983170  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.0556s
I0815 14:32:19.126997  8794 solver.cpp:409] Finding and applying sparsity: 0.67
I0815 14:33:03.220955  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:33:03.223006  8764 solver.cpp:312] Iteration 86000 (1.07132 iter/s, 93.3427s/100 iter), loss = 1.36528
I0815 14:33:03.223026  8764 solver.cpp:334]     Train net output #0: loss = 1.73774 (* 1 = 1.73774 loss)
I0815 14:33:03.223033  8764 sgd_solver.cpp:136] Iteration 86000, lr = 0.004625, m = 0.9
I0815 14:33:19.546802  8764 solver.cpp:312] Iteration 86100 (6.1262 iter/s, 16.3233s/100 iter), loss = 1.38526
I0815 14:33:19.546854  8764 solver.cpp:334]     Train net output #0: loss = 1.22924 (* 1 = 1.22924 loss)
I0815 14:33:19.546864  8764 sgd_solver.cpp:136] Iteration 86100, lr = 0.00461875, m = 0.9
I0815 14:33:38.928611  8764 solver.cpp:312] Iteration 86200 (5.15962 iter/s, 19.3813s/100 iter), loss = 1.38776
I0815 14:33:38.928694  8764 solver.cpp:334]     Train net output #0: loss = 1.16959 (* 1 = 1.16959 loss)
I0815 14:33:38.928707  8764 sgd_solver.cpp:136] Iteration 86200, lr = 0.0046125, m = 0.9
I0815 14:33:58.141645  8764 solver.cpp:312] Iteration 86300 (5.20494 iter/s, 19.2125s/100 iter), loss = 1.4739
I0815 14:33:58.141674  8764 solver.cpp:334]     Train net output #0: loss = 1.41462 (* 1 = 1.41462 loss)
I0815 14:33:58.141681  8764 sgd_solver.cpp:136] Iteration 86300, lr = 0.00460625, m = 0.9
I0815 14:34:15.887079  8764 solver.cpp:312] Iteration 86400 (5.63541 iter/s, 17.7449s/100 iter), loss = 1.20137
I0815 14:34:15.887176  8764 solver.cpp:334]     Train net output #0: loss = 1.12167 (* 1 = 1.12167 loss)
I0815 14:34:15.887195  8764 sgd_solver.cpp:136] Iteration 86400, lr = 0.0046, m = 0.9
I0815 14:34:34.841568  8764 solver.cpp:312] Iteration 86500 (5.27594 iter/s, 18.954s/100 iter), loss = 1.16531
I0815 14:34:34.841593  8764 solver.cpp:334]     Train net output #0: loss = 1.22759 (* 1 = 1.22759 loss)
I0815 14:34:34.841599  8764 sgd_solver.cpp:136] Iteration 86500, lr = 0.00459375, m = 0.9
I0815 14:34:53.444231  8764 solver.cpp:312] Iteration 86600 (5.37572 iter/s, 18.6022s/100 iter), loss = 1.62414
I0815 14:34:53.444283  8764 solver.cpp:334]     Train net output #0: loss = 1.58769 (* 1 = 1.58769 loss)
I0815 14:34:53.444289  8764 sgd_solver.cpp:136] Iteration 86600, lr = 0.0045875, m = 0.9
I0815 14:35:11.453657  8764 solver.cpp:312] Iteration 86700 (5.5528 iter/s, 18.0089s/100 iter), loss = 1.29274
I0815 14:35:11.453728  8764 solver.cpp:334]     Train net output #0: loss = 1.08006 (* 1 = 1.08006 loss)
I0815 14:35:11.453747  8764 sgd_solver.cpp:136] Iteration 86700, lr = 0.00458125, m = 0.9
I0815 14:35:27.215175  8764 solver.cpp:312] Iteration 86800 (6.34474 iter/s, 15.7611s/100 iter), loss = 1.50709
I0815 14:35:27.215240  8764 solver.cpp:334]     Train net output #0: loss = 1.5891 (* 1 = 1.5891 loss)
I0815 14:35:27.215247  8764 sgd_solver.cpp:136] Iteration 86800, lr = 0.004575, m = 0.9
I0815 14:35:44.578204  8764 solver.cpp:312] Iteration 86900 (5.75952 iter/s, 17.3626s/100 iter), loss = 1.11304
I0815 14:35:44.578228  8764 solver.cpp:334]     Train net output #0: loss = 1.05889 (* 1 = 1.05889 loss)
I0815 14:35:44.578233  8764 sgd_solver.cpp:136] Iteration 86900, lr = 0.00456875, m = 0.9
I0815 14:36:05.213459  8764 solver.cpp:363] Sparsity after update:
I0815 14:36:05.225980  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:36:05.226024  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:36:05.226044  8764 net.cpp:2192] conv1b_param_0(0.632) 
I0815 14:36:05.226056  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:36:05.226068  8764 net.cpp:2192] res2a_branch2a_param_0(0.665) 
I0815 14:36:05.226079  8764 net.cpp:2192] res2a_branch2b_param_0(0.64) 
I0815 14:36:05.226092  8764 net.cpp:2192] res3a_branch2a_param_0(0.668) 
I0815 14:36:05.226104  8764 net.cpp:2192] res3a_branch2b_param_0(0.664) 
I0815 14:36:05.226115  8764 net.cpp:2192] res4a_branch2a_param_0(0.669) 
I0815 14:36:05.226127  8764 net.cpp:2192] res4a_branch2b_param_0(0.668) 
I0815 14:36:05.226138  8764 net.cpp:2192] res5a_branch2a_param_0(0.67) 
I0815 14:36:05.226150  8764 net.cpp:2192] res5a_branch2b_param_0(0.669) 
I0815 14:36:05.226161  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.57482e+06/2.86678e+06) 0.549
I0815 14:36:05.356005  8794 solver.cpp:409] Finding and applying sparsity: 0.68
I0815 14:37:17.611243  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:37:17.613293  8764 solver.cpp:312] Iteration 87000 (1.07489 iter/s, 93.0326s/100 iter), loss = 1.1714
I0815 14:37:17.613312  8764 solver.cpp:334]     Train net output #0: loss = 1.27568 (* 1 = 1.27568 loss)
I0815 14:37:17.613317  8764 sgd_solver.cpp:136] Iteration 87000, lr = 0.0045625, m = 0.9
I0815 14:37:36.229511  8764 solver.cpp:312] Iteration 87100 (5.37181 iter/s, 18.6157s/100 iter), loss = 1.28489
I0815 14:37:36.229535  8764 solver.cpp:334]     Train net output #0: loss = 1.58833 (* 1 = 1.58833 loss)
I0815 14:37:36.229539  8764 sgd_solver.cpp:136] Iteration 87100, lr = 0.00455625, m = 0.9
I0815 14:37:52.585364  8764 solver.cpp:312] Iteration 87200 (6.11419 iter/s, 16.3554s/100 iter), loss = 1.45596
I0815 14:37:52.585563  8764 solver.cpp:334]     Train net output #0: loss = 1.38621 (* 1 = 1.38621 loss)
I0815 14:37:52.585571  8764 sgd_solver.cpp:136] Iteration 87200, lr = 0.00455, m = 0.9
I0815 14:38:10.834036  8764 solver.cpp:312] Iteration 87300 (5.48 iter/s, 18.2482s/100 iter), loss = 1.88859
I0815 14:38:10.848152  8764 solver.cpp:334]     Train net output #0: loss = 2.08316 (* 1 = 2.08316 loss)
I0815 14:38:10.848193  8764 sgd_solver.cpp:136] Iteration 87300, lr = 0.00454375, m = 0.9
I0815 14:38:29.648268  8764 solver.cpp:312] Iteration 87400 (5.31527 iter/s, 18.8137s/100 iter), loss = 1.46712
I0815 14:38:29.648385  8764 solver.cpp:334]     Train net output #0: loss = 1.55166 (* 1 = 1.55166 loss)
I0815 14:38:29.648411  8764 sgd_solver.cpp:136] Iteration 87400, lr = 0.0045375, m = 0.9
I0815 14:38:47.247921  8764 solver.cpp:312] Iteration 87500 (5.68209 iter/s, 17.5992s/100 iter), loss = 1.30199
I0815 14:38:47.248009  8764 solver.cpp:334]     Train net output #0: loss = 1.59722 (* 1 = 1.59722 loss)
I0815 14:38:47.248040  8764 sgd_solver.cpp:136] Iteration 87500, lr = 0.00453125, m = 0.9
I0815 14:39:04.814240  8764 solver.cpp:312] Iteration 87600 (5.69287 iter/s, 17.5658s/100 iter), loss = 1.35647
I0815 14:39:04.814344  8764 solver.cpp:334]     Train net output #0: loss = 1.52769 (* 1 = 1.52769 loss)
I0815 14:39:04.814358  8764 sgd_solver.cpp:136] Iteration 87600, lr = 0.004525, m = 0.9
I0815 14:39:26.154489  8764 solver.cpp:312] Iteration 87700 (4.68611 iter/s, 21.3397s/100 iter), loss = 1.37923
I0815 14:39:26.154533  8764 solver.cpp:334]     Train net output #0: loss = 1.38089 (* 1 = 1.38089 loss)
I0815 14:39:26.154546  8764 sgd_solver.cpp:136] Iteration 87700, lr = 0.00451875, m = 0.9
I0815 14:39:45.530897  8764 solver.cpp:312] Iteration 87800 (5.16106 iter/s, 19.3759s/100 iter), loss = 1.60718
I0815 14:39:45.531052  8764 solver.cpp:334]     Train net output #0: loss = 1.86513 (* 1 = 1.86513 loss)
I0815 14:39:45.531091  8764 sgd_solver.cpp:136] Iteration 87800, lr = 0.0045125, m = 0.9
I0815 14:40:01.879732  8764 solver.cpp:312] Iteration 87900 (6.11681 iter/s, 16.3484s/100 iter), loss = 1.47245
I0815 14:40:01.879801  8764 solver.cpp:334]     Train net output #0: loss = 1.43464 (* 1 = 1.43464 loss)
I0815 14:40:01.879815  8764 sgd_solver.cpp:136] Iteration 87900, lr = 0.00450625, m = 0.9
I0815 14:40:19.332657  8764 solver.cpp:363] Sparsity after update:
I0815 14:40:19.337808  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:40:19.337819  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:40:19.337857  8764 net.cpp:2192] conv1b_param_0(0.632) 
I0815 14:40:19.337872  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:40:19.337883  8764 net.cpp:2192] res2a_branch2a_param_0(0.675) 
I0815 14:40:19.337895  8764 net.cpp:2192] res2a_branch2b_param_0(0.645) 
I0815 14:40:19.337909  8764 net.cpp:2192] res3a_branch2a_param_0(0.679) 
I0815 14:40:19.337923  8764 net.cpp:2192] res3a_branch2b_param_0(0.673) 
I0815 14:40:19.337934  8764 net.cpp:2192] res4a_branch2a_param_0(0.68) 
I0815 14:40:19.337946  8764 net.cpp:2192] res4a_branch2b_param_0(0.678) 
I0815 14:40:19.337959  8764 net.cpp:2192] res5a_branch2a_param_0(0.68) 
I0815 14:40:19.337970  8764 net.cpp:2192] res5a_branch2b_param_0(0.68) 
I0815 14:40:19.337981  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.59865e+06/2.86678e+06) 0.558
I0815 14:40:19.338011  8764 solver.cpp:509] Iteration 88000, Testing net (#0)
I0815 14:40:23.270232  8747 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 14:40:31.452127  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:40:48.271651  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585059
I0815 14:40:48.271672  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.813115
I0815 14:40:48.271679  8764 solver.cpp:594]     Test net output #2: loss = 1.81377 (* 1 = 1.81377 loss)
I0815 14:40:48.271698  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.9329s
I0815 14:40:48.424757  8794 solver.cpp:409] Finding and applying sparsity: 0.69
I0815 14:41:34.376721  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:41:34.378772  8764 solver.cpp:312] Iteration 88000 (1.08112 iter/s, 92.4965s/100 iter), loss = 1.16957
I0815 14:41:34.378794  8764 solver.cpp:334]     Train net output #0: loss = 1.0959 (* 1 = 1.0959 loss)
I0815 14:41:34.378801  8764 sgd_solver.cpp:136] Iteration 88000, lr = 0.0045, m = 0.9
I0815 14:41:51.256121  8764 solver.cpp:312] Iteration 88100 (5.92527 iter/s, 16.8769s/100 iter), loss = 1.24707
I0815 14:41:51.256150  8764 solver.cpp:334]     Train net output #0: loss = 1.04758 (* 1 = 1.04758 loss)
I0815 14:41:51.256155  8764 sgd_solver.cpp:136] Iteration 88100, lr = 0.00449375, m = 0.9
I0815 14:42:08.360824  8764 solver.cpp:312] Iteration 88200 (5.84651 iter/s, 17.1042s/100 iter), loss = 1.19267
I0815 14:42:08.360877  8764 solver.cpp:334]     Train net output #0: loss = 1.11566 (* 1 = 1.11566 loss)
I0815 14:42:08.360885  8764 sgd_solver.cpp:136] Iteration 88200, lr = 0.0044875, m = 0.9
I0815 14:42:26.063160  8764 solver.cpp:312] Iteration 88300 (5.64913 iter/s, 17.7018s/100 iter), loss = 1.57109
I0815 14:42:26.063202  8764 solver.cpp:334]     Train net output #0: loss = 1.6236 (* 1 = 1.6236 loss)
I0815 14:42:26.063211  8764 sgd_solver.cpp:136] Iteration 88300, lr = 0.00448125, m = 0.9
I0815 14:42:44.205312  8764 solver.cpp:312] Iteration 88400 (5.51218 iter/s, 18.1417s/100 iter), loss = 1.51424
I0815 14:42:44.205373  8764 solver.cpp:334]     Train net output #0: loss = 1.76304 (* 1 = 1.76304 loss)
I0815 14:42:44.205380  8764 sgd_solver.cpp:136] Iteration 88400, lr = 0.004475, m = 0.9
I0815 14:43:05.165290  8764 solver.cpp:312] Iteration 88500 (4.77113 iter/s, 20.9594s/100 iter), loss = 1.2715
I0815 14:43:05.165356  8764 solver.cpp:334]     Train net output #0: loss = 1.26755 (* 1 = 1.26755 loss)
I0815 14:43:05.165374  8764 sgd_solver.cpp:136] Iteration 88500, lr = 0.00446875, m = 0.9
I0815 14:43:24.708734  8764 solver.cpp:312] Iteration 88600 (5.11695 iter/s, 19.5429s/100 iter), loss = 1.47151
I0815 14:43:24.708827  8764 solver.cpp:334]     Train net output #0: loss = 1.4769 (* 1 = 1.4769 loss)
I0815 14:43:24.708847  8764 sgd_solver.cpp:136] Iteration 88600, lr = 0.0044625, m = 0.9
I0815 14:43:42.052404  8764 solver.cpp:312] Iteration 88700 (5.76596 iter/s, 17.3432s/100 iter), loss = 1.66332
I0815 14:43:42.052469  8764 solver.cpp:334]     Train net output #0: loss = 2.09987 (* 1 = 2.09987 loss)
I0815 14:43:42.052481  8764 sgd_solver.cpp:136] Iteration 88700, lr = 0.00445625, m = 0.9
I0815 14:44:03.166551  8764 solver.cpp:312] Iteration 88800 (4.73629 iter/s, 21.1136s/100 iter), loss = 1.4042
I0815 14:44:03.172232  8764 solver.cpp:334]     Train net output #0: loss = 1.42055 (* 1 = 1.42055 loss)
I0815 14:44:03.172264  8764 sgd_solver.cpp:136] Iteration 88800, lr = 0.00445, m = 0.9
I0815 14:44:20.548537  8764 solver.cpp:312] Iteration 88900 (5.75324 iter/s, 17.3815s/100 iter), loss = 1.42491
I0815 14:44:20.548569  8764 solver.cpp:334]     Train net output #0: loss = 0.877914 (* 1 = 0.877914 loss)
I0815 14:44:20.548575  8764 sgd_solver.cpp:136] Iteration 88900, lr = 0.00444375, m = 0.9
I0815 14:44:37.611142  8764 solver.cpp:363] Sparsity after update:
I0815 14:44:37.623896  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:44:37.623913  8764 net.cpp:2192] conv1a_param_0(0.331) 
I0815 14:44:37.623922  8764 net.cpp:2192] conv1b_param_0(0.638) 
I0815 14:44:37.623926  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:44:37.623929  8764 net.cpp:2192] res2a_branch2a_param_0(0.684) 
I0815 14:44:37.623934  8764 net.cpp:2192] res2a_branch2b_param_0(0.653) 
I0815 14:44:37.623940  8764 net.cpp:2192] res3a_branch2a_param_0(0.689) 
I0815 14:44:37.623946  8764 net.cpp:2192] res3a_branch2b_param_0(0.682) 
I0815 14:44:37.623951  8764 net.cpp:2192] res4a_branch2a_param_0(0.689) 
I0815 14:44:37.623956  8764 net.cpp:2192] res4a_branch2b_param_0(0.688) 
I0815 14:44:37.623961  8764 net.cpp:2192] res5a_branch2a_param_0(0.69) 
I0815 14:44:37.623965  8764 net.cpp:2192] res5a_branch2b_param_0(0.689) 
I0815 14:44:37.623967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.62174e+06/2.86678e+06) 0.566
I0815 14:44:37.757796  8794 solver.cpp:409] Finding and applying sparsity: 0.7
I0815 14:45:26.977248  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:45:26.979370  8764 solver.cpp:312] Iteration 89000 (1.50537 iter/s, 66.429s/100 iter), loss = 1.56292
I0815 14:45:26.979393  8764 solver.cpp:334]     Train net output #0: loss = 1.76041 (* 1 = 1.76041 loss)
I0815 14:45:26.979399  8764 sgd_solver.cpp:136] Iteration 89000, lr = 0.0044375, m = 0.9
I0815 14:45:46.550619  8764 solver.cpp:312] Iteration 89100 (5.10968 iter/s, 19.5707s/100 iter), loss = 1.08682
I0815 14:45:46.550649  8764 solver.cpp:334]     Train net output #0: loss = 1.47752 (* 1 = 1.47752 loss)
I0815 14:45:46.550655  8764 sgd_solver.cpp:136] Iteration 89100, lr = 0.00443125, m = 0.9
I0815 14:46:04.225690  8764 solver.cpp:312] Iteration 89200 (5.65784 iter/s, 17.6746s/100 iter), loss = 1.86854
I0815 14:46:04.225783  8764 solver.cpp:334]     Train net output #0: loss = 1.37382 (* 1 = 1.37382 loss)
I0815 14:46:04.225803  8764 sgd_solver.cpp:136] Iteration 89200, lr = 0.004425, m = 0.9
I0815 14:46:22.927530  8764 solver.cpp:312] Iteration 89300 (5.34722 iter/s, 18.7013s/100 iter), loss = 1.12347
I0815 14:46:22.927620  8764 solver.cpp:334]     Train net output #0: loss = 0.965724 (* 1 = 0.965724 loss)
I0815 14:46:22.927644  8764 sgd_solver.cpp:136] Iteration 89300, lr = 0.00441875, m = 0.9
I0815 14:46:41.663413  8764 solver.cpp:312] Iteration 89400 (5.3375 iter/s, 18.7354s/100 iter), loss = 1.121
I0815 14:46:41.663488  8764 solver.cpp:334]     Train net output #0: loss = 1.21622 (* 1 = 1.21622 loss)
I0815 14:46:41.663496  8764 sgd_solver.cpp:136] Iteration 89400, lr = 0.0044125, m = 0.9
I0815 14:47:02.932246  8764 solver.cpp:312] Iteration 89500 (4.70184 iter/s, 21.2682s/100 iter), loss = 1.21942
I0815 14:47:02.932293  8764 solver.cpp:334]     Train net output #0: loss = 0.935765 (* 1 = 0.935765 loss)
I0815 14:47:02.932304  8764 sgd_solver.cpp:136] Iteration 89500, lr = 0.00440625, m = 0.9
I0815 14:47:23.321877  8764 solver.cpp:312] Iteration 89600 (4.90459 iter/s, 20.3891s/100 iter), loss = 1.34851
I0815 14:47:23.321972  8764 solver.cpp:334]     Train net output #0: loss = 1.31478 (* 1 = 1.31478 loss)
I0815 14:47:23.321992  8764 sgd_solver.cpp:136] Iteration 89600, lr = 0.0044, m = 0.9
I0815 14:47:42.123622  8764 solver.cpp:312] Iteration 89700 (5.3188 iter/s, 18.8012s/100 iter), loss = 1.45222
I0815 14:47:42.123675  8764 solver.cpp:334]     Train net output #0: loss = 1.49028 (* 1 = 1.49028 loss)
I0815 14:47:42.123688  8764 sgd_solver.cpp:136] Iteration 89700, lr = 0.00439375, m = 0.9
I0815 14:48:01.423529  8764 solver.cpp:312] Iteration 89800 (5.18152 iter/s, 19.2994s/100 iter), loss = 1.67831
I0815 14:48:01.423624  8764 solver.cpp:334]     Train net output #0: loss = 1.96836 (* 1 = 1.96836 loss)
I0815 14:48:01.423636  8764 sgd_solver.cpp:136] Iteration 89800, lr = 0.0043875, m = 0.9
I0815 14:48:19.164810  8764 solver.cpp:312] Iteration 89900 (5.63672 iter/s, 17.7408s/100 iter), loss = 1.4803
I0815 14:48:19.164837  8764 solver.cpp:334]     Train net output #0: loss = 1.77303 (* 1 = 1.77303 loss)
I0815 14:48:19.164842  8764 sgd_solver.cpp:136] Iteration 89900, lr = 0.00438125, m = 0.9
I0815 14:48:40.358762  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_90000.caffemodel
I0815 14:48:40.416069  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_90000.solverstate
I0815 14:48:40.428184  8764 solver.cpp:363] Sparsity after update:
I0815 14:48:40.430234  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:48:40.430250  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:48:40.430260  8764 net.cpp:2192] conv1b_param_0(0.645) 
I0815 14:48:40.430265  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:48:40.430269  8764 net.cpp:2192] res2a_branch2a_param_0(0.694) 
I0815 14:48:40.430272  8764 net.cpp:2192] res2a_branch2b_param_0(0.657) 
I0815 14:48:40.430275  8764 net.cpp:2192] res3a_branch2a_param_0(0.7) 
I0815 14:48:40.430279  8764 net.cpp:2192] res3a_branch2b_param_0(0.691) 
I0815 14:48:40.430281  8764 net.cpp:2192] res4a_branch2a_param_0(0.7) 
I0815 14:48:40.430284  8764 net.cpp:2192] res4a_branch2b_param_0(0.698) 
I0815 14:48:40.430287  8764 net.cpp:2192] res5a_branch2a_param_0(0.7) 
I0815 14:48:40.430290  8764 net.cpp:2192] res5a_branch2b_param_0(0.7) 
I0815 14:48:40.430294  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.64552e+06/2.86678e+06) 0.574
I0815 14:48:40.430304  8764 solver.cpp:509] Iteration 90000, Testing net (#0)
I0815 14:49:13.643223  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588118
I0815 14:49:13.643260  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817468
I0815 14:49:13.643266  8764 solver.cpp:594]     Test net output #2: loss = 1.80664 (* 1 = 1.80664 loss)
I0815 14:49:13.643285  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.2121s
I0815 14:49:13.783923  8794 solver.cpp:409] Finding and applying sparsity: 0.71
I0815 14:50:02.106982  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:50:02.109035  8764 solver.cpp:312] Iteration 90000 (0.971426 iter/s, 102.941s/100 iter), loss = 1.75389
I0815 14:50:02.109055  8764 solver.cpp:334]     Train net output #0: loss = 1.93343 (* 1 = 1.93343 loss)
I0815 14:50:02.109063  8764 sgd_solver.cpp:136] Iteration 90000, lr = 0.004375, m = 0.9
I0815 14:50:20.406991  8764 solver.cpp:312] Iteration 90100 (5.46524 iter/s, 18.2974s/100 iter), loss = 1.4065
I0815 14:50:20.407016  8764 solver.cpp:334]     Train net output #0: loss = 1.26586 (* 1 = 1.26586 loss)
I0815 14:50:20.407021  8764 sgd_solver.cpp:136] Iteration 90100, lr = 0.00436875, m = 0.9
I0815 14:50:40.117106  8764 solver.cpp:312] Iteration 90200 (5.07368 iter/s, 19.7096s/100 iter), loss = 1.23461
I0815 14:50:40.117197  8764 solver.cpp:334]     Train net output #0: loss = 1.14493 (* 1 = 1.14493 loss)
I0815 14:50:40.117215  8764 sgd_solver.cpp:136] Iteration 90200, lr = 0.0043625, m = 0.9
I0815 14:50:58.961380  8764 solver.cpp:312] Iteration 90300 (5.3068 iter/s, 18.8438s/100 iter), loss = 1.01083
I0815 14:50:58.961405  8764 solver.cpp:334]     Train net output #0: loss = 0.798531 (* 1 = 0.798531 loss)
I0815 14:50:58.961411  8764 sgd_solver.cpp:136] Iteration 90300, lr = 0.00435625, m = 0.9
I0815 14:51:16.247588  8764 solver.cpp:312] Iteration 90400 (5.78512 iter/s, 17.2857s/100 iter), loss = 1.34646
I0815 14:51:16.247680  8764 solver.cpp:334]     Train net output #0: loss = 1.20842 (* 1 = 1.20842 loss)
I0815 14:51:16.247697  8764 sgd_solver.cpp:136] Iteration 90400, lr = 0.00435, m = 0.9
I0815 14:51:32.535013  8764 solver.cpp:312] Iteration 90500 (6.13992 iter/s, 16.2869s/100 iter), loss = 1.11883
I0815 14:51:32.535308  8764 solver.cpp:334]     Train net output #0: loss = 1.34529 (* 1 = 1.34529 loss)
I0815 14:51:32.535362  8764 sgd_solver.cpp:136] Iteration 90500, lr = 0.00434375, m = 0.9
I0815 14:51:47.855242  8764 solver.cpp:312] Iteration 90600 (6.52747 iter/s, 15.3199s/100 iter), loss = 1.29711
I0815 14:51:47.855357  8764 solver.cpp:334]     Train net output #0: loss = 0.983411 (* 1 = 0.983411 loss)
I0815 14:51:47.855381  8764 sgd_solver.cpp:136] Iteration 90600, lr = 0.0043375, m = 0.9
I0815 14:52:03.638200  8764 solver.cpp:312] Iteration 90700 (6.33612 iter/s, 15.7825s/100 iter), loss = 1.0757
I0815 14:52:03.640756  8764 solver.cpp:334]     Train net output #0: loss = 1.22026 (* 1 = 1.22026 loss)
I0815 14:52:03.640768  8764 sgd_solver.cpp:136] Iteration 90700, lr = 0.00433125, m = 0.9
I0815 14:52:21.888161  8764 solver.cpp:312] Iteration 90800 (5.47961 iter/s, 18.2495s/100 iter), loss = 1.45639
I0815 14:52:21.888219  8764 solver.cpp:334]     Train net output #0: loss = 1.0713 (* 1 = 1.0713 loss)
I0815 14:52:21.888226  8764 sgd_solver.cpp:136] Iteration 90800, lr = 0.004325, m = 0.9
I0815 14:52:39.398057  8764 solver.cpp:312] Iteration 90900 (5.71121 iter/s, 17.5094s/100 iter), loss = 1.86629
I0815 14:52:39.398090  8764 solver.cpp:334]     Train net output #0: loss = 2.06372 (* 1 = 2.06372 loss)
I0815 14:52:39.398097  8764 sgd_solver.cpp:136] Iteration 90900, lr = 0.00431875, m = 0.9
I0815 14:52:58.067046  8764 solver.cpp:363] Sparsity after update:
I0815 14:52:58.079773  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:52:58.079813  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:52:58.079834  8764 net.cpp:2192] conv1b_param_0(0.651) 
I0815 14:52:58.079848  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:52:58.079860  8764 net.cpp:2192] res2a_branch2a_param_0(0.704) 
I0815 14:52:58.079874  8764 net.cpp:2192] res2a_branch2b_param_0(0.664) 
I0815 14:52:58.079885  8764 net.cpp:2192] res3a_branch2a_param_0(0.708) 
I0815 14:52:58.079897  8764 net.cpp:2192] res3a_branch2b_param_0(0.7) 
I0815 14:52:58.079910  8764 net.cpp:2192] res4a_branch2a_param_0(0.709) 
I0815 14:52:58.079923  8764 net.cpp:2192] res4a_branch2b_param_0(0.707) 
I0815 14:52:58.079937  8764 net.cpp:2192] res5a_branch2a_param_0(0.71) 
I0815 14:52:58.079953  8764 net.cpp:2192] res5a_branch2b_param_0(0.709) 
I0815 14:52:58.079967  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.66818e+06/2.86678e+06) 0.582
I0815 14:52:58.277683  8794 solver.cpp:409] Finding and applying sparsity: 0.72
I0815 14:53:50.910208  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:53:50.912241  8764 solver.cpp:312] Iteration 91000 (1.39836 iter/s, 71.5122s/100 iter), loss = 1.68311
I0815 14:53:50.912264  8764 solver.cpp:334]     Train net output #0: loss = 1.81641 (* 1 = 1.81641 loss)
I0815 14:53:50.912272  8764 sgd_solver.cpp:136] Iteration 91000, lr = 0.0043125, m = 0.9
I0815 14:54:06.602551  8764 solver.cpp:312] Iteration 91100 (6.37354 iter/s, 15.6899s/100 iter), loss = 1.4389
I0815 14:54:06.602597  8764 solver.cpp:334]     Train net output #0: loss = 1.71615 (* 1 = 1.71615 loss)
I0815 14:54:06.602605  8764 sgd_solver.cpp:136] Iteration 91100, lr = 0.00430625, m = 0.9
I0815 14:54:25.640394  8764 solver.cpp:312] Iteration 91200 (5.25284 iter/s, 19.0373s/100 iter), loss = 1.29427
I0815 14:54:25.640455  8764 solver.cpp:334]     Train net output #0: loss = 1.05526 (* 1 = 1.05526 loss)
I0815 14:54:25.640461  8764 sgd_solver.cpp:136] Iteration 91200, lr = 0.0043, m = 0.9
I0815 14:54:42.470305  8764 solver.cpp:312] Iteration 91300 (5.94197 iter/s, 16.8294s/100 iter), loss = 1.02891
I0815 14:54:42.470335  8764 solver.cpp:334]     Train net output #0: loss = 1.1025 (* 1 = 1.1025 loss)
I0815 14:54:42.470340  8764 sgd_solver.cpp:136] Iteration 91300, lr = 0.00429375, m = 0.9
I0815 14:54:59.841661  8764 solver.cpp:312] Iteration 91400 (5.75676 iter/s, 17.3709s/100 iter), loss = 1.28412
I0815 14:54:59.841744  8764 solver.cpp:334]     Train net output #0: loss = 1.45173 (* 1 = 1.45173 loss)
I0815 14:54:59.841760  8764 sgd_solver.cpp:136] Iteration 91400, lr = 0.0042875, m = 0.9
I0815 14:55:16.444577  8764 solver.cpp:312] Iteration 91500 (6.0232 iter/s, 16.6025s/100 iter), loss = 1.2571
I0815 14:55:16.444648  8764 solver.cpp:334]     Train net output #0: loss = 1.07985 (* 1 = 1.07985 loss)
I0815 14:55:16.444669  8764 sgd_solver.cpp:136] Iteration 91500, lr = 0.00428125, m = 0.9
I0815 14:55:32.497277  8764 solver.cpp:312] Iteration 91600 (6.22965 iter/s, 16.0523s/100 iter), loss = 1.26925
I0815 14:55:32.497390  8764 solver.cpp:334]     Train net output #0: loss = 1.1627 (* 1 = 1.1627 loss)
I0815 14:55:32.497406  8764 sgd_solver.cpp:136] Iteration 91600, lr = 0.004275, m = 0.9
I0815 14:55:50.877060  8764 solver.cpp:312] Iteration 91700 (5.44091 iter/s, 18.3793s/100 iter), loss = 1.12325
I0815 14:55:50.877270  8764 solver.cpp:334]     Train net output #0: loss = 1.11918 (* 1 = 1.11918 loss)
I0815 14:55:50.877377  8764 sgd_solver.cpp:136] Iteration 91700, lr = 0.00426875, m = 0.9
I0815 14:56:12.024325  8764 solver.cpp:312] Iteration 91800 (4.72887 iter/s, 21.1467s/100 iter), loss = 1.53254
I0815 14:56:12.024461  8764 solver.cpp:334]     Train net output #0: loss = 1.35674 (* 1 = 1.35674 loss)
I0815 14:56:12.024492  8764 sgd_solver.cpp:136] Iteration 91800, lr = 0.0042625, m = 0.9
I0815 14:56:28.134866  8764 solver.cpp:312] Iteration 91900 (6.20728 iter/s, 16.1101s/100 iter), loss = 1.31805
I0815 14:56:28.134933  8764 solver.cpp:334]     Train net output #0: loss = 1.50366 (* 1 = 1.50366 loss)
I0815 14:56:28.134951  8764 sgd_solver.cpp:136] Iteration 91900, lr = 0.00425625, m = 0.9
I0815 14:56:44.085495  8764 solver.cpp:363] Sparsity after update:
I0815 14:56:44.092715  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 14:56:44.092726  8764 net.cpp:2192] conv1a_param_0(0.344) 
I0815 14:56:44.092733  8764 net.cpp:2192] conv1b_param_0(0.651) 
I0815 14:56:44.092736  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 14:56:44.092737  8764 net.cpp:2192] res2a_branch2a_param_0(0.714) 
I0815 14:56:44.092739  8764 net.cpp:2192] res2a_branch2b_param_0(0.668) 
I0815 14:56:44.092741  8764 net.cpp:2192] res3a_branch2a_param_0(0.718) 
I0815 14:56:44.092743  8764 net.cpp:2192] res3a_branch2b_param_0(0.708) 
I0815 14:56:44.092746  8764 net.cpp:2192] res4a_branch2a_param_0(0.72) 
I0815 14:56:44.092747  8764 net.cpp:2192] res4a_branch2b_param_0(0.716) 
I0815 14:56:44.092762  8764 net.cpp:2192] res5a_branch2a_param_0(0.72) 
I0815 14:56:44.092769  8764 net.cpp:2192] res5a_branch2b_param_0(0.72) 
I0815 14:56:44.092773  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.69181e+06/2.86678e+06) 0.59
I0815 14:56:44.092787  8764 solver.cpp:509] Iteration 92000, Testing net (#0)
I0815 14:56:46.722347  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 14:57:13.761045  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575647
I0815 14:57:13.761070  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812232
I0815 14:57:13.761078  8764 solver.cpp:594]     Test net output #2: loss = 1.84922 (* 1 = 1.84922 loss)
I0815 14:57:13.761098  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.6675s
I0815 14:57:13.924527  8794 solver.cpp:409] Finding and applying sparsity: 0.73
I0815 14:58:43.919446  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 14:58:43.921542  8764 solver.cpp:312] Iteration 92000 (0.736469 iter/s, 135.783s/100 iter), loss = 1.50542
I0815 14:58:43.921571  8764 solver.cpp:334]     Train net output #0: loss = 1.29436 (* 1 = 1.29436 loss)
I0815 14:58:43.921581  8764 sgd_solver.cpp:136] Iteration 92000, lr = 0.00425, m = 0.9
I0815 14:59:00.991418  8764 solver.cpp:312] Iteration 92100 (5.85844 iter/s, 17.0694s/100 iter), loss = 1.61081
I0815 14:59:00.991480  8764 solver.cpp:334]     Train net output #0: loss = 1.3064 (* 1 = 1.3064 loss)
I0815 14:59:00.991492  8764 sgd_solver.cpp:136] Iteration 92100, lr = 0.00424375, m = 0.9
I0815 14:59:22.086182  8764 solver.cpp:312] Iteration 92200 (4.74064 iter/s, 21.0942s/100 iter), loss = 1.36812
I0815 14:59:22.086272  8764 solver.cpp:334]     Train net output #0: loss = 1.31836 (* 1 = 1.31836 loss)
I0815 14:59:22.086285  8764 sgd_solver.cpp:136] Iteration 92200, lr = 0.0042375, m = 0.9
I0815 14:59:42.399063  8764 solver.cpp:312] Iteration 92300 (4.92314 iter/s, 20.3122s/100 iter), loss = 1.78761
I0815 14:59:42.399364  8764 solver.cpp:334]     Train net output #0: loss = 1.79527 (* 1 = 1.79527 loss)
I0815 14:59:42.399493  8764 sgd_solver.cpp:136] Iteration 92300, lr = 0.00423125, m = 0.9
I0815 15:00:02.483204  8764 solver.cpp:312] Iteration 92400 (4.97918 iter/s, 20.0836s/100 iter), loss = 1.17203
I0815 15:00:02.483317  8764 solver.cpp:334]     Train net output #0: loss = 1.04947 (* 1 = 1.04947 loss)
I0815 15:00:02.483335  8764 sgd_solver.cpp:136] Iteration 92400, lr = 0.004225, m = 0.9
I0815 15:00:20.604173  8764 solver.cpp:312] Iteration 92500 (5.51862 iter/s, 18.1205s/100 iter), loss = 1.09313
I0815 15:00:20.604218  8764 solver.cpp:334]     Train net output #0: loss = 1.16514 (* 1 = 1.16514 loss)
I0815 15:00:20.604228  8764 sgd_solver.cpp:136] Iteration 92500, lr = 0.00421875, m = 0.9
I0815 15:00:42.773773  8764 solver.cpp:312] Iteration 92600 (4.51081 iter/s, 22.169s/100 iter), loss = 1.46807
I0815 15:00:42.779340  8764 solver.cpp:334]     Train net output #0: loss = 1.73134 (* 1 = 1.73134 loss)
I0815 15:00:42.779368  8764 sgd_solver.cpp:136] Iteration 92600, lr = 0.0042125, m = 0.9
I0815 15:01:03.994838  8764 solver.cpp:312] Iteration 92700 (4.71243 iter/s, 21.2205s/100 iter), loss = 1.32071
I0815 15:01:03.994880  8764 solver.cpp:334]     Train net output #0: loss = 0.73759 (* 1 = 0.73759 loss)
I0815 15:01:03.994889  8764 sgd_solver.cpp:136] Iteration 92700, lr = 0.00420625, m = 0.9
I0815 15:01:24.794260  8764 solver.cpp:312] Iteration 92800 (4.80796 iter/s, 20.7988s/100 iter), loss = 1.93022
I0815 15:01:24.794371  8764 solver.cpp:334]     Train net output #0: loss = 2.3832 (* 1 = 2.3832 loss)
I0815 15:01:24.794386  8764 sgd_solver.cpp:136] Iteration 92800, lr = 0.0042, m = 0.9
I0815 15:01:44.753849  8764 solver.cpp:312] Iteration 92900 (5.01026 iter/s, 19.959s/100 iter), loss = 1.28271
I0815 15:01:44.753944  8764 solver.cpp:334]     Train net output #0: loss = 1.19581 (* 1 = 1.19581 loss)
I0815 15:01:44.753975  8764 sgd_solver.cpp:136] Iteration 92900, lr = 0.00419375, m = 0.9
I0815 15:02:05.950651  8764 solver.cpp:363] Sparsity after update:
I0815 15:02:05.961494  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:02:05.961511  8764 net.cpp:2192] conv1a_param_0(0.357) 
I0815 15:02:05.961519  8764 net.cpp:2192] conv1b_param_0(0.656) 
I0815 15:02:05.961522  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:02:05.961526  8764 net.cpp:2192] res2a_branch2a_param_0(0.723) 
I0815 15:02:05.961529  8764 net.cpp:2192] res2a_branch2b_param_0(0.673) 
I0815 15:02:05.961534  8764 net.cpp:2192] res3a_branch2a_param_0(0.729) 
I0815 15:02:05.961537  8764 net.cpp:2192] res3a_branch2b_param_0(0.716) 
I0815 15:02:05.961540  8764 net.cpp:2192] res4a_branch2a_param_0(0.729) 
I0815 15:02:05.961546  8764 net.cpp:2192] res4a_branch2b_param_0(0.725) 
I0815 15:02:05.961550  8764 net.cpp:2192] res5a_branch2a_param_0(0.73) 
I0815 15:02:05.961558  8764 net.cpp:2192] res5a_branch2b_param_0(0.729) 
I0815 15:02:05.961562  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.71463e+06/2.86678e+06) 0.598
I0815 15:02:06.098601  8794 solver.cpp:409] Finding and applying sparsity: 0.74
I0815 15:03:40.515523  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:03:40.517537  8764 solver.cpp:312] Iteration 93000 (0.863852 iter/s, 115.761s/100 iter), loss = 1.42549
I0815 15:03:40.517562  8764 solver.cpp:334]     Train net output #0: loss = 1.68914 (* 1 = 1.68914 loss)
I0815 15:03:40.517571  8764 sgd_solver.cpp:136] Iteration 93000, lr = 0.0041875, m = 0.9
I0815 15:03:59.112527  8764 solver.cpp:312] Iteration 93100 (5.37794 iter/s, 18.5945s/100 iter), loss = 1.26989
I0815 15:03:59.112553  8764 solver.cpp:334]     Train net output #0: loss = 1.51034 (* 1 = 1.51034 loss)
I0815 15:03:59.112557  8764 sgd_solver.cpp:136] Iteration 93100, lr = 0.00418125, m = 0.9
I0815 15:04:20.557979  8764 solver.cpp:312] Iteration 93200 (4.66312 iter/s, 21.4449s/100 iter), loss = 1.26564
I0815 15:04:20.558037  8764 solver.cpp:334]     Train net output #0: loss = 1.33966 (* 1 = 1.33966 loss)
I0815 15:04:20.558044  8764 sgd_solver.cpp:136] Iteration 93200, lr = 0.004175, m = 0.9
I0815 15:04:41.064877  8764 solver.cpp:312] Iteration 93300 (4.87654 iter/s, 20.5063s/100 iter), loss = 1.28937
I0815 15:04:41.064905  8764 solver.cpp:334]     Train net output #0: loss = 1.15885 (* 1 = 1.15885 loss)
I0815 15:04:41.064911  8764 sgd_solver.cpp:136] Iteration 93300, lr = 0.00416875, m = 0.9
I0815 15:04:59.800959  8764 solver.cpp:312] Iteration 93400 (5.33745 iter/s, 18.7355s/100 iter), loss = 1.74997
I0815 15:04:59.801064  8764 solver.cpp:334]     Train net output #0: loss = 1.90497 (* 1 = 1.90497 loss)
I0815 15:04:59.801080  8764 sgd_solver.cpp:136] Iteration 93400, lr = 0.0041625, m = 0.9
I0815 15:05:20.215062  8764 solver.cpp:312] Iteration 93500 (4.89871 iter/s, 20.4135s/100 iter), loss = 1.41445
I0815 15:05:20.215155  8764 solver.cpp:334]     Train net output #0: loss = 1.50016 (* 1 = 1.50016 loss)
I0815 15:05:20.215179  8764 sgd_solver.cpp:136] Iteration 93500, lr = 0.00415625, m = 0.9
I0815 15:05:41.187738  8764 solver.cpp:312] Iteration 93600 (4.76824 iter/s, 20.9721s/100 iter), loss = 1.49785
I0815 15:05:41.187825  8764 solver.cpp:334]     Train net output #0: loss = 1.31616 (* 1 = 1.31616 loss)
I0815 15:05:41.187839  8764 sgd_solver.cpp:136] Iteration 93600, lr = 0.00415, m = 0.9
I0815 15:05:59.292770  8764 solver.cpp:312] Iteration 93700 (5.52348 iter/s, 18.1045s/100 iter), loss = 1.20637
I0815 15:05:59.320186  8764 solver.cpp:334]     Train net output #0: loss = 1.4168 (* 1 = 1.4168 loss)
I0815 15:05:59.320312  8764 sgd_solver.cpp:136] Iteration 93700, lr = 0.00414375, m = 0.9
I0815 15:06:15.992934  8764 solver.cpp:312] Iteration 93800 (5.98813 iter/s, 16.6997s/100 iter), loss = 1.20546
I0815 15:06:15.993034  8764 solver.cpp:334]     Train net output #0: loss = 1.23131 (* 1 = 1.23131 loss)
I0815 15:06:15.993050  8764 sgd_solver.cpp:136] Iteration 93800, lr = 0.0041375, m = 0.9
I0815 15:06:38.433292  8764 solver.cpp:312] Iteration 93900 (4.45638 iter/s, 22.4397s/100 iter), loss = 1.28
I0815 15:06:38.433318  8764 solver.cpp:334]     Train net output #0: loss = 1.31703 (* 1 = 1.31703 loss)
I0815 15:06:38.433324  8764 sgd_solver.cpp:136] Iteration 93900, lr = 0.00413125, m = 0.9
I0815 15:06:56.397475  8764 solver.cpp:363] Sparsity after update:
I0815 15:06:56.398926  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:06:56.398938  8764 net.cpp:2192] conv1a_param_0(0.357) 
I0815 15:06:56.398947  8764 net.cpp:2192] conv1b_param_0(0.662) 
I0815 15:06:56.398952  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:06:56.398954  8764 net.cpp:2192] res2a_branch2a_param_0(0.732) 
I0815 15:06:56.398958  8764 net.cpp:2192] res2a_branch2b_param_0(0.677) 
I0815 15:06:56.398962  8764 net.cpp:2192] res3a_branch2a_param_0(0.738) 
I0815 15:06:56.398965  8764 net.cpp:2192] res3a_branch2b_param_0(0.724) 
I0815 15:06:56.398968  8764 net.cpp:2192] res4a_branch2a_param_0(0.74) 
I0815 15:06:56.398972  8764 net.cpp:2192] res4a_branch2b_param_0(0.734) 
I0815 15:06:56.398975  8764 net.cpp:2192] res5a_branch2a_param_0(0.74) 
I0815 15:06:56.398978  8764 net.cpp:2192] res5a_branch2b_param_0(0.74) 
I0815 15:06:56.398983  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.73816e+06/2.86678e+06) 0.606
I0815 15:06:56.398993  8764 solver.cpp:509] Iteration 94000, Testing net (#0)
I0815 15:07:24.161453  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.576058
I0815 15:07:24.161487  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.808174
I0815 15:07:24.161497  8764 solver.cpp:594]     Test net output #2: loss = 1.84861 (* 1 = 1.84861 loss)
I0815 15:07:24.161530  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.7618s
I0815 15:07:24.395162  8794 solver.cpp:409] Finding and applying sparsity: 0.75
I0815 15:08:53.455729  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:08:53.457746  8764 solver.cpp:312] Iteration 94000 (0.740627 iter/s, 135.021s/100 iter), loss = 1.50801
I0815 15:08:53.457764  8764 solver.cpp:334]     Train net output #0: loss = 1.85236 (* 1 = 1.85236 loss)
I0815 15:08:53.457772  8764 sgd_solver.cpp:136] Iteration 94000, lr = 0.004125, m = 0.9
I0815 15:09:11.730159  8764 solver.cpp:312] Iteration 94100 (5.47289 iter/s, 18.2719s/100 iter), loss = 1.52659
I0815 15:09:11.730208  8764 solver.cpp:334]     Train net output #0: loss = 1.34791 (* 1 = 1.34791 loss)
I0815 15:09:11.730219  8764 sgd_solver.cpp:136] Iteration 94100, lr = 0.00411875, m = 0.9
I0815 15:09:29.238188  8764 solver.cpp:312] Iteration 94200 (5.71182 iter/s, 17.5075s/100 iter), loss = 1.80044
I0815 15:09:29.238302  8764 solver.cpp:334]     Train net output #0: loss = 1.70249 (* 1 = 1.70249 loss)
I0815 15:09:29.238322  8764 sgd_solver.cpp:136] Iteration 94200, lr = 0.0041125, m = 0.9
I0815 15:09:46.300448  8764 solver.cpp:312] Iteration 94300 (5.86105 iter/s, 17.0618s/100 iter), loss = 1.36339
I0815 15:09:46.300474  8764 solver.cpp:334]     Train net output #0: loss = 1.69927 (* 1 = 1.69927 loss)
I0815 15:09:46.300478  8764 sgd_solver.cpp:136] Iteration 94300, lr = 0.00410625, m = 0.9
I0815 15:10:04.608036  8764 solver.cpp:312] Iteration 94400 (5.46237 iter/s, 18.3071s/100 iter), loss = 1.70226
I0815 15:10:04.608319  8764 solver.cpp:334]     Train net output #0: loss = 1.93784 (* 1 = 1.93784 loss)
I0815 15:10:04.608434  8764 sgd_solver.cpp:136] Iteration 94400, lr = 0.0041, m = 0.9
I0815 15:10:22.194685  8764 solver.cpp:312] Iteration 94500 (5.68629 iter/s, 17.5862s/100 iter), loss = 1.78266
I0815 15:10:22.194731  8764 solver.cpp:334]     Train net output #0: loss = 1.7468 (* 1 = 1.7468 loss)
I0815 15:10:22.194743  8764 sgd_solver.cpp:136] Iteration 94500, lr = 0.00409375, m = 0.9
I0815 15:10:38.786896  8764 solver.cpp:312] Iteration 94600 (6.02709 iter/s, 16.5918s/100 iter), loss = 0.961031
I0815 15:10:38.786954  8764 solver.cpp:334]     Train net output #0: loss = 1.22116 (* 1 = 1.22116 loss)
I0815 15:10:38.786962  8764 sgd_solver.cpp:136] Iteration 94600, lr = 0.0040875, m = 0.9
I0815 15:10:55.313697  8764 solver.cpp:312] Iteration 94700 (6.05094 iter/s, 16.5263s/100 iter), loss = 1.73321
I0815 15:10:55.313724  8764 solver.cpp:334]     Train net output #0: loss = 1.81741 (* 1 = 1.81741 loss)
I0815 15:10:55.313730  8764 sgd_solver.cpp:136] Iteration 94700, lr = 0.00408125, m = 0.9
I0815 15:11:13.353451  8764 solver.cpp:312] Iteration 94800 (5.54347 iter/s, 18.0393s/100 iter), loss = 1.13181
I0815 15:11:13.353514  8764 solver.cpp:334]     Train net output #0: loss = 1.03672 (* 1 = 1.03672 loss)
I0815 15:11:13.353521  8764 sgd_solver.cpp:136] Iteration 94800, lr = 0.004075, m = 0.9
I0815 15:11:14.173437  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:11:32.083815  8764 solver.cpp:312] Iteration 94900 (5.33907 iter/s, 18.7298s/100 iter), loss = 1.27376
I0815 15:11:32.083839  8764 solver.cpp:334]     Train net output #0: loss = 1.29663 (* 1 = 1.29663 loss)
I0815 15:11:32.083845  8764 sgd_solver.cpp:136] Iteration 94900, lr = 0.00406875, m = 0.9
I0815 15:11:51.562678  8764 solver.cpp:363] Sparsity after update:
I0815 15:11:51.575052  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:11:51.575090  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:11:51.575109  8764 net.cpp:2192] conv1b_param_0(0.663) 
I0815 15:11:51.575121  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:11:51.575134  8764 net.cpp:2192] res2a_branch2a_param_0(0.738) 
I0815 15:11:51.575145  8764 net.cpp:2192] res2a_branch2b_param_0(0.68) 
I0815 15:11:51.575157  8764 net.cpp:2192] res3a_branch2a_param_0(0.747) 
I0815 15:11:51.575168  8764 net.cpp:2192] res3a_branch2b_param_0(0.729) 
I0815 15:11:51.575181  8764 net.cpp:2192] res4a_branch2a_param_0(0.749) 
I0815 15:11:51.575191  8764 net.cpp:2192] res4a_branch2b_param_0(0.741) 
I0815 15:11:51.575202  8764 net.cpp:2192] res5a_branch2a_param_0(0.75) 
I0815 15:11:51.575214  8764 net.cpp:2192] res5a_branch2b_param_0(0.749) 
I0815 15:11:51.575225  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.76036e+06/2.86678e+06) 0.614
I0815 15:11:51.705307  8794 solver.cpp:409] Finding and applying sparsity: 0.76
I0815 15:13:20.845085  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:13:20.847239  8764 solver.cpp:312] Iteration 95000 (0.919452 iter/s, 108.76s/100 iter), loss = 1.49047
I0815 15:13:20.847256  8764 solver.cpp:334]     Train net output #0: loss = 1.50446 (* 1 = 1.50446 loss)
I0815 15:13:20.847261  8764 sgd_solver.cpp:136] Iteration 95000, lr = 0.0040625, m = 0.9
I0815 15:13:38.631568  8764 solver.cpp:312] Iteration 95100 (5.62309 iter/s, 17.7838s/100 iter), loss = 1.35874
I0815 15:13:38.631597  8764 solver.cpp:334]     Train net output #0: loss = 1.35765 (* 1 = 1.35765 loss)
I0815 15:13:38.631603  8764 sgd_solver.cpp:136] Iteration 95100, lr = 0.00405625, m = 0.9
I0815 15:13:57.010221  8764 solver.cpp:312] Iteration 95200 (5.44125 iter/s, 18.3781s/100 iter), loss = 1.35214
I0815 15:13:57.010265  8764 solver.cpp:334]     Train net output #0: loss = 1.49137 (* 1 = 1.49137 loss)
I0815 15:13:57.010270  8764 sgd_solver.cpp:136] Iteration 95200, lr = 0.00405, m = 0.9
I0815 15:14:14.239728  8764 solver.cpp:312] Iteration 95300 (5.80416 iter/s, 17.229s/100 iter), loss = 1.24904
I0815 15:14:14.239951  8764 solver.cpp:334]     Train net output #0: loss = 1.28033 (* 1 = 1.28033 loss)
I0815 15:14:14.240058  8764 sgd_solver.cpp:136] Iteration 95300, lr = 0.00404375, m = 0.9
I0815 15:14:31.059710  8764 solver.cpp:312] Iteration 95400 (5.94548 iter/s, 16.8195s/100 iter), loss = 1.39141
I0815 15:14:31.059767  8764 solver.cpp:334]     Train net output #0: loss = 1.35 (* 1 = 1.35 loss)
I0815 15:14:31.059773  8764 sgd_solver.cpp:136] Iteration 95400, lr = 0.0040375, m = 0.9
I0815 15:14:48.572883  8764 solver.cpp:312] Iteration 95500 (5.71014 iter/s, 17.5127s/100 iter), loss = 1.66396
I0815 15:14:48.572906  8764 solver.cpp:334]     Train net output #0: loss = 1.43773 (* 1 = 1.43773 loss)
I0815 15:14:48.572911  8764 sgd_solver.cpp:136] Iteration 95500, lr = 0.00403125, m = 0.9
I0815 15:15:06.080777  8764 solver.cpp:312] Iteration 95600 (5.71187 iter/s, 17.5074s/100 iter), loss = 1.26276
I0815 15:15:06.080858  8764 solver.cpp:334]     Train net output #0: loss = 1.2878 (* 1 = 1.2878 loss)
I0815 15:15:06.080955  8764 sgd_solver.cpp:136] Iteration 95600, lr = 0.004025, m = 0.9
I0815 15:15:24.414449  8764 solver.cpp:312] Iteration 95700 (5.4546 iter/s, 18.3332s/100 iter), loss = 1.54495
I0815 15:15:24.414512  8764 solver.cpp:334]     Train net output #0: loss = 1.51319 (* 1 = 1.51319 loss)
I0815 15:15:24.414531  8764 sgd_solver.cpp:136] Iteration 95700, lr = 0.00401875, m = 0.9
I0815 15:15:42.651855  8764 solver.cpp:312] Iteration 95800 (5.48339 iter/s, 18.2369s/100 iter), loss = 1.36663
I0815 15:15:42.651932  8764 solver.cpp:334]     Train net output #0: loss = 1.41798 (* 1 = 1.41798 loss)
I0815 15:15:42.651950  8764 sgd_solver.cpp:136] Iteration 95800, lr = 0.0040125, m = 0.9
I0815 15:16:05.404757  8764 solver.cpp:312] Iteration 95900 (4.39517 iter/s, 22.7523s/100 iter), loss = 1.32898
I0815 15:16:05.405021  8764 solver.cpp:334]     Train net output #0: loss = 1.21221 (* 1 = 1.21221 loss)
I0815 15:16:05.405154  8764 sgd_solver.cpp:136] Iteration 95900, lr = 0.00400625, m = 0.9
I0815 15:16:25.664582  8764 solver.cpp:363] Sparsity after update:
I0815 15:16:25.668666  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:16:25.668701  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:16:25.668715  8764 net.cpp:2192] conv1b_param_0(0.667) 
I0815 15:16:25.668725  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:16:25.668732  8764 net.cpp:2192] res2a_branch2a_param_0(0.747) 
I0815 15:16:25.668740  8764 net.cpp:2192] res2a_branch2b_param_0(0.685) 
I0815 15:16:25.668747  8764 net.cpp:2192] res3a_branch2a_param_0(0.756) 
I0815 15:16:25.668756  8764 net.cpp:2192] res3a_branch2b_param_0(0.736) 
I0815 15:16:25.668762  8764 net.cpp:2192] res4a_branch2a_param_0(0.759) 
I0815 15:16:25.668771  8764 net.cpp:2192] res4a_branch2b_param_0(0.749) 
I0815 15:16:25.668777  8764 net.cpp:2192] res5a_branch2a_param_0(0.76) 
I0815 15:16:25.668786  8764 net.cpp:2192] res5a_branch2b_param_0(0.759) 
I0815 15:16:25.668792  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.78413e+06/2.86678e+06) 0.622
I0815 15:16:25.668809  8764 solver.cpp:509] Iteration 96000, Testing net (#0)
I0815 15:16:55.149899  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583941
I0815 15:16:55.149991  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.813703
I0815 15:16:55.150035  8764 solver.cpp:594]     Test net output #2: loss = 1.84159 (* 1 = 1.84159 loss)
I0815 15:16:55.150163  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.4805s
I0815 15:16:55.452987  8794 solver.cpp:409] Finding and applying sparsity: 0.77
I0815 15:17:56.326067  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:17:56.328094  8764 solver.cpp:312] Iteration 96000 (0.901548 iter/s, 110.92s/100 iter), loss = 1.5161
I0815 15:17:56.328116  8764 solver.cpp:334]     Train net output #0: loss = 1.57328 (* 1 = 1.57328 loss)
I0815 15:17:56.328124  8764 sgd_solver.cpp:136] Iteration 96000, lr = 0.004, m = 0.9
I0815 15:18:15.286288  8764 solver.cpp:312] Iteration 96100 (5.27491 iter/s, 18.9577s/100 iter), loss = 1.5573
I0815 15:18:15.286319  8764 solver.cpp:334]     Train net output #0: loss = 1.52354 (* 1 = 1.52354 loss)
I0815 15:18:15.286324  8764 sgd_solver.cpp:136] Iteration 96100, lr = 0.00399375, m = 0.9
I0815 15:18:35.821398  8764 solver.cpp:312] Iteration 96200 (4.86985 iter/s, 20.5345s/100 iter), loss = 1.59131
I0815 15:18:35.821538  8764 solver.cpp:334]     Train net output #0: loss = 1.23457 (* 1 = 1.23457 loss)
I0815 15:18:35.821563  8764 sgd_solver.cpp:136] Iteration 96200, lr = 0.0039875, m = 0.9
I0815 15:18:54.011181  8764 solver.cpp:312] Iteration 96300 (5.49775 iter/s, 18.1893s/100 iter), loss = 1.39646
I0815 15:18:54.011423  8764 solver.cpp:334]     Train net output #0: loss = 1.46119 (* 1 = 1.46119 loss)
I0815 15:18:54.011543  8764 sgd_solver.cpp:136] Iteration 96300, lr = 0.00398125, m = 0.9
I0815 15:19:12.656100  8764 solver.cpp:312] Iteration 96400 (5.36354 iter/s, 18.6444s/100 iter), loss = 1.26525
I0815 15:19:12.656162  8764 solver.cpp:334]     Train net output #0: loss = 1.23114 (* 1 = 1.23114 loss)
I0815 15:19:12.656169  8764 sgd_solver.cpp:136] Iteration 96400, lr = 0.003975, m = 0.9
I0815 15:19:30.434979  8764 solver.cpp:312] Iteration 96500 (5.62481 iter/s, 17.7784s/100 iter), loss = 1.41452
I0815 15:19:30.435005  8764 solver.cpp:334]     Train net output #0: loss = 1.61121 (* 1 = 1.61121 loss)
I0815 15:19:30.435009  8764 sgd_solver.cpp:136] Iteration 96500, lr = 0.00396875, m = 0.9
I0815 15:19:47.989297  8764 solver.cpp:312] Iteration 96600 (5.69676 iter/s, 17.5538s/100 iter), loss = 1.00598
I0815 15:19:47.995628  8764 solver.cpp:334]     Train net output #0: loss = 0.976773 (* 1 = 0.976773 loss)
I0815 15:19:47.995649  8764 sgd_solver.cpp:136] Iteration 96600, lr = 0.0039625, m = 0.9
I0815 15:20:06.263543  8764 solver.cpp:312] Iteration 96700 (5.47233 iter/s, 18.2737s/100 iter), loss = 1.38291
I0815 15:20:06.263573  8764 solver.cpp:334]     Train net output #0: loss = 1.37435 (* 1 = 1.37435 loss)
I0815 15:20:06.263579  8764 sgd_solver.cpp:136] Iteration 96700, lr = 0.00395625, m = 0.9
I0815 15:20:23.587369  8764 solver.cpp:312] Iteration 96800 (5.77256 iter/s, 17.3233s/100 iter), loss = 1.42991
I0815 15:20:23.587458  8764 solver.cpp:334]     Train net output #0: loss = 1.04492 (* 1 = 1.04492 loss)
I0815 15:20:23.587467  8764 sgd_solver.cpp:136] Iteration 96800, lr = 0.00395, m = 0.9
I0815 15:20:43.058655  8764 solver.cpp:312] Iteration 96900 (5.13591 iter/s, 19.4708s/100 iter), loss = 1.22658
I0815 15:20:43.058862  8764 solver.cpp:334]     Train net output #0: loss = 1.21035 (* 1 = 1.21035 loss)
I0815 15:20:43.058970  8764 sgd_solver.cpp:136] Iteration 96900, lr = 0.00394375, m = 0.9
I0815 15:21:01.001579  8764 solver.cpp:363] Sparsity after update:
I0815 15:21:01.012076  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:21:01.012090  8764 net.cpp:2192] conv1a_param_0(0.369) 
I0815 15:21:01.012099  8764 net.cpp:2192] conv1b_param_0(0.67) 
I0815 15:21:01.012102  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:21:01.012109  8764 net.cpp:2192] res2a_branch2a_param_0(0.756) 
I0815 15:21:01.012114  8764 net.cpp:2192] res2a_branch2b_param_0(0.687) 
I0815 15:21:01.012118  8764 net.cpp:2192] res3a_branch2a_param_0(0.766) 
I0815 15:21:01.012123  8764 net.cpp:2192] res3a_branch2b_param_0(0.742) 
I0815 15:21:01.012126  8764 net.cpp:2192] res4a_branch2a_param_0(0.769) 
I0815 15:21:01.012136  8764 net.cpp:2192] res4a_branch2b_param_0(0.757) 
I0815 15:21:01.012140  8764 net.cpp:2192] res5a_branch2a_param_0(0.77) 
I0815 15:21:01.012145  8764 net.cpp:2192] res5a_branch2b_param_0(0.77) 
I0815 15:21:01.012148  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.80728e+06/2.86678e+06) 0.63
I0815 15:21:01.140736  8794 solver.cpp:409] Finding and applying sparsity: 0.78
I0815 15:22:47.167987  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:22:47.170308  8764 solver.cpp:312] Iteration 97000 (0.805748 iter/s, 124.108s/100 iter), loss = 1.62172
I0815 15:22:47.170352  8764 solver.cpp:334]     Train net output #0: loss = 1.64237 (* 1 = 1.64237 loss)
I0815 15:22:47.170370  8764 sgd_solver.cpp:136] Iteration 97000, lr = 0.0039375, m = 0.9
I0815 15:23:07.572432  8764 solver.cpp:312] Iteration 97100 (4.90159 iter/s, 20.4015s/100 iter), loss = 1.78638
I0815 15:23:07.572520  8764 solver.cpp:334]     Train net output #0: loss = 1.38922 (* 1 = 1.38922 loss)
I0815 15:23:07.572546  8764 sgd_solver.cpp:136] Iteration 97100, lr = 0.00393125, m = 0.9
I0815 15:23:26.112995  8764 solver.cpp:312] Iteration 97200 (5.39373 iter/s, 18.5401s/100 iter), loss = 1.38585
I0815 15:23:26.113054  8764 solver.cpp:334]     Train net output #0: loss = 1.33909 (* 1 = 1.33909 loss)
I0815 15:23:26.113060  8764 sgd_solver.cpp:136] Iteration 97200, lr = 0.003925, m = 0.9
I0815 15:23:44.853570  8764 solver.cpp:312] Iteration 97300 (5.33616 iter/s, 18.7401s/100 iter), loss = 1.5266
I0815 15:23:44.853624  8764 solver.cpp:334]     Train net output #0: loss = 1.29738 (* 1 = 1.29738 loss)
I0815 15:23:44.853637  8764 sgd_solver.cpp:136] Iteration 97300, lr = 0.00391875, m = 0.9
I0815 15:24:01.112861  8764 solver.cpp:312] Iteration 97400 (6.1505 iter/s, 16.2588s/100 iter), loss = 1.65497
I0815 15:24:01.112936  8764 solver.cpp:334]     Train net output #0: loss = 1.01399 (* 1 = 1.01399 loss)
I0815 15:24:01.112951  8764 sgd_solver.cpp:136] Iteration 97400, lr = 0.0039125, m = 0.9
I0815 15:24:23.493810  8764 solver.cpp:312] Iteration 97500 (4.46821 iter/s, 22.3803s/100 iter), loss = 1.14017
I0815 15:24:23.493851  8764 solver.cpp:334]     Train net output #0: loss = 1.0051 (* 1 = 1.0051 loss)
I0815 15:24:23.493860  8764 sgd_solver.cpp:136] Iteration 97500, lr = 0.00390625, m = 0.9
I0815 15:24:42.541182  8764 solver.cpp:312] Iteration 97600 (5.25021 iter/s, 19.0468s/100 iter), loss = 1.23418
I0815 15:24:42.541275  8764 solver.cpp:334]     Train net output #0: loss = 1.24007 (* 1 = 1.24007 loss)
I0815 15:24:42.541296  8764 sgd_solver.cpp:136] Iteration 97600, lr = 0.0039, m = 0.9
I0815 15:24:57.591622  8764 solver.cpp:312] Iteration 97700 (6.64451 iter/s, 15.05s/100 iter), loss = 1.67353
I0815 15:24:57.591648  8764 solver.cpp:334]     Train net output #0: loss = 1.44158 (* 1 = 1.44158 loss)
I0815 15:24:57.591653  8764 sgd_solver.cpp:136] Iteration 97700, lr = 0.00389375, m = 0.9
I0815 15:25:16.789162  8764 solver.cpp:312] Iteration 97800 (5.20914 iter/s, 19.197s/100 iter), loss = 1.30634
I0815 15:25:16.789225  8764 solver.cpp:334]     Train net output #0: loss = 0.688109 (* 1 = 0.688109 loss)
I0815 15:25:16.789232  8764 sgd_solver.cpp:136] Iteration 97800, lr = 0.0038875, m = 0.9
I0815 15:25:34.934655  8764 solver.cpp:312] Iteration 97900 (5.51116 iter/s, 18.145s/100 iter), loss = 1.64101
I0815 15:25:34.934861  8764 solver.cpp:334]     Train net output #0: loss = 1.90743 (* 1 = 1.90743 loss)
I0815 15:25:34.934973  8764 sgd_solver.cpp:136] Iteration 97900, lr = 0.00388125, m = 0.9
I0815 15:25:54.690176  8764 solver.cpp:363] Sparsity after update:
I0815 15:25:54.694886  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:25:54.694957  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:25:54.694993  8764 net.cpp:2192] conv1b_param_0(0.673) 
I0815 15:25:54.695017  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:25:54.695045  8764 net.cpp:2192] res2a_branch2a_param_0(0.764) 
I0815 15:25:54.695071  8764 net.cpp:2192] res2a_branch2b_param_0(0.691) 
I0815 15:25:54.695094  8764 net.cpp:2192] res3a_branch2a_param_0(0.776) 
I0815 15:25:54.695116  8764 net.cpp:2192] res3a_branch2b_param_0(0.748) 
I0815 15:25:54.695137  8764 net.cpp:2192] res4a_branch2a_param_0(0.779) 
I0815 15:25:54.695158  8764 net.cpp:2192] res4a_branch2b_param_0(0.764) 
I0815 15:25:54.695178  8764 net.cpp:2192] res5a_branch2a_param_0(0.78) 
I0815 15:25:54.695197  8764 net.cpp:2192] res5a_branch2b_param_0(0.779) 
I0815 15:25:54.695217  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.82954e+06/2.86678e+06) 0.638
I0815 15:25:54.695261  8764 solver.cpp:509] Iteration 98000, Testing net (#0)
I0815 15:26:23.604164  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:26:29.667541  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.57947
I0815 15:26:29.667651  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.810998
I0815 15:26:29.667661  8764 solver.cpp:594]     Test net output #2: loss = 1.82851 (* 1 = 1.82851 loss)
I0815 15:26:29.667681  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.9715s
I0815 15:26:29.809829  8794 solver.cpp:409] Finding and applying sparsity: 0.79
I0815 15:28:13.664402  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:28:13.666893  8764 solver.cpp:312] Iteration 98000 (0.630009 iter/s, 158.728s/100 iter), loss = 1.49494
I0815 15:28:13.666929  8764 solver.cpp:334]     Train net output #0: loss = 1.45985 (* 1 = 1.45985 loss)
I0815 15:28:13.666941  8764 sgd_solver.cpp:136] Iteration 98000, lr = 0.003875, m = 0.9
I0815 15:28:31.035413  8764 solver.cpp:312] Iteration 98100 (5.7577 iter/s, 17.368s/100 iter), loss = 1.22629
I0815 15:28:31.035437  8764 solver.cpp:334]     Train net output #0: loss = 1.03451 (* 1 = 1.03451 loss)
I0815 15:28:31.035444  8764 sgd_solver.cpp:136] Iteration 98100, lr = 0.00386875, m = 0.9
I0815 15:28:49.616538  8764 solver.cpp:312] Iteration 98200 (5.38196 iter/s, 18.5806s/100 iter), loss = 1.4253
I0815 15:28:49.616613  8764 solver.cpp:334]     Train net output #0: loss = 1.45612 (* 1 = 1.45612 loss)
I0815 15:28:49.616626  8764 sgd_solver.cpp:136] Iteration 98200, lr = 0.0038625, m = 0.9
I0815 15:29:08.935221  8764 solver.cpp:312] Iteration 98300 (5.17648 iter/s, 19.3181s/100 iter), loss = 1.46963
I0815 15:29:08.935257  8764 solver.cpp:334]     Train net output #0: loss = 1.37614 (* 1 = 1.37614 loss)
I0815 15:29:08.935266  8764 sgd_solver.cpp:136] Iteration 98300, lr = 0.00385625, m = 0.9
I0815 15:29:29.316766  8764 solver.cpp:312] Iteration 98400 (4.90654 iter/s, 20.381s/100 iter), loss = 1.62255
I0815 15:29:29.316864  8764 solver.cpp:334]     Train net output #0: loss = 1.38108 (* 1 = 1.38108 loss)
I0815 15:29:29.316871  8764 sgd_solver.cpp:136] Iteration 98400, lr = 0.00385, m = 0.9
I0815 15:29:45.441030  8764 solver.cpp:312] Iteration 98500 (6.20203 iter/s, 16.1238s/100 iter), loss = 1.67365
I0815 15:29:45.441120  8764 solver.cpp:334]     Train net output #0: loss = 1.04715 (* 1 = 1.04715 loss)
I0815 15:29:45.441162  8764 sgd_solver.cpp:136] Iteration 98500, lr = 0.00384375, m = 0.9
I0815 15:30:04.826122  8764 solver.cpp:312] Iteration 98600 (5.15874 iter/s, 19.3846s/100 iter), loss = 1.8912
I0815 15:30:04.826179  8764 solver.cpp:334]     Train net output #0: loss = 1.60452 (* 1 = 1.60452 loss)
I0815 15:30:04.826185  8764 sgd_solver.cpp:136] Iteration 98600, lr = 0.0038375, m = 0.9
I0815 15:30:22.385090  8764 solver.cpp:312] Iteration 98700 (5.69526 iter/s, 17.5585s/100 iter), loss = 1.42214
I0815 15:30:22.385308  8764 solver.cpp:334]     Train net output #0: loss = 1.35854 (* 1 = 1.35854 loss)
I0815 15:30:22.385408  8764 sgd_solver.cpp:136] Iteration 98700, lr = 0.00383125, m = 0.9
I0815 15:30:39.798665  8764 solver.cpp:312] Iteration 98800 (5.7428 iter/s, 17.4131s/100 iter), loss = 1.61621
I0815 15:30:39.798743  8764 solver.cpp:334]     Train net output #0: loss = 1.96296 (* 1 = 1.96296 loss)
I0815 15:30:39.798751  8764 sgd_solver.cpp:136] Iteration 98800, lr = 0.003825, m = 0.9
I0815 15:30:57.157068  8764 solver.cpp:312] Iteration 98900 (5.76106 iter/s, 17.3579s/100 iter), loss = 1.41937
I0815 15:30:57.157094  8764 solver.cpp:334]     Train net output #0: loss = 1.34817 (* 1 = 1.34817 loss)
I0815 15:30:57.157099  8764 sgd_solver.cpp:136] Iteration 98900, lr = 0.00381875, m = 0.9
I0815 15:31:15.232698  8764 solver.cpp:363] Sparsity after update:
I0815 15:31:15.244277  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:31:15.244305  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:31:15.244320  8764 net.cpp:2192] conv1b_param_0(0.675) 
I0815 15:31:15.244328  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:31:15.244336  8764 net.cpp:2192] res2a_branch2a_param_0(0.772) 
I0815 15:31:15.244344  8764 net.cpp:2192] res2a_branch2b_param_0(0.693) 
I0815 15:31:15.244352  8764 net.cpp:2192] res3a_branch2a_param_0(0.785) 
I0815 15:31:15.244360  8764 net.cpp:2192] res3a_branch2b_param_0(0.753) 
I0815 15:31:15.244367  8764 net.cpp:2192] res4a_branch2a_param_0(0.788) 
I0815 15:31:15.244375  8764 net.cpp:2192] res4a_branch2b_param_0(0.77) 
I0815 15:31:15.244385  8764 net.cpp:2192] res5a_branch2a_param_0(0.79) 
I0815 15:31:15.244393  8764 net.cpp:2192] res5a_branch2b_param_0(0.789) 
I0815 15:31:15.244402  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.85232e+06/2.86678e+06) 0.646
I0815 15:31:15.398624  8794 solver.cpp:409] Finding and applying sparsity: 0.8
I0815 15:33:02.868700  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:33:02.870798  8764 solver.cpp:312] Iteration 99000 (0.79548 iter/s, 125.71s/100 iter), loss = 1.84481
I0815 15:33:02.870818  8764 solver.cpp:334]     Train net output #0: loss = 2.06868 (* 1 = 2.06868 loss)
I0815 15:33:02.870826  8764 sgd_solver.cpp:136] Iteration 99000, lr = 0.0038125, m = 0.9
I0815 15:33:24.284648  8764 solver.cpp:312] Iteration 99100 (4.67001 iter/s, 21.4132s/100 iter), loss = 1.58187
I0815 15:33:24.284700  8764 solver.cpp:334]     Train net output #0: loss = 1.48142 (* 1 = 1.48142 loss)
I0815 15:33:24.284713  8764 sgd_solver.cpp:136] Iteration 99100, lr = 0.00380625, m = 0.9
I0815 15:33:44.096164  8764 solver.cpp:312] Iteration 99200 (5.04771 iter/s, 19.811s/100 iter), loss = 1.75487
I0815 15:33:44.096266  8764 solver.cpp:334]     Train net output #0: loss = 1.71351 (* 1 = 1.71351 loss)
I0815 15:33:44.096287  8764 sgd_solver.cpp:136] Iteration 99200, lr = 0.0038, m = 0.9
I0815 15:34:03.581439  8764 solver.cpp:312] Iteration 99300 (5.13223 iter/s, 19.4847s/100 iter), loss = 1.48841
I0815 15:34:03.581478  8764 solver.cpp:334]     Train net output #0: loss = 1.50102 (* 1 = 1.50102 loss)
I0815 15:34:03.581485  8764 sgd_solver.cpp:136] Iteration 99300, lr = 0.00379375, m = 0.9
I0815 15:34:22.966205  8764 solver.cpp:312] Iteration 99400 (5.15883 iter/s, 19.3842s/100 iter), loss = 1.50794
I0815 15:34:22.966267  8764 solver.cpp:334]     Train net output #0: loss = 1.41021 (* 1 = 1.41021 loss)
I0815 15:34:22.966274  8764 sgd_solver.cpp:136] Iteration 99400, lr = 0.0037875, m = 0.9
I0815 15:34:42.040640  8764 solver.cpp:312] Iteration 99500 (5.24277 iter/s, 19.0739s/100 iter), loss = 1.62552
I0815 15:34:42.040668  8764 solver.cpp:334]     Train net output #0: loss = 1.66402 (* 1 = 1.66402 loss)
I0815 15:34:42.040673  8764 sgd_solver.cpp:136] Iteration 99500, lr = 0.00378125, m = 0.9
I0815 15:35:02.816900  8764 solver.cpp:312] Iteration 99600 (4.81332 iter/s, 20.7757s/100 iter), loss = 1.24257
I0815 15:35:02.817001  8764 solver.cpp:334]     Train net output #0: loss = 1.16879 (* 1 = 1.16879 loss)
I0815 15:35:02.817015  8764 sgd_solver.cpp:136] Iteration 99600, lr = 0.003775, m = 0.9
I0815 15:35:23.869906  8764 solver.cpp:312] Iteration 99700 (4.75005 iter/s, 21.0524s/100 iter), loss = 1.40957
I0815 15:35:23.869971  8764 solver.cpp:334]     Train net output #0: loss = 1.47679 (* 1 = 1.47679 loss)
I0815 15:35:23.869988  8764 sgd_solver.cpp:136] Iteration 99700, lr = 0.00376875, m = 0.9
I0815 15:35:41.284255  8764 solver.cpp:312] Iteration 99800 (5.74256 iter/s, 17.4138s/100 iter), loss = 1.43133
I0815 15:35:41.284421  8764 solver.cpp:334]     Train net output #0: loss = 1.27589 (* 1 = 1.27589 loss)
I0815 15:35:41.284451  8764 sgd_solver.cpp:136] Iteration 99800, lr = 0.0037625, m = 0.9
I0815 15:36:00.169349  8764 solver.cpp:312] Iteration 99900 (5.29533 iter/s, 18.8846s/100 iter), loss = 1.30312
I0815 15:36:00.169397  8764 solver.cpp:334]     Train net output #0: loss = 1.17972 (* 1 = 1.17972 loss)
I0815 15:36:00.169412  8764 sgd_solver.cpp:136] Iteration 99900, lr = 0.00375625, m = 0.9
I0815 15:36:20.801858  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_100000.caffemodel
I0815 15:36:20.877348  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_100000.solverstate
I0815 15:36:20.881842  8764 solver.cpp:363] Sparsity after update:
I0815 15:36:20.885818  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:36:20.885840  8764 net.cpp:2192] conv1a_param_0(0.382) 
I0815 15:36:20.885855  8764 net.cpp:2192] conv1b_param_0(0.677) 
I0815 15:36:20.885865  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:36:20.885874  8764 net.cpp:2192] res2a_branch2a_param_0(0.781) 
I0815 15:36:20.885885  8764 net.cpp:2192] res2a_branch2b_param_0(0.697) 
I0815 15:36:20.885895  8764 net.cpp:2192] res3a_branch2a_param_0(0.792) 
I0815 15:36:20.885902  8764 net.cpp:2192] res3a_branch2b_param_0(0.758) 
I0815 15:36:20.885911  8764 net.cpp:2192] res4a_branch2a_param_0(0.797) 
I0815 15:36:20.885921  8764 net.cpp:2192] res4a_branch2b_param_0(0.776) 
I0815 15:36:20.885928  8764 net.cpp:2192] res5a_branch2a_param_0(0.8) 
I0815 15:36:20.885937  8764 net.cpp:2192] res5a_branch2b_param_0(0.799) 
I0815 15:36:20.885946  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.87401e+06/2.86678e+06) 0.654
I0815 15:36:20.885962  8764 solver.cpp:509] Iteration 100000, Testing net (#0)
I0815 15:36:45.656796  8747 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 15:36:46.301048  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.580588
I0815 15:36:46.301070  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.809174
I0815 15:36:46.301077  8764 solver.cpp:594]     Test net output #2: loss = 1.83862 (* 1 = 1.83862 loss)
I0815 15:36:46.301100  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.4144s
I0815 15:36:46.458827  8794 solver.cpp:409] Finding and applying sparsity: 0.81
I0815 15:37:47.070982  8794 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 15:37:47.073060  8764 solver.cpp:312] Iteration 100000 (0.935447 iter/s, 106.901s/100 iter), loss = 1.14955
I0815 15:37:47.073077  8764 solver.cpp:334]     Train net output #0: loss = 1.28295 (* 1 = 1.28295 loss)
I0815 15:37:47.073082  8764 sgd_solver.cpp:136] Iteration 100000, lr = 0.00375, m = 0.9
I0815 15:38:06.283458  8764 solver.cpp:312] Iteration 100100 (5.20566 iter/s, 19.2099s/100 iter), loss = 2.03431
I0815 15:38:06.283483  8764 solver.cpp:334]     Train net output #0: loss = 1.99049 (* 1 = 1.99049 loss)
I0815 15:38:06.283489  8764 sgd_solver.cpp:136] Iteration 100100, lr = 0.00374375, m = 0.9
I0815 15:38:26.894295  8764 solver.cpp:312] Iteration 100200 (4.85195 iter/s, 20.6103s/100 iter), loss = 1.69256
I0815 15:38:26.894394  8764 solver.cpp:334]     Train net output #0: loss = 1.96718 (* 1 = 1.96718 loss)
I0815 15:38:26.894419  8764 sgd_solver.cpp:136] Iteration 100200, lr = 0.0037375, m = 0.9
I0815 15:38:44.360554  8764 solver.cpp:312] Iteration 100300 (5.72548 iter/s, 17.4658s/100 iter), loss = 1.13287
I0815 15:38:44.360631  8764 solver.cpp:334]     Train net output #0: loss = 1.07133 (* 1 = 1.07133 loss)
I0815 15:38:44.360651  8764 sgd_solver.cpp:136] Iteration 100300, lr = 0.00373125, m = 0.9
I0815 15:39:02.059878  8764 solver.cpp:312] Iteration 100400 (5.6501 iter/s, 17.6988s/100 iter), loss = 1.92705
I0815 15:39:02.060098  8764 solver.cpp:334]     Train net output #0: loss = 2.03329 (* 1 = 2.03329 loss)
I0815 15:39:02.060142  8764 sgd_solver.cpp:136] Iteration 100400, lr = 0.003725, m = 0.9
I0815 15:39:22.593220  8764 solver.cpp:312] Iteration 100500 (4.87028 iter/s, 20.5327s/100 iter), loss = 1.60862
I0815 15:39:22.593334  8764 solver.cpp:334]     Train net output #0: loss = 1.94227 (* 1 = 1.94227 loss)
I0815 15:39:22.593369  8764 sgd_solver.cpp:136] Iteration 100500, lr = 0.00371875, m = 0.9
I0815 15:39:41.181401  8764 solver.cpp:312] Iteration 100600 (5.3799 iter/s, 18.5877s/100 iter), loss = 1.80421
I0815 15:39:41.181493  8764 solver.cpp:334]     Train net output #0: loss = 1.9565 (* 1 = 1.9565 loss)
I0815 15:39:41.181502  8764 sgd_solver.cpp:136] Iteration 100600, lr = 0.0037125, m = 0.9
I0815 15:40:03.465258  8764 solver.cpp:312] Iteration 100700 (4.48768 iter/s, 22.2832s/100 iter), loss = 1.35829
I0815 15:40:03.465299  8764 solver.cpp:334]     Train net output #0: loss = 1.95666 (* 1 = 1.95666 loss)
I0815 15:40:03.465309  8764 sgd_solver.cpp:136] Iteration 100700, lr = 0.00370625, m = 0.9
I0815 15:40:21.480336  8764 solver.cpp:312] Iteration 100800 (5.55106 iter/s, 18.0146s/100 iter), loss = 0.946652
I0815 15:40:21.480396  8764 solver.cpp:334]     Train net output #0: loss = 0.824354 (* 1 = 0.824354 loss)
I0815 15:40:21.480432  8764 sgd_solver.cpp:136] Iteration 100800, lr = 0.0037, m = 0.9
I0815 15:40:39.789916  8764 solver.cpp:312] Iteration 100900 (5.46177 iter/s, 18.3091s/100 iter), loss = 1.42122
I0815 15:40:39.789938  8764 solver.cpp:334]     Train net output #0: loss = 1.62381 (* 1 = 1.62381 loss)
I0815 15:40:39.790043  8764 sgd_solver.cpp:136] Iteration 100900, lr = 0.00369375, m = 0.9
I0815 15:40:59.432695  8764 solver.cpp:363] Sparsity after update:
I0815 15:40:59.441146  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:40:59.441164  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:40:59.441172  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:40:59.441175  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:40:59.441179  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:40:59.441184  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:40:59.441187  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:40:59.441190  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:40:59.441193  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:40:59.441197  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:40:59.441200  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:40:59.441205  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:40:59.441207  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:40:59.774333  8764 solver.cpp:312] Iteration 101000 (5.00404 iter/s, 19.9838s/100 iter), loss = 1.8552
I0815 15:40:59.774413  8764 solver.cpp:334]     Train net output #0: loss = 1.99506 (* 1 = 1.99506 loss)
I0815 15:40:59.774435  8764 sgd_solver.cpp:136] Iteration 101000, lr = 0.0036875, m = 0.9
I0815 15:41:20.365310  8764 solver.cpp:312] Iteration 101100 (4.85663 iter/s, 20.5904s/100 iter), loss = 1.47291
I0815 15:41:20.365336  8764 solver.cpp:334]     Train net output #0: loss = 1.15817 (* 1 = 1.15817 loss)
I0815 15:41:20.365342  8764 sgd_solver.cpp:136] Iteration 101100, lr = 0.00368125, m = 0.9
I0815 15:41:38.547646  8764 solver.cpp:312] Iteration 101200 (5.5 iter/s, 18.1818s/100 iter), loss = 1.26918
I0815 15:41:38.547703  8764 solver.cpp:334]     Train net output #0: loss = 1.26826 (* 1 = 1.26826 loss)
I0815 15:41:38.547708  8764 sgd_solver.cpp:136] Iteration 101200, lr = 0.003675, m = 0.9
I0815 15:41:57.215495  8764 solver.cpp:312] Iteration 101300 (5.35695 iter/s, 18.6673s/100 iter), loss = 1.55624
I0815 15:41:57.215570  8764 solver.cpp:334]     Train net output #0: loss = 1.68862 (* 1 = 1.68862 loss)
I0815 15:41:57.215590  8764 sgd_solver.cpp:136] Iteration 101300, lr = 0.00366875, m = 0.9
I0815 15:42:16.547626  8764 solver.cpp:312] Iteration 101400 (5.17288 iter/s, 19.3316s/100 iter), loss = 1.20204
I0815 15:42:16.547736  8764 solver.cpp:334]     Train net output #0: loss = 1.10286 (* 1 = 1.10286 loss)
I0815 15:42:16.547757  8764 sgd_solver.cpp:136] Iteration 101400, lr = 0.0036625, m = 0.9
I0815 15:42:34.634840  8764 solver.cpp:312] Iteration 101500 (5.52892 iter/s, 18.0867s/100 iter), loss = 1.46288
I0815 15:42:34.635000  8764 solver.cpp:334]     Train net output #0: loss = 1.66568 (* 1 = 1.66568 loss)
I0815 15:42:34.635084  8764 sgd_solver.cpp:136] Iteration 101500, lr = 0.00365625, m = 0.9
I0815 15:42:54.509402  8764 solver.cpp:312] Iteration 101600 (5.03171 iter/s, 19.874s/100 iter), loss = 1.37365
I0815 15:42:54.509676  8764 solver.cpp:334]     Train net output #0: loss = 1.63657 (* 1 = 1.63657 loss)
I0815 15:42:54.509745  8764 sgd_solver.cpp:136] Iteration 101600, lr = 0.00365, m = 0.9
I0815 15:43:13.409581  8764 solver.cpp:312] Iteration 101700 (5.29109 iter/s, 18.8997s/100 iter), loss = 1.35914
I0815 15:43:13.409605  8764 solver.cpp:334]     Train net output #0: loss = 0.893922 (* 1 = 0.893922 loss)
I0815 15:43:13.409610  8764 sgd_solver.cpp:136] Iteration 101700, lr = 0.00364375, m = 0.9
I0815 15:43:31.941485  8764 solver.cpp:312] Iteration 101800 (5.39625 iter/s, 18.5314s/100 iter), loss = 1.64824
I0815 15:43:31.941541  8764 solver.cpp:334]     Train net output #0: loss = 2.23721 (* 1 = 2.23721 loss)
I0815 15:43:31.941550  8764 sgd_solver.cpp:136] Iteration 101800, lr = 0.0036375, m = 0.9
I0815 15:43:49.072962  8764 solver.cpp:312] Iteration 101900 (5.83737 iter/s, 17.131s/100 iter), loss = 1.44591
I0815 15:43:49.072988  8764 solver.cpp:334]     Train net output #0: loss = 1.45907 (* 1 = 1.45907 loss)
I0815 15:43:49.072994  8764 sgd_solver.cpp:136] Iteration 101900, lr = 0.00363125, m = 0.9
I0815 15:44:07.052655  8764 solver.cpp:363] Sparsity after update:
I0815 15:44:07.061389  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:44:07.061413  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:44:07.061419  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:44:07.061424  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:44:07.061427  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:44:07.061434  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:44:07.061437  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:44:07.061441  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:44:07.061444  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:44:07.061449  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:44:07.061451  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:44:07.061455  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:44:07.061458  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:44:07.061470  8764 solver.cpp:509] Iteration 102000, Testing net (#0)
I0815 15:44:16.640926  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:44:42.873944  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.571353
I0815 15:44:42.874110  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.807998
I0815 15:44:42.874130  8764 solver.cpp:594]     Test net output #2: loss = 1.86307 (* 1 = 1.86307 loss)
I0815 15:44:42.874280  8764 solver.cpp:264] [MultiGPU] Tests completed in 35.8118s
I0815 15:44:43.126494  8764 solver.cpp:312] Iteration 102000 (1.85007 iter/s, 54.0521s/100 iter), loss = 1.93814
I0815 15:44:43.126526  8764 solver.cpp:334]     Train net output #0: loss = 2.47777 (* 1 = 2.47777 loss)
I0815 15:44:43.126533  8764 sgd_solver.cpp:136] Iteration 102000, lr = 0.003625, m = 0.9
I0815 15:44:59.149490  8764 solver.cpp:312] Iteration 102100 (6.2412 iter/s, 16.0226s/100 iter), loss = 1.1357
I0815 15:44:59.149543  8764 solver.cpp:334]     Train net output #0: loss = 1.03683 (* 1 = 1.03683 loss)
I0815 15:44:59.149555  8764 sgd_solver.cpp:136] Iteration 102100, lr = 0.00361875, m = 0.9
I0815 15:45:16.710481  8764 solver.cpp:312] Iteration 102200 (5.6946 iter/s, 17.5605s/100 iter), loss = 1.38303
I0815 15:45:16.710582  8764 solver.cpp:334]     Train net output #0: loss = 1.40306 (* 1 = 1.40306 loss)
I0815 15:45:16.710597  8764 sgd_solver.cpp:136] Iteration 102200, lr = 0.0036125, m = 0.9
I0815 15:45:32.933929  8764 solver.cpp:312] Iteration 102300 (6.16409 iter/s, 16.223s/100 iter), loss = 1.72274
I0815 15:45:32.934106  8764 solver.cpp:334]     Train net output #0: loss = 2.13346 (* 1 = 2.13346 loss)
I0815 15:45:32.934191  8764 sgd_solver.cpp:136] Iteration 102300, lr = 0.00360625, m = 0.9
I0815 15:45:50.844070  8764 solver.cpp:312] Iteration 102400 (5.58358 iter/s, 17.9096s/100 iter), loss = 1.15329
I0815 15:45:50.844136  8764 solver.cpp:334]     Train net output #0: loss = 1.37996 (* 1 = 1.37996 loss)
I0815 15:45:50.844143  8764 sgd_solver.cpp:136] Iteration 102400, lr = 0.0036, m = 0.9
I0815 15:46:07.246655  8764 solver.cpp:312] Iteration 102500 (6.09677 iter/s, 16.4021s/100 iter), loss = 1.10059
I0815 15:46:07.246686  8764 solver.cpp:334]     Train net output #0: loss = 1.04606 (* 1 = 1.04606 loss)
I0815 15:46:07.246693  8764 sgd_solver.cpp:136] Iteration 102500, lr = 0.00359375, m = 0.9
I0815 15:46:23.381911  8764 solver.cpp:312] Iteration 102600 (6.19778 iter/s, 16.1348s/100 iter), loss = 1.18578
I0815 15:46:23.381999  8764 solver.cpp:334]     Train net output #0: loss = 0.967594 (* 1 = 0.967594 loss)
I0815 15:46:23.382011  8764 sgd_solver.cpp:136] Iteration 102600, lr = 0.0035875, m = 0.9
I0815 15:46:42.461120  8764 solver.cpp:312] Iteration 102700 (5.24145 iter/s, 19.0787s/100 iter), loss = 1.50521
I0815 15:46:42.461370  8764 solver.cpp:334]     Train net output #0: loss = 1.54623 (* 1 = 1.54623 loss)
I0815 15:46:42.461486  8764 sgd_solver.cpp:136] Iteration 102700, lr = 0.00358125, m = 0.9
I0815 15:46:59.255754  8764 solver.cpp:312] Iteration 102800 (5.95445 iter/s, 16.7942s/100 iter), loss = 1.71349
I0815 15:46:59.255954  8764 solver.cpp:334]     Train net output #0: loss = 1.30193 (* 1 = 1.30193 loss)
I0815 15:46:59.255967  8764 sgd_solver.cpp:136] Iteration 102800, lr = 0.003575, m = 0.9
I0815 15:47:15.530717  8764 solver.cpp:312] Iteration 102900 (6.14458 iter/s, 16.2745s/100 iter), loss = 1.38444
I0815 15:47:15.530766  8764 solver.cpp:334]     Train net output #0: loss = 1.20229 (* 1 = 1.20229 loss)
I0815 15:47:15.530776  8764 sgd_solver.cpp:136] Iteration 102900, lr = 0.00356875, m = 0.9
I0815 15:47:32.358680  8764 solver.cpp:363] Sparsity after update:
I0815 15:47:32.370265  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:47:32.370307  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:47:32.370327  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:47:32.370339  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:47:32.370352  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:47:32.370368  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:47:32.370381  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:47:32.370393  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:47:32.370404  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:47:32.370416  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:47:32.370430  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:47:32.370443  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:47:32.370455  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:47:32.518007  8764 solver.cpp:312] Iteration 103000 (5.88692 iter/s, 16.9868s/100 iter), loss = 1.47524
I0815 15:47:32.518069  8764 solver.cpp:334]     Train net output #0: loss = 0.976127 (* 1 = 0.976127 loss)
I0815 15:47:32.518086  8764 sgd_solver.cpp:136] Iteration 103000, lr = 0.0035625, m = 0.9
I0815 15:47:49.597251  8764 solver.cpp:312] Iteration 103100 (5.85522 iter/s, 17.0788s/100 iter), loss = 1.19524
I0815 15:47:49.597296  8764 solver.cpp:334]     Train net output #0: loss = 0.979024 (* 1 = 0.979024 loss)
I0815 15:47:49.597309  8764 sgd_solver.cpp:136] Iteration 103100, lr = 0.00355625, m = 0.9
I0815 15:48:06.996870  8764 solver.cpp:312] Iteration 103200 (5.74741 iter/s, 17.3991s/100 iter), loss = 1.41383
I0815 15:48:06.997120  8764 solver.cpp:334]     Train net output #0: loss = 1.42129 (* 1 = 1.42129 loss)
I0815 15:48:06.997231  8764 sgd_solver.cpp:136] Iteration 103200, lr = 0.00355, m = 0.9
I0815 15:48:24.237177  8764 solver.cpp:312] Iteration 103300 (5.80052 iter/s, 17.2398s/100 iter), loss = 1.60673
I0815 15:48:24.237205  8764 solver.cpp:334]     Train net output #0: loss = 1.89872 (* 1 = 1.89872 loss)
I0815 15:48:24.237210  8764 sgd_solver.cpp:136] Iteration 103300, lr = 0.00354375, m = 0.9
I0815 15:48:39.427260  8764 solver.cpp:312] Iteration 103400 (6.58342 iter/s, 15.1897s/100 iter), loss = 1.55347
I0815 15:48:39.427321  8764 solver.cpp:334]     Train net output #0: loss = 1.75798 (* 1 = 1.75798 loss)
I0815 15:48:39.427330  8764 sgd_solver.cpp:136] Iteration 103400, lr = 0.0035375, m = 0.9
I0815 15:48:56.237741  8764 solver.cpp:312] Iteration 103500 (5.94884 iter/s, 16.81s/100 iter), loss = 1.34739
I0815 15:48:56.237828  8764 solver.cpp:334]     Train net output #0: loss = 1.11753 (* 1 = 1.11753 loss)
I0815 15:48:56.237853  8764 sgd_solver.cpp:136] Iteration 103500, lr = 0.00353125, m = 0.9
I0815 15:49:14.273829  8764 solver.cpp:312] Iteration 103600 (5.54459 iter/s, 18.0356s/100 iter), loss = 1.34195
I0815 15:49:14.280164  8764 solver.cpp:334]     Train net output #0: loss = 1.40104 (* 1 = 1.40104 loss)
I0815 15:49:14.280174  8764 sgd_solver.cpp:136] Iteration 103600, lr = 0.003525, m = 0.9
I0815 15:49:32.768304  8764 solver.cpp:312] Iteration 103700 (5.40717 iter/s, 18.494s/100 iter), loss = 1.16567
I0815 15:49:32.768409  8764 solver.cpp:334]     Train net output #0: loss = 1.47474 (* 1 = 1.47474 loss)
I0815 15:49:32.768427  8764 sgd_solver.cpp:136] Iteration 103700, lr = 0.00351875, m = 0.9
I0815 15:49:51.665650  8764 solver.cpp:312] Iteration 103800 (5.29189 iter/s, 18.8968s/100 iter), loss = 1.32617
I0815 15:49:51.665736  8764 solver.cpp:334]     Train net output #0: loss = 1.24121 (* 1 = 1.24121 loss)
I0815 15:49:51.665752  8764 sgd_solver.cpp:136] Iteration 103800, lr = 0.0035125, m = 0.9
I0815 15:50:12.116166  8764 solver.cpp:312] Iteration 103900 (4.88999 iter/s, 20.45s/100 iter), loss = 1.5258
I0815 15:50:12.116195  8764 solver.cpp:334]     Train net output #0: loss = 1.40691 (* 1 = 1.40691 loss)
I0815 15:50:12.116201  8764 sgd_solver.cpp:136] Iteration 103900, lr = 0.00350625, m = 0.9
I0815 15:50:31.972296  8764 solver.cpp:363] Sparsity after update:
I0815 15:50:31.978124  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:50:31.978288  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:50:31.978389  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:50:31.978483  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:50:31.978571  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:50:31.978670  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:50:31.978760  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:50:31.978853  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:50:31.978945  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:50:31.979038  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:50:31.979132  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:50:31.979220  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:50:31.979310  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:50:31.979418  8764 solver.cpp:509] Iteration 104000, Testing net (#0)
I0815 15:51:05.452145  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.575647
I0815 15:51:05.452198  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.809645
I0815 15:51:05.452203  8764 solver.cpp:594]     Test net output #2: loss = 1.84575 (* 1 = 1.84575 loss)
I0815 15:51:05.452224  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.4719s
I0815 15:51:05.611678  8764 solver.cpp:312] Iteration 104000 (1.86937 iter/s, 53.494s/100 iter), loss = 1.38233
I0815 15:51:05.611834  8764 solver.cpp:334]     Train net output #0: loss = 1.24265 (* 1 = 1.24265 loss)
I0815 15:51:05.611922  8764 sgd_solver.cpp:136] Iteration 104000, lr = 0.0035, m = 0.9
I0815 15:51:26.564837  8764 solver.cpp:312] Iteration 104100 (4.77268 iter/s, 20.9526s/100 iter), loss = 1.33392
I0815 15:51:26.564865  8764 solver.cpp:334]     Train net output #0: loss = 1.15895 (* 1 = 1.15895 loss)
I0815 15:51:26.564870  8764 sgd_solver.cpp:136] Iteration 104100, lr = 0.00349375, m = 0.9
I0815 15:51:48.548784  8764 solver.cpp:312] Iteration 104200 (4.5489 iter/s, 21.9833s/100 iter), loss = 1.52176
I0815 15:51:48.548912  8764 solver.cpp:334]     Train net output #0: loss = 1.88896 (* 1 = 1.88896 loss)
I0815 15:51:48.548930  8764 sgd_solver.cpp:136] Iteration 104200, lr = 0.0034875, m = 0.9
I0815 15:52:06.783097  8764 solver.cpp:312] Iteration 104300 (5.48432 iter/s, 18.2338s/100 iter), loss = 1.40122
I0815 15:52:06.783139  8764 solver.cpp:334]     Train net output #0: loss = 1.64431 (* 1 = 1.64431 loss)
I0815 15:52:06.783151  8764 sgd_solver.cpp:136] Iteration 104300, lr = 0.00348125, m = 0.9
I0815 15:52:26.731212  8764 solver.cpp:312] Iteration 104400 (5.01314 iter/s, 19.9476s/100 iter), loss = 1.95052
I0815 15:52:26.731315  8764 solver.cpp:334]     Train net output #0: loss = 1.93198 (* 1 = 1.93198 loss)
I0815 15:52:26.731329  8764 sgd_solver.cpp:136] Iteration 104400, lr = 0.003475, m = 0.9
I0815 15:52:50.787714  8764 solver.cpp:312] Iteration 104500 (4.157 iter/s, 24.0558s/100 iter), loss = 1.54016
I0815 15:52:50.801390  8764 solver.cpp:334]     Train net output #0: loss = 1.41076 (* 1 = 1.41076 loss)
I0815 15:52:50.801409  8764 sgd_solver.cpp:136] Iteration 104500, lr = 0.00346875, m = 0.9
I0815 15:53:11.339115  8764 solver.cpp:312] Iteration 104600 (4.86598 iter/s, 20.5508s/100 iter), loss = 1.34923
I0815 15:53:11.339380  8764 solver.cpp:334]     Train net output #0: loss = 1.33608 (* 1 = 1.33608 loss)
I0815 15:53:11.339494  8764 sgd_solver.cpp:136] Iteration 104600, lr = 0.0034625, m = 0.9
I0815 15:53:30.745293  8764 solver.cpp:312] Iteration 104700 (5.15314 iter/s, 19.4056s/100 iter), loss = 1.06672
I0815 15:53:30.745316  8764 solver.cpp:334]     Train net output #0: loss = 1.24763 (* 1 = 1.24763 loss)
I0815 15:53:30.745321  8764 sgd_solver.cpp:136] Iteration 104700, lr = 0.00345625, m = 0.9
I0815 15:53:51.236145  8764 solver.cpp:312] Iteration 104800 (4.88036 iter/s, 20.4903s/100 iter), loss = 1.33759
I0815 15:53:51.236202  8764 solver.cpp:334]     Train net output #0: loss = 1.29361 (* 1 = 1.29361 loss)
I0815 15:53:51.236212  8764 sgd_solver.cpp:136] Iteration 104800, lr = 0.00345, m = 0.9
I0815 15:54:12.701725  8764 solver.cpp:312] Iteration 104900 (4.65875 iter/s, 21.465s/100 iter), loss = 1.72312
I0815 15:54:12.701764  8764 solver.cpp:334]     Train net output #0: loss = 1.57296 (* 1 = 1.57296 loss)
I0815 15:54:12.701773  8764 sgd_solver.cpp:136] Iteration 104900, lr = 0.00344375, m = 0.9
I0815 15:54:30.579212  8764 solver.cpp:363] Sparsity after update:
I0815 15:54:30.591642  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:54:30.591658  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:54:30.591666  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:54:30.591670  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:54:30.591675  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:54:30.591678  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:54:30.591681  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:54:30.591686  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:54:30.591689  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:54:30.591692  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:54:30.591697  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:54:30.591701  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:54:30.591704  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:54:30.728979  8764 solver.cpp:312] Iteration 105000 (5.54731 iter/s, 18.0268s/100 iter), loss = 1.09316
I0815 15:54:30.739414  8764 solver.cpp:334]     Train net output #0: loss = 1.4025 (* 1 = 1.4025 loss)
I0815 15:54:30.739559  8764 sgd_solver.cpp:136] Iteration 105000, lr = 0.0034375, m = 0.9
I0815 15:54:52.243422  8764 solver.cpp:312] Iteration 105100 (4.64817 iter/s, 21.5138s/100 iter), loss = 1.32188
I0815 15:54:52.243508  8764 solver.cpp:334]     Train net output #0: loss = 1.12064 (* 1 = 1.12064 loss)
I0815 15:54:52.243526  8764 sgd_solver.cpp:136] Iteration 105100, lr = 0.00343125, m = 0.9
I0815 15:55:12.399266  8764 solver.cpp:312] Iteration 105200 (4.96148 iter/s, 20.1553s/100 iter), loss = 1.58474
I0815 15:55:12.399531  8764 solver.cpp:334]     Train net output #0: loss = 1.40318 (* 1 = 1.40318 loss)
I0815 15:55:12.399638  8764 sgd_solver.cpp:136] Iteration 105200, lr = 0.003425, m = 0.9
I0815 15:55:29.802278  8764 solver.cpp:312] Iteration 105300 (5.74629 iter/s, 17.4025s/100 iter), loss = 1.60502
I0815 15:55:29.802335  8764 solver.cpp:334]     Train net output #0: loss = 1.50929 (* 1 = 1.50929 loss)
I0815 15:55:29.802351  8764 sgd_solver.cpp:136] Iteration 105300, lr = 0.00341875, m = 0.9
I0815 15:55:49.457494  8764 solver.cpp:312] Iteration 105400 (5.08786 iter/s, 19.6546s/100 iter), loss = 1.56611
I0815 15:55:49.457676  8764 solver.cpp:334]     Train net output #0: loss = 1.39362 (* 1 = 1.39362 loss)
I0815 15:55:49.457747  8764 sgd_solver.cpp:136] Iteration 105400, lr = 0.0034125, m = 0.9
I0815 15:56:02.020067  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 15:56:10.540179  8764 solver.cpp:312] Iteration 105500 (4.74335 iter/s, 21.0821s/100 iter), loss = 1.20497
I0815 15:56:10.540223  8764 solver.cpp:334]     Train net output #0: loss = 1.01518 (* 1 = 1.01518 loss)
I0815 15:56:10.540236  8764 sgd_solver.cpp:136] Iteration 105500, lr = 0.00340625, m = 0.9
I0815 15:56:29.573487  8764 solver.cpp:312] Iteration 105600 (5.2541 iter/s, 19.0328s/100 iter), loss = 1.5095
I0815 15:56:29.573585  8764 solver.cpp:334]     Train net output #0: loss = 1.09912 (* 1 = 1.09912 loss)
I0815 15:56:29.573596  8764 sgd_solver.cpp:136] Iteration 105600, lr = 0.0034, m = 0.9
I0815 15:56:48.018499  8764 solver.cpp:312] Iteration 105700 (5.42167 iter/s, 18.4445s/100 iter), loss = 1.5605
I0815 15:56:48.018527  8764 solver.cpp:334]     Train net output #0: loss = 1.499 (* 1 = 1.499 loss)
I0815 15:56:48.018532  8764 sgd_solver.cpp:136] Iteration 105700, lr = 0.00339375, m = 0.9
I0815 15:57:05.602295  8764 solver.cpp:312] Iteration 105800 (5.68721 iter/s, 17.5833s/100 iter), loss = 1.5108
I0815 15:57:05.602519  8764 solver.cpp:334]     Train net output #0: loss = 1.79747 (* 1 = 1.79747 loss)
I0815 15:57:05.602532  8764 sgd_solver.cpp:136] Iteration 105800, lr = 0.0033875, m = 0.9
I0815 15:57:23.931872  8764 solver.cpp:312] Iteration 105900 (5.45581 iter/s, 18.3291s/100 iter), loss = 1.45179
I0815 15:57:23.931900  8764 solver.cpp:334]     Train net output #0: loss = 1.87381 (* 1 = 1.87381 loss)
I0815 15:57:23.931906  8764 sgd_solver.cpp:136] Iteration 105900, lr = 0.00338125, m = 0.9
I0815 15:57:41.494163  8764 solver.cpp:363] Sparsity after update:
I0815 15:57:41.500218  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 15:57:41.500237  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 15:57:41.500246  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 15:57:41.500249  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 15:57:41.500252  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 15:57:41.500257  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 15:57:41.500259  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 15:57:41.500262  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 15:57:41.500265  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 15:57:41.500268  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 15:57:41.500272  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 15:57:41.500274  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 15:57:41.500277  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 15:57:41.500289  8764 solver.cpp:509] Iteration 106000, Testing net (#0)
I0815 15:58:11.369966  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.582058
I0815 15:58:11.369990  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.81282
I0815 15:58:11.369995  8764 solver.cpp:594]     Test net output #2: loss = 1.83708 (* 1 = 1.83708 loss)
I0815 15:58:11.370020  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.8689s
I0815 15:58:11.518431  8764 solver.cpp:312] Iteration 106000 (2.10149 iter/s, 47.5853s/100 iter), loss = 1.41676
I0815 15:58:11.518512  8764 solver.cpp:334]     Train net output #0: loss = 1.43165 (* 1 = 1.43165 loss)
I0815 15:58:11.518518  8764 sgd_solver.cpp:136] Iteration 106000, lr = 0.003375, m = 0.9
I0815 15:58:30.700357  8764 solver.cpp:312] Iteration 106100 (5.21339 iter/s, 19.1814s/100 iter), loss = 1.34735
I0815 15:58:30.700419  8764 solver.cpp:334]     Train net output #0: loss = 1.51439 (* 1 = 1.51439 loss)
I0815 15:58:30.700431  8764 sgd_solver.cpp:136] Iteration 106100, lr = 0.00336875, m = 0.9
I0815 15:58:49.955842  8764 solver.cpp:312] Iteration 106200 (5.19348 iter/s, 19.2549s/100 iter), loss = 1.55182
I0815 15:58:49.956059  8764 solver.cpp:334]     Train net output #0: loss = 1.51968 (* 1 = 1.51968 loss)
I0815 15:58:49.956096  8764 sgd_solver.cpp:136] Iteration 106200, lr = 0.0033625, m = 0.9
I0815 15:59:07.415392  8764 solver.cpp:312] Iteration 106300 (5.72767 iter/s, 17.4591s/100 iter), loss = 1.18725
I0815 15:59:07.415465  8764 solver.cpp:334]     Train net output #0: loss = 0.978001 (* 1 = 0.978001 loss)
I0815 15:59:07.415485  8764 sgd_solver.cpp:136] Iteration 106300, lr = 0.00335625, m = 0.9
I0815 15:59:27.260335  8764 solver.cpp:312] Iteration 106400 (5.03921 iter/s, 19.8444s/100 iter), loss = 1.36138
I0815 15:59:27.260480  8764 solver.cpp:334]     Train net output #0: loss = 1.68374 (* 1 = 1.68374 loss)
I0815 15:59:27.260511  8764 sgd_solver.cpp:136] Iteration 106400, lr = 0.00335, m = 0.9
I0815 15:59:44.420017  8764 solver.cpp:312] Iteration 106500 (5.82778 iter/s, 17.1592s/100 iter), loss = 1.42783
I0815 15:59:44.420275  8764 solver.cpp:334]     Train net output #0: loss = 1.26234 (* 1 = 1.26234 loss)
I0815 15:59:44.420392  8764 sgd_solver.cpp:136] Iteration 106500, lr = 0.00334375, m = 0.9
I0815 16:00:01.718258  8764 solver.cpp:312] Iteration 106600 (5.78109 iter/s, 17.2978s/100 iter), loss = 1.57907
I0815 16:00:01.718341  8764 solver.cpp:334]     Train net output #0: loss = 1.67161 (* 1 = 1.67161 loss)
I0815 16:00:01.718353  8764 sgd_solver.cpp:136] Iteration 106600, lr = 0.0033375, m = 0.9
I0815 16:00:18.916278  8764 solver.cpp:312] Iteration 106700 (5.81478 iter/s, 17.1975s/100 iter), loss = 1.66473
I0815 16:00:18.916303  8764 solver.cpp:334]     Train net output #0: loss = 1.45946 (* 1 = 1.45946 loss)
I0815 16:00:18.916307  8764 sgd_solver.cpp:136] Iteration 106700, lr = 0.00333125, m = 0.9
I0815 16:00:37.306687  8764 solver.cpp:312] Iteration 106800 (5.43777 iter/s, 18.3899s/100 iter), loss = 1.76232
I0815 16:00:37.307668  8764 solver.cpp:334]     Train net output #0: loss = 1.62883 (* 1 = 1.62883 loss)
I0815 16:00:37.307687  8764 sgd_solver.cpp:136] Iteration 106800, lr = 0.003325, m = 0.9
I0815 16:00:55.958892  8764 solver.cpp:312] Iteration 106900 (5.36144 iter/s, 18.6517s/100 iter), loss = 2.12076
I0815 16:00:55.958920  8764 solver.cpp:334]     Train net output #0: loss = 2.03765 (* 1 = 2.03765 loss)
I0815 16:00:55.958925  8764 sgd_solver.cpp:136] Iteration 106900, lr = 0.00331875, m = 0.9
I0815 16:01:13.724467  8764 solver.cpp:363] Sparsity after update:
I0815 16:01:13.735654  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:01:13.735880  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:01:13.735986  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:01:13.736078  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:01:13.736177  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:01:13.736270  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:01:13.736361  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:01:13.736450  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:01:13.736539  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:01:13.736629  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:01:13.736726  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:01:13.736819  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:01:13.736910  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:01:14.012845  8764 solver.cpp:312] Iteration 107000 (5.5391 iter/s, 18.0535s/100 iter), loss = 1.61992
I0815 16:01:14.012874  8764 solver.cpp:334]     Train net output #0: loss = 1.58592 (* 1 = 1.58592 loss)
I0815 16:01:14.012879  8764 sgd_solver.cpp:136] Iteration 107000, lr = 0.0033125, m = 0.9
I0815 16:01:33.771396  8764 solver.cpp:312] Iteration 107100 (5.06124 iter/s, 19.758s/100 iter), loss = 1.75157
I0815 16:01:33.771494  8764 solver.cpp:334]     Train net output #0: loss = 2.00514 (* 1 = 2.00514 loss)
I0815 16:01:33.771524  8764 sgd_solver.cpp:136] Iteration 107100, lr = 0.00330625, m = 0.9
I0815 16:01:52.321934  8764 solver.cpp:312] Iteration 107200 (5.39083 iter/s, 18.55s/100 iter), loss = 1.71748
I0815 16:01:52.322185  8764 solver.cpp:334]     Train net output #0: loss = 1.34009 (* 1 = 1.34009 loss)
I0815 16:01:52.322203  8764 sgd_solver.cpp:136] Iteration 107200, lr = 0.0033, m = 0.9
I0815 16:02:10.882153  8764 solver.cpp:312] Iteration 107300 (5.38802 iter/s, 18.5597s/100 iter), loss = 1.51668
I0815 16:02:10.882181  8764 solver.cpp:334]     Train net output #0: loss = 1.72483 (* 1 = 1.72483 loss)
I0815 16:02:10.882186  8764 sgd_solver.cpp:136] Iteration 107300, lr = 0.00329375, m = 0.9
I0815 16:02:28.280745  8764 solver.cpp:312] Iteration 107400 (5.74775 iter/s, 17.3981s/100 iter), loss = 1.23842
I0815 16:02:28.280809  8764 solver.cpp:334]     Train net output #0: loss = 1.3992 (* 1 = 1.3992 loss)
I0815 16:02:28.280817  8764 sgd_solver.cpp:136] Iteration 107400, lr = 0.0032875, m = 0.9
I0815 16:02:44.756265  8764 solver.cpp:312] Iteration 107500 (6.06978 iter/s, 16.4751s/100 iter), loss = 1.62787
I0815 16:02:44.756338  8764 solver.cpp:334]     Train net output #0: loss = 1.94362 (* 1 = 1.94362 loss)
I0815 16:02:44.756357  8764 sgd_solver.cpp:136] Iteration 107500, lr = 0.00328125, m = 0.9
I0815 16:03:03.645947  8764 solver.cpp:312] Iteration 107600 (5.29404 iter/s, 18.8892s/100 iter), loss = 1.33059
I0815 16:03:03.646023  8764 solver.cpp:334]     Train net output #0: loss = 1.36918 (* 1 = 1.36918 loss)
I0815 16:03:03.646049  8764 sgd_solver.cpp:136] Iteration 107600, lr = 0.003275, m = 0.9
I0815 16:03:23.104223  8764 solver.cpp:312] Iteration 107700 (5.13934 iter/s, 19.4577s/100 iter), loss = 1.22445
I0815 16:03:23.104272  8764 solver.cpp:334]     Train net output #0: loss = 1.28329 (* 1 = 1.28329 loss)
I0815 16:03:23.104281  8764 sgd_solver.cpp:136] Iteration 107700, lr = 0.00326875, m = 0.9
I0815 16:03:40.876061  8764 solver.cpp:312] Iteration 107800 (5.62703 iter/s, 17.7714s/100 iter), loss = 1.36565
I0815 16:03:40.876144  8764 solver.cpp:334]     Train net output #0: loss = 1.31426 (* 1 = 1.31426 loss)
I0815 16:03:40.876155  8764 sgd_solver.cpp:136] Iteration 107800, lr = 0.0032625, m = 0.9
I0815 16:03:57.741446  8764 solver.cpp:312] Iteration 107900 (5.92947 iter/s, 16.8649s/100 iter), loss = 1.48015
I0815 16:03:57.741475  8764 solver.cpp:334]     Train net output #0: loss = 1.34884 (* 1 = 1.34884 loss)
I0815 16:03:57.741480  8764 sgd_solver.cpp:136] Iteration 107900, lr = 0.00325625, m = 0.9
I0815 16:04:15.441298  8764 solver.cpp:363] Sparsity after update:
I0815 16:04:15.446190  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:04:15.446228  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:04:15.446247  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:04:15.446259  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:04:15.446271  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:04:15.446285  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:04:15.446296  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:04:15.446307  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:04:15.446319  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:04:15.446331  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:04:15.446348  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:04:15.446363  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:04:15.446377  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:04:15.446396  8764 solver.cpp:509] Iteration 108000, Testing net (#0)
I0815 16:04:43.529120  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.581588
I0815 16:04:43.529144  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.81335
I0815 16:04:43.529150  8764 solver.cpp:594]     Test net output #2: loss = 1.82613 (* 1 = 1.82613 loss)
I0815 16:04:43.529171  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.082s
I0815 16:04:43.671911  8764 solver.cpp:312] Iteration 108000 (2.17726 iter/s, 45.9292s/100 iter), loss = 1.27125
I0815 16:04:43.671934  8764 solver.cpp:334]     Train net output #0: loss = 1.1074 (* 1 = 1.1074 loss)
I0815 16:04:43.671939  8764 sgd_solver.cpp:136] Iteration 108000, lr = 0.00325, m = 0.9
I0815 16:05:00.810331  8764 solver.cpp:312] Iteration 108100 (5.83501 iter/s, 17.1379s/100 iter), loss = 1.46527
I0815 16:05:00.810395  8764 solver.cpp:334]     Train net output #0: loss = 1.49723 (* 1 = 1.49723 loss)
I0815 16:05:00.810401  8764 sgd_solver.cpp:136] Iteration 108100, lr = 0.00324375, m = 0.9
I0815 16:05:19.436190  8764 solver.cpp:312] Iteration 108200 (5.36903 iter/s, 18.6253s/100 iter), loss = 1.32026
I0815 16:05:19.436215  8764 solver.cpp:334]     Train net output #0: loss = 1.31307 (* 1 = 1.31307 loss)
I0815 16:05:19.436218  8764 sgd_solver.cpp:136] Iteration 108200, lr = 0.0032375, m = 0.9
I0815 16:05:38.938267  8764 solver.cpp:312] Iteration 108300 (5.1278 iter/s, 19.5015s/100 iter), loss = 1.33295
I0815 16:05:38.938346  8764 solver.cpp:334]     Train net output #0: loss = 1.33877 (* 1 = 1.33877 loss)
I0815 16:05:38.938360  8764 sgd_solver.cpp:136] Iteration 108300, lr = 0.00323125, m = 0.9
I0815 16:05:56.558003  8764 solver.cpp:312] Iteration 108400 (5.67561 iter/s, 17.6192s/100 iter), loss = 1.73851
I0815 16:05:56.558256  8764 solver.cpp:334]     Train net output #0: loss = 1.67147 (* 1 = 1.67147 loss)
I0815 16:05:56.558377  8764 sgd_solver.cpp:136] Iteration 108400, lr = 0.003225, m = 0.9
I0815 16:06:15.457666  8764 solver.cpp:312] Iteration 108500 (5.29124 iter/s, 18.8991s/100 iter), loss = 1.38747
I0815 16:06:15.457733  8764 solver.cpp:334]     Train net output #0: loss = 1.56171 (* 1 = 1.56171 loss)
I0815 16:06:15.457741  8764 sgd_solver.cpp:136] Iteration 108500, lr = 0.00321875, m = 0.9
I0815 16:06:34.134361  8764 solver.cpp:312] Iteration 108600 (5.35441 iter/s, 18.6762s/100 iter), loss = 1.37272
I0815 16:06:34.134389  8764 solver.cpp:334]     Train net output #0: loss = 1.56209 (* 1 = 1.56209 loss)
I0815 16:06:34.134395  8764 sgd_solver.cpp:136] Iteration 108600, lr = 0.0032125, m = 0.9
I0815 16:06:37.722013  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:06:57.896026  8764 solver.cpp:312] Iteration 108700 (4.20858 iter/s, 23.761s/100 iter), loss = 1.59915
I0815 16:06:57.896117  8764 solver.cpp:334]     Train net output #0: loss = 1.59753 (* 1 = 1.59753 loss)
I0815 16:06:57.896148  8764 sgd_solver.cpp:136] Iteration 108700, lr = 0.00320625, m = 0.9
I0815 16:07:17.832505  8764 solver.cpp:312] Iteration 108800 (5.01607 iter/s, 19.9359s/100 iter), loss = 1.45992
I0815 16:07:17.832577  8764 solver.cpp:334]     Train net output #0: loss = 1.42627 (* 1 = 1.42627 loss)
I0815 16:07:17.832597  8764 sgd_solver.cpp:136] Iteration 108800, lr = 0.0032, m = 0.9
I0815 16:07:38.434265  8764 solver.cpp:312] Iteration 108900 (4.85409 iter/s, 20.6012s/100 iter), loss = 1.71638
I0815 16:07:38.434355  8764 solver.cpp:334]     Train net output #0: loss = 1.80799 (* 1 = 1.80799 loss)
I0815 16:07:38.434367  8764 sgd_solver.cpp:136] Iteration 108900, lr = 0.00319375, m = 0.9
I0815 16:07:58.615411  8764 solver.cpp:363] Sparsity after update:
I0815 16:07:58.620473  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:07:58.620507  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:07:58.620527  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:07:58.620532  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:07:58.620540  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:07:58.620548  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:07:58.620554  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:07:58.620561  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:07:58.620568  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:07:58.620575  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:07:58.620582  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:07:58.620589  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:07:58.620596  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:07:59.029526  8764 solver.cpp:312] Iteration 109000 (4.85563 iter/s, 20.5947s/100 iter), loss = 1.38637
I0815 16:07:59.029798  8764 solver.cpp:334]     Train net output #0: loss = 1.44425 (* 1 = 1.44425 loss)
I0815 16:07:59.029944  8764 sgd_solver.cpp:136] Iteration 109000, lr = 0.0031875, m = 0.9
I0815 16:08:20.045441  8764 solver.cpp:312] Iteration 109100 (4.75843 iter/s, 21.0153s/100 iter), loss = 1.50472
I0815 16:08:20.070117  8764 solver.cpp:334]     Train net output #0: loss = 1.60986 (* 1 = 1.60986 loss)
I0815 16:08:20.070243  8764 sgd_solver.cpp:136] Iteration 109100, lr = 0.00318125, m = 0.9
I0815 16:08:37.859380  8764 solver.cpp:312] Iteration 109200 (5.61374 iter/s, 17.8134s/100 iter), loss = 1.39216
I0815 16:08:37.859408  8764 solver.cpp:334]     Train net output #0: loss = 1.32406 (* 1 = 1.32406 loss)
I0815 16:08:37.859414  8764 sgd_solver.cpp:136] Iteration 109200, lr = 0.003175, m = 0.9
I0815 16:08:57.700304  8764 solver.cpp:312] Iteration 109300 (5.04023 iter/s, 19.8404s/100 iter), loss = 1.25909
I0815 16:08:57.700388  8764 solver.cpp:334]     Train net output #0: loss = 1.46571 (* 1 = 1.46571 loss)
I0815 16:08:57.700398  8764 sgd_solver.cpp:136] Iteration 109300, lr = 0.00316875, m = 0.9
I0815 16:09:17.407801  8764 solver.cpp:312] Iteration 109400 (5.07435 iter/s, 19.707s/100 iter), loss = 1.11777
I0815 16:09:17.407840  8764 solver.cpp:334]     Train net output #0: loss = 1.06542 (* 1 = 1.06542 loss)
I0815 16:09:17.407850  8764 sgd_solver.cpp:136] Iteration 109400, lr = 0.0031625, m = 0.9
I0815 16:09:36.911404  8764 solver.cpp:312] Iteration 109500 (5.1274 iter/s, 19.5031s/100 iter), loss = 1.28335
I0815 16:09:36.911459  8764 solver.cpp:334]     Train net output #0: loss = 1.11181 (* 1 = 1.11181 loss)
I0815 16:09:36.911465  8764 sgd_solver.cpp:136] Iteration 109500, lr = 0.00315625, m = 0.9
I0815 16:09:54.857429  8764 solver.cpp:312] Iteration 109600 (5.57242 iter/s, 17.9455s/100 iter), loss = 1.04551
I0815 16:09:54.857458  8764 solver.cpp:334]     Train net output #0: loss = 0.91011 (* 1 = 0.91011 loss)
I0815 16:09:54.857465  8764 sgd_solver.cpp:136] Iteration 109600, lr = 0.00315, m = 0.9
I0815 16:10:14.625538  8764 solver.cpp:312] Iteration 109700 (5.05879 iter/s, 19.7676s/100 iter), loss = 1.58956
I0815 16:10:14.625633  8764 solver.cpp:334]     Train net output #0: loss = 1.61332 (* 1 = 1.61332 loss)
I0815 16:10:14.625650  8764 sgd_solver.cpp:136] Iteration 109700, lr = 0.00314375, m = 0.9
I0815 16:10:33.533391  8764 solver.cpp:312] Iteration 109800 (5.28896 iter/s, 18.9073s/100 iter), loss = 1.59597
I0815 16:10:33.533437  8764 solver.cpp:334]     Train net output #0: loss = 1.5603 (* 1 = 1.5603 loss)
I0815 16:10:33.533449  8764 sgd_solver.cpp:136] Iteration 109800, lr = 0.0031375, m = 0.9
I0815 16:10:52.014552  8764 solver.cpp:312] Iteration 109900 (5.41106 iter/s, 18.4807s/100 iter), loss = 1.4372
I0815 16:10:52.024179  8764 solver.cpp:334]     Train net output #0: loss = 1.5296 (* 1 = 1.5296 loss)
I0815 16:10:52.024199  8764 sgd_solver.cpp:136] Iteration 109900, lr = 0.00313125, m = 0.9
I0815 16:11:09.628958  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_110000.caffemodel
I0815 16:11:09.682989  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_110000.solverstate
I0815 16:11:09.689448  8764 solver.cpp:363] Sparsity after update:
I0815 16:11:09.694461  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:11:09.694492  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:11:09.694507  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:11:09.694515  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:11:09.694524  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:11:09.694531  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:11:09.694540  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:11:09.694548  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:11:09.694557  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:11:09.694566  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:11:09.694574  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:11:09.694582  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:11:09.694591  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:11:09.694607  8764 solver.cpp:509] Iteration 110000, Testing net (#0)
I0815 16:11:42.601620  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.584765
I0815 16:11:42.601774  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814879
I0815 16:11:42.601796  8764 solver.cpp:594]     Test net output #2: loss = 1.82971 (* 1 = 1.82971 loss)
I0815 16:11:42.601837  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.9063s
I0815 16:11:42.881901  8764 solver.cpp:312] Iteration 110000 (1.96595 iter/s, 50.8659s/100 iter), loss = 1.49945
I0815 16:11:42.881947  8764 solver.cpp:334]     Train net output #0: loss = 1.69433 (* 1 = 1.69433 loss)
I0815 16:11:42.881956  8764 sgd_solver.cpp:136] Iteration 110000, lr = 0.003125, m = 0.9
I0815 16:12:01.620116  8764 solver.cpp:312] Iteration 110100 (5.33683 iter/s, 18.7377s/100 iter), loss = 1.25771
I0815 16:12:01.620208  8764 solver.cpp:334]     Train net output #0: loss = 0.776489 (* 1 = 0.776489 loss)
I0815 16:12:01.620229  8764 sgd_solver.cpp:136] Iteration 110100, lr = 0.00311875, m = 0.9
I0815 16:12:21.626164  8764 solver.cpp:312] Iteration 110200 (4.99863 iter/s, 20.0055s/100 iter), loss = 1.68591
I0815 16:12:21.626242  8764 solver.cpp:334]     Train net output #0: loss = 1.5258 (* 1 = 1.5258 loss)
I0815 16:12:21.626257  8764 sgd_solver.cpp:136] Iteration 110200, lr = 0.0031125, m = 0.9
I0815 16:12:42.091493  8764 solver.cpp:312] Iteration 110300 (4.88645 iter/s, 20.4648s/100 iter), loss = 1.46002
I0815 16:12:42.091523  8764 solver.cpp:334]     Train net output #0: loss = 1.30471 (* 1 = 1.30471 loss)
I0815 16:12:42.091528  8764 sgd_solver.cpp:136] Iteration 110300, lr = 0.00310625, m = 0.9
I0815 16:13:00.519323  8764 solver.cpp:312] Iteration 110400 (5.42673 iter/s, 18.4273s/100 iter), loss = 1.56342
I0815 16:13:00.520190  8764 solver.cpp:334]     Train net output #0: loss = 1.68352 (* 1 = 1.68352 loss)
I0815 16:13:00.520220  8764 sgd_solver.cpp:136] Iteration 110400, lr = 0.0031, m = 0.9
I0815 16:13:17.919270  8764 solver.cpp:312] Iteration 110500 (5.7473 iter/s, 17.3995s/100 iter), loss = 0.93888
I0815 16:13:17.919293  8764 solver.cpp:334]     Train net output #0: loss = 0.716048 (* 1 = 0.716048 loss)
I0815 16:13:17.919299  8764 sgd_solver.cpp:136] Iteration 110500, lr = 0.00309375, m = 0.9
I0815 16:13:35.675952  8764 solver.cpp:312] Iteration 110600 (5.63184 iter/s, 17.7562s/100 iter), loss = 1.59368
I0815 16:13:35.676033  8764 solver.cpp:334]     Train net output #0: loss = 1.32365 (* 1 = 1.32365 loss)
I0815 16:13:35.676044  8764 sgd_solver.cpp:136] Iteration 110600, lr = 0.0030875, m = 0.9
I0815 16:13:53.464164  8764 solver.cpp:312] Iteration 110700 (5.62185 iter/s, 17.7877s/100 iter), loss = 1.58618
I0815 16:13:53.464233  8764 solver.cpp:334]     Train net output #0: loss = 1.55262 (* 1 = 1.55262 loss)
I0815 16:13:53.464253  8764 sgd_solver.cpp:136] Iteration 110700, lr = 0.00308125, m = 0.9
I0815 16:14:11.639042  8764 solver.cpp:312] Iteration 110800 (5.50225 iter/s, 18.1744s/100 iter), loss = 1.42627
I0815 16:14:11.639135  8764 solver.cpp:334]     Train net output #0: loss = 1.6947 (* 1 = 1.6947 loss)
I0815 16:14:11.639143  8764 sgd_solver.cpp:136] Iteration 110800, lr = 0.003075, m = 0.9
I0815 16:14:30.043834  8764 solver.cpp:312] Iteration 110900 (5.43352 iter/s, 18.4043s/100 iter), loss = 1.72753
I0815 16:14:30.043864  8764 solver.cpp:334]     Train net output #0: loss = 1.65599 (* 1 = 1.65599 loss)
I0815 16:14:30.043870  8764 sgd_solver.cpp:136] Iteration 110900, lr = 0.00306875, m = 0.9
I0815 16:14:46.575855  8764 solver.cpp:363] Sparsity after update:
I0815 16:14:46.585286  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:14:46.597460  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:14:46.597525  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:14:46.597558  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:14:46.597600  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:14:46.597628  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:14:46.597667  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:14:46.597703  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:14:46.597739  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:14:46.597764  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:14:46.597790  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:14:46.597821  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:14:46.597857  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:14:46.789885  8764 solver.cpp:312] Iteration 111000 (5.97172 iter/s, 16.7456s/100 iter), loss = 1.21831
I0815 16:14:46.789912  8764 solver.cpp:334]     Train net output #0: loss = 1.36962 (* 1 = 1.36962 loss)
I0815 16:14:46.789918  8764 sgd_solver.cpp:136] Iteration 111000, lr = 0.0030625, m = 0.9
I0815 16:15:06.472501  8764 solver.cpp:312] Iteration 111100 (5.08076 iter/s, 19.6821s/100 iter), loss = 1.37602
I0815 16:15:06.472650  8764 solver.cpp:334]     Train net output #0: loss = 1.56952 (* 1 = 1.56952 loss)
I0815 16:15:06.472666  8764 sgd_solver.cpp:136] Iteration 111100, lr = 0.00305625, m = 0.9
I0815 16:15:24.894866  8764 solver.cpp:312] Iteration 111200 (5.42834 iter/s, 18.4219s/100 iter), loss = 1.54344
I0815 16:15:24.894950  8764 solver.cpp:334]     Train net output #0: loss = 1.49478 (* 1 = 1.49478 loss)
I0815 16:15:24.894961  8764 sgd_solver.cpp:136] Iteration 111200, lr = 0.00305, m = 0.9
I0815 16:15:43.654660  8764 solver.cpp:312] Iteration 111300 (5.33069 iter/s, 18.7593s/100 iter), loss = 1.81423
I0815 16:15:43.654686  8764 solver.cpp:334]     Train net output #0: loss = 2.02344 (* 1 = 2.02344 loss)
I0815 16:15:43.654691  8764 sgd_solver.cpp:136] Iteration 111300, lr = 0.00304375, m = 0.9
I0815 16:16:00.184144  8764 solver.cpp:312] Iteration 111400 (6.04997 iter/s, 16.529s/100 iter), loss = 1.38263
I0815 16:16:00.184288  8764 solver.cpp:334]     Train net output #0: loss = 1.51345 (* 1 = 1.51345 loss)
I0815 16:16:00.184303  8764 sgd_solver.cpp:136] Iteration 111400, lr = 0.0030375, m = 0.9
I0815 16:16:19.025842  8764 solver.cpp:312] Iteration 111500 (5.30752 iter/s, 18.8412s/100 iter), loss = 1.55067
I0815 16:16:19.025871  8764 solver.cpp:334]     Train net output #0: loss = 1.3168 (* 1 = 1.3168 loss)
I0815 16:16:19.025878  8764 sgd_solver.cpp:136] Iteration 111500, lr = 0.00303125, m = 0.9
I0815 16:16:37.176420  8764 solver.cpp:312] Iteration 111600 (5.50962 iter/s, 18.1501s/100 iter), loss = 1.06307
I0815 16:16:37.176555  8764 solver.cpp:334]     Train net output #0: loss = 1.20734 (* 1 = 1.20734 loss)
I0815 16:16:37.176584  8764 sgd_solver.cpp:136] Iteration 111600, lr = 0.003025, m = 0.9
I0815 16:16:54.951289  8764 solver.cpp:312] Iteration 111700 (5.62607 iter/s, 17.7744s/100 iter), loss = 1.3033
I0815 16:16:54.951318  8764 solver.cpp:334]     Train net output #0: loss = 1.43555 (* 1 = 1.43555 loss)
I0815 16:16:54.951323  8764 sgd_solver.cpp:136] Iteration 111700, lr = 0.00301875, m = 0.9
I0815 16:17:13.699733  8764 solver.cpp:312] Iteration 111800 (5.33392 iter/s, 18.7479s/100 iter), loss = 1.16842
I0815 16:17:13.699992  8764 solver.cpp:334]     Train net output #0: loss = 1.06594 (* 1 = 1.06594 loss)
I0815 16:17:13.700103  8764 sgd_solver.cpp:136] Iteration 111800, lr = 0.0030125, m = 0.9
I0815 16:17:31.835150  8764 solver.cpp:312] Iteration 111900 (5.51422 iter/s, 18.1349s/100 iter), loss = 1.35228
I0815 16:17:31.835211  8764 solver.cpp:334]     Train net output #0: loss = 0.930728 (* 1 = 0.930728 loss)
I0815 16:17:31.835230  8764 sgd_solver.cpp:136] Iteration 111900, lr = 0.00300625, m = 0.9
I0815 16:17:50.053299  8764 solver.cpp:363] Sparsity after update:
I0815 16:17:50.070384  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:17:50.070446  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:17:50.070485  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:17:50.070504  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:17:50.070523  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:17:50.070533  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:17:50.070549  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:17:50.070564  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:17:50.070581  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:17:50.070595  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:17:50.070611  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:17:50.070624  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:17:50.070641  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:17:50.070663  8764 solver.cpp:509] Iteration 112000, Testing net (#0)
I0815 16:18:07.931627  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:18:18.175268  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.579058
I0815 16:18:18.175293  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.811644
I0815 16:18:18.175300  8764 solver.cpp:594]     Test net output #2: loss = 1.83388 (* 1 = 1.83388 loss)
I0815 16:18:18.175334  8764 solver.cpp:264] [MultiGPU] Tests completed in 28.1039s
I0815 16:18:18.330319  8764 solver.cpp:312] Iteration 112000 (2.15082 iter/s, 46.4939s/100 iter), loss = 1.47464
I0815 16:18:18.330341  8764 solver.cpp:334]     Train net output #0: loss = 1.56839 (* 1 = 1.56839 loss)
I0815 16:18:18.330344  8764 sgd_solver.cpp:136] Iteration 112000, lr = 0.003, m = 0.9
I0815 16:18:37.313483  8764 solver.cpp:312] Iteration 112100 (5.26798 iter/s, 18.9826s/100 iter), loss = 1.60382
I0815 16:18:37.313592  8764 solver.cpp:334]     Train net output #0: loss = 1.54944 (* 1 = 1.54944 loss)
I0815 16:18:37.313609  8764 sgd_solver.cpp:136] Iteration 112100, lr = 0.00299375, m = 0.9
I0815 16:18:54.105809  8764 solver.cpp:312] Iteration 112200 (5.95526 iter/s, 16.7919s/100 iter), loss = 1.28287
I0815 16:18:54.105836  8764 solver.cpp:334]     Train net output #0: loss = 1.26569 (* 1 = 1.26569 loss)
I0815 16:18:54.105842  8764 sgd_solver.cpp:136] Iteration 112200, lr = 0.0029875, m = 0.9
I0815 16:19:10.213157  8764 solver.cpp:312] Iteration 112300 (6.20852 iter/s, 16.1069s/100 iter), loss = 1.54892
I0815 16:19:10.213210  8764 solver.cpp:334]     Train net output #0: loss = 1.4449 (* 1 = 1.4449 loss)
I0815 16:19:10.213217  8764 sgd_solver.cpp:136] Iteration 112300, lr = 0.00298125, m = 0.9
I0815 16:19:27.344463  8764 solver.cpp:312] Iteration 112400 (5.83743 iter/s, 17.1308s/100 iter), loss = 1.36692
I0815 16:19:27.344489  8764 solver.cpp:334]     Train net output #0: loss = 1.2299 (* 1 = 1.2299 loss)
I0815 16:19:27.344494  8764 sgd_solver.cpp:136] Iteration 112400, lr = 0.002975, m = 0.9
I0815 16:19:45.671284  8764 solver.cpp:312] Iteration 112500 (5.45664 iter/s, 18.3263s/100 iter), loss = 1.68395
I0815 16:19:45.671452  8764 solver.cpp:334]     Train net output #0: loss = 1.3391 (* 1 = 1.3391 loss)
I0815 16:19:45.671581  8764 sgd_solver.cpp:136] Iteration 112500, lr = 0.00296875, m = 0.9
I0815 16:20:05.182237  8764 solver.cpp:312] Iteration 112600 (5.12546 iter/s, 19.5104s/100 iter), loss = 1.43015
I0815 16:20:05.182262  8764 solver.cpp:334]     Train net output #0: loss = 1.47554 (* 1 = 1.47554 loss)
I0815 16:20:05.182267  8764 sgd_solver.cpp:136] Iteration 112600, lr = 0.0029625, m = 0.9
I0815 16:20:24.093184  8764 solver.cpp:312] Iteration 112700 (5.28809 iter/s, 18.9104s/100 iter), loss = 1.37813
I0815 16:20:24.093257  8764 solver.cpp:334]     Train net output #0: loss = 1.45073 (* 1 = 1.45073 loss)
I0815 16:20:24.093263  8764 sgd_solver.cpp:136] Iteration 112700, lr = 0.00295625, m = 0.9
I0815 16:20:42.641340  8764 solver.cpp:312] Iteration 112800 (5.39152 iter/s, 18.5476s/100 iter), loss = 1.33457
I0815 16:20:42.641367  8764 solver.cpp:334]     Train net output #0: loss = 1.5436 (* 1 = 1.5436 loss)
I0815 16:20:42.641372  8764 sgd_solver.cpp:136] Iteration 112800, lr = 0.00295, m = 0.9
I0815 16:21:00.348939  8764 solver.cpp:312] Iteration 112900 (5.64745 iter/s, 17.7071s/100 iter), loss = 1.57976
I0815 16:21:00.349001  8764 solver.cpp:334]     Train net output #0: loss = 1.40332 (* 1 = 1.40332 loss)
I0815 16:21:00.349009  8764 sgd_solver.cpp:136] Iteration 112900, lr = 0.00294375, m = 0.9
I0815 16:21:19.280153  8764 solver.cpp:363] Sparsity after update:
I0815 16:21:19.301458  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:21:19.301548  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:21:19.301607  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:21:19.301641  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:21:19.301668  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:21:19.301692  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:21:19.301724  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:21:19.301751  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:21:19.301774  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:21:19.301796  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:21:19.301818  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:21:19.301847  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:21:19.301877  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:21:19.510453  8764 solver.cpp:312] Iteration 113000 (5.21894 iter/s, 19.161s/100 iter), loss = 1.69799
I0815 16:21:19.510510  8764 solver.cpp:334]     Train net output #0: loss = 1.5059 (* 1 = 1.5059 loss)
I0815 16:21:19.510524  8764 sgd_solver.cpp:136] Iteration 113000, lr = 0.0029375, m = 0.9
I0815 16:21:37.752043  8764 solver.cpp:312] Iteration 113100 (5.48213 iter/s, 18.2411s/100 iter), loss = 1.50452
I0815 16:21:37.752127  8764 solver.cpp:334]     Train net output #0: loss = 1.49685 (* 1 = 1.49685 loss)
I0815 16:21:37.752152  8764 sgd_solver.cpp:136] Iteration 113100, lr = 0.00293125, m = 0.9
I0815 16:21:54.733639  8764 solver.cpp:312] Iteration 113200 (5.88889 iter/s, 16.9811s/100 iter), loss = 1.19931
I0815 16:21:54.733666  8764 solver.cpp:334]     Train net output #0: loss = 0.962372 (* 1 = 0.962372 loss)
I0815 16:21:54.733672  8764 sgd_solver.cpp:136] Iteration 113200, lr = 0.002925, m = 0.9
I0815 16:22:13.984164  8764 solver.cpp:312] Iteration 113300 (5.19481 iter/s, 19.25s/100 iter), loss = 1.54982
I0815 16:22:13.984302  8764 solver.cpp:334]     Train net output #0: loss = 1.21857 (* 1 = 1.21857 loss)
I0815 16:22:13.984323  8764 sgd_solver.cpp:136] Iteration 113300, lr = 0.00291875, m = 0.9
I0815 16:22:32.190157  8764 solver.cpp:312] Iteration 113400 (5.49285 iter/s, 18.2055s/100 iter), loss = 1.34007
I0815 16:22:32.190224  8764 solver.cpp:334]     Train net output #0: loss = 1.36837 (* 1 = 1.36837 loss)
I0815 16:22:32.190240  8764 sgd_solver.cpp:136] Iteration 113400, lr = 0.0029125, m = 0.9
I0815 16:22:49.439285  8764 solver.cpp:312] Iteration 113500 (5.79755 iter/s, 17.2487s/100 iter), loss = 1.34059
I0815 16:22:49.439380  8764 solver.cpp:334]     Train net output #0: loss = 1.4265 (* 1 = 1.4265 loss)
I0815 16:22:49.439388  8764 sgd_solver.cpp:136] Iteration 113500, lr = 0.00290625, m = 0.9
I0815 16:23:09.498170  8764 solver.cpp:312] Iteration 113600 (4.98546 iter/s, 20.0583s/100 iter), loss = 1.25502
I0815 16:23:09.498232  8764 solver.cpp:334]     Train net output #0: loss = 1.35989 (* 1 = 1.35989 loss)
I0815 16:23:09.498277  8764 sgd_solver.cpp:136] Iteration 113600, lr = 0.0029, m = 0.9
I0815 16:23:31.172164  8764 solver.cpp:312] Iteration 113700 (4.61395 iter/s, 21.6734s/100 iter), loss = 1.84899
I0815 16:23:31.172245  8764 solver.cpp:334]     Train net output #0: loss = 1.75144 (* 1 = 1.75144 loss)
I0815 16:23:31.172256  8764 sgd_solver.cpp:136] Iteration 113700, lr = 0.00289375, m = 0.9
I0815 16:23:49.799049  8764 solver.cpp:312] Iteration 113800 (5.36873 iter/s, 18.6264s/100 iter), loss = 1.47792
I0815 16:23:49.799129  8764 solver.cpp:334]     Train net output #0: loss = 1.73262 (* 1 = 1.73262 loss)
I0815 16:23:49.799147  8764 sgd_solver.cpp:136] Iteration 113800, lr = 0.0028875, m = 0.9
I0815 16:24:10.355128  8764 solver.cpp:312] Iteration 113900 (4.86488 iter/s, 20.5555s/100 iter), loss = 1.52759
I0815 16:24:10.355244  8764 solver.cpp:334]     Train net output #0: loss = 1.53869 (* 1 = 1.53869 loss)
I0815 16:24:10.355258  8764 sgd_solver.cpp:136] Iteration 113900, lr = 0.00288125, m = 0.9
I0815 16:24:31.831159  8764 solver.cpp:363] Sparsity after update:
I0815 16:24:31.835824  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:24:31.835834  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:24:31.835840  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:24:31.835844  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:24:31.835845  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:24:31.835847  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:24:31.835850  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:24:31.835851  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:24:31.835855  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:24:31.835857  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:24:31.835860  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:24:31.835862  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:24:31.835866  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:24:31.835877  8764 solver.cpp:509] Iteration 114000, Testing net (#0)
I0815 16:25:09.040432  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.581412
I0815 16:25:09.040478  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812939
I0815 16:25:09.040485  8764 solver.cpp:594]     Test net output #2: loss = 1.82856 (* 1 = 1.82856 loss)
I0815 16:25:09.040504  8764 solver.cpp:264] [MultiGPU] Tests completed in 37.2036s
I0815 16:25:09.192549  8764 solver.cpp:312] Iteration 114000 (1.69965 iter/s, 58.8358s/100 iter), loss = 0.926058
I0815 16:25:09.192659  8764 solver.cpp:334]     Train net output #0: loss = 0.953272 (* 1 = 0.953272 loss)
I0815 16:25:09.192680  8764 sgd_solver.cpp:136] Iteration 114000, lr = 0.002875, m = 0.9
I0815 16:25:28.844578  8764 solver.cpp:312] Iteration 114100 (5.08867 iter/s, 19.6515s/100 iter), loss = 1.36714
I0815 16:25:28.844646  8764 solver.cpp:334]     Train net output #0: loss = 1.46082 (* 1 = 1.46082 loss)
I0815 16:25:28.844663  8764 sgd_solver.cpp:136] Iteration 114100, lr = 0.00286875, m = 0.9
I0815 16:25:50.146183  8764 solver.cpp:312] Iteration 114200 (4.69461 iter/s, 21.301s/100 iter), loss = 1.15622
I0815 16:25:50.146306  8764 solver.cpp:334]     Train net output #0: loss = 1.02446 (* 1 = 1.02446 loss)
I0815 16:25:50.146327  8764 sgd_solver.cpp:136] Iteration 114200, lr = 0.0028625, m = 0.9
I0815 16:25:52.067590  8726 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 16:26:11.413108  8764 solver.cpp:312] Iteration 114300 (4.70227 iter/s, 21.2663s/100 iter), loss = 1.59443
I0815 16:26:11.413137  8764 solver.cpp:334]     Train net output #0: loss = 2.10321 (* 1 = 2.10321 loss)
I0815 16:26:11.413143  8764 sgd_solver.cpp:136] Iteration 114300, lr = 0.00285625, m = 0.9
I0815 16:26:33.141044  8764 solver.cpp:312] Iteration 114400 (4.6025 iter/s, 21.7273s/100 iter), loss = 1.73548
I0815 16:26:33.141300  8764 solver.cpp:334]     Train net output #0: loss = 2.00047 (* 1 = 2.00047 loss)
I0815 16:26:33.141316  8764 sgd_solver.cpp:136] Iteration 114400, lr = 0.00285, m = 0.9
I0815 16:26:55.514225  8764 solver.cpp:312] Iteration 114500 (4.46976 iter/s, 22.3725s/100 iter), loss = 2.00495
I0815 16:26:55.514328  8764 solver.cpp:334]     Train net output #0: loss = 2.16445 (* 1 = 2.16445 loss)
I0815 16:26:55.514564  8764 sgd_solver.cpp:136] Iteration 114500, lr = 0.00284375, m = 0.9
I0815 16:27:16.401558  8764 solver.cpp:312] Iteration 114600 (4.78772 iter/s, 20.8868s/100 iter), loss = 1.5646
I0815 16:27:16.401665  8764 solver.cpp:334]     Train net output #0: loss = 1.43328 (* 1 = 1.43328 loss)
I0815 16:27:16.401679  8764 sgd_solver.cpp:136] Iteration 114600, lr = 0.0028375, m = 0.9
I0815 16:27:37.003939  8764 solver.cpp:312] Iteration 114700 (4.85394 iter/s, 20.6018s/100 iter), loss = 1.26367
I0815 16:27:37.003963  8764 solver.cpp:334]     Train net output #0: loss = 1.26792 (* 1 = 1.26792 loss)
I0815 16:27:37.003967  8764 sgd_solver.cpp:136] Iteration 114700, lr = 0.00283125, m = 0.9
I0815 16:27:56.045708  8764 solver.cpp:312] Iteration 114800 (5.25176 iter/s, 19.0412s/100 iter), loss = 1.38579
I0815 16:27:56.045842  8764 solver.cpp:334]     Train net output #0: loss = 1.53976 (* 1 = 1.53976 loss)
I0815 16:27:56.045868  8764 sgd_solver.cpp:136] Iteration 114800, lr = 0.002825, m = 0.9
I0815 16:28:14.766316  8764 solver.cpp:312] Iteration 114900 (5.34186 iter/s, 18.7201s/100 iter), loss = 1.24201
I0815 16:28:14.766386  8764 solver.cpp:334]     Train net output #0: loss = 1.28948 (* 1 = 1.28948 loss)
I0815 16:28:14.766403  8764 sgd_solver.cpp:136] Iteration 114900, lr = 0.00281875, m = 0.9
I0815 16:28:35.484385  8764 solver.cpp:363] Sparsity after update:
I0815 16:28:35.497906  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:28:35.497923  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:28:35.497930  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:28:35.497932  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:28:35.497936  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:28:35.497938  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:28:35.497942  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:28:35.497946  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:28:35.497949  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:28:35.497952  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:28:35.497956  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:28:35.497958  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:28:35.497961  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:28:35.645674  8764 solver.cpp:312] Iteration 115000 (4.78955 iter/s, 20.8788s/100 iter), loss = 1.56565
I0815 16:28:35.645701  8764 solver.cpp:334]     Train net output #0: loss = 1.22918 (* 1 = 1.22918 loss)
I0815 16:28:35.645707  8764 sgd_solver.cpp:136] Iteration 115000, lr = 0.0028125, m = 0.9
I0815 16:28:53.534525  8764 solver.cpp:312] Iteration 115100 (5.59023 iter/s, 17.8884s/100 iter), loss = 1.50343
I0815 16:28:53.534579  8764 solver.cpp:334]     Train net output #0: loss = 1.12271 (* 1 = 1.12271 loss)
I0815 16:28:53.534590  8764 sgd_solver.cpp:136] Iteration 115100, lr = 0.00280625, m = 0.9
I0815 16:29:16.736421  8764 solver.cpp:312] Iteration 115200 (4.31011 iter/s, 23.2013s/100 iter), loss = 1.33077
I0815 16:29:16.736513  8764 solver.cpp:334]     Train net output #0: loss = 1.33849 (* 1 = 1.33849 loss)
I0815 16:29:16.736521  8764 sgd_solver.cpp:136] Iteration 115200, lr = 0.0028, m = 0.9
I0815 16:29:35.983247  8764 solver.cpp:312] Iteration 115300 (5.1958 iter/s, 19.2463s/100 iter), loss = 1.70338
I0815 16:29:35.983275  8764 solver.cpp:334]     Train net output #0: loss = 2.06592 (* 1 = 2.06592 loss)
I0815 16:29:35.983302  8764 sgd_solver.cpp:136] Iteration 115300, lr = 0.00279375, m = 0.9
I0815 16:29:55.159436  8764 solver.cpp:312] Iteration 115400 (5.21495 iter/s, 19.1757s/100 iter), loss = 0.873009
I0815 16:29:55.159555  8764 solver.cpp:334]     Train net output #0: loss = 0.958324 (* 1 = 0.958324 loss)
I0815 16:29:55.159571  8764 sgd_solver.cpp:136] Iteration 115400, lr = 0.0027875, m = 0.9
I0815 16:30:15.019711  8764 solver.cpp:312] Iteration 115500 (5.03532 iter/s, 19.8597s/100 iter), loss = 1.55157
I0815 16:30:15.019757  8764 solver.cpp:334]     Train net output #0: loss = 1.56662 (* 1 = 1.56662 loss)
I0815 16:30:15.019763  8764 sgd_solver.cpp:136] Iteration 115500, lr = 0.00278125, m = 0.9
I0815 16:30:32.911799  8764 solver.cpp:312] Iteration 115600 (5.58922 iter/s, 17.8916s/100 iter), loss = 1.5506
I0815 16:30:32.911864  8764 solver.cpp:334]     Train net output #0: loss = 1.76794 (* 1 = 1.76794 loss)
I0815 16:30:32.911871  8764 sgd_solver.cpp:136] Iteration 115600, lr = 0.002775, m = 0.9
I0815 16:30:51.004429  8764 solver.cpp:312] Iteration 115700 (5.52727 iter/s, 18.0921s/100 iter), loss = 1.14737
I0815 16:30:51.004462  8764 solver.cpp:334]     Train net output #0: loss = 1.08021 (* 1 = 1.08021 loss)
I0815 16:30:51.008157  8764 sgd_solver.cpp:136] Iteration 115700, lr = 0.00276875, m = 0.9
I0815 16:31:08.021540  8764 solver.cpp:312] Iteration 115800 (5.8766 iter/s, 17.0166s/100 iter), loss = 1.75267
I0815 16:31:08.021667  8764 solver.cpp:334]     Train net output #0: loss = 1.79353 (* 1 = 1.79353 loss)
I0815 16:31:08.021675  8764 sgd_solver.cpp:136] Iteration 115800, lr = 0.0027625, m = 0.9
I0815 16:31:28.980455  8764 solver.cpp:312] Iteration 115900 (4.77137 iter/s, 20.9583s/100 iter), loss = 1.47975
I0815 16:31:28.980515  8764 solver.cpp:334]     Train net output #0: loss = 1.50996 (* 1 = 1.50996 loss)
I0815 16:31:28.980530  8764 sgd_solver.cpp:136] Iteration 115900, lr = 0.00275625, m = 0.9
I0815 16:31:39.472842  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:31:47.401012  8764 solver.cpp:363] Sparsity after update:
I0815 16:31:47.402860  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:31:47.402878  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:31:47.402890  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:31:47.402896  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:31:47.402902  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:31:47.402909  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:31:47.402914  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:31:47.402920  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:31:47.402926  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:31:47.402932  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:31:47.402938  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:31:47.402945  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:31:47.402951  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:31:47.402967  8764 solver.cpp:509] Iteration 116000, Testing net (#0)
I0815 16:32:16.898381  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.59047
I0815 16:32:16.898572  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.816174
I0815 16:32:16.898670  8764 solver.cpp:594]     Test net output #2: loss = 1.78854 (* 1 = 1.78854 loss)
I0815 16:32:16.898763  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.495s
I0815 16:32:17.156985  8764 solver.cpp:312] Iteration 116000 (2.07576 iter/s, 48.1752s/100 iter), loss = 1.12442
I0815 16:32:17.157013  8764 solver.cpp:334]     Train net output #0: loss = 1.18765 (* 1 = 1.18765 loss)
I0815 16:32:17.157019  8764 sgd_solver.cpp:136] Iteration 116000, lr = 0.00275, m = 0.9
I0815 16:32:34.448874  8764 solver.cpp:312] Iteration 116100 (5.78322 iter/s, 17.2914s/100 iter), loss = 1.56479
I0815 16:32:34.448904  8764 solver.cpp:334]     Train net output #0: loss = 1.32537 (* 1 = 1.32537 loss)
I0815 16:32:34.448909  8764 sgd_solver.cpp:136] Iteration 116100, lr = 0.00274375, m = 0.9
I0815 16:32:52.128892  8764 solver.cpp:312] Iteration 116200 (5.65626 iter/s, 17.6795s/100 iter), loss = 1.33268
I0815 16:32:52.128965  8764 solver.cpp:334]     Train net output #0: loss = 1.20308 (* 1 = 1.20308 loss)
I0815 16:32:52.128973  8764 sgd_solver.cpp:136] Iteration 116200, lr = 0.0027375, m = 0.9
I0815 16:33:10.899914  8764 solver.cpp:312] Iteration 116300 (5.32751 iter/s, 18.7705s/100 iter), loss = 1.8364
I0815 16:33:10.899976  8764 solver.cpp:334]     Train net output #0: loss = 1.93223 (* 1 = 1.93223 loss)
I0815 16:33:10.899993  8764 sgd_solver.cpp:136] Iteration 116300, lr = 0.00273125, m = 0.9
I0815 16:33:27.084623  8764 solver.cpp:312] Iteration 116400 (6.17884 iter/s, 16.1843s/100 iter), loss = 1.3644
I0815 16:33:27.084715  8764 solver.cpp:334]     Train net output #0: loss = 1.0715 (* 1 = 1.0715 loss)
I0815 16:33:27.084733  8764 sgd_solver.cpp:136] Iteration 116400, lr = 0.002725, m = 0.9
I0815 16:33:45.003770  8764 solver.cpp:312] Iteration 116500 (5.58078 iter/s, 17.9186s/100 iter), loss = 1.32778
I0815 16:33:45.003805  8764 solver.cpp:334]     Train net output #0: loss = 1.30482 (* 1 = 1.30482 loss)
I0815 16:33:45.003810  8764 sgd_solver.cpp:136] Iteration 116500, lr = 0.00271875, m = 0.9
I0815 16:34:03.292412  8764 solver.cpp:312] Iteration 116600 (5.46803 iter/s, 18.2881s/100 iter), loss = 1.30271
I0815 16:34:03.292629  8764 solver.cpp:334]     Train net output #0: loss = 1.27325 (* 1 = 1.27325 loss)
I0815 16:34:03.292660  8764 sgd_solver.cpp:136] Iteration 116600, lr = 0.0027125, m = 0.9
I0815 16:34:22.776865  8764 solver.cpp:312] Iteration 116700 (5.13244 iter/s, 19.4839s/100 iter), loss = 0.975956
I0815 16:34:22.776918  8764 solver.cpp:334]     Train net output #0: loss = 1.02228 (* 1 = 1.02228 loss)
I0815 16:34:22.776931  8764 sgd_solver.cpp:136] Iteration 116700, lr = 0.00270625, m = 0.9
I0815 16:34:39.706707  8764 solver.cpp:312] Iteration 116800 (5.90689 iter/s, 16.9294s/100 iter), loss = 1.53282
I0815 16:34:39.706804  8764 solver.cpp:334]     Train net output #0: loss = 1.44322 (* 1 = 1.44322 loss)
I0815 16:34:39.706827  8764 sgd_solver.cpp:136] Iteration 116800, lr = 0.0027, m = 0.9
I0815 16:34:57.693024  8764 solver.cpp:312] Iteration 116900 (5.55993 iter/s, 17.9858s/100 iter), loss = 1.38285
I0815 16:34:57.693092  8764 solver.cpp:334]     Train net output #0: loss = 1.36682 (* 1 = 1.36682 loss)
I0815 16:34:57.693111  8764 sgd_solver.cpp:136] Iteration 116900, lr = 0.00269375, m = 0.9
I0815 16:35:14.868151  8764 solver.cpp:363] Sparsity after update:
I0815 16:35:14.875260  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:35:14.875301  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:35:14.875327  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:35:14.875349  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:35:14.875366  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:35:14.875382  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:35:14.875397  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:35:14.875468  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:35:14.875484  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:35:14.875499  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:35:14.875516  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:35:14.875532  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:35:14.875547  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:35:15.125295  8764 solver.cpp:312] Iteration 117000 (5.73665 iter/s, 17.4318s/100 iter), loss = 1.49418
I0815 16:35:15.125362  8764 solver.cpp:334]     Train net output #0: loss = 1.41482 (* 1 = 1.41482 loss)
I0815 16:35:15.125382  8764 sgd_solver.cpp:136] Iteration 117000, lr = 0.0026875, m = 0.9
I0815 16:35:34.640763  8764 solver.cpp:312] Iteration 117100 (5.12428 iter/s, 19.5149s/100 iter), loss = 0.970678
I0815 16:35:34.640812  8764 solver.cpp:334]     Train net output #0: loss = 1.08113 (* 1 = 1.08113 loss)
I0815 16:35:34.640825  8764 sgd_solver.cpp:136] Iteration 117100, lr = 0.00268125, m = 0.9
I0815 16:35:51.578513  8764 solver.cpp:312] Iteration 117200 (5.90413 iter/s, 16.9373s/100 iter), loss = 1.57464
I0815 16:35:51.578601  8764 solver.cpp:334]     Train net output #0: loss = 1.55022 (* 1 = 1.55022 loss)
I0815 16:35:51.578610  8764 sgd_solver.cpp:136] Iteration 117200, lr = 0.002675, m = 0.9
I0815 16:36:07.847534  8764 solver.cpp:312] Iteration 117300 (6.14682 iter/s, 16.2686s/100 iter), loss = 1.44004
I0815 16:36:07.847611  8764 solver.cpp:334]     Train net output #0: loss = 1.73514 (* 1 = 1.73514 loss)
I0815 16:36:07.847633  8764 sgd_solver.cpp:136] Iteration 117300, lr = 0.00266875, m = 0.9
I0815 16:36:26.957701  8764 solver.cpp:312] Iteration 117400 (5.23296 iter/s, 19.1096s/100 iter), loss = 1.51405
I0815 16:36:26.957754  8764 solver.cpp:334]     Train net output #0: loss = 1.56155 (* 1 = 1.56155 loss)
I0815 16:36:26.957762  8764 sgd_solver.cpp:136] Iteration 117400, lr = 0.0026625, m = 0.9
I0815 16:36:44.220196  8764 solver.cpp:312] Iteration 117500 (5.79306 iter/s, 17.262s/100 iter), loss = 1.38498
I0815 16:36:44.220222  8764 solver.cpp:334]     Train net output #0: loss = 0.959721 (* 1 = 0.959721 loss)
I0815 16:36:44.220228  8764 sgd_solver.cpp:136] Iteration 117500, lr = 0.00265625, m = 0.9
I0815 16:37:01.964162  8764 solver.cpp:312] Iteration 117600 (5.63587 iter/s, 17.7435s/100 iter), loss = 1.12726
I0815 16:37:01.968156  8764 solver.cpp:334]     Train net output #0: loss = 1.02013 (* 1 = 1.02013 loss)
I0815 16:37:01.968183  8764 sgd_solver.cpp:136] Iteration 117600, lr = 0.00265, m = 0.9
I0815 16:37:19.740970  8764 solver.cpp:312] Iteration 117700 (5.62546 iter/s, 17.7763s/100 iter), loss = 1.29783
I0815 16:37:19.740994  8764 solver.cpp:334]     Train net output #0: loss = 1.21238 (* 1 = 1.21238 loss)
I0815 16:37:19.740999  8764 sgd_solver.cpp:136] Iteration 117700, lr = 0.00264375, m = 0.9
I0815 16:37:38.017568  8764 solver.cpp:312] Iteration 117800 (5.47165 iter/s, 18.276s/100 iter), loss = 1.29843
I0815 16:37:38.017964  8764 solver.cpp:334]     Train net output #0: loss = 1.09797 (* 1 = 1.09797 loss)
I0815 16:37:38.018024  8764 sgd_solver.cpp:136] Iteration 117800, lr = 0.0026375, m = 0.9
I0815 16:37:56.238483  8764 solver.cpp:312] Iteration 117900 (5.48834 iter/s, 18.2204s/100 iter), loss = 1.26849
I0815 16:37:56.238509  8764 solver.cpp:334]     Train net output #0: loss = 1.23976 (* 1 = 1.23976 loss)
I0815 16:37:56.238513  8764 sgd_solver.cpp:136] Iteration 117900, lr = 0.00263125, m = 0.9
I0815 16:38:13.615406  8764 solver.cpp:363] Sparsity after update:
I0815 16:38:13.621476  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:38:13.621490  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:38:13.621611  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:38:13.621685  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:38:13.621812  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:38:13.621882  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:38:13.621953  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:38:13.622020  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:38:13.622519  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:38:13.622596  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:38:13.622668  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:38:13.622738  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:38:13.622809  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:38:13.622931  8764 solver.cpp:509] Iteration 118000, Testing net (#0)
I0815 16:38:40.525478  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58347
I0815 16:38:40.525518  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815409
I0815 16:38:40.525529  8764 solver.cpp:594]     Test net output #2: loss = 1.80735 (* 1 = 1.80735 loss)
I0815 16:38:40.525563  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9019s
I0815 16:38:40.739624  8764 solver.cpp:312] Iteration 118000 (2.24719 iter/s, 44.4999s/100 iter), loss = 1.47512
I0815 16:38:40.739653  8764 solver.cpp:334]     Train net output #0: loss = 1.38109 (* 1 = 1.38109 loss)
I0815 16:38:40.739658  8764 sgd_solver.cpp:136] Iteration 118000, lr = 0.002625, m = 0.9
I0815 16:38:58.140447  8764 solver.cpp:312] Iteration 118100 (5.74701 iter/s, 17.4003s/100 iter), loss = 1.49844
I0815 16:38:58.140535  8764 solver.cpp:334]     Train net output #0: loss = 1.54564 (* 1 = 1.54564 loss)
I0815 16:38:58.140543  8764 sgd_solver.cpp:136] Iteration 118100, lr = 0.00261875, m = 0.9
I0815 16:39:15.630507  8764 solver.cpp:312] Iteration 118200 (5.71769 iter/s, 17.4896s/100 iter), loss = 1.60109
I0815 16:39:15.630535  8764 solver.cpp:334]     Train net output #0: loss = 1.26314 (* 1 = 1.26314 loss)
I0815 16:39:15.630542  8764 sgd_solver.cpp:136] Iteration 118200, lr = 0.0026125, m = 0.9
I0815 16:39:30.750941  8764 solver.cpp:312] Iteration 118300 (6.61375 iter/s, 15.12s/100 iter), loss = 1.46331
I0815 16:39:30.752180  8764 solver.cpp:334]     Train net output #0: loss = 1.67239 (* 1 = 1.67239 loss)
I0815 16:39:30.752198  8764 sgd_solver.cpp:136] Iteration 118300, lr = 0.00260625, m = 0.9
I0815 16:39:48.820966  8764 solver.cpp:312] Iteration 118400 (5.53418 iter/s, 18.0695s/100 iter), loss = 1.55364
I0815 16:39:48.821012  8764 solver.cpp:334]     Train net output #0: loss = 1.28088 (* 1 = 1.28088 loss)
I0815 16:39:48.821023  8764 sgd_solver.cpp:136] Iteration 118400, lr = 0.0026, m = 0.9
I0815 16:40:11.935317  8764 solver.cpp:312] Iteration 118500 (4.32644 iter/s, 23.1137s/100 iter), loss = 1.68218
I0815 16:40:11.935405  8764 solver.cpp:334]     Train net output #0: loss = 1.78796 (* 1 = 1.78796 loss)
I0815 16:40:11.935415  8764 sgd_solver.cpp:136] Iteration 118500, lr = 0.00259375, m = 0.9
I0815 16:40:33.341030  8764 solver.cpp:312] Iteration 118600 (4.67178 iter/s, 21.4051s/100 iter), loss = 1.19838
I0815 16:40:33.341073  8764 solver.cpp:334]     Train net output #0: loss = 1.37162 (* 1 = 1.37162 loss)
I0815 16:40:33.341084  8764 sgd_solver.cpp:136] Iteration 118600, lr = 0.0025875, m = 0.9
I0815 16:40:53.358410  8764 solver.cpp:312] Iteration 118700 (4.9958 iter/s, 20.0168s/100 iter), loss = 1.68358
I0815 16:40:53.358675  8764 solver.cpp:334]     Train net output #0: loss = 1.39555 (* 1 = 1.39555 loss)
I0815 16:40:53.358695  8764 sgd_solver.cpp:136] Iteration 118700, lr = 0.00258125, m = 0.9
I0815 16:41:11.592865  8764 solver.cpp:312] Iteration 118800 (5.48427 iter/s, 18.234s/100 iter), loss = 1.28401
I0815 16:41:11.592926  8764 solver.cpp:334]     Train net output #0: loss = 1.39824 (* 1 = 1.39824 loss)
I0815 16:41:11.592941  8764 sgd_solver.cpp:136] Iteration 118800, lr = 0.002575, m = 0.9
I0815 16:41:32.026688  8764 solver.cpp:312] Iteration 118900 (4.89398 iter/s, 20.4333s/100 iter), loss = 1.23373
I0815 16:41:32.026741  8764 solver.cpp:334]     Train net output #0: loss = 1.10099 (* 1 = 1.10099 loss)
I0815 16:41:32.026748  8764 sgd_solver.cpp:136] Iteration 118900, lr = 0.00256875, m = 0.9
I0815 16:41:51.404461  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:41:52.735671  8764 solver.cpp:363] Sparsity after update:
I0815 16:41:52.746937  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:41:52.746966  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:41:52.746980  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:41:52.746989  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:41:52.746997  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:41:52.747005  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:41:52.747014  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:41:52.747021  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:41:52.747030  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:41:52.747040  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:41:52.747052  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:41:52.747066  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:41:52.747073  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:41:52.948786  8764 solver.cpp:312] Iteration 119000 (4.77977 iter/s, 20.9215s/100 iter), loss = 1.39004
I0815 16:41:52.957671  8764 solver.cpp:334]     Train net output #0: loss = 1.35477 (* 1 = 1.35477 loss)
I0815 16:41:52.957693  8764 sgd_solver.cpp:136] Iteration 119000, lr = 0.0025625, m = 0.9
I0815 16:42:14.021688  8764 solver.cpp:312] Iteration 119100 (4.74556 iter/s, 21.0723s/100 iter), loss = 1.49827
I0815 16:42:14.021809  8764 solver.cpp:334]     Train net output #0: loss = 1.90254 (* 1 = 1.90254 loss)
I0815 16:42:14.021829  8764 sgd_solver.cpp:136] Iteration 119100, lr = 0.00255625, m = 0.9
I0815 16:42:33.367374  8764 solver.cpp:312] Iteration 119200 (5.16925 iter/s, 19.3452s/100 iter), loss = 0.821491
I0815 16:42:33.367403  8764 solver.cpp:334]     Train net output #0: loss = 0.602199 (* 1 = 0.602199 loss)
I0815 16:42:33.367408  8764 sgd_solver.cpp:136] Iteration 119200, lr = 0.00255, m = 0.9
I0815 16:42:54.635284  8764 solver.cpp:312] Iteration 119300 (4.70205 iter/s, 21.2673s/100 iter), loss = 1.13889
I0815 16:42:54.635397  8764 solver.cpp:334]     Train net output #0: loss = 1.29693 (* 1 = 1.29693 loss)
I0815 16:42:54.635411  8764 sgd_solver.cpp:136] Iteration 119300, lr = 0.00254375, m = 0.9
I0815 16:43:12.934885  8764 solver.cpp:312] Iteration 119400 (5.46475 iter/s, 18.2991s/100 iter), loss = 1.38219
I0815 16:43:12.934911  8764 solver.cpp:334]     Train net output #0: loss = 1.24938 (* 1 = 1.24938 loss)
I0815 16:43:12.934916  8764 sgd_solver.cpp:136] Iteration 119400, lr = 0.0025375, m = 0.9
I0815 16:43:34.931947  8764 solver.cpp:312] Iteration 119500 (4.54619 iter/s, 21.9965s/100 iter), loss = 1.39768
I0815 16:43:34.932035  8764 solver.cpp:334]     Train net output #0: loss = 1.13036 (* 1 = 1.13036 loss)
I0815 16:43:34.932052  8764 sgd_solver.cpp:136] Iteration 119500, lr = 0.00253125, m = 0.9
I0815 16:43:58.376698  8764 solver.cpp:312] Iteration 119600 (4.26547 iter/s, 23.4441s/100 iter), loss = 1.25854
I0815 16:43:58.376796  8764 solver.cpp:334]     Train net output #0: loss = 1.43815 (* 1 = 1.43815 loss)
I0815 16:43:58.376821  8764 sgd_solver.cpp:136] Iteration 119600, lr = 0.002525, m = 0.9
I0815 16:44:19.809144  8764 solver.cpp:312] Iteration 119700 (4.66595 iter/s, 21.4319s/100 iter), loss = 1.24189
I0815 16:44:19.809242  8764 solver.cpp:334]     Train net output #0: loss = 1.05919 (* 1 = 1.05919 loss)
I0815 16:44:19.809257  8764 sgd_solver.cpp:136] Iteration 119700, lr = 0.00251875, m = 0.9
I0815 16:44:40.490239  8764 solver.cpp:312] Iteration 119800 (4.83547 iter/s, 20.6805s/100 iter), loss = 1.59107
I0815 16:44:40.490290  8764 solver.cpp:334]     Train net output #0: loss = 1.26387 (* 1 = 1.26387 loss)
I0815 16:44:40.490303  8764 sgd_solver.cpp:136] Iteration 119800, lr = 0.0025125, m = 0.9
I0815 16:45:00.260637  8764 solver.cpp:312] Iteration 119900 (5.05821 iter/s, 19.7699s/100 iter), loss = 1.30199
I0815 16:45:00.260697  8764 solver.cpp:334]     Train net output #0: loss = 1.17324 (* 1 = 1.17324 loss)
I0815 16:45:00.260704  8764 sgd_solver.cpp:136] Iteration 119900, lr = 0.00250625, m = 0.9
I0815 16:45:18.624758  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_120000.caffemodel
I0815 16:45:18.666470  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_120000.solverstate
I0815 16:45:18.673167  8764 solver.cpp:363] Sparsity after update:
I0815 16:45:18.675505  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:45:18.675693  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:45:18.675791  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:45:18.675879  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:45:18.675966  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:45:18.676065  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:45:18.676167  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:45:18.676255  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:45:18.676343  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:45:18.676434  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:45:18.676519  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:45:18.676635  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:45:18.676736  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:45:18.676862  8764 solver.cpp:509] Iteration 120000, Testing net (#0)
I0815 16:45:51.235754  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.587235
I0815 16:45:51.235857  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815879
I0815 16:45:51.235877  8764 solver.cpp:594]     Test net output #2: loss = 1.80091 (* 1 = 1.80091 loss)
I0815 16:45:51.235921  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.5582s
I0815 16:45:51.473497  8764 solver.cpp:312] Iteration 120000 (1.95269 iter/s, 51.2115s/100 iter), loss = 1.4675
I0815 16:45:51.473536  8764 solver.cpp:334]     Train net output #0: loss = 1.52106 (* 1 = 1.52106 loss)
I0815 16:45:51.473542  8764 sgd_solver.cpp:136] Iteration 120000, lr = 0.0025, m = 0.9
I0815 16:46:10.122344  8764 solver.cpp:312] Iteration 120100 (5.36241 iter/s, 18.6483s/100 iter), loss = 1.46773
I0815 16:46:10.122370  8764 solver.cpp:334]     Train net output #0: loss = 1.62554 (* 1 = 1.62554 loss)
I0815 16:46:10.122376  8764 sgd_solver.cpp:136] Iteration 120100, lr = 0.00249375, m = 0.9
I0815 16:46:29.465642  8764 solver.cpp:312] Iteration 120200 (5.16989 iter/s, 19.3428s/100 iter), loss = 1.52636
I0815 16:46:29.465762  8764 solver.cpp:334]     Train net output #0: loss = 1.38008 (* 1 = 1.38008 loss)
I0815 16:46:29.465770  8764 sgd_solver.cpp:136] Iteration 120200, lr = 0.0024875, m = 0.9
I0815 16:46:46.704790  8764 solver.cpp:312] Iteration 120300 (5.80091 iter/s, 17.2387s/100 iter), loss = 1.77569
I0815 16:46:46.704988  8764 solver.cpp:334]     Train net output #0: loss = 0.898904 (* 1 = 0.898904 loss)
I0815 16:46:46.705129  8764 sgd_solver.cpp:136] Iteration 120300, lr = 0.00248125, m = 0.9
I0815 16:47:07.877898  8764 solver.cpp:312] Iteration 120400 (4.7231 iter/s, 21.1725s/100 iter), loss = 1.25235
I0815 16:47:07.877959  8764 solver.cpp:334]     Train net output #0: loss = 1.00787 (* 1 = 1.00787 loss)
I0815 16:47:07.877966  8764 sgd_solver.cpp:136] Iteration 120400, lr = 0.002475, m = 0.9
I0815 16:47:27.694578  8764 solver.cpp:312] Iteration 120500 (5.04641 iter/s, 19.8161s/100 iter), loss = 1.50518
I0815 16:47:27.694747  8764 solver.cpp:334]     Train net output #0: loss = 1.31917 (* 1 = 1.31917 loss)
I0815 16:47:27.694788  8764 sgd_solver.cpp:136] Iteration 120500, lr = 0.00246875, m = 0.9
I0815 16:47:43.641980  8764 solver.cpp:312] Iteration 120600 (6.27078 iter/s, 15.947s/100 iter), loss = 1.30507
I0815 16:47:43.642060  8764 solver.cpp:334]     Train net output #0: loss = 1.31046 (* 1 = 1.31046 loss)
I0815 16:47:43.642073  8764 sgd_solver.cpp:136] Iteration 120600, lr = 0.0024625, m = 0.9
I0815 16:48:04.690767  8764 solver.cpp:312] Iteration 120700 (4.751 iter/s, 21.0482s/100 iter), loss = 1.19732
I0815 16:48:04.690824  8764 solver.cpp:334]     Train net output #0: loss = 1.01484 (* 1 = 1.01484 loss)
I0815 16:48:04.690830  8764 sgd_solver.cpp:136] Iteration 120700, lr = 0.00245625, m = 0.9
I0815 16:48:24.615062  8764 solver.cpp:312] Iteration 120800 (5.01914 iter/s, 19.9237s/100 iter), loss = 1.59322
I0815 16:48:24.615198  8764 solver.cpp:334]     Train net output #0: loss = 1.38399 (* 1 = 1.38399 loss)
I0815 16:48:24.615227  8764 sgd_solver.cpp:136] Iteration 120800, lr = 0.00245, m = 0.9
I0815 16:48:44.378449  8764 solver.cpp:312] Iteration 120900 (5.06 iter/s, 19.7628s/100 iter), loss = 0.938661
I0815 16:48:44.378515  8764 solver.cpp:334]     Train net output #0: loss = 1.01031 (* 1 = 1.01031 loss)
I0815 16:48:44.378532  8764 sgd_solver.cpp:136] Iteration 120900, lr = 0.00244375, m = 0.9
I0815 16:49:01.199909  8764 solver.cpp:363] Sparsity after update:
I0815 16:49:01.210317  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:49:01.210350  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:49:01.210371  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:49:01.210381  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:49:01.210389  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:49:01.210398  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:49:01.210407  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:49:01.210415  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:49:01.210424  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:49:01.210433  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:49:01.210443  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:49:01.210450  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:49:01.210459  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:49:01.362154  8764 solver.cpp:312] Iteration 121000 (5.88816 iter/s, 16.9832s/100 iter), loss = 1.323
I0815 16:49:01.362182  8764 solver.cpp:334]     Train net output #0: loss = 1.03387 (* 1 = 1.03387 loss)
I0815 16:49:01.362188  8764 sgd_solver.cpp:136] Iteration 121000, lr = 0.0024375, m = 0.9
I0815 16:49:21.489238  8764 solver.cpp:312] Iteration 121100 (4.96857 iter/s, 20.1265s/100 iter), loss = 1.43609
I0815 16:49:21.489274  8764 solver.cpp:334]     Train net output #0: loss = 1.41473 (* 1 = 1.41473 loss)
I0815 16:49:21.489279  8764 sgd_solver.cpp:136] Iteration 121100, lr = 0.00243125, m = 0.9
I0815 16:49:41.729523  8764 solver.cpp:312] Iteration 121200 (4.94078 iter/s, 20.2397s/100 iter), loss = 1.63406
I0815 16:49:41.729578  8764 solver.cpp:334]     Train net output #0: loss = 1.47157 (* 1 = 1.47157 loss)
I0815 16:49:41.729583  8764 sgd_solver.cpp:136] Iteration 121200, lr = 0.002425, m = 0.9
I0815 16:49:58.368386  8764 solver.cpp:312] Iteration 121300 (6.01019 iter/s, 16.6384s/100 iter), loss = 1.43773
I0815 16:49:58.368412  8764 solver.cpp:334]     Train net output #0: loss = 1.02348 (* 1 = 1.02348 loss)
I0815 16:49:58.368418  8764 sgd_solver.cpp:136] Iteration 121300, lr = 0.00241875, m = 0.9
I0815 16:50:16.628170  8764 solver.cpp:312] Iteration 121400 (5.47667 iter/s, 18.2593s/100 iter), loss = 1.36189
I0815 16:50:16.632172  8764 solver.cpp:334]     Train net output #0: loss = 0.84588 (* 1 = 0.84588 loss)
I0815 16:50:16.632189  8764 sgd_solver.cpp:136] Iteration 121400, lr = 0.0024125, m = 0.9
I0815 16:50:36.053176  8764 solver.cpp:312] Iteration 121500 (5.14814 iter/s, 19.4245s/100 iter), loss = 1.53727
I0815 16:50:36.053222  8764 solver.cpp:334]     Train net output #0: loss = 1.56684 (* 1 = 1.56684 loss)
I0815 16:50:36.053236  8764 sgd_solver.cpp:136] Iteration 121500, lr = 0.00240625, m = 0.9
I0815 16:50:53.541620  8764 solver.cpp:312] Iteration 121600 (5.71822 iter/s, 17.488s/100 iter), loss = 1.56711
I0815 16:50:53.541677  8764 solver.cpp:334]     Train net output #0: loss = 1.32909 (* 1 = 1.32909 loss)
I0815 16:50:53.541683  8764 sgd_solver.cpp:136] Iteration 121600, lr = 0.0024, m = 0.9
I0815 16:51:11.825129  8764 solver.cpp:312] Iteration 121700 (5.46956 iter/s, 18.283s/100 iter), loss = 1.41876
I0815 16:51:11.825155  8764 solver.cpp:334]     Train net output #0: loss = 1.41382 (* 1 = 1.41382 loss)
I0815 16:51:11.825160  8764 sgd_solver.cpp:136] Iteration 121700, lr = 0.00239375, m = 0.9
I0815 16:51:29.925391  8764 solver.cpp:312] Iteration 121800 (5.52494 iter/s, 18.0998s/100 iter), loss = 1.31249
I0815 16:51:29.925493  8764 solver.cpp:334]     Train net output #0: loss = 1.24379 (* 1 = 1.24379 loss)
I0815 16:51:29.925513  8764 sgd_solver.cpp:136] Iteration 121800, lr = 0.0023875, m = 0.9
I0815 16:51:49.532654  8764 solver.cpp:312] Iteration 121900 (5.10029 iter/s, 19.6067s/100 iter), loss = 1.86534
I0815 16:51:49.532676  8764 solver.cpp:334]     Train net output #0: loss = 1.74769 (* 1 = 1.74769 loss)
I0815 16:51:49.532680  8764 sgd_solver.cpp:136] Iteration 121900, lr = 0.00238125, m = 0.9
I0815 16:52:09.011860  8764 solver.cpp:363] Sparsity after update:
I0815 16:52:09.016762  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:52:09.016808  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:52:09.016842  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:52:09.016863  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:52:09.016880  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:52:09.016897  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:52:09.016914  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:52:09.016933  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:52:09.016952  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:52:09.016970  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:52:09.016988  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:52:09.017004  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:52:09.017019  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:52:09.017045  8764 solver.cpp:509] Iteration 122000, Testing net (#0)
I0815 16:52:27.728909  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 16:52:39.974268  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.583588
I0815 16:52:39.974483  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.815997
I0815 16:52:39.974512  8764 solver.cpp:594]     Test net output #2: loss = 1.80454 (* 1 = 1.80454 loss)
I0815 16:52:39.974576  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.9567s
I0815 16:52:40.234491  8764 solver.cpp:312] Iteration 122000 (1.97237 iter/s, 50.7005s/100 iter), loss = 1.23455
I0815 16:52:40.234550  8764 solver.cpp:334]     Train net output #0: loss = 1.29382 (* 1 = 1.29382 loss)
I0815 16:52:40.234567  8764 sgd_solver.cpp:136] Iteration 122000, lr = 0.002375, m = 0.9
I0815 16:52:58.608296  8764 solver.cpp:312] Iteration 122100 (5.44268 iter/s, 18.3733s/100 iter), loss = 1.44568
I0815 16:52:58.608321  8764 solver.cpp:334]     Train net output #0: loss = 1.35227 (* 1 = 1.35227 loss)
I0815 16:52:58.608327  8764 sgd_solver.cpp:136] Iteration 122100, lr = 0.00236875, m = 0.9
I0815 16:53:16.711833  8764 solver.cpp:312] Iteration 122200 (5.52394 iter/s, 18.103s/100 iter), loss = 1.10274
I0815 16:53:16.711895  8764 solver.cpp:334]     Train net output #0: loss = 1.0669 (* 1 = 1.0669 loss)
I0815 16:53:16.711901  8764 sgd_solver.cpp:136] Iteration 122200, lr = 0.0023625, m = 0.9
I0815 16:53:32.852174  8764 solver.cpp:312] Iteration 122300 (6.19583 iter/s, 16.1399s/100 iter), loss = 1.28606
I0815 16:53:32.852232  8764 solver.cpp:334]     Train net output #0: loss = 1.24172 (* 1 = 1.24172 loss)
I0815 16:53:32.852244  8764 sgd_solver.cpp:136] Iteration 122300, lr = 0.00235625, m = 0.9
I0815 16:53:51.769840  8764 solver.cpp:312] Iteration 122400 (5.28621 iter/s, 18.9171s/100 iter), loss = 1.34715
I0815 16:53:51.770097  8764 solver.cpp:334]     Train net output #0: loss = 1.43305 (* 1 = 1.43305 loss)
I0815 16:53:51.770117  8764 sgd_solver.cpp:136] Iteration 122400, lr = 0.00235, m = 0.9
I0815 16:54:10.961478  8764 solver.cpp:312] Iteration 122500 (5.21075 iter/s, 19.1911s/100 iter), loss = 1.42986
I0815 16:54:10.961510  8764 solver.cpp:334]     Train net output #0: loss = 1.4375 (* 1 = 1.4375 loss)
I0815 16:54:10.961518  8764 sgd_solver.cpp:136] Iteration 122500, lr = 0.00234375, m = 0.9
I0815 16:54:29.465615  8764 solver.cpp:312] Iteration 122600 (5.40435 iter/s, 18.5036s/100 iter), loss = 1.51816
I0815 16:54:29.465689  8764 solver.cpp:334]     Train net output #0: loss = 1.56053 (* 1 = 1.56053 loss)
I0815 16:54:29.465697  8764 sgd_solver.cpp:136] Iteration 122600, lr = 0.0023375, m = 0.9
I0815 16:54:46.873265  8764 solver.cpp:312] Iteration 122700 (5.74476 iter/s, 17.4072s/100 iter), loss = 1.34204
I0815 16:54:46.873288  8764 solver.cpp:334]     Train net output #0: loss = 1.38463 (* 1 = 1.38463 loss)
I0815 16:54:46.873293  8764 sgd_solver.cpp:136] Iteration 122700, lr = 0.00233125, m = 0.9
I0815 16:55:03.527963  8764 solver.cpp:312] Iteration 122800 (6.00448 iter/s, 16.6542s/100 iter), loss = 1.57483
I0815 16:55:03.528023  8764 solver.cpp:334]     Train net output #0: loss = 1.70635 (* 1 = 1.70635 loss)
I0815 16:55:03.528028  8764 sgd_solver.cpp:136] Iteration 122800, lr = 0.002325, m = 0.9
I0815 16:55:21.731648  8764 solver.cpp:312] Iteration 122900 (5.49354 iter/s, 18.2032s/100 iter), loss = 1.53598
I0815 16:55:21.731673  8764 solver.cpp:334]     Train net output #0: loss = 1.65499 (* 1 = 1.65499 loss)
I0815 16:55:21.731676  8764 sgd_solver.cpp:136] Iteration 122900, lr = 0.00231875, m = 0.9
I0815 16:55:39.265154  8764 solver.cpp:363] Sparsity after update:
I0815 16:55:39.275604  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:55:39.275630  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:55:39.275646  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:55:39.275655  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:55:39.275665  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:55:39.275674  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:55:39.275683  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:55:39.275693  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:55:39.275702  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:55:39.275712  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:55:39.275722  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:55:39.275730  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:55:39.275740  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:55:39.408109  8764 solver.cpp:312] Iteration 123000 (5.6574 iter/s, 17.676s/100 iter), loss = 1.31762
I0815 16:55:39.408161  8764 solver.cpp:334]     Train net output #0: loss = 1.07071 (* 1 = 1.07071 loss)
I0815 16:55:39.408181  8764 sgd_solver.cpp:136] Iteration 123000, lr = 0.0023125, m = 0.9
I0815 16:55:56.402187  8764 solver.cpp:312] Iteration 123100 (5.88457 iter/s, 16.9936s/100 iter), loss = 1.37889
I0815 16:55:56.402215  8764 solver.cpp:334]     Train net output #0: loss = 1.65764 (* 1 = 1.65764 loss)
I0815 16:55:56.402221  8764 sgd_solver.cpp:136] Iteration 123100, lr = 0.00230625, m = 0.9
I0815 16:56:12.264235  8764 solver.cpp:312] Iteration 123200 (6.30455 iter/s, 15.8616s/100 iter), loss = 1.69522
I0815 16:56:12.264420  8764 solver.cpp:334]     Train net output #0: loss = 1.76185 (* 1 = 1.76185 loss)
I0815 16:56:12.264468  8764 sgd_solver.cpp:136] Iteration 123200, lr = 0.0023, m = 0.9
I0815 16:56:29.528717  8764 solver.cpp:312] Iteration 123300 (5.79239 iter/s, 17.264s/100 iter), loss = 1.48133
I0815 16:56:29.528740  8764 solver.cpp:334]     Train net output #0: loss = 1.2064 (* 1 = 1.2064 loss)
I0815 16:56:29.528745  8764 sgd_solver.cpp:136] Iteration 123300, lr = 0.00229375, m = 0.9
I0815 16:56:46.258316  8764 solver.cpp:312] Iteration 123400 (5.97759 iter/s, 16.7291s/100 iter), loss = 1.3797
I0815 16:56:46.258374  8764 solver.cpp:334]     Train net output #0: loss = 1.43731 (* 1 = 1.43731 loss)
I0815 16:56:46.258381  8764 sgd_solver.cpp:136] Iteration 123400, lr = 0.0022875, m = 0.9
I0815 16:57:02.679810  8764 solver.cpp:312] Iteration 123500 (6.08975 iter/s, 16.421s/100 iter), loss = 1.36917
I0815 16:57:02.679838  8764 solver.cpp:334]     Train net output #0: loss = 1.00788 (* 1 = 1.00788 loss)
I0815 16:57:02.679843  8764 sgd_solver.cpp:136] Iteration 123500, lr = 0.00228125, m = 0.9
I0815 16:57:21.611925  8764 solver.cpp:312] Iteration 123600 (5.28218 iter/s, 18.9316s/100 iter), loss = 1.35995
I0815 16:57:21.612009  8764 solver.cpp:334]     Train net output #0: loss = 1.51726 (* 1 = 1.51726 loss)
I0815 16:57:21.612022  8764 sgd_solver.cpp:136] Iteration 123600, lr = 0.002275, m = 0.9
I0815 16:57:43.186204  8764 solver.cpp:312] Iteration 123700 (4.63528 iter/s, 21.5737s/100 iter), loss = 1.60536
I0815 16:57:43.186256  8764 solver.cpp:334]     Train net output #0: loss = 1.78078 (* 1 = 1.78078 loss)
I0815 16:57:43.186271  8764 sgd_solver.cpp:136] Iteration 123700, lr = 0.00226875, m = 0.9
I0815 16:58:05.750020  8764 solver.cpp:312] Iteration 123800 (4.43199 iter/s, 22.5632s/100 iter), loss = 1.29464
I0815 16:58:05.750123  8764 solver.cpp:334]     Train net output #0: loss = 1.45853 (* 1 = 1.45853 loss)
I0815 16:58:05.750135  8764 sgd_solver.cpp:136] Iteration 123800, lr = 0.0022625, m = 0.9
I0815 16:58:23.996311  8764 solver.cpp:312] Iteration 123900 (5.48072 iter/s, 18.2458s/100 iter), loss = 1.52835
I0815 16:58:23.996379  8764 solver.cpp:334]     Train net output #0: loss = 1.72224 (* 1 = 1.72224 loss)
I0815 16:58:23.996402  8764 sgd_solver.cpp:136] Iteration 123900, lr = 0.00225625, m = 0.9
I0815 16:58:47.462265  8764 solver.cpp:363] Sparsity after update:
I0815 16:58:47.465039  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 16:58:47.465060  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 16:58:47.465077  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 16:58:47.465085  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 16:58:47.465092  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 16:58:47.465101  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 16:58:47.465106  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 16:58:47.465113  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 16:58:47.465121  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 16:58:47.465127  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 16:58:47.465134  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 16:58:47.465140  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 16:58:47.465147  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 16:58:47.465168  8764 solver.cpp:509] Iteration 124000, Testing net (#0)
I0815 16:59:17.309409  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.585883
I0815 16:59:17.309458  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814879
I0815 16:59:17.309475  8764 solver.cpp:594]     Test net output #2: loss = 1.80197 (* 1 = 1.80197 loss)
I0815 16:59:17.309587  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.8436s
I0815 16:59:17.688177  8764 solver.cpp:312] Iteration 124000 (1.86253 iter/s, 53.6904s/100 iter), loss = 1.60112
I0815 16:59:17.688356  8764 solver.cpp:334]     Train net output #0: loss = 1.61323 (* 1 = 1.61323 loss)
I0815 16:59:17.688393  8764 sgd_solver.cpp:136] Iteration 124000, lr = 0.00225, m = 0.9
I0815 16:59:33.651476  8764 solver.cpp:312] Iteration 124100 (6.26454 iter/s, 15.9629s/100 iter), loss = 1.41488
I0815 16:59:33.651521  8764 solver.cpp:334]     Train net output #0: loss = 1.20451 (* 1 = 1.20451 loss)
I0815 16:59:33.651531  8764 sgd_solver.cpp:136] Iteration 124100, lr = 0.00224375, m = 0.9
I0815 16:59:53.134361  8764 solver.cpp:312] Iteration 124200 (5.13285 iter/s, 19.4823s/100 iter), loss = 1.53534
I0815 16:59:53.134441  8764 solver.cpp:334]     Train net output #0: loss = 1.24198 (* 1 = 1.24198 loss)
I0815 16:59:53.134454  8764 sgd_solver.cpp:136] Iteration 124200, lr = 0.0022375, m = 0.9
I0815 17:00:13.985292  8764 solver.cpp:312] Iteration 124300 (4.79608 iter/s, 20.8504s/100 iter), loss = 1.35472
I0815 17:00:13.985366  8764 solver.cpp:334]     Train net output #0: loss = 1.49106 (* 1 = 1.49106 loss)
I0815 17:00:13.985386  8764 sgd_solver.cpp:136] Iteration 124300, lr = 0.00223125, m = 0.9
I0815 17:00:34.017964  8764 solver.cpp:312] Iteration 124400 (4.99198 iter/s, 20.0321s/100 iter), loss = 1.12927
I0815 17:00:34.018193  8764 solver.cpp:334]     Train net output #0: loss = 1.12176 (* 1 = 1.12176 loss)
I0815 17:00:34.018206  8764 sgd_solver.cpp:136] Iteration 124400, lr = 0.002225, m = 0.9
I0815 17:00:51.444598  8764 solver.cpp:312] Iteration 124500 (5.73852 iter/s, 17.4261s/100 iter), loss = 1.5884
I0815 17:00:51.444795  8764 solver.cpp:334]     Train net output #0: loss = 1.32972 (* 1 = 1.32972 loss)
I0815 17:00:51.444834  8764 sgd_solver.cpp:136] Iteration 124500, lr = 0.00221875, m = 0.9
I0815 17:01:09.807231  8764 solver.cpp:312] Iteration 124600 (5.44599 iter/s, 18.3621s/100 iter), loss = 1.34828
I0815 17:01:09.807293  8764 solver.cpp:334]     Train net output #0: loss = 1.22787 (* 1 = 1.22787 loss)
I0815 17:01:09.807301  8764 sgd_solver.cpp:136] Iteration 124600, lr = 0.0022125, m = 0.9
I0815 17:01:32.710289  8764 solver.cpp:312] Iteration 124700 (4.36635 iter/s, 22.9024s/100 iter), loss = 1.40382
I0815 17:01:32.710312  8764 solver.cpp:334]     Train net output #0: loss = 1.252 (* 1 = 1.252 loss)
I0815 17:01:32.710317  8764 sgd_solver.cpp:136] Iteration 124700, lr = 0.00220625, m = 0.9
I0815 17:01:50.270531  8764 solver.cpp:312] Iteration 124800 (5.69484 iter/s, 17.5598s/100 iter), loss = 1.23158
I0815 17:01:50.270778  8764 solver.cpp:334]     Train net output #0: loss = 1.25457 (* 1 = 1.25457 loss)
I0815 17:01:50.270794  8764 sgd_solver.cpp:136] Iteration 124800, lr = 0.0022, m = 0.9
I0815 17:02:09.040791  8764 solver.cpp:312] Iteration 124900 (5.32772 iter/s, 18.7697s/100 iter), loss = 1.53368
I0815 17:02:09.040814  8764 solver.cpp:334]     Train net output #0: loss = 1.68444 (* 1 = 1.68444 loss)
I0815 17:02:09.040820  8764 sgd_solver.cpp:136] Iteration 124900, lr = 0.00219375, m = 0.9
I0815 17:02:28.300740  8764 solver.cpp:363] Sparsity after update:
I0815 17:02:28.341627  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:02:28.341665  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:02:28.341687  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:02:28.341696  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:02:28.341704  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:02:28.341711  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:02:28.341718  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:02:28.341725  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:02:28.341732  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:02:28.341738  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:02:28.341745  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:02:28.341753  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:02:28.341759  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:02:28.563820  8764 solver.cpp:312] Iteration 125000 (5.1223 iter/s, 19.5225s/100 iter), loss = 1.68212
I0815 17:02:28.563891  8764 solver.cpp:334]     Train net output #0: loss = 1.33714 (* 1 = 1.33714 loss)
I0815 17:02:28.563910  8764 sgd_solver.cpp:136] Iteration 125000, lr = 0.0021875, m = 0.9
I0815 17:02:48.677747  8764 solver.cpp:312] Iteration 125100 (4.97182 iter/s, 20.1134s/100 iter), loss = 1.36924
I0815 17:02:48.677927  8764 solver.cpp:334]     Train net output #0: loss = 1.18669 (* 1 = 1.18669 loss)
I0815 17:02:48.678002  8764 sgd_solver.cpp:136] Iteration 125100, lr = 0.00218125, m = 0.9
I0815 17:03:05.698107  8764 solver.cpp:312] Iteration 125200 (5.87548 iter/s, 17.0199s/100 iter), loss = 1.15267
I0815 17:03:05.698168  8764 solver.cpp:334]     Train net output #0: loss = 1.28245 (* 1 = 1.28245 loss)
I0815 17:03:05.698174  8764 sgd_solver.cpp:136] Iteration 125200, lr = 0.002175, m = 0.9
I0815 17:03:25.089874  8764 solver.cpp:312] Iteration 125300 (5.15697 iter/s, 19.3912s/100 iter), loss = 1.56301
I0815 17:03:25.089932  8764 solver.cpp:334]     Train net output #0: loss = 1.42842 (* 1 = 1.42842 loss)
I0815 17:03:25.089951  8764 sgd_solver.cpp:136] Iteration 125300, lr = 0.00216875, m = 0.9
I0815 17:03:46.719861  8764 solver.cpp:312] Iteration 125400 (4.62334 iter/s, 21.6294s/100 iter), loss = 1.5784
I0815 17:03:46.719938  8764 solver.cpp:334]     Train net output #0: loss = 1.4181 (* 1 = 1.4181 loss)
I0815 17:03:46.719951  8764 sgd_solver.cpp:136] Iteration 125400, lr = 0.0021625, m = 0.9
I0815 17:04:06.789422  8764 solver.cpp:312] Iteration 125500 (4.98281 iter/s, 20.069s/100 iter), loss = 1.50448
I0815 17:04:06.789453  8764 solver.cpp:334]     Train net output #0: loss = 1.45248 (* 1 = 1.45248 loss)
I0815 17:04:06.789458  8764 sgd_solver.cpp:136] Iteration 125500, lr = 0.00215625, m = 0.9
I0815 17:04:26.275662  8764 solver.cpp:312] Iteration 125600 (5.13197 iter/s, 19.4857s/100 iter), loss = 1.70883
I0815 17:04:26.275744  8764 solver.cpp:334]     Train net output #0: loss = 1.77564 (* 1 = 1.77564 loss)
I0815 17:04:26.275759  8764 sgd_solver.cpp:136] Iteration 125600, lr = 0.00215, m = 0.9
I0815 17:04:45.636158  8764 solver.cpp:312] Iteration 125700 (5.1653 iter/s, 19.36s/100 iter), loss = 1.32027
I0815 17:04:45.636229  8764 solver.cpp:334]     Train net output #0: loss = 1.37896 (* 1 = 1.37896 loss)
I0815 17:04:45.636248  8764 sgd_solver.cpp:136] Iteration 125700, lr = 0.00214375, m = 0.9
I0815 17:05:06.461311  8764 solver.cpp:312] Iteration 125800 (4.80202 iter/s, 20.8246s/100 iter), loss = 1.22197
I0815 17:05:06.463407  8764 solver.cpp:334]     Train net output #0: loss = 1.45306 (* 1 = 1.45306 loss)
I0815 17:05:06.463515  8764 sgd_solver.cpp:136] Iteration 125800, lr = 0.0021375, m = 0.9
I0815 17:05:23.392499  8764 solver.cpp:312] Iteration 125900 (5.90642 iter/s, 16.9307s/100 iter), loss = 1.44101
I0815 17:05:23.392561  8764 solver.cpp:334]     Train net output #0: loss = 1.50898 (* 1 = 1.50898 loss)
I0815 17:05:23.392580  8764 sgd_solver.cpp:136] Iteration 125900, lr = 0.00213125, m = 0.9
I0815 17:05:43.426791  8764 solver.cpp:363] Sparsity after update:
I0815 17:05:43.432145  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:05:43.432164  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:05:43.432176  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:05:43.432180  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:05:43.432183  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:05:43.432188  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:05:43.432191  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:05:43.432195  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:05:43.432199  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:05:43.432202  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:05:43.432205  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:05:43.432209  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:05:43.432212  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:05:43.432225  8764 solver.cpp:509] Iteration 126000, Testing net (#0)
I0815 17:05:48.058756  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:06:16.505400  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.589764
I0815 17:06:16.505456  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819526
I0815 17:06:16.505465  8764 solver.cpp:594]     Test net output #2: loss = 1.79839 (* 1 = 1.79839 loss)
I0815 17:06:16.505486  8764 solver.cpp:264] [MultiGPU] Tests completed in 33.0724s
I0815 17:06:16.735556  8764 solver.cpp:312] Iteration 126000 (1.87471 iter/s, 53.3416s/100 iter), loss = 1.82476
I0815 17:06:16.735600  8764 solver.cpp:334]     Train net output #0: loss = 1.64501 (* 1 = 1.64501 loss)
I0815 17:06:16.735615  8764 sgd_solver.cpp:136] Iteration 126000, lr = 0.002125, m = 0.9
I0815 17:06:34.124151  8764 solver.cpp:312] Iteration 126100 (5.75106 iter/s, 17.3881s/100 iter), loss = 1.20154
I0815 17:06:34.124320  8764 solver.cpp:334]     Train net output #0: loss = 1.07767 (* 1 = 1.07767 loss)
I0815 17:06:34.124348  8764 sgd_solver.cpp:136] Iteration 126100, lr = 0.00211875, m = 0.9
I0815 17:06:53.305510  8764 solver.cpp:312] Iteration 126200 (5.21354 iter/s, 19.1808s/100 iter), loss = 1.45512
I0815 17:06:53.305625  8764 solver.cpp:334]     Train net output #0: loss = 1.59632 (* 1 = 1.59632 loss)
I0815 17:06:53.305641  8764 sgd_solver.cpp:136] Iteration 126200, lr = 0.0021125, m = 0.9
I0815 17:07:13.596163  8764 solver.cpp:312] Iteration 126300 (4.92851 iter/s, 20.2901s/100 iter), loss = 1.68595
I0815 17:07:13.596236  8764 solver.cpp:334]     Train net output #0: loss = 2.17678 (* 1 = 2.17678 loss)
I0815 17:07:13.596256  8764 sgd_solver.cpp:136] Iteration 126300, lr = 0.00210625, m = 0.9
I0815 17:07:32.603162  8764 solver.cpp:312] Iteration 126400 (5.26137 iter/s, 19.0065s/100 iter), loss = 1.35398
I0815 17:07:32.616180  8764 solver.cpp:334]     Train net output #0: loss = 1.36872 (* 1 = 1.36872 loss)
I0815 17:07:32.616209  8764 sgd_solver.cpp:136] Iteration 126400, lr = 0.0021, m = 0.9
I0815 17:07:49.243892  8764 solver.cpp:312] Iteration 126500 (6.00952 iter/s, 16.6403s/100 iter), loss = 1.84949
I0815 17:07:49.243984  8764 solver.cpp:334]     Train net output #0: loss = 2.01681 (* 1 = 2.01681 loss)
I0815 17:07:49.244010  8764 sgd_solver.cpp:136] Iteration 126500, lr = 0.00209375, m = 0.9
I0815 17:08:09.406693  8764 solver.cpp:312] Iteration 126600 (4.95976 iter/s, 20.1623s/100 iter), loss = 1.63575
I0815 17:08:09.406761  8764 solver.cpp:334]     Train net output #0: loss = 1.41282 (* 1 = 1.41282 loss)
I0815 17:08:09.406769  8764 sgd_solver.cpp:136] Iteration 126600, lr = 0.0020875, m = 0.9
I0815 17:08:27.970481  8764 solver.cpp:312] Iteration 126700 (5.38698 iter/s, 18.5633s/100 iter), loss = 1.37446
I0815 17:08:27.970527  8764 solver.cpp:334]     Train net output #0: loss = 1.36834 (* 1 = 1.36834 loss)
I0815 17:08:27.970540  8764 sgd_solver.cpp:136] Iteration 126700, lr = 0.00208125, m = 0.9
I0815 17:08:47.746351  8764 solver.cpp:312] Iteration 126800 (5.05681 iter/s, 19.7753s/100 iter), loss = 1.1602
I0815 17:08:47.746448  8764 solver.cpp:334]     Train net output #0: loss = 1.34307 (* 1 = 1.34307 loss)
I0815 17:08:47.746464  8764 sgd_solver.cpp:136] Iteration 126800, lr = 0.002075, m = 0.9
I0815 17:09:06.390178  8764 solver.cpp:312] Iteration 126900 (5.36386 iter/s, 18.6433s/100 iter), loss = 1.20543
I0815 17:09:06.412174  8764 solver.cpp:334]     Train net output #0: loss = 1.10808 (* 1 = 1.10808 loss)
I0815 17:09:06.412200  8764 sgd_solver.cpp:136] Iteration 126900, lr = 0.00206875, m = 0.9
I0815 17:09:24.463171  8764 solver.cpp:363] Sparsity after update:
I0815 17:09:24.478569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:09:24.478587  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:09:24.478596  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:09:24.478600  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:09:24.478605  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:09:24.478624  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:09:24.478636  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:09:24.478644  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:09:24.478653  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:09:24.478672  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:09:24.478682  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:09:24.478691  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:09:24.478700  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:09:24.607796  8764 solver.cpp:312] Iteration 127000 (5.48934 iter/s, 18.2171s/100 iter), loss = 1.40548
I0815 17:09:24.607825  8764 solver.cpp:334]     Train net output #0: loss = 1.52936 (* 1 = 1.52936 loss)
I0815 17:09:24.607831  8764 sgd_solver.cpp:136] Iteration 127000, lr = 0.0020625, m = 0.9
I0815 17:09:43.816354  8764 solver.cpp:312] Iteration 127100 (5.20616 iter/s, 19.208s/100 iter), loss = 1.27825
I0815 17:09:43.816380  8764 solver.cpp:334]     Train net output #0: loss = 1.3444 (* 1 = 1.3444 loss)
I0815 17:09:43.816385  8764 sgd_solver.cpp:136] Iteration 127100, lr = 0.00205625, m = 0.9
I0815 17:10:01.339206  8764 solver.cpp:312] Iteration 127200 (5.70699 iter/s, 17.5224s/100 iter), loss = 1.1807
I0815 17:10:01.339248  8764 solver.cpp:334]     Train net output #0: loss = 1.16003 (* 1 = 1.16003 loss)
I0815 17:10:01.339254  8764 sgd_solver.cpp:136] Iteration 127200, lr = 0.00205, m = 0.9
I0815 17:10:17.944097  8764 solver.cpp:312] Iteration 127300 (6.02249 iter/s, 16.6044s/100 iter), loss = 1.32736
I0815 17:10:17.944149  8764 solver.cpp:334]     Train net output #0: loss = 1.30993 (* 1 = 1.30993 loss)
I0815 17:10:17.944159  8764 sgd_solver.cpp:136] Iteration 127300, lr = 0.00204375, m = 0.9
I0815 17:10:37.823766  8764 solver.cpp:312] Iteration 127400 (5.0304 iter/s, 19.8791s/100 iter), loss = 1.26724
I0815 17:10:37.823860  8764 solver.cpp:334]     Train net output #0: loss = 1.49237 (* 1 = 1.49237 loss)
I0815 17:10:37.823874  8764 sgd_solver.cpp:136] Iteration 127400, lr = 0.0020375, m = 0.9
I0815 17:10:54.629554  8764 solver.cpp:312] Iteration 127500 (5.95049 iter/s, 16.8053s/100 iter), loss = 1.43378
I0815 17:10:54.629582  8764 solver.cpp:334]     Train net output #0: loss = 1.42391 (* 1 = 1.42391 loss)
I0815 17:10:54.629588  8764 sgd_solver.cpp:136] Iteration 127500, lr = 0.00203125, m = 0.9
I0815 17:11:12.209220  8764 solver.cpp:312] Iteration 127600 (5.68855 iter/s, 17.5792s/100 iter), loss = 1.30301
I0815 17:11:12.209355  8764 solver.cpp:334]     Train net output #0: loss = 1.4197 (* 1 = 1.4197 loss)
I0815 17:11:12.209369  8764 sgd_solver.cpp:136] Iteration 127600, lr = 0.002025, m = 0.9
I0815 17:11:28.565254  8764 solver.cpp:312] Iteration 127700 (6.11412 iter/s, 16.3556s/100 iter), loss = 1.57528
I0815 17:11:28.565280  8764 solver.cpp:334]     Train net output #0: loss = 1.79272 (* 1 = 1.79272 loss)
I0815 17:11:28.565287  8764 sgd_solver.cpp:136] Iteration 127700, lr = 0.00201875, m = 0.9
I0815 17:11:47.973165  8764 solver.cpp:312] Iteration 127800 (5.15268 iter/s, 19.4074s/100 iter), loss = 1.55694
I0815 17:11:47.973453  8764 solver.cpp:334]     Train net output #0: loss = 1.39133 (* 1 = 1.39133 loss)
I0815 17:11:47.973574  8764 sgd_solver.cpp:136] Iteration 127800, lr = 0.0020125, m = 0.9
I0815 17:12:04.644223  8764 solver.cpp:312] Iteration 127900 (5.99858 iter/s, 16.6706s/100 iter), loss = 1.21328
I0815 17:12:04.644251  8764 solver.cpp:334]     Train net output #0: loss = 0.999575 (* 1 = 0.999575 loss)
I0815 17:12:04.644256  8764 sgd_solver.cpp:136] Iteration 127900, lr = 0.00200625, m = 0.9
I0815 17:12:23.299715  8764 solver.cpp:363] Sparsity after update:
I0815 17:12:23.311008  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:12:23.311084  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:12:23.311136  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:12:23.311151  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:12:23.311163  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:12:23.311173  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:12:23.311190  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:12:23.311198  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:12:23.311218  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:12:23.311234  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:12:23.311252  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:12:23.311277  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:12:23.311305  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:12:23.311368  8764 solver.cpp:509] Iteration 128000, Testing net (#0)
I0815 17:12:50.298908  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.584823
I0815 17:12:50.298938  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814291
I0815 17:12:50.298943  8764 solver.cpp:594]     Test net output #2: loss = 1.79734 (* 1 = 1.79734 loss)
I0815 17:12:50.298967  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.9869s
I0815 17:12:50.518247  8764 solver.cpp:312] Iteration 128000 (2.17994 iter/s, 45.8728s/100 iter), loss = 1.34882
I0815 17:12:50.518301  8764 solver.cpp:334]     Train net output #0: loss = 1.39404 (* 1 = 1.39404 loss)
I0815 17:12:50.518313  8764 sgd_solver.cpp:136] Iteration 128000, lr = 0.002, m = 0.9
I0815 17:13:07.989948  8764 solver.cpp:312] Iteration 128100 (5.7237 iter/s, 17.4712s/100 iter), loss = 1.02687
I0815 17:13:07.998904  8764 solver.cpp:334]     Train net output #0: loss = 1.11087 (* 1 = 1.11087 loss)
I0815 17:13:07.998916  8764 sgd_solver.cpp:136] Iteration 128100, lr = 0.00199375, m = 0.9
I0815 17:13:25.117156  8764 solver.cpp:312] Iteration 128200 (5.83883 iter/s, 17.1267s/100 iter), loss = 1.05284
I0815 17:13:25.117252  8764 solver.cpp:334]     Train net output #0: loss = 0.72964 (* 1 = 0.72964 loss)
I0815 17:13:25.117280  8764 sgd_solver.cpp:136] Iteration 128200, lr = 0.0019875, m = 0.9
I0815 17:13:40.631798  8764 solver.cpp:312] Iteration 128300 (6.4457 iter/s, 15.5142s/100 iter), loss = 1.23448
I0815 17:13:40.631863  8764 solver.cpp:334]     Train net output #0: loss = 1.04904 (* 1 = 1.04904 loss)
I0815 17:13:40.631870  8764 sgd_solver.cpp:136] Iteration 128300, lr = 0.00198125, m = 0.9
I0815 17:13:57.455849  8764 solver.cpp:312] Iteration 128400 (5.94404 iter/s, 16.8236s/100 iter), loss = 1.30775
I0815 17:13:57.455873  8764 solver.cpp:334]     Train net output #0: loss = 1.23768 (* 1 = 1.23768 loss)
I0815 17:13:57.455878  8764 sgd_solver.cpp:136] Iteration 128400, lr = 0.001975, m = 0.9
I0815 17:14:15.677525  8764 solver.cpp:312] Iteration 128500 (5.48812 iter/s, 18.2212s/100 iter), loss = 1.72007
I0815 17:14:15.677577  8764 solver.cpp:334]     Train net output #0: loss = 1.77705 (* 1 = 1.77705 loss)
I0815 17:14:15.677584  8764 sgd_solver.cpp:136] Iteration 128500, lr = 0.00196875, m = 0.9
I0815 17:14:38.199393  8764 solver.cpp:312] Iteration 128600 (4.44025 iter/s, 22.5212s/100 iter), loss = 1.21492
I0815 17:14:38.199425  8764 solver.cpp:334]     Train net output #0: loss = 1.01693 (* 1 = 1.01693 loss)
I0815 17:14:38.199434  8764 sgd_solver.cpp:136] Iteration 128600, lr = 0.0019625, m = 0.9
I0815 17:14:59.981477  8764 solver.cpp:312] Iteration 128700 (4.59106 iter/s, 21.7815s/100 iter), loss = 1.21398
I0815 17:14:59.981567  8764 solver.cpp:334]     Train net output #0: loss = 0.842298 (* 1 = 0.842298 loss)
I0815 17:14:59.981580  8764 sgd_solver.cpp:136] Iteration 128700, lr = 0.00195625, m = 0.9
I0815 17:15:20.903497  8764 solver.cpp:312] Iteration 128800 (4.77979 iter/s, 20.9214s/100 iter), loss = 1.35614
I0815 17:15:20.903543  8764 solver.cpp:334]     Train net output #0: loss = 1.4568 (* 1 = 1.4568 loss)
I0815 17:15:20.903554  8764 sgd_solver.cpp:136] Iteration 128800, lr = 0.00195, m = 0.9
I0815 17:15:44.544180  8764 solver.cpp:312] Iteration 128900 (4.23011 iter/s, 23.64s/100 iter), loss = 1.74407
I0815 17:15:44.544281  8764 solver.cpp:334]     Train net output #0: loss = 1.70053 (* 1 = 1.70053 loss)
I0815 17:15:44.544296  8764 sgd_solver.cpp:136] Iteration 128900, lr = 0.00194375, m = 0.9
I0815 17:16:02.387634  8764 solver.cpp:363] Sparsity after update:
I0815 17:16:02.398764  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:16:02.398782  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:16:02.398792  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:16:02.398794  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:16:02.398797  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:16:02.398803  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:16:02.398808  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:16:02.398811  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:16:02.398814  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:16:02.398818  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:16:02.398820  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:16:02.398823  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:16:02.398828  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:16:02.543727  8764 solver.cpp:312] Iteration 129000 (5.55585 iter/s, 17.9991s/100 iter), loss = 1.45426
I0815 17:16:02.543750  8764 solver.cpp:334]     Train net output #0: loss = 1.43579 (* 1 = 1.43579 loss)
I0815 17:16:02.543753  8764 sgd_solver.cpp:136] Iteration 129000, lr = 0.0019375, m = 0.9
I0815 17:16:21.953409  8764 solver.cpp:312] Iteration 129100 (5.15221 iter/s, 19.4091s/100 iter), loss = 1.73234
I0815 17:16:21.953506  8764 solver.cpp:334]     Train net output #0: loss = 1.63236 (* 1 = 1.63236 loss)
I0815 17:16:21.953526  8764 sgd_solver.cpp:136] Iteration 129100, lr = 0.00193125, m = 0.9
I0815 17:16:24.155280  8726 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 17:16:40.826834  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:16:46.138929  8764 solver.cpp:312] Iteration 129200 (4.13482 iter/s, 24.1848s/100 iter), loss = 1.42239
I0815 17:16:46.138981  8764 solver.cpp:334]     Train net output #0: loss = 1.12906 (* 1 = 1.12906 loss)
I0815 17:16:46.138993  8764 sgd_solver.cpp:136] Iteration 129200, lr = 0.001925, m = 0.9
I0815 17:17:04.383561  8764 solver.cpp:312] Iteration 129300 (5.48122 iter/s, 18.2441s/100 iter), loss = 1.0938
I0815 17:17:04.383759  8764 solver.cpp:334]     Train net output #0: loss = 1.34953 (* 1 = 1.34953 loss)
I0815 17:17:04.383783  8764 sgd_solver.cpp:136] Iteration 129300, lr = 0.00191875, m = 0.9
I0815 17:17:23.883965  8764 solver.cpp:312] Iteration 129400 (5.12824 iter/s, 19.4999s/100 iter), loss = 1.35504
I0815 17:17:23.884012  8764 solver.cpp:334]     Train net output #0: loss = 1.19291 (* 1 = 1.19291 loss)
I0815 17:17:23.884021  8764 sgd_solver.cpp:136] Iteration 129400, lr = 0.0019125, m = 0.9
I0815 17:17:43.936698  8764 solver.cpp:312] Iteration 129500 (4.98699 iter/s, 20.0522s/100 iter), loss = 1.20573
I0815 17:17:43.940174  8764 solver.cpp:334]     Train net output #0: loss = 1.43859 (* 1 = 1.43859 loss)
I0815 17:17:43.940191  8764 sgd_solver.cpp:136] Iteration 129500, lr = 0.00190625, m = 0.9
I0815 17:18:09.395064  8764 solver.cpp:312] Iteration 129600 (3.92809 iter/s, 25.4577s/100 iter), loss = 1.83191
I0815 17:18:09.395138  8764 solver.cpp:334]     Train net output #0: loss = 2.27386 (* 1 = 2.27386 loss)
I0815 17:18:09.395160  8764 sgd_solver.cpp:136] Iteration 129600, lr = 0.0019, m = 0.9
I0815 17:18:28.652477  8764 solver.cpp:312] Iteration 129700 (5.19295 iter/s, 19.2569s/100 iter), loss = 0.960912
I0815 17:18:28.652573  8764 solver.cpp:334]     Train net output #0: loss = 1.26243 (* 1 = 1.26243 loss)
I0815 17:18:28.652580  8764 sgd_solver.cpp:136] Iteration 129700, lr = 0.00189375, m = 0.9
I0815 17:18:48.800379  8764 solver.cpp:312] Iteration 129800 (4.96343 iter/s, 20.1473s/100 iter), loss = 1.82519
I0815 17:18:48.800410  8764 solver.cpp:334]     Train net output #0: loss = 1.84679 (* 1 = 1.84679 loss)
I0815 17:18:48.800415  8764 sgd_solver.cpp:136] Iteration 129800, lr = 0.0018875, m = 0.9
I0815 17:19:08.588856  8764 solver.cpp:312] Iteration 129900 (5.05359 iter/s, 19.7879s/100 iter), loss = 1.56422
I0815 17:19:08.588949  8764 solver.cpp:334]     Train net output #0: loss = 1.3593 (* 1 = 1.3593 loss)
I0815 17:19:08.588961  8764 sgd_solver.cpp:136] Iteration 129900, lr = 0.00188125, m = 0.9
I0815 17:19:29.908202  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_130000.caffemodel
I0815 17:19:30.005930  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_130000.solverstate
I0815 17:19:30.012125  8764 solver.cpp:363] Sparsity after update:
I0815 17:19:30.013435  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:19:30.013447  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:19:30.013454  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:19:30.013458  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:19:30.013460  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:19:30.013464  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:19:30.013468  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:19:30.013470  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:19:30.013473  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:19:30.013476  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:19:30.013480  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:19:30.013484  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:19:30.013486  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:19:30.013509  8764 solver.cpp:509] Iteration 130000, Testing net (#0)
I0815 17:20:02.439919  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.589412
I0815 17:20:02.440052  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.818996
I0815 17:20:02.440099  8764 solver.cpp:594]     Test net output #2: loss = 1.77976 (* 1 = 1.77976 loss)
I0815 17:20:02.440176  8764 solver.cpp:264] [MultiGPU] Tests completed in 32.4258s
I0815 17:20:02.764709  8764 solver.cpp:312] Iteration 130000 (1.84589 iter/s, 54.1743s/100 iter), loss = 1.27043
I0815 17:20:02.764789  8764 solver.cpp:334]     Train net output #0: loss = 1.63499 (* 1 = 1.63499 loss)
I0815 17:20:02.764816  8764 sgd_solver.cpp:136] Iteration 130000, lr = 0.001875, m = 0.9
I0815 17:20:22.427492  8764 solver.cpp:312] Iteration 130100 (5.08589 iter/s, 19.6622s/100 iter), loss = 1.49812
I0815 17:20:22.427541  8764 solver.cpp:334]     Train net output #0: loss = 1.54984 (* 1 = 1.54984 loss)
I0815 17:20:22.427551  8764 sgd_solver.cpp:136] Iteration 130100, lr = 0.00186875, m = 0.9
I0815 17:20:41.068940  8764 solver.cpp:312] Iteration 130200 (5.36454 iter/s, 18.6409s/100 iter), loss = 1.53985
I0815 17:20:41.069016  8764 solver.cpp:334]     Train net output #0: loss = 1.45333 (* 1 = 1.45333 loss)
I0815 17:20:41.069023  8764 sgd_solver.cpp:136] Iteration 130200, lr = 0.0018625, m = 0.9
I0815 17:20:59.703496  8764 solver.cpp:312] Iteration 130300 (5.36653 iter/s, 18.634s/100 iter), loss = 1.3469
I0815 17:20:59.703634  8764 solver.cpp:334]     Train net output #0: loss = 1.25095 (* 1 = 1.25095 loss)
I0815 17:20:59.703668  8764 sgd_solver.cpp:136] Iteration 130300, lr = 0.00185625, m = 0.9
I0815 17:21:21.067569  8764 solver.cpp:312] Iteration 130400 (4.68088 iter/s, 21.3635s/100 iter), loss = 1.05584
I0815 17:21:21.067631  8764 solver.cpp:334]     Train net output #0: loss = 1.18852 (* 1 = 1.18852 loss)
I0815 17:21:21.067637  8764 sgd_solver.cpp:136] Iteration 130400, lr = 0.00185, m = 0.9
I0815 17:21:40.853276  8764 solver.cpp:312] Iteration 130500 (5.05429 iter/s, 19.7852s/100 iter), loss = 1.29286
I0815 17:21:40.853340  8764 solver.cpp:334]     Train net output #0: loss = 1.53981 (* 1 = 1.53981 loss)
I0815 17:21:40.853358  8764 sgd_solver.cpp:136] Iteration 130500, lr = 0.00184375, m = 0.9
I0815 17:21:57.243203  8764 solver.cpp:312] Iteration 130600 (6.10148 iter/s, 16.3895s/100 iter), loss = 1.99654
I0815 17:21:57.245162  8764 solver.cpp:334]     Train net output #0: loss = 2.66144 (* 1 = 2.66144 loss)
I0815 17:21:57.245187  8764 sgd_solver.cpp:136] Iteration 130600, lr = 0.0018375, m = 0.9
I0815 17:22:14.241308  8764 solver.cpp:312] Iteration 130700 (5.88317 iter/s, 16.9976s/100 iter), loss = 1.25891
I0815 17:22:14.241344  8764 solver.cpp:334]     Train net output #0: loss = 1.40489 (* 1 = 1.40489 loss)
I0815 17:22:14.241351  8764 sgd_solver.cpp:136] Iteration 130700, lr = 0.00183125, m = 0.9
I0815 17:22:32.313590  8764 solver.cpp:312] Iteration 130800 (5.53349 iter/s, 18.0718s/100 iter), loss = 1.27486
I0815 17:22:32.313695  8764 solver.cpp:334]     Train net output #0: loss = 1.51742 (* 1 = 1.51742 loss)
I0815 17:22:32.313704  8764 sgd_solver.cpp:136] Iteration 130800, lr = 0.001825, m = 0.9
I0815 17:22:52.209511  8764 solver.cpp:312] Iteration 130900 (5.02629 iter/s, 19.8954s/100 iter), loss = 1.17972
I0815 17:22:52.209533  8764 solver.cpp:334]     Train net output #0: loss = 1.23541 (* 1 = 1.23541 loss)
I0815 17:22:52.209538  8764 sgd_solver.cpp:136] Iteration 130900, lr = 0.00181875, m = 0.9
I0815 17:23:09.027231  8764 solver.cpp:363] Sparsity after update:
I0815 17:23:09.038789  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:23:09.038805  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:23:09.038813  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:23:09.038816  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:23:09.038822  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:23:09.038826  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:23:09.038830  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:23:09.038832  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:23:09.038836  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:23:09.038839  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:23:09.038842  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:23:09.038846  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:23:09.038848  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:23:09.179023  8764 solver.cpp:312] Iteration 131000 (5.89308 iter/s, 16.969s/100 iter), loss = 1.86589
I0815 17:23:09.179076  8764 solver.cpp:334]     Train net output #0: loss = 2.10535 (* 1 = 2.10535 loss)
I0815 17:23:09.179090  8764 sgd_solver.cpp:136] Iteration 131000, lr = 0.0018125, m = 0.9
I0815 17:23:25.728157  8764 solver.cpp:312] Iteration 131100 (6.04278 iter/s, 16.5487s/100 iter), loss = 1.52704
I0815 17:23:25.728207  8764 solver.cpp:334]     Train net output #0: loss = 1.84386 (* 1 = 1.84386 loss)
I0815 17:23:25.728219  8764 sgd_solver.cpp:136] Iteration 131100, lr = 0.00180625, m = 0.9
I0815 17:23:45.408041  8764 solver.cpp:312] Iteration 131200 (5.08147 iter/s, 19.6793s/100 iter), loss = 1.34465
I0815 17:23:45.408107  8764 solver.cpp:334]     Train net output #0: loss = 1.58709 (* 1 = 1.58709 loss)
I0815 17:23:45.408113  8764 sgd_solver.cpp:136] Iteration 131200, lr = 0.0018, m = 0.9
I0815 17:24:07.257922  8764 solver.cpp:312] Iteration 131300 (4.57681 iter/s, 21.8493s/100 iter), loss = 1.56876
I0815 17:24:07.257997  8764 solver.cpp:334]     Train net output #0: loss = 1.55274 (* 1 = 1.55274 loss)
I0815 17:24:07.258016  8764 sgd_solver.cpp:136] Iteration 131300, lr = 0.00179375, m = 0.9
I0815 17:24:25.645272  8764 solver.cpp:312] Iteration 131400 (5.43867 iter/s, 18.3868s/100 iter), loss = 1.31606
I0815 17:24:25.645355  8764 solver.cpp:334]     Train net output #0: loss = 1.48385 (* 1 = 1.48385 loss)
I0815 17:24:25.645375  8764 sgd_solver.cpp:136] Iteration 131400, lr = 0.0017875, m = 0.9
I0815 17:24:44.396997  8764 solver.cpp:312] Iteration 131500 (5.33299 iter/s, 18.7512s/100 iter), loss = 1.93663
I0815 17:24:44.397027  8764 solver.cpp:334]     Train net output #0: loss = 1.8183 (* 1 = 1.8183 loss)
I0815 17:24:44.397032  8764 sgd_solver.cpp:136] Iteration 131500, lr = 0.00178125, m = 0.9
I0815 17:25:04.074728  8764 solver.cpp:312] Iteration 131600 (5.08203 iter/s, 19.6772s/100 iter), loss = 0.959314
I0815 17:25:04.074780  8764 solver.cpp:334]     Train net output #0: loss = 0.76447 (* 1 = 0.76447 loss)
I0815 17:25:04.074785  8764 sgd_solver.cpp:136] Iteration 131600, lr = 0.001775, m = 0.9
I0815 17:25:25.822871  8764 solver.cpp:312] Iteration 131700 (4.59822 iter/s, 21.7475s/100 iter), loss = 1.33738
I0815 17:25:25.822939  8764 solver.cpp:334]     Train net output #0: loss = 1.64302 (* 1 = 1.64302 loss)
I0815 17:25:25.822955  8764 sgd_solver.cpp:136] Iteration 131700, lr = 0.00176875, m = 0.9
I0815 17:25:42.141067  8764 solver.cpp:312] Iteration 131800 (6.1283 iter/s, 16.3177s/100 iter), loss = 1.41934
I0815 17:25:42.141149  8764 solver.cpp:334]     Train net output #0: loss = 1.55354 (* 1 = 1.55354 loss)
I0815 17:25:42.141171  8764 sgd_solver.cpp:136] Iteration 131800, lr = 0.0017625, m = 0.9
I0815 17:26:01.213870  8764 solver.cpp:312] Iteration 131900 (5.24321 iter/s, 19.0723s/100 iter), loss = 1.47877
I0815 17:26:01.213945  8764 solver.cpp:334]     Train net output #0: loss = 1.79184 (* 1 = 1.79184 loss)
I0815 17:26:01.213965  8764 sgd_solver.cpp:136] Iteration 131900, lr = 0.00175625, m = 0.9
I0815 17:26:19.057579  8764 solver.cpp:363] Sparsity after update:
I0815 17:26:19.061997  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:26:19.062036  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:26:19.062068  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:26:19.062093  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:26:19.062116  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:26:19.062140  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:26:19.062161  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:26:19.062180  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:26:19.062201  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:26:19.062218  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:26:19.062237  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:26:19.062255  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:26:19.062273  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:26:19.062314  8764 solver.cpp:509] Iteration 132000, Testing net (#0)
I0815 17:26:41.878682  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:26:44.570834  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.593117
I0815 17:26:44.570858  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.820115
I0815 17:26:44.570863  8764 solver.cpp:594]     Test net output #2: loss = 1.77681 (* 1 = 1.77681 loss)
I0815 17:26:44.570878  8764 solver.cpp:264] [MultiGPU] Tests completed in 25.5079s
I0815 17:26:44.743706  8764 solver.cpp:312] Iteration 132000 (2.29734 iter/s, 43.5286s/100 iter), loss = 1.39118
I0815 17:26:44.743734  8764 solver.cpp:334]     Train net output #0: loss = 1.32367 (* 1 = 1.32367 loss)
I0815 17:26:44.743741  8764 sgd_solver.cpp:136] Iteration 132000, lr = 0.00175, m = 0.9
I0815 17:27:06.078294  8764 solver.cpp:312] Iteration 132100 (4.68735 iter/s, 21.334s/100 iter), loss = 1.74822
I0815 17:27:06.078383  8764 solver.cpp:334]     Train net output #0: loss = 1.44198 (* 1 = 1.44198 loss)
I0815 17:27:06.078392  8764 sgd_solver.cpp:136] Iteration 132100, lr = 0.00174375, m = 0.9
I0815 17:27:25.558334  8764 solver.cpp:312] Iteration 132200 (5.1336 iter/s, 19.4795s/100 iter), loss = 1.17014
I0815 17:27:25.558362  8764 solver.cpp:334]     Train net output #0: loss = 0.899074 (* 1 = 0.899074 loss)
I0815 17:27:25.558368  8764 sgd_solver.cpp:136] Iteration 132200, lr = 0.0017375, m = 0.9
I0815 17:27:43.469065  8764 solver.cpp:312] Iteration 132300 (5.5834 iter/s, 17.9102s/100 iter), loss = 1.56735
I0815 17:27:43.469123  8764 solver.cpp:334]     Train net output #0: loss = 1.94571 (* 1 = 1.94571 loss)
I0815 17:27:43.469127  8764 sgd_solver.cpp:136] Iteration 132300, lr = 0.00173125, m = 0.9
I0815 17:28:00.434290  8764 solver.cpp:312] Iteration 132400 (5.89457 iter/s, 16.9648s/100 iter), loss = 1.32808
I0815 17:28:00.434319  8764 solver.cpp:334]     Train net output #0: loss = 1.36799 (* 1 = 1.36799 loss)
I0815 17:28:00.434325  8764 sgd_solver.cpp:136] Iteration 132400, lr = 0.001725, m = 0.9
I0815 17:28:18.370705  8764 solver.cpp:312] Iteration 132500 (5.5754 iter/s, 17.9359s/100 iter), loss = 1.41387
I0815 17:28:18.370757  8764 solver.cpp:334]     Train net output #0: loss = 1.30757 (* 1 = 1.30757 loss)
I0815 17:28:18.370764  8764 sgd_solver.cpp:136] Iteration 132500, lr = 0.00171875, m = 0.9
I0815 17:28:36.163020  8764 solver.cpp:312] Iteration 132600 (5.62056 iter/s, 17.7918s/100 iter), loss = 1.23818
I0815 17:28:36.163045  8764 solver.cpp:334]     Train net output #0: loss = 1.20743 (* 1 = 1.20743 loss)
I0815 17:28:36.163051  8764 sgd_solver.cpp:136] Iteration 132600, lr = 0.0017125, m = 0.9
I0815 17:28:54.526492  8764 solver.cpp:312] Iteration 132700 (5.44574 iter/s, 18.363s/100 iter), loss = 1.34947
I0815 17:28:54.526553  8764 solver.cpp:334]     Train net output #0: loss = 1.44059 (* 1 = 1.44059 loss)
I0815 17:28:54.526561  8764 sgd_solver.cpp:136] Iteration 132700, lr = 0.00170625, m = 0.9
I0815 17:29:11.706342  8764 solver.cpp:312] Iteration 132800 (5.82093 iter/s, 17.1794s/100 iter), loss = 1.16655
I0815 17:29:11.706406  8764 solver.cpp:334]     Train net output #0: loss = 0.98528 (* 1 = 0.98528 loss)
I0815 17:29:11.706423  8764 sgd_solver.cpp:136] Iteration 132800, lr = 0.0017, m = 0.9
I0815 17:29:30.256767  8764 solver.cpp:312] Iteration 132900 (5.39086 iter/s, 18.5499s/100 iter), loss = 1.48015
I0815 17:29:30.256880  8764 solver.cpp:334]     Train net output #0: loss = 1.34148 (* 1 = 1.34148 loss)
I0815 17:29:30.256903  8764 sgd_solver.cpp:136] Iteration 132900, lr = 0.00169375, m = 0.9
I0815 17:29:47.959995  8764 solver.cpp:363] Sparsity after update:
I0815 17:29:47.968354  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:29:47.968399  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:29:47.968436  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:29:47.968464  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:29:47.968488  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:29:47.968513  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:29:47.968536  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:29:47.968566  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:29:47.968593  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:29:47.968618  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:29:47.968642  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:29:47.968667  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:29:47.968693  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:29:48.290923  8764 solver.cpp:312] Iteration 133000 (5.54519 iter/s, 18.0337s/100 iter), loss = 1.10578
I0815 17:29:48.290959  8764 solver.cpp:334]     Train net output #0: loss = 1.06993 (* 1 = 1.06993 loss)
I0815 17:29:48.290966  8764 sgd_solver.cpp:136] Iteration 133000, lr = 0.0016875, m = 0.9
I0815 17:30:05.662282  8764 solver.cpp:312] Iteration 133100 (5.75676 iter/s, 17.3709s/100 iter), loss = 1.42551
I0815 17:30:05.662523  8764 solver.cpp:334]     Train net output #0: loss = 1.8064 (* 1 = 1.8064 loss)
I0815 17:30:05.662541  8764 sgd_solver.cpp:136] Iteration 133100, lr = 0.00168125, m = 0.9
I0815 17:30:22.838812  8764 solver.cpp:312] Iteration 133200 (5.82206 iter/s, 17.1761s/100 iter), loss = 1.12785
I0815 17:30:22.838851  8764 solver.cpp:334]     Train net output #0: loss = 0.991353 (* 1 = 0.991353 loss)
I0815 17:30:22.839015  8764 sgd_solver.cpp:136] Iteration 133200, lr = 0.001675, m = 0.9
I0815 17:30:41.658640  8764 solver.cpp:312] Iteration 133300 (5.31369 iter/s, 18.8193s/100 iter), loss = 1.60237
I0815 17:30:41.658704  8764 solver.cpp:334]     Train net output #0: loss = 1.7237 (* 1 = 1.7237 loss)
I0815 17:30:41.658711  8764 sgd_solver.cpp:136] Iteration 133300, lr = 0.00166875, m = 0.9
I0815 17:31:00.340356  8764 solver.cpp:312] Iteration 133400 (5.35297 iter/s, 18.6812s/100 iter), loss = 1.14864
I0815 17:31:00.340538  8764 solver.cpp:334]     Train net output #0: loss = 1.165 (* 1 = 1.165 loss)
I0815 17:31:00.340617  8764 sgd_solver.cpp:136] Iteration 133400, lr = 0.0016625, m = 0.9
I0815 17:31:17.644100  8764 solver.cpp:312] Iteration 133500 (5.77925 iter/s, 17.3033s/100 iter), loss = 1.17668
I0815 17:31:17.644325  8764 solver.cpp:334]     Train net output #0: loss = 1.07934 (* 1 = 1.07934 loss)
I0815 17:31:17.644413  8764 sgd_solver.cpp:136] Iteration 133500, lr = 0.00165625, m = 0.9
I0815 17:31:35.185799  8764 solver.cpp:312] Iteration 133600 (5.70086 iter/s, 17.5412s/100 iter), loss = 1.52824
I0815 17:31:35.185849  8764 solver.cpp:334]     Train net output #0: loss = 1.68763 (* 1 = 1.68763 loss)
I0815 17:31:35.185860  8764 sgd_solver.cpp:136] Iteration 133600, lr = 0.00165, m = 0.9
I0815 17:31:57.120683  8764 solver.cpp:312] Iteration 133700 (4.55907 iter/s, 21.9343s/100 iter), loss = 1.38269
I0815 17:31:57.120774  8764 solver.cpp:334]     Train net output #0: loss = 0.985292 (* 1 = 0.985292 loss)
I0815 17:31:57.120793  8764 sgd_solver.cpp:136] Iteration 133700, lr = 0.00164375, m = 0.9
I0815 17:32:19.377300  8764 solver.cpp:312] Iteration 133800 (4.49317 iter/s, 22.256s/100 iter), loss = 0.981417
I0815 17:32:19.377373  8764 solver.cpp:334]     Train net output #0: loss = 0.885605 (* 1 = 0.885605 loss)
I0815 17:32:19.377394  8764 sgd_solver.cpp:136] Iteration 133800, lr = 0.0016375, m = 0.9
I0815 17:32:41.567522  8764 solver.cpp:312] Iteration 133900 (4.50661 iter/s, 22.1896s/100 iter), loss = 1.32559
I0815 17:32:41.567605  8764 solver.cpp:334]     Train net output #0: loss = 1.70436 (* 1 = 1.70436 loss)
I0815 17:32:41.567617  8764 sgd_solver.cpp:136] Iteration 133900, lr = 0.00163125, m = 0.9
I0815 17:33:02.465883  8764 solver.cpp:363] Sparsity after update:
I0815 17:33:02.471698  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:33:02.471706  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:33:02.471714  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:33:02.471719  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:33:02.471724  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:33:02.471726  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:33:02.471729  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:33:02.471734  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:33:02.471736  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:33:02.471740  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:33:02.471745  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:33:02.471747  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:33:02.471751  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:33:02.471765  8764 solver.cpp:509] Iteration 134000, Testing net (#0)
I0815 17:33:37.252636  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588411
I0815 17:33:37.252707  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.814291
I0815 17:33:37.252712  8764 solver.cpp:594]     Test net output #2: loss = 1.79674 (* 1 = 1.79674 loss)
I0815 17:33:37.252732  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.78s
I0815 17:33:37.418210  8764 solver.cpp:312] Iteration 134000 (1.79054 iter/s, 55.8492s/100 iter), loss = 1.26797
I0815 17:33:37.418395  8764 solver.cpp:334]     Train net output #0: loss = 1.13422 (* 1 = 1.13422 loss)
I0815 17:33:37.418483  8764 sgd_solver.cpp:136] Iteration 134000, lr = 0.001625, m = 0.9
I0815 17:33:55.536168  8764 solver.cpp:312] Iteration 134100 (5.51954 iter/s, 18.1174s/100 iter), loss = 1.5052
I0815 17:33:55.536203  8764 solver.cpp:334]     Train net output #0: loss = 1.64976 (* 1 = 1.64976 loss)
I0815 17:33:55.536209  8764 sgd_solver.cpp:136] Iteration 134100, lr = 0.00161875, m = 0.9
I0815 17:34:17.248491  8764 solver.cpp:312] Iteration 134200 (4.60581 iter/s, 21.7117s/100 iter), loss = 1.23115
I0815 17:34:17.256224  8764 solver.cpp:334]     Train net output #0: loss = 1.54866 (* 1 = 1.54866 loss)
I0815 17:34:17.256263  8764 sgd_solver.cpp:136] Iteration 134200, lr = 0.0016125, m = 0.9
I0815 17:34:39.705312  8764 solver.cpp:312] Iteration 134300 (4.45311 iter/s, 22.4562s/100 iter), loss = 1.38126
I0815 17:34:39.705359  8764 solver.cpp:334]     Train net output #0: loss = 1.30305 (* 1 = 1.30305 loss)
I0815 17:34:39.705369  8764 sgd_solver.cpp:136] Iteration 134300, lr = 0.00160625, m = 0.9
I0815 17:34:58.465956  8764 solver.cpp:312] Iteration 134400 (5.33045 iter/s, 18.7601s/100 iter), loss = 1.47109
I0815 17:34:58.466018  8764 solver.cpp:334]     Train net output #0: loss = 1.54395 (* 1 = 1.54395 loss)
I0815 17:34:58.466025  8764 sgd_solver.cpp:136] Iteration 134400, lr = 0.0016, m = 0.9
I0815 17:35:23.224927  8764 solver.cpp:312] Iteration 134500 (4.03905 iter/s, 24.7583s/100 iter), loss = 1.36413
I0815 17:35:23.224998  8764 solver.cpp:334]     Train net output #0: loss = 1.49748 (* 1 = 1.49748 loss)
I0815 17:35:23.225016  8764 sgd_solver.cpp:136] Iteration 134500, lr = 0.00159375, m = 0.9
I0815 17:35:43.034340  8764 solver.cpp:312] Iteration 134600 (5.04825 iter/s, 19.8089s/100 iter), loss = 1.569
I0815 17:35:43.034451  8764 solver.cpp:334]     Train net output #0: loss = 1.17108 (* 1 = 1.17108 loss)
I0815 17:35:43.034466  8764 sgd_solver.cpp:136] Iteration 134600, lr = 0.0015875, m = 0.9
I0815 17:36:01.234887  8764 solver.cpp:312] Iteration 134700 (5.49449 iter/s, 18.2s/100 iter), loss = 1.5279
I0815 17:36:01.234915  8764 solver.cpp:334]     Train net output #0: loss = 1.5855 (* 1 = 1.5855 loss)
I0815 17:36:01.234920  8764 sgd_solver.cpp:136] Iteration 134700, lr = 0.00158125, m = 0.9
I0815 17:36:20.340768  8764 solver.cpp:312] Iteration 134800 (5.23414 iter/s, 19.1054s/100 iter), loss = 1.11819
I0815 17:36:20.342906  8764 solver.cpp:334]     Train net output #0: loss = 1.23668 (* 1 = 1.23668 loss)
I0815 17:36:20.342917  8764 sgd_solver.cpp:136] Iteration 134800, lr = 0.001575, m = 0.9
I0815 17:36:40.411533  8764 solver.cpp:312] Iteration 134900 (4.98252 iter/s, 20.0702s/100 iter), loss = 1.14739
I0815 17:36:40.411630  8764 solver.cpp:334]     Train net output #0: loss = 1.10965 (* 1 = 1.10965 loss)
I0815 17:36:40.411669  8764 sgd_solver.cpp:136] Iteration 134900, lr = 0.00156875, m = 0.9
I0815 17:37:02.490578  8764 solver.cpp:363] Sparsity after update:
I0815 17:37:02.501132  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:37:02.501165  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:37:02.501184  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:37:02.501195  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:37:02.501205  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:37:02.501215  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:37:02.501225  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:37:02.501235  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:37:02.501245  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:37:02.501255  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:37:02.501265  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:37:02.501274  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:37:02.501284  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:37:02.722560  8764 solver.cpp:312] Iteration 135000 (4.48221 iter/s, 22.3104s/100 iter), loss = 1.76965
I0815 17:37:02.722590  8764 solver.cpp:334]     Train net output #0: loss = 2.32844 (* 1 = 2.32844 loss)
I0815 17:37:02.722597  8764 sgd_solver.cpp:136] Iteration 135000, lr = 0.0015625, m = 0.9
I0815 17:37:20.986145  8764 solver.cpp:312] Iteration 135100 (5.47553 iter/s, 18.2631s/100 iter), loss = 1.95622
I0815 17:37:20.986172  8764 solver.cpp:334]     Train net output #0: loss = 1.98313 (* 1 = 1.98313 loss)
I0815 17:37:20.986178  8764 sgd_solver.cpp:136] Iteration 135100, lr = 0.00155625, m = 0.9
I0815 17:37:40.394294  8764 solver.cpp:312] Iteration 135200 (5.15262 iter/s, 19.4076s/100 iter), loss = 1.83368
I0815 17:37:40.394356  8764 solver.cpp:334]     Train net output #0: loss = 2.08937 (* 1 = 2.08937 loss)
I0815 17:37:40.394363  8764 sgd_solver.cpp:136] Iteration 135200, lr = 0.00155, m = 0.9
I0815 17:37:59.102579  8764 solver.cpp:312] Iteration 135300 (5.34538 iter/s, 18.7078s/100 iter), loss = 1.31902
I0815 17:37:59.102675  8764 solver.cpp:334]     Train net output #0: loss = 1.35356 (* 1 = 1.35356 loss)
I0815 17:37:59.102701  8764 sgd_solver.cpp:136] Iteration 135300, lr = 0.00154375, m = 0.9
I0815 17:38:21.569319  8764 solver.cpp:312] Iteration 135400 (4.45115 iter/s, 22.4661s/100 iter), loss = 1.30169
I0815 17:38:21.569412  8764 solver.cpp:334]     Train net output #0: loss = 1.5134 (* 1 = 1.5134 loss)
I0815 17:38:21.569420  8764 sgd_solver.cpp:136] Iteration 135400, lr = 0.0015375, m = 0.9
I0815 17:38:41.524209  8764 solver.cpp:312] Iteration 135500 (5.01144 iter/s, 19.9543s/100 iter), loss = 1.32333
I0815 17:38:41.524232  8764 solver.cpp:334]     Train net output #0: loss = 1.12029 (* 1 = 1.12029 loss)
I0815 17:38:41.524240  8764 sgd_solver.cpp:136] Iteration 135500, lr = 0.00153125, m = 0.9
I0815 17:39:02.407090  8764 solver.cpp:312] Iteration 135600 (4.78874 iter/s, 20.8823s/100 iter), loss = 1.54498
I0815 17:39:02.407166  8764 solver.cpp:334]     Train net output #0: loss = 1.37691 (* 1 = 1.37691 loss)
I0815 17:39:02.407174  8764 sgd_solver.cpp:136] Iteration 135600, lr = 0.001525, m = 0.9
I0815 17:39:19.986604  8764 solver.cpp:312] Iteration 135700 (5.6886 iter/s, 17.579s/100 iter), loss = 1.50859
I0815 17:39:19.986630  8764 solver.cpp:334]     Train net output #0: loss = 1.81547 (* 1 = 1.81547 loss)
I0815 17:39:19.986635  8764 sgd_solver.cpp:136] Iteration 135700, lr = 0.00151875, m = 0.9
I0815 17:39:37.262624  8764 solver.cpp:312] Iteration 135800 (5.78853 iter/s, 17.2755s/100 iter), loss = 1.59621
I0815 17:39:37.263718  8764 solver.cpp:334]     Train net output #0: loss = 1.82919 (* 1 = 1.82919 loss)
I0815 17:39:37.263736  8764 sgd_solver.cpp:136] Iteration 135800, lr = 0.0015125, m = 0.9
I0815 17:39:55.039541  8764 solver.cpp:312] Iteration 135900 (5.62544 iter/s, 17.7764s/100 iter), loss = 0.91459
I0815 17:39:55.039619  8764 solver.cpp:334]     Train net output #0: loss = 1.07175 (* 1 = 1.07175 loss)
I0815 17:39:55.039641  8764 sgd_solver.cpp:136] Iteration 135900, lr = 0.00150625, m = 0.9
I0815 17:40:13.356025  8764 solver.cpp:363] Sparsity after update:
I0815 17:40:13.361065  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:40:13.361078  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:40:13.361086  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:40:13.361090  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:40:13.361094  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:40:13.361099  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:40:13.361104  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:40:13.361107  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:40:13.361111  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:40:13.361115  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:40:13.361119  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:40:13.361124  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:40:13.361129  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:40:13.361140  8764 solver.cpp:509] Iteration 136000, Testing net (#0)
I0815 17:40:20.952467  8765 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:40:45.192246  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.59253
I0815 17:40:45.192304  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.823291
I0815 17:40:45.192311  8764 solver.cpp:594]     Test net output #2: loss = 1.76064 (* 1 = 1.76064 loss)
I0815 17:40:45.192332  8764 solver.cpp:264] [MultiGPU] Tests completed in 31.8303s
I0815 17:40:45.353452  8764 solver.cpp:312] Iteration 136000 (1.98758 iter/s, 50.3125s/100 iter), loss = 1.14895
I0815 17:40:45.353524  8764 solver.cpp:334]     Train net output #0: loss = 0.881727 (* 1 = 0.881727 loss)
I0815 17:40:45.353544  8764 sgd_solver.cpp:136] Iteration 136000, lr = 0.0015, m = 0.9
I0815 17:41:03.600164  8764 solver.cpp:312] Iteration 136100 (5.48059 iter/s, 18.2462s/100 iter), loss = 1.44576
I0815 17:41:03.600234  8764 solver.cpp:334]     Train net output #0: loss = 1.17001 (* 1 = 1.17001 loss)
I0815 17:41:03.600251  8764 sgd_solver.cpp:136] Iteration 136100, lr = 0.00149375, m = 0.9
I0815 17:41:23.109555  8764 solver.cpp:312] Iteration 136200 (5.12588 iter/s, 19.5089s/100 iter), loss = 1.11632
I0815 17:41:23.109630  8764 solver.cpp:334]     Train net output #0: loss = 1.11082 (* 1 = 1.11082 loss)
I0815 17:41:23.109644  8764 sgd_solver.cpp:136] Iteration 136200, lr = 0.0014875, m = 0.9
I0815 17:41:43.188300  8764 solver.cpp:312] Iteration 136300 (4.98053 iter/s, 20.0782s/100 iter), loss = 1.29399
I0815 17:41:43.188343  8764 solver.cpp:334]     Train net output #0: loss = 1.21433 (* 1 = 1.21433 loss)
I0815 17:41:43.188351  8764 sgd_solver.cpp:136] Iteration 136300, lr = 0.00148125, m = 0.9
I0815 17:42:01.328286  8764 solver.cpp:312] Iteration 136400 (5.51283 iter/s, 18.1395s/100 iter), loss = 1.2211
I0815 17:42:01.328364  8764 solver.cpp:334]     Train net output #0: loss = 0.924369 (* 1 = 0.924369 loss)
I0815 17:42:01.328380  8764 sgd_solver.cpp:136] Iteration 136400, lr = 0.001475, m = 0.9
I0815 17:42:17.747169  8764 solver.cpp:312] Iteration 136500 (6.09072 iter/s, 16.4184s/100 iter), loss = 1.32803
I0815 17:42:17.747220  8764 solver.cpp:334]     Train net output #0: loss = 1.32719 (* 1 = 1.32719 loss)
I0815 17:42:17.747233  8764 sgd_solver.cpp:136] Iteration 136500, lr = 0.00146875, m = 0.9
I0815 17:42:35.534116  8764 solver.cpp:312] Iteration 136600 (5.62225 iter/s, 17.7865s/100 iter), loss = 1.01832
I0815 17:42:35.534184  8764 solver.cpp:334]     Train net output #0: loss = 1.08146 (* 1 = 1.08146 loss)
I0815 17:42:35.534190  8764 sgd_solver.cpp:136] Iteration 136600, lr = 0.0014625, m = 0.9
I0815 17:42:54.665385  8764 solver.cpp:312] Iteration 136700 (5.22719 iter/s, 19.1307s/100 iter), loss = 1.18513
I0815 17:42:54.665410  8764 solver.cpp:334]     Train net output #0: loss = 1.51201 (* 1 = 1.51201 loss)
I0815 17:42:54.665415  8764 sgd_solver.cpp:136] Iteration 136700, lr = 0.00145625, m = 0.9
I0815 17:43:11.602516  8764 solver.cpp:312] Iteration 136800 (5.90435 iter/s, 16.9367s/100 iter), loss = 1.40795
I0815 17:43:11.602582  8764 solver.cpp:334]     Train net output #0: loss = 1.42816 (* 1 = 1.42816 loss)
I0815 17:43:11.602589  8764 sgd_solver.cpp:136] Iteration 136800, lr = 0.00145, m = 0.9
I0815 17:43:30.369717  8764 solver.cpp:312] Iteration 136900 (5.32859 iter/s, 18.7667s/100 iter), loss = 1.25333
I0815 17:43:30.369740  8764 solver.cpp:334]     Train net output #0: loss = 1.10217 (* 1 = 1.10217 loss)
I0815 17:43:30.369745  8764 sgd_solver.cpp:136] Iteration 136900, lr = 0.00144375, m = 0.9
I0815 17:43:47.454567  8764 solver.cpp:363] Sparsity after update:
I0815 17:43:47.467392  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:43:47.467432  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:43:47.467458  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:43:47.467473  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:43:47.467488  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:43:47.467504  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:43:47.467516  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:43:47.467531  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:43:47.467545  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:43:47.467561  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:43:47.467576  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:43:47.467591  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:43:47.467605  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:43:47.661599  8764 solver.cpp:312] Iteration 137000 (5.78322 iter/s, 17.2914s/100 iter), loss = 1.14758
I0815 17:43:47.661644  8764 solver.cpp:334]     Train net output #0: loss = 1.38834 (* 1 = 1.38834 loss)
I0815 17:43:47.661654  8764 sgd_solver.cpp:136] Iteration 137000, lr = 0.0014375, m = 0.9
I0815 17:44:06.105000  8764 solver.cpp:312] Iteration 137100 (5.42215 iter/s, 18.4429s/100 iter), loss = 1.27495
I0815 17:44:06.105067  8764 solver.cpp:334]     Train net output #0: loss = 1.47891 (* 1 = 1.47891 loss)
I0815 17:44:06.105079  8764 sgd_solver.cpp:136] Iteration 137100, lr = 0.00143125, m = 0.9
I0815 17:44:23.201130  8764 solver.cpp:312] Iteration 137200 (5.84943 iter/s, 17.0957s/100 iter), loss = 1.37619
I0815 17:44:23.201616  8764 solver.cpp:334]     Train net output #0: loss = 1.38702 (* 1 = 1.38702 loss)
I0815 17:44:23.201623  8764 sgd_solver.cpp:136] Iteration 137200, lr = 0.001425, m = 0.9
I0815 17:44:39.739867  8764 solver.cpp:312] Iteration 137300 (6.04658 iter/s, 16.5383s/100 iter), loss = 1.07997
I0815 17:44:39.739895  8764 solver.cpp:334]     Train net output #0: loss = 1.19601 (* 1 = 1.19601 loss)
I0815 17:44:39.739902  8764 sgd_solver.cpp:136] Iteration 137300, lr = 0.00141875, m = 0.9
I0815 17:44:57.247925  8764 solver.cpp:312] Iteration 137400 (5.71181 iter/s, 17.5076s/100 iter), loss = 1.59418
I0815 17:44:57.247987  8764 solver.cpp:334]     Train net output #0: loss = 1.52616 (* 1 = 1.52616 loss)
I0815 17:44:57.247993  8764 sgd_solver.cpp:136] Iteration 137400, lr = 0.0014125, m = 0.9
I0815 17:45:17.918146  8764 solver.cpp:312] Iteration 137500 (4.83801 iter/s, 20.6697s/100 iter), loss = 1.25178
I0815 17:45:17.918216  8764 solver.cpp:334]     Train net output #0: loss = 1.51097 (* 1 = 1.51097 loss)
I0815 17:45:17.918236  8764 sgd_solver.cpp:136] Iteration 137500, lr = 0.00140625, m = 0.9
I0815 17:45:36.116096  8764 solver.cpp:312] Iteration 137600 (5.49528 iter/s, 18.1974s/100 iter), loss = 1.31203
I0815 17:45:36.116900  8764 solver.cpp:334]     Train net output #0: loss = 1.29878 (* 1 = 1.29878 loss)
I0815 17:45:36.116919  8764 sgd_solver.cpp:136] Iteration 137600, lr = 0.0014, m = 0.9
I0815 17:45:54.031064  8764 solver.cpp:312] Iteration 137700 (5.58208 iter/s, 17.9145s/100 iter), loss = 1.18106
I0815 17:45:54.031128  8764 solver.cpp:334]     Train net output #0: loss = 1.08756 (* 1 = 1.08756 loss)
I0815 17:45:54.031146  8764 sgd_solver.cpp:136] Iteration 137700, lr = 0.00139375, m = 0.9
I0815 17:46:11.623597  8764 solver.cpp:312] Iteration 137800 (5.68439 iter/s, 17.592s/100 iter), loss = 1.62696
I0815 17:46:11.623664  8764 solver.cpp:334]     Train net output #0: loss = 1.65726 (* 1 = 1.65726 loss)
I0815 17:46:11.623670  8764 sgd_solver.cpp:136] Iteration 137800, lr = 0.0013875, m = 0.9
I0815 17:46:30.626822  8764 solver.cpp:312] Iteration 137900 (5.26241 iter/s, 19.0027s/100 iter), loss = 1.15448
I0815 17:46:30.627053  8764 solver.cpp:334]     Train net output #0: loss = 0.940812 (* 1 = 0.940812 loss)
I0815 17:46:30.627168  8764 sgd_solver.cpp:136] Iteration 137900, lr = 0.00138125, m = 0.9
I0815 17:46:46.779886  8764 solver.cpp:363] Sparsity after update:
I0815 17:46:46.784196  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:46:46.784209  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:46:46.784217  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:46:46.784221  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:46:46.784236  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:46:46.784245  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:46:46.784253  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:46:46.784262  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:46:46.784271  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:46:46.784279  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:46:46.784287  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:46:46.784296  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:46:46.784303  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:46:46.784318  8764 solver.cpp:509] Iteration 138000, Testing net (#0)
I0815 17:47:14.464735  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.588764
I0815 17:47:14.464778  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.812997
I0815 17:47:14.464797  8764 solver.cpp:594]     Test net output #2: loss = 1.79901 (* 1 = 1.79901 loss)
I0815 17:47:14.464834  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.6798s
I0815 17:47:14.631477  8764 solver.cpp:312] Iteration 138000 (2.27255 iter/s, 44.0034s/100 iter), loss = 1.32484
I0815 17:47:14.631507  8764 solver.cpp:334]     Train net output #0: loss = 1.36437 (* 1 = 1.36437 loss)
I0815 17:47:14.631515  8764 sgd_solver.cpp:136] Iteration 138000, lr = 0.001375, m = 0.9
I0815 17:47:33.162962  8764 solver.cpp:312] Iteration 138100 (5.39637 iter/s, 18.531s/100 iter), loss = 0.822635
I0815 17:47:33.163017  8764 solver.cpp:334]     Train net output #0: loss = 0.77485 (* 1 = 0.77485 loss)
I0815 17:47:33.163023  8764 sgd_solver.cpp:136] Iteration 138100, lr = 0.00136875, m = 0.9
I0815 17:47:50.674090  8764 solver.cpp:312] Iteration 138200 (5.71081 iter/s, 17.5106s/100 iter), loss = 1.43506
I0815 17:47:50.674161  8764 solver.cpp:334]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0815 17:47:50.674180  8764 sgd_solver.cpp:136] Iteration 138200, lr = 0.0013625, m = 0.9
I0815 17:48:09.449272  8764 solver.cpp:312] Iteration 138300 (5.32633 iter/s, 18.7747s/100 iter), loss = 1.4102
I0815 17:48:09.449333  8764 solver.cpp:334]     Train net output #0: loss = 1.36217 (* 1 = 1.36217 loss)
I0815 17:48:09.449339  8764 sgd_solver.cpp:136] Iteration 138300, lr = 0.00135625, m = 0.9
I0815 17:48:27.120164  8764 solver.cpp:312] Iteration 138400 (5.65918 iter/s, 17.6704s/100 iter), loss = 1.34344
I0815 17:48:27.120220  8764 solver.cpp:334]     Train net output #0: loss = 1.45101 (* 1 = 1.45101 loss)
I0815 17:48:27.120247  8764 sgd_solver.cpp:136] Iteration 138400, lr = 0.00135, m = 0.9
I0815 17:48:49.032104  8764 solver.cpp:312] Iteration 138500 (4.56385 iter/s, 21.9113s/100 iter), loss = 1.61092
I0815 17:48:49.032222  8764 solver.cpp:334]     Train net output #0: loss = 1.68261 (* 1 = 1.68261 loss)
I0815 17:48:49.032239  8764 sgd_solver.cpp:136] Iteration 138500, lr = 0.00134375, m = 0.9
I0815 17:49:10.591570  8764 solver.cpp:312] Iteration 138600 (4.63846 iter/s, 21.5589s/100 iter), loss = 1.41586
I0815 17:49:10.591791  8764 solver.cpp:334]     Train net output #0: loss = 1.41867 (* 1 = 1.41867 loss)
I0815 17:49:10.591905  8764 sgd_solver.cpp:136] Iteration 138600, lr = 0.0013375, m = 0.9
I0815 17:49:31.173488  8764 solver.cpp:312] Iteration 138700 (4.85877 iter/s, 20.5813s/100 iter), loss = 1.74588
I0815 17:49:31.173576  8764 solver.cpp:334]     Train net output #0: loss = 1.13694 (* 1 = 1.13694 loss)
I0815 17:49:31.173581  8764 sgd_solver.cpp:136] Iteration 138700, lr = 0.00133125, m = 0.9
I0815 17:49:54.938567  8764 solver.cpp:312] Iteration 138800 (4.20797 iter/s, 23.7644s/100 iter), loss = 1.28607
I0815 17:49:54.938591  8764 solver.cpp:334]     Train net output #0: loss = 1.28711 (* 1 = 1.28711 loss)
I0815 17:49:54.938597  8764 sgd_solver.cpp:136] Iteration 138800, lr = 0.001325, m = 0.9
I0815 17:50:15.272161  8764 solver.cpp:312] Iteration 138900 (4.91811 iter/s, 20.333s/100 iter), loss = 1.25435
I0815 17:50:15.272284  8764 solver.cpp:334]     Train net output #0: loss = 1.12658 (* 1 = 1.12658 loss)
I0815 17:50:15.272301  8764 sgd_solver.cpp:136] Iteration 138900, lr = 0.00131875, m = 0.9
I0815 17:50:38.089361  8764 solver.cpp:363] Sparsity after update:
I0815 17:50:38.117295  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:50:38.117332  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:50:38.117344  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:50:38.117352  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:50:38.117358  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:50:38.117364  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:50:38.117372  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:50:38.117377  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:50:38.117383  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:50:38.117389  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:50:38.117395  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:50:38.117401  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:50:38.117408  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:50:38.262748  8764 solver.cpp:312] Iteration 139000 (4.34973 iter/s, 22.99s/100 iter), loss = 1.48917
I0815 17:50:38.262776  8764 solver.cpp:334]     Train net output #0: loss = 1.17114 (* 1 = 1.17114 loss)
I0815 17:50:38.262784  8764 sgd_solver.cpp:136] Iteration 139000, lr = 0.0013125, m = 0.9
I0815 17:50:58.841826  8764 solver.cpp:312] Iteration 139100 (4.85944 iter/s, 20.5785s/100 iter), loss = 1.56421
I0815 17:50:58.852162  8764 solver.cpp:334]     Train net output #0: loss = 1.72726 (* 1 = 1.72726 loss)
I0815 17:50:58.852180  8764 sgd_solver.cpp:136] Iteration 139100, lr = 0.00130625, m = 0.9
I0815 17:51:18.768172  8764 solver.cpp:312] Iteration 139200 (5.01862 iter/s, 19.9258s/100 iter), loss = 1.13512
I0815 17:51:18.768220  8764 solver.cpp:334]     Train net output #0: loss = 1.1238 (* 1 = 1.1238 loss)
I0815 17:51:18.768230  8764 sgd_solver.cpp:136] Iteration 139200, lr = 0.0013, m = 0.9
I0815 17:51:29.867888  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 17:51:36.812602  8764 solver.cpp:312] Iteration 139300 (5.54203 iter/s, 18.0439s/100 iter), loss = 1.31009
I0815 17:51:36.812667  8764 solver.cpp:334]     Train net output #0: loss = 1.27005 (* 1 = 1.27005 loss)
I0815 17:51:36.812685  8764 sgd_solver.cpp:136] Iteration 139300, lr = 0.00129375, m = 0.9
I0815 17:51:57.532510  8764 solver.cpp:312] Iteration 139400 (4.82641 iter/s, 20.7193s/100 iter), loss = 1.52984
I0815 17:51:57.532624  8764 solver.cpp:334]     Train net output #0: loss = 1.46082 (* 1 = 1.46082 loss)
I0815 17:51:57.532636  8764 sgd_solver.cpp:136] Iteration 139400, lr = 0.0012875, m = 0.9
I0815 17:52:18.064563  8764 solver.cpp:312] Iteration 139500 (4.87057 iter/s, 20.5315s/100 iter), loss = 1.03846
I0815 17:52:18.064628  8764 solver.cpp:334]     Train net output #0: loss = 1.11443 (* 1 = 1.11443 loss)
I0815 17:52:18.064637  8764 sgd_solver.cpp:136] Iteration 139500, lr = 0.00128125, m = 0.9
I0815 17:52:37.906922  8764 solver.cpp:312] Iteration 139600 (5.03986 iter/s, 19.8418s/100 iter), loss = 1.49914
I0815 17:52:37.906947  8764 solver.cpp:334]     Train net output #0: loss = 1.51967 (* 1 = 1.51967 loss)
I0815 17:52:37.906951  8764 sgd_solver.cpp:136] Iteration 139600, lr = 0.001275, m = 0.9
I0815 17:52:56.683394  8764 solver.cpp:312] Iteration 139700 (5.32596 iter/s, 18.776s/100 iter), loss = 1.20373
I0815 17:52:56.683465  8764 solver.cpp:334]     Train net output #0: loss = 0.887653 (* 1 = 0.887653 loss)
I0815 17:52:56.683471  8764 sgd_solver.cpp:136] Iteration 139700, lr = 0.00126875, m = 0.9
I0815 17:53:16.098732  8764 solver.cpp:312] Iteration 139800 (5.15071 iter/s, 19.4148s/100 iter), loss = 1.21737
I0815 17:53:16.098759  8764 solver.cpp:334]     Train net output #0: loss = 1.14637 (* 1 = 1.14637 loss)
I0815 17:53:16.098765  8764 sgd_solver.cpp:136] Iteration 139800, lr = 0.0012625, m = 0.9
I0815 17:53:38.102414  8764 solver.cpp:312] Iteration 139900 (4.54482 iter/s, 22.0031s/100 iter), loss = 1.3324
I0815 17:53:38.102484  8764 solver.cpp:334]     Train net output #0: loss = 1.32427 (* 1 = 1.32427 loss)
I0815 17:53:38.102491  8764 sgd_solver.cpp:136] Iteration 139900, lr = 0.00125625, m = 0.9
I0815 17:53:55.182837  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_140000.caffemodel
I0815 17:53:55.307399  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_140000.solverstate
I0815 17:53:55.317852  8764 solver.cpp:363] Sparsity after update:
I0815 17:53:55.320282  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:53:55.320333  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:53:55.320365  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:53:55.320384  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:53:55.320401  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:53:55.320420  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:53:55.320436  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:53:55.320453  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:53:55.320469  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:53:55.320487  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:53:55.320502  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:53:55.320519  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:53:55.320536  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:53:55.320574  8764 solver.cpp:509] Iteration 140000, Testing net (#0)
I0815 17:54:30.001000  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.58947
I0815 17:54:30.001096  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819997
I0815 17:54:30.001117  8764 solver.cpp:594]     Test net output #2: loss = 1.78096 (* 1 = 1.78096 loss)
I0815 17:54:30.001159  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.6796s
I0815 17:54:30.418001  8764 solver.cpp:312] Iteration 140000 (1.91153 iter/s, 52.3141s/100 iter), loss = 1.35551
I0815 17:54:30.418027  8764 solver.cpp:334]     Train net output #0: loss = 1.46042 (* 1 = 1.46042 loss)
I0815 17:54:30.418032  8764 sgd_solver.cpp:136] Iteration 140000, lr = 0.00125, m = 0.9
I0815 17:54:50.150614  8764 solver.cpp:312] Iteration 140100 (5.0679 iter/s, 19.7321s/100 iter), loss = 1.44753
I0815 17:54:50.150655  8764 solver.cpp:334]     Train net output #0: loss = 1.59608 (* 1 = 1.59608 loss)
I0815 17:54:50.150665  8764 sgd_solver.cpp:136] Iteration 140100, lr = 0.00124375, m = 0.9
I0815 17:55:06.584578  8764 solver.cpp:312] Iteration 140200 (6.08513 iter/s, 16.4335s/100 iter), loss = 1.70665
I0815 17:55:06.584668  8764 solver.cpp:334]     Train net output #0: loss = 1.81339 (* 1 = 1.81339 loss)
I0815 17:55:06.584686  8764 sgd_solver.cpp:136] Iteration 140200, lr = 0.0012375, m = 0.9
I0815 17:55:27.144171  8764 solver.cpp:312] Iteration 140300 (4.86405 iter/s, 20.559s/100 iter), loss = 1.25533
I0815 17:55:27.144229  8764 solver.cpp:334]     Train net output #0: loss = 1.36387 (* 1 = 1.36387 loss)
I0815 17:55:27.144243  8764 sgd_solver.cpp:136] Iteration 140300, lr = 0.00123125, m = 0.9
I0815 17:55:46.078588  8764 solver.cpp:312] Iteration 140400 (5.28154 iter/s, 18.9339s/100 iter), loss = 1.59459
I0815 17:55:46.078712  8764 solver.cpp:334]     Train net output #0: loss = 1.49622 (* 1 = 1.49622 loss)
I0815 17:55:46.078729  8764 sgd_solver.cpp:136] Iteration 140400, lr = 0.001225, m = 0.9
I0815 17:56:05.341476  8764 solver.cpp:312] Iteration 140500 (5.19147 iter/s, 19.2624s/100 iter), loss = 1.71288
I0815 17:56:05.341506  8764 solver.cpp:334]     Train net output #0: loss = 1.76404 (* 1 = 1.76404 loss)
I0815 17:56:05.341511  8764 sgd_solver.cpp:136] Iteration 140500, lr = 0.00121875, m = 0.9
I0815 17:56:22.166039  8764 solver.cpp:312] Iteration 140600 (5.94385 iter/s, 16.8241s/100 iter), loss = 1.34819
I0815 17:56:22.166093  8764 solver.cpp:334]     Train net output #0: loss = 1.40758 (* 1 = 1.40758 loss)
I0815 17:56:22.166100  8764 sgd_solver.cpp:136] Iteration 140600, lr = 0.0012125, m = 0.9
I0815 17:56:41.821146  8764 solver.cpp:312] Iteration 140700 (5.08788 iter/s, 19.6546s/100 iter), loss = 1.368
I0815 17:56:41.821171  8764 solver.cpp:334]     Train net output #0: loss = 1.13915 (* 1 = 1.13915 loss)
I0815 17:56:41.821177  8764 sgd_solver.cpp:136] Iteration 140700, lr = 0.00120625, m = 0.9
I0815 17:57:01.617628  8764 solver.cpp:312] Iteration 140800 (5.05154 iter/s, 19.7959s/100 iter), loss = 1.17013
I0815 17:57:01.620170  8764 solver.cpp:334]     Train net output #0: loss = 1.23675 (* 1 = 1.23675 loss)
I0815 17:57:01.620187  8764 sgd_solver.cpp:136] Iteration 140800, lr = 0.0012, m = 0.9
I0815 17:57:21.231899  8764 solver.cpp:312] Iteration 140900 (5.09847 iter/s, 19.6137s/100 iter), loss = 1.62627
I0815 17:57:21.231925  8764 solver.cpp:334]     Train net output #0: loss = 1.65989 (* 1 = 1.65989 loss)
I0815 17:57:21.231928  8764 sgd_solver.cpp:136] Iteration 140900, lr = 0.00119375, m = 0.9
I0815 17:57:38.646288  8764 solver.cpp:363] Sparsity after update:
I0815 17:57:38.662535  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 17:57:38.662631  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 17:57:38.662663  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 17:57:38.662674  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 17:57:38.662683  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 17:57:38.662693  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 17:57:38.662703  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 17:57:38.662714  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 17:57:38.662724  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 17:57:38.662732  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 17:57:38.662742  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 17:57:38.662751  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 17:57:38.662760  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 17:57:38.984992  8764 solver.cpp:312] Iteration 141000 (5.63298 iter/s, 17.7526s/100 iter), loss = 1.39315
I0815 17:57:38.985047  8764 solver.cpp:334]     Train net output #0: loss = 0.956002 (* 1 = 0.956002 loss)
I0815 17:57:38.985056  8764 sgd_solver.cpp:136] Iteration 141000, lr = 0.0011875, m = 0.9
I0815 17:57:58.056679  8764 solver.cpp:312] Iteration 141100 (5.24352 iter/s, 19.0712s/100 iter), loss = 1.41718
I0815 17:57:58.056756  8764 solver.cpp:334]     Train net output #0: loss = 1.97423 (* 1 = 1.97423 loss)
I0815 17:57:58.056785  8764 sgd_solver.cpp:136] Iteration 141100, lr = 0.00118125, m = 0.9
I0815 17:58:16.579645  8764 solver.cpp:312] Iteration 141200 (5.39885 iter/s, 18.5225s/100 iter), loss = 1.45915
I0815 17:58:16.579710  8764 solver.cpp:334]     Train net output #0: loss = 1.38899 (* 1 = 1.38899 loss)
I0815 17:58:16.579718  8764 sgd_solver.cpp:136] Iteration 141200, lr = 0.001175, m = 0.9
I0815 17:58:33.454031  8764 solver.cpp:312] Iteration 141300 (5.9263 iter/s, 16.8739s/100 iter), loss = 1.28441
I0815 17:58:33.454059  8764 solver.cpp:334]     Train net output #0: loss = 1.06432 (* 1 = 1.06432 loss)
I0815 17:58:33.454066  8764 sgd_solver.cpp:136] Iteration 141300, lr = 0.00116875, m = 0.9
I0815 17:58:50.292281  8764 solver.cpp:312] Iteration 141400 (5.93902 iter/s, 16.8378s/100 iter), loss = 1.45143
I0815 17:58:50.292361  8764 solver.cpp:334]     Train net output #0: loss = 1.41304 (* 1 = 1.41304 loss)
I0815 17:58:50.292368  8764 sgd_solver.cpp:136] Iteration 141400, lr = 0.0011625, m = 0.9
I0815 17:59:08.231704  8764 solver.cpp:312] Iteration 141500 (5.57447 iter/s, 17.9389s/100 iter), loss = 1.25331
I0815 17:59:08.231747  8764 solver.cpp:334]     Train net output #0: loss = 1.2311 (* 1 = 1.2311 loss)
I0815 17:59:08.231755  8764 sgd_solver.cpp:136] Iteration 141500, lr = 0.00115625, m = 0.9
I0815 17:59:27.437798  8764 solver.cpp:312] Iteration 141600 (5.20682 iter/s, 19.2056s/100 iter), loss = 1.40819
I0815 17:59:27.438048  8764 solver.cpp:334]     Train net output #0: loss = 1.45161 (* 1 = 1.45161 loss)
I0815 17:59:27.438158  8764 sgd_solver.cpp:136] Iteration 141600, lr = 0.00115, m = 0.9
I0815 17:59:47.669544  8764 solver.cpp:312] Iteration 141700 (4.94286 iter/s, 20.2312s/100 iter), loss = 1.35848
I0815 17:59:47.669713  8764 solver.cpp:334]     Train net output #0: loss = 1.3211 (* 1 = 1.3211 loss)
I0815 17:59:47.669801  8764 sgd_solver.cpp:136] Iteration 141700, lr = 0.00114375, m = 0.9
I0815 18:00:05.298149  8764 solver.cpp:312] Iteration 141800 (5.67276 iter/s, 17.6281s/100 iter), loss = 0.976221
I0815 18:00:05.298247  8764 solver.cpp:334]     Train net output #0: loss = 1.13249 (* 1 = 1.13249 loss)
I0815 18:00:05.298267  8764 sgd_solver.cpp:136] Iteration 141800, lr = 0.0011375, m = 0.9
I0815 18:00:23.264578  8764 solver.cpp:312] Iteration 141900 (5.5661 iter/s, 17.9659s/100 iter), loss = 1.55309
I0815 18:00:23.264652  8764 solver.cpp:334]     Train net output #0: loss = 1.21867 (* 1 = 1.21867 loss)
I0815 18:00:23.264672  8764 sgd_solver.cpp:136] Iteration 141900, lr = 0.00113125, m = 0.9
I0815 18:00:42.068178  8764 solver.cpp:363] Sparsity after update:
I0815 18:00:42.075137  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:00:42.075170  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:00:42.075212  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:00:42.075234  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:00:42.075266  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:00:42.075296  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:00:42.075327  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:00:42.075358  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:00:42.075374  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:00:42.075394  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:00:42.075414  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:00:42.075438  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:00:42.075466  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:00:42.075513  8764 solver.cpp:509] Iteration 142000, Testing net (#0)
I0815 18:01:08.625633  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.596529
I0815 18:01:08.625653  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.82088
I0815 18:01:08.625658  8764 solver.cpp:594]     Test net output #2: loss = 1.75786 (* 1 = 1.75786 loss)
I0815 18:01:08.625676  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.5494s
I0815 18:01:08.793457  8764 solver.cpp:312] Iteration 142000 (2.19647 iter/s, 45.5276s/100 iter), loss = 1.33667
I0815 18:01:08.801151  8764 solver.cpp:334]     Train net output #0: loss = 1.51418 (* 1 = 1.51418 loss)
I0815 18:01:08.801162  8764 sgd_solver.cpp:136] Iteration 142000, lr = 0.001125, m = 0.9
I0815 18:01:14.716812  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:01:28.478665  8764 solver.cpp:312] Iteration 142100 (5.0801 iter/s, 19.6847s/100 iter), loss = 1.188
I0815 18:01:28.478708  8764 solver.cpp:334]     Train net output #0: loss = 1.16061 (* 1 = 1.16061 loss)
I0815 18:01:28.478715  8764 sgd_solver.cpp:136] Iteration 142100, lr = 0.00111875, m = 0.9
I0815 18:01:47.555893  8764 solver.cpp:312] Iteration 142200 (5.242 iter/s, 19.0767s/100 iter), loss = 1.43272
I0815 18:01:47.555970  8764 solver.cpp:334]     Train net output #0: loss = 1.43287 (* 1 = 1.43287 loss)
I0815 18:01:47.555977  8764 sgd_solver.cpp:136] Iteration 142200, lr = 0.0011125, m = 0.9
I0815 18:02:04.430253  8764 solver.cpp:312] Iteration 142300 (5.92632 iter/s, 16.8739s/100 iter), loss = 1.25467
I0815 18:02:04.430275  8764 solver.cpp:334]     Train net output #0: loss = 1.25932 (* 1 = 1.25932 loss)
I0815 18:02:04.430279  8764 sgd_solver.cpp:136] Iteration 142300, lr = 0.00110625, m = 0.9
I0815 18:02:21.930901  8764 solver.cpp:312] Iteration 142400 (5.71423 iter/s, 17.5002s/100 iter), loss = 1.39794
I0815 18:02:21.936416  8764 solver.cpp:334]     Train net output #0: loss = 1.73288 (* 1 = 1.73288 loss)
I0815 18:02:21.936439  8764 sgd_solver.cpp:136] Iteration 142400, lr = 0.0011, m = 0.9
I0815 18:02:39.988863  8764 solver.cpp:312] Iteration 142500 (5.53788 iter/s, 18.0575s/100 iter), loss = 1.76049
I0815 18:02:39.988943  8764 solver.cpp:334]     Train net output #0: loss = 1.79612 (* 1 = 1.79612 loss)
I0815 18:02:39.988965  8764 sgd_solver.cpp:136] Iteration 142500, lr = 0.00109375, m = 0.9
I0815 18:02:58.274222  8764 solver.cpp:312] Iteration 142600 (5.46901 iter/s, 18.2849s/100 iter), loss = 1.41834
I0815 18:02:58.274286  8764 solver.cpp:334]     Train net output #0: loss = 1.17691 (* 1 = 1.17691 loss)
I0815 18:02:58.274293  8764 sgd_solver.cpp:136] Iteration 142600, lr = 0.0010875, m = 0.9
I0815 18:03:16.285676  8764 solver.cpp:312] Iteration 142700 (5.55218 iter/s, 18.0109s/100 iter), loss = 1.29608
I0815 18:03:16.285732  8764 solver.cpp:334]     Train net output #0: loss = 1.14727 (* 1 = 1.14727 loss)
I0815 18:03:16.285742  8764 sgd_solver.cpp:136] Iteration 142700, lr = 0.00108125, m = 0.9
I0815 18:03:33.383854  8764 solver.cpp:312] Iteration 142800 (5.84873 iter/s, 17.0977s/100 iter), loss = 1.21964
I0815 18:03:33.383919  8764 solver.cpp:334]     Train net output #0: loss = 1.11921 (* 1 = 1.11921 loss)
I0815 18:03:33.383924  8764 sgd_solver.cpp:136] Iteration 142800, lr = 0.001075, m = 0.9
I0815 18:03:52.402137  8764 solver.cpp:312] Iteration 142900 (5.25824 iter/s, 19.0178s/100 iter), loss = 1.41301
I0815 18:03:52.402166  8764 solver.cpp:334]     Train net output #0: loss = 1.68517 (* 1 = 1.68517 loss)
I0815 18:03:52.402173  8764 sgd_solver.cpp:136] Iteration 142900, lr = 0.00106875, m = 0.9
I0815 18:04:12.215577  8764 solver.cpp:363] Sparsity after update:
I0815 18:04:12.226796  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:04:12.226838  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:04:12.226855  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:04:12.226866  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:04:12.226876  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:04:12.226884  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:04:12.226893  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:04:12.226902  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:04:12.226910  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:04:12.226919  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:04:12.226928  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:04:12.226938  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:04:12.226946  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:04:12.419339  8764 solver.cpp:312] Iteration 143000 (4.99584 iter/s, 20.0166s/100 iter), loss = 1.2833
I0815 18:04:12.419371  8764 solver.cpp:334]     Train net output #0: loss = 1.30975 (* 1 = 1.30975 loss)
I0815 18:04:12.419376  8764 sgd_solver.cpp:136] Iteration 143000, lr = 0.0010625, m = 0.9
I0815 18:04:30.815928  8764 solver.cpp:312] Iteration 143100 (5.43594 iter/s, 18.3961s/100 iter), loss = 1.23688
I0815 18:04:30.815960  8764 solver.cpp:334]     Train net output #0: loss = 1.1033 (* 1 = 1.1033 loss)
I0815 18:04:30.815965  8764 sgd_solver.cpp:136] Iteration 143100, lr = 0.00105625, m = 0.9
I0815 18:04:48.096513  8764 solver.cpp:312] Iteration 143200 (5.787 iter/s, 17.2801s/100 iter), loss = 1.28413
I0815 18:04:48.096602  8764 solver.cpp:334]     Train net output #0: loss = 1.42287 (* 1 = 1.42287 loss)
I0815 18:04:48.096609  8764 sgd_solver.cpp:136] Iteration 143200, lr = 0.00105, m = 0.9
I0815 18:05:05.148787  8764 solver.cpp:312] Iteration 143300 (5.86448 iter/s, 17.0518s/100 iter), loss = 1.18326
I0815 18:05:05.148814  8764 solver.cpp:334]     Train net output #0: loss = 0.915706 (* 1 = 0.915706 loss)
I0815 18:05:05.148821  8764 sgd_solver.cpp:136] Iteration 143300, lr = 0.00104375, m = 0.9
I0815 18:05:22.618831  8764 solver.cpp:312] Iteration 143400 (5.72424 iter/s, 17.4696s/100 iter), loss = 1.41134
I0815 18:05:22.618914  8764 solver.cpp:334]     Train net output #0: loss = 1.51103 (* 1 = 1.51103 loss)
I0815 18:05:22.618927  8764 sgd_solver.cpp:136] Iteration 143400, lr = 0.0010375, m = 0.9
I0815 18:05:39.067729  8764 solver.cpp:312] Iteration 143500 (6.0796 iter/s, 16.4484s/100 iter), loss = 1.12462
I0815 18:05:39.067811  8764 solver.cpp:334]     Train net output #0: loss = 1.263 (* 1 = 1.263 loss)
I0815 18:05:39.067831  8764 sgd_solver.cpp:136] Iteration 143500, lr = 0.00103125, m = 0.9
I0815 18:05:55.295142  8764 solver.cpp:312] Iteration 143600 (6.16258 iter/s, 16.227s/100 iter), loss = 1.73647
I0815 18:05:55.295198  8764 solver.cpp:334]     Train net output #0: loss = 1.42315 (* 1 = 1.42315 loss)
I0815 18:05:55.295204  8764 sgd_solver.cpp:136] Iteration 143600, lr = 0.001025, m = 0.9
I0815 18:06:15.940618  8764 solver.cpp:312] Iteration 143700 (4.84381 iter/s, 20.6449s/100 iter), loss = 1.46177
I0815 18:06:15.940667  8764 solver.cpp:334]     Train net output #0: loss = 1.75691 (* 1 = 1.75691 loss)
I0815 18:06:15.940678  8764 sgd_solver.cpp:136] Iteration 143700, lr = 0.00101875, m = 0.9
I0815 18:06:35.436874  8764 solver.cpp:312] Iteration 143800 (5.12933 iter/s, 19.4957s/100 iter), loss = 1.19742
I0815 18:06:35.436985  8764 solver.cpp:334]     Train net output #0: loss = 0.896615 (* 1 = 0.896615 loss)
I0815 18:06:35.437001  8764 sgd_solver.cpp:136] Iteration 143800, lr = 0.0010125, m = 0.9
I0815 18:06:55.318963  8764 solver.cpp:312] Iteration 143900 (5.02979 iter/s, 19.8815s/100 iter), loss = 1.51174
I0815 18:06:55.318992  8764 solver.cpp:334]     Train net output #0: loss = 1.25507 (* 1 = 1.25507 loss)
I0815 18:06:55.318997  8764 sgd_solver.cpp:136] Iteration 143900, lr = 0.00100625, m = 0.9
I0815 18:07:13.808414  8764 solver.cpp:363] Sparsity after update:
I0815 18:07:13.812338  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:07:13.812369  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:07:13.812388  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:07:13.812402  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:07:13.812412  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:07:13.812425  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:07:13.812436  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:07:13.812448  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:07:13.812459  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:07:13.812470  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:07:13.812481  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:07:13.812494  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:07:13.812505  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:07:13.812525  8764 solver.cpp:509] Iteration 144000, Testing net (#0)
I0815 18:07:33.054859  8747 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 18:07:48.112546  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.593412
I0815 18:07:48.112669  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817938
I0815 18:07:48.112679  8764 solver.cpp:594]     Test net output #2: loss = 1.77148 (* 1 = 1.77148 loss)
I0815 18:07:48.112809  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.2993s
I0815 18:07:48.417847  8764 solver.cpp:312] Iteration 144000 (1.88333 iter/s, 53.0974s/100 iter), loss = 1.21107
I0815 18:07:48.417889  8764 solver.cpp:334]     Train net output #0: loss = 1.31603 (* 1 = 1.31603 loss)
I0815 18:07:48.417912  8764 sgd_solver.cpp:136] Iteration 144000, lr = 0.001, m = 0.9
I0815 18:08:06.167287  8764 solver.cpp:312] Iteration 144100 (5.63414 iter/s, 17.7489s/100 iter), loss = 1.08475
I0815 18:08:06.167310  8764 solver.cpp:334]     Train net output #0: loss = 0.880872 (* 1 = 0.880872 loss)
I0815 18:08:06.167316  8764 sgd_solver.cpp:136] Iteration 144100, lr = 0.00099375, m = 0.9
I0815 18:08:25.894873  8764 solver.cpp:312] Iteration 144200 (5.06918 iter/s, 19.727s/100 iter), loss = 1.59796
I0815 18:08:25.894991  8764 solver.cpp:334]     Train net output #0: loss = 1.13678 (* 1 = 1.13678 loss)
I0815 18:08:25.895009  8764 sgd_solver.cpp:136] Iteration 144200, lr = 0.0009875, m = 0.9
I0815 18:08:46.794845  8764 solver.cpp:312] Iteration 144300 (4.78483 iter/s, 20.8994s/100 iter), loss = 1.55355
I0815 18:08:46.794875  8764 solver.cpp:334]     Train net output #0: loss = 2.17058 (* 1 = 2.17058 loss)
I0815 18:08:46.794881  8764 sgd_solver.cpp:136] Iteration 144300, lr = 0.00098125, m = 0.9
I0815 18:09:09.836156  8764 solver.cpp:312] Iteration 144400 (4.34015 iter/s, 23.0407s/100 iter), loss = 1.17666
I0815 18:09:09.836259  8764 solver.cpp:334]     Train net output #0: loss = 1.14441 (* 1 = 1.14441 loss)
I0815 18:09:09.836406  8764 sgd_solver.cpp:136] Iteration 144400, lr = 0.000975, m = 0.9
I0815 18:09:31.270547  8764 solver.cpp:312] Iteration 144500 (4.66553 iter/s, 21.4338s/100 iter), loss = 1.03128
I0815 18:09:31.270680  8764 solver.cpp:334]     Train net output #0: loss = 0.871275 (* 1 = 0.871275 loss)
I0815 18:09:31.270725  8764 sgd_solver.cpp:136] Iteration 144500, lr = 0.00096875, m = 0.9
I0815 18:09:52.051170  8764 solver.cpp:312] Iteration 144600 (4.81231 iter/s, 20.7801s/100 iter), loss = 1.21673
I0815 18:09:52.051235  8764 solver.cpp:334]     Train net output #0: loss = 1.39767 (* 1 = 1.39767 loss)
I0815 18:09:52.051242  8764 sgd_solver.cpp:136] Iteration 144600, lr = 0.0009625, m = 0.9
I0815 18:10:15.356173  8764 solver.cpp:312] Iteration 144700 (4.29105 iter/s, 23.3043s/100 iter), loss = 1.22525
I0815 18:10:15.356442  8764 solver.cpp:334]     Train net output #0: loss = 1.57252 (* 1 = 1.57252 loss)
I0815 18:10:15.356572  8764 sgd_solver.cpp:136] Iteration 144700, lr = 0.00095625, m = 0.9
I0815 18:10:35.409934  8764 solver.cpp:312] Iteration 144800 (4.98676 iter/s, 20.0531s/100 iter), loss = 1.45445
I0815 18:10:35.416189  8764 solver.cpp:334]     Train net output #0: loss = 1.42374 (* 1 = 1.42374 loss)
I0815 18:10:35.416229  8764 sgd_solver.cpp:136] Iteration 144800, lr = 0.00095, m = 0.9
I0815 18:10:54.077018  8764 solver.cpp:312] Iteration 144900 (5.35716 iter/s, 18.6666s/100 iter), loss = 1.76503
I0815 18:10:54.077044  8764 solver.cpp:334]     Train net output #0: loss = 1.74115 (* 1 = 1.74115 loss)
I0815 18:10:54.077050  8764 sgd_solver.cpp:136] Iteration 144900, lr = 0.00094375, m = 0.9
I0815 18:11:14.843029  8764 solver.cpp:363] Sparsity after update:
I0815 18:11:14.857729  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:11:14.857755  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:11:14.857771  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:11:14.857777  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:11:14.857782  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:11:14.857789  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:11:14.857795  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:11:14.857801  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:11:14.857807  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:11:14.857813  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:11:14.857820  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:11:14.857826  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:11:14.857832  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:11:14.999878  8764 solver.cpp:312] Iteration 145000 (4.77959 iter/s, 20.9223s/100 iter), loss = 1.17689
I0815 18:11:14.999907  8764 solver.cpp:334]     Train net output #0: loss = 1.40365 (* 1 = 1.40365 loss)
I0815 18:11:14.999912  8764 sgd_solver.cpp:136] Iteration 145000, lr = 0.0009375, m = 0.9
I0815 18:11:35.153436  8764 solver.cpp:312] Iteration 145100 (4.96204 iter/s, 20.153s/100 iter), loss = 1.48255
I0815 18:11:35.153492  8764 solver.cpp:334]     Train net output #0: loss = 1.55291 (* 1 = 1.55291 loss)
I0815 18:11:35.153502  8764 sgd_solver.cpp:136] Iteration 145100, lr = 0.00093125, m = 0.9
I0815 18:11:53.893126  8764 solver.cpp:312] Iteration 145200 (5.33642 iter/s, 18.7392s/100 iter), loss = 1.47679
I0815 18:11:53.893246  8764 solver.cpp:334]     Train net output #0: loss = 1.99516 (* 1 = 1.99516 loss)
I0815 18:11:53.893261  8764 sgd_solver.cpp:136] Iteration 145200, lr = 0.000925, m = 0.9
I0815 18:12:16.416522  8764 solver.cpp:312] Iteration 145300 (4.43995 iter/s, 22.5228s/100 iter), loss = 1.55123
I0815 18:12:16.416563  8764 solver.cpp:334]     Train net output #0: loss = 1.583 (* 1 = 1.583 loss)
I0815 18:12:16.416574  8764 sgd_solver.cpp:136] Iteration 145300, lr = 0.00091875, m = 0.9
I0815 18:12:38.105657  8764 solver.cpp:312] Iteration 145400 (4.61073 iter/s, 21.6885s/100 iter), loss = 1.4198
I0815 18:12:38.105775  8764 solver.cpp:334]     Train net output #0: loss = 1.36147 (* 1 = 1.36147 loss)
I0815 18:12:38.105794  8764 sgd_solver.cpp:136] Iteration 145400, lr = 0.0009125, m = 0.9
I0815 18:12:57.528532  8764 solver.cpp:312] Iteration 145500 (5.14871 iter/s, 19.4223s/100 iter), loss = 1.2938
I0815 18:12:57.528556  8764 solver.cpp:334]     Train net output #0: loss = 1.23705 (* 1 = 1.23705 loss)
I0815 18:12:57.528560  8764 sgd_solver.cpp:136] Iteration 145500, lr = 0.00090625, m = 0.9
I0815 18:13:16.017442  8764 solver.cpp:312] Iteration 145600 (5.4088 iter/s, 18.4884s/100 iter), loss = 1.18555
I0815 18:13:16.017621  8764 solver.cpp:334]     Train net output #0: loss = 1.33523 (* 1 = 1.33523 loss)
I0815 18:13:16.017638  8764 sgd_solver.cpp:136] Iteration 145600, lr = 0.0009, m = 0.9
I0815 18:13:38.416702  8764 solver.cpp:312] Iteration 145700 (4.46456 iter/s, 22.3986s/100 iter), loss = 1.39726
I0815 18:13:38.416774  8764 solver.cpp:334]     Train net output #0: loss = 1.34397 (* 1 = 1.34397 loss)
I0815 18:13:38.416793  8764 sgd_solver.cpp:136] Iteration 145700, lr = 0.00089375, m = 0.9
I0815 18:13:57.486547  8764 solver.cpp:312] Iteration 145800 (5.24403 iter/s, 19.0693s/100 iter), loss = 1.35753
I0815 18:13:57.486598  8764 solver.cpp:334]     Train net output #0: loss = 1.41632 (* 1 = 1.41632 loss)
I0815 18:13:57.486604  8764 sgd_solver.cpp:136] Iteration 145800, lr = 0.0008875, m = 0.9
I0815 18:14:17.385567  8764 solver.cpp:312] Iteration 145900 (5.02551 iter/s, 19.8985s/100 iter), loss = 1.72988
I0815 18:14:17.385632  8764 solver.cpp:334]     Train net output #0: loss = 1.84464 (* 1 = 1.84464 loss)
I0815 18:14:17.385648  8764 sgd_solver.cpp:136] Iteration 145900, lr = 0.00088125, m = 0.9
I0815 18:14:40.530297  8764 solver.cpp:363] Sparsity after update:
I0815 18:14:40.534569  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:14:40.534591  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:14:40.534612  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:14:40.534621  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:14:40.534628  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:14:40.534636  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:14:40.534644  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:14:40.534652  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:14:40.534660  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:14:40.534668  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:14:40.534677  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:14:40.534684  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:14:40.534693  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:14:40.534715  8764 solver.cpp:509] Iteration 146000, Testing net (#0)
I0815 18:14:48.654812  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:15:08.022887  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.591471
I0815 18:15:08.022924  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.822762
I0815 18:15:08.022943  8764 solver.cpp:594]     Test net output #2: loss = 1.76654 (* 1 = 1.76654 loss)
I0815 18:15:08.022987  8764 solver.cpp:264] [MultiGPU] Tests completed in 27.4875s
I0815 18:15:08.202162  8764 solver.cpp:312] Iteration 146000 (1.96792 iter/s, 50.8152s/100 iter), loss = 1.30681
I0815 18:15:08.202229  8764 solver.cpp:334]     Train net output #0: loss = 1.2685 (* 1 = 1.2685 loss)
I0815 18:15:08.202327  8764 sgd_solver.cpp:136] Iteration 146000, lr = 0.000875, m = 0.9
I0815 18:15:25.561092  8764 solver.cpp:312] Iteration 146100 (5.76088 iter/s, 17.3585s/100 iter), loss = 1.16874
I0815 18:15:25.561199  8764 solver.cpp:334]     Train net output #0: loss = 0.961865 (* 1 = 0.961865 loss)
I0815 18:15:25.561218  8764 sgd_solver.cpp:136] Iteration 146100, lr = 0.00086875, m = 0.9
I0815 18:15:44.016202  8764 solver.cpp:312] Iteration 146200 (5.41871 iter/s, 18.4546s/100 iter), loss = 1.09653
I0815 18:15:44.016305  8764 solver.cpp:334]     Train net output #0: loss = 1.18996 (* 1 = 1.18996 loss)
I0815 18:15:44.016329  8764 sgd_solver.cpp:136] Iteration 146200, lr = 0.0008625, m = 0.9
I0815 18:16:04.459015  8764 solver.cpp:312] Iteration 146300 (4.89183 iter/s, 20.4423s/100 iter), loss = 1.2781
I0815 18:16:04.459086  8764 solver.cpp:334]     Train net output #0: loss = 1.18562 (* 1 = 1.18562 loss)
I0815 18:16:04.459095  8764 sgd_solver.cpp:136] Iteration 146300, lr = 0.00085625, m = 0.9
I0815 18:16:24.012154  8764 solver.cpp:312] Iteration 146400 (5.11441 iter/s, 19.5526s/100 iter), loss = 1.57977
I0815 18:16:24.012182  8764 solver.cpp:334]     Train net output #0: loss = 1.72565 (* 1 = 1.72565 loss)
I0815 18:16:24.012187  8764 sgd_solver.cpp:136] Iteration 146400, lr = 0.00085, m = 0.9
I0815 18:16:42.160375  8764 solver.cpp:312] Iteration 146500 (5.51034 iter/s, 18.1477s/100 iter), loss = 1.28108
I0815 18:16:42.168166  8764 solver.cpp:334]     Train net output #0: loss = 1.35219 (* 1 = 1.35219 loss)
I0815 18:16:42.168315  8764 sgd_solver.cpp:136] Iteration 146500, lr = 0.00084375, m = 0.9
I0815 18:17:00.498065  8764 solver.cpp:312] Iteration 146600 (5.4534 iter/s, 18.3372s/100 iter), loss = 1.52768
I0815 18:17:00.498088  8764 solver.cpp:334]     Train net output #0: loss = 1.60172 (* 1 = 1.60172 loss)
I0815 18:17:00.498095  8764 sgd_solver.cpp:136] Iteration 146600, lr = 0.0008375, m = 0.9
I0815 18:17:19.139434  8764 solver.cpp:312] Iteration 146700 (5.36456 iter/s, 18.6409s/100 iter), loss = 1.22082
I0815 18:17:19.139495  8764 solver.cpp:334]     Train net output #0: loss = 1.15194 (* 1 = 1.15194 loss)
I0815 18:17:19.139503  8764 sgd_solver.cpp:136] Iteration 146700, lr = 0.00083125, m = 0.9
I0815 18:17:37.327904  8764 solver.cpp:312] Iteration 146800 (5.49814 iter/s, 18.188s/100 iter), loss = 1.55639
I0815 18:17:37.327957  8764 solver.cpp:334]     Train net output #0: loss = 1.96382 (* 1 = 1.96382 loss)
I0815 18:17:37.327967  8764 sgd_solver.cpp:136] Iteration 146800, lr = 0.000825, m = 0.9
I0815 18:17:54.797890  8764 solver.cpp:312] Iteration 146900 (5.72426 iter/s, 17.4695s/100 iter), loss = 1.32375
I0815 18:17:54.797996  8764 solver.cpp:334]     Train net output #0: loss = 1.36496 (* 1 = 1.36496 loss)
I0815 18:17:54.798014  8764 sgd_solver.cpp:136] Iteration 146900, lr = 0.00081875, m = 0.9
I0815 18:18:14.038336  8764 solver.cpp:363] Sparsity after update:
I0815 18:18:14.040801  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:18:14.040819  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:18:14.040837  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:18:14.040845  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:18:14.040851  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:18:14.040858  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:18:14.040863  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:18:14.040868  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:18:14.040874  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:18:14.040879  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:18:14.040884  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:18:14.040891  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:18:14.040897  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:18:14.267163  8764 solver.cpp:312] Iteration 147000 (5.13644 iter/s, 19.4687s/100 iter), loss = 1.87763
I0815 18:18:14.267205  8764 solver.cpp:334]     Train net output #0: loss = 2.01834 (* 1 = 2.01834 loss)
I0815 18:18:14.267213  8764 sgd_solver.cpp:136] Iteration 147000, lr = 0.0008125, m = 0.9
I0815 18:18:32.495434  8764 solver.cpp:312] Iteration 147100 (5.48614 iter/s, 18.2278s/100 iter), loss = 1.44026
I0815 18:18:32.495548  8764 solver.cpp:334]     Train net output #0: loss = 1.18449 (* 1 = 1.18449 loss)
I0815 18:18:32.495561  8764 sgd_solver.cpp:136] Iteration 147100, lr = 0.00080625, m = 0.9
I0815 18:18:49.951290  8764 solver.cpp:312] Iteration 147200 (5.7289 iter/s, 17.4554s/100 iter), loss = 1.51572
I0815 18:18:49.951336  8764 solver.cpp:334]     Train net output #0: loss = 1.95641 (* 1 = 1.95641 loss)
I0815 18:18:49.951350  8764 sgd_solver.cpp:136] Iteration 147200, lr = 0.0008, m = 0.9
I0815 18:19:07.653311  8764 solver.cpp:312] Iteration 147300 (5.64923 iter/s, 17.7015s/100 iter), loss = 1.03106
I0815 18:19:07.653388  8764 solver.cpp:334]     Train net output #0: loss = 1.02803 (* 1 = 1.02803 loss)
I0815 18:19:07.653403  8764 sgd_solver.cpp:136] Iteration 147300, lr = 0.00079375, m = 0.9
I0815 18:19:27.797684  8764 solver.cpp:312] Iteration 147400 (4.9643 iter/s, 20.1438s/100 iter), loss = 1.32335
I0815 18:19:27.797710  8764 solver.cpp:334]     Train net output #0: loss = 1.41664 (* 1 = 1.41664 loss)
I0815 18:19:27.797716  8764 sgd_solver.cpp:136] Iteration 147400, lr = 0.0007875, m = 0.9
I0815 18:19:45.468873  8764 solver.cpp:312] Iteration 147500 (5.65909 iter/s, 17.6707s/100 iter), loss = 1.30397
I0815 18:19:45.468971  8764 solver.cpp:334]     Train net output #0: loss = 1.16632 (* 1 = 1.16632 loss)
I0815 18:19:45.468991  8764 sgd_solver.cpp:136] Iteration 147500, lr = 0.00078125, m = 0.9
I0815 18:20:02.325141  8764 solver.cpp:312] Iteration 147600 (5.93267 iter/s, 16.8558s/100 iter), loss = 1.28866
I0815 18:20:02.325173  8764 solver.cpp:334]     Train net output #0: loss = 1.43856 (* 1 = 1.43856 loss)
I0815 18:20:02.325178  8764 sgd_solver.cpp:136] Iteration 147600, lr = 0.000775, m = 0.9
I0815 18:20:20.612856  8764 solver.cpp:312] Iteration 147700 (5.4683 iter/s, 18.2872s/100 iter), loss = 1.37963
I0815 18:20:20.612926  8764 solver.cpp:334]     Train net output #0: loss = 1.32779 (* 1 = 1.32779 loss)
I0815 18:20:20.612934  8764 sgd_solver.cpp:136] Iteration 147700, lr = 0.00076875, m = 0.9
I0815 18:20:38.241430  8764 solver.cpp:312] Iteration 147800 (5.67276 iter/s, 17.6281s/100 iter), loss = 1.701
I0815 18:20:38.241629  8764 solver.cpp:334]     Train net output #0: loss = 1.56866 (* 1 = 1.56866 loss)
I0815 18:20:38.241737  8764 sgd_solver.cpp:136] Iteration 147800, lr = 0.0007625, m = 0.9
I0815 18:20:57.653668  8764 solver.cpp:312] Iteration 147900 (5.15153 iter/s, 19.4117s/100 iter), loss = 1.40123
I0815 18:20:57.653729  8764 solver.cpp:334]     Train net output #0: loss = 1.68219 (* 1 = 1.68219 loss)
I0815 18:20:57.653735  8764 sgd_solver.cpp:136] Iteration 147900, lr = 0.00075625, m = 0.9
I0815 18:21:14.048931  8764 solver.cpp:363] Sparsity after update:
I0815 18:21:14.053117  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:21:14.053128  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:21:14.053134  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:21:14.053138  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:21:14.053143  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:21:14.053145  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:21:14.053148  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:21:14.053151  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:21:14.053156  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:21:14.053159  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:21:14.053164  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:21:14.053166  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:21:14.053169  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:21:14.053179  8764 solver.cpp:509] Iteration 148000, Testing net (#0)
I0815 18:21:43.223417  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.598294
I0815 18:21:43.223523  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.821115
I0815 18:21:43.223534  8764 solver.cpp:594]     Test net output #2: loss = 1.75191 (* 1 = 1.75191 loss)
I0815 18:21:43.223557  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.1696s
I0815 18:21:43.385782  8764 solver.cpp:312] Iteration 148000 (2.18671 iter/s, 45.7309s/100 iter), loss = 0.953746
I0815 18:21:43.385814  8764 solver.cpp:334]     Train net output #0: loss = 1.10621 (* 1 = 1.10621 loss)
I0815 18:21:43.385820  8764 sgd_solver.cpp:136] Iteration 148000, lr = 0.00075, m = 0.9
I0815 18:22:00.613473  8764 solver.cpp:312] Iteration 148100 (5.80477 iter/s, 17.2272s/100 iter), loss = 1.38388
I0815 18:22:00.613497  8764 solver.cpp:334]     Train net output #0: loss = 1.53856 (* 1 = 1.53856 loss)
I0815 18:22:00.613503  8764 sgd_solver.cpp:136] Iteration 148100, lr = 0.00074375, m = 0.9
I0815 18:22:16.949339  8764 solver.cpp:312] Iteration 148200 (6.12167 iter/s, 16.3354s/100 iter), loss = 1.08029
I0815 18:22:16.949405  8764 solver.cpp:334]     Train net output #0: loss = 1.13575 (* 1 = 1.13575 loss)
I0815 18:22:16.949410  8764 sgd_solver.cpp:136] Iteration 148200, lr = 0.0007375, m = 0.9
I0815 18:22:33.588157  8764 solver.cpp:312] Iteration 148300 (6.01021 iter/s, 16.6384s/100 iter), loss = 1.29462
I0815 18:22:33.588222  8764 solver.cpp:334]     Train net output #0: loss = 0.888163 (* 1 = 0.888163 loss)
I0815 18:22:33.588239  8764 sgd_solver.cpp:136] Iteration 148300, lr = 0.00073125, m = 0.9
I0815 18:22:51.810058  8764 solver.cpp:312] Iteration 148400 (5.48805 iter/s, 18.2214s/100 iter), loss = 1.37311
I0815 18:22:51.810150  8764 solver.cpp:334]     Train net output #0: loss = 1.5012 (* 1 = 1.5012 loss)
I0815 18:22:51.810168  8764 sgd_solver.cpp:136] Iteration 148400, lr = 0.000725, m = 0.9
I0815 18:23:11.734082  8764 solver.cpp:312] Iteration 148500 (5.01921 iter/s, 19.9235s/100 iter), loss = 1.18009
I0815 18:23:11.734136  8764 solver.cpp:334]     Train net output #0: loss = 1.20323 (* 1 = 1.20323 loss)
I0815 18:23:11.734148  8764 sgd_solver.cpp:136] Iteration 148500, lr = 0.00071875, m = 0.9
I0815 18:23:29.408939  8764 solver.cpp:312] Iteration 148600 (5.65791 iter/s, 17.6744s/100 iter), loss = 1.48631
I0815 18:23:29.408999  8764 solver.cpp:334]     Train net output #0: loss = 1.36233 (* 1 = 1.36233 loss)
I0815 18:23:29.409006  8764 sgd_solver.cpp:136] Iteration 148600, lr = 0.0007125, m = 0.9
I0815 18:23:47.569486  8764 solver.cpp:312] Iteration 148700 (5.5066 iter/s, 18.16s/100 iter), loss = 1.40502
I0815 18:23:47.569527  8764 solver.cpp:334]     Train net output #0: loss = 1.43613 (* 1 = 1.43613 loss)
I0815 18:23:47.569537  8764 sgd_solver.cpp:136] Iteration 148700, lr = 0.00070625, m = 0.9
I0815 18:24:08.987157  8764 solver.cpp:312] Iteration 148800 (4.66917 iter/s, 21.4171s/100 iter), loss = 1.13462
I0815 18:24:08.987228  8764 solver.cpp:334]     Train net output #0: loss = 0.960904 (* 1 = 0.960904 loss)
I0815 18:24:08.987236  8764 sgd_solver.cpp:136] Iteration 148800, lr = 0.0007, m = 0.9
I0815 18:24:31.011000  8764 solver.cpp:312] Iteration 148900 (4.54066 iter/s, 22.0232s/100 iter), loss = 1.17485
I0815 18:24:31.011023  8764 solver.cpp:334]     Train net output #0: loss = 0.990165 (* 1 = 0.990165 loss)
I0815 18:24:31.011027  8764 sgd_solver.cpp:136] Iteration 148900, lr = 0.00069375, m = 0.9
I0815 18:24:50.457396  8764 solver.cpp:363] Sparsity after update:
I0815 18:24:50.463256  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:24:50.463536  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:24:50.463634  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:24:50.463892  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:24:50.463946  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:24:50.464149  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:24:50.464184  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:24:50.464213  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:24:50.464248  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:24:50.464282  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:24:50.464320  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:24:50.464356  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:24:50.464391  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:24:50.796201  8764 solver.cpp:312] Iteration 149000 (5.05442 iter/s, 19.7846s/100 iter), loss = 1.60309
I0815 18:24:50.796247  8764 solver.cpp:334]     Train net output #0: loss = 1.6185 (* 1 = 1.6185 loss)
I0815 18:24:50.796254  8764 sgd_solver.cpp:136] Iteration 149000, lr = 0.0006875, m = 0.9
I0815 18:25:11.172166  8764 solver.cpp:312] Iteration 149100 (4.90788 iter/s, 20.3754s/100 iter), loss = 1.4493
I0815 18:25:11.172214  8764 solver.cpp:334]     Train net output #0: loss = 1.64427 (* 1 = 1.64427 loss)
I0815 18:25:11.172225  8764 sgd_solver.cpp:136] Iteration 149100, lr = 0.00068125, m = 0.9
I0815 18:25:31.553834  8764 solver.cpp:312] Iteration 149200 (4.90651 iter/s, 20.3811s/100 iter), loss = 1.25752
I0815 18:25:31.553935  8764 solver.cpp:334]     Train net output #0: loss = 1.32981 (* 1 = 1.32981 loss)
I0815 18:25:31.553951  8764 sgd_solver.cpp:136] Iteration 149200, lr = 0.000675, m = 0.9
I0815 18:25:51.174134  8764 solver.cpp:312] Iteration 149300 (5.0969 iter/s, 19.6198s/100 iter), loss = 1.15842
I0815 18:25:51.174161  8764 solver.cpp:334]     Train net output #0: loss = 1.00472 (* 1 = 1.00472 loss)
I0815 18:25:51.174166  8764 sgd_solver.cpp:136] Iteration 149300, lr = 0.00066875, m = 0.9
I0815 18:26:09.737733  8764 solver.cpp:312] Iteration 149400 (5.38704 iter/s, 18.5631s/100 iter), loss = 1.57416
I0815 18:26:09.737799  8764 solver.cpp:334]     Train net output #0: loss = 1.73623 (* 1 = 1.73623 loss)
I0815 18:26:09.737809  8764 sgd_solver.cpp:136] Iteration 149400, lr = 0.0006625, m = 0.9
I0815 18:26:22.359629  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:26:29.145144  8764 solver.cpp:312] Iteration 149500 (5.15281 iter/s, 19.4069s/100 iter), loss = 1.36699
I0815 18:26:29.145171  8764 solver.cpp:334]     Train net output #0: loss = 1.3766 (* 1 = 1.3766 loss)
I0815 18:26:29.145176  8764 sgd_solver.cpp:136] Iteration 149500, lr = 0.00065625, m = 0.9
I0815 18:26:52.337752  8764 solver.cpp:312] Iteration 149600 (4.31184 iter/s, 23.192s/100 iter), loss = 1.34906
I0815 18:26:52.337983  8764 solver.cpp:334]     Train net output #0: loss = 1.36515 (* 1 = 1.36515 loss)
I0815 18:26:52.338003  8764 sgd_solver.cpp:136] Iteration 149600, lr = 0.00065, m = 0.9
I0815 18:27:09.333326  8764 solver.cpp:312] Iteration 149700 (5.88405 iter/s, 16.9951s/100 iter), loss = 1.50831
I0815 18:27:09.333369  8764 solver.cpp:334]     Train net output #0: loss = 1.32581 (* 1 = 1.32581 loss)
I0815 18:27:09.333376  8764 sgd_solver.cpp:136] Iteration 149700, lr = 0.00064375, m = 0.9
I0815 18:27:31.019954  8764 solver.cpp:312] Iteration 149800 (4.61126 iter/s, 21.686s/100 iter), loss = 1.17475
I0815 18:27:31.020043  8764 solver.cpp:334]     Train net output #0: loss = 1.20085 (* 1 = 1.20085 loss)
I0815 18:27:31.020061  8764 sgd_solver.cpp:136] Iteration 149800, lr = 0.0006375, m = 0.9
I0815 18:27:50.891664  8764 solver.cpp:312] Iteration 149900 (5.03242 iter/s, 19.8712s/100 iter), loss = 1.13607
I0815 18:27:50.891693  8764 solver.cpp:334]     Train net output #0: loss = 1.10972 (* 1 = 1.10972 loss)
I0815 18:27:50.891700  8764 sgd_solver.cpp:136] Iteration 149900, lr = 0.00063125, m = 0.9
I0815 18:28:10.720301  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_150000.caffemodel
I0815 18:28:11.453502  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_150000.solverstate
I0815 18:28:11.460384  8764 solver.cpp:363] Sparsity after update:
I0815 18:28:11.462697  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:28:11.462713  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:28:11.462725  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:28:11.462728  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:28:11.462733  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:28:11.462736  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:28:11.462739  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:28:11.462743  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:28:11.462745  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:28:11.462749  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:28:11.462751  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:28:11.462754  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:28:11.462757  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:28:11.462769  8764 solver.cpp:509] Iteration 150000, Testing net (#0)
I0815 18:28:45.506237  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.595353
I0815 18:28:45.506284  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.819879
I0815 18:28:45.506292  8764 solver.cpp:594]     Test net output #2: loss = 1.77304 (* 1 = 1.77304 loss)
I0815 18:28:45.506314  8764 solver.cpp:264] [MultiGPU] Tests completed in 34.0426s
I0815 18:28:45.655216  8764 solver.cpp:312] Iteration 150000 (1.82608 iter/s, 54.762s/100 iter), loss = 1.43232
I0815 18:28:45.655267  8764 solver.cpp:334]     Train net output #0: loss = 1.59865 (* 1 = 1.59865 loss)
I0815 18:28:45.655282  8764 sgd_solver.cpp:136] Iteration 150000, lr = 0.000625, m = 0.9
I0815 18:29:06.778203  8764 solver.cpp:312] Iteration 150100 (4.73432 iter/s, 21.1224s/100 iter), loss = 1.00255
I0815 18:29:06.778342  8764 solver.cpp:334]     Train net output #0: loss = 1.06394 (* 1 = 1.06394 loss)
I0815 18:29:06.778390  8764 sgd_solver.cpp:136] Iteration 150100, lr = 0.00061875, m = 0.9
I0815 18:29:25.715240  8764 solver.cpp:312] Iteration 150200 (5.2808 iter/s, 18.9365s/100 iter), loss = 1.15076
I0815 18:29:25.715296  8764 solver.cpp:334]     Train net output #0: loss = 1.16023 (* 1 = 1.16023 loss)
I0815 18:29:25.715301  8764 sgd_solver.cpp:136] Iteration 150200, lr = 0.0006125, m = 0.9
I0815 18:29:47.223006  8764 solver.cpp:312] Iteration 150300 (4.64961 iter/s, 21.5072s/100 iter), loss = 1.23269
I0815 18:29:47.223034  8764 solver.cpp:334]     Train net output #0: loss = 1.07968 (* 1 = 1.07968 loss)
I0815 18:29:47.223040  8764 sgd_solver.cpp:136] Iteration 150300, lr = 0.00060625, m = 0.9
I0815 18:30:08.687047  8764 solver.cpp:312] Iteration 150400 (4.65908 iter/s, 21.4634s/100 iter), loss = 1.45584
I0815 18:30:08.687104  8764 solver.cpp:334]     Train net output #0: loss = 1.42681 (* 1 = 1.42681 loss)
I0815 18:30:08.687111  8764 sgd_solver.cpp:136] Iteration 150400, lr = 0.0006, m = 0.9
I0815 18:30:28.392871  8764 solver.cpp:312] Iteration 150500 (5.07478 iter/s, 19.7053s/100 iter), loss = 1.29201
I0815 18:30:28.392897  8764 solver.cpp:334]     Train net output #0: loss = 1.14599 (* 1 = 1.14599 loss)
I0815 18:30:28.392902  8764 sgd_solver.cpp:136] Iteration 150500, lr = 0.00059375, m = 0.9
I0815 18:30:45.299581  8764 solver.cpp:312] Iteration 150600 (5.91498 iter/s, 16.9062s/100 iter), loss = 0.933011
I0815 18:30:45.299746  8764 solver.cpp:334]     Train net output #0: loss = 0.951353 (* 1 = 0.951353 loss)
I0815 18:30:45.299778  8764 sgd_solver.cpp:136] Iteration 150600, lr = 0.0005875, m = 0.9
I0815 18:31:06.404839  8764 solver.cpp:312] Iteration 150700 (4.73829 iter/s, 21.1047s/100 iter), loss = 1.40058
I0815 18:31:06.405057  8764 solver.cpp:334]     Train net output #0: loss = 1.58134 (* 1 = 1.58134 loss)
I0815 18:31:06.405167  8764 sgd_solver.cpp:136] Iteration 150700, lr = 0.00058125, m = 0.9
I0815 18:31:26.867621  8764 solver.cpp:312] Iteration 150800 (4.88706 iter/s, 20.4622s/100 iter), loss = 1.6061
I0815 18:31:26.867681  8764 solver.cpp:334]     Train net output #0: loss = 1.6731 (* 1 = 1.6731 loss)
I0815 18:31:26.867686  8764 sgd_solver.cpp:136] Iteration 150800, lr = 0.000575, m = 0.9
I0815 18:31:45.967932  8764 solver.cpp:312] Iteration 150900 (5.23566 iter/s, 19.0998s/100 iter), loss = 1.19172
I0815 18:31:45.967998  8764 solver.cpp:334]     Train net output #0: loss = 0.923005 (* 1 = 0.923005 loss)
I0815 18:31:45.968014  8764 sgd_solver.cpp:136] Iteration 150900, lr = 0.00056875, m = 0.9
I0815 18:32:04.068944  8764 solver.cpp:363] Sparsity after update:
I0815 18:32:04.080021  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:32:04.080034  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:32:04.080042  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:32:04.080046  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:32:04.080049  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:32:04.080055  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:32:04.080058  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:32:04.080062  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:32:04.080065  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:32:04.080067  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:32:04.080070  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:32:04.080073  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:32:04.080076  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:32:04.215924  8764 solver.cpp:312] Iteration 151000 (5.48021 iter/s, 18.2475s/100 iter), loss = 1.22529
I0815 18:32:04.215991  8764 solver.cpp:334]     Train net output #0: loss = 0.946673 (* 1 = 0.946673 loss)
I0815 18:32:04.216009  8764 sgd_solver.cpp:136] Iteration 151000, lr = 0.0005625, m = 0.9
I0815 18:32:23.696883  8764 solver.cpp:312] Iteration 151100 (5.13336 iter/s, 19.4804s/100 iter), loss = 1.23055
I0815 18:32:23.696921  8764 solver.cpp:334]     Train net output #0: loss = 1.28593 (* 1 = 1.28593 loss)
I0815 18:32:23.696928  8764 sgd_solver.cpp:136] Iteration 151100, lr = 0.00055625, m = 0.9
I0815 18:32:42.246331  8764 solver.cpp:312] Iteration 151200 (5.39114 iter/s, 18.5489s/100 iter), loss = 1.51923
I0815 18:32:42.246395  8764 solver.cpp:334]     Train net output #0: loss = 1.6087 (* 1 = 1.6087 loss)
I0815 18:32:42.246402  8764 sgd_solver.cpp:136] Iteration 151200, lr = 0.00055, m = 0.9
I0815 18:32:57.503809  8764 solver.cpp:312] Iteration 151300 (6.55435 iter/s, 15.2571s/100 iter), loss = 1.15101
I0815 18:32:57.503839  8764 solver.cpp:334]     Train net output #0: loss = 1.30019 (* 1 = 1.30019 loss)
I0815 18:32:57.503845  8764 sgd_solver.cpp:136] Iteration 151300, lr = 0.00054375, m = 0.9
I0815 18:33:17.384719  8764 solver.cpp:312] Iteration 151400 (5.03009 iter/s, 19.8804s/100 iter), loss = 1.35276
I0815 18:33:17.384814  8764 solver.cpp:334]     Train net output #0: loss = 1.43692 (* 1 = 1.43692 loss)
I0815 18:33:17.384824  8764 sgd_solver.cpp:136] Iteration 151400, lr = 0.0005375, m = 0.9
I0815 18:33:36.718017  8764 solver.cpp:312] Iteration 151500 (5.17256 iter/s, 19.3328s/100 iter), loss = 1.97151
I0815 18:33:36.718080  8764 solver.cpp:334]     Train net output #0: loss = 2.19349 (* 1 = 2.19349 loss)
I0815 18:33:36.718096  8764 sgd_solver.cpp:136] Iteration 151500, lr = 0.00053125, m = 0.9
I0815 18:33:58.061908  8764 solver.cpp:312] Iteration 151600 (4.68531 iter/s, 21.3433s/100 iter), loss = 1.30797
I0815 18:33:58.064055  8764 solver.cpp:334]     Train net output #0: loss = 1.10491 (* 1 = 1.10491 loss)
I0815 18:33:58.064064  8764 sgd_solver.cpp:136] Iteration 151600, lr = 0.000525, m = 0.9
I0815 18:34:13.996166  8764 solver.cpp:312] Iteration 151700 (6.27596 iter/s, 15.9338s/100 iter), loss = 1.69743
I0815 18:34:13.996194  8764 solver.cpp:334]     Train net output #0: loss = 1.84674 (* 1 = 1.84674 loss)
I0815 18:34:13.996201  8764 sgd_solver.cpp:136] Iteration 151700, lr = 0.00051875, m = 0.9
I0815 18:34:33.494230  8764 solver.cpp:312] Iteration 151800 (5.12886 iter/s, 19.4975s/100 iter), loss = 1.25187
I0815 18:34:33.494832  8764 solver.cpp:334]     Train net output #0: loss = 1.57915 (* 1 = 1.57915 loss)
I0815 18:34:33.494860  8764 sgd_solver.cpp:136] Iteration 151800, lr = 0.0005125, m = 0.9
I0815 18:34:51.897344  8764 solver.cpp:312] Iteration 151900 (5.43401 iter/s, 18.4026s/100 iter), loss = 1.94326
I0815 18:34:51.897385  8764 solver.cpp:334]     Train net output #0: loss = 2.24104 (* 1 = 2.24104 loss)
I0815 18:34:51.897399  8764 sgd_solver.cpp:136] Iteration 151900, lr = 0.00050625, m = 0.9
I0815 18:35:10.725005  8764 solver.cpp:363] Sparsity after update:
I0815 18:35:10.729004  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:35:10.729146  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:35:10.729241  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:35:10.729332  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:35:10.729423  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:35:10.729516  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:35:10.729607  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:35:10.729696  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:35:10.729784  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:35:10.729887  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:35:10.729982  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:35:10.730072  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:35:10.730165  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:35:10.730278  8764 solver.cpp:509] Iteration 152000, Testing net (#0)
I0815 18:35:39.845170  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.598764
I0815 18:35:39.845216  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.824056
I0815 18:35:39.845237  8764 solver.cpp:594]     Test net output #2: loss = 1.7396 (* 1 = 1.7396 loss)
I0815 18:35:39.845289  8764 solver.cpp:264] [MultiGPU] Tests completed in 29.1142s
I0815 18:35:40.132589  8764 solver.cpp:312] Iteration 152000 (2.07323 iter/s, 48.2339s/100 iter), loss = 1.41104
I0815 18:35:40.132654  8764 solver.cpp:334]     Train net output #0: loss = 1.01892 (* 1 = 1.01892 loss)
I0815 18:35:40.132670  8764 sgd_solver.cpp:136] Iteration 152000, lr = 0.0005, m = 0.9
I0815 18:35:56.351090  8764 solver.cpp:312] Iteration 152100 (6.16597 iter/s, 16.218s/100 iter), loss = 1.63816
I0815 18:35:56.351186  8764 solver.cpp:334]     Train net output #0: loss = 1.9253 (* 1 = 1.9253 loss)
I0815 18:35:56.351209  8764 sgd_solver.cpp:136] Iteration 152100, lr = 0.00049375, m = 0.9
I0815 18:35:57.839457  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:36:15.620582  8764 solver.cpp:312] Iteration 152200 (5.18969 iter/s, 19.269s/100 iter), loss = 1.25786
I0815 18:36:15.620610  8764 solver.cpp:334]     Train net output #0: loss = 1.11758 (* 1 = 1.11758 loss)
I0815 18:36:15.620616  8764 sgd_solver.cpp:136] Iteration 152200, lr = 0.0004875, m = 0.9
I0815 18:36:33.645117  8764 solver.cpp:312] Iteration 152300 (5.54815 iter/s, 18.024s/100 iter), loss = 1.23997
I0815 18:36:33.645179  8764 solver.cpp:334]     Train net output #0: loss = 1.01218 (* 1 = 1.01218 loss)
I0815 18:36:33.645186  8764 sgd_solver.cpp:136] Iteration 152300, lr = 0.00048125, m = 0.9
I0815 18:36:51.432621  8764 solver.cpp:312] Iteration 152400 (5.62208 iter/s, 17.787s/100 iter), loss = 1.58834
I0815 18:36:51.432845  8764 solver.cpp:334]     Train net output #0: loss = 1.51593 (* 1 = 1.51593 loss)
I0815 18:36:51.432962  8764 sgd_solver.cpp:136] Iteration 152400, lr = 0.000475, m = 0.9
I0815 18:37:09.708428  8764 solver.cpp:312] Iteration 152500 (5.47187 iter/s, 18.2753s/100 iter), loss = 1.28746
I0815 18:37:09.708549  8764 solver.cpp:334]     Train net output #0: loss = 1.62995 (* 1 = 1.62995 loss)
I0815 18:37:09.708562  8764 sgd_solver.cpp:136] Iteration 152500, lr = 0.00046875, m = 0.9
I0815 18:37:31.352792  8764 solver.cpp:312] Iteration 152600 (4.62027 iter/s, 21.6438s/100 iter), loss = 1.47707
I0815 18:37:31.352818  8764 solver.cpp:334]     Train net output #0: loss = 1.62689 (* 1 = 1.62689 loss)
I0815 18:37:31.352823  8764 sgd_solver.cpp:136] Iteration 152600, lr = 0.0004625, m = 0.9
I0815 18:37:48.125290  8764 solver.cpp:312] Iteration 152700 (5.96231 iter/s, 16.772s/100 iter), loss = 1.02447
I0815 18:37:48.125550  8764 solver.cpp:334]     Train net output #0: loss = 1.13782 (* 1 = 1.13782 loss)
I0815 18:37:48.125660  8764 sgd_solver.cpp:136] Iteration 152700, lr = 0.00045625, m = 0.9
I0815 18:38:05.260617  8764 solver.cpp:312] Iteration 152800 (5.83606 iter/s, 17.1349s/100 iter), loss = 1.19726
I0815 18:38:05.260643  8764 solver.cpp:334]     Train net output #0: loss = 1.23304 (* 1 = 1.23304 loss)
I0815 18:38:05.260649  8764 sgd_solver.cpp:136] Iteration 152800, lr = 0.00045, m = 0.9
I0815 18:38:22.648862  8764 solver.cpp:312] Iteration 152900 (5.75117 iter/s, 17.3878s/100 iter), loss = 1.35153
I0815 18:38:22.648927  8764 solver.cpp:334]     Train net output #0: loss = 1.34392 (* 1 = 1.34392 loss)
I0815 18:38:22.648934  8764 sgd_solver.cpp:136] Iteration 152900, lr = 0.00044375, m = 0.9
I0815 18:38:39.218204  8764 solver.cpp:363] Sparsity after update:
I0815 18:38:39.230659  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:38:39.230674  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:38:39.230682  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:38:39.230685  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:38:39.230690  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:38:39.230705  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:38:39.230713  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:38:39.230721  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:38:39.230728  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:38:39.230736  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:38:39.230743  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:38:39.230751  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:38:39.230759  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:38:39.371408  8764 solver.cpp:312] Iteration 153000 (5.98012 iter/s, 16.7221s/100 iter), loss = 1.96595
I0815 18:38:39.371435  8764 solver.cpp:334]     Train net output #0: loss = 1.71888 (* 1 = 1.71888 loss)
I0815 18:38:39.371441  8764 sgd_solver.cpp:136] Iteration 153000, lr = 0.0004375, m = 0.9
I0815 18:38:55.656560  8764 solver.cpp:312] Iteration 153100 (6.14073 iter/s, 16.2847s/100 iter), loss = 1.41584
I0815 18:38:55.656666  8764 solver.cpp:334]     Train net output #0: loss = 1.53891 (* 1 = 1.53891 loss)
I0815 18:38:55.656687  8764 sgd_solver.cpp:136] Iteration 153100, lr = 0.00043125, m = 0.9
I0815 18:39:12.923862  8764 solver.cpp:312] Iteration 153200 (5.79145 iter/s, 17.2668s/100 iter), loss = 1.23655
I0815 18:39:12.923887  8764 solver.cpp:334]     Train net output #0: loss = 1.1194 (* 1 = 1.1194 loss)
I0815 18:39:12.923892  8764 sgd_solver.cpp:136] Iteration 153200, lr = 0.000425, m = 0.9
I0815 18:39:30.870236  8764 solver.cpp:312] Iteration 153300 (5.57231 iter/s, 17.9459s/100 iter), loss = 1.40606
I0815 18:39:30.870291  8764 solver.cpp:334]     Train net output #0: loss = 1.42648 (* 1 = 1.42648 loss)
I0815 18:39:30.870298  8764 sgd_solver.cpp:136] Iteration 153300, lr = 0.00041875, m = 0.9
I0815 18:39:48.920145  8764 solver.cpp:312] Iteration 153400 (5.54035 iter/s, 18.0494s/100 iter), loss = 1.57363
I0815 18:39:48.920215  8764 solver.cpp:334]     Train net output #0: loss = 1.4369 (* 1 = 1.4369 loss)
I0815 18:39:48.920233  8764 sgd_solver.cpp:136] Iteration 153400, lr = 0.0004125, m = 0.9
I0815 18:40:04.744233  8764 solver.cpp:312] Iteration 153500 (6.31965 iter/s, 15.8236s/100 iter), loss = 1.07969
I0815 18:40:04.744349  8764 solver.cpp:334]     Train net output #0: loss = 1.10668 (* 1 = 1.10668 loss)
I0815 18:40:04.744366  8764 sgd_solver.cpp:136] Iteration 153500, lr = 0.00040625, m = 0.9
I0815 18:40:24.102942  8764 solver.cpp:312] Iteration 153600 (5.16579 iter/s, 19.3581s/100 iter), loss = 1.17691
I0815 18:40:24.103044  8764 solver.cpp:334]     Train net output #0: loss = 0.976355 (* 1 = 0.976355 loss)
I0815 18:40:24.103091  8764 sgd_solver.cpp:136] Iteration 153600, lr = 0.0004, m = 0.9
I0815 18:40:45.753482  8764 solver.cpp:312] Iteration 153700 (4.61895 iter/s, 21.65s/100 iter), loss = 1.56353
I0815 18:40:45.753837  8764 solver.cpp:334]     Train net output #0: loss = 1.36288 (* 1 = 1.36288 loss)
I0815 18:40:45.753945  8764 sgd_solver.cpp:136] Iteration 153700, lr = 0.00039375, m = 0.9
I0815 18:41:10.269384  8764 solver.cpp:312] Iteration 153800 (4.0791 iter/s, 24.5152s/100 iter), loss = 1.50103
I0815 18:41:10.269410  8764 solver.cpp:334]     Train net output #0: loss = 1.79272 (* 1 = 1.79272 loss)
I0815 18:41:10.269417  8764 sgd_solver.cpp:136] Iteration 153800, lr = 0.0003875, m = 0.9
I0815 18:41:28.730636  8764 solver.cpp:312] Iteration 153900 (5.4169 iter/s, 18.4607s/100 iter), loss = 1.21363
I0815 18:41:28.733196  8764 solver.cpp:334]     Train net output #0: loss = 1.37718 (* 1 = 1.37718 loss)
I0815 18:41:28.733225  8764 sgd_solver.cpp:136] Iteration 153900, lr = 0.00038125, m = 0.9
I0815 18:41:50.036556  8764 solver.cpp:363] Sparsity after update:
I0815 18:41:50.040364  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:41:50.040374  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:41:50.040381  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:41:50.040395  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:41:50.040405  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:41:50.040415  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:41:50.040422  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:41:50.040431  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:41:50.040439  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:41:50.040448  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:41:50.040457  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:41:50.040467  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:41:50.040477  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:41:50.040495  8764 solver.cpp:509] Iteration 154000, Testing net (#0)
I0815 18:42:28.575345  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.595
I0815 18:42:28.575445  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.817762
I0815 18:42:28.575455  8764 solver.cpp:594]     Test net output #2: loss = 1.76586 (* 1 = 1.76586 loss)
I0815 18:42:28.575475  8764 solver.cpp:264] [MultiGPU] Tests completed in 38.5339s
I0815 18:42:28.748162  8764 solver.cpp:312] Iteration 154000 (1.66623 iter/s, 60.0159s/100 iter), loss = 1.16872
I0815 18:42:28.748189  8764 solver.cpp:334]     Train net output #0: loss = 1.31151 (* 1 = 1.31151 loss)
I0815 18:42:28.748195  8764 sgd_solver.cpp:136] Iteration 154000, lr = 0.000375, m = 0.9
I0815 18:42:50.195693  8764 solver.cpp:312] Iteration 154100 (4.66268 iter/s, 21.4469s/100 iter), loss = 1.42239
I0815 18:42:50.195894  8764 solver.cpp:334]     Train net output #0: loss = 1.23233 (* 1 = 1.23233 loss)
I0815 18:42:50.195955  8764 sgd_solver.cpp:136] Iteration 154100, lr = 0.00036875, m = 0.9
I0815 18:43:10.727771  8764 solver.cpp:312] Iteration 154200 (4.87056 iter/s, 20.5315s/100 iter), loss = 1.4063
I0815 18:43:10.727905  8764 solver.cpp:334]     Train net output #0: loss = 1.48207 (* 1 = 1.48207 loss)
I0815 18:43:10.727921  8764 sgd_solver.cpp:136] Iteration 154200, lr = 0.0003625, m = 0.9
I0815 18:43:29.665632  8764 solver.cpp:312] Iteration 154300 (5.28057 iter/s, 18.9373s/100 iter), loss = 1.48915
I0815 18:43:29.665697  8764 solver.cpp:334]     Train net output #0: loss = 1.7187 (* 1 = 1.7187 loss)
I0815 18:43:29.665716  8764 sgd_solver.cpp:136] Iteration 154300, lr = 0.00035625, m = 0.9
I0815 18:43:48.284703  8764 solver.cpp:312] Iteration 154400 (5.37099 iter/s, 18.6185s/100 iter), loss = 1.31694
I0815 18:43:48.284821  8764 solver.cpp:334]     Train net output #0: loss = 1.4617 (* 1 = 1.4617 loss)
I0815 18:43:48.284837  8764 sgd_solver.cpp:136] Iteration 154400, lr = 0.00035, m = 0.9
I0815 18:44:09.060143  8764 solver.cpp:312] Iteration 154500 (4.81437 iter/s, 20.7712s/100 iter), loss = 1.27142
I0815 18:44:09.060216  8764 solver.cpp:334]     Train net output #0: loss = 1.35171 (* 1 = 1.35171 loss)
I0815 18:44:09.060235  8764 sgd_solver.cpp:136] Iteration 154500, lr = 0.00034375, m = 0.9
I0815 18:44:28.131482  8764 solver.cpp:312] Iteration 154600 (5.2426 iter/s, 19.0745s/100 iter), loss = 1.00581
I0815 18:44:28.131561  8764 solver.cpp:334]     Train net output #0: loss = 1.20939 (* 1 = 1.20939 loss)
I0815 18:44:28.131572  8764 sgd_solver.cpp:136] Iteration 154600, lr = 0.0003375, m = 0.9
I0815 18:44:45.821696  8764 solver.cpp:312] Iteration 154700 (5.653 iter/s, 17.6897s/100 iter), loss = 1.6365
I0815 18:44:45.821722  8764 solver.cpp:334]     Train net output #0: loss = 1.47154 (* 1 = 1.47154 loss)
I0815 18:44:45.821727  8764 sgd_solver.cpp:136] Iteration 154700, lr = 0.00033125, m = 0.9
I0815 18:45:04.731978  8764 solver.cpp:312] Iteration 154800 (5.28828 iter/s, 18.9098s/100 iter), loss = 1.16141
I0815 18:45:04.732070  8764 solver.cpp:334]     Train net output #0: loss = 1.09887 (* 1 = 1.09887 loss)
I0815 18:45:04.732086  8764 sgd_solver.cpp:136] Iteration 154800, lr = 0.000325, m = 0.9
I0815 18:45:25.410691  8764 solver.cpp:312] Iteration 154900 (4.83603 iter/s, 20.6781s/100 iter), loss = 1.15684
I0815 18:45:25.410782  8764 solver.cpp:334]     Train net output #0: loss = 0.904558 (* 1 = 0.904558 loss)
I0815 18:45:25.410809  8764 sgd_solver.cpp:136] Iteration 154900, lr = 0.00031875, m = 0.9
I0815 18:45:44.878413  8764 solver.cpp:363] Sparsity after update:
I0815 18:45:44.888825  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:45:44.888839  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:45:44.888849  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:45:44.888852  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:45:44.888856  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:45:44.888860  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:45:44.888864  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:45:44.888866  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:45:44.888870  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:45:44.888872  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:45:44.888875  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:45:44.888877  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:45:44.888881  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:45:45.052816  8764 solver.cpp:312] Iteration 155000 (5.09124 iter/s, 19.6416s/100 iter), loss = 1.23533
I0815 18:45:45.052839  8764 solver.cpp:334]     Train net output #0: loss = 1.02986 (* 1 = 1.02986 loss)
I0815 18:45:45.052845  8764 sgd_solver.cpp:136] Iteration 155000, lr = 0.0003125, m = 0.9
I0815 18:46:01.943240  8764 solver.cpp:312] Iteration 155100 (5.92068 iter/s, 16.89s/100 iter), loss = 1.12267
I0815 18:46:01.943266  8764 solver.cpp:334]     Train net output #0: loss = 1.02877 (* 1 = 1.02877 loss)
I0815 18:46:01.943272  8764 sgd_solver.cpp:136] Iteration 155100, lr = 0.00030625, m = 0.9
I0815 18:46:21.578686  8764 solver.cpp:312] Iteration 155200 (5.09297 iter/s, 19.6349s/100 iter), loss = 1.24077
I0815 18:46:21.578794  8764 solver.cpp:334]     Train net output #0: loss = 1.09811 (* 1 = 1.09811 loss)
I0815 18:46:21.578807  8764 sgd_solver.cpp:136] Iteration 155200, lr = 0.0003, m = 0.9
I0815 18:46:40.829329  8764 solver.cpp:312] Iteration 155300 (5.19477 iter/s, 19.2501s/100 iter), loss = 1.32439
I0815 18:46:40.829545  8764 solver.cpp:334]     Train net output #0: loss = 1.16146 (* 1 = 1.16146 loss)
I0815 18:46:40.829654  8764 sgd_solver.cpp:136] Iteration 155300, lr = 0.00029375, m = 0.9
I0815 18:46:59.990130  8764 solver.cpp:312] Iteration 155400 (5.21913 iter/s, 19.1603s/100 iter), loss = 1.23928
I0815 18:46:59.990218  8764 solver.cpp:334]     Train net output #0: loss = 1.17727 (* 1 = 1.17727 loss)
I0815 18:46:59.990236  8764 sgd_solver.cpp:136] Iteration 155400, lr = 0.0002875, m = 0.9
I0815 18:47:17.016304  8764 solver.cpp:312] Iteration 155500 (5.87348 iter/s, 17.0257s/100 iter), loss = 1.28226
I0815 18:47:17.016355  8764 solver.cpp:334]     Train net output #0: loss = 1.1649 (* 1 = 1.1649 loss)
I0815 18:47:17.016366  8764 sgd_solver.cpp:136] Iteration 155500, lr = 0.00028125, m = 0.9
I0815 18:47:38.730672  8764 solver.cpp:312] Iteration 155600 (4.60537 iter/s, 21.7138s/100 iter), loss = 1.49242
I0815 18:47:38.730725  8764 solver.cpp:334]     Train net output #0: loss = 1.52554 (* 1 = 1.52554 loss)
I0815 18:47:38.730728  8764 sgd_solver.cpp:136] Iteration 155600, lr = 0.000275, m = 0.9
I0815 18:47:56.452909  8764 solver.cpp:312] Iteration 155700 (5.64278 iter/s, 17.7217s/100 iter), loss = 1.40468
I0815 18:47:56.452931  8764 solver.cpp:334]     Train net output #0: loss = 1.15485 (* 1 = 1.15485 loss)
I0815 18:47:56.452935  8764 sgd_solver.cpp:136] Iteration 155700, lr = 0.00026875, m = 0.9
I0815 18:48:15.494017  8764 solver.cpp:312] Iteration 155800 (5.25194 iter/s, 19.0406s/100 iter), loss = 1.26209
I0815 18:48:15.494102  8764 solver.cpp:334]     Train net output #0: loss = 1.84643 (* 1 = 1.84643 loss)
I0815 18:48:15.494119  8764 sgd_solver.cpp:136] Iteration 155800, lr = 0.0002625, m = 0.9
I0815 18:48:35.472816  8764 solver.cpp:312] Iteration 155900 (5.00544 iter/s, 19.9782s/100 iter), loss = 1.29878
I0815 18:48:35.472837  8764 solver.cpp:334]     Train net output #0: loss = 1.4207 (* 1 = 1.4207 loss)
I0815 18:48:35.472843  8764 sgd_solver.cpp:136] Iteration 155900, lr = 0.00025625, m = 0.9
I0815 18:48:54.588459  8764 solver.cpp:363] Sparsity after update:
I0815 18:48:54.598816  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:48:54.598842  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:48:54.598853  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:48:54.598856  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:48:54.598860  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:48:54.598863  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:48:54.598870  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:48:54.598872  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:48:54.598875  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:48:54.598878  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:48:54.598881  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:48:54.598884  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:48:54.598887  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:48:54.598899  8764 solver.cpp:509] Iteration 156000, Testing net (#0)
I0815 18:49:06.967571  8766 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 18:49:25.279548  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.591941
I0815 18:49:25.279600  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.822585
I0815 18:49:25.279606  8764 solver.cpp:594]     Test net output #2: loss = 1.763 (* 1 = 1.763 loss)
I0815 18:49:25.279628  8764 solver.cpp:264] [MultiGPU] Tests completed in 30.6799s
I0815 18:49:25.450697  8764 solver.cpp:312] Iteration 156000 (2.00094 iter/s, 49.9765s/100 iter), loss = 1.33885
I0815 18:49:25.450728  8764 solver.cpp:334]     Train net output #0: loss = 0.956053 (* 1 = 0.956053 loss)
I0815 18:49:25.450736  8764 sgd_solver.cpp:136] Iteration 156000, lr = 0.00025, m = 0.9
I0815 18:49:44.420570  8764 solver.cpp:312] Iteration 156100 (5.27166 iter/s, 18.9693s/100 iter), loss = 1.79744
I0815 18:49:44.420598  8764 solver.cpp:334]     Train net output #0: loss = 1.53345 (* 1 = 1.53345 loss)
I0815 18:49:44.420604  8764 sgd_solver.cpp:136] Iteration 156100, lr = 0.00024375, m = 0.9
I0815 18:50:02.348085  8764 solver.cpp:312] Iteration 156200 (5.57817 iter/s, 17.927s/100 iter), loss = 1.3352
I0815 18:50:02.348199  8764 solver.cpp:334]     Train net output #0: loss = 1.41296 (* 1 = 1.41296 loss)
I0815 18:50:02.348218  8764 sgd_solver.cpp:136] Iteration 156200, lr = 0.0002375, m = 0.9
I0815 18:50:22.292476  8764 solver.cpp:312] Iteration 156300 (5.01408 iter/s, 19.9438s/100 iter), loss = 1.48692
I0815 18:50:22.292502  8764 solver.cpp:334]     Train net output #0: loss = 1.79811 (* 1 = 1.79811 loss)
I0815 18:50:22.292507  8764 sgd_solver.cpp:136] Iteration 156300, lr = 0.00023125, m = 0.9
I0815 18:50:41.927470  8764 solver.cpp:312] Iteration 156400 (5.09309 iter/s, 19.6344s/100 iter), loss = 1.17254
I0815 18:50:41.927536  8764 solver.cpp:334]     Train net output #0: loss = 1.15651 (* 1 = 1.15651 loss)
I0815 18:50:41.927546  8764 sgd_solver.cpp:136] Iteration 156400, lr = 0.000225, m = 0.9
I0815 18:51:00.399458  8764 solver.cpp:312] Iteration 156500 (5.41375 iter/s, 18.4715s/100 iter), loss = 1.29194
I0815 18:51:00.399642  8764 solver.cpp:334]     Train net output #0: loss = 1.31863 (* 1 = 1.31863 loss)
I0815 18:51:00.399732  8764 sgd_solver.cpp:136] Iteration 156500, lr = 0.00021875, m = 0.9
I0815 18:51:19.857738  8764 solver.cpp:312] Iteration 156600 (5.13934 iter/s, 19.4577s/100 iter), loss = 1.36625
I0815 18:51:19.857796  8764 solver.cpp:334]     Train net output #0: loss = 1.53626 (* 1 = 1.53626 loss)
I0815 18:51:19.857803  8764 sgd_solver.cpp:136] Iteration 156600, lr = 0.0002125, m = 0.9
I0815 18:51:39.703359  8764 solver.cpp:312] Iteration 156700 (5.03904 iter/s, 19.8451s/100 iter), loss = 1.13723
I0815 18:51:39.703400  8764 solver.cpp:334]     Train net output #0: loss = 1.08302 (* 1 = 1.08302 loss)
I0815 18:51:39.703408  8764 sgd_solver.cpp:136] Iteration 156700, lr = 0.00020625, m = 0.9
I0815 18:51:56.900867  8764 solver.cpp:312] Iteration 156800 (5.81496 iter/s, 17.197s/100 iter), loss = 1.46413
I0815 18:51:56.900964  8764 solver.cpp:334]     Train net output #0: loss = 1.38787 (* 1 = 1.38787 loss)
I0815 18:51:56.900981  8764 sgd_solver.cpp:136] Iteration 156800, lr = 0.0002, m = 0.9
I0815 18:52:17.465224  8764 solver.cpp:312] Iteration 156900 (4.86292 iter/s, 20.5638s/100 iter), loss = 1.19512
I0815 18:52:17.465275  8764 solver.cpp:334]     Train net output #0: loss = 1.10139 (* 1 = 1.10139 loss)
I0815 18:52:17.465288  8764 sgd_solver.cpp:136] Iteration 156900, lr = 0.00019375, m = 0.9
I0815 18:52:36.403556  8764 solver.cpp:363] Sparsity after update:
I0815 18:52:36.417870  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:52:36.417912  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:52:36.417932  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:52:36.417945  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:52:36.417958  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:52:36.417970  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:52:36.417984  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:52:36.417995  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:52:36.418007  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:52:36.418020  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:52:36.418032  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:52:36.418045  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:52:36.418057  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:52:36.559213  8764 solver.cpp:312] Iteration 157000 (5.23739 iter/s, 19.0935s/100 iter), loss = 1.30169
I0815 18:52:36.559242  8764 solver.cpp:334]     Train net output #0: loss = 1.22552 (* 1 = 1.22552 loss)
I0815 18:52:36.559247  8764 sgd_solver.cpp:136] Iteration 157000, lr = 0.0001875, m = 0.9
I0815 18:52:54.445412  8764 solver.cpp:312] Iteration 157100 (5.59106 iter/s, 17.8857s/100 iter), loss = 1.09729
I0815 18:52:54.445441  8764 solver.cpp:334]     Train net output #0: loss = 1.3234 (* 1 = 1.3234 loss)
I0815 18:52:54.445446  8764 sgd_solver.cpp:136] Iteration 157100, lr = 0.00018125, m = 0.9
I0815 18:53:10.117681  8764 solver.cpp:312] Iteration 157200 (6.38088 iter/s, 15.6718s/100 iter), loss = 1.72391
I0815 18:53:10.117817  8764 solver.cpp:334]     Train net output #0: loss = 2.03706 (* 1 = 2.03706 loss)
I0815 18:53:10.117841  8764 sgd_solver.cpp:136] Iteration 157200, lr = 0.000175, m = 0.9
I0815 18:53:29.774924  8764 solver.cpp:312] Iteration 157300 (5.08732 iter/s, 19.6567s/100 iter), loss = 1.61389
I0815 18:53:29.774947  8764 solver.cpp:334]     Train net output #0: loss = 1.54747 (* 1 = 1.54747 loss)
I0815 18:53:29.774952  8764 sgd_solver.cpp:136] Iteration 157300, lr = 0.00016875, m = 0.9
I0815 18:53:49.892237  8764 solver.cpp:312] Iteration 157400 (4.97098 iter/s, 20.1168s/100 iter), loss = 1.35175
I0815 18:53:49.892320  8764 solver.cpp:334]     Train net output #0: loss = 1.2872 (* 1 = 1.2872 loss)
I0815 18:53:49.892333  8764 sgd_solver.cpp:136] Iteration 157400, lr = 0.0001625, m = 0.9
I0815 18:54:07.419236  8764 solver.cpp:312] Iteration 157500 (5.70564 iter/s, 17.5265s/100 iter), loss = 1.17605
I0815 18:54:07.419286  8764 solver.cpp:334]     Train net output #0: loss = 1.33095 (* 1 = 1.33095 loss)
I0815 18:54:07.419298  8764 sgd_solver.cpp:136] Iteration 157500, lr = 0.00015625, m = 0.9
I0815 18:54:25.262441  8764 solver.cpp:312] Iteration 157600 (5.60453 iter/s, 17.8427s/100 iter), loss = 1.37864
I0815 18:54:25.262501  8764 solver.cpp:334]     Train net output #0: loss = 1.12817 (* 1 = 1.12817 loss)
I0815 18:54:25.262507  8764 sgd_solver.cpp:136] Iteration 157600, lr = 0.00015, m = 0.9
I0815 18:54:45.116144  8764 solver.cpp:312] Iteration 157700 (5.03698 iter/s, 19.8532s/100 iter), loss = 1.26332
I0815 18:54:45.116171  8764 solver.cpp:334]     Train net output #0: loss = 1.53083 (* 1 = 1.53083 loss)
I0815 18:54:45.116178  8764 sgd_solver.cpp:136] Iteration 157700, lr = 0.00014375, m = 0.9
I0815 18:55:03.654886  8764 solver.cpp:312] Iteration 157800 (5.39426 iter/s, 18.5382s/100 iter), loss = 1.33705
I0815 18:55:03.654974  8764 solver.cpp:334]     Train net output #0: loss = 1.36447 (* 1 = 1.36447 loss)
I0815 18:55:03.654992  8764 sgd_solver.cpp:136] Iteration 157800, lr = 0.0001375, m = 0.9
I0815 18:55:20.970484  8764 solver.cpp:312] Iteration 157900 (5.7753 iter/s, 17.3151s/100 iter), loss = 1.43534
I0815 18:55:20.970511  8764 solver.cpp:334]     Train net output #0: loss = 1.48158 (* 1 = 1.48158 loss)
I0815 18:55:20.970517  8764 sgd_solver.cpp:136] Iteration 157900, lr = 0.00013125, m = 0.9
I0815 18:55:37.896369  8764 solver.cpp:363] Sparsity after update:
I0815 18:55:37.902242  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:55:37.902256  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:55:37.902266  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:55:37.902271  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:55:37.902273  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:55:37.902277  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:55:37.902281  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:55:37.902283  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:55:37.902287  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:55:37.902288  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:55:37.902292  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:55:37.902294  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:55:37.902297  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:55:37.902308  8764 solver.cpp:509] Iteration 158000, Testing net (#0)
I0815 18:55:49.445585  8747 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 18:56:04.486696  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.600412
I0815 18:56:04.486721  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.820115
I0815 18:56:04.486727  8764 solver.cpp:594]     Test net output #2: loss = 1.73733 (* 1 = 1.73733 loss)
I0815 18:56:04.486748  8764 solver.cpp:264] [MultiGPU] Tests completed in 26.5837s
I0815 18:56:04.636364  8764 solver.cpp:312] Iteration 158000 (2.29018 iter/s, 43.6647s/100 iter), loss = 1.15153
I0815 18:56:04.636389  8764 solver.cpp:334]     Train net output #0: loss = 0.878272 (* 1 = 0.878272 loss)
I0815 18:56:04.636395  8764 sgd_solver.cpp:136] Iteration 158000, lr = 0.000125, m = 0.9
I0815 18:56:22.733921  8764 solver.cpp:312] Iteration 158100 (5.52576 iter/s, 18.0971s/100 iter), loss = 1.31369
I0815 18:56:22.734015  8764 solver.cpp:334]     Train net output #0: loss = 1.41089 (* 1 = 1.41089 loss)
I0815 18:56:22.734033  8764 sgd_solver.cpp:136] Iteration 158100, lr = 0.00011875, m = 0.9
I0815 18:56:38.225419  8764 solver.cpp:312] Iteration 158200 (6.45533 iter/s, 15.4911s/100 iter), loss = 1.30583
I0815 18:56:38.225443  8764 solver.cpp:334]     Train net output #0: loss = 1.22907 (* 1 = 1.22907 loss)
I0815 18:56:38.225448  8764 sgd_solver.cpp:136] Iteration 158200, lr = 0.0001125, m = 0.9
I0815 18:56:56.883030  8764 solver.cpp:312] Iteration 158300 (5.35989 iter/s, 18.6571s/100 iter), loss = 1.26269
I0815 18:56:56.883107  8764 solver.cpp:334]     Train net output #0: loss = 1.31755 (* 1 = 1.31755 loss)
I0815 18:56:56.883114  8764 sgd_solver.cpp:136] Iteration 158300, lr = 0.00010625, m = 0.9
I0815 18:57:15.986559  8764 solver.cpp:312] Iteration 158400 (5.23479 iter/s, 19.103s/100 iter), loss = 1.43858
I0815 18:57:15.986619  8764 solver.cpp:334]     Train net output #0: loss = 1.41753 (* 1 = 1.41753 loss)
I0815 18:57:15.986637  8764 sgd_solver.cpp:136] Iteration 158400, lr = 9.99999e-05, m = 0.9
I0815 18:57:35.712939  8764 solver.cpp:312] Iteration 158500 (5.06949 iter/s, 19.7258s/100 iter), loss = 1.38257
I0815 18:57:35.713028  8764 solver.cpp:334]     Train net output #0: loss = 1.66007 (* 1 = 1.66007 loss)
I0815 18:57:35.713038  8764 sgd_solver.cpp:136] Iteration 158500, lr = 9.37498e-05, m = 0.9
I0815 18:57:53.040740  8764 solver.cpp:312] Iteration 158600 (5.77123 iter/s, 17.3273s/100 iter), loss = 1.27794
I0815 18:57:53.040781  8764 solver.cpp:334]     Train net output #0: loss = 1.16817 (* 1 = 1.16817 loss)
I0815 18:57:53.040792  8764 sgd_solver.cpp:136] Iteration 158600, lr = 8.75002e-05, m = 0.9
I0815 18:58:13.963186  8764 solver.cpp:312] Iteration 158700 (4.77969 iter/s, 20.9219s/100 iter), loss = 1.31456
I0815 18:58:13.963289  8764 solver.cpp:334]     Train net output #0: loss = 1.13414 (* 1 = 1.13414 loss)
I0815 18:58:13.963312  8764 sgd_solver.cpp:136] Iteration 158700, lr = 8.12501e-05, m = 0.9
I0815 18:58:35.560240  8764 solver.cpp:312] Iteration 158800 (4.63039 iter/s, 21.5965s/100 iter), loss = 1.4967
I0815 18:58:35.560267  8764 solver.cpp:334]     Train net output #0: loss = 1.34251 (* 1 = 1.34251 loss)
I0815 18:58:35.560271  8764 sgd_solver.cpp:136] Iteration 158800, lr = 7.49999e-05, m = 0.9
I0815 18:58:56.826381  8764 solver.cpp:312] Iteration 158900 (4.70244 iter/s, 21.2655s/100 iter), loss = 1.15607
I0815 18:58:56.826593  8764 solver.cpp:334]     Train net output #0: loss = 1.25582 (* 1 = 1.25582 loss)
I0815 18:58:56.826617  8764 sgd_solver.cpp:136] Iteration 158900, lr = 6.87498e-05, m = 0.9
I0815 18:59:16.293910  8764 solver.cpp:363] Sparsity after update:
I0815 18:59:16.305222  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 18:59:16.305243  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 18:59:16.305256  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 18:59:16.305260  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 18:59:16.305263  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 18:59:16.305266  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 18:59:16.305270  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 18:59:16.305274  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 18:59:16.305276  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 18:59:16.305280  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 18:59:16.305284  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 18:59:16.305286  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 18:59:16.305289  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 18:59:16.646153  8764 solver.cpp:312] Iteration 159000 (5.04561 iter/s, 19.8192s/100 iter), loss = 1.72539
I0815 18:59:16.646199  8764 solver.cpp:334]     Train net output #0: loss = 1.72858 (* 1 = 1.72858 loss)
I0815 18:59:16.646210  8764 sgd_solver.cpp:136] Iteration 159000, lr = 6.25002e-05, m = 0.9
I0815 18:59:38.536093  8764 solver.cpp:312] Iteration 159100 (4.56843 iter/s, 21.8893s/100 iter), loss = 1.46191
I0815 18:59:38.536201  8764 solver.cpp:334]     Train net output #0: loss = 1.51227 (* 1 = 1.51227 loss)
I0815 18:59:38.536218  8764 sgd_solver.cpp:136] Iteration 159100, lr = 5.62501e-05, m = 0.9
I0815 18:59:59.253667  8764 solver.cpp:312] Iteration 159200 (4.82696 iter/s, 20.717s/100 iter), loss = 1.45128
I0815 18:59:59.253697  8764 solver.cpp:334]     Train net output #0: loss = 1.90562 (* 1 = 1.90562 loss)
I0815 18:59:59.253703  8764 sgd_solver.cpp:136] Iteration 159200, lr = 5e-05, m = 0.9
I0815 19:00:18.853642  8764 solver.cpp:312] Iteration 159300 (5.10219 iter/s, 19.5994s/100 iter), loss = 1.28292
I0815 19:00:18.853704  8764 solver.cpp:334]     Train net output #0: loss = 1.19643 (* 1 = 1.19643 loss)
I0815 19:00:18.853710  8764 sgd_solver.cpp:136] Iteration 159300, lr = 4.37498e-05, m = 0.9
I0815 19:00:33.789043  8764 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 19:00:38.260581  8764 solver.cpp:312] Iteration 159400 (5.15294 iter/s, 19.4064s/100 iter), loss = 1.57283
I0815 19:00:38.260622  8764 solver.cpp:334]     Train net output #0: loss = 1.80493 (* 1 = 1.80493 loss)
I0815 19:00:38.260635  8764 sgd_solver.cpp:136] Iteration 159400, lr = 3.75003e-05, m = 0.9
I0815 19:00:58.882683  8764 solver.cpp:312] Iteration 159500 (4.8493 iter/s, 20.6215s/100 iter), loss = 1.35713
I0815 19:00:58.882767  8764 solver.cpp:334]     Train net output #0: loss = 1.32614 (* 1 = 1.32614 loss)
I0815 19:00:58.882781  8764 sgd_solver.cpp:136] Iteration 159500, lr = 3.12501e-05, m = 0.9
I0815 19:01:18.123251  8764 solver.cpp:312] Iteration 159600 (5.19749 iter/s, 19.24s/100 iter), loss = 1.53796
I0815 19:01:18.123327  8764 solver.cpp:334]     Train net output #0: loss = 1.79531 (* 1 = 1.79531 loss)
I0815 19:01:18.123347  8764 sgd_solver.cpp:136] Iteration 159600, lr = 2.5e-05, m = 0.9
I0815 19:01:37.260257  8764 solver.cpp:312] Iteration 159700 (5.22563 iter/s, 19.1365s/100 iter), loss = 1.31863
I0815 19:01:37.260359  8764 solver.cpp:334]     Train net output #0: loss = 1.22497 (* 1 = 1.22497 loss)
I0815 19:01:37.260371  8764 sgd_solver.cpp:136] Iteration 159700, lr = 1.87498e-05, m = 0.9
I0815 19:01:59.484719  8764 solver.cpp:312] Iteration 159800 (4.49967 iter/s, 22.2238s/100 iter), loss = 1.52736
I0815 19:01:59.484827  8764 solver.cpp:334]     Train net output #0: loss = 1.44795 (* 1 = 1.44795 loss)
I0815 19:01:59.484848  8764 sgd_solver.cpp:136] Iteration 159800, lr = 1.25003e-05, m = 0.9
I0815 19:02:19.288565  8764 solver.cpp:312] Iteration 159900 (5.04966 iter/s, 19.8033s/100 iter), loss = 0.941216
I0815 19:02:19.288841  8764 solver.cpp:334]     Train net output #0: loss = 0.732497 (* 1 = 0.732497 loss)
I0815 19:02:19.288954  8764 sgd_solver.cpp:136] Iteration 159900, lr = 6.25014e-06, m = 0.9
I0815 19:02:37.516433  8764 solver.cpp:312] Iteration 159999 (5.43139 iter/s, 18.2274s/99 iter), loss = 1.5553
I0815 19:02:37.516458  8764 solver.cpp:334]     Train net output #0: loss = 1.76265 (* 1 = 1.76265 loss)
I0815 19:02:37.516463  8764 solver.cpp:639] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_160000.caffemodel
I0815 19:02:37.843142  8764 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-08-14_19-50-34/sparse/imagenet_jacintonet11v2_iter_160000.solverstate
I0815 19:02:37.849011  8764 solver.cpp:363] Sparsity after update:
I0815 19:02:37.854293  8764 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0815 19:02:37.854305  8764 net.cpp:2192] conv1a_param_0(0.395) 
I0815 19:02:37.854311  8764 net.cpp:2192] conv1b_param_0(0.679) 
I0815 19:02:37.854313  8764 net.cpp:2192] fc1000_param_0(0) 
I0815 19:02:37.854315  8764 net.cpp:2192] res2a_branch2a_param_0(0.789) 
I0815 19:02:37.854318  8764 net.cpp:2192] res2a_branch2b_param_0(0.7) 
I0815 19:02:37.854326  8764 net.cpp:2192] res3a_branch2a_param_0(0.801) 
I0815 19:02:37.854328  8764 net.cpp:2192] res3a_branch2b_param_0(0.762) 
I0815 19:02:37.854332  8764 net.cpp:2192] res4a_branch2a_param_0(0.807) 
I0815 19:02:37.854336  8764 net.cpp:2192] res4a_branch2b_param_0(0.782) 
I0815 19:02:37.854338  8764 net.cpp:2192] res5a_branch2a_param_0(0.81) 
I0815 19:02:37.854342  8764 net.cpp:2192] res5a_branch2b_param_0(0.809) 
I0815 19:02:37.854346  8764 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.89637e+06/2.86678e+06) 0.661
I0815 19:02:37.878444  8764 solver.cpp:486] Iteration 160000, loss = 1.25444
I0815 19:02:37.878471  8764 solver.cpp:509] Iteration 160000, Testing net (#0)
I0815 19:03:13.394717  8764 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.596824
I0815 19:03:13.394825  8764 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.821644
I0815 19:03:13.394842  8764 solver.cpp:594]     Test net output #2: loss = 1.74932 (* 1 = 1.74932 loss)
I0815 19:03:13.423722  8671 parallel.cpp:71] Root Solver performance on device 0: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423768  8671 parallel.cpp:76]      Solver performance on device 1: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423787  8671 parallel.cpp:76]      Solver performance on device 2: 4.83 * 43 = 207.7 img/sec (160000 itr in 3.313e+04 sec)
I0815 19:03:13.423797  8671 parallel.cpp:79] Overall multi-GPU performance: 623.075 img/sec
I0815 19:03:13.949198  8671 caffe.cpp:247] Optimization Done in 9h 12m 46s
