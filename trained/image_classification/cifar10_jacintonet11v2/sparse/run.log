I0814 19:16:12.246976 24692 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 19:16:12 2017
I0814 19:16:12.247100 24692 caffe.cpp:611] CuDNN version: 6021
I0814 19:16:12.247105 24692 caffe.cpp:612] CuBLAS version: 8000
I0814 19:16:12.247108 24692 caffe.cpp:613] CUDA version: 8000
I0814 19:16:12.247110 24692 caffe.cpp:614] CUDA driver version: 8000
I0814 19:16:12.496837 24692 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 19:16:12.497409 24692 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 19:16:12.497931 24692 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0814 19:16:12.498445 24692 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0814 19:16:12.498456 24692 caffe.cpp:208] Using GPUs 0, 1, 2
I0814 19:16:12.498781 24692 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0814 19:16:12.499105 24692 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0814 19:16:12.499429 24692 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0814 19:16:12.499464 24692 solver.cpp:42] Solver data type: FLOAT
I0814 19:16:12.499500 24692 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0814 19:16:12.506762 24692 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/train.prototxt
I0814 19:16:12.507329 24692 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0814 19:16:12.507345 24692 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0814 19:16:12.507385 24692 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:12.507647 24692 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0814 19:16:12.507796 24692 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:12.507800 24692 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:12.507805 24692 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:16:12.507835 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.507938 24692 net.cpp:184] Created Layer data (0)
I0814 19:16:12.507954 24692 net.cpp:530] data -> data
I0814 19:16:12.507982 24692 net.cpp:530] data -> label
I0814 19:16:12.508016 24692 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:12.508038 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.508898 24716 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:12.510085 24692 data_layer.cpp:185] [0] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:12.510177 24692 data_layer.cpp:209] [0] Output data size: 22, 3, 32, 32
I0814 19:16:12.510185 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.510208 24692 net.cpp:245] Setting up data
I0814 19:16:12.510217 24692 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0814 19:16:12.510224 24692 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0814 19:16:12.510232 24692 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:16:12.510236 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.510252 24692 net.cpp:184] Created Layer data/bias (1)
I0814 19:16:12.510257 24692 net.cpp:561] data/bias <- data
I0814 19:16:12.510264 24692 net.cpp:530] data/bias -> data/bias
I0814 19:16:12.512287 24692 net.cpp:245] Setting up data/bias
I0814 19:16:12.512300 24692 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0814 19:16:12.512310 24692 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:16:12.512315 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.512328 24692 net.cpp:184] Created Layer conv1a (2)
I0814 19:16:12.512331 24692 net.cpp:561] conv1a <- data/bias
I0814 19:16:12.512334 24692 net.cpp:530] conv1a -> conv1a
I0814 19:16:12.798638 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0814 19:16:12.798658 24692 net.cpp:245] Setting up conv1a
I0814 19:16:12.798665 24692 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0814 19:16:12.798672 24692 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:16:12.798676 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.798686 24692 net.cpp:184] Created Layer conv1a/bn (3)
I0814 19:16:12.798689 24692 net.cpp:561] conv1a/bn <- conv1a
I0814 19:16:12.798694 24692 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:16:12.799355 24692 net.cpp:245] Setting up conv1a/bn
I0814 19:16:12.799362 24692 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0814 19:16:12.799368 24692 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:16:12.799371 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.799376 24692 net.cpp:184] Created Layer conv1a/relu (4)
I0814 19:16:12.799378 24692 net.cpp:561] conv1a/relu <- conv1a
I0814 19:16:12.799381 24692 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:16:12.799393 24692 net.cpp:245] Setting up conv1a/relu
I0814 19:16:12.799396 24692 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0814 19:16:12.799398 24692 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:16:12.799401 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.799408 24692 net.cpp:184] Created Layer conv1b (5)
I0814 19:16:12.799410 24692 net.cpp:561] conv1b <- conv1a
I0814 19:16:12.799413 24692 net.cpp:530] conv1b -> conv1b
I0814 19:16:12.806608 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0814 19:16:12.806619 24692 net.cpp:245] Setting up conv1b
I0814 19:16:12.806624 24692 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0814 19:16:12.806630 24692 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:16:12.806633 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.806651 24692 net.cpp:184] Created Layer conv1b/bn (6)
I0814 19:16:12.806658 24692 net.cpp:561] conv1b/bn <- conv1b
I0814 19:16:12.806660 24692 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:16:12.807273 24692 net.cpp:245] Setting up conv1b/bn
I0814 19:16:12.807284 24692 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0814 19:16:12.807291 24692 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:16:12.807294 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807301 24692 net.cpp:184] Created Layer conv1b/relu (7)
I0814 19:16:12.807303 24692 net.cpp:561] conv1b/relu <- conv1b
I0814 19:16:12.807307 24692 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:16:12.807312 24692 net.cpp:245] Setting up conv1b/relu
I0814 19:16:12.807317 24692 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0814 19:16:12.807320 24692 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:16:12.807324 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807332 24692 net.cpp:184] Created Layer pool1 (8)
I0814 19:16:12.807337 24692 net.cpp:561] pool1 <- conv1b
I0814 19:16:12.807339 24692 net.cpp:530] pool1 -> pool1
I0814 19:16:12.807416 24692 net.cpp:245] Setting up pool1
I0814 19:16:12.807422 24692 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0814 19:16:12.807425 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:16:12.807430 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807441 24692 net.cpp:184] Created Layer res2a_branch2a (9)
I0814 19:16:12.807445 24692 net.cpp:561] res2a_branch2a <- pool1
I0814 19:16:12.807447 24692 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:16:12.816977 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0814 19:16:12.816990 24692 net.cpp:245] Setting up res2a_branch2a
I0814 19:16:12.816994 24692 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0814 19:16:12.817004 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.817008 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817014 24692 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0814 19:16:12.817018 24692 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:16:12.817020 24692 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:16:12.817644 24692 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:16:12.817652 24692 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0814 19:16:12.817658 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.817662 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817665 24692 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0814 19:16:12.817668 24692 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:16:12.817672 24692 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:16:12.817675 24692 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:16:12.817678 24692 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0814 19:16:12.817680 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:16:12.817683 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817694 24692 net.cpp:184] Created Layer res2a_branch2b (12)
I0814 19:16:12.817698 24692 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:16:12.817701 24692 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:16:12.824290 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0814 19:16:12.824301 24692 net.cpp:245] Setting up res2a_branch2b
I0814 19:16:12.824306 24692 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0814 19:16:12.824314 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.824318 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824322 24692 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0814 19:16:12.824326 24692 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:16:12.824328 24692 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:16:12.824935 24692 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:16:12.824942 24692 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0814 19:16:12.824949 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.824952 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824955 24692 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0814 19:16:12.824959 24692 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:16:12.824960 24692 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:16:12.824965 24692 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:16:12.824967 24692 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0814 19:16:12.824970 24692 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:16:12.824972 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824976 24692 net.cpp:184] Created Layer pool2 (15)
I0814 19:16:12.824980 24692 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:16:12.824981 24692 net.cpp:530] pool2 -> pool2
I0814 19:16:12.825037 24692 net.cpp:245] Setting up pool2
I0814 19:16:12.825042 24692 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0814 19:16:12.825044 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:16:12.825047 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.825054 24692 net.cpp:184] Created Layer res3a_branch2a (16)
I0814 19:16:12.825057 24692 net.cpp:561] res3a_branch2a <- pool2
I0814 19:16:12.825059 24692 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:16:12.835561 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0814 19:16:12.835579 24692 net.cpp:245] Setting up res3a_branch2a
I0814 19:16:12.835585 24692 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0814 19:16:12.835592 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.835597 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.835605 24692 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0814 19:16:12.835609 24692 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:16:12.835613 24692 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:16:12.836254 24692 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:16:12.836263 24692 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0814 19:16:12.836272 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.836274 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.836278 24692 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0814 19:16:12.836282 24692 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:16:12.836283 24692 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:16:12.836287 24692 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:16:12.836300 24692 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0814 19:16:12.836304 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:16:12.836308 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.836320 24692 net.cpp:184] Created Layer res3a_branch2b (19)
I0814 19:16:12.836325 24692 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:16:12.836328 24692 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:16:12.841171 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0814 19:16:12.841181 24692 net.cpp:245] Setting up res3a_branch2b
I0814 19:16:12.841184 24692 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0814 19:16:12.841189 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.841192 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841197 24692 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0814 19:16:12.841200 24692 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:16:12.841203 24692 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:16:12.841784 24692 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:16:12.841790 24692 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0814 19:16:12.841796 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.841800 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841804 24692 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0814 19:16:12.841805 24692 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:16:12.841807 24692 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:16:12.841811 24692 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:16:12.841814 24692 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0814 19:16:12.841816 24692 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:16:12.841820 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841823 24692 net.cpp:184] Created Layer pool3 (22)
I0814 19:16:12.841825 24692 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:16:12.841828 24692 net.cpp:530] pool3 -> pool3
I0814 19:16:12.841886 24692 net.cpp:245] Setting up pool3
I0814 19:16:12.841892 24692 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0814 19:16:12.841893 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:16:12.841897 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841904 24692 net.cpp:184] Created Layer res4a_branch2a (23)
I0814 19:16:12.841908 24692 net.cpp:561] res4a_branch2a <- pool3
I0814 19:16:12.841912 24692 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:16:12.860702 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0814 19:16:12.860720 24692 net.cpp:245] Setting up res4a_branch2a
I0814 19:16:12.860726 24692 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0814 19:16:12.860734 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.860740 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.860750 24692 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0814 19:16:12.860755 24692 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:16:12.860760 24692 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:16:12.861424 24692 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:16:12.861433 24692 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0814 19:16:12.861452 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.861457 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.861464 24692 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0814 19:16:12.861467 24692 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:16:12.861471 24692 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:16:12.861479 24692 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:16:12.861484 24692 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0814 19:16:12.861487 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:16:12.861491 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.861501 24692 net.cpp:184] Created Layer res4a_branch2b (26)
I0814 19:16:12.861505 24692 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:16:12.861510 24692 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:16:12.869359 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0814 19:16:12.869374 24692 net.cpp:245] Setting up res4a_branch2b
I0814 19:16:12.869379 24692 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0814 19:16:12.869386 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.869391 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.869400 24692 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0814 19:16:12.869403 24692 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:16:12.869408 24692 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:16:12.870019 24692 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:16:12.870028 24692 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0814 19:16:12.870039 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.870043 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870049 24692 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0814 19:16:12.870054 24692 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:16:12.870059 24692 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:16:12.870064 24692 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:16:12.870069 24692 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0814 19:16:12.870074 24692 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:16:12.870077 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870085 24692 net.cpp:184] Created Layer pool4 (29)
I0814 19:16:12.870088 24692 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:16:12.870093 24692 net.cpp:530] pool4 -> pool4
I0814 19:16:12.870153 24692 net.cpp:245] Setting up pool4
I0814 19:16:12.870159 24692 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0814 19:16:12.870164 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:16:12.870168 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870177 24692 net.cpp:184] Created Layer res5a_branch2a (30)
I0814 19:16:12.870180 24692 net.cpp:561] res5a_branch2a <- pool4
I0814 19:16:12.870184 24692 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:16:12.911263 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0814 19:16:12.911281 24692 net.cpp:245] Setting up res5a_branch2a
I0814 19:16:12.911288 24692 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0814 19:16:12.911308 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.911312 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.911322 24692 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0814 19:16:12.911327 24692 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:16:12.911334 24692 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:16:12.912051 24692 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:16:12.912060 24692 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0814 19:16:12.912070 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.912073 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.912081 24692 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0814 19:16:12.912084 24692 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:16:12.912088 24692 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:16:12.912096 24692 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:16:12.912101 24692 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0814 19:16:12.912104 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:16:12.912109 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.912123 24692 net.cpp:184] Created Layer res5a_branch2b (33)
I0814 19:16:12.912127 24692 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:16:12.912137 24692 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:16:12.930754 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0814 19:16:12.930773 24692 net.cpp:245] Setting up res5a_branch2b
I0814 19:16:12.930779 24692 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0814 19:16:12.930790 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.930796 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.930806 24692 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0814 19:16:12.930811 24692 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:16:12.930816 24692 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:16:12.931473 24692 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:16:12.931483 24692 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0814 19:16:12.931490 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.931495 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931500 24692 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0814 19:16:12.931504 24692 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:16:12.931509 24692 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:16:12.931515 24692 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:16:12.931520 24692 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0814 19:16:12.931524 24692 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:16:12.931529 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931535 24692 net.cpp:184] Created Layer pool5 (36)
I0814 19:16:12.931540 24692 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:16:12.931543 24692 net.cpp:530] pool5 -> pool5
I0814 19:16:12.931577 24692 net.cpp:245] Setting up pool5
I0814 19:16:12.931583 24692 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0814 19:16:12.931587 24692 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 19:16:12.931591 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931607 24692 net.cpp:184] Created Layer fc10 (37)
I0814 19:16:12.931612 24692 net.cpp:561] fc10 <- pool5
I0814 19:16:12.931617 24692 net.cpp:530] fc10 -> fc10
I0814 19:16:12.931886 24692 net.cpp:245] Setting up fc10
I0814 19:16:12.931893 24692 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0814 19:16:12.931901 24692 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:16:12.931905 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931917 24692 net.cpp:184] Created Layer loss (38)
I0814 19:16:12.931921 24692 net.cpp:561] loss <- fc10
I0814 19:16:12.931926 24692 net.cpp:561] loss <- label
I0814 19:16:12.931931 24692 net.cpp:530] loss -> loss
I0814 19:16:12.932085 24692 net.cpp:245] Setting up loss
I0814 19:16:12.932092 24692 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0814 19:16:12.932096 24692 net.cpp:256]     with loss weight 1
I0814 19:16:12.932102 24692 net.cpp:323] loss needs backward computation.
I0814 19:16:12.932106 24692 net.cpp:323] fc10 needs backward computation.
I0814 19:16:12.932111 24692 net.cpp:323] pool5 needs backward computation.
I0814 19:16:12.932114 24692 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:16:12.932118 24692 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:16:12.932122 24692 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:16:12.932126 24692 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:16:12.932137 24692 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:16:12.932140 24692 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:16:12.932144 24692 net.cpp:323] pool4 needs backward computation.
I0814 19:16:12.932148 24692 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:16:12.932152 24692 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:16:12.932155 24692 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:16:12.932159 24692 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:16:12.932163 24692 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:16:12.932168 24692 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:16:12.932171 24692 net.cpp:323] pool3 needs backward computation.
I0814 19:16:12.932175 24692 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:16:12.932179 24692 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:16:12.932183 24692 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:16:12.932188 24692 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:16:12.932191 24692 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:16:12.932195 24692 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:16:12.932199 24692 net.cpp:323] pool2 needs backward computation.
I0814 19:16:12.932204 24692 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:16:12.932207 24692 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:16:12.932210 24692 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:16:12.932214 24692 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:16:12.932219 24692 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:16:12.932221 24692 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:16:12.932225 24692 net.cpp:323] pool1 needs backward computation.
I0814 19:16:12.932229 24692 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:16:12.932234 24692 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:16:12.932237 24692 net.cpp:323] conv1b needs backward computation.
I0814 19:16:12.932241 24692 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:16:12.932245 24692 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:16:12.932248 24692 net.cpp:323] conv1a needs backward computation.
I0814 19:16:12.932258 24692 net.cpp:325] data/bias does not need backward computation.
I0814 19:16:12.932263 24692 net.cpp:325] data does not need backward computation.
I0814 19:16:12.932267 24692 net.cpp:367] This network produces output loss
I0814 19:16:12.932296 24692 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0814 19:16:12.932299 24692 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0814 19:16:12.932303 24692 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0814 19:16:12.932307 24692 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0814 19:16:12.932310 24692 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0814 19:16:12.932313 24692 net.cpp:407] Network initialization done.
I0814 19:16:12.932672 24692 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:12.932719 24692 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:12.932844 24692 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 19:16:12.932938 24692 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:12.932943 24692 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:12.932947 24692 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:16:12.932951 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.932963 24692 net.cpp:184] Created Layer data (0)
I0814 19:16:12.932967 24692 net.cpp:530] data -> data
I0814 19:16:12.932972 24692 net.cpp:530] data -> label
I0814 19:16:12.932982 24692 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:12.932991 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.934274 24730 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:12.934339 24692 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:12.934404 24692 data_layer.cpp:209] (0) Output data size: 17, 3, 32, 32
I0814 19:16:12.934408 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.934422 24692 net.cpp:245] Setting up data
I0814 19:16:12.934428 24692 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0814 19:16:12.934434 24692 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0814 19:16:12.934438 24692 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 19:16:12.934443 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934454 24692 net.cpp:184] Created Layer label_data_1_split (1)
I0814 19:16:12.934459 24692 net.cpp:561] label_data_1_split <- label
I0814 19:16:12.934464 24692 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 19:16:12.934469 24692 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 19:16:12.934474 24692 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 19:16:12.934535 24692 net.cpp:245] Setting up label_data_1_split
I0814 19:16:12.934540 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934545 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934550 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934553 24692 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:16:12.934557 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934564 24692 net.cpp:184] Created Layer data/bias (2)
I0814 19:16:12.934568 24692 net.cpp:561] data/bias <- data
I0814 19:16:12.934572 24692 net.cpp:530] data/bias -> data/bias
I0814 19:16:12.934705 24692 net.cpp:245] Setting up data/bias
I0814 19:16:12.934712 24692 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0814 19:16:12.934720 24692 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:16:12.934723 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934733 24692 net.cpp:184] Created Layer conv1a (3)
I0814 19:16:12.934737 24692 net.cpp:561] conv1a <- data/bias
I0814 19:16:12.934741 24692 net.cpp:530] conv1a -> conv1a
I0814 19:16:12.935092 24731 data_layer.cpp:97] (0) Parser threads: 1
I0814 19:16:12.935101 24731 data_layer.cpp:99] (0) Transformer threads: 1
I0814 19:16:12.937893 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0814 19:16:12.937904 24692 net.cpp:245] Setting up conv1a
I0814 19:16:12.937911 24692 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0814 19:16:12.937921 24692 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:16:12.937924 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.937937 24692 net.cpp:184] Created Layer conv1a/bn (4)
I0814 19:16:12.937940 24692 net.cpp:561] conv1a/bn <- conv1a
I0814 19:16:12.937944 24692 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:16:12.938659 24692 net.cpp:245] Setting up conv1a/bn
I0814 19:16:12.938668 24692 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0814 19:16:12.938678 24692 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:16:12.938683 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.938688 24692 net.cpp:184] Created Layer conv1a/relu (5)
I0814 19:16:12.938691 24692 net.cpp:561] conv1a/relu <- conv1a
I0814 19:16:12.938695 24692 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:16:12.938701 24692 net.cpp:245] Setting up conv1a/relu
I0814 19:16:12.938706 24692 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0814 19:16:12.938710 24692 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:16:12.938715 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.938724 24692 net.cpp:184] Created Layer conv1b (6)
I0814 19:16:12.938727 24692 net.cpp:561] conv1b <- conv1a
I0814 19:16:12.938731 24692 net.cpp:530] conv1b -> conv1b
I0814 19:16:12.941779 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0814 19:16:12.941790 24692 net.cpp:245] Setting up conv1b
I0814 19:16:12.941797 24692 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0814 19:16:12.941804 24692 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:16:12.941814 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.941823 24692 net.cpp:184] Created Layer conv1b/bn (7)
I0814 19:16:12.941826 24692 net.cpp:561] conv1b/bn <- conv1b
I0814 19:16:12.941830 24692 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:16:12.942487 24692 net.cpp:245] Setting up conv1b/bn
I0814 19:16:12.942494 24692 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0814 19:16:12.942504 24692 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:16:12.942508 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942513 24692 net.cpp:184] Created Layer conv1b/relu (8)
I0814 19:16:12.942517 24692 net.cpp:561] conv1b/relu <- conv1b
I0814 19:16:12.942522 24692 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:16:12.942528 24692 net.cpp:245] Setting up conv1b/relu
I0814 19:16:12.942533 24692 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0814 19:16:12.942536 24692 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:16:12.942540 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942546 24692 net.cpp:184] Created Layer pool1 (9)
I0814 19:16:12.942550 24692 net.cpp:561] pool1 <- conv1b
I0814 19:16:12.942554 24692 net.cpp:530] pool1 -> pool1
I0814 19:16:12.942621 24692 net.cpp:245] Setting up pool1
I0814 19:16:12.942627 24692 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0814 19:16:12.942632 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:16:12.942636 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942646 24692 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 19:16:12.942649 24692 net.cpp:561] res2a_branch2a <- pool1
I0814 19:16:12.942653 24692 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:16:12.946233 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0814 19:16:12.946244 24692 net.cpp:245] Setting up res2a_branch2a
I0814 19:16:12.946250 24692 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0814 19:16:12.946259 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.946262 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946269 24692 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 19:16:12.946274 24692 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:16:12.946279 24692 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:16:12.946931 24692 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:16:12.946940 24692 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0814 19:16:12.946950 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.946954 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946959 24692 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 19:16:12.946964 24692 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:16:12.946969 24692 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:16:12.946976 24692 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:16:12.946980 24692 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0814 19:16:12.946985 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:16:12.946990 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946997 24692 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 19:16:12.947001 24692 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:16:12.947005 24692 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:16:12.950170 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0814 19:16:12.950181 24692 net.cpp:245] Setting up res2a_branch2b
I0814 19:16:12.950187 24692 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0814 19:16:12.950196 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.950199 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950206 24692 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 19:16:12.950211 24692 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:16:12.950215 24692 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:16:12.950882 24692 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:16:12.950891 24692 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0814 19:16:12.950898 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.950902 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950908 24692 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 19:16:12.950912 24692 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:16:12.950917 24692 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:16:12.950923 24692 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:16:12.950928 24692 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0814 19:16:12.950932 24692 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:16:12.950937 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950943 24692 net.cpp:184] Created Layer pool2 (16)
I0814 19:16:12.950947 24692 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:16:12.950950 24692 net.cpp:530] pool2 -> pool2
I0814 19:16:12.951021 24692 net.cpp:245] Setting up pool2
I0814 19:16:12.951027 24692 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0814 19:16:12.951031 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:16:12.951036 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.951045 24692 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 19:16:12.951048 24692 net.cpp:561] res3a_branch2a <- pool2
I0814 19:16:12.951052 24692 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:16:12.956998 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 19:16:12.957010 24692 net.cpp:245] Setting up res3a_branch2a
I0814 19:16:12.957015 24692 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0814 19:16:12.957021 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.957026 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957033 24692 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 19:16:12.957037 24692 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:16:12.957041 24692 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:16:12.957693 24692 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:16:12.957701 24692 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0814 19:16:12.957712 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.957716 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957722 24692 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 19:16:12.957726 24692 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:16:12.957731 24692 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:16:12.957736 24692 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:16:12.957749 24692 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0814 19:16:12.957753 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:16:12.957758 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957767 24692 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 19:16:12.957772 24692 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:16:12.957775 24692 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:16:12.960934 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0814 19:16:12.960944 24692 net.cpp:245] Setting up res3a_branch2b
I0814 19:16:12.960950 24692 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0814 19:16:12.960958 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.960961 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.960968 24692 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 19:16:12.960973 24692 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:16:12.960978 24692 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:16:12.961640 24692 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:16:12.961649 24692 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0814 19:16:12.961658 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.961663 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961668 24692 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 19:16:12.961671 24692 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:16:12.961676 24692 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:16:12.961683 24692 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:16:12.961688 24692 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0814 19:16:12.961691 24692 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:16:12.961695 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961701 24692 net.cpp:184] Created Layer pool3 (23)
I0814 19:16:12.961705 24692 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:16:12.961709 24692 net.cpp:530] pool3 -> pool3
I0814 19:16:12.961778 24692 net.cpp:245] Setting up pool3
I0814 19:16:12.961784 24692 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0814 19:16:12.961789 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:16:12.961793 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961802 24692 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 19:16:12.961805 24692 net.cpp:561] res4a_branch2a <- pool3
I0814 19:16:12.961809 24692 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:16:12.972277 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:12.972290 24692 net.cpp:245] Setting up res4a_branch2a
I0814 19:16:12.972295 24692 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0814 19:16:12.972303 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.972308 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.972316 24692 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 19:16:12.972321 24692 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:16:12.972324 24692 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:16:12.973012 24692 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:16:12.973021 24692 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0814 19:16:12.973037 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.973042 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.973047 24692 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 19:16:12.973052 24692 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:16:12.973055 24692 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:16:12.973062 24692 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:16:12.973067 24692 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0814 19:16:12.973070 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:16:12.973074 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.973088 24692 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 19:16:12.973091 24692 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:16:12.973095 24692 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:16:12.978668 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:12.978679 24692 net.cpp:245] Setting up res4a_branch2b
I0814 19:16:12.978691 24692 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0814 19:16:12.978698 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.978703 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.978711 24692 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 19:16:12.978715 24692 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:16:12.978719 24692 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:16:12.979394 24692 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:16:12.979403 24692 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0814 19:16:12.979413 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.979416 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979423 24692 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 19:16:12.979426 24692 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:16:12.979432 24692 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:16:12.979439 24692 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:16:12.979444 24692 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0814 19:16:12.979447 24692 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:16:12.979451 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979459 24692 net.cpp:184] Created Layer pool4 (30)
I0814 19:16:12.979462 24692 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:16:12.979466 24692 net.cpp:530] pool4 -> pool4
I0814 19:16:12.979537 24692 net.cpp:245] Setting up pool4
I0814 19:16:12.979542 24692 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0814 19:16:12.979547 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:16:12.979552 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979559 24692 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 19:16:12.979563 24692 net.cpp:561] res5a_branch2a <- pool4
I0814 19:16:12.979568 24692 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:16:13.010231 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0814 19:16:13.010248 24692 net.cpp:245] Setting up res5a_branch2a
I0814 19:16:13.010255 24692 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0814 19:16:13.010264 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:13.010278 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.010291 24692 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 19:16:13.010296 24692 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:16:13.010301 24692 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:16:13.011034 24692 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:16:13.011042 24692 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0814 19:16:13.011052 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:16:13.011057 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.011067 24692 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 19:16:13.011071 24692 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:16:13.011075 24692 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:16:13.011082 24692 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:16:13.011086 24692 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0814 19:16:13.011090 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:16:13.011096 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.011106 24692 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 19:16:13.011108 24692 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:16:13.011112 24692 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:16:13.027585 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0814 19:16:13.027601 24692 net.cpp:245] Setting up res5a_branch2b
I0814 19:16:13.027607 24692 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0814 19:16:13.027618 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:13.027623 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.027633 24692 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 19:16:13.027638 24692 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:16:13.027643 24692 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:16:13.028348 24692 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:16:13.028357 24692 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0814 19:16:13.028367 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:16:13.028372 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028378 24692 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 19:16:13.028381 24692 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:16:13.028386 24692 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:16:13.028393 24692 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:16:13.028398 24692 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0814 19:16:13.028401 24692 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:16:13.028406 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028416 24692 net.cpp:184] Created Layer pool5 (37)
I0814 19:16:13.028419 24692 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:16:13.028424 24692 net.cpp:530] pool5 -> pool5
I0814 19:16:13.028457 24692 net.cpp:245] Setting up pool5
I0814 19:16:13.028463 24692 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0814 19:16:13.028470 24692 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 19:16:13.028473 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028479 24692 net.cpp:184] Created Layer fc10 (38)
I0814 19:16:13.028492 24692 net.cpp:561] fc10 <- pool5
I0814 19:16:13.028497 24692 net.cpp:530] fc10 -> fc10
I0814 19:16:13.028776 24692 net.cpp:245] Setting up fc10
I0814 19:16:13.028784 24692 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0814 19:16:13.028791 24692 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0814 19:16:13.028795 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028801 24692 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0814 19:16:13.028805 24692 net.cpp:561] fc10_fc10_0_split <- fc10
I0814 19:16:13.028810 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0814 19:16:13.028815 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0814 19:16:13.028821 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0814 19:16:13.028897 24692 net.cpp:245] Setting up fc10_fc10_0_split
I0814 19:16:13.028903 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028908 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028913 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028916 24692 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:16:13.028921 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028985 24692 net.cpp:184] Created Layer loss (40)
I0814 19:16:13.028990 24692 net.cpp:561] loss <- fc10_fc10_0_split_0
I0814 19:16:13.028995 24692 net.cpp:561] loss <- label_data_1_split_0
I0814 19:16:13.028998 24692 net.cpp:530] loss -> loss
I0814 19:16:13.029150 24692 net.cpp:245] Setting up loss
I0814 19:16:13.029157 24692 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 19:16:13.029161 24692 net.cpp:256]     with loss weight 1
I0814 19:16:13.029167 24692 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 19:16:13.029171 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.029183 24692 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 19:16:13.029187 24692 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0814 19:16:13.029191 24692 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 19:16:13.029196 24692 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 19:16:13.029203 24692 net.cpp:245] Setting up accuracy/top1
I0814 19:16:13.029208 24692 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 19:16:13.029212 24692 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 19:16:13.029217 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.029222 24692 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 19:16:13.029227 24692 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0814 19:16:13.029232 24692 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 19:16:13.029235 24692 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 19:16:13.029242 24692 net.cpp:245] Setting up accuracy/top5
I0814 19:16:13.029247 24692 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 19:16:13.029251 24692 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 19:16:13.029255 24692 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 19:16:13.029259 24692 net.cpp:323] loss needs backward computation.
I0814 19:16:13.029264 24692 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0814 19:16:13.029268 24692 net.cpp:323] fc10 needs backward computation.
I0814 19:16:13.029271 24692 net.cpp:323] pool5 needs backward computation.
I0814 19:16:13.029275 24692 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:16:13.029278 24692 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:16:13.029281 24692 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:16:13.029285 24692 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:16:13.029294 24692 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:16:13.029299 24692 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:16:13.029305 24692 net.cpp:323] pool4 needs backward computation.
I0814 19:16:13.029309 24692 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:16:13.029312 24692 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:16:13.029316 24692 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:16:13.029320 24692 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:16:13.029325 24692 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:16:13.029328 24692 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:16:13.029332 24692 net.cpp:323] pool3 needs backward computation.
I0814 19:16:13.029336 24692 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:16:13.029340 24692 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:16:13.029343 24692 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:16:13.029348 24692 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:16:13.029351 24692 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:16:13.029356 24692 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:16:13.029359 24692 net.cpp:323] pool2 needs backward computation.
I0814 19:16:13.029363 24692 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:16:13.029367 24692 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:16:13.029371 24692 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:16:13.029376 24692 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:16:13.029379 24692 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:16:13.029383 24692 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:16:13.029387 24692 net.cpp:323] pool1 needs backward computation.
I0814 19:16:13.029392 24692 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:16:13.029395 24692 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:16:13.029400 24692 net.cpp:323] conv1b needs backward computation.
I0814 19:16:13.029403 24692 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:16:13.029407 24692 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:16:13.029412 24692 net.cpp:323] conv1a needs backward computation.
I0814 19:16:13.029417 24692 net.cpp:325] data/bias does not need backward computation.
I0814 19:16:13.029420 24692 net.cpp:325] label_data_1_split does not need backward computation.
I0814 19:16:13.029425 24692 net.cpp:325] data does not need backward computation.
I0814 19:16:13.029429 24692 net.cpp:367] This network produces output accuracy/top1
I0814 19:16:13.029433 24692 net.cpp:367] This network produces output accuracy/top5
I0814 19:16:13.029436 24692 net.cpp:367] This network produces output loss
I0814 19:16:13.029465 24692 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0814 19:16:13.029469 24692 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0814 19:16:13.029472 24692 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0814 19:16:13.029475 24692 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 19:16:13.029479 24692 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 19:16:13.029484 24692 net.cpp:407] Network initialization done.
I0814 19:16:13.029532 24692 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.033550 24692 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 19:16:13.037422 24692 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 19:16:13.037443 24692 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 19:16:13.037475 24692 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 19:16:13.037500 24692 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.037751 24692 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 19:16:13.037757 24692 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 19:16:13.037770 24692 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.037917 24692 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 19:16:13.037923 24692 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 19:16:13.037926 24692 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.037943 24692 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038099 24692 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038103 24692 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038117 24692 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038261 24692 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.038266 24692 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 19:16:13.038270 24692 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.038311 24692 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038442 24692 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038447 24692 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038470 24692 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038590 24692 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.038595 24692 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 19:16:13.038599 24692 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.038712 24692 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038836 24692 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038841 24692 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038898 24692 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039021 24692 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.039026 24692 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 19:16:13.039029 24692 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.039373 24692 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039506 24692 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.039511 24692 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.039661 24692 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039786 24692 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.039791 24692 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 19:16:13.039794 24692 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 19:16:13.039808 24692 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 19:16:13.042537 24692 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 19:16:13.042558 24692 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 19:16:13.042592 24692 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 19:16:13.042604 24692 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.042862 24692 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 19:16:13.042876 24692 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 19:16:13.042889 24692 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043036 24692 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 19:16:13.043042 24692 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 19:16:13.043045 24692 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043063 24692 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043220 24692 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043226 24692 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.043241 24692 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043382 24692 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.043387 24692 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 19:16:13.043391 24692 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043431 24692 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043563 24692 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043568 24692 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.043591 24692 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043710 24692 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.043715 24692 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 19:16:13.043720 24692 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043829 24692 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043954 24692 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043959 24692 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.044019 24692 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044145 24692 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.044152 24692 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 19:16:13.044154 24692 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.044512 24692 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044646 24692 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.044651 24692 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.044812 24692 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044939 24692 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.044945 24692 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 19:16:13.044948 24692 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 19:16:13.044961 24692 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 19:16:13.045027 24692 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0814 19:16:13.045032 24692 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0814 19:16:13.045037 24692 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0814 19:16:13.045042 24692 parallel.cpp:59] Starting Optimization
I0814 19:16:13.045044 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:13.045074 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.045090 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.045724 24732 device_alternate.hpp:116] NVML initialized on thread 140557102425856
I0814 19:16:13.058157 24732 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:16:13.058214 24733 device_alternate.hpp:116] NVML initialized on thread 140557094033152
I0814 19:16:13.058955 24733 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:16:13.058970 24734 device_alternate.hpp:116] NVML initialized on thread 140557085640448
I0814 19:16:13.059604 24734 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:16:13.063271 24733 solver.cpp:42] Solver data type: FLOAT
W0814 19:16:13.063776 24733 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:13.063886 24733 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.063892 24733 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.063927 24733 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:13.063940 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.067106 24734 solver.cpp:42] Solver data type: FLOAT
W0814 19:16:13.067440 24734 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:13.067500 24734 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.067504 24734 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.067523 24734 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:13.067529 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.067797 24735 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:13.068756 24736 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:13.068892 24733 data_layer.cpp:185] [1] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:13.069808 24734 data_layer.cpp:185] [2] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:13.069895 24733 data_layer.cpp:209] [1] Output data size: 22, 3, 32, 32
I0814 19:16:13.069905 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.069950 24734 data_layer.cpp:209] [2] Output data size: 22, 3, 32, 32
I0814 19:16:13.069960 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.474692 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 19:16:13.530175 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 19:16:13.531107 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 19:16:13.540727 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 19:16:13.544351 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 19:16:13.553397 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 19:16:13.554816 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 1 3  (limit 8.19G, req 0G)
I0814 19:16:13.571120 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 19:16:13.585873 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.18G, req 0G)
I0814 19:16:13.591416 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 19:16:13.592169 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0G)
I0814 19:16:13.598197 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 19:16:13.616382 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0G)
I0814 19:16:13.620589 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 19:16:13.627022 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0G)
I0814 19:16:13.630693 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0814 19:16:13.678791 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 19:16:13.681542 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 19:16:13.700254 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 19:16:13.702005 24733 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:13.702051 24733 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:13.702085 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 19:16:13.702141 24733 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.702145 24733 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.702160 24733 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:13.702167 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.702877 24777 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:13.702965 24733 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:13.703063 24733 data_layer.cpp:209] (1) Output data size: 17, 3, 32, 32
I0814 19:16:13.703068 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.703743 24778 data_layer.cpp:97] (1) Parser threads: 1
I0814 19:16:13.703750 24778 data_layer.cpp:99] (1) Transformer threads: 1
I0814 19:16:13.704162 24734 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:13.704216 24734 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:13.704305 24734 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.704310 24734 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.704326 24734 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:13.704334 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.705435 24779 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:13.705502 24734 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:13.705593 24734 data_layer.cpp:209] (2) Output data size: 17, 3, 32, 32
I0814 19:16:13.705597 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.706352 24780 data_layer.cpp:97] (2) Parser threads: 1
I0814 19:16:13.706358 24780 data_layer.cpp:99] (2) Transformer threads: 1
I0814 19:16:13.707751 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 19:16:13.709529 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 19:16:13.712790 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 19:16:13.714145 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 19:16:13.718060 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.720329 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.723400 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.725205 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.731783 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 19:16:13.733403 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 19:16:13.736258 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 19:16:13.738384 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 19:16:13.748807 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.750608 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.755908 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.757557 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.789119 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 19:16:13.791642 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 19:16:13.807274 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 19:16:13.808969 24733 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.809353 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 19:16:13.811841 24734 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.855973 24732 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0814 19:16:13.855973 24734 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0814 19:16:13.855973 24733 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0814 19:16:14.053553 24732 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:16:14.060941 24734 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.060961 24734 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.061065 24733 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.061075 24733 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.066520 24732 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.066530 24732 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.073741 24733 solver.cpp:227] Starting Optimization on GPU 1
I0814 19:16:14.073743 24732 solver.cpp:227] Starting Optimization on GPU 0
I0814 19:16:14.073742 24734 solver.cpp:227] Starting Optimization on GPU 2
I0814 19:16:14.073904 24732 solver.cpp:509] Iteration 0, Testing net (#0)
I0814 19:16:14.074002 24781 device_alternate.hpp:116] NVML initialized on thread 140556330219264
I0814 19:16:14.074021 24781 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:16:14.074718 24782 device_alternate.hpp:116] NVML initialized on thread 140556321826560
I0814 19:16:14.074730 24782 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:16:14.074811 24783 device_alternate.hpp:116] NVML initialized on thread 140556313433856
I0814 19:16:14.074823 24783 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:16:14.084015 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0814 19:16:14.084233 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0814 19:16:14.089944 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0814 19:16:14.090318 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0814 19:16:14.091374 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0814 19:16:14.097708 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:14.099035 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:14.100181 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0814 19:16:14.104154 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:14.106902 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:14.108598 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0814 19:16:14.110101 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0814 19:16:14.114573 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0814 19:16:14.114815 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0814 19:16:14.116791 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0814 19:16:14.121697 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0814 19:16:14.122226 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0814 19:16:14.124912 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0814 19:16:14.128100 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0814 19:16:14.130647 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0814 19:16:14.131142 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0814 19:16:14.135939 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0814 19:16:14.137435 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0814 19:16:14.141229 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0814 19:16:14.141580 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0814 19:16:14.146869 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0814 19:16:14.147147 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0814 19:16:14.150694 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0814 19:16:14.153164 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0814 19:16:14.155694 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0814 19:16:14.158076 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 1
I0814 19:16:14.158084 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0814 19:16:14.158088 24732 solver.cpp:594]     Test net output #2: loss = 0.045447 (* 1 = 0.045447 loss)
I0814 19:16:14.158092 24732 solver.cpp:254] [MultiGPU] Initial Test completed
I0814 19:16:14.158114 24732 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 19:16:14.167560 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0814 19:16:14.168050 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0814 19:16:14.169070 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0814 19:16:14.177225 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0814 19:16:14.177783 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0814 19:16:14.178200 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 4 3  (limit 7.79G, req 0G)
I0814 19:16:14.188315 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0814 19:16:14.188869 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0814 19:16:14.189529 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0814 19:16:14.197700 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0814 19:16:14.198231 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0814 19:16:14.198411 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0814 19:16:14.209532 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0814 19:16:14.209708 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0814 19:16:14.210427 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0814 19:16:14.217317 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0814 19:16:14.217862 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0814 19:16:14.218317 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0.01G)
I0814 19:16:14.233507 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0814 19:16:14.234000 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0814 19:16:14.241078 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0814 19:16:14.245455 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0814 19:16:14.247647 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0814 19:16:14.252099 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0814 19:16:14.259434 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0814 19:16:14.265215 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0814 19:16:14.266857 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0814 19:16:14.270272 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.69G, req 0.01G)
I0814 19:16:14.272994 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0814 19:16:14.278158 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0814 19:16:14.296975 24737 data_layer.cpp:97] [1] Parser threads: 1
I0814 19:16:14.296989 24737 data_layer.cpp:99] [1] Transformer threads: 1
I0814 19:16:14.311866 24738 data_layer.cpp:97] [2] Parser threads: 1
I0814 19:16:14.311882 24738 data_layer.cpp:99] [2] Transformer threads: 1
I0814 19:16:14.315632 24719 data_layer.cpp:97] [0] Parser threads: 1
I0814 19:16:14.315642 24719 data_layer.cpp:99] [0] Transformer threads: 1
I0814 19:16:14.325557 24732 solver.cpp:317] Iteration 0 (0.167431 s), loss = 0.000793919
I0814 19:16:14.325578 24732 solver.cpp:334]     Train net output #0: loss = 0.000793919 (* 1 = 0.000793919 loss)
I0814 19:16:14.325582 24732 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0814 19:16:14.351681 24732 solver.cpp:317] Iteration 1 (0.026113 s), loss = 0.00193649
I0814 19:16:14.351732 24732 solver.cpp:334]     Train net output #0: loss = 0.00193649 (* 1 = 0.00193649 loss)
I0814 19:16:14.362622 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0814 19:16:14.362893 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.97G, req 0.01G)
I0814 19:16:14.367728 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0814 19:16:14.372381 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 6 1 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.372856 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0814 19:16:14.375470 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.386443 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 1  (limit 6.42G, req 0.01G)
I0814 19:16:14.386715 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 1  (limit 6.33G, req 0.01G)
I0814 19:16:14.389232 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.393772 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.394269 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0814 19:16:14.396325 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.403630 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0814 19:16:14.404139 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.01G)
I0814 19:16:14.406261 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0814 19:16:14.408789 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.42G, req 0.01G)
I0814 19:16:14.409436 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.33G, req 0.01G)
I0814 19:16:14.411270 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.433231 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0814 19:16:14.433406 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.02G)
I0814 19:16:14.435832 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0814 19:16:14.441102 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0814 19:16:14.441431 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.02G)
I0814 19:16:14.443490 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0814 19:16:14.477843 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.478006 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.33G, req 0.03G)
I0814 19:16:14.479323 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.488833 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.33G, req 0.03G)
I0814 19:16:14.489063 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.489296 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.501296 24732 solver.cpp:317] Iteration 2 (0.149608 s), loss = 0.000792426
I0814 19:16:14.501310 24732 solver.cpp:334]     Train net output #0: loss = 0.000792426 (* 1 = 0.000792426 loss)
I0814 19:16:14.501329 24734 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:14.501329 24733 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:14.501562 24732 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:16.175485 24732 solver.cpp:312] Iteration 100 (58.5377 iter/s, 1.67413s/98 iter), loss = 0.00130582
I0814 19:16:16.175508 24732 solver.cpp:334]     Train net output #0: loss = 0.00130582 (* 1 = 0.00130582 loss)
I0814 19:16:16.175514 24732 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0814 19:16:17.840752 24732 solver.cpp:312] Iteration 200 (60.0522 iter/s, 1.66522s/100 iter), loss = 0.00184174
I0814 19:16:17.840804 24732 solver.cpp:334]     Train net output #0: loss = 0.00184174 (* 1 = 0.00184174 loss)
I0814 19:16:17.840818 24732 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0814 19:16:19.514755 24732 solver.cpp:312] Iteration 300 (59.7392 iter/s, 1.67394s/100 iter), loss = 0.51044
I0814 19:16:19.514816 24732 solver.cpp:334]     Train net output #0: loss = 0.51044 (* 1 = 0.51044 loss)
I0814 19:16:19.514834 24732 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0814 19:16:21.204285 24732 solver.cpp:312] Iteration 400 (59.1897 iter/s, 1.68948s/100 iter), loss = 0.42444
I0814 19:16:21.204354 24732 solver.cpp:334]     Train net output #0: loss = 0.42444 (* 1 = 0.42444 loss)
I0814 19:16:21.204375 24732 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0814 19:16:22.894191 24732 solver.cpp:312] Iteration 500 (59.1768 iter/s, 1.68985s/100 iter), loss = 0.817678
I0814 19:16:22.894218 24732 solver.cpp:334]     Train net output #0: loss = 0.817678 (* 1 = 0.817678 loss)
I0814 19:16:22.894222 24732 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0814 19:16:24.610862 24732 solver.cpp:312] Iteration 600 (58.2541 iter/s, 1.71662s/100 iter), loss = 0.690847
I0814 19:16:24.610885 24732 solver.cpp:334]     Train net output #0: loss = 0.690847 (* 1 = 0.690847 loss)
I0814 19:16:24.610891 24732 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0814 19:16:26.254678 24732 solver.cpp:312] Iteration 700 (60.836 iter/s, 1.64376s/100 iter), loss = 0.100845
I0814 19:16:26.254719 24732 solver.cpp:334]     Train net output #0: loss = 0.100845 (* 1 = 0.100845 loss)
I0814 19:16:26.254725 24732 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0814 19:16:27.127147 24716 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:16:27.913887 24732 solver.cpp:312] Iteration 800 (60.2715 iter/s, 1.65916s/100 iter), loss = 0.277027
I0814 19:16:27.913913 24732 solver.cpp:334]     Train net output #0: loss = 0.277027 (* 1 = 0.277027 loss)
I0814 19:16:27.913918 24732 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0814 19:16:29.575366 24732 solver.cpp:312] Iteration 900 (60.1892 iter/s, 1.66143s/100 iter), loss = 0.0669548
I0814 19:16:29.575422 24732 solver.cpp:334]     Train net output #0: loss = 0.0669546 (* 1 = 0.0669546 loss)
I0814 19:16:29.575438 24732 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0814 19:16:31.214596 24732 solver.cpp:363] Sparsity after update:
I0814 19:16:31.218852 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:16:31.218864 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:16:31.218875 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:16:31.218881 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:16:31.218885 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:16:31.218888 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:16:31.218891 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:16:31.218895 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:16:31.218899 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:16:31.218901 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:16:31.218905 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:16:31.218909 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:16:31.218912 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:16:31.218921 24732 solver.cpp:509] Iteration 1000, Testing net (#0)
I0814 19:16:32.046360 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.796766
I0814 19:16:32.046377 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992059
I0814 19:16:32.046382 24732 solver.cpp:594]     Test net output #2: loss = 0.69504 (* 1 = 0.69504 loss)
I0814 19:16:32.046398 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82745s
I0814 19:16:32.064290 24732 solver.cpp:312] Iteration 1000 (40.1792 iter/s, 2.48885s/100 iter), loss = 0.543985
I0814 19:16:32.064306 24732 solver.cpp:334]     Train net output #0: loss = 0.543984 (* 1 = 0.543984 loss)
I0814 19:16:32.064309 24732 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0814 19:16:33.744786 24732 solver.cpp:312] Iteration 1100 (59.5082 iter/s, 1.68044s/100 iter), loss = 0.352151
I0814 19:16:33.744832 24732 solver.cpp:334]     Train net output #0: loss = 0.352151 (* 1 = 0.352151 loss)
I0814 19:16:33.744844 24732 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0814 19:16:35.395673 24732 solver.cpp:312] Iteration 1200 (60.5753 iter/s, 1.65084s/100 iter), loss = 0.199608
I0814 19:16:35.395732 24732 solver.cpp:334]     Train net output #0: loss = 0.199608 (* 1 = 0.199608 loss)
I0814 19:16:35.395750 24732 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0814 19:16:37.045028 24732 solver.cpp:312] Iteration 1300 (60.6316 iter/s, 1.64931s/100 iter), loss = 0.12136
I0814 19:16:37.045258 24732 solver.cpp:334]     Train net output #0: loss = 0.12136 (* 1 = 0.12136 loss)
I0814 19:16:37.045372 24732 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0814 19:16:38.680660 24732 solver.cpp:312] Iteration 1400 (61.1404 iter/s, 1.63558s/100 iter), loss = 0.326903
I0814 19:16:38.680686 24732 solver.cpp:334]     Train net output #0: loss = 0.326903 (* 1 = 0.326903 loss)
I0814 19:16:38.680692 24732 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0814 19:16:40.356690 24732 solver.cpp:312] Iteration 1500 (59.6666 iter/s, 1.67598s/100 iter), loss = 0.155354
I0814 19:16:40.356762 24732 solver.cpp:334]     Train net output #0: loss = 0.155354 (* 1 = 0.155354 loss)
I0814 19:16:40.356792 24732 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0814 19:16:42.032835 24732 solver.cpp:312] Iteration 1600 (59.6626 iter/s, 1.67609s/100 iter), loss = 0.337506
I0814 19:16:42.032896 24732 solver.cpp:334]     Train net output #0: loss = 0.337505 (* 1 = 0.337505 loss)
I0814 19:16:42.032912 24732 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0814 19:16:43.701170 24732 solver.cpp:312] Iteration 1700 (59.9419 iter/s, 1.66828s/100 iter), loss = 0.361451
I0814 19:16:43.701283 24732 solver.cpp:334]     Train net output #0: loss = 0.361451 (* 1 = 0.361451 loss)
I0814 19:16:43.701297 24732 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0814 19:16:45.326378 24732 solver.cpp:312] Iteration 1800 (61.5325 iter/s, 1.62516s/100 iter), loss = 0.16691
I0814 19:16:45.326438 24732 solver.cpp:334]     Train net output #0: loss = 0.16691 (* 1 = 0.16691 loss)
I0814 19:16:45.326455 24732 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0814 19:16:47.012797 24732 solver.cpp:312] Iteration 1900 (59.299 iter/s, 1.68637s/100 iter), loss = 0.0448435
I0814 19:16:47.012861 24732 solver.cpp:334]     Train net output #0: loss = 0.0448431 (* 1 = 0.0448431 loss)
I0814 19:16:47.012881 24732 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0814 19:16:48.683086 24732 solver.cpp:363] Sparsity after update:
I0814 19:16:48.684618 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:16:48.684628 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:16:48.684635 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:16:48.684639 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:16:48.684648 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:16:48.684659 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:16:48.684662 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:16:48.684670 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:16:48.684674 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:16:48.684677 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:16:48.684685 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:16:48.684689 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:16:48.684697 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:16:48.684708 24732 solver.cpp:509] Iteration 2000, Testing net (#0)
I0814 19:16:49.494000 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.832648
I0814 19:16:49.494019 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.991765
I0814 19:16:49.494025 24732 solver.cpp:594]     Test net output #2: loss = 0.591745 (* 1 = 0.591745 loss)
I0814 19:16:49.494042 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.809306s
I0814 19:16:49.509876 24732 solver.cpp:312] Iteration 2000 (40.048 iter/s, 2.49701s/100 iter), loss = 0.0917527
I0814 19:16:49.509892 24732 solver.cpp:334]     Train net output #0: loss = 0.0917523 (* 1 = 0.0917523 loss)
I0814 19:16:49.509905 24732 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0814 19:16:51.199396 24732 solver.cpp:312] Iteration 2100 (59.1903 iter/s, 1.68947s/100 iter), loss = 0.0906303
I0814 19:16:51.199442 24732 solver.cpp:334]     Train net output #0: loss = 0.0906299 (* 1 = 0.0906299 loss)
I0814 19:16:51.199455 24732 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0814 19:16:52.881829 24732 solver.cpp:312] Iteration 2200 (59.4396 iter/s, 1.68238s/100 iter), loss = 0.0839446
I0814 19:16:52.881852 24732 solver.cpp:334]     Train net output #0: loss = 0.0839441 (* 1 = 0.0839441 loss)
I0814 19:16:52.881858 24732 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0814 19:16:54.530294 24732 solver.cpp:312] Iteration 2300 (60.6644 iter/s, 1.64841s/100 iter), loss = 0.0932575
I0814 19:16:54.530318 24732 solver.cpp:334]     Train net output #0: loss = 0.093257 (* 1 = 0.093257 loss)
I0814 19:16:54.530323 24732 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0814 19:16:56.170112 24732 solver.cpp:312] Iteration 2400 (60.9843 iter/s, 1.63977s/100 iter), loss = 0.0856774
I0814 19:16:56.170161 24732 solver.cpp:334]     Train net output #0: loss = 0.085677 (* 1 = 0.085677 loss)
I0814 19:16:56.170173 24732 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0814 19:16:57.848856 24732 solver.cpp:312] Iteration 2500 (59.5702 iter/s, 1.67869s/100 iter), loss = 0.0447884
I0814 19:16:57.848886 24732 solver.cpp:334]     Train net output #0: loss = 0.0447881 (* 1 = 0.0447881 loss)
I0814 19:16:57.848893 24732 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0814 19:16:59.528997 24732 solver.cpp:312] Iteration 2600 (59.5206 iter/s, 1.68009s/100 iter), loss = 0.0972106
I0814 19:16:59.529027 24732 solver.cpp:334]     Train net output #0: loss = 0.0972103 (* 1 = 0.0972103 loss)
I0814 19:16:59.529034 24732 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0814 19:17:01.180443 24732 solver.cpp:312] Iteration 2700 (60.5548 iter/s, 1.6514s/100 iter), loss = 0.0156879
I0814 19:17:01.180503 24732 solver.cpp:334]     Train net output #0: loss = 0.0156876 (* 1 = 0.0156876 loss)
I0814 19:17:01.180522 24732 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0814 19:17:02.815518 24732 solver.cpp:312] Iteration 2800 (61.1612 iter/s, 1.63502s/100 iter), loss = 0.0305617
I0814 19:17:02.815543 24732 solver.cpp:334]     Train net output #0: loss = 0.0305614 (* 1 = 0.0305614 loss)
I0814 19:17:02.815548 24732 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0814 19:17:04.486409 24732 solver.cpp:312] Iteration 2900 (59.8501 iter/s, 1.67084s/100 iter), loss = 0.117334
I0814 19:17:04.486433 24732 solver.cpp:334]     Train net output #0: loss = 0.117334 (* 1 = 0.117334 loss)
I0814 19:17:04.486439 24732 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0814 19:17:06.104096 24732 solver.cpp:363] Sparsity after update:
I0814 19:17:06.105787 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:17:06.105796 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:17:06.105804 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:17:06.105808 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:17:06.105819 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:17:06.105825 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:17:06.105829 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:17:06.105837 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:17:06.105841 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:17:06.105845 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:17:06.105852 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:17:06.105860 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:17:06.105865 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:17:06.105880 24732 solver.cpp:509] Iteration 3000, Testing net (#0)
I0814 19:17:06.920213 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.833531
I0814 19:17:06.920236 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989118
I0814 19:17:06.920243 24732 solver.cpp:594]     Test net output #2: loss = 0.619492 (* 1 = 0.619492 loss)
I0814 19:17:06.920320 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.814411s
I0814 19:17:06.936173 24732 solver.cpp:312] Iteration 3000 (40.8214 iter/s, 2.44969s/100 iter), loss = 0.0739115
I0814 19:17:06.936189 24732 solver.cpp:334]     Train net output #0: loss = 0.0739112 (* 1 = 0.0739112 loss)
I0814 19:17:06.936194 24732 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0814 19:17:08.613256 24732 solver.cpp:312] Iteration 3100 (59.6293 iter/s, 1.67703s/100 iter), loss = 0.0565981
I0814 19:17:08.613320 24732 solver.cpp:334]     Train net output #0: loss = 0.0565978 (* 1 = 0.0565978 loss)
I0814 19:17:08.613344 24732 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0814 19:17:10.242123 24732 solver.cpp:312] Iteration 3200 (61.3943 iter/s, 1.62882s/100 iter), loss = 0.130127
I0814 19:17:10.242148 24732 solver.cpp:334]     Train net output #0: loss = 0.130127 (* 1 = 0.130127 loss)
I0814 19:17:10.242154 24732 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0814 19:17:11.876368 24732 solver.cpp:312] Iteration 3300 (61.1923 iter/s, 1.63419s/100 iter), loss = 0.0155956
I0814 19:17:11.876392 24732 solver.cpp:334]     Train net output #0: loss = 0.0155953 (* 1 = 0.0155953 loss)
I0814 19:17:11.876397 24732 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0814 19:17:13.494652 24732 solver.cpp:312] Iteration 3400 (61.7957 iter/s, 1.61823s/100 iter), loss = 0.126574
I0814 19:17:13.494881 24732 solver.cpp:334]     Train net output #0: loss = 0.126574 (* 1 = 0.126574 loss)
I0814 19:17:13.494909 24732 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0814 19:17:15.199416 24732 solver.cpp:312] Iteration 3500 (58.6609 iter/s, 1.70471s/100 iter), loss = 0.322088
I0814 19:17:15.199522 24732 solver.cpp:334]     Train net output #0: loss = 0.322088 (* 1 = 0.322088 loss)
I0814 19:17:15.199543 24732 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0814 19:17:16.895972 24732 solver.cpp:312] Iteration 3600 (58.9448 iter/s, 1.6965s/100 iter), loss = 0.0153916
I0814 19:17:16.895997 24732 solver.cpp:334]     Train net output #0: loss = 0.0153914 (* 1 = 0.0153914 loss)
I0814 19:17:16.896000 24732 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0814 19:17:18.549640 24732 solver.cpp:312] Iteration 3700 (60.4735 iter/s, 1.65362s/100 iter), loss = 0.0263022
I0814 19:17:18.549664 24732 solver.cpp:334]     Train net output #0: loss = 0.026302 (* 1 = 0.026302 loss)
I0814 19:17:18.549670 24732 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0814 19:17:20.172900 24732 solver.cpp:312] Iteration 3800 (61.6065 iter/s, 1.62321s/100 iter), loss = 0.0770882
I0814 19:17:20.172935 24732 solver.cpp:334]     Train net output #0: loss = 0.077088 (* 1 = 0.077088 loss)
I0814 19:17:20.172942 24732 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0814 19:17:21.842576 24732 solver.cpp:312] Iteration 3900 (59.8937 iter/s, 1.66963s/100 iter), loss = 0.216337
I0814 19:17:21.842644 24732 solver.cpp:334]     Train net output #0: loss = 0.216337 (* 1 = 0.216337 loss)
I0814 19:17:21.842666 24732 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0814 19:17:23.500836 24732 solver.cpp:363] Sparsity after update:
I0814 19:17:23.502636 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:17:23.502648 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:17:23.502657 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:17:23.502662 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:17:23.502667 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:17:23.502671 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:17:23.502676 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:17:23.502679 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:17:23.502684 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:17:23.502687 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:17:23.502691 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:17:23.502696 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:17:23.502701 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:17:23.502710 24732 solver.cpp:509] Iteration 4000, Testing net (#0)
I0814 19:17:24.315106 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.856472
I0814 19:17:24.315124 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:17:24.315129 24732 solver.cpp:594]     Test net output #2: loss = 0.52191 (* 1 = 0.52191 loss)
I0814 19:17:24.315145 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.812407s
I0814 19:17:24.332573 24783 solver.cpp:409] Finding and applying sparsity: 0.02
I0814 19:17:45.687419 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:17:45.689441 24732 solver.cpp:312] Iteration 4000 (4.19354 iter/s, 23.8462s/100 iter), loss = 0.112001
I0814 19:17:45.689462 24732 solver.cpp:334]     Train net output #0: loss = 0.112001 (* 1 = 0.112001 loss)
I0814 19:17:45.689471 24732 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0814 19:17:47.502506 24732 solver.cpp:312] Iteration 4100 (55.1569 iter/s, 1.81301s/100 iter), loss = 0.112267
I0814 19:17:47.502557 24732 solver.cpp:334]     Train net output #0: loss = 0.112267 (* 1 = 0.112267 loss)
I0814 19:17:47.502570 24732 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0814 19:17:49.201146 24732 solver.cpp:312] Iteration 4200 (58.8725 iter/s, 1.69859s/100 iter), loss = 0.0437128
I0814 19:17:49.201171 24732 solver.cpp:334]     Train net output #0: loss = 0.0437127 (* 1 = 0.0437127 loss)
I0814 19:17:49.201179 24732 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0814 19:17:50.885701 24732 solver.cpp:312] Iteration 4300 (59.3647 iter/s, 1.6845s/100 iter), loss = 0.207798
I0814 19:17:50.885725 24732 solver.cpp:334]     Train net output #0: loss = 0.207798 (* 1 = 0.207798 loss)
I0814 19:17:50.885731 24732 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0814 19:17:52.546205 24732 solver.cpp:312] Iteration 4400 (60.2245 iter/s, 1.66045s/100 iter), loss = 0.243687
I0814 19:17:52.546253 24732 solver.cpp:334]     Train net output #0: loss = 0.243687 (* 1 = 0.243687 loss)
I0814 19:17:52.546265 24732 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0814 19:17:54.173009 24732 solver.cpp:312] Iteration 4500 (61.4722 iter/s, 1.62675s/100 iter), loss = 0.0557635
I0814 19:17:54.173033 24732 solver.cpp:334]     Train net output #0: loss = 0.0557634 (* 1 = 0.0557634 loss)
I0814 19:17:54.173039 24732 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0814 19:17:55.835841 24732 solver.cpp:312] Iteration 4600 (60.1402 iter/s, 1.66278s/100 iter), loss = 0.0198239
I0814 19:17:55.835911 24732 solver.cpp:334]     Train net output #0: loss = 0.0198238 (* 1 = 0.0198238 loss)
I0814 19:17:55.835930 24732 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0814 19:17:57.504529 24732 solver.cpp:312] Iteration 4700 (59.9291 iter/s, 1.66864s/100 iter), loss = 0.0374973
I0814 19:17:57.504554 24732 solver.cpp:334]     Train net output #0: loss = 0.0374973 (* 1 = 0.0374973 loss)
I0814 19:17:57.504559 24732 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0814 19:17:59.191362 24732 solver.cpp:312] Iteration 4800 (59.2845 iter/s, 1.68678s/100 iter), loss = 0.15797
I0814 19:17:59.191383 24732 solver.cpp:334]     Train net output #0: loss = 0.15797 (* 1 = 0.15797 loss)
I0814 19:17:59.191390 24732 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0814 19:18:00.836530 24732 solver.cpp:312] Iteration 4900 (60.7859 iter/s, 1.64512s/100 iter), loss = 0.0388677
I0814 19:18:00.836557 24732 solver.cpp:334]     Train net output #0: loss = 0.0388676 (* 1 = 0.0388676 loss)
I0814 19:18:00.836563 24732 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0814 19:18:02.497040 24732 solver.cpp:363] Sparsity after update:
I0814 19:18:02.498653 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:18:02.498662 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:18:02.498677 24732 net.cpp:2192] conv1b_param_0(0.0104) 
I0814 19:18:02.498684 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:18:02.498693 24732 net.cpp:2192] res2a_branch2a_param_0(0.0174) 
I0814 19:18:02.498703 24732 net.cpp:2192] res2a_branch2b_param_0(0.0139) 
I0814 19:18:02.498708 24732 net.cpp:2192] res3a_branch2a_param_0(0.0191) 
I0814 19:18:02.498715 24732 net.cpp:2192] res3a_branch2b_param_0(0.0174) 
I0814 19:18:02.498724 24732 net.cpp:2192] res4a_branch2a_param_0(0.02) 
I0814 19:18:02.498729 24732 net.cpp:2192] res4a_branch2b_param_0(0.0191) 
I0814 19:18:02.498738 24732 net.cpp:2192] res5a_branch2a_param_0(0.0199) 
I0814 19:18:02.498746 24732 net.cpp:2192] res5a_branch2b_param_0(0.0199) 
I0814 19:18:02.498750 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (46499/2.3599e+06) 0.0197
I0814 19:18:02.498775 24732 solver.cpp:509] Iteration 5000, Testing net (#0)
I0814 19:18:03.181918 24730 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:18:03.260169 24778 blocking_queue.cpp:40] Waiting for datum
I0814 19:18:03.323015 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.872942
I0814 19:18:03.323035 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:18:03.323040 24732 solver.cpp:594]     Test net output #2: loss = 0.472134 (* 1 = 0.472134 loss)
I0814 19:18:03.323060 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824256s
I0814 19:18:03.344606 24783 solver.cpp:409] Finding and applying sparsity: 0.04
I0814 19:18:24.106091 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:18:24.108299 24732 solver.cpp:312] Iteration 5000 (4.29717 iter/s, 23.2711s/100 iter), loss = 0.136508
I0814 19:18:24.108319 24732 solver.cpp:334]     Train net output #0: loss = 0.136508 (* 1 = 0.136508 loss)
I0814 19:18:24.108328 24732 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0814 19:18:26.040460 24732 solver.cpp:312] Iteration 5100 (51.7571 iter/s, 1.9321s/100 iter), loss = 0.108689
I0814 19:18:26.040518 24732 solver.cpp:334]     Train net output #0: loss = 0.108689 (* 1 = 0.108689 loss)
I0814 19:18:26.040537 24732 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0814 19:18:27.716502 24732 solver.cpp:312] Iteration 5200 (59.6662 iter/s, 1.67599s/100 iter), loss = 0.0221201
I0814 19:18:27.716560 24732 solver.cpp:334]     Train net output #0: loss = 0.0221201 (* 1 = 0.0221201 loss)
I0814 19:18:27.716579 24732 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0814 19:18:29.360836 24732 solver.cpp:312] Iteration 5300 (60.8169 iter/s, 1.64428s/100 iter), loss = 0.202996
I0814 19:18:29.360898 24732 solver.cpp:334]     Train net output #0: loss = 0.202996 (* 1 = 0.202996 loss)
I0814 19:18:29.360913 24732 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0814 19:18:31.000803 24732 solver.cpp:312] Iteration 5400 (60.9787 iter/s, 1.63992s/100 iter), loss = 0.029633
I0814 19:18:31.000861 24732 solver.cpp:334]     Train net output #0: loss = 0.0296329 (* 1 = 0.0296329 loss)
I0814 19:18:31.000880 24732 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0814 19:18:32.661025 24732 solver.cpp:312] Iteration 5500 (60.235 iter/s, 1.66016s/100 iter), loss = 0.0400521
I0814 19:18:32.661062 24732 solver.cpp:334]     Train net output #0: loss = 0.040052 (* 1 = 0.040052 loss)
I0814 19:18:32.661069 24732 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0814 19:18:34.336009 24732 solver.cpp:312] Iteration 5600 (59.7038 iter/s, 1.67494s/100 iter), loss = 0.0857176
I0814 19:18:34.336035 24732 solver.cpp:334]     Train net output #0: loss = 0.0857175 (* 1 = 0.0857175 loss)
I0814 19:18:34.336040 24732 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0814 19:18:36.019050 24732 solver.cpp:312] Iteration 5700 (59.4182 iter/s, 1.68299s/100 iter), loss = 0.00253385
I0814 19:18:36.019104 24732 solver.cpp:334]     Train net output #0: loss = 0.00253375 (* 1 = 0.00253375 loss)
I0814 19:18:36.019119 24732 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0814 19:18:37.638882 24732 solver.cpp:312] Iteration 5800 (61.7367 iter/s, 1.61978s/100 iter), loss = 0.0214135
I0814 19:18:37.638912 24732 solver.cpp:334]     Train net output #0: loss = 0.0214134 (* 1 = 0.0214134 loss)
I0814 19:18:37.638918 24732 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0814 19:18:39.277315 24732 solver.cpp:312] Iteration 5900 (61.0359 iter/s, 1.63838s/100 iter), loss = 0.0072344
I0814 19:18:39.277339 24732 solver.cpp:334]     Train net output #0: loss = 0.00723426 (* 1 = 0.00723426 loss)
I0814 19:18:39.277345 24732 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0814 19:18:40.928761 24732 solver.cpp:363] Sparsity after update:
I0814 19:18:40.930397 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:18:40.930405 24732 net.cpp:2192] conv1a_param_0(0.0129) 
I0814 19:18:40.930413 24732 net.cpp:2192] conv1b_param_0(0.0208) 
I0814 19:18:40.930414 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:18:40.930416 24732 net.cpp:2192] res2a_branch2a_param_0(0.0382) 
I0814 19:18:40.930418 24732 net.cpp:2192] res2a_branch2b_param_0(0.0347) 
I0814 19:18:40.930420 24732 net.cpp:2192] res3a_branch2a_param_0(0.0399) 
I0814 19:18:40.930423 24732 net.cpp:2192] res3a_branch2b_param_0(0.0382) 
I0814 19:18:40.930424 24732 net.cpp:2192] res4a_branch2a_param_0(0.0399) 
I0814 19:18:40.930426 24732 net.cpp:2192] res4a_branch2b_param_0(0.0399) 
I0814 19:18:40.930428 24732 net.cpp:2192] res5a_branch2a_param_0(0.0398) 
I0814 19:18:40.930429 24732 net.cpp:2192] res5a_branch2b_param_0(0.0399) 
I0814 19:18:40.930433 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (93649/2.3599e+06) 0.0397
I0814 19:18:40.930451 24732 solver.cpp:509] Iteration 6000, Testing net (#0)
I0814 19:18:41.745473 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861766
I0814 19:18:41.745489 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989412
I0814 19:18:41.745494 24732 solver.cpp:594]     Test net output #2: loss = 0.640049 (* 1 = 0.640049 loss)
I0814 19:18:41.745510 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815031s
I0814 19:18:41.761589 24783 solver.cpp:409] Finding and applying sparsity: 0.06
I0814 19:19:02.545819 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:19:02.547879 24732 solver.cpp:312] Iteration 6000 (4.29739 iter/s, 23.2699s/100 iter), loss = 0.0283866
I0814 19:19:02.547904 24732 solver.cpp:334]     Train net output #0: loss = 0.0283864 (* 1 = 0.0283864 loss)
I0814 19:19:02.547912 24732 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0814 19:19:04.412271 24732 solver.cpp:312] Iteration 6100 (53.6385 iter/s, 1.86433s/100 iter), loss = 0.103321
I0814 19:19:04.412441 24732 solver.cpp:334]     Train net output #0: loss = 0.103321 (* 1 = 0.103321 loss)
I0814 19:19:04.412528 24732 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0814 19:19:06.074769 24732 solver.cpp:312] Iteration 6200 (60.1524 iter/s, 1.66244s/100 iter), loss = 0.0287645
I0814 19:19:06.074792 24732 solver.cpp:334]     Train net output #0: loss = 0.0287644 (* 1 = 0.0287644 loss)
I0814 19:19:06.074797 24732 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0814 19:19:07.768961 24732 solver.cpp:312] Iteration 6300 (59.027 iter/s, 1.69414s/100 iter), loss = 0.0572716
I0814 19:19:07.768986 24732 solver.cpp:334]     Train net output #0: loss = 0.0572715 (* 1 = 0.0572715 loss)
I0814 19:19:07.768992 24732 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0814 19:19:09.423302 24732 solver.cpp:312] Iteration 6400 (60.4489 iter/s, 1.65429s/100 iter), loss = 0.0334602
I0814 19:19:09.423326 24732 solver.cpp:334]     Train net output #0: loss = 0.03346 (* 1 = 0.03346 loss)
I0814 19:19:09.423332 24732 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0814 19:19:11.048087 24732 solver.cpp:312] Iteration 6500 (61.5485 iter/s, 1.62474s/100 iter), loss = 0.0116171
I0814 19:19:11.048140 24732 solver.cpp:334]     Train net output #0: loss = 0.011617 (* 1 = 0.011617 loss)
I0814 19:19:11.048153 24732 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0814 19:19:12.707631 24732 solver.cpp:312] Iteration 6600 (60.2595 iter/s, 1.65949s/100 iter), loss = 0.0662347
I0814 19:19:12.707659 24732 solver.cpp:334]     Train net output #0: loss = 0.0662346 (* 1 = 0.0662346 loss)
I0814 19:19:12.707664 24732 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0814 19:19:14.316431 24732 solver.cpp:312] Iteration 6700 (62.1601 iter/s, 1.60875s/100 iter), loss = 0.000953641
I0814 19:19:14.316483 24732 solver.cpp:334]     Train net output #0: loss = 0.00095356 (* 1 = 0.00095356 loss)
I0814 19:19:14.316500 24732 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0814 19:19:15.946666 24732 solver.cpp:312] Iteration 6800 (61.3428 iter/s, 1.63018s/100 iter), loss = 0.025519
I0814 19:19:15.946691 24732 solver.cpp:334]     Train net output #0: loss = 0.0255189 (* 1 = 0.0255189 loss)
I0814 19:19:15.946696 24732 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0814 19:19:17.612814 24732 solver.cpp:312] Iteration 6900 (60.0205 iter/s, 1.6661s/100 iter), loss = 0.0308791
I0814 19:19:17.612844 24732 solver.cpp:334]     Train net output #0: loss = 0.030879 (* 1 = 0.030879 loss)
I0814 19:19:17.612850 24732 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0814 19:19:19.261881 24732 solver.cpp:363] Sparsity after update:
I0814 19:19:19.263371 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:19:19.263380 24732 net.cpp:2192] conv1a_param_0(0.0262) 
I0814 19:19:19.263386 24732 net.cpp:2192] conv1b_param_0(0.0417) 
I0814 19:19:19.263388 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:19:19.263391 24732 net.cpp:2192] res2a_branch2a_param_0(0.059) 
I0814 19:19:19.263393 24732 net.cpp:2192] res2a_branch2b_param_0(0.0556) 
I0814 19:19:19.263396 24732 net.cpp:2192] res3a_branch2a_param_0(0.059) 
I0814 19:19:19.263397 24732 net.cpp:2192] res3a_branch2b_param_0(0.059) 
I0814 19:19:19.263399 24732 net.cpp:2192] res4a_branch2a_param_0(0.0599) 
I0814 19:19:19.263401 24732 net.cpp:2192] res4a_branch2b_param_0(0.059) 
I0814 19:19:19.263403 24732 net.cpp:2192] res5a_branch2a_param_0(0.0595) 
I0814 19:19:19.263406 24732 net.cpp:2192] res5a_branch2b_param_0(0.0598) 
I0814 19:19:19.263408 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (140133/2.3599e+06) 0.0594
I0814 19:19:19.263428 24732 solver.cpp:509] Iteration 7000, Testing net (#0)
I0814 19:19:20.082304 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861178
I0814 19:19:20.082320 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992353
I0814 19:19:20.082325 24732 solver.cpp:594]     Test net output #2: loss = 0.522509 (* 1 = 0.522509 loss)
I0814 19:19:20.082341 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818885s
I0814 19:19:20.098415 24783 solver.cpp:409] Finding and applying sparsity: 0.08
I0814 19:19:40.632807 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:19:40.634853 24732 solver.cpp:312] Iteration 7000 (4.34378 iter/s, 23.0214s/100 iter), loss = 0.00821189
I0814 19:19:40.634878 24732 solver.cpp:334]     Train net output #0: loss = 0.00821184 (* 1 = 0.00821184 loss)
I0814 19:19:40.634887 24732 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0814 19:19:42.486637 24732 solver.cpp:312] Iteration 7100 (54.0037 iter/s, 1.85172s/100 iter), loss = 0.041075
I0814 19:19:42.486663 24732 solver.cpp:334]     Train net output #0: loss = 0.0410749 (* 1 = 0.0410749 loss)
I0814 19:19:42.486668 24732 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0814 19:19:44.110129 24732 solver.cpp:312] Iteration 7200 (61.5976 iter/s, 1.62344s/100 iter), loss = 0.145245
I0814 19:19:44.110189 24732 solver.cpp:334]     Train net output #0: loss = 0.145245 (* 1 = 0.145245 loss)
I0814 19:19:44.110208 24732 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0814 19:19:45.813191 24732 solver.cpp:312] Iteration 7300 (58.7196 iter/s, 1.70301s/100 iter), loss = 0.0453097
I0814 19:19:45.813215 24732 solver.cpp:334]     Train net output #0: loss = 0.0453096 (* 1 = 0.0453096 loss)
I0814 19:19:45.813220 24732 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0814 19:19:47.454913 24732 solver.cpp:312] Iteration 7400 (60.9135 iter/s, 1.64167s/100 iter), loss = 0.00334753
I0814 19:19:47.454939 24732 solver.cpp:334]     Train net output #0: loss = 0.00334747 (* 1 = 0.00334747 loss)
I0814 19:19:47.454946 24732 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0814 19:19:49.123100 24732 solver.cpp:312] Iteration 7500 (59.9472 iter/s, 1.66814s/100 iter), loss = 0.00608127
I0814 19:19:49.123126 24732 solver.cpp:334]     Train net output #0: loss = 0.00608118 (* 1 = 0.00608118 loss)
I0814 19:19:49.123131 24732 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0814 19:19:50.721071 24732 solver.cpp:312] Iteration 7600 (62.5813 iter/s, 1.59792s/100 iter), loss = 0.0568019
I0814 19:19:50.721122 24732 solver.cpp:334]     Train net output #0: loss = 0.0568018 (* 1 = 0.0568018 loss)
I0814 19:19:50.721134 24732 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0814 19:19:52.371222 24732 solver.cpp:312] Iteration 7700 (60.6024 iter/s, 1.6501s/100 iter), loss = 0.00666856
I0814 19:19:52.371273 24732 solver.cpp:334]     Train net output #0: loss = 0.00666846 (* 1 = 0.00666846 loss)
I0814 19:19:52.371286 24732 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0814 19:19:54.013660 24732 solver.cpp:312] Iteration 7800 (60.887 iter/s, 1.64239s/100 iter), loss = 0.0197749
I0814 19:19:54.013730 24732 solver.cpp:334]     Train net output #0: loss = 0.0197748 (* 1 = 0.0197748 loss)
I0814 19:19:54.013748 24732 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0814 19:19:55.646589 24732 solver.cpp:312] Iteration 7900 (61.2416 iter/s, 1.63288s/100 iter), loss = 0.00503622
I0814 19:19:55.646615 24732 solver.cpp:334]     Train net output #0: loss = 0.00503612 (* 1 = 0.00503612 loss)
I0814 19:19:55.646620 24732 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0814 19:19:57.235558 24732 solver.cpp:363] Sparsity after update:
I0814 19:19:57.237262 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:19:57.237270 24732 net.cpp:2192] conv1a_param_0(0.0392) 
I0814 19:19:57.237278 24732 net.cpp:2192] conv1b_param_0(0.0521) 
I0814 19:19:57.237290 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:19:57.237295 24732 net.cpp:2192] res2a_branch2a_param_0(0.0799) 
I0814 19:19:57.237304 24732 net.cpp:2192] res2a_branch2b_param_0(0.0764) 
I0814 19:19:57.237308 24732 net.cpp:2192] res3a_branch2a_param_0(0.0799) 
I0814 19:19:57.237313 24732 net.cpp:2192] res3a_branch2b_param_0(0.0799) 
I0814 19:19:57.237320 24732 net.cpp:2192] res4a_branch2a_param_0(0.0799) 
I0814 19:19:57.237324 24732 net.cpp:2192] res4a_branch2b_param_0(0.0799) 
I0814 19:19:57.237332 24732 net.cpp:2192] res5a_branch2a_param_0(0.0792) 
I0814 19:19:57.237336 24732 net.cpp:2192] res5a_branch2b_param_0(0.0798) 
I0814 19:19:57.237344 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (187058/2.3599e+06) 0.0793
I0814 19:19:57.237368 24732 solver.cpp:509] Iteration 8000, Testing net (#0)
I0814 19:19:58.061476 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.867942
I0814 19:19:58.061494 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992941
I0814 19:19:58.061499 24732 solver.cpp:594]     Test net output #2: loss = 0.578133 (* 1 = 0.578133 loss)
I0814 19:19:58.061517 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82412s
I0814 19:19:58.077247 24783 solver.cpp:409] Finding and applying sparsity: 0.1
I0814 19:20:18.529935 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:20:18.531989 24732 solver.cpp:312] Iteration 8000 (4.36972 iter/s, 22.8848s/100 iter), loss = 0.0149878
I0814 19:20:18.532011 24732 solver.cpp:334]     Train net output #0: loss = 0.0149877 (* 1 = 0.0149877 loss)
I0814 19:20:18.532019 24732 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0814 19:20:20.336511 24732 solver.cpp:312] Iteration 8100 (55.4181 iter/s, 1.80447s/100 iter), loss = 0.0200955
I0814 19:20:20.336560 24732 solver.cpp:334]     Train net output #0: loss = 0.0200954 (* 1 = 0.0200954 loss)
I0814 19:20:20.336571 24732 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0814 19:20:22.003707 24732 solver.cpp:312] Iteration 8200 (59.9828 iter/s, 1.66714s/100 iter), loss = 0.0121011
I0814 19:20:22.003768 24732 solver.cpp:334]     Train net output #0: loss = 0.012101 (* 1 = 0.012101 loss)
I0814 19:20:22.003790 24732 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0814 19:20:23.671622 24732 solver.cpp:312] Iteration 8300 (59.9571 iter/s, 1.66786s/100 iter), loss = 0.0666558
I0814 19:20:23.671677 24732 solver.cpp:334]     Train net output #0: loss = 0.0666556 (* 1 = 0.0666556 loss)
I0814 19:20:23.671691 24732 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0814 19:20:25.306351 24732 solver.cpp:312] Iteration 8400 (61.1741 iter/s, 1.63468s/100 iter), loss = 0.00567244
I0814 19:20:25.306375 24732 solver.cpp:334]     Train net output #0: loss = 0.00567231 (* 1 = 0.00567231 loss)
I0814 19:20:25.306380 24732 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0814 19:20:26.963155 24732 solver.cpp:312] Iteration 8500 (60.3591 iter/s, 1.65675s/100 iter), loss = 0.0407569
I0814 19:20:26.963201 24732 solver.cpp:334]     Train net output #0: loss = 0.0407568 (* 1 = 0.0407568 loss)
I0814 19:20:26.963213 24732 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0814 19:20:28.594146 24732 solver.cpp:312] Iteration 8600 (61.3143 iter/s, 1.63094s/100 iter), loss = 0.000875662
I0814 19:20:28.594172 24732 solver.cpp:334]     Train net output #0: loss = 0.000875571 (* 1 = 0.000875571 loss)
I0814 19:20:28.594177 24732 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0814 19:20:30.246922 24732 solver.cpp:312] Iteration 8700 (60.5061 iter/s, 1.65273s/100 iter), loss = 0.0311668
I0814 19:20:30.246973 24732 solver.cpp:334]     Train net output #0: loss = 0.0311667 (* 1 = 0.0311667 loss)
I0814 19:20:30.246985 24732 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0814 19:20:31.874056 24732 solver.cpp:312] Iteration 8800 (61.4597 iter/s, 1.62708s/100 iter), loss = 0.00413388
I0814 19:20:31.874080 24732 solver.cpp:334]     Train net output #0: loss = 0.00413377 (* 1 = 0.00413377 loss)
I0814 19:20:31.874085 24732 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0814 19:20:33.548640 24732 solver.cpp:312] Iteration 8900 (59.7183 iter/s, 1.67453s/100 iter), loss = 0.0278068
I0814 19:20:33.548698 24732 solver.cpp:334]     Train net output #0: loss = 0.0278067 (* 1 = 0.0278067 loss)
I0814 19:20:33.548715 24732 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0814 19:20:35.171685 24732 solver.cpp:363] Sparsity after update:
I0814 19:20:35.173333 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:20:35.173342 24732 net.cpp:2192] conv1a_param_0(0.0396) 
I0814 19:20:35.173352 24732 net.cpp:2192] conv1b_param_0(0.079) 
I0814 19:20:35.173357 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:20:35.173360 24732 net.cpp:2192] res2a_branch2a_param_0(0.0972) 
I0814 19:20:35.173365 24732 net.cpp:2192] res2a_branch2b_param_0(0.0972) 
I0814 19:20:35.173368 24732 net.cpp:2192] res3a_branch2a_param_0(0.099) 
I0814 19:20:35.173372 24732 net.cpp:2192] res3a_branch2b_param_0(0.0972) 
I0814 19:20:35.173377 24732 net.cpp:2192] res4a_branch2a_param_0(0.0998) 
I0814 19:20:35.173379 24732 net.cpp:2192] res4a_branch2b_param_0(0.099) 
I0814 19:20:35.173383 24732 net.cpp:2192] res5a_branch2a_param_0(0.0992) 
I0814 19:20:35.173388 24732 net.cpp:2192] res5a_branch2b_param_0(0.0998) 
I0814 19:20:35.173391 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (233683/2.3599e+06) 0.099
I0814 19:20:35.173424 24732 solver.cpp:509] Iteration 9000, Testing net (#0)
I0814 19:20:35.986474 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.858825
I0814 19:20:35.986490 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:20:35.986495 24732 solver.cpp:594]     Test net output #2: loss = 0.585828 (* 1 = 0.585828 loss)
I0814 19:20:35.986510 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813058s
I0814 19:20:36.002166 24783 solver.cpp:409] Finding and applying sparsity: 0.12
I0814 19:20:56.175587 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:20:56.177620 24732 solver.cpp:312] Iteration 9000 (4.41923 iter/s, 22.6284s/100 iter), loss = 0.0145252
I0814 19:20:56.177644 24732 solver.cpp:334]     Train net output #0: loss = 0.0145251 (* 1 = 0.0145251 loss)
I0814 19:20:56.177654 24732 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0814 19:20:57.743373 24716 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:20:57.964887 24732 solver.cpp:312] Iteration 9100 (55.9532 iter/s, 1.78721s/100 iter), loss = 0.0325974
I0814 19:20:57.964913 24732 solver.cpp:334]     Train net output #0: loss = 0.0325973 (* 1 = 0.0325973 loss)
I0814 19:20:57.964920 24732 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0814 19:20:59.581303 24732 solver.cpp:312] Iteration 9200 (61.8672 iter/s, 1.61637s/100 iter), loss = 0.0401114
I0814 19:20:59.581328 24732 solver.cpp:334]     Train net output #0: loss = 0.0401113 (* 1 = 0.0401113 loss)
I0814 19:20:59.581333 24732 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0814 19:21:01.196466 24732 solver.cpp:312] Iteration 9300 (61.9152 iter/s, 1.61511s/100 iter), loss = 0.00562272
I0814 19:21:01.196566 24732 solver.cpp:334]     Train net output #0: loss = 0.0056226 (* 1 = 0.0056226 loss)
I0814 19:21:01.196575 24732 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0814 19:21:02.843112 24732 solver.cpp:312] Iteration 9400 (60.7313 iter/s, 1.6466s/100 iter), loss = 0.0146899
I0814 19:21:02.843158 24732 solver.cpp:334]     Train net output #0: loss = 0.0146898 (* 1 = 0.0146898 loss)
I0814 19:21:02.843170 24732 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0814 19:21:04.458920 24732 solver.cpp:312] Iteration 9500 (61.8906 iter/s, 1.61575s/100 iter), loss = 0.0536371
I0814 19:21:04.458948 24732 solver.cpp:334]     Train net output #0: loss = 0.053637 (* 1 = 0.053637 loss)
I0814 19:21:04.458956 24732 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0814 19:21:06.126191 24732 solver.cpp:312] Iteration 9600 (59.9801 iter/s, 1.66722s/100 iter), loss = 0.0117439
I0814 19:21:06.126253 24732 solver.cpp:334]     Train net output #0: loss = 0.0117438 (* 1 = 0.0117438 loss)
I0814 19:21:06.126271 24732 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0814 19:21:07.790086 24732 solver.cpp:312] Iteration 9700 (60.1019 iter/s, 1.66384s/100 iter), loss = 0.0615666
I0814 19:21:07.790154 24732 solver.cpp:334]     Train net output #0: loss = 0.0615665 (* 1 = 0.0615665 loss)
I0814 19:21:07.790174 24732 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0814 19:21:09.434990 24732 solver.cpp:312] Iteration 9800 (60.7957 iter/s, 1.64485s/100 iter), loss = 0.00999332
I0814 19:21:09.435019 24732 solver.cpp:334]     Train net output #0: loss = 0.0099932 (* 1 = 0.0099932 loss)
I0814 19:21:09.435025 24732 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0814 19:21:11.088933 24732 solver.cpp:312] Iteration 9900 (60.4634 iter/s, 1.65389s/100 iter), loss = 0.00118539
I0814 19:21:11.088958 24732 solver.cpp:334]     Train net output #0: loss = 0.0011853 (* 1 = 0.0011853 loss)
I0814 19:21:11.088964 24732 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0814 19:21:12.676767 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0814 19:21:12.691593 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0814 19:21:12.695134 24732 solver.cpp:363] Sparsity after update:
I0814 19:21:12.697767 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:21:12.697777 24732 net.cpp:2192] conv1a_param_0(0.0525) 
I0814 19:21:12.697785 24732 net.cpp:2192] conv1b_param_0(0.0903) 
I0814 19:21:12.697796 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:21:12.697805 24732 net.cpp:2192] res2a_branch2a_param_0(0.118) 
I0814 19:21:12.697818 24732 net.cpp:2192] res2a_branch2b_param_0(0.118) 
I0814 19:21:12.697825 24732 net.cpp:2192] res3a_branch2a_param_0(0.12) 
I0814 19:21:12.697842 24732 net.cpp:2192] res3a_branch2b_param_0(0.118) 
I0814 19:21:12.697851 24732 net.cpp:2192] res4a_branch2a_param_0(0.12) 
I0814 19:21:12.697862 24732 net.cpp:2192] res4a_branch2b_param_0(0.12) 
I0814 19:21:12.697872 24732 net.cpp:2192] res5a_branch2a_param_0(0.119) 
I0814 19:21:12.697882 24732 net.cpp:2192] res5a_branch2b_param_0(0.12) 
I0814 19:21:12.697887 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (280283/2.3599e+06) 0.119
I0814 19:21:12.697904 24732 solver.cpp:509] Iteration 10000, Testing net (#0)
I0814 19:21:13.501556 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861766
I0814 19:21:13.501576 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:21:13.501583 24732 solver.cpp:594]     Test net output #2: loss = 0.536421 (* 1 = 0.536421 loss)
I0814 19:21:13.501601 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.803669s
I0814 19:21:13.517264 24783 solver.cpp:409] Finding and applying sparsity: 0.14
I0814 19:21:33.793207 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:21:33.795333 24732 solver.cpp:312] Iteration 10000 (4.40417 iter/s, 22.7058s/100 iter), loss = 0.00449185
I0814 19:21:33.795351 24732 solver.cpp:334]     Train net output #0: loss = 0.00449177 (* 1 = 0.00449177 loss)
I0814 19:21:33.795356 24732 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0814 19:21:35.640600 24732 solver.cpp:312] Iteration 10100 (54.1945 iter/s, 1.84521s/100 iter), loss = 0.00606891
I0814 19:21:35.640656 24732 solver.cpp:334]     Train net output #0: loss = 0.00606883 (* 1 = 0.00606883 loss)
I0814 19:21:35.640671 24732 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0814 19:21:37.321328 24732 solver.cpp:312] Iteration 10200 (59.4998 iter/s, 1.68068s/100 iter), loss = 0.0389266
I0814 19:21:37.321400 24732 solver.cpp:334]     Train net output #0: loss = 0.0389265 (* 1 = 0.0389265 loss)
I0814 19:21:37.321422 24732 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0814 19:21:38.953063 24732 solver.cpp:312] Iteration 10300 (61.2864 iter/s, 1.63168s/100 iter), loss = 0.00270737
I0814 19:21:38.953127 24732 solver.cpp:334]     Train net output #0: loss = 0.00270732 (* 1 = 0.00270732 loss)
I0814 19:21:38.953147 24732 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0814 19:21:40.609830 24732 solver.cpp:312] Iteration 10400 (60.3605 iter/s, 1.65671s/100 iter), loss = 0.0197287
I0814 19:21:40.609854 24732 solver.cpp:334]     Train net output #0: loss = 0.0197287 (* 1 = 0.0197287 loss)
I0814 19:21:40.609860 24732 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0814 19:21:42.257455 24732 solver.cpp:312] Iteration 10500 (60.6953 iter/s, 1.64757s/100 iter), loss = 0.0205076
I0814 19:21:42.257510 24732 solver.cpp:334]     Train net output #0: loss = 0.0205075 (* 1 = 0.0205075 loss)
I0814 19:21:42.257524 24732 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0814 19:21:43.932659 24732 solver.cpp:312] Iteration 10600 (59.6961 iter/s, 1.67515s/100 iter), loss = 0.0260483
I0814 19:21:43.932705 24732 solver.cpp:334]     Train net output #0: loss = 0.0260483 (* 1 = 0.0260483 loss)
I0814 19:21:43.932713 24732 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0814 19:21:45.592733 24732 solver.cpp:312] Iteration 10700 (60.2402 iter/s, 1.66002s/100 iter), loss = 0.0125613
I0814 19:21:45.592763 24732 solver.cpp:334]     Train net output #0: loss = 0.0125612 (* 1 = 0.0125612 loss)
I0814 19:21:45.592772 24732 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0814 19:21:47.228042 24732 solver.cpp:312] Iteration 10800 (61.1524 iter/s, 1.63526s/100 iter), loss = 0.0351802
I0814 19:21:47.228106 24732 solver.cpp:334]     Train net output #0: loss = 0.0351801 (* 1 = 0.0351801 loss)
I0814 19:21:47.228124 24732 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0814 19:21:48.912852 24732 solver.cpp:312] Iteration 10900 (59.3558 iter/s, 1.68476s/100 iter), loss = 0.00205397
I0814 19:21:48.913084 24732 solver.cpp:334]     Train net output #0: loss = 0.00205388 (* 1 = 0.00205388 loss)
I0814 19:21:48.913091 24732 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0814 19:21:50.512024 24732 solver.cpp:363] Sparsity after update:
I0814 19:21:50.513788 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:21:50.513798 24732 net.cpp:2192] conv1a_param_0(0.0658) 
I0814 19:21:50.513808 24732 net.cpp:2192] conv1b_param_0(0.139) 
I0814 19:21:50.513813 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:21:50.513818 24732 net.cpp:2192] res2a_branch2a_param_0(0.139) 
I0814 19:21:50.513821 24732 net.cpp:2192] res2a_branch2b_param_0(0.139) 
I0814 19:21:50.513826 24732 net.cpp:2192] res3a_branch2a_param_0(0.139) 
I0814 19:21:50.513829 24732 net.cpp:2192] res3a_branch2b_param_0(0.139) 
I0814 19:21:50.513833 24732 net.cpp:2192] res4a_branch2a_param_0(0.14) 
I0814 19:21:50.513835 24732 net.cpp:2192] res4a_branch2b_param_0(0.139) 
I0814 19:21:50.513839 24732 net.cpp:2192] res5a_branch2a_param_0(0.138) 
I0814 19:21:50.513841 24732 net.cpp:2192] res5a_branch2b_param_0(0.14) 
I0814 19:21:50.513846 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (326330/2.3599e+06) 0.138
I0814 19:21:50.513871 24732 solver.cpp:509] Iteration 11000, Testing net (#0)
I0814 19:21:51.324314 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.884119
I0814 19:21:51.324334 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:21:51.324342 24732 solver.cpp:594]     Test net output #2: loss = 0.458849 (* 1 = 0.458849 loss)
I0814 19:21:51.324357 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810459s
I0814 19:21:51.341850 24783 solver.cpp:409] Finding and applying sparsity: 0.16
I0814 19:22:11.165117 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:22:11.167142 24732 solver.cpp:312] Iteration 11000 (4.49364 iter/s, 22.2537s/100 iter), loss = 0.0170473
I0814 19:22:11.167165 24732 solver.cpp:334]     Train net output #0: loss = 0.0170472 (* 1 = 0.0170472 loss)
I0814 19:22:11.167173 24732 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0814 19:22:13.029328 24732 solver.cpp:312] Iteration 11100 (53.702 iter/s, 1.86213s/100 iter), loss = 0.00276705
I0814 19:22:13.029353 24732 solver.cpp:334]     Train net output #0: loss = 0.00276692 (* 1 = 0.00276692 loss)
I0814 19:22:13.029361 24732 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0814 19:22:14.648807 24732 solver.cpp:312] Iteration 11200 (61.7503 iter/s, 1.61943s/100 iter), loss = 0.0562505
I0814 19:22:14.648833 24732 solver.cpp:334]     Train net output #0: loss = 0.0562503 (* 1 = 0.0562503 loss)
I0814 19:22:14.648840 24732 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0814 19:22:16.291115 24732 solver.cpp:312] Iteration 11300 (60.8919 iter/s, 1.64226s/100 iter), loss = 0.000931007
I0814 19:22:16.291167 24732 solver.cpp:334]     Train net output #0: loss = 0.000930859 (* 1 = 0.000930859 loss)
I0814 19:22:16.291179 24732 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0814 19:22:17.943562 24732 solver.cpp:312] Iteration 11400 (60.5182 iter/s, 1.6524s/100 iter), loss = 0.00305178
I0814 19:22:17.943584 24732 solver.cpp:334]     Train net output #0: loss = 0.00305163 (* 1 = 0.00305163 loss)
I0814 19:22:17.943590 24732 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0814 19:22:19.580310 24732 solver.cpp:312] Iteration 11500 (61.0987 iter/s, 1.6367s/100 iter), loss = 0.00348338
I0814 19:22:19.580376 24732 solver.cpp:334]     Train net output #0: loss = 0.00348322 (* 1 = 0.00348322 loss)
I0814 19:22:19.580396 24732 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0814 19:22:21.264436 24732 solver.cpp:312] Iteration 11600 (59.3799 iter/s, 1.68407s/100 iter), loss = 0.00606551
I0814 19:22:21.264461 24732 solver.cpp:334]     Train net output #0: loss = 0.00606534 (* 1 = 0.00606534 loss)
I0814 19:22:21.264467 24732 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0814 19:22:22.932440 24732 solver.cpp:312] Iteration 11700 (59.9538 iter/s, 1.66795s/100 iter), loss = 0.00193452
I0814 19:22:22.932466 24732 solver.cpp:334]     Train net output #0: loss = 0.00193434 (* 1 = 0.00193434 loss)
I0814 19:22:22.932471 24732 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0814 19:22:24.620512 24732 solver.cpp:312] Iteration 11800 (59.241 iter/s, 1.68802s/100 iter), loss = 0.000500849
I0814 19:22:24.620559 24732 solver.cpp:334]     Train net output #0: loss = 0.000500674 (* 1 = 0.000500674 loss)
I0814 19:22:24.620573 24732 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0814 19:22:26.287770 24732 solver.cpp:312] Iteration 11900 (59.9806 iter/s, 1.66721s/100 iter), loss = 0.173825
I0814 19:22:26.287799 24732 solver.cpp:334]     Train net output #0: loss = 0.173825 (* 1 = 0.173825 loss)
I0814 19:22:26.287806 24732 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0814 19:22:27.976153 24732 solver.cpp:363] Sparsity after update:
I0814 19:22:27.978051 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:22:27.978063 24732 net.cpp:2192] conv1a_param_0(0.0658) 
I0814 19:22:27.978130 24732 net.cpp:2192] conv1b_param_0(0.153) 
I0814 19:22:27.978138 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:22:27.978154 24732 net.cpp:2192] res2a_branch2a_param_0(0.16) 
I0814 19:22:27.978165 24732 net.cpp:2192] res2a_branch2b_param_0(0.16) 
I0814 19:22:27.978174 24732 net.cpp:2192] res3a_branch2a_param_0(0.16) 
I0814 19:22:27.978183 24732 net.cpp:2192] res3a_branch2b_param_0(0.16) 
I0814 19:22:27.978193 24732 net.cpp:2192] res4a_branch2a_param_0(0.16) 
I0814 19:22:27.978204 24732 net.cpp:2192] res4a_branch2b_param_0(0.16) 
I0814 19:22:27.978214 24732 net.cpp:2192] res5a_branch2a_param_0(0.158) 
I0814 19:22:27.978224 24732 net.cpp:2192] res5a_branch2b_param_0(0.16) 
I0814 19:22:27.978235 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (373383/2.3599e+06) 0.158
I0814 19:22:27.978265 24732 solver.cpp:509] Iteration 12000, Testing net (#0)
I0814 19:22:28.819255 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.894707
I0814 19:22:28.819274 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:22:28.819279 24732 solver.cpp:594]     Test net output #2: loss = 0.447308 (* 1 = 0.447308 loss)
I0814 19:22:28.819304 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.841012s
I0814 19:22:28.836529 24783 solver.cpp:409] Finding and applying sparsity: 0.18
I0814 19:22:48.833614 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:22:48.835667 24732 solver.cpp:312] Iteration 12000 (4.43512 iter/s, 22.5473s/100 iter), loss = 0.000283893
I0814 19:22:48.835690 24732 solver.cpp:334]     Train net output #0: loss = 0.000283734 (* 1 = 0.000283734 loss)
I0814 19:22:48.835701 24732 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0814 19:22:50.631300 24732 solver.cpp:312] Iteration 12100 (55.6924 iter/s, 1.79558s/100 iter), loss = 0.00120777
I0814 19:22:50.631325 24732 solver.cpp:334]     Train net output #0: loss = 0.00120761 (* 1 = 0.00120761 loss)
I0814 19:22:50.631331 24732 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0814 19:22:52.261951 24732 solver.cpp:312] Iteration 12200 (61.3272 iter/s, 1.6306s/100 iter), loss = 0.0508494
I0814 19:22:52.261976 24732 solver.cpp:334]     Train net output #0: loss = 0.0508492 (* 1 = 0.0508492 loss)
I0814 19:22:52.261982 24732 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0814 19:22:53.908377 24732 solver.cpp:312] Iteration 12300 (60.7395 iter/s, 1.64637s/100 iter), loss = 0.000822553
I0814 19:22:53.908408 24732 solver.cpp:334]     Train net output #0: loss = 0.000822384 (* 1 = 0.000822384 loss)
I0814 19:22:53.908416 24732 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0814 19:22:55.585548 24732 solver.cpp:312] Iteration 12400 (59.6261 iter/s, 1.67712s/100 iter), loss = 0.0011282
I0814 19:22:55.585572 24732 solver.cpp:334]     Train net output #0: loss = 0.00112804 (* 1 = 0.00112804 loss)
I0814 19:22:55.585577 24732 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0814 19:22:57.256640 24732 solver.cpp:312] Iteration 12500 (59.843 iter/s, 1.67104s/100 iter), loss = 0.00692147
I0814 19:22:57.256667 24732 solver.cpp:334]     Train net output #0: loss = 0.00692131 (* 1 = 0.00692131 loss)
I0814 19:22:57.256672 24732 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0814 19:22:58.964196 24732 solver.cpp:312] Iteration 12600 (58.5654 iter/s, 1.70749s/100 iter), loss = 0.00467734
I0814 19:22:58.964254 24732 solver.cpp:334]     Train net output #0: loss = 0.00467716 (* 1 = 0.00467716 loss)
I0814 19:22:58.964267 24732 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0814 19:23:00.586798 24732 solver.cpp:312] Iteration 12700 (61.6311 iter/s, 1.62256s/100 iter), loss = 0.00160508
I0814 19:23:00.586823 24732 solver.cpp:334]     Train net output #0: loss = 0.0016049 (* 1 = 0.0016049 loss)
I0814 19:23:00.586829 24732 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0814 19:23:02.248893 24732 solver.cpp:312] Iteration 12800 (60.167 iter/s, 1.66204s/100 iter), loss = 0.0407584
I0814 19:23:02.248947 24732 solver.cpp:334]     Train net output #0: loss = 0.0407582 (* 1 = 0.0407582 loss)
I0814 19:23:02.248961 24732 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0814 19:23:03.912156 24732 solver.cpp:312] Iteration 12900 (60.1246 iter/s, 1.66321s/100 iter), loss = 0.0214099
I0814 19:23:03.912184 24732 solver.cpp:334]     Train net output #0: loss = 0.0214097 (* 1 = 0.0214097 loss)
I0814 19:23:03.912191 24732 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0814 19:23:05.556000 24732 solver.cpp:363] Sparsity after update:
I0814 19:23:05.557713 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:23:05.557723 24732 net.cpp:2192] conv1a_param_0(0.0787) 
I0814 19:23:05.557729 24732 net.cpp:2192] conv1b_param_0(0.167) 
I0814 19:23:05.557730 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:23:05.557732 24732 net.cpp:2192] res2a_branch2a_param_0(0.177) 
I0814 19:23:05.557734 24732 net.cpp:2192] res2a_branch2b_param_0(0.174) 
I0814 19:23:05.557736 24732 net.cpp:2192] res3a_branch2a_param_0(0.179) 
I0814 19:23:05.557739 24732 net.cpp:2192] res3a_branch2b_param_0(0.177) 
I0814 19:23:05.557741 24732 net.cpp:2192] res4a_branch2a_param_0(0.18) 
I0814 19:23:05.557744 24732 net.cpp:2192] res4a_branch2b_param_0(0.179) 
I0814 19:23:05.557746 24732 net.cpp:2192] res5a_branch2a_param_0(0.176) 
I0814 19:23:05.557749 24732 net.cpp:2192] res5a_branch2b_param_0(0.179) 
I0814 19:23:05.557750 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (418411/2.3599e+06) 0.177
I0814 19:23:05.557770 24732 solver.cpp:509] Iteration 13000, Testing net (#0)
I0814 19:23:06.382977 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.897354
I0814 19:23:06.382997 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:23:06.383003 24732 solver.cpp:594]     Test net output #2: loss = 0.404941 (* 1 = 0.404941 loss)
I0814 19:23:06.383019 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825221s
I0814 19:23:06.398967 24783 solver.cpp:409] Finding and applying sparsity: 0.2
I0814 19:23:25.906666 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:23:25.908761 24732 solver.cpp:312] Iteration 13000 (4.54628 iter/s, 21.996s/100 iter), loss = 0.00205158
I0814 19:23:25.908779 24732 solver.cpp:334]     Train net output #0: loss = 0.00205138 (* 1 = 0.00205138 loss)
I0814 19:23:25.908784 24732 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0814 19:23:27.752334 24732 solver.cpp:312] Iteration 13100 (54.2442 iter/s, 1.84352s/100 iter), loss = 0.00191839
I0814 19:23:27.752398 24732 solver.cpp:334]     Train net output #0: loss = 0.0019182 (* 1 = 0.0019182 loss)
I0814 19:23:27.752418 24732 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0814 19:23:29.383154 24732 solver.cpp:312] Iteration 13200 (61.3208 iter/s, 1.63077s/100 iter), loss = 0.00631823
I0814 19:23:29.383306 24732 solver.cpp:334]     Train net output #0: loss = 0.00631801 (* 1 = 0.00631801 loss)
I0814 19:23:29.383329 24732 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0814 19:23:31.001551 24732 solver.cpp:312] Iteration 13300 (61.7915 iter/s, 1.61835s/100 iter), loss = 0.00179946
I0814 19:23:31.001574 24732 solver.cpp:334]     Train net output #0: loss = 0.00179925 (* 1 = 0.00179925 loss)
I0814 19:23:31.001577 24732 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0814 19:23:32.649865 24732 solver.cpp:312] Iteration 13400 (60.67 iter/s, 1.64826s/100 iter), loss = 0.000793511
I0814 19:23:32.649931 24732 solver.cpp:334]     Train net output #0: loss = 0.00079331 (* 1 = 0.00079331 loss)
I0814 19:23:32.649951 24732 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0814 19:23:34.293196 24732 solver.cpp:312] Iteration 13500 (60.8538 iter/s, 1.64328s/100 iter), loss = 0.0358798
I0814 19:23:34.293220 24732 solver.cpp:334]     Train net output #0: loss = 0.0358796 (* 1 = 0.0358796 loss)
I0814 19:23:34.293226 24732 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0814 19:23:35.947861 24732 solver.cpp:312] Iteration 13600 (60.4372 iter/s, 1.65461s/100 iter), loss = 0.00118653
I0814 19:23:35.947906 24732 solver.cpp:334]     Train net output #0: loss = 0.00118635 (* 1 = 0.00118635 loss)
I0814 19:23:35.947918 24732 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0814 19:23:36.506425 24716 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:23:37.589046 24732 solver.cpp:312] Iteration 13700 (60.9422 iter/s, 1.6409s/100 iter), loss = 0.020078
I0814 19:23:37.589074 24732 solver.cpp:334]     Train net output #0: loss = 0.0200778 (* 1 = 0.0200778 loss)
I0814 19:23:37.589082 24732 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0814 19:23:39.243427 24732 solver.cpp:312] Iteration 13800 (60.439 iter/s, 1.65456s/100 iter), loss = 0.00647985
I0814 19:23:39.243451 24732 solver.cpp:334]     Train net output #0: loss = 0.00647965 (* 1 = 0.00647965 loss)
I0814 19:23:39.243458 24732 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0814 19:23:40.875916 24732 solver.cpp:312] Iteration 13900 (61.2579 iter/s, 1.63244s/100 iter), loss = 0.00362782
I0814 19:23:40.875965 24732 solver.cpp:334]     Train net output #0: loss = 0.00362761 (* 1 = 0.00362761 loss)
I0814 19:23:40.875979 24732 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0814 19:23:42.506799 24732 solver.cpp:363] Sparsity after update:
I0814 19:23:42.508273 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:23:42.508283 24732 net.cpp:2192] conv1a_param_0(0.0917) 
I0814 19:23:42.508291 24732 net.cpp:2192] conv1b_param_0(0.194) 
I0814 19:23:42.508302 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:23:42.508308 24732 net.cpp:2192] res2a_branch2a_param_0(0.198) 
I0814 19:23:42.508316 24732 net.cpp:2192] res2a_branch2b_param_0(0.194) 
I0814 19:23:42.508321 24732 net.cpp:2192] res3a_branch2a_param_0(0.2) 
I0814 19:23:42.508324 24732 net.cpp:2192] res3a_branch2b_param_0(0.198) 
I0814 19:23:42.508333 24732 net.cpp:2192] res4a_branch2a_param_0(0.2) 
I0814 19:23:42.508337 24732 net.cpp:2192] res4a_branch2b_param_0(0.2) 
I0814 19:23:42.508340 24732 net.cpp:2192] res5a_branch2a_param_0(0.196) 
I0814 19:23:42.508348 24732 net.cpp:2192] res5a_branch2b_param_0(0.199) 
I0814 19:23:42.508368 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (465402/2.3599e+06) 0.197
I0814 19:23:42.508383 24732 solver.cpp:509] Iteration 14000, Testing net (#0)
I0814 19:23:43.317541 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902354
I0814 19:23:43.317560 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:23:43.317567 24732 solver.cpp:594]     Test net output #2: loss = 0.397323 (* 1 = 0.397323 loss)
I0814 19:23:43.317584 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.809173s
I0814 19:23:43.333268 24783 solver.cpp:409] Finding and applying sparsity: 0.22
I0814 19:24:02.541982 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:24:02.544035 24732 solver.cpp:312] Iteration 14000 (4.6152 iter/s, 21.6675s/100 iter), loss = 0.00258346
I0814 19:24:02.544055 24732 solver.cpp:334]     Train net output #0: loss = 0.00258325 (* 1 = 0.00258325 loss)
I0814 19:24:02.544064 24732 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0814 19:24:04.419647 24732 solver.cpp:312] Iteration 14100 (53.3176 iter/s, 1.87555s/100 iter), loss = 0.0185316
I0814 19:24:04.419674 24732 solver.cpp:334]     Train net output #0: loss = 0.0185314 (* 1 = 0.0185314 loss)
I0814 19:24:04.419679 24732 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0814 19:24:06.073278 24732 solver.cpp:312] Iteration 14200 (60.4749 iter/s, 1.65358s/100 iter), loss = 0.00103831
I0814 19:24:06.073298 24732 solver.cpp:334]     Train net output #0: loss = 0.00103809 (* 1 = 0.00103809 loss)
I0814 19:24:06.073302 24732 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0814 19:24:07.704717 24732 solver.cpp:312] Iteration 14300 (61.2975 iter/s, 1.63139s/100 iter), loss = 0.00158774
I0814 19:24:07.704741 24732 solver.cpp:334]     Train net output #0: loss = 0.00158754 (* 1 = 0.00158754 loss)
I0814 19:24:07.704747 24732 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0814 19:24:09.384285 24732 solver.cpp:312] Iteration 14400 (59.541 iter/s, 1.67951s/100 iter), loss = 0.00595956
I0814 19:24:09.384310 24732 solver.cpp:334]     Train net output #0: loss = 0.00595936 (* 1 = 0.00595936 loss)
I0814 19:24:09.384316 24732 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0814 19:24:11.037570 24732 solver.cpp:312] Iteration 14500 (60.4876 iter/s, 1.65323s/100 iter), loss = 0.0100729
I0814 19:24:11.037771 24732 solver.cpp:334]     Train net output #0: loss = 0.0100727 (* 1 = 0.0100727 loss)
I0814 19:24:11.037778 24732 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0814 19:24:12.697700 24732 solver.cpp:312] Iteration 14600 (60.2381 iter/s, 1.66008s/100 iter), loss = 0.0103488
I0814 19:24:12.697774 24732 solver.cpp:334]     Train net output #0: loss = 0.0103486 (* 1 = 0.0103486 loss)
I0814 19:24:12.697798 24732 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0814 19:24:14.341539 24732 solver.cpp:312] Iteration 14700 (60.8353 iter/s, 1.64378s/100 iter), loss = 0.00148125
I0814 19:24:14.341588 24732 solver.cpp:334]     Train net output #0: loss = 0.00148105 (* 1 = 0.00148105 loss)
I0814 19:24:14.341601 24732 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0814 19:24:16.006736 24732 solver.cpp:312] Iteration 14800 (60.0548 iter/s, 1.66515s/100 iter), loss = 0.00496316
I0814 19:24:16.006784 24732 solver.cpp:334]     Train net output #0: loss = 0.00496297 (* 1 = 0.00496297 loss)
I0814 19:24:16.006798 24732 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0814 19:24:17.674269 24732 solver.cpp:312] Iteration 14900 (59.9706 iter/s, 1.66748s/100 iter), loss = 0.00170011
I0814 19:24:17.674320 24732 solver.cpp:334]     Train net output #0: loss = 0.00169992 (* 1 = 0.00169992 loss)
I0814 19:24:17.674335 24732 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0814 19:24:19.316359 24732 solver.cpp:363] Sparsity after update:
I0814 19:24:19.317980 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:24:19.317988 24732 net.cpp:2192] conv1a_param_0(0.105) 
I0814 19:24:19.317994 24732 net.cpp:2192] conv1b_param_0(0.208) 
I0814 19:24:19.317996 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:24:19.317999 24732 net.cpp:2192] res2a_branch2a_param_0(0.219) 
I0814 19:24:19.318002 24732 net.cpp:2192] res2a_branch2b_param_0(0.215) 
I0814 19:24:19.318004 24732 net.cpp:2192] res3a_branch2a_param_0(0.219) 
I0814 19:24:19.318006 24732 net.cpp:2192] res3a_branch2b_param_0(0.219) 
I0814 19:24:19.318008 24732 net.cpp:2192] res4a_branch2a_param_0(0.22) 
I0814 19:24:19.318011 24732 net.cpp:2192] res4a_branch2b_param_0(0.219) 
I0814 19:24:19.318012 24732 net.cpp:2192] res5a_branch2a_param_0(0.213) 
I0814 19:24:19.318014 24732 net.cpp:2192] res5a_branch2b_param_0(0.219) 
I0814 19:24:19.318015 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (509010/2.3599e+06) 0.216
I0814 19:24:19.318035 24732 solver.cpp:509] Iteration 15000, Testing net (#0)
I0814 19:24:20.124647 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.893825
I0814 19:24:20.124665 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 19:24:20.124670 24732 solver.cpp:594]     Test net output #2: loss = 0.406789 (* 1 = 0.406789 loss)
I0814 19:24:20.124685 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.806622s
I0814 19:24:20.140568 24783 solver.cpp:409] Finding and applying sparsity: 0.24
I0814 19:24:39.458744 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:24:39.460844 24732 solver.cpp:312] Iteration 15000 (4.59011 iter/s, 21.786s/100 iter), loss = 0.00224286
I0814 19:24:39.460861 24732 solver.cpp:334]     Train net output #0: loss = 0.00224267 (* 1 = 0.00224267 loss)
I0814 19:24:39.460866 24732 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0814 19:24:41.355484 24732 solver.cpp:312] Iteration 15100 (52.7822 iter/s, 1.89458s/100 iter), loss = 0.000958177
I0814 19:24:41.355543 24732 solver.cpp:334]     Train net output #0: loss = 0.000957986 (* 1 = 0.000957986 loss)
I0814 19:24:41.355561 24732 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0814 19:24:43.014632 24732 solver.cpp:312] Iteration 15200 (60.2738 iter/s, 1.6591s/100 iter), loss = 0.000888344
I0814 19:24:43.014657 24732 solver.cpp:334]     Train net output #0: loss = 0.000888147 (* 1 = 0.000888147 loss)
I0814 19:24:43.014662 24732 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0814 19:24:44.655565 24732 solver.cpp:312] Iteration 15300 (60.9428 iter/s, 1.64088s/100 iter), loss = 0.00125217
I0814 19:24:44.655661 24732 solver.cpp:334]     Train net output #0: loss = 0.00125198 (* 1 = 0.00125198 loss)
I0814 19:24:44.655669 24732 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0814 19:24:46.305740 24732 solver.cpp:312] Iteration 15400 (60.6016 iter/s, 1.65012s/100 iter), loss = 0.0253144
I0814 19:24:46.305766 24732 solver.cpp:334]     Train net output #0: loss = 0.0253142 (* 1 = 0.0253142 loss)
I0814 19:24:46.305771 24732 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0814 19:24:47.958312 24732 solver.cpp:312] Iteration 15500 (60.5137 iter/s, 1.65252s/100 iter), loss = 0.000817344
I0814 19:24:47.958336 24732 solver.cpp:334]     Train net output #0: loss = 0.000817158 (* 1 = 0.000817158 loss)
I0814 19:24:47.958343 24732 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0814 19:24:49.621125 24732 solver.cpp:312] Iteration 15600 (60.1408 iter/s, 1.66277s/100 iter), loss = 0.00103404
I0814 19:24:49.621152 24732 solver.cpp:334]     Train net output #0: loss = 0.00103385 (* 1 = 0.00103385 loss)
I0814 19:24:49.621158 24732 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0814 19:24:51.248046 24732 solver.cpp:312] Iteration 15700 (61.4677 iter/s, 1.62687s/100 iter), loss = 0.00369776
I0814 19:24:51.248205 24732 solver.cpp:334]     Train net output #0: loss = 0.00369757 (* 1 = 0.00369757 loss)
I0814 19:24:51.248226 24732 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0814 19:24:52.894896 24732 solver.cpp:312] Iteration 15800 (60.7239 iter/s, 1.6468s/100 iter), loss = 0.00080073
I0814 19:24:52.894920 24732 solver.cpp:334]     Train net output #0: loss = 0.000800541 (* 1 = 0.000800541 loss)
I0814 19:24:52.894927 24732 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0814 19:24:54.565218 24732 solver.cpp:312] Iteration 15900 (59.8707 iter/s, 1.67027s/100 iter), loss = 0.00127364
I0814 19:24:54.565243 24732 solver.cpp:334]     Train net output #0: loss = 0.00127344 (* 1 = 0.00127344 loss)
I0814 19:24:54.565248 24732 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0814 19:24:56.224027 24732 solver.cpp:363] Sparsity after update:
I0814 19:24:56.225576 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:24:56.225586 24732 net.cpp:2192] conv1a_param_0(0.105) 
I0814 19:24:56.225594 24732 net.cpp:2192] conv1b_param_0(0.236) 
I0814 19:24:56.225608 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:24:56.225620 24732 net.cpp:2192] res2a_branch2a_param_0(0.24) 
I0814 19:24:56.225627 24732 net.cpp:2192] res2a_branch2b_param_0(0.236) 
I0814 19:24:56.225636 24732 net.cpp:2192] res3a_branch2a_param_0(0.24) 
I0814 19:24:56.225643 24732 net.cpp:2192] res3a_branch2b_param_0(0.24) 
I0814 19:24:56.225652 24732 net.cpp:2192] res4a_branch2a_param_0(0.24) 
I0814 19:24:56.225662 24732 net.cpp:2192] res4a_branch2b_param_0(0.24) 
I0814 19:24:56.225672 24732 net.cpp:2192] res5a_branch2a_param_0(0.235) 
I0814 19:24:56.225680 24732 net.cpp:2192] res5a_branch2b_param_0(0.239) 
I0814 19:24:56.225690 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (558485/2.3599e+06) 0.237
I0814 19:24:56.225728 24732 solver.cpp:509] Iteration 16000, Testing net (#0)
I0814 19:24:57.045595 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.90206
I0814 19:24:57.045614 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:24:57.045619 24732 solver.cpp:594]     Test net output #2: loss = 0.417604 (* 1 = 0.417604 loss)
I0814 19:24:57.045637 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819883s
I0814 19:24:57.061151 24783 solver.cpp:409] Finding and applying sparsity: 0.26
I0814 19:25:16.118401 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:25:16.120580 24732 solver.cpp:312] Iteration 16000 (4.63935 iter/s, 21.5548s/100 iter), loss = 0.00459923
I0814 19:25:16.120607 24732 solver.cpp:334]     Train net output #0: loss = 0.00459903 (* 1 = 0.00459903 loss)
I0814 19:25:16.120615 24732 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0814 19:25:17.963963 24732 solver.cpp:312] Iteration 16100 (54.2497 iter/s, 1.84333s/100 iter), loss = 0.00911604
I0814 19:25:17.964012 24732 solver.cpp:334]     Train net output #0: loss = 0.00911585 (* 1 = 0.00911585 loss)
I0814 19:25:17.964026 24732 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0814 19:25:19.584833 24732 solver.cpp:312] Iteration 16200 (61.6972 iter/s, 1.62082s/100 iter), loss = 0.000581402
I0814 19:25:19.584857 24732 solver.cpp:334]     Train net output #0: loss = 0.000581206 (* 1 = 0.000581206 loss)
I0814 19:25:19.584863 24732 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0814 19:25:21.231947 24732 solver.cpp:312] Iteration 16300 (60.7141 iter/s, 1.64706s/100 iter), loss = 0.00098809
I0814 19:25:21.232015 24732 solver.cpp:334]     Train net output #0: loss = 0.000987897 (* 1 = 0.000987897 loss)
I0814 19:25:21.232038 24732 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0814 19:25:22.837409 24732 solver.cpp:312] Iteration 16400 (62.2894 iter/s, 1.60541s/100 iter), loss = 0.00559085
I0814 19:25:22.837436 24732 solver.cpp:334]     Train net output #0: loss = 0.00559066 (* 1 = 0.00559066 loss)
I0814 19:25:22.837671 24732 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0814 19:25:24.469357 24732 solver.cpp:312] Iteration 16500 (61.2783 iter/s, 1.6319s/100 iter), loss = 0.00203228
I0814 19:25:24.469424 24732 solver.cpp:334]     Train net output #0: loss = 0.00203209 (* 1 = 0.00203209 loss)
I0814 19:25:24.469442 24732 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0814 19:25:26.101030 24732 solver.cpp:312] Iteration 16600 (61.2887 iter/s, 1.63162s/100 iter), loss = 0.00397477
I0814 19:25:26.101055 24732 solver.cpp:334]     Train net output #0: loss = 0.00397457 (* 1 = 0.00397457 loss)
I0814 19:25:26.101060 24732 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0814 19:25:27.772845 24732 solver.cpp:312] Iteration 16700 (59.8171 iter/s, 1.67176s/100 iter), loss = 0.00166215
I0814 19:25:27.772915 24732 solver.cpp:334]     Train net output #0: loss = 0.00166196 (* 1 = 0.00166196 loss)
I0814 19:25:27.772935 24732 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0814 19:25:29.423104 24732 solver.cpp:312] Iteration 16800 (60.5985 iter/s, 1.65021s/100 iter), loss = 0.00181501
I0814 19:25:29.423164 24732 solver.cpp:334]     Train net output #0: loss = 0.00181482 (* 1 = 0.00181482 loss)
I0814 19:25:29.423183 24732 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0814 19:25:31.098502 24732 solver.cpp:312] Iteration 16900 (59.6891 iter/s, 1.67535s/100 iter), loss = 0.00122949
I0814 19:25:31.098552 24732 solver.cpp:334]     Train net output #0: loss = 0.00122931 (* 1 = 0.00122931 loss)
I0814 19:25:31.098565 24732 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0814 19:25:32.723767 24732 solver.cpp:363] Sparsity after update:
I0814 19:25:32.725456 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:25:32.725466 24732 net.cpp:2192] conv1a_param_0(0.117) 
I0814 19:25:32.725474 24732 net.cpp:2192] conv1b_param_0(0.25) 
I0814 19:25:32.725486 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:25:32.725502 24732 net.cpp:2192] res2a_branch2a_param_0(0.257) 
I0814 19:25:32.725507 24732 net.cpp:2192] res2a_branch2b_param_0(0.257) 
I0814 19:25:32.725510 24732 net.cpp:2192] res3a_branch2a_param_0(0.259) 
I0814 19:25:32.725518 24732 net.cpp:2192] res3a_branch2b_param_0(0.257) 
I0814 19:25:32.725527 24732 net.cpp:2192] res4a_branch2a_param_0(0.26) 
I0814 19:25:32.725531 24732 net.cpp:2192] res4a_branch2b_param_0(0.259) 
I0814 19:25:32.725539 24732 net.cpp:2192] res5a_branch2a_param_0(0.252) 
I0814 19:25:32.725548 24732 net.cpp:2192] res5a_branch2b_param_0(0.259) 
I0814 19:25:32.725553 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (601508/2.3599e+06) 0.255
I0814 19:25:32.725574 24732 solver.cpp:509] Iteration 17000, Testing net (#0)
I0814 19:25:33.550528 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912648
I0814 19:25:33.550546 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:25:33.550554 24732 solver.cpp:594]     Test net output #2: loss = 0.352792 (* 1 = 0.352792 loss)
I0814 19:25:33.550571 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824969s
I0814 19:25:33.567263 24783 solver.cpp:409] Finding and applying sparsity: 0.28
I0814 19:25:51.833163 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:25:51.835222 24732 solver.cpp:312] Iteration 17000 (4.8225 iter/s, 20.7362s/100 iter), loss = 0.0027044
I0814 19:25:51.835240 24732 solver.cpp:334]     Train net output #0: loss = 0.00270421 (* 1 = 0.00270421 loss)
I0814 19:25:51.835247 24732 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0814 19:25:53.685092 24732 solver.cpp:312] Iteration 17100 (54.0595 iter/s, 1.84981s/100 iter), loss = 0.0032926
I0814 19:25:53.685117 24732 solver.cpp:334]     Train net output #0: loss = 0.00329241 (* 1 = 0.00329241 loss)
I0814 19:25:53.685120 24732 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0814 19:25:55.333523 24732 solver.cpp:312] Iteration 17200 (60.6656 iter/s, 1.64838s/100 iter), loss = 0.000255504
I0814 19:25:55.333554 24732 solver.cpp:334]     Train net output #0: loss = 0.000255316 (* 1 = 0.000255316 loss)
I0814 19:25:55.333560 24732 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0814 19:25:56.997534 24732 solver.cpp:312] Iteration 17300 (60.0977 iter/s, 1.66396s/100 iter), loss = 0.00171162
I0814 19:25:56.997562 24732 solver.cpp:334]     Train net output #0: loss = 0.00171144 (* 1 = 0.00171144 loss)
I0814 19:25:56.997570 24732 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0814 19:25:58.661736 24732 solver.cpp:312] Iteration 17400 (60.0907 iter/s, 1.66415s/100 iter), loss = 0.000400008
I0814 19:25:58.661782 24732 solver.cpp:334]     Train net output #0: loss = 0.00039982 (* 1 = 0.00039982 loss)
I0814 19:25:58.661793 24732 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0814 19:26:00.325840 24732 solver.cpp:312] Iteration 17500 (60.0942 iter/s, 1.66405s/100 iter), loss = 0.00337782
I0814 19:26:00.325891 24732 solver.cpp:334]     Train net output #0: loss = 0.00337763 (* 1 = 0.00337763 loss)
I0814 19:26:00.325904 24732 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0814 19:26:01.948827 24732 solver.cpp:312] Iteration 17600 (61.6167 iter/s, 1.62294s/100 iter), loss = 0.00152812
I0814 19:26:01.948876 24732 solver.cpp:334]     Train net output #0: loss = 0.00152793 (* 1 = 0.00152793 loss)
I0814 19:26:01.948891 24732 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0814 19:26:03.636660 24732 solver.cpp:312] Iteration 17700 (59.2495 iter/s, 1.68778s/100 iter), loss = 0.000561906
I0814 19:26:03.636689 24732 solver.cpp:334]     Train net output #0: loss = 0.000561716 (* 1 = 0.000561716 loss)
I0814 19:26:03.636695 24732 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0814 19:26:05.306705 24732 solver.cpp:312] Iteration 17800 (59.8805 iter/s, 1.66999s/100 iter), loss = 0.0235166
I0814 19:26:05.306771 24732 solver.cpp:334]     Train net output #0: loss = 0.0235164 (* 1 = 0.0235164 loss)
I0814 19:26:05.306798 24732 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0814 19:26:06.983981 24732 solver.cpp:312] Iteration 17900 (59.6222 iter/s, 1.67723s/100 iter), loss = 0.0037681
I0814 19:26:06.984045 24732 solver.cpp:334]     Train net output #0: loss = 0.0037679 (* 1 = 0.0037679 loss)
I0814 19:26:06.984063 24732 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0814 19:26:08.605217 24732 solver.cpp:363] Sparsity after update:
I0814 19:26:08.606905 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:26:08.606915 24732 net.cpp:2192] conv1a_param_0(0.131) 
I0814 19:26:08.606920 24732 net.cpp:2192] conv1b_param_0(0.278) 
I0814 19:26:08.606922 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:26:08.606925 24732 net.cpp:2192] res2a_branch2a_param_0(0.278) 
I0814 19:26:08.606926 24732 net.cpp:2192] res2a_branch2b_param_0(0.278) 
I0814 19:26:08.606928 24732 net.cpp:2192] res3a_branch2a_param_0(0.28) 
I0814 19:26:08.606930 24732 net.cpp:2192] res3a_branch2b_param_0(0.278) 
I0814 19:26:08.606935 24732 net.cpp:2192] res4a_branch2a_param_0(0.28) 
I0814 19:26:08.606937 24732 net.cpp:2192] res4a_branch2b_param_0(0.28) 
I0814 19:26:08.606940 24732 net.cpp:2192] res5a_branch2a_param_0(0.272) 
I0814 19:26:08.606943 24732 net.cpp:2192] res5a_branch2b_param_0(0.278) 
I0814 19:26:08.606948 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (648716/2.3599e+06) 0.275
I0814 19:26:08.606971 24732 solver.cpp:509] Iteration 18000, Testing net (#0)
I0814 19:26:09.089166 24730 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:26:09.431809 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910001
I0814 19:26:09.431828 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:26:09.431833 24732 solver.cpp:594]     Test net output #2: loss = 0.362977 (* 1 = 0.362977 loss)
I0814 19:26:09.431846 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824849s
I0814 19:26:09.447471 24783 solver.cpp:409] Finding and applying sparsity: 0.3
I0814 19:26:27.494105 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:26:27.496201 24732 solver.cpp:312] Iteration 18000 (4.87528 iter/s, 20.5117s/100 iter), loss = 0.00116718
I0814 19:26:27.496222 24732 solver.cpp:334]     Train net output #0: loss = 0.00116699 (* 1 = 0.00116699 loss)
I0814 19:26:27.496232 24732 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0814 19:26:29.327950 24732 solver.cpp:312] Iteration 18100 (54.5944 iter/s, 1.83169s/100 iter), loss = 0.000726388
I0814 19:26:29.327975 24732 solver.cpp:334]     Train net output #0: loss = 0.000726198 (* 1 = 0.000726198 loss)
I0814 19:26:29.327980 24732 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0814 19:26:30.962028 24732 solver.cpp:312] Iteration 18200 (61.1985 iter/s, 1.63403s/100 iter), loss = 0.00112927
I0814 19:26:30.962054 24732 solver.cpp:334]     Train net output #0: loss = 0.00112908 (* 1 = 0.00112908 loss)
I0814 19:26:30.962059 24732 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0814 19:26:32.625844 24732 solver.cpp:312] Iteration 18300 (60.1047 iter/s, 1.66376s/100 iter), loss = 0.00370702
I0814 19:26:32.625869 24732 solver.cpp:334]     Train net output #0: loss = 0.00370683 (* 1 = 0.00370683 loss)
I0814 19:26:32.625874 24732 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0814 19:26:34.253134 24732 solver.cpp:312] Iteration 18400 (61.4538 iter/s, 1.62724s/100 iter), loss = 0.00169106
I0814 19:26:34.253199 24732 solver.cpp:334]     Train net output #0: loss = 0.00169087 (* 1 = 0.00169087 loss)
I0814 19:26:34.253219 24732 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0814 19:26:35.903472 24732 solver.cpp:312] Iteration 18500 (60.5956 iter/s, 1.65029s/100 iter), loss = 0.000595505
I0814 19:26:35.903519 24732 solver.cpp:334]     Train net output #0: loss = 0.000595316 (* 1 = 0.000595316 loss)
I0814 19:26:35.903532 24732 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0814 19:26:37.570554 24732 solver.cpp:312] Iteration 18600 (59.9869 iter/s, 1.66703s/100 iter), loss = 0.00122615
I0814 19:26:37.570581 24732 solver.cpp:334]     Train net output #0: loss = 0.00122597 (* 1 = 0.00122597 loss)
I0814 19:26:37.570587 24732 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0814 19:26:39.251938 24732 solver.cpp:312] Iteration 18700 (59.4766 iter/s, 1.68133s/100 iter), loss = 0.00290339
I0814 19:26:39.251960 24732 solver.cpp:334]     Train net output #0: loss = 0.00290321 (* 1 = 0.00290321 loss)
I0814 19:26:39.251965 24732 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0814 19:26:40.872900 24732 solver.cpp:312] Iteration 18800 (61.6937 iter/s, 1.62091s/100 iter), loss = 0.000255397
I0814 19:26:40.872963 24732 solver.cpp:334]     Train net output #0: loss = 0.000255219 (* 1 = 0.000255219 loss)
I0814 19:26:40.872982 24732 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0814 19:26:42.545444 24732 solver.cpp:312] Iteration 18900 (59.791 iter/s, 1.67249s/100 iter), loss = 0.00111211
I0814 19:26:42.545469 24732 solver.cpp:334]     Train net output #0: loss = 0.00111193 (* 1 = 0.00111193 loss)
I0814 19:26:42.545473 24732 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0814 19:26:44.194943 24732 solver.cpp:363] Sparsity after update:
I0814 19:26:44.196842 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:26:44.196866 24732 net.cpp:2192] conv1a_param_0(0.144) 
I0814 19:26:44.196876 24732 net.cpp:2192] conv1b_param_0(0.292) 
I0814 19:26:44.196878 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:26:44.196882 24732 net.cpp:2192] res2a_branch2a_param_0(0.299) 
I0814 19:26:44.196884 24732 net.cpp:2192] res2a_branch2b_param_0(0.299) 
I0814 19:26:44.196887 24732 net.cpp:2192] res3a_branch2a_param_0(0.299) 
I0814 19:26:44.196893 24732 net.cpp:2192] res3a_branch2b_param_0(0.299) 
I0814 19:26:44.196897 24732 net.cpp:2192] res4a_branch2a_param_0(0.299) 
I0814 19:26:44.196899 24732 net.cpp:2192] res4a_branch2b_param_0(0.299) 
I0814 19:26:44.196902 24732 net.cpp:2192] res5a_branch2a_param_0(0.291) 
I0814 19:26:44.196905 24732 net.cpp:2192] res5a_branch2b_param_0(0.298) 
I0814 19:26:44.196908 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (694260/2.3599e+06) 0.294
I0814 19:26:44.196944 24732 solver.cpp:509] Iteration 19000, Testing net (#0)
I0814 19:26:45.008393 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919707
I0814 19:26:45.008410 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:26:45.008416 24732 solver.cpp:594]     Test net output #2: loss = 0.352241 (* 1 = 0.352241 loss)
I0814 19:26:45.008435 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811463s
I0814 19:26:45.028421 24783 solver.cpp:409] Finding and applying sparsity: 0.32
I0814 19:27:01.880056 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:27:01.882175 24732 solver.cpp:312] Iteration 19000 (5.17165 iter/s, 19.3362s/100 iter), loss = 0.00388482
I0814 19:27:01.882195 24732 solver.cpp:334]     Train net output #0: loss = 0.00388464 (* 1 = 0.00388464 loss)
I0814 19:27:01.882205 24732 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0814 19:27:03.714684 24732 solver.cpp:312] Iteration 19100 (54.5718 iter/s, 1.83245s/100 iter), loss = 0.000352213
I0814 19:27:03.714736 24732 solver.cpp:334]     Train net output #0: loss = 0.000352037 (* 1 = 0.000352037 loss)
I0814 19:27:03.714752 24732 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0814 19:27:05.373013 24732 solver.cpp:312] Iteration 19200 (60.3036 iter/s, 1.65828s/100 iter), loss = 0.000106831
I0814 19:27:05.373162 24732 solver.cpp:334]     Train net output #0: loss = 0.000106656 (* 1 = 0.000106656 loss)
I0814 19:27:05.373186 24732 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0814 19:27:07.044878 24732 solver.cpp:312] Iteration 19300 (59.8153 iter/s, 1.67181s/100 iter), loss = 0.000979713
I0814 19:27:07.044903 24732 solver.cpp:334]     Train net output #0: loss = 0.000979536 (* 1 = 0.000979536 loss)
I0814 19:27:07.044909 24732 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0814 19:27:08.717198 24732 solver.cpp:312] Iteration 19400 (59.7991 iter/s, 1.67227s/100 iter), loss = 0.00395256
I0814 19:27:08.717226 24732 solver.cpp:334]     Train net output #0: loss = 0.00395239 (* 1 = 0.00395239 loss)
I0814 19:27:08.717233 24732 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0814 19:27:10.363628 24732 solver.cpp:312] Iteration 19500 (60.7394 iter/s, 1.64638s/100 iter), loss = 0.000343357
I0814 19:27:10.363653 24732 solver.cpp:334]     Train net output #0: loss = 0.000343181 (* 1 = 0.000343181 loss)
I0814 19:27:10.363659 24732 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0814 19:27:12.005643 24732 solver.cpp:312] Iteration 19600 (60.9027 iter/s, 1.64196s/100 iter), loss = 0.000878822
I0814 19:27:12.005671 24732 solver.cpp:334]     Train net output #0: loss = 0.000878649 (* 1 = 0.000878649 loss)
I0814 19:27:12.005676 24732 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0814 19:27:13.684731 24732 solver.cpp:312] Iteration 19700 (59.558 iter/s, 1.67903s/100 iter), loss = 0.000262267
I0814 19:27:13.684756 24732 solver.cpp:334]     Train net output #0: loss = 0.000262094 (* 1 = 0.000262094 loss)
I0814 19:27:13.684762 24732 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0814 19:27:15.308837 24732 solver.cpp:312] Iteration 19800 (61.5742 iter/s, 1.62406s/100 iter), loss = 0.000242115
I0814 19:27:15.308863 24732 solver.cpp:334]     Train net output #0: loss = 0.000241944 (* 1 = 0.000241944 loss)
I0814 19:27:15.308868 24732 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0814 19:27:16.936738 24732 solver.cpp:312] Iteration 19900 (61.4307 iter/s, 1.62785s/100 iter), loss = 0.00233469
I0814 19:27:16.936763 24732 solver.cpp:334]     Train net output #0: loss = 0.00233452 (* 1 = 0.00233452 loss)
I0814 19:27:16.936769 24732 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0814 19:27:18.577703 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0814 19:27:18.587214 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0814 19:27:18.592003 24732 solver.cpp:363] Sparsity after update:
I0814 19:27:18.594238 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:27:18.594249 24732 net.cpp:2192] conv1a_param_0(0.156) 
I0814 19:27:18.594267 24732 net.cpp:2192] conv1b_param_0(0.319) 
I0814 19:27:18.594277 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:27:18.594288 24732 net.cpp:2192] res2a_branch2a_param_0(0.319) 
I0814 19:27:18.594300 24732 net.cpp:2192] res2a_branch2b_param_0(0.319) 
I0814 19:27:18.594311 24732 net.cpp:2192] res3a_branch2a_param_0(0.319) 
I0814 19:27:18.595923 24732 net.cpp:2192] res3a_branch2b_param_0(0.319) 
I0814 19:27:18.595937 24732 net.cpp:2192] res4a_branch2a_param_0(0.319) 
I0814 19:27:18.595947 24732 net.cpp:2192] res4a_branch2b_param_0(0.319) 
I0814 19:27:18.595958 24732 net.cpp:2192] res5a_branch2a_param_0(0.309) 
I0814 19:27:18.595970 24732 net.cpp:2192] res5a_branch2b_param_0(0.316) 
I0814 19:27:18.595980 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (738132/2.3599e+06) 0.313
I0814 19:27:18.595999 24732 solver.cpp:509] Iteration 20000, Testing net (#0)
I0814 19:27:19.412324 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912354
I0814 19:27:19.412341 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:27:19.412346 24732 solver.cpp:594]     Test net output #2: loss = 0.368176 (* 1 = 0.368176 loss)
I0814 19:27:19.412361 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.816337s
I0814 19:27:19.428688 24783 solver.cpp:409] Finding and applying sparsity: 0.34
I0814 19:27:36.405364 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:27:36.407443 24732 solver.cpp:312] Iteration 20000 (5.13606 iter/s, 19.4702s/100 iter), loss = 0.000164382
I0814 19:27:36.407465 24732 solver.cpp:334]     Train net output #0: loss = 0.000164211 (* 1 = 0.000164211 loss)
I0814 19:27:36.407472 24732 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0814 19:27:38.236939 24732 solver.cpp:312] Iteration 20100 (54.6615 iter/s, 1.82944s/100 iter), loss = 0.00135074
I0814 19:27:38.236966 24732 solver.cpp:334]     Train net output #0: loss = 0.00135057 (* 1 = 0.00135057 loss)
I0814 19:27:38.236971 24732 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0814 19:27:39.888635 24732 solver.cpp:312] Iteration 20200 (60.5458 iter/s, 1.65164s/100 iter), loss = 0.00340738
I0814 19:27:39.888660 24732 solver.cpp:334]     Train net output #0: loss = 0.00340721 (* 1 = 0.00340721 loss)
I0814 19:27:39.888665 24732 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0814 19:27:41.526932 24732 solver.cpp:312] Iteration 20300 (61.0409 iter/s, 1.63825s/100 iter), loss = 0.000823781
I0814 19:27:41.526955 24732 solver.cpp:334]     Train net output #0: loss = 0.000823609 (* 1 = 0.000823609 loss)
I0814 19:27:41.526962 24732 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0814 19:27:43.187641 24732 solver.cpp:312] Iteration 20400 (60.2171 iter/s, 1.66066s/100 iter), loss = 0.000592722
I0814 19:27:43.187708 24732 solver.cpp:334]     Train net output #0: loss = 0.00059255 (* 1 = 0.00059255 loss)
I0814 19:27:43.187729 24732 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0814 19:27:44.843792 24732 solver.cpp:312] Iteration 20500 (60.3829 iter/s, 1.6561s/100 iter), loss = 0.000271238
I0814 19:27:44.843842 24732 solver.cpp:334]     Train net output #0: loss = 0.000271066 (* 1 = 0.000271066 loss)
I0814 19:27:44.843857 24732 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0814 19:27:46.524277 24732 solver.cpp:312] Iteration 20600 (59.5085 iter/s, 1.68043s/100 iter), loss = 0.00159072
I0814 19:27:46.524302 24732 solver.cpp:334]     Train net output #0: loss = 0.00159055 (* 1 = 0.00159055 loss)
I0814 19:27:46.524307 24732 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0814 19:27:48.216410 24732 solver.cpp:312] Iteration 20700 (59.0988 iter/s, 1.69208s/100 iter), loss = 0.00603862
I0814 19:27:48.216461 24732 solver.cpp:334]     Train net output #0: loss = 0.00603845 (* 1 = 0.00603845 loss)
I0814 19:27:48.216473 24732 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0814 19:27:49.831873 24732 solver.cpp:312] Iteration 20800 (61.9038 iter/s, 1.61541s/100 iter), loss = 0.000219169
I0814 19:27:49.831898 24732 solver.cpp:334]     Train net output #0: loss = 0.000218999 (* 1 = 0.000218999 loss)
I0814 19:27:49.831904 24732 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0814 19:27:51.499860 24732 solver.cpp:312] Iteration 20900 (59.9545 iter/s, 1.66793s/100 iter), loss = 0.000560707
I0814 19:27:51.499928 24732 solver.cpp:334]     Train net output #0: loss = 0.000560537 (* 1 = 0.000560537 loss)
I0814 19:27:51.499949 24732 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0814 19:27:53.156365 24732 solver.cpp:363] Sparsity after update:
I0814 19:27:53.157809 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:27:53.157819 24732 net.cpp:2192] conv1a_param_0(0.157) 
I0814 19:27:53.157826 24732 net.cpp:2192] conv1b_param_0(0.333) 
I0814 19:27:53.157837 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:27:53.157848 24732 net.cpp:2192] res2a_branch2a_param_0(0.337) 
I0814 19:27:53.157857 24732 net.cpp:2192] res2a_branch2b_param_0(0.333) 
I0814 19:27:53.157866 24732 net.cpp:2192] res3a_branch2a_param_0(0.339) 
I0814 19:27:53.157873 24732 net.cpp:2192] res3a_branch2b_param_0(0.337) 
I0814 19:27:53.157881 24732 net.cpp:2192] res4a_branch2a_param_0(0.339) 
I0814 19:27:53.157891 24732 net.cpp:2192] res4a_branch2b_param_0(0.339) 
I0814 19:27:53.157899 24732 net.cpp:2192] res5a_branch2a_param_0(0.328) 
I0814 19:27:53.157908 24732 net.cpp:2192] res5a_branch2b_param_0(0.336) 
I0814 19:27:53.157934 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (783046/2.3599e+06) 0.332
I0814 19:27:53.157958 24732 solver.cpp:509] Iteration 21000, Testing net (#0)
I0814 19:27:54.000222 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91353
I0814 19:27:54.000239 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:27:54.000246 24732 solver.cpp:594]     Test net output #2: loss = 0.346492 (* 1 = 0.346492 loss)
I0814 19:27:54.000262 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.842277s
I0814 19:27:54.016428 24783 solver.cpp:409] Finding and applying sparsity: 0.36
I0814 19:28:10.973839 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:28:10.975875 24732 solver.cpp:312] Iteration 21000 (5.13466 iter/s, 19.4755s/100 iter), loss = 0.00115068
I0814 19:28:10.975894 24732 solver.cpp:334]     Train net output #0: loss = 0.00115051 (* 1 = 0.00115051 loss)
I0814 19:28:10.975901 24732 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0814 19:28:12.817991 24732 solver.cpp:312] Iteration 21100 (54.2871 iter/s, 1.84206s/100 iter), loss = 0.000520022
I0814 19:28:12.818040 24732 solver.cpp:334]     Train net output #0: loss = 0.000519855 (* 1 = 0.000519855 loss)
I0814 19:28:12.818053 24732 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0814 19:28:14.491020 24732 solver.cpp:312] Iteration 21200 (59.7737 iter/s, 1.67298s/100 iter), loss = 0.00170081
I0814 19:28:14.491045 24732 solver.cpp:334]     Train net output #0: loss = 0.00170064 (* 1 = 0.00170064 loss)
I0814 19:28:14.491050 24732 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0814 19:28:16.169443 24732 solver.cpp:312] Iteration 21300 (59.5816 iter/s, 1.67837s/100 iter), loss = 0.000727657
I0814 19:28:16.169489 24732 solver.cpp:334]     Train net output #0: loss = 0.00072749 (* 1 = 0.00072749 loss)
I0814 19:28:16.169502 24732 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0814 19:28:17.843453 24732 solver.cpp:312] Iteration 21400 (59.7387 iter/s, 1.67396s/100 iter), loss = 0.000424811
I0814 19:28:17.843533 24732 solver.cpp:334]     Train net output #0: loss = 0.000424643 (* 1 = 0.000424643 loss)
I0814 19:28:17.843539 24732 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0814 19:28:19.513025 24732 solver.cpp:312] Iteration 21500 (59.8974 iter/s, 1.66952s/100 iter), loss = 0.000602788
I0814 19:28:19.513095 24732 solver.cpp:334]     Train net output #0: loss = 0.00060262 (* 1 = 0.00060262 loss)
I0814 19:28:19.513119 24732 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0814 19:28:21.190906 24732 solver.cpp:312] Iteration 21600 (59.6008 iter/s, 1.67783s/100 iter), loss = 0.00025335
I0814 19:28:21.190971 24732 solver.cpp:334]     Train net output #0: loss = 0.000253181 (* 1 = 0.000253181 loss)
I0814 19:28:21.190990 24732 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0814 19:28:22.844830 24732 solver.cpp:312] Iteration 21700 (60.4642 iter/s, 1.65387s/100 iter), loss = 0.00126061
I0814 19:28:22.844856 24732 solver.cpp:334]     Train net output #0: loss = 0.00126044 (* 1 = 0.00126044 loss)
I0814 19:28:22.844861 24732 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0814 19:28:24.467742 24732 solver.cpp:312] Iteration 21800 (61.6195 iter/s, 1.62286s/100 iter), loss = 0.00286068
I0814 19:28:24.467806 24732 solver.cpp:334]     Train net output #0: loss = 0.00286051 (* 1 = 0.00286051 loss)
I0814 19:28:24.467825 24732 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0814 19:28:26.116878 24732 solver.cpp:312] Iteration 21900 (60.6397 iter/s, 1.64908s/100 iter), loss = 0.000200331
I0814 19:28:26.116925 24732 solver.cpp:334]     Train net output #0: loss = 0.000200163 (* 1 = 0.000200163 loss)
I0814 19:28:26.116938 24732 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0814 19:28:27.753262 24732 solver.cpp:363] Sparsity after update:
I0814 19:28:27.754979 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:28:27.754988 24732 net.cpp:2192] conv1a_param_0(0.17) 
I0814 19:28:27.754997 24732 net.cpp:2192] conv1b_param_0(0.347) 
I0814 19:28:27.755013 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:28:27.755018 24732 net.cpp:2192] res2a_branch2a_param_0(0.358) 
I0814 19:28:27.755022 24732 net.cpp:2192] res2a_branch2b_param_0(0.354) 
I0814 19:28:27.755030 24732 net.cpp:2192] res3a_branch2a_param_0(0.359) 
I0814 19:28:27.755034 24732 net.cpp:2192] res3a_branch2b_param_0(0.358) 
I0814 19:28:27.755038 24732 net.cpp:2192] res4a_branch2a_param_0(0.359) 
I0814 19:28:27.755048 24732 net.cpp:2192] res4a_branch2b_param_0(0.359) 
I0814 19:28:27.755051 24732 net.cpp:2192] res5a_branch2a_param_0(0.349) 
I0814 19:28:27.755054 24732 net.cpp:2192] res5a_branch2b_param_0(0.357) 
I0814 19:28:27.755067 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (832165/2.3599e+06) 0.353
I0814 19:28:27.755084 24732 solver.cpp:509] Iteration 22000, Testing net (#0)
I0814 19:28:28.171067 24730 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:28:28.571316 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910589
I0814 19:28:28.571336 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:28:28.571341 24732 solver.cpp:594]     Test net output #2: loss = 0.356396 (* 1 = 0.356396 loss)
I0814 19:28:28.571359 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.816246s
I0814 19:28:28.586876 24783 solver.cpp:409] Finding and applying sparsity: 0.38
I0814 19:28:44.376334 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:28:44.378371 24732 solver.cpp:312] Iteration 22000 (5.47615 iter/s, 18.261s/100 iter), loss = 0.000714687
I0814 19:28:44.378393 24732 solver.cpp:334]     Train net output #0: loss = 0.000714519 (* 1 = 0.000714519 loss)
I0814 19:28:44.378399 24732 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0814 19:28:46.252866 24732 solver.cpp:312] Iteration 22100 (53.3493 iter/s, 1.87444s/100 iter), loss = 0.00162986
I0814 19:28:46.252926 24732 solver.cpp:334]     Train net output #0: loss = 0.00162969 (* 1 = 0.00162969 loss)
I0814 19:28:46.252945 24732 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0814 19:28:47.878057 24732 solver.cpp:312] Iteration 22200 (61.5332 iter/s, 1.62514s/100 iter), loss = 0.000720889
I0814 19:28:47.878129 24732 solver.cpp:334]     Train net output #0: loss = 0.000720721 (* 1 = 0.000720721 loss)
I0814 19:28:47.878151 24732 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0814 19:28:49.577708 24732 solver.cpp:312] Iteration 22300 (58.8374 iter/s, 1.6996s/100 iter), loss = 0.000363916
I0814 19:28:49.577780 24732 solver.cpp:334]     Train net output #0: loss = 0.000363749 (* 1 = 0.000363749 loss)
I0814 19:28:49.577800 24732 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0814 19:28:51.218849 24732 solver.cpp:312] Iteration 22400 (60.9352 iter/s, 1.64109s/100 iter), loss = 0.00120533
I0814 19:28:51.218897 24732 solver.cpp:334]     Train net output #0: loss = 0.00120516 (* 1 = 0.00120516 loss)
I0814 19:28:51.218909 24732 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0814 19:28:52.895344 24732 solver.cpp:312] Iteration 22500 (59.6502 iter/s, 1.67644s/100 iter), loss = 0.000975279
I0814 19:28:52.895368 24732 solver.cpp:334]     Train net output #0: loss = 0.000975111 (* 1 = 0.000975111 loss)
I0814 19:28:52.895373 24732 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0814 19:28:54.539022 24732 solver.cpp:312] Iteration 22600 (60.8411 iter/s, 1.64363s/100 iter), loss = 8.19431e-05
I0814 19:28:54.539047 24732 solver.cpp:334]     Train net output #0: loss = 8.17747e-05 (* 1 = 8.17747e-05 loss)
I0814 19:28:54.539052 24732 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0814 19:28:56.221141 24732 solver.cpp:312] Iteration 22700 (59.4506 iter/s, 1.68207s/100 iter), loss = 0.00176383
I0814 19:28:56.221173 24732 solver.cpp:334]     Train net output #0: loss = 0.00176367 (* 1 = 0.00176367 loss)
I0814 19:28:56.221179 24732 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0814 19:28:57.896613 24732 solver.cpp:312] Iteration 22800 (59.6865 iter/s, 1.67542s/100 iter), loss = 0.00214933
I0814 19:28:57.896643 24732 solver.cpp:334]     Train net output #0: loss = 0.00214916 (* 1 = 0.00214916 loss)
I0814 19:28:57.896649 24732 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0814 19:28:59.572464 24732 solver.cpp:312] Iteration 22900 (59.6731 iter/s, 1.6758s/100 iter), loss = 0.000130738
I0814 19:28:59.572490 24732 solver.cpp:334]     Train net output #0: loss = 0.000130571 (* 1 = 0.000130571 loss)
I0814 19:28:59.572496 24732 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0814 19:29:01.191967 24732 solver.cpp:363] Sparsity after update:
I0814 19:29:01.194038 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:29:01.194073 24732 net.cpp:2192] conv1a_param_0(0.182) 
I0814 19:29:01.194094 24732 net.cpp:2192] conv1b_param_0(0.375) 
I0814 19:29:01.194111 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:29:01.194128 24732 net.cpp:2192] res2a_branch2a_param_0(0.378) 
I0814 19:29:01.194142 24732 net.cpp:2192] res2a_branch2b_param_0(0.375) 
I0814 19:29:01.194157 24732 net.cpp:2192] res3a_branch2a_param_0(0.378) 
I0814 19:29:01.194173 24732 net.cpp:2192] res3a_branch2b_param_0(0.378) 
I0814 19:29:01.194190 24732 net.cpp:2192] res4a_branch2a_param_0(0.379) 
I0814 19:29:01.194203 24732 net.cpp:2192] res4a_branch2b_param_0(0.378) 
I0814 19:29:01.194217 24732 net.cpp:2192] res5a_branch2a_param_0(0.364) 
I0814 19:29:01.194231 24732 net.cpp:2192] res5a_branch2b_param_0(0.373) 
I0814 19:29:01.194258 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (870749/2.3599e+06) 0.369
I0814 19:29:01.194281 24732 solver.cpp:509] Iteration 23000, Testing net (#0)
I0814 19:29:02.012845 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914118
I0814 19:29:02.012862 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:29:02.012867 24732 solver.cpp:594]     Test net output #2: loss = 0.336545 (* 1 = 0.336545 loss)
I0814 19:29:02.012882 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818574s
I0814 19:29:02.028509 24783 solver.cpp:409] Finding and applying sparsity: 0.4
I0814 19:29:17.956312 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:29:17.958441 24732 solver.cpp:312] Iteration 23000 (5.43908 iter/s, 18.3855s/100 iter), loss = 0.000925433
I0814 19:29:17.958461 24732 solver.cpp:334]     Train net output #0: loss = 0.000925266 (* 1 = 0.000925266 loss)
I0814 19:29:17.958465 24732 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0814 19:29:19.830512 24732 solver.cpp:312] Iteration 23100 (53.4185 iter/s, 1.87201s/100 iter), loss = 0.0034249
I0814 19:29:19.830574 24732 solver.cpp:334]     Train net output #0: loss = 0.00342473 (* 1 = 0.00342473 loss)
I0814 19:29:19.830592 24732 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0814 19:29:21.521045 24732 solver.cpp:312] Iteration 23200 (59.1547 iter/s, 1.69048s/100 iter), loss = 0.00118038
I0814 19:29:21.521073 24732 solver.cpp:334]     Train net output #0: loss = 0.00118021 (* 1 = 0.00118021 loss)
I0814 19:29:21.521080 24732 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0814 19:29:23.172874 24732 solver.cpp:312] Iteration 23300 (60.5409 iter/s, 1.65178s/100 iter), loss = 0.00128308
I0814 19:29:23.172924 24732 solver.cpp:334]     Train net output #0: loss = 0.00128291 (* 1 = 0.00128291 loss)
I0814 19:29:23.172937 24732 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0814 19:29:24.797850 24732 solver.cpp:312] Iteration 23400 (61.5413 iter/s, 1.62492s/100 iter), loss = 0.000200392
I0814 19:29:24.797896 24732 solver.cpp:334]     Train net output #0: loss = 0.000200228 (* 1 = 0.000200228 loss)
I0814 19:29:24.797909 24732 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0814 19:29:26.433384 24732 solver.cpp:312] Iteration 23500 (61.144 iter/s, 1.63548s/100 iter), loss = 0.000291699
I0814 19:29:26.433409 24732 solver.cpp:334]     Train net output #0: loss = 0.000291535 (* 1 = 0.000291535 loss)
I0814 19:29:26.433414 24732 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0814 19:29:28.101857 24732 solver.cpp:312] Iteration 23600 (59.9369 iter/s, 1.66842s/100 iter), loss = 0.00163935
I0814 19:29:28.101919 24732 solver.cpp:334]     Train net output #0: loss = 0.00163919 (* 1 = 0.00163919 loss)
I0814 19:29:28.101938 24732 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0814 19:29:29.777272 24732 solver.cpp:312] Iteration 23700 (59.6886 iter/s, 1.67536s/100 iter), loss = 0.000572867
I0814 19:29:29.777297 24732 solver.cpp:334]     Train net output #0: loss = 0.000572702 (* 1 = 0.000572702 loss)
I0814 19:29:29.777302 24732 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0814 19:29:31.466960 24732 solver.cpp:312] Iteration 23800 (59.1843 iter/s, 1.68964s/100 iter), loss = 0.000369331
I0814 19:29:31.466985 24732 solver.cpp:334]     Train net output #0: loss = 0.000369167 (* 1 = 0.000369167 loss)
I0814 19:29:31.466989 24732 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0814 19:29:33.113544 24732 solver.cpp:312] Iteration 23900 (60.7337 iter/s, 1.64653s/100 iter), loss = 0.000809183
I0814 19:29:33.113636 24732 solver.cpp:334]     Train net output #0: loss = 0.000809019 (* 1 = 0.000809019 loss)
I0814 19:29:33.113646 24732 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0814 19:29:34.788504 24732 solver.cpp:363] Sparsity after update:
I0814 19:29:34.790416 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:29:34.790426 24732 net.cpp:2192] conv1a_param_0(0.195) 
I0814 19:29:34.790434 24732 net.cpp:2192] conv1b_param_0(0.389) 
I0814 19:29:34.790439 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:29:34.790444 24732 net.cpp:2192] res2a_branch2a_param_0(0.399) 
I0814 19:29:34.790448 24732 net.cpp:2192] res2a_branch2b_param_0(0.396) 
I0814 19:29:34.790452 24732 net.cpp:2192] res3a_branch2a_param_0(0.399) 
I0814 19:29:34.790455 24732 net.cpp:2192] res3a_branch2b_param_0(0.399) 
I0814 19:29:34.790459 24732 net.cpp:2192] res4a_branch2a_param_0(0.399) 
I0814 19:29:34.790462 24732 net.cpp:2192] res4a_branch2b_param_0(0.399) 
I0814 19:29:34.790467 24732 net.cpp:2192] res5a_branch2a_param_0(0.385) 
I0814 19:29:34.790470 24732 net.cpp:2192] res5a_branch2b_param_0(0.393) 
I0814 19:29:34.790496 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (919675/2.3599e+06) 0.39
I0814 19:29:34.790509 24732 solver.cpp:509] Iteration 24000, Testing net (#0)
I0814 19:29:35.615821 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91353
I0814 19:29:35.615839 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:29:35.615844 24732 solver.cpp:594]     Test net output #2: loss = 0.348943 (* 1 = 0.348943 loss)
I0814 19:29:35.615859 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825322s
I0814 19:29:35.631471 24783 solver.cpp:409] Finding and applying sparsity: 0.42
I0814 19:29:51.840077 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:29:51.842149 24732 solver.cpp:312] Iteration 24000 (5.33957 iter/s, 18.7281s/100 iter), loss = 0.00265554
I0814 19:29:51.842169 24732 solver.cpp:334]     Train net output #0: loss = 0.00265537 (* 1 = 0.00265537 loss)
I0814 19:29:51.842175 24732 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0814 19:29:53.710666 24732 solver.cpp:312] Iteration 24100 (53.5201 iter/s, 1.86846s/100 iter), loss = 0.00319621
I0814 19:29:53.710695 24732 solver.cpp:334]     Train net output #0: loss = 0.00319604 (* 1 = 0.00319604 loss)
I0814 19:29:53.710700 24732 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0814 19:29:55.384690 24732 solver.cpp:312] Iteration 24200 (59.7382 iter/s, 1.67397s/100 iter), loss = 0.000680116
I0814 19:29:55.384737 24732 solver.cpp:334]     Train net output #0: loss = 0.000679951 (* 1 = 0.000679951 loss)
I0814 19:29:55.384748 24732 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0814 19:29:57.017369 24732 solver.cpp:312] Iteration 24300 (61.2509 iter/s, 1.63263s/100 iter), loss = 0.000161865
I0814 19:29:57.017398 24732 solver.cpp:334]     Train net output #0: loss = 0.000161698 (* 1 = 0.000161698 loss)
I0814 19:29:57.017405 24732 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0814 19:29:58.674170 24732 solver.cpp:312] Iteration 24400 (60.3592 iter/s, 1.65675s/100 iter), loss = 0.000320495
I0814 19:29:58.674214 24732 solver.cpp:334]     Train net output #0: loss = 0.000320329 (* 1 = 0.000320329 loss)
I0814 19:29:58.674226 24732 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0814 19:30:00.320890 24732 solver.cpp:312] Iteration 24500 (60.7286 iter/s, 1.64667s/100 iter), loss = 0.000541651
I0814 19:30:00.320916 24732 solver.cpp:334]     Train net output #0: loss = 0.000541484 (* 1 = 0.000541484 loss)
I0814 19:30:00.320922 24732 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0814 19:30:01.961957 24732 solver.cpp:312] Iteration 24600 (60.9379 iter/s, 1.64101s/100 iter), loss = 0.00101073
I0814 19:30:01.961982 24732 solver.cpp:334]     Train net output #0: loss = 0.00101056 (* 1 = 0.00101056 loss)
I0814 19:30:01.961987 24732 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0814 19:30:03.595782 24732 solver.cpp:312] Iteration 24700 (61.208 iter/s, 1.63377s/100 iter), loss = 0.000269648
I0814 19:30:03.595808 24732 solver.cpp:334]     Train net output #0: loss = 0.000269482 (* 1 = 0.000269482 loss)
I0814 19:30:03.595813 24732 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0814 19:30:05.243026 24732 solver.cpp:312] Iteration 24800 (60.7094 iter/s, 1.64719s/100 iter), loss = 0.000247004
I0814 19:30:05.243093 24732 solver.cpp:334]     Train net output #0: loss = 0.000246839 (* 1 = 0.000246839 loss)
I0814 19:30:05.243113 24732 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0814 19:30:06.887406 24732 solver.cpp:312] Iteration 24900 (60.815 iter/s, 1.64433s/100 iter), loss = 0.000273741
I0814 19:30:06.887454 24732 solver.cpp:334]     Train net output #0: loss = 0.000273578 (* 1 = 0.000273578 loss)
I0814 19:30:06.887467 24732 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0814 19:30:08.501466 24732 solver.cpp:363] Sparsity after update:
I0814 19:30:08.503096 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:30:08.503105 24732 net.cpp:2192] conv1a_param_0(0.195) 
I0814 19:30:08.503110 24732 net.cpp:2192] conv1b_param_0(0.417) 
I0814 19:30:08.503113 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:30:08.503116 24732 net.cpp:2192] res2a_branch2a_param_0(0.417) 
I0814 19:30:08.503119 24732 net.cpp:2192] res2a_branch2b_param_0(0.417) 
I0814 19:30:08.503123 24732 net.cpp:2192] res3a_branch2a_param_0(0.418) 
I0814 19:30:08.503126 24732 net.cpp:2192] res3a_branch2b_param_0(0.417) 
I0814 19:30:08.503130 24732 net.cpp:2192] res4a_branch2a_param_0(0.419) 
I0814 19:30:08.503134 24732 net.cpp:2192] res4a_branch2b_param_0(0.418) 
I0814 19:30:08.503139 24732 net.cpp:2192] res5a_branch2a_param_0(0.402) 
I0814 19:30:08.503142 24732 net.cpp:2192] res5a_branch2b_param_0(0.413) 
I0814 19:30:08.503159 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (962243/2.3599e+06) 0.408
I0814 19:30:08.503171 24732 solver.cpp:509] Iteration 25000, Testing net (#0)
I0814 19:30:09.312572 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.913824
I0814 19:30:09.312589 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:30:09.312593 24732 solver.cpp:594]     Test net output #2: loss = 0.347035 (* 1 = 0.347035 loss)
I0814 19:30:09.312608 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.80941s
I0814 19:30:09.328233 24783 solver.cpp:409] Finding and applying sparsity: 0.44
I0814 19:30:25.837566 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:30:25.839690 24732 solver.cpp:312] Iteration 25000 (5.27655 iter/s, 18.9518s/100 iter), loss = 0.000333569
I0814 19:30:25.839709 24732 solver.cpp:334]     Train net output #0: loss = 0.000333405 (* 1 = 0.000333405 loss)
I0814 19:30:25.839715 24732 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0814 19:30:27.737409 24732 solver.cpp:312] Iteration 25100 (52.6965 iter/s, 1.89766s/100 iter), loss = 0.00061552
I0814 19:30:27.737467 24732 solver.cpp:334]     Train net output #0: loss = 0.000615355 (* 1 = 0.000615355 loss)
I0814 19:30:27.737483 24732 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0814 19:30:29.373842 24732 solver.cpp:312] Iteration 25200 (61.1105 iter/s, 1.63638s/100 iter), loss = 0.00137501
I0814 19:30:29.373865 24732 solver.cpp:334]     Train net output #0: loss = 0.00137484 (* 1 = 0.00137484 loss)
I0814 19:30:29.373872 24732 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0814 19:30:31.017971 24732 solver.cpp:312] Iteration 25300 (60.8243 iter/s, 1.64408s/100 iter), loss = 0.000502649
I0814 19:30:31.017992 24732 solver.cpp:334]     Train net output #0: loss = 0.000502485 (* 1 = 0.000502485 loss)
I0814 19:30:31.017997 24732 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0814 19:30:32.684404 24732 solver.cpp:312] Iteration 25400 (60.0102 iter/s, 1.66638s/100 iter), loss = 0.000292053
I0814 19:30:32.684428 24732 solver.cpp:334]     Train net output #0: loss = 0.000291889 (* 1 = 0.000291889 loss)
I0814 19:30:32.684433 24732 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0814 19:30:34.345160 24732 solver.cpp:312] Iteration 25500 (60.2155 iter/s, 1.6607s/100 iter), loss = 0.00109013
I0814 19:30:34.345185 24732 solver.cpp:334]     Train net output #0: loss = 0.00108996 (* 1 = 0.00108996 loss)
I0814 19:30:34.345191 24732 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0814 19:30:36.009021 24732 solver.cpp:312] Iteration 25600 (60.103 iter/s, 1.66381s/100 iter), loss = 0.000689709
I0814 19:30:36.009258 24732 solver.cpp:334]     Train net output #0: loss = 0.000689545 (* 1 = 0.000689545 loss)
I0814 19:30:36.009372 24732 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0814 19:30:37.646986 24732 solver.cpp:312] Iteration 25700 (61.0535 iter/s, 1.63791s/100 iter), loss = 0.000457109
I0814 19:30:37.647011 24732 solver.cpp:334]     Train net output #0: loss = 0.000456944 (* 1 = 0.000456944 loss)
I0814 19:30:37.647017 24732 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0814 19:30:39.271766 24732 solver.cpp:312] Iteration 25800 (61.5487 iter/s, 1.62473s/100 iter), loss = 0.00173828
I0814 19:30:39.271792 24732 solver.cpp:334]     Train net output #0: loss = 0.00173811 (* 1 = 0.00173811 loss)
I0814 19:30:39.271798 24732 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0814 19:30:40.929139 24732 solver.cpp:312] Iteration 25900 (60.3383 iter/s, 1.65732s/100 iter), loss = 0.000481751
I0814 19:30:40.929165 24732 solver.cpp:334]     Train net output #0: loss = 0.000481586 (* 1 = 0.000481586 loss)
I0814 19:30:40.929172 24732 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0814 19:30:42.516952 24732 solver.cpp:363] Sparsity after update:
I0814 19:30:42.518739 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:30:42.518750 24732 net.cpp:2192] conv1a_param_0(0.207) 
I0814 19:30:42.518759 24732 net.cpp:2192] conv1b_param_0(0.431) 
I0814 19:30:42.518762 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:30:42.518766 24732 net.cpp:2192] res2a_branch2a_param_0(0.438) 
I0814 19:30:42.518770 24732 net.cpp:2192] res2a_branch2b_param_0(0.438) 
I0814 19:30:42.518774 24732 net.cpp:2192] res3a_branch2a_param_0(0.439) 
I0814 19:30:42.518779 24732 net.cpp:2192] res3a_branch2b_param_0(0.438) 
I0814 19:30:42.518781 24732 net.cpp:2192] res4a_branch2a_param_0(0.439) 
I0814 19:30:42.518786 24732 net.cpp:2192] res4a_branch2b_param_0(0.439) 
I0814 19:30:42.518790 24732 net.cpp:2192] res5a_branch2a_param_0(0.42) 
I0814 19:30:42.518795 24732 net.cpp:2192] res5a_branch2b_param_0(0.432) 
I0814 19:30:42.518815 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.00632e+06/2.3599e+06) 0.426
I0814 19:30:42.518827 24732 solver.cpp:509] Iteration 26000, Testing net (#0)
I0814 19:30:43.330585 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.90853
I0814 19:30:43.330603 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:30:43.330608 24732 solver.cpp:594]     Test net output #2: loss = 0.357479 (* 1 = 0.357479 loss)
I0814 19:30:43.330622 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811768s
I0814 19:30:43.348109 24783 solver.cpp:409] Finding and applying sparsity: 0.46
I0814 19:31:00.378372 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:31:00.380445 24732 solver.cpp:312] Iteration 26000 (5.14118 iter/s, 19.4508s/100 iter), loss = 0.00160062
I0814 19:31:00.380470 24732 solver.cpp:334]     Train net output #0: loss = 0.00160046 (* 1 = 0.00160046 loss)
I0814 19:31:00.380480 24732 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0814 19:31:02.212952 24732 solver.cpp:312] Iteration 26100 (54.5718 iter/s, 1.83245s/100 iter), loss = 0.000865614
I0814 19:31:02.212975 24732 solver.cpp:334]     Train net output #0: loss = 0.000865449 (* 1 = 0.000865449 loss)
I0814 19:31:02.212980 24732 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0814 19:31:03.833384 24732 solver.cpp:312] Iteration 26200 (61.7139 iter/s, 1.62038s/100 iter), loss = 0.00111741
I0814 19:31:03.833408 24732 solver.cpp:334]     Train net output #0: loss = 0.00111724 (* 1 = 0.00111724 loss)
I0814 19:31:03.833415 24732 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0814 19:31:05.481046 24732 solver.cpp:312] Iteration 26300 (60.6939 iter/s, 1.64761s/100 iter), loss = 0.000301433
I0814 19:31:05.481072 24732 solver.cpp:334]     Train net output #0: loss = 0.000301269 (* 1 = 0.000301269 loss)
I0814 19:31:05.481078 24732 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0814 19:31:07.133733 24732 solver.cpp:312] Iteration 26400 (60.5094 iter/s, 1.65264s/100 iter), loss = 0.000989552
I0814 19:31:07.133792 24732 solver.cpp:334]     Train net output #0: loss = 0.000989388 (* 1 = 0.000989388 loss)
I0814 19:31:07.133810 24732 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0814 19:31:08.750846 24732 solver.cpp:312] Iteration 26500 (61.8405 iter/s, 1.61706s/100 iter), loss = 0.000129571
I0814 19:31:08.750871 24732 solver.cpp:334]     Train net output #0: loss = 0.000129408 (* 1 = 0.000129408 loss)
I0814 19:31:08.750876 24732 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0814 19:31:08.941117 24716 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:31:10.351873 24732 solver.cpp:312] Iteration 26600 (62.4618 iter/s, 1.60098s/100 iter), loss = 0.000611006
I0814 19:31:10.351897 24732 solver.cpp:334]     Train net output #0: loss = 0.000610843 (* 1 = 0.000610843 loss)
I0814 19:31:10.351903 24732 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0814 19:31:12.018054 24732 solver.cpp:312] Iteration 26700 (60.0194 iter/s, 1.66613s/100 iter), loss = 0.000370696
I0814 19:31:12.018105 24732 solver.cpp:334]     Train net output #0: loss = 0.000370532 (* 1 = 0.000370532 loss)
I0814 19:31:12.018117 24732 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0814 19:31:13.640735 24732 solver.cpp:312] Iteration 26800 (61.6284 iter/s, 1.62263s/100 iter), loss = 0.00152918
I0814 19:31:13.640760 24732 solver.cpp:334]     Train net output #0: loss = 0.00152902 (* 1 = 0.00152902 loss)
I0814 19:31:13.640765 24732 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0814 19:31:15.274369 24732 solver.cpp:312] Iteration 26900 (61.2151 iter/s, 1.63358s/100 iter), loss = 0.000436234
I0814 19:31:15.274417 24732 solver.cpp:334]     Train net output #0: loss = 0.00043607 (* 1 = 0.00043607 loss)
I0814 19:31:15.274430 24732 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0814 19:31:16.896174 24732 solver.cpp:363] Sparsity after update:
I0814 19:31:16.897851 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:31:16.897863 24732 net.cpp:2192] conv1a_param_0(0.218) 
I0814 19:31:16.897873 24732 net.cpp:2192] conv1b_param_0(0.458) 
I0814 19:31:16.897877 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:31:16.897881 24732 net.cpp:2192] res2a_branch2a_param_0(0.458) 
I0814 19:31:16.897884 24732 net.cpp:2192] res2a_branch2b_param_0(0.458) 
I0814 19:31:16.897887 24732 net.cpp:2192] res3a_branch2a_param_0(0.458) 
I0814 19:31:16.897891 24732 net.cpp:2192] res3a_branch2b_param_0(0.458) 
I0814 19:31:16.897893 24732 net.cpp:2192] res4a_branch2a_param_0(0.459) 
I0814 19:31:16.897897 24732 net.cpp:2192] res4a_branch2b_param_0(0.458) 
I0814 19:31:16.897900 24732 net.cpp:2192] res5a_branch2a_param_0(0.443) 
I0814 19:31:16.897919 24732 net.cpp:2192] res5a_branch2b_param_0(0.453) 
I0814 19:31:16.897922 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.05779e+06/2.3599e+06) 0.448
I0814 19:31:16.897933 24732 solver.cpp:509] Iteration 27000, Testing net (#0)
I0814 19:31:17.728698 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905883
I0814 19:31:17.728718 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:31:17.728723 24732 solver.cpp:594]     Test net output #2: loss = 0.370931 (* 1 = 0.370931 loss)
I0814 19:31:17.728739 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.830777s
I0814 19:31:17.744547 24783 solver.cpp:409] Finding and applying sparsity: 0.48
I0814 19:31:36.680570 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:31:36.682652 24732 solver.cpp:312] Iteration 27000 (4.67122 iter/s, 21.4077s/100 iter), loss = 0.000309829
I0814 19:31:36.682672 24732 solver.cpp:334]     Train net output #0: loss = 0.000309666 (* 1 = 0.000309666 loss)
I0814 19:31:36.682678 24732 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0814 19:31:38.506826 24732 solver.cpp:312] Iteration 27100 (54.821 iter/s, 1.82412s/100 iter), loss = 0.000403272
I0814 19:31:38.506850 24732 solver.cpp:334]     Train net output #0: loss = 0.000403109 (* 1 = 0.000403109 loss)
I0814 19:31:38.506855 24732 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0814 19:31:40.165752 24732 solver.cpp:312] Iteration 27200 (60.2819 iter/s, 1.65887s/100 iter), loss = 0.0023312
I0814 19:31:40.165782 24732 solver.cpp:334]     Train net output #0: loss = 0.00233103 (* 1 = 0.00233103 loss)
I0814 19:31:40.165789 24732 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0814 19:31:41.839046 24732 solver.cpp:312] Iteration 27300 (59.7642 iter/s, 1.67324s/100 iter), loss = 0.00058463
I0814 19:31:41.839076 24732 solver.cpp:334]     Train net output #0: loss = 0.000584469 (* 1 = 0.000584469 loss)
I0814 19:31:41.839082 24732 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0814 19:31:43.487133 24732 solver.cpp:312] Iteration 27400 (60.6782 iter/s, 1.64804s/100 iter), loss = 0.000583487
I0814 19:31:43.487157 24732 solver.cpp:334]     Train net output #0: loss = 0.000583326 (* 1 = 0.000583326 loss)
I0814 19:31:43.487161 24732 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0814 19:31:45.085449 24732 solver.cpp:312] Iteration 27500 (62.5677 iter/s, 1.59827s/100 iter), loss = 0.000790025
I0814 19:31:45.085695 24732 solver.cpp:334]     Train net output #0: loss = 0.000789866 (* 1 = 0.000789866 loss)
I0814 19:31:45.085710 24732 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0814 19:31:46.701463 24732 solver.cpp:312] Iteration 27600 (61.8827 iter/s, 1.61596s/100 iter), loss = 0.00206349
I0814 19:31:46.701491 24732 solver.cpp:334]     Train net output #0: loss = 0.00206333 (* 1 = 0.00206333 loss)
I0814 19:31:46.701498 24732 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0814 19:31:48.339429 24732 solver.cpp:312] Iteration 27700 (61.0532 iter/s, 1.63791s/100 iter), loss = 0.00050414
I0814 19:31:48.339453 24732 solver.cpp:334]     Train net output #0: loss = 0.00050398 (* 1 = 0.00050398 loss)
I0814 19:31:48.339459 24732 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0814 19:31:49.960377 24732 solver.cpp:312] Iteration 27800 (61.6942 iter/s, 1.6209s/100 iter), loss = 0.000481089
I0814 19:31:49.960403 24732 solver.cpp:334]     Train net output #0: loss = 0.000480929 (* 1 = 0.000480929 loss)
I0814 19:31:49.960408 24732 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0814 19:31:51.621775 24732 solver.cpp:312] Iteration 27900 (60.1921 iter/s, 1.66135s/100 iter), loss = 0.00072739
I0814 19:31:51.621798 24732 solver.cpp:334]     Train net output #0: loss = 0.00072723 (* 1 = 0.00072723 loss)
I0814 19:31:51.621803 24732 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0814 19:31:53.228600 24732 solver.cpp:363] Sparsity after update:
I0814 19:31:53.230195 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:31:53.230203 24732 net.cpp:2192] conv1a_param_0(0.233) 
I0814 19:31:53.230211 24732 net.cpp:2192] conv1b_param_0(0.472) 
I0814 19:31:53.230212 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:31:53.230216 24732 net.cpp:2192] res2a_branch2a_param_0(0.479) 
I0814 19:31:53.230217 24732 net.cpp:2192] res2a_branch2b_param_0(0.479) 
I0814 19:31:53.230219 24732 net.cpp:2192] res3a_branch2a_param_0(0.479) 
I0814 19:31:53.230221 24732 net.cpp:2192] res3a_branch2b_param_0(0.479) 
I0814 19:31:53.230223 24732 net.cpp:2192] res4a_branch2a_param_0(0.479) 
I0814 19:31:53.230224 24732 net.cpp:2192] res4a_branch2b_param_0(0.479) 
I0814 19:31:53.230226 24732 net.cpp:2192] res5a_branch2a_param_0(0.465) 
I0814 19:31:53.230228 24732 net.cpp:2192] res5a_branch2b_param_0(0.477) 
I0814 19:31:53.230252 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.11005e+06/2.3599e+06) 0.47
I0814 19:31:53.230260 24732 solver.cpp:509] Iteration 28000, Testing net (#0)
I0814 19:31:54.050652 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.904707
I0814 19:31:54.050669 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:31:54.050674 24732 solver.cpp:594]     Test net output #2: loss = 0.367862 (* 1 = 0.367862 loss)
I0814 19:31:54.050689 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.820402s
I0814 19:31:54.066413 24783 solver.cpp:409] Finding and applying sparsity: 0.5
I0814 19:32:13.692682 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:32:13.694844 24732 solver.cpp:312] Iteration 28000 (4.53053 iter/s, 22.0725s/100 iter), loss = 0.00112252
I0814 19:32:13.694865 24732 solver.cpp:334]     Train net output #0: loss = 0.00112236 (* 1 = 0.00112236 loss)
I0814 19:32:13.694923 24732 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0814 19:32:15.503646 24732 solver.cpp:312] Iteration 28100 (55.287 iter/s, 1.80874s/100 iter), loss = 0.000911977
I0814 19:32:15.503674 24732 solver.cpp:334]     Train net output #0: loss = 0.000911817 (* 1 = 0.000911817 loss)
I0814 19:32:15.503677 24732 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0814 19:32:17.159174 24732 solver.cpp:312] Iteration 28200 (60.4055 iter/s, 1.65548s/100 iter), loss = 0.000901886
I0814 19:32:17.159198 24732 solver.cpp:334]     Train net output #0: loss = 0.000901727 (* 1 = 0.000901727 loss)
I0814 19:32:17.159204 24732 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0814 19:32:18.843464 24732 solver.cpp:312] Iteration 28300 (59.3741 iter/s, 1.68424s/100 iter), loss = 0.000218925
I0814 19:32:18.843493 24732 solver.cpp:334]     Train net output #0: loss = 0.000218766 (* 1 = 0.000218766 loss)
I0814 19:32:18.843498 24732 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0814 19:32:20.500494 24732 solver.cpp:312] Iteration 28400 (60.3508 iter/s, 1.65698s/100 iter), loss = 0.000774538
I0814 19:32:20.500641 24732 solver.cpp:334]     Train net output #0: loss = 0.000774378 (* 1 = 0.000774378 loss)
I0814 19:32:20.500665 24732 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0814 19:32:22.126641 24732 solver.cpp:312] Iteration 28500 (61.497 iter/s, 1.62609s/100 iter), loss = 0.00396671
I0814 19:32:22.126708 24732 solver.cpp:334]     Train net output #0: loss = 0.00396655 (* 1 = 0.00396655 loss)
I0814 19:32:22.126729 24732 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0814 19:32:23.764106 24732 solver.cpp:312] Iteration 28600 (61.0719 iter/s, 1.63741s/100 iter), loss = 0.000655519
I0814 19:32:23.764158 24732 solver.cpp:334]     Train net output #0: loss = 0.00065536 (* 1 = 0.00065536 loss)
I0814 19:32:23.764170 24732 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0814 19:32:25.413324 24732 solver.cpp:312] Iteration 28700 (60.6366 iter/s, 1.64917s/100 iter), loss = 0.000288579
I0814 19:32:25.413349 24732 solver.cpp:334]     Train net output #0: loss = 0.00028842 (* 1 = 0.00028842 loss)
I0814 19:32:25.413354 24732 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0814 19:32:27.078706 24732 solver.cpp:312] Iteration 28800 (60.0482 iter/s, 1.66533s/100 iter), loss = 0.000924513
I0814 19:32:27.078732 24732 solver.cpp:334]     Train net output #0: loss = 0.000924354 (* 1 = 0.000924354 loss)
I0814 19:32:27.078739 24732 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0814 19:32:28.727010 24732 solver.cpp:312] Iteration 28900 (60.6703 iter/s, 1.64825s/100 iter), loss = 0.00735213
I0814 19:32:28.727058 24732 solver.cpp:334]     Train net output #0: loss = 0.00735198 (* 1 = 0.00735198 loss)
I0814 19:32:28.727071 24732 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0814 19:32:30.325834 24732 solver.cpp:363] Sparsity after update:
I0814 19:32:30.327410 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:32:30.327420 24732 net.cpp:2192] conv1a_param_0(0.233) 
I0814 19:32:30.327426 24732 net.cpp:2192] conv1b_param_0(0.5) 
I0814 19:32:30.327430 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:32:30.327440 24732 net.cpp:2192] res2a_branch2a_param_0(0.5) 
I0814 19:32:30.327450 24732 net.cpp:2192] res2a_branch2b_param_0(0.5) 
I0814 19:32:30.327455 24732 net.cpp:2192] res3a_branch2a_param_0(0.5) 
I0814 19:32:30.327462 24732 net.cpp:2192] res3a_branch2b_param_0(0.5) 
I0814 19:32:30.327466 24732 net.cpp:2192] res4a_branch2a_param_0(0.5) 
I0814 19:32:30.327474 24732 net.cpp:2192] res4a_branch2b_param_0(0.5) 
I0814 19:32:30.327478 24732 net.cpp:2192] res5a_branch2a_param_0(0.484) 
I0814 19:32:30.327486 24732 net.cpp:2192] res5a_branch2b_param_0(0.498) 
I0814 19:32:30.327491 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.1569e+06/2.3599e+06) 0.49
I0814 19:32:30.327512 24732 solver.cpp:509] Iteration 29000, Testing net (#0)
I0814 19:32:31.138837 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912942
I0814 19:32:31.138859 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:32:31.138864 24732 solver.cpp:594]     Test net output #2: loss = 0.354866 (* 1 = 0.354866 loss)
I0814 19:32:31.138883 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811339s
I0814 19:32:31.156527 24783 solver.cpp:409] Finding and applying sparsity: 0.52
I0814 19:32:50.707296 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:32:50.709336 24732 solver.cpp:312] Iteration 29000 (4.54923 iter/s, 21.9817s/100 iter), loss = 0.00149718
I0814 19:32:50.709359 24732 solver.cpp:334]     Train net output #0: loss = 0.00149702 (* 1 = 0.00149702 loss)
I0814 19:32:50.709367 24732 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0814 19:32:52.548439 24732 solver.cpp:312] Iteration 29100 (54.376 iter/s, 1.83905s/100 iter), loss = 0.000561861
I0814 19:32:52.548487 24732 solver.cpp:334]     Train net output #0: loss = 0.000561702 (* 1 = 0.000561702 loss)
I0814 19:32:52.548499 24732 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0814 19:32:54.181155 24732 solver.cpp:312] Iteration 29200 (61.2496 iter/s, 1.63266s/100 iter), loss = 0.00187966
I0814 19:32:54.181202 24732 solver.cpp:334]     Train net output #0: loss = 0.00187951 (* 1 = 0.00187951 loss)
I0814 19:32:54.181216 24732 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0814 19:32:55.814488 24732 solver.cpp:312] Iteration 29300 (61.2264 iter/s, 1.63328s/100 iter), loss = 0.00044311
I0814 19:32:55.814553 24732 solver.cpp:334]     Train net output #0: loss = 0.000442951 (* 1 = 0.000442951 loss)
I0814 19:32:55.814571 24732 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0814 19:32:57.441095 24732 solver.cpp:312] Iteration 29400 (61.4796 iter/s, 1.62655s/100 iter), loss = 0.000546513
I0814 19:32:57.441123 24732 solver.cpp:334]     Train net output #0: loss = 0.000546354 (* 1 = 0.000546354 loss)
I0814 19:32:57.441128 24732 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0814 19:32:59.082372 24732 solver.cpp:312] Iteration 29500 (60.93 iter/s, 1.64123s/100 iter), loss = 0.00222468
I0814 19:32:59.082433 24732 solver.cpp:334]     Train net output #0: loss = 0.00222452 (* 1 = 0.00222452 loss)
I0814 19:32:59.082451 24732 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0814 19:33:00.734133 24732 solver.cpp:312] Iteration 29600 (60.5434 iter/s, 1.65171s/100 iter), loss = 0.000151832
I0814 19:33:00.734179 24732 solver.cpp:334]     Train net output #0: loss = 0.000151673 (* 1 = 0.000151673 loss)
I0814 19:33:00.734190 24732 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0814 19:33:02.346547 24732 solver.cpp:312] Iteration 29700 (62.0206 iter/s, 1.61237s/100 iter), loss = 0.00547447
I0814 19:33:02.346688 24732 solver.cpp:334]     Train net output #0: loss = 0.00547431 (* 1 = 0.00547431 loss)
I0814 19:33:02.346707 24732 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0814 19:33:04.012025 24732 solver.cpp:312] Iteration 29800 (60.0447 iter/s, 1.66543s/100 iter), loss = 0.0014514
I0814 19:33:04.012053 24732 solver.cpp:334]     Train net output #0: loss = 0.00145124 (* 1 = 0.00145124 loss)
I0814 19:33:04.012058 24732 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0814 19:33:05.701334 24732 solver.cpp:312] Iteration 29900 (59.1976 iter/s, 1.68926s/100 iter), loss = 0.000373155
I0814 19:33:05.701357 24732 solver.cpp:334]     Train net output #0: loss = 0.000372997 (* 1 = 0.000372997 loss)
I0814 19:33:05.701362 24732 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0814 19:33:07.339547 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0814 19:33:07.347620 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0814 19:33:07.351246 24732 solver.cpp:363] Sparsity after update:
I0814 19:33:07.352985 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:33:07.352996 24732 net.cpp:2192] conv1a_param_0(0.246) 
I0814 19:33:07.353005 24732 net.cpp:2192] conv1b_param_0(0.514) 
I0814 19:33:07.353018 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:33:07.353029 24732 net.cpp:2192] res2a_branch2a_param_0(0.517) 
I0814 19:33:07.353034 24732 net.cpp:2192] res2a_branch2b_param_0(0.514) 
I0814 19:33:07.353036 24732 net.cpp:2192] res3a_branch2a_param_0(0.519) 
I0814 19:33:07.353052 24732 net.cpp:2192] res3a_branch2b_param_0(0.517) 
I0814 19:33:07.353061 24732 net.cpp:2192] res4a_branch2a_param_0(0.52) 
I0814 19:33:07.353065 24732 net.cpp:2192] res4a_branch2b_param_0(0.519) 
I0814 19:33:07.353068 24732 net.cpp:2192] res5a_branch2a_param_0(0.501) 
I0814 19:33:07.353076 24732 net.cpp:2192] res5a_branch2b_param_0(0.517) 
I0814 19:33:07.353081 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.19849e+06/2.3599e+06) 0.508
I0814 19:33:07.353096 24732 solver.cpp:509] Iteration 30000, Testing net (#0)
I0814 19:33:08.172219 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917354
I0814 19:33:08.172236 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:33:08.172241 24732 solver.cpp:594]     Test net output #2: loss = 0.326121 (* 1 = 0.326121 loss)
I0814 19:33:08.172255 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819133s
I0814 19:33:08.187853 24783 solver.cpp:409] Finding and applying sparsity: 0.54
I0814 19:33:28.976816 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:33:28.978943 24732 solver.cpp:312] Iteration 30000 (4.29609 iter/s, 23.277s/100 iter), loss = 0.00156051
I0814 19:33:28.978962 24732 solver.cpp:334]     Train net output #0: loss = 0.00156035 (* 1 = 0.00156035 loss)
I0814 19:33:28.978971 24732 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0814 19:33:30.748152 24732 solver.cpp:312] Iteration 30100 (56.5242 iter/s, 1.76915s/100 iter), loss = 0.00143317
I0814 19:33:30.748175 24732 solver.cpp:334]     Train net output #0: loss = 0.00143301 (* 1 = 0.00143301 loss)
I0814 19:33:30.748181 24732 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0814 19:33:32.415329 24732 solver.cpp:312] Iteration 30200 (59.9835 iter/s, 1.66713s/100 iter), loss = 0.000640238
I0814 19:33:32.415359 24732 solver.cpp:334]     Train net output #0: loss = 0.00064008 (* 1 = 0.00064008 loss)
I0814 19:33:32.415364 24732 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0814 19:33:34.121999 24732 solver.cpp:312] Iteration 30300 (58.5956 iter/s, 1.70661s/100 iter), loss = 0.000781193
I0814 19:33:34.122025 24732 solver.cpp:334]     Train net output #0: loss = 0.000781035 (* 1 = 0.000781035 loss)
I0814 19:33:34.122030 24732 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0814 19:33:35.799001 24732 solver.cpp:312] Iteration 30400 (59.6321 iter/s, 1.67695s/100 iter), loss = 0.000635232
I0814 19:33:35.799026 24732 solver.cpp:334]     Train net output #0: loss = 0.000635074 (* 1 = 0.000635074 loss)
I0814 19:33:35.799031 24732 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0814 19:33:37.440696 24732 solver.cpp:312] Iteration 30500 (60.9146 iter/s, 1.64164s/100 iter), loss = 0.00117349
I0814 19:33:37.440847 24732 solver.cpp:334]     Train net output #0: loss = 0.00117333 (* 1 = 0.00117333 loss)
I0814 19:33:37.440873 24732 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0814 19:33:39.088780 24732 solver.cpp:312] Iteration 30600 (60.6784 iter/s, 1.64803s/100 iter), loss = 0.000742603
I0814 19:33:39.088804 24732 solver.cpp:334]     Train net output #0: loss = 0.000742445 (* 1 = 0.000742445 loss)
I0814 19:33:39.088809 24732 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0814 19:33:40.750020 24732 solver.cpp:312] Iteration 30700 (60.1978 iter/s, 1.66119s/100 iter), loss = 0.000245756
I0814 19:33:40.750072 24732 solver.cpp:334]     Train net output #0: loss = 0.000245597 (* 1 = 0.000245597 loss)
I0814 19:33:40.750085 24732 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0814 19:33:42.384970 24732 solver.cpp:312] Iteration 30800 (61.166 iter/s, 1.6349s/100 iter), loss = 0.000856125
I0814 19:33:42.385193 24732 solver.cpp:334]     Train net output #0: loss = 0.000855968 (* 1 = 0.000855968 loss)
I0814 19:33:42.385255 24732 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0814 19:33:44.028551 24732 solver.cpp:312] Iteration 30900 (60.8447 iter/s, 1.64353s/100 iter), loss = 0.00266532
I0814 19:33:44.028573 24732 solver.cpp:334]     Train net output #0: loss = 0.00266516 (* 1 = 0.00266516 loss)
I0814 19:33:44.028579 24732 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0814 19:33:45.620663 24732 solver.cpp:363] Sparsity after update:
I0814 19:33:45.622337 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:33:45.622345 24732 net.cpp:2192] conv1a_param_0(0.249) 
I0814 19:33:45.622354 24732 net.cpp:2192] conv1b_param_0(0.528) 
I0814 19:33:45.622359 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:33:45.622362 24732 net.cpp:2192] res2a_branch2a_param_0(0.538) 
I0814 19:33:45.622365 24732 net.cpp:2192] res2a_branch2b_param_0(0.534) 
I0814 19:33:45.622369 24732 net.cpp:2192] res3a_branch2a_param_0(0.54) 
I0814 19:33:45.622371 24732 net.cpp:2192] res3a_branch2b_param_0(0.538) 
I0814 19:33:45.622375 24732 net.cpp:2192] res4a_branch2a_param_0(0.54) 
I0814 19:33:45.622378 24732 net.cpp:2192] res4a_branch2b_param_0(0.54) 
I0814 19:33:45.622381 24732 net.cpp:2192] res5a_branch2a_param_0(0.521) 
I0814 19:33:45.622385 24732 net.cpp:2192] res5a_branch2b_param_0(0.537) 
I0814 19:33:45.622387 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.24669e+06/2.3599e+06) 0.528
I0814 19:33:45.622416 24732 solver.cpp:509] Iteration 31000, Testing net (#0)
I0814 19:33:45.915220 24730 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:33:46.445405 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919119
I0814 19:33:46.445423 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:33:46.445430 24732 solver.cpp:594]     Test net output #2: loss = 0.318284 (* 1 = 0.318284 loss)
I0814 19:33:46.445447 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.823002s
I0814 19:33:46.463084 24783 solver.cpp:409] Finding and applying sparsity: 0.56
I0814 19:34:07.834796 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:34:07.836855 24732 solver.cpp:312] Iteration 31000 (4.20033 iter/s, 23.8077s/100 iter), loss = 0.000863788
I0814 19:34:07.836879 24732 solver.cpp:334]     Train net output #0: loss = 0.000863629 (* 1 = 0.000863629 loss)
I0814 19:34:07.836889 24732 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0814 19:34:09.664777 24732 solver.cpp:312] Iteration 31100 (54.7086 iter/s, 1.82786s/100 iter), loss = 0.00257866
I0814 19:34:09.664839 24732 solver.cpp:334]     Train net output #0: loss = 0.0025785 (* 1 = 0.0025785 loss)
I0814 19:34:09.664860 24732 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0814 19:34:11.274726 24732 solver.cpp:312] Iteration 31200 (62.1158 iter/s, 1.6099s/100 iter), loss = 0.00112861
I0814 19:34:11.274751 24732 solver.cpp:334]     Train net output #0: loss = 0.00112845 (* 1 = 0.00112845 loss)
I0814 19:34:11.274756 24732 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0814 19:34:12.925189 24732 solver.cpp:312] Iteration 31300 (60.5909 iter/s, 1.65041s/100 iter), loss = 0.00224881
I0814 19:34:12.925213 24732 solver.cpp:334]     Train net output #0: loss = 0.00224865 (* 1 = 0.00224865 loss)
I0814 19:34:12.925218 24732 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0814 19:34:14.576901 24732 solver.cpp:312] Iteration 31400 (60.5451 iter/s, 1.65166s/100 iter), loss = 0.00220639
I0814 19:34:14.576966 24732 solver.cpp:334]     Train net output #0: loss = 0.00220623 (* 1 = 0.00220623 loss)
I0814 19:34:14.576982 24732 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0814 19:34:16.256436 24732 solver.cpp:312] Iteration 31500 (59.5421 iter/s, 1.67948s/100 iter), loss = 0.00083995
I0814 19:34:16.256464 24732 solver.cpp:334]     Train net output #0: loss = 0.00083979 (* 1 = 0.00083979 loss)
I0814 19:34:16.256470 24732 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0814 19:34:17.887924 24732 solver.cpp:312] Iteration 31600 (61.2957 iter/s, 1.63144s/100 iter), loss = 0.00205442
I0814 19:34:17.887975 24732 solver.cpp:334]     Train net output #0: loss = 0.00205426 (* 1 = 0.00205426 loss)
I0814 19:34:17.887990 24732 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0814 19:34:19.519552 24732 solver.cpp:312] Iteration 31700 (61.2905 iter/s, 1.63157s/100 iter), loss = 0.000185766
I0814 19:34:19.519578 24732 solver.cpp:334]     Train net output #0: loss = 0.000185607 (* 1 = 0.000185607 loss)
I0814 19:34:19.519584 24732 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0814 19:34:21.195189 24732 solver.cpp:312] Iteration 31800 (59.6806 iter/s, 1.67559s/100 iter), loss = 0.000254502
I0814 19:34:21.195214 24732 solver.cpp:334]     Train net output #0: loss = 0.000254342 (* 1 = 0.000254342 loss)
I0814 19:34:21.195219 24732 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0814 19:34:22.830309 24732 solver.cpp:312] Iteration 31900 (61.1595 iter/s, 1.63507s/100 iter), loss = 0.000240022
I0814 19:34:22.830334 24732 solver.cpp:334]     Train net output #0: loss = 0.000239862 (* 1 = 0.000239862 loss)
I0814 19:34:22.830340 24732 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0814 19:34:24.446499 24732 solver.cpp:363] Sparsity after update:
I0814 19:34:24.448176 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:34:24.448185 24732 net.cpp:2192] conv1a_param_0(0.264) 
I0814 19:34:24.448191 24732 net.cpp:2192] conv1b_param_0(0.554) 
I0814 19:34:24.448194 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:34:24.448196 24732 net.cpp:2192] res2a_branch2a_param_0(0.559) 
I0814 19:34:24.448199 24732 net.cpp:2192] res2a_branch2b_param_0(0.554) 
I0814 19:34:24.448202 24732 net.cpp:2192] res3a_branch2a_param_0(0.559) 
I0814 19:34:24.448205 24732 net.cpp:2192] res3a_branch2b_param_0(0.559) 
I0814 19:34:24.448209 24732 net.cpp:2192] res4a_branch2a_param_0(0.56) 
I0814 19:34:24.448211 24732 net.cpp:2192] res4a_branch2b_param_0(0.559) 
I0814 19:34:24.448215 24732 net.cpp:2192] res5a_branch2a_param_0(0.54) 
I0814 19:34:24.448217 24732 net.cpp:2192] res5a_branch2b_param_0(0.557) 
I0814 19:34:24.448222 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.29177e+06/2.3599e+06) 0.547
I0814 19:34:24.448246 24732 solver.cpp:509] Iteration 32000, Testing net (#0)
I0814 19:34:25.271944 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91206
I0814 19:34:25.271962 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:34:25.271970 24732 solver.cpp:594]     Test net output #2: loss = 0.350543 (* 1 = 0.350543 loss)
I0814 19:34:25.271987 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.823713s
I0814 19:34:25.287766 24783 solver.cpp:409] Finding and applying sparsity: 0.58
I0814 19:34:47.480098 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:34:47.482236 24732 solver.cpp:312] Iteration 32000 (4.05659 iter/s, 24.6512s/100 iter), loss = 0.00222368
I0814 19:34:47.482256 24732 solver.cpp:334]     Train net output #0: loss = 0.00222352 (* 1 = 0.00222352 loss)
I0814 19:34:47.482264 24732 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0814 19:34:49.316520 24732 solver.cpp:312] Iteration 32100 (54.519 iter/s, 1.83422s/100 iter), loss = 0.00046678
I0814 19:34:49.316545 24732 solver.cpp:334]     Train net output #0: loss = 0.000466619 (* 1 = 0.000466619 loss)
I0814 19:34:49.316550 24732 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0814 19:34:50.969218 24732 solver.cpp:312] Iteration 32200 (60.509 iter/s, 1.65265s/100 iter), loss = 0.00314207
I0814 19:34:50.969244 24732 solver.cpp:334]     Train net output #0: loss = 0.00314191 (* 1 = 0.00314191 loss)
I0814 19:34:50.969249 24732 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0814 19:34:52.608822 24732 solver.cpp:312] Iteration 32300 (60.9922 iter/s, 1.63955s/100 iter), loss = 0.00190914
I0814 19:34:52.608883 24732 solver.cpp:334]     Train net output #0: loss = 0.00190898 (* 1 = 0.00190898 loss)
I0814 19:34:52.608898 24732 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0814 19:34:54.272204 24732 solver.cpp:312] Iteration 32400 (60.1204 iter/s, 1.66333s/100 iter), loss = 0.000523257
I0814 19:34:54.272229 24732 solver.cpp:334]     Train net output #0: loss = 0.000523098 (* 1 = 0.000523098 loss)
I0814 19:34:54.272235 24732 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0814 19:34:55.918228 24732 solver.cpp:312] Iteration 32500 (60.7543 iter/s, 1.64597s/100 iter), loss = 0.00119819
I0814 19:34:55.918256 24732 solver.cpp:334]     Train net output #0: loss = 0.00119803 (* 1 = 0.00119803 loss)
I0814 19:34:55.918262 24732 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0814 19:34:57.557601 24732 solver.cpp:312] Iteration 32600 (61.0009 iter/s, 1.63932s/100 iter), loss = 0.00219254
I0814 19:34:57.557627 24732 solver.cpp:334]     Train net output #0: loss = 0.00219238 (* 1 = 0.00219238 loss)
I0814 19:34:57.557632 24732 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0814 19:34:59.177565 24732 solver.cpp:312] Iteration 32700 (61.7317 iter/s, 1.61991s/100 iter), loss = 0.000100954
I0814 19:34:59.177633 24732 solver.cpp:334]     Train net output #0: loss = 0.000100796 (* 1 = 0.000100796 loss)
I0814 19:34:59.177650 24732 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0814 19:35:00.829152 24732 solver.cpp:312] Iteration 32800 (60.5497 iter/s, 1.65154s/100 iter), loss = 0.000854233
I0814 19:35:00.829175 24732 solver.cpp:334]     Train net output #0: loss = 0.000854073 (* 1 = 0.000854073 loss)
I0814 19:35:00.829180 24732 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0814 19:35:02.529527 24732 solver.cpp:312] Iteration 32900 (58.8124 iter/s, 1.70032s/100 iter), loss = 0.000936709
I0814 19:35:02.529587 24732 solver.cpp:334]     Train net output #0: loss = 0.00093655 (* 1 = 0.00093655 loss)
I0814 19:35:02.529603 24732 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0814 19:35:04.150136 24732 solver.cpp:363] Sparsity after update:
I0814 19:35:04.151659 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:35:04.151669 24732 net.cpp:2192] conv1a_param_0(0.263) 
I0814 19:35:04.151677 24732 net.cpp:2192] conv1b_param_0(0.567) 
I0814 19:35:04.151690 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:35:04.151700 24732 net.cpp:2192] res2a_branch2a_param_0(0.58) 
I0814 19:35:04.151708 24732 net.cpp:2192] res2a_branch2b_param_0(0.574) 
I0814 19:35:04.151716 24732 net.cpp:2192] res3a_branch2a_param_0(0.58) 
I0814 19:35:04.151724 24732 net.cpp:2192] res3a_branch2b_param_0(0.58) 
I0814 19:35:04.151733 24732 net.cpp:2192] res4a_branch2a_param_0(0.58) 
I0814 19:35:04.151742 24732 net.cpp:2192] res4a_branch2b_param_0(0.58) 
I0814 19:35:04.151751 24732 net.cpp:2192] res5a_branch2a_param_0(0.56) 
I0814 19:35:04.151760 24732 net.cpp:2192] res5a_branch2b_param_0(0.577) 
I0814 19:35:04.151769 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.33914e+06/2.3599e+06) 0.567
I0814 19:35:04.151805 24732 solver.cpp:509] Iteration 33000, Testing net (#0)
I0814 19:35:04.964606 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909707
I0814 19:35:04.964623 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:35:04.964628 24732 solver.cpp:594]     Test net output #2: loss = 0.345967 (* 1 = 0.345967 loss)
I0814 19:35:04.964645 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.812813s
I0814 19:35:04.980237 24783 solver.cpp:409] Finding and applying sparsity: 0.6
I0814 19:35:28.398360 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:35:28.400420 24732 solver.cpp:312] Iteration 33000 (3.86545 iter/s, 25.8702s/100 iter), loss = 0.00117594
I0814 19:35:28.400439 24732 solver.cpp:334]     Train net output #0: loss = 0.00117578 (* 1 = 0.00117578 loss)
I0814 19:35:28.400445 24732 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0814 19:35:30.310865 24732 solver.cpp:312] Iteration 33100 (52.3455 iter/s, 1.91038s/100 iter), loss = 0.0109256
I0814 19:35:30.310935 24732 solver.cpp:334]     Train net output #0: loss = 0.0109255 (* 1 = 0.0109255 loss)
I0814 19:35:30.310955 24732 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0814 19:35:31.964501 24732 solver.cpp:312] Iteration 33200 (60.4746 iter/s, 1.65359s/100 iter), loss = 0.000171514
I0814 19:35:31.964548 24732 solver.cpp:334]     Train net output #0: loss = 0.000171357 (* 1 = 0.000171357 loss)
I0814 19:35:31.964560 24732 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0814 19:35:33.581265 24732 solver.cpp:312] Iteration 33300 (61.854 iter/s, 1.61671s/100 iter), loss = 0.00197934
I0814 19:35:33.581290 24732 solver.cpp:334]     Train net output #0: loss = 0.00197918 (* 1 = 0.00197918 loss)
I0814 19:35:33.581295 24732 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0814 19:35:35.269011 24732 solver.cpp:312] Iteration 33400 (59.2525 iter/s, 1.68769s/100 iter), loss = 0.000788773
I0814 19:35:35.269038 24732 solver.cpp:334]     Train net output #0: loss = 0.000788615 (* 1 = 0.000788615 loss)
I0814 19:35:35.269045 24732 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0814 19:35:36.948596 24732 solver.cpp:312] Iteration 33500 (59.5404 iter/s, 1.67953s/100 iter), loss = 0.00603715
I0814 19:35:36.948642 24732 solver.cpp:334]     Train net output #0: loss = 0.00603699 (* 1 = 0.00603699 loss)
I0814 19:35:36.948653 24732 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0814 19:35:38.611431 24732 solver.cpp:312] Iteration 33600 (60.1401 iter/s, 1.66278s/100 iter), loss = 0.000277507
I0814 19:35:38.611456 24732 solver.cpp:334]     Train net output #0: loss = 0.000277348 (* 1 = 0.000277348 loss)
I0814 19:35:38.611462 24732 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0814 19:35:40.274881 24732 solver.cpp:312] Iteration 33700 (60.1179 iter/s, 1.6634s/100 iter), loss = 0.000240681
I0814 19:35:40.274905 24732 solver.cpp:334]     Train net output #0: loss = 0.000240522 (* 1 = 0.000240522 loss)
I0814 19:35:40.274911 24732 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0814 19:35:41.931851 24732 solver.cpp:312] Iteration 33800 (60.353 iter/s, 1.65692s/100 iter), loss = 0.00152685
I0814 19:35:41.931872 24732 solver.cpp:334]     Train net output #0: loss = 0.00152669 (* 1 = 0.00152669 loss)
I0814 19:35:41.931876 24732 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0814 19:35:43.592573 24732 solver.cpp:312] Iteration 33900 (60.2167 iter/s, 1.66067s/100 iter), loss = 0.00136296
I0814 19:35:43.592603 24732 solver.cpp:334]     Train net output #0: loss = 0.0013628 (* 1 = 0.0013628 loss)
I0814 19:35:43.592610 24732 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0814 19:35:45.236728 24732 solver.cpp:363] Sparsity after update:
I0814 19:35:45.238250 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:35:45.238260 24732 net.cpp:2192] conv1a_param_0(0.277) 
I0814 19:35:45.238277 24732 net.cpp:2192] conv1b_param_0(0.592) 
I0814 19:35:45.238287 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:35:45.238297 24732 net.cpp:2192] res2a_branch2a_param_0(0.597) 
I0814 19:35:45.238306 24732 net.cpp:2192] res2a_branch2b_param_0(0.594) 
I0814 19:35:45.238313 24732 net.cpp:2192] res3a_branch2a_param_0(0.599) 
I0814 19:35:45.238322 24732 net.cpp:2192] res3a_branch2b_param_0(0.597) 
I0814 19:35:45.238330 24732 net.cpp:2192] res4a_branch2a_param_0(0.6) 
I0814 19:35:45.238339 24732 net.cpp:2192] res4a_branch2b_param_0(0.599) 
I0814 19:35:45.238348 24732 net.cpp:2192] res5a_branch2a_param_0(0.576) 
I0814 19:35:45.238358 24732 net.cpp:2192] res5a_branch2b_param_0(0.598) 
I0814 19:35:45.238368 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.38182e+06/2.3599e+06) 0.586
I0814 19:35:45.238400 24732 solver.cpp:509] Iteration 34000, Testing net (#0)
I0814 19:35:46.046381 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.904119
I0814 19:35:46.046397 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:35:46.046402 24732 solver.cpp:594]     Test net output #2: loss = 0.364571 (* 1 = 0.364571 loss)
I0814 19:35:46.046421 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.807992s
I0814 19:35:46.062113 24783 solver.cpp:409] Finding and applying sparsity: 0.62
I0814 19:36:10.650403 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:36:10.652444 24732 solver.cpp:312] Iteration 34000 (3.69561 iter/s, 27.0591s/100 iter), loss = 0.000525296
I0814 19:36:10.652468 24732 solver.cpp:334]     Train net output #0: loss = 0.000525138 (* 1 = 0.000525138 loss)
I0814 19:36:10.652477 24732 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0814 19:36:12.516901 24732 solver.cpp:312] Iteration 34100 (53.6366 iter/s, 1.8644s/100 iter), loss = 0.00103679
I0814 19:36:12.516927 24732 solver.cpp:334]     Train net output #0: loss = 0.00103663 (* 1 = 0.00103663 loss)
I0814 19:36:12.516930 24732 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0814 19:36:14.194895 24732 solver.cpp:312] Iteration 34200 (59.5969 iter/s, 1.67794s/100 iter), loss = 0.00256843
I0814 19:36:14.195042 24732 solver.cpp:334]     Train net output #0: loss = 0.00256827 (* 1 = 0.00256827 loss)
I0814 19:36:14.195063 24732 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0814 19:36:15.860190 24732 solver.cpp:312] Iteration 34300 (60.0513 iter/s, 1.66524s/100 iter), loss = 0.000546362
I0814 19:36:15.860245 24732 solver.cpp:334]     Train net output #0: loss = 0.000546202 (* 1 = 0.000546202 loss)
I0814 19:36:15.860262 24732 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0814 19:36:17.544739 24732 solver.cpp:312] Iteration 34400 (59.365 iter/s, 1.68449s/100 iter), loss = 0.000945464
I0814 19:36:17.544764 24732 solver.cpp:334]     Train net output #0: loss = 0.000945305 (* 1 = 0.000945305 loss)
I0814 19:36:17.544770 24732 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0814 19:36:19.200068 24732 solver.cpp:312] Iteration 34500 (60.4128 iter/s, 1.65528s/100 iter), loss = 0.000312246
I0814 19:36:19.200090 24732 solver.cpp:334]     Train net output #0: loss = 0.000312088 (* 1 = 0.000312088 loss)
I0814 19:36:19.200095 24732 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0814 19:36:20.797149 24732 solver.cpp:312] Iteration 34600 (62.6162 iter/s, 1.59703s/100 iter), loss = 0.00193179
I0814 19:36:20.797174 24732 solver.cpp:334]     Train net output #0: loss = 0.00193164 (* 1 = 0.00193164 loss)
I0814 19:36:20.797180 24732 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0814 19:36:22.453724 24732 solver.cpp:312] Iteration 34700 (60.3674 iter/s, 1.65652s/100 iter), loss = 0.00554511
I0814 19:36:22.453748 24732 solver.cpp:334]     Train net output #0: loss = 0.00554495 (* 1 = 0.00554495 loss)
I0814 19:36:22.453754 24732 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0814 19:36:24.140456 24732 solver.cpp:312] Iteration 34800 (59.288 iter/s, 1.68668s/100 iter), loss = 0.00187929
I0814 19:36:24.140506 24732 solver.cpp:334]     Train net output #0: loss = 0.00187914 (* 1 = 0.00187914 loss)
I0814 19:36:24.140519 24732 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0814 19:36:25.770324 24732 solver.cpp:312] Iteration 34900 (61.3566 iter/s, 1.62982s/100 iter), loss = 0.000575335
I0814 19:36:25.770494 24732 solver.cpp:334]     Train net output #0: loss = 0.000575177 (* 1 = 0.000575177 loss)
I0814 19:36:25.770584 24732 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0814 19:36:27.370738 24732 solver.cpp:363] Sparsity after update:
I0814 19:36:27.372261 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:36:27.372269 24732 net.cpp:2192] conv1a_param_0(0.289) 
I0814 19:36:27.372277 24732 net.cpp:2192] conv1b_param_0(0.605) 
I0814 19:36:27.372282 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:36:27.372292 24732 net.cpp:2192] res2a_branch2a_param_0(0.618) 
I0814 19:36:27.372301 24732 net.cpp:2192] res2a_branch2b_param_0(0.611) 
I0814 19:36:27.372310 24732 net.cpp:2192] res3a_branch2a_param_0(0.62) 
I0814 19:36:27.372319 24732 net.cpp:2192] res3a_branch2b_param_0(0.618) 
I0814 19:36:27.372328 24732 net.cpp:2192] res4a_branch2a_param_0(0.62) 
I0814 19:36:27.372335 24732 net.cpp:2192] res4a_branch2b_param_0(0.62) 
I0814 19:36:27.372344 24732 net.cpp:2192] res5a_branch2a_param_0(0.597) 
I0814 19:36:27.372354 24732 net.cpp:2192] res5a_branch2b_param_0(0.618) 
I0814 19:36:27.372362 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.43065e+06/2.3599e+06) 0.606
I0814 19:36:27.372397 24732 solver.cpp:509] Iteration 35000, Testing net (#0)
I0814 19:36:27.597641 24730 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:36:28.202903 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:36:28.202921 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:36:28.202929 24732 solver.cpp:594]     Test net output #2: loss = 0.366422 (* 1 = 0.366422 loss)
I0814 19:36:28.202945 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83052s
I0814 19:36:28.219471 24783 solver.cpp:409] Finding and applying sparsity: 0.64
I0814 19:36:54.335464 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:36:54.337527 24732 solver.cpp:312] Iteration 35000 (3.50061 iter/s, 28.5664s/100 iter), loss = 0.0012337
I0814 19:36:54.337551 24732 solver.cpp:334]     Train net output #0: loss = 0.00123354 (* 1 = 0.00123354 loss)
I0814 19:36:54.337560 24732 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0814 19:36:56.216511 24732 solver.cpp:312] Iteration 35100 (53.222 iter/s, 1.87892s/100 iter), loss = 0.00221305
I0814 19:36:56.216540 24732 solver.cpp:334]     Train net output #0: loss = 0.00221289 (* 1 = 0.00221289 loss)
I0814 19:36:56.216547 24732 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0814 19:36:57.861150 24732 solver.cpp:312] Iteration 35200 (60.8055 iter/s, 1.64459s/100 iter), loss = 0.00102358
I0814 19:36:57.861173 24732 solver.cpp:334]     Train net output #0: loss = 0.00102342 (* 1 = 0.00102342 loss)
I0814 19:36:57.861177 24732 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0814 19:36:59.494670 24732 solver.cpp:312] Iteration 35300 (61.2193 iter/s, 1.63347s/100 iter), loss = 0.00127552
I0814 19:36:59.494699 24732 solver.cpp:334]     Train net output #0: loss = 0.00127536 (* 1 = 0.00127536 loss)
I0814 19:36:59.494705 24732 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0814 19:37:01.170147 24732 solver.cpp:312] Iteration 35400 (59.6864 iter/s, 1.67542s/100 iter), loss = 0.000261689
I0814 19:37:01.170171 24732 solver.cpp:334]     Train net output #0: loss = 0.000261529 (* 1 = 0.000261529 loss)
I0814 19:37:01.170176 24732 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0814 19:37:02.793546 24732 solver.cpp:312] Iteration 35500 (61.6011 iter/s, 1.62335s/100 iter), loss = 0.00285857
I0814 19:37:02.793614 24732 solver.cpp:334]     Train net output #0: loss = 0.00285841 (* 1 = 0.00285841 loss)
I0814 19:37:02.793637 24732 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0814 19:37:04.465471 24732 solver.cpp:312] Iteration 35600 (59.8132 iter/s, 1.67187s/100 iter), loss = 0.00262108
I0814 19:37:04.465497 24732 solver.cpp:334]     Train net output #0: loss = 0.00262092 (* 1 = 0.00262092 loss)
I0814 19:37:04.465503 24732 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0814 19:37:06.116808 24732 solver.cpp:312] Iteration 35700 (60.5589 iter/s, 1.65129s/100 iter), loss = 0.00110272
I0814 19:37:06.116832 24732 solver.cpp:334]     Train net output #0: loss = 0.00110256 (* 1 = 0.00110256 loss)
I0814 19:37:06.116837 24732 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0814 19:37:07.748167 24732 solver.cpp:312] Iteration 35800 (61.3006 iter/s, 1.63131s/100 iter), loss = 0.00396452
I0814 19:37:07.748193 24732 solver.cpp:334]     Train net output #0: loss = 0.00396436 (* 1 = 0.00396436 loss)
I0814 19:37:07.748198 24732 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0814 19:37:09.403978 24732 solver.cpp:312] Iteration 35900 (60.3952 iter/s, 1.65576s/100 iter), loss = 0.000975836
I0814 19:37:09.404008 24732 solver.cpp:334]     Train net output #0: loss = 0.000975673 (* 1 = 0.000975673 loss)
I0814 19:37:09.404016 24732 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0814 19:37:11.095005 24732 solver.cpp:363] Sparsity after update:
I0814 19:37:11.096905 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:37:11.096916 24732 net.cpp:2192] conv1a_param_0(0.29) 
I0814 19:37:11.096926 24732 net.cpp:2192] conv1b_param_0(0.629) 
I0814 19:37:11.096931 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:37:11.096936 24732 net.cpp:2192] res2a_branch2a_param_0(0.639) 
I0814 19:37:11.096940 24732 net.cpp:2192] res2a_branch2b_param_0(0.627) 
I0814 19:37:11.096945 24732 net.cpp:2192] res3a_branch2a_param_0(0.639) 
I0814 19:37:11.096948 24732 net.cpp:2192] res3a_branch2b_param_0(0.638) 
I0814 19:37:11.096952 24732 net.cpp:2192] res4a_branch2a_param_0(0.64) 
I0814 19:37:11.096956 24732 net.cpp:2192] res4a_branch2b_param_0(0.639) 
I0814 19:37:11.096961 24732 net.cpp:2192] res5a_branch2a_param_0(0.611) 
I0814 19:37:11.096964 24732 net.cpp:2192] res5a_branch2b_param_0(0.638) 
I0814 19:37:11.096968 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.46989e+06/2.3599e+06) 0.623
I0814 19:37:11.096997 24732 solver.cpp:509] Iteration 36000, Testing net (#0)
I0814 19:37:11.915361 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909707
I0814 19:37:11.915385 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:37:11.915391 24732 solver.cpp:594]     Test net output #2: loss = 0.359379 (* 1 = 0.359379 loss)
I0814 19:37:11.915416 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818391s
I0814 19:37:11.934762 24783 solver.cpp:409] Finding and applying sparsity: 0.66
I0814 19:37:39.371173 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:37:39.373263 24732 solver.cpp:312] Iteration 36000 (3.33684 iter/s, 29.9685s/100 iter), loss = 0.00183845
I0814 19:37:39.373287 24732 solver.cpp:334]     Train net output #0: loss = 0.00183828 (* 1 = 0.00183828 loss)
I0814 19:37:39.373296 24732 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0814 19:37:41.242388 24732 solver.cpp:312] Iteration 36100 (53.5026 iter/s, 1.86907s/100 iter), loss = 0.000688174
I0814 19:37:41.242441 24732 solver.cpp:334]     Train net output #0: loss = 0.00068801 (* 1 = 0.00068801 loss)
I0814 19:37:41.242456 24732 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0814 19:37:42.852712 24732 solver.cpp:312] Iteration 36200 (62.1012 iter/s, 1.61028s/100 iter), loss = 0.000879698
I0814 19:37:42.852761 24732 solver.cpp:334]     Train net output #0: loss = 0.000879534 (* 1 = 0.000879534 loss)
I0814 19:37:42.852774 24732 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0814 19:37:44.544373 24732 solver.cpp:312] Iteration 36300 (59.1154 iter/s, 1.69161s/100 iter), loss = 0.000450227
I0814 19:37:44.544422 24732 solver.cpp:334]     Train net output #0: loss = 0.000450061 (* 1 = 0.000450061 loss)
I0814 19:37:44.544436 24732 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0814 19:37:46.228101 24732 solver.cpp:312] Iteration 36400 (59.3939 iter/s, 1.68367s/100 iter), loss = 0.00354766
I0814 19:37:46.228124 24732 solver.cpp:334]     Train net output #0: loss = 0.00354749 (* 1 = 0.00354749 loss)
I0814 19:37:46.228134 24732 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0814 19:37:47.889587 24732 solver.cpp:312] Iteration 36500 (60.1889 iter/s, 1.66143s/100 iter), loss = 0.00163231
I0814 19:37:47.889612 24732 solver.cpp:334]     Train net output #0: loss = 0.00163215 (* 1 = 0.00163215 loss)
I0814 19:37:47.889617 24732 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0814 19:37:49.516026 24732 solver.cpp:312] Iteration 36600 (61.486 iter/s, 1.62639s/100 iter), loss = 0.00295144
I0814 19:37:49.516095 24732 solver.cpp:334]     Train net output #0: loss = 0.00295127 (* 1 = 0.00295127 loss)
I0814 19:37:49.516119 24732 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0814 19:37:51.159335 24732 solver.cpp:312] Iteration 36700 (60.8627 iter/s, 1.64304s/100 iter), loss = 0.00350237
I0814 19:37:51.159359 24732 solver.cpp:334]     Train net output #0: loss = 0.0035022 (* 1 = 0.0035022 loss)
I0814 19:37:51.159363 24732 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0814 19:37:52.784338 24732 solver.cpp:312] Iteration 36800 (61.5403 iter/s, 1.62495s/100 iter), loss = 0.00142127
I0814 19:37:52.784386 24732 solver.cpp:334]     Train net output #0: loss = 0.00142109 (* 1 = 0.00142109 loss)
I0814 19:37:52.784399 24732 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0814 19:37:54.423712 24732 solver.cpp:312] Iteration 36900 (61.0008 iter/s, 1.63932s/100 iter), loss = 0.000529255
I0814 19:37:54.423760 24732 solver.cpp:334]     Train net output #0: loss = 0.000529084 (* 1 = 0.000529084 loss)
I0814 19:37:54.423773 24732 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0814 19:37:56.043133 24732 solver.cpp:363] Sparsity after update:
I0814 19:37:56.045537 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:37:56.045550 24732 net.cpp:2192] conv1a_param_0(0.303) 
I0814 19:37:56.045557 24732 net.cpp:2192] conv1b_param_0(0.639) 
I0814 19:37:56.045563 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:37:56.045567 24732 net.cpp:2192] res2a_branch2a_param_0(0.66) 
I0814 19:37:56.045572 24732 net.cpp:2192] res2a_branch2b_param_0(0.643) 
I0814 19:37:56.045577 24732 net.cpp:2192] res3a_branch2a_param_0(0.66) 
I0814 19:37:56.045580 24732 net.cpp:2192] res3a_branch2b_param_0(0.657) 
I0814 19:37:56.045584 24732 net.cpp:2192] res4a_branch2a_param_0(0.66) 
I0814 19:37:56.045588 24732 net.cpp:2192] res4a_branch2b_param_0(0.66) 
I0814 19:37:56.045593 24732 net.cpp:2192] res5a_branch2a_param_0(0.635) 
I0814 19:37:56.045598 24732 net.cpp:2192] res5a_branch2b_param_0(0.659) 
I0814 19:37:56.045600 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.52254e+06/2.3599e+06) 0.645
I0814 19:37:56.045627 24732 solver.cpp:509] Iteration 37000, Testing net (#0)
I0814 19:37:56.858752 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909413
I0814 19:37:56.858769 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:37:56.858774 24732 solver.cpp:594]     Test net output #2: loss = 0.373866 (* 1 = 0.373866 loss)
I0814 19:37:56.858790 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813135s
I0814 19:37:56.880726 24783 solver.cpp:409] Finding and applying sparsity: 0.68
I0814 19:38:25.348343 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:38:25.350414 24732 solver.cpp:312] Iteration 37000 (3.23354 iter/s, 30.9259s/100 iter), loss = 0.00039917
I0814 19:38:25.350431 24732 solver.cpp:334]     Train net output #0: loss = 0.000398996 (* 1 = 0.000398996 loss)
I0814 19:38:25.350436 24732 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0814 19:38:27.172596 24732 solver.cpp:312] Iteration 37100 (54.8811 iter/s, 1.82212s/100 iter), loss = 0.000275406
I0814 19:38:27.172621 24732 solver.cpp:334]     Train net output #0: loss = 0.000275228 (* 1 = 0.000275228 loss)
I0814 19:38:27.172626 24732 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0814 19:38:28.847478 24732 solver.cpp:312] Iteration 37200 (59.7075 iter/s, 1.67483s/100 iter), loss = 0.000402289
I0814 19:38:28.847509 24732 solver.cpp:334]     Train net output #0: loss = 0.000402112 (* 1 = 0.000402112 loss)
I0814 19:38:28.847515 24732 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0814 19:38:30.463680 24732 solver.cpp:312] Iteration 37300 (61.8755 iter/s, 1.61615s/100 iter), loss = 0.000458645
I0814 19:38:30.463703 24732 solver.cpp:334]     Train net output #0: loss = 0.000458469 (* 1 = 0.000458469 loss)
I0814 19:38:30.463709 24732 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0814 19:38:32.105969 24732 solver.cpp:312] Iteration 37400 (60.8924 iter/s, 1.64224s/100 iter), loss = 0.00144599
I0814 19:38:32.106019 24732 solver.cpp:334]     Train net output #0: loss = 0.00144582 (* 1 = 0.00144582 loss)
I0814 19:38:32.106031 24732 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0814 19:38:33.770913 24732 solver.cpp:312] Iteration 37500 (60.064 iter/s, 1.66489s/100 iter), loss = 0.00184216
I0814 19:38:33.770969 24732 solver.cpp:334]     Train net output #0: loss = 0.00184199 (* 1 = 0.00184199 loss)
I0814 19:38:33.770984 24732 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0814 19:38:35.419451 24732 solver.cpp:312] Iteration 37600 (60.6617 iter/s, 1.64849s/100 iter), loss = 0.00120978
I0814 19:38:35.419477 24732 solver.cpp:334]     Train net output #0: loss = 0.00120961 (* 1 = 0.00120961 loss)
I0814 19:38:35.419482 24732 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0814 19:38:37.041492 24732 solver.cpp:312] Iteration 37700 (61.6527 iter/s, 1.62199s/100 iter), loss = 0.00109412
I0814 19:38:37.041522 24732 solver.cpp:334]     Train net output #0: loss = 0.00109394 (* 1 = 0.00109394 loss)
I0814 19:38:37.041527 24732 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0814 19:38:38.680531 24732 solver.cpp:312] Iteration 37800 (61.0132 iter/s, 1.63899s/100 iter), loss = 0.002531
I0814 19:38:38.680558 24732 solver.cpp:334]     Train net output #0: loss = 0.00253082 (* 1 = 0.00253082 loss)
I0814 19:38:38.680565 24732 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0814 19:38:40.343515 24732 solver.cpp:312] Iteration 37900 (60.1347 iter/s, 1.66293s/100 iter), loss = 0.00168532
I0814 19:38:40.343749 24732 solver.cpp:334]     Train net output #0: loss = 0.00168514 (* 1 = 0.00168514 loss)
I0814 19:38:40.343755 24732 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0814 19:38:41.974885 24732 solver.cpp:363] Sparsity after update:
I0814 19:38:41.976487 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:38:41.976496 24732 net.cpp:2192] conv1a_param_0(0.315) 
I0814 19:38:41.976505 24732 net.cpp:2192] conv1b_param_0(0.65) 
I0814 19:38:41.976508 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:38:41.976511 24732 net.cpp:2192] res2a_branch2a_param_0(0.677) 
I0814 19:38:41.976516 24732 net.cpp:2192] res2a_branch2b_param_0(0.652) 
I0814 19:38:41.976519 24732 net.cpp:2192] res3a_branch2a_param_0(0.679) 
I0814 19:38:41.976523 24732 net.cpp:2192] res3a_branch2b_param_0(0.673) 
I0814 19:38:41.976528 24732 net.cpp:2192] res4a_branch2a_param_0(0.68) 
I0814 19:38:41.976532 24732 net.cpp:2192] res4a_branch2b_param_0(0.679) 
I0814 19:38:41.976536 24732 net.cpp:2192] res5a_branch2a_param_0(0.653) 
I0814 19:38:41.976541 24732 net.cpp:2192] res5a_branch2b_param_0(0.679) 
I0814 19:38:41.976544 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.56648e+06/2.3599e+06) 0.664
I0814 19:38:41.976565 24732 solver.cpp:509] Iteration 38000, Testing net (#0)
I0814 19:38:42.783870 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:38:42.783890 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:38:42.783896 24732 solver.cpp:594]     Test net output #2: loss = 0.392221 (* 1 = 0.392221 loss)
I0814 19:38:42.783912 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.807321s
I0814 19:38:42.799494 24783 solver.cpp:409] Finding and applying sparsity: 0.7
I0814 19:39:13.278408 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:39:13.280521 24732 solver.cpp:312] Iteration 38000 (3.03618 iter/s, 32.9361s/100 iter), loss = 0.00240681
I0814 19:39:13.280541 24732 solver.cpp:334]     Train net output #0: loss = 0.00240663 (* 1 = 0.00240663 loss)
I0814 19:39:13.280549 24732 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0814 19:39:15.175199 24732 solver.cpp:312] Iteration 38100 (52.7811 iter/s, 1.89462s/100 iter), loss = 0.00119895
I0814 19:39:15.175249 24732 solver.cpp:334]     Train net output #0: loss = 0.00119877 (* 1 = 0.00119877 loss)
I0814 19:39:15.175263 24732 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0814 19:39:16.820976 24732 solver.cpp:312] Iteration 38200 (60.7635 iter/s, 1.64573s/100 iter), loss = 0.00291007
I0814 19:39:16.821024 24732 solver.cpp:334]     Train net output #0: loss = 0.00290989 (* 1 = 0.00290989 loss)
I0814 19:39:16.821038 24732 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0814 19:39:18.445725 24732 solver.cpp:312] Iteration 38300 (61.55 iter/s, 1.62469s/100 iter), loss = 0.00115298
I0814 19:39:18.445801 24732 solver.cpp:334]     Train net output #0: loss = 0.00115279 (* 1 = 0.00115279 loss)
I0814 19:39:18.445816 24732 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0814 19:39:20.054780 24732 solver.cpp:312] Iteration 38400 (62.1502 iter/s, 1.609s/100 iter), loss = 0.000374273
I0814 19:39:20.054846 24732 solver.cpp:334]     Train net output #0: loss = 0.000374088 (* 1 = 0.000374088 loss)
I0814 19:39:20.054863 24732 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0814 19:39:21.668599 24732 solver.cpp:312] Iteration 38500 (61.9668 iter/s, 1.61377s/100 iter), loss = 0.00165111
I0814 19:39:21.668659 24732 solver.cpp:334]     Train net output #0: loss = 0.00165093 (* 1 = 0.00165093 loss)
I0814 19:39:21.668678 24732 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0814 19:39:23.308624 24732 solver.cpp:312] Iteration 38600 (60.9766 iter/s, 1.63997s/100 iter), loss = 0.000540637
I0814 19:39:23.308686 24732 solver.cpp:334]     Train net output #0: loss = 0.000540447 (* 1 = 0.000540447 loss)
I0814 19:39:23.308704 24732 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0814 19:39:24.972779 24732 solver.cpp:312] Iteration 38700 (60.0924 iter/s, 1.6641s/100 iter), loss = 0.000831069
I0814 19:39:24.972806 24732 solver.cpp:334]     Train net output #0: loss = 0.000830878 (* 1 = 0.000830878 loss)
I0814 19:39:24.972813 24732 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0814 19:39:26.632979 24732 solver.cpp:312] Iteration 38800 (60.2357 iter/s, 1.66015s/100 iter), loss = 0.000754916
I0814 19:39:26.633050 24732 solver.cpp:334]     Train net output #0: loss = 0.000754726 (* 1 = 0.000754726 loss)
I0814 19:39:26.633069 24732 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0814 19:39:28.262464 24732 solver.cpp:312] Iteration 38900 (61.371 iter/s, 1.62943s/100 iter), loss = 0.00249735
I0814 19:39:28.262488 24732 solver.cpp:334]     Train net output #0: loss = 0.00249716 (* 1 = 0.00249716 loss)
I0814 19:39:28.262495 24732 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0814 19:39:29.875116 24732 solver.cpp:363] Sparsity after update:
I0814 19:39:29.876773 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:39:29.876782 24732 net.cpp:2192] conv1a_param_0(0.333) 
I0814 19:39:29.876791 24732 net.cpp:2192] conv1b_param_0(0.67) 
I0814 19:39:29.876802 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:39:29.876808 24732 net.cpp:2192] res2a_branch2a_param_0(0.698) 
I0814 19:39:29.876817 24732 net.cpp:2192] res2a_branch2b_param_0(0.665) 
I0814 19:39:29.876821 24732 net.cpp:2192] res3a_branch2a_param_0(0.7) 
I0814 19:39:29.876826 24732 net.cpp:2192] res3a_branch2b_param_0(0.69) 
I0814 19:39:29.876833 24732 net.cpp:2192] res4a_branch2a_param_0(0.7) 
I0814 19:39:29.876837 24732 net.cpp:2192] res4a_branch2b_param_0(0.7) 
I0814 19:39:29.876847 24732 net.cpp:2192] res5a_branch2a_param_0(0.672) 
I0814 19:39:29.876850 24732 net.cpp:2192] res5a_branch2b_param_0(0.699) 
I0814 19:39:29.876858 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.6132e+06/2.3599e+06) 0.684
I0814 19:39:29.876883 24732 solver.cpp:509] Iteration 39000, Testing net (#0)
I0814 19:39:30.687156 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.900883
I0814 19:39:30.687175 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:39:30.687182 24732 solver.cpp:594]     Test net output #2: loss = 0.40881 (* 1 = 0.40881 loss)
I0814 19:39:30.687199 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810289s
I0814 19:39:30.702818 24783 solver.cpp:409] Finding and applying sparsity: 0.72
I0814 19:40:03.346062 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:40:03.348179 24732 solver.cpp:312] Iteration 39000 (2.85024 iter/s, 35.0848s/100 iter), loss = 0.000750362
I0814 19:40:03.348203 24732 solver.cpp:334]     Train net output #0: loss = 0.000750167 (* 1 = 0.000750167 loss)
I0814 19:40:03.348213 24732 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0814 19:40:05.158613 24732 solver.cpp:312] Iteration 39100 (55.2371 iter/s, 1.81038s/100 iter), loss = 0.000351459
I0814 19:40:05.158659 24732 solver.cpp:334]     Train net output #0: loss = 0.000351263 (* 1 = 0.000351263 loss)
I0814 19:40:05.158675 24732 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0814 19:40:06.801012 24732 solver.cpp:312] Iteration 39200 (60.8885 iter/s, 1.64235s/100 iter), loss = 0.000522055
I0814 19:40:06.801183 24732 solver.cpp:334]     Train net output #0: loss = 0.000521857 (* 1 = 0.000521857 loss)
I0814 19:40:06.801193 24732 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0814 19:40:08.492977 24732 solver.cpp:312] Iteration 39300 (59.1048 iter/s, 1.69191s/100 iter), loss = 0.00227447
I0814 19:40:08.493032 24732 solver.cpp:334]     Train net output #0: loss = 0.00227427 (* 1 = 0.00227427 loss)
I0814 19:40:08.493044 24732 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0814 19:40:09.985829 24716 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:40:10.156713 24732 solver.cpp:312] Iteration 39400 (60.1077 iter/s, 1.66368s/100 iter), loss = 0.0034588
I0814 19:40:10.156771 24732 solver.cpp:334]     Train net output #0: loss = 0.0034586 (* 1 = 0.0034586 loss)
I0814 19:40:10.156790 24732 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0814 19:40:11.781260 24732 solver.cpp:312] Iteration 39500 (61.5574 iter/s, 1.6245s/100 iter), loss = 0.00317818
I0814 19:40:11.781323 24732 solver.cpp:334]     Train net output #0: loss = 0.00317798 (* 1 = 0.00317798 loss)
I0814 19:40:11.781352 24732 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0814 19:40:13.436818 24732 solver.cpp:312] Iteration 39600 (60.4045 iter/s, 1.65551s/100 iter), loss = 0.00258334
I0814 19:40:13.436862 24732 solver.cpp:334]     Train net output #0: loss = 0.00258315 (* 1 = 0.00258315 loss)
I0814 19:40:13.436875 24732 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0814 19:40:15.091969 24732 solver.cpp:312] Iteration 39700 (60.4194 iter/s, 1.6551s/100 iter), loss = 0.00438747
I0814 19:40:15.092017 24732 solver.cpp:334]     Train net output #0: loss = 0.00438727 (* 1 = 0.00438727 loss)
I0814 19:40:15.092030 24732 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0814 19:40:16.737113 24732 solver.cpp:312] Iteration 39800 (60.7868 iter/s, 1.64509s/100 iter), loss = 0.00173293
I0814 19:40:16.737169 24732 solver.cpp:334]     Train net output #0: loss = 0.00173273 (* 1 = 0.00173273 loss)
I0814 19:40:16.737184 24732 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0814 19:40:18.353957 24732 solver.cpp:312] Iteration 39900 (61.8508 iter/s, 1.61679s/100 iter), loss = 0.000918361
I0814 19:40:18.354087 24732 solver.cpp:334]     Train net output #0: loss = 0.000918167 (* 1 = 0.000918167 loss)
I0814 19:40:18.354104 24732 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0814 19:40:20.021356 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0814 19:40:20.029238 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0814 19:40:20.032728 24732 solver.cpp:363] Sparsity after update:
I0814 19:40:20.034801 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:40:20.034809 24732 net.cpp:2192] conv1a_param_0(0.333) 
I0814 19:40:20.034818 24732 net.cpp:2192] conv1b_param_0(0.679) 
I0814 19:40:20.034823 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:40:20.034827 24732 net.cpp:2192] res2a_branch2a_param_0(0.718) 
I0814 19:40:20.034832 24732 net.cpp:2192] res2a_branch2b_param_0(0.676) 
I0814 19:40:20.034847 24732 net.cpp:2192] res3a_branch2a_param_0(0.719) 
I0814 19:40:20.034852 24732 net.cpp:2192] res3a_branch2b_param_0(0.706) 
I0814 19:40:20.034857 24732 net.cpp:2192] res4a_branch2a_param_0(0.72) 
I0814 19:40:20.034859 24732 net.cpp:2192] res4a_branch2b_param_0(0.719) 
I0814 19:40:20.034863 24732 net.cpp:2192] res5a_branch2a_param_0(0.69) 
I0814 19:40:20.034868 24732 net.cpp:2192] res5a_branch2b_param_0(0.719) 
I0814 19:40:20.034873 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.65736e+06/2.3599e+06) 0.702
I0814 19:40:20.034883 24732 solver.cpp:509] Iteration 40000, Testing net (#0)
I0814 19:40:20.849427 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902354
I0814 19:40:20.849447 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:40:20.849453 24732 solver.cpp:594]     Test net output #2: loss = 0.390551 (* 1 = 0.390551 loss)
I0814 19:40:20.849470 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.81456s
I0814 19:40:20.868772 24783 solver.cpp:409] Finding and applying sparsity: 0.74
I0814 19:40:54.905506 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:40:54.907546 24732 solver.cpp:312] Iteration 40000 (2.73578 iter/s, 36.5526s/100 iter), loss = 0.00185068
I0814 19:40:54.907568 24732 solver.cpp:334]     Train net output #0: loss = 0.00185049 (* 1 = 0.00185049 loss)
I0814 19:40:54.907582 24732 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0814 19:40:56.783093 24732 solver.cpp:312] Iteration 40100 (53.3195 iter/s, 1.87549s/100 iter), loss = 0.00275401
I0814 19:40:56.783118 24732 solver.cpp:334]     Train net output #0: loss = 0.00275382 (* 1 = 0.00275382 loss)
I0814 19:40:56.783123 24732 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0814 19:40:58.423709 24732 solver.cpp:312] Iteration 40200 (60.9546 iter/s, 1.64057s/100 iter), loss = 0.000428766
I0814 19:40:58.423756 24732 solver.cpp:334]     Train net output #0: loss = 0.000428577 (* 1 = 0.000428577 loss)
I0814 19:40:58.423768 24732 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0814 19:41:00.050674 24732 solver.cpp:312] Iteration 40300 (61.466 iter/s, 1.62692s/100 iter), loss = 0.00121535
I0814 19:41:00.050739 24732 solver.cpp:334]     Train net output #0: loss = 0.00121516 (* 1 = 0.00121516 loss)
I0814 19:41:00.050760 24732 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0814 19:41:01.741339 24732 solver.cpp:312] Iteration 40400 (59.1503 iter/s, 1.69061s/100 iter), loss = 0.00862113
I0814 19:41:01.741365 24732 solver.cpp:334]     Train net output #0: loss = 0.00862094 (* 1 = 0.00862094 loss)
I0814 19:41:01.741371 24732 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0814 19:41:03.391124 24732 solver.cpp:312] Iteration 40500 (60.6158 iter/s, 1.64974s/100 iter), loss = 0.00239062
I0814 19:41:03.391189 24732 solver.cpp:334]     Train net output #0: loss = 0.00239044 (* 1 = 0.00239044 loss)
I0814 19:41:03.391208 24732 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0814 19:41:05.072435 24732 solver.cpp:312] Iteration 40600 (59.4792 iter/s, 1.68126s/100 iter), loss = 0.000548397
I0814 19:41:05.072515 24732 solver.cpp:334]     Train net output #0: loss = 0.000548211 (* 1 = 0.000548211 loss)
I0814 19:41:05.072525 24732 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0814 19:41:06.764266 24732 solver.cpp:312] Iteration 40700 (59.1095 iter/s, 1.69177s/100 iter), loss = 0.000512979
I0814 19:41:06.764291 24732 solver.cpp:334]     Train net output #0: loss = 0.000512792 (* 1 = 0.000512792 loss)
I0814 19:41:06.764297 24732 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0814 19:41:08.403115 24732 solver.cpp:312] Iteration 40800 (61.0203 iter/s, 1.6388s/100 iter), loss = 0.000467377
I0814 19:41:08.403139 24732 solver.cpp:334]     Train net output #0: loss = 0.00046719 (* 1 = 0.00046719 loss)
I0814 19:41:08.403143 24732 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0814 19:41:10.004137 24732 solver.cpp:312] Iteration 40900 (62.4622 iter/s, 1.60097s/100 iter), loss = 0.00289349
I0814 19:41:10.004159 24732 solver.cpp:334]     Train net output #0: loss = 0.0028933 (* 1 = 0.0028933 loss)
I0814 19:41:10.004163 24732 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0814 19:41:11.657166 24732 solver.cpp:363] Sparsity after update:
I0814 19:41:11.658931 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:41:11.658941 24732 net.cpp:2192] conv1a_param_0(0.344) 
I0814 19:41:11.658951 24732 net.cpp:2192] conv1b_param_0(0.694) 
I0814 19:41:11.658956 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:41:11.658960 24732 net.cpp:2192] res2a_branch2a_param_0(0.737) 
I0814 19:41:11.658965 24732 net.cpp:2192] res2a_branch2b_param_0(0.684) 
I0814 19:41:11.658970 24732 net.cpp:2192] res3a_branch2a_param_0(0.739) 
I0814 19:41:11.658974 24732 net.cpp:2192] res3a_branch2b_param_0(0.719) 
I0814 19:41:11.658979 24732 net.cpp:2192] res4a_branch2a_param_0(0.74) 
I0814 19:41:11.658983 24732 net.cpp:2192] res4a_branch2b_param_0(0.739) 
I0814 19:41:11.658988 24732 net.cpp:2192] res5a_branch2a_param_0(0.706) 
I0814 19:41:11.658993 24732 net.cpp:2192] res5a_branch2b_param_0(0.739) 
I0814 19:41:11.658998 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.69971e+06/2.3599e+06) 0.72
I0814 19:41:11.659023 24732 solver.cpp:509] Iteration 41000, Testing net (#0)
I0814 19:41:12.497175 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907942
I0814 19:41:12.497195 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:41:12.497200 24732 solver.cpp:594]     Test net output #2: loss = 0.3791 (* 1 = 0.3791 loss)
I0814 19:41:12.497460 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.838168s
I0814 19:41:12.512814 24783 solver.cpp:409] Finding and applying sparsity: 0.76
I0814 19:41:48.404572 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:41:48.406741 24732 solver.cpp:312] Iteration 41000 (2.60406 iter/s, 38.4016s/100 iter), loss = 0.00119698
I0814 19:41:48.406761 24732 solver.cpp:334]     Train net output #0: loss = 0.0011968 (* 1 = 0.0011968 loss)
I0814 19:41:48.406770 24732 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0814 19:41:50.260859 24732 solver.cpp:312] Iteration 41100 (53.9357 iter/s, 1.85406s/100 iter), loss = 0.000816818
I0814 19:41:50.260906 24732 solver.cpp:334]     Train net output #0: loss = 0.00081663 (* 1 = 0.00081663 loss)
I0814 19:41:50.260921 24732 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0814 19:41:51.901219 24732 solver.cpp:312] Iteration 41200 (60.9641 iter/s, 1.64031s/100 iter), loss = 0.00551306
I0814 19:41:51.901266 24732 solver.cpp:334]     Train net output #0: loss = 0.00551287 (* 1 = 0.00551287 loss)
I0814 19:41:51.901279 24732 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0814 19:41:53.569437 24732 solver.cpp:312] Iteration 41300 (59.9461 iter/s, 1.66817s/100 iter), loss = 0.0023845
I0814 19:41:53.569461 24732 solver.cpp:334]     Train net output #0: loss = 0.00238431 (* 1 = 0.00238431 loss)
I0814 19:41:53.569466 24732 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0814 19:41:55.169376 24732 solver.cpp:312] Iteration 41400 (62.5043 iter/s, 1.59989s/100 iter), loss = 0.00184531
I0814 19:41:55.169438 24732 solver.cpp:334]     Train net output #0: loss = 0.00184512 (* 1 = 0.00184512 loss)
I0814 19:41:55.169457 24732 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0814 19:41:56.829900 24732 solver.cpp:312] Iteration 41500 (60.2239 iter/s, 1.66047s/100 iter), loss = 0.0043073
I0814 19:41:56.829926 24732 solver.cpp:334]     Train net output #0: loss = 0.00430711 (* 1 = 0.00430711 loss)
I0814 19:41:56.829931 24732 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0814 19:41:58.494933 24732 solver.cpp:312] Iteration 41600 (60.0607 iter/s, 1.66498s/100 iter), loss = 0.00144118
I0814 19:41:58.494992 24732 solver.cpp:334]     Train net output #0: loss = 0.00144099 (* 1 = 0.00144099 loss)
I0814 19:41:58.495010 24732 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0814 19:42:00.114933 24732 solver.cpp:312] Iteration 41700 (61.7304 iter/s, 1.61995s/100 iter), loss = 0.00301795
I0814 19:42:00.114956 24732 solver.cpp:334]     Train net output #0: loss = 0.00301777 (* 1 = 0.00301777 loss)
I0814 19:42:00.114960 24732 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0814 19:42:01.743558 24732 solver.cpp:312] Iteration 41800 (61.4035 iter/s, 1.62857s/100 iter), loss = 0.00376672
I0814 19:42:01.743584 24732 solver.cpp:334]     Train net output #0: loss = 0.00376654 (* 1 = 0.00376654 loss)
I0814 19:42:01.743590 24732 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0814 19:42:03.461050 24732 solver.cpp:312] Iteration 41900 (58.2262 iter/s, 1.71744s/100 iter), loss = 0.00418962
I0814 19:42:03.461122 24732 solver.cpp:334]     Train net output #0: loss = 0.00418944 (* 1 = 0.00418944 loss)
I0814 19:42:03.461145 24732 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0814 19:42:05.108731 24732 solver.cpp:363] Sparsity after update:
I0814 19:42:05.110352 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:42:05.110361 24732 net.cpp:2192] conv1a_param_0(0.358) 
I0814 19:42:05.110370 24732 net.cpp:2192] conv1b_param_0(0.702) 
I0814 19:42:05.110380 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:42:05.110386 24732 net.cpp:2192] res2a_branch2a_param_0(0.753) 
I0814 19:42:05.110390 24732 net.cpp:2192] res2a_branch2b_param_0(0.691) 
I0814 19:42:05.110399 24732 net.cpp:2192] res3a_branch2a_param_0(0.758) 
I0814 19:42:05.110404 24732 net.cpp:2192] res3a_branch2b_param_0(0.727) 
I0814 19:42:05.110407 24732 net.cpp:2192] res4a_branch2a_param_0(0.76) 
I0814 19:42:05.110415 24732 net.cpp:2192] res4a_branch2b_param_0(0.758) 
I0814 19:42:05.110420 24732 net.cpp:2192] res5a_branch2a_param_0(0.723) 
I0814 19:42:05.110424 24732 net.cpp:2192] res5a_branch2b_param_0(0.759) 
I0814 19:42:05.110427 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.74243e+06/2.3599e+06) 0.738
I0814 19:42:05.112087 24732 solver.cpp:509] Iteration 42000, Testing net (#0)
I0814 19:42:05.932862 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905001
I0814 19:42:05.932883 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:42:05.932888 24732 solver.cpp:594]     Test net output #2: loss = 0.390497 (* 1 = 0.390497 loss)
I0814 19:42:05.932902 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.820788s
I0814 19:42:05.948473 24783 solver.cpp:409] Finding and applying sparsity: 0.78
I0814 19:42:44.288733 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:42:44.290925 24732 solver.cpp:312] Iteration 42000 (2.44926 iter/s, 40.8287s/100 iter), loss = 0.00223816
I0814 19:42:44.290958 24732 solver.cpp:334]     Train net output #0: loss = 0.00223798 (* 1 = 0.00223798 loss)
I0814 19:42:44.290967 24732 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0814 19:42:46.185745 24732 solver.cpp:312] Iteration 42100 (52.777 iter/s, 1.89476s/100 iter), loss = 0.00208022
I0814 19:42:46.185768 24732 solver.cpp:334]     Train net output #0: loss = 0.00208003 (* 1 = 0.00208003 loss)
I0814 19:42:46.185773 24732 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0814 19:42:47.837906 24732 solver.cpp:312] Iteration 42200 (60.5287 iter/s, 1.65211s/100 iter), loss = 0.00034668
I0814 19:42:47.837955 24732 solver.cpp:334]     Train net output #0: loss = 0.000346499 (* 1 = 0.000346499 loss)
I0814 19:42:47.837967 24732 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0814 19:42:49.489023 24732 solver.cpp:312] Iteration 42300 (60.567 iter/s, 1.65106s/100 iter), loss = 0.00295807
I0814 19:42:49.489047 24732 solver.cpp:334]     Train net output #0: loss = 0.00295788 (* 1 = 0.00295788 loss)
I0814 19:42:49.489050 24732 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0814 19:42:51.112115 24732 solver.cpp:312] Iteration 42400 (61.6127 iter/s, 1.62304s/100 iter), loss = 0.00186282
I0814 19:42:51.112166 24732 solver.cpp:334]     Train net output #0: loss = 0.00186263 (* 1 = 0.00186263 loss)
I0814 19:42:51.112179 24732 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0814 19:42:52.764791 24732 solver.cpp:312] Iteration 42500 (60.5098 iter/s, 1.65262s/100 iter), loss = 0.00510715
I0814 19:42:52.764818 24732 solver.cpp:334]     Train net output #0: loss = 0.00510696 (* 1 = 0.00510696 loss)
I0814 19:42:52.764824 24732 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0814 19:42:54.427196 24732 solver.cpp:312] Iteration 42600 (60.1558 iter/s, 1.66235s/100 iter), loss = 0.00719461
I0814 19:42:54.427220 24732 solver.cpp:334]     Train net output #0: loss = 0.00719442 (* 1 = 0.00719442 loss)
I0814 19:42:54.427225 24732 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0814 19:42:56.089000 24732 solver.cpp:312] Iteration 42700 (60.1773 iter/s, 1.66175s/100 iter), loss = 0.00916129
I0814 19:42:56.089054 24732 solver.cpp:334]     Train net output #0: loss = 0.0091611 (* 1 = 0.0091611 loss)
I0814 19:42:56.089066 24732 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0814 19:42:57.744937 24732 solver.cpp:312] Iteration 42800 (60.3908 iter/s, 1.65588s/100 iter), loss = 0.000491053
I0814 19:42:57.745003 24732 solver.cpp:334]     Train net output #0: loss = 0.000490862 (* 1 = 0.000490862 loss)
I0814 19:42:57.745023 24732 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0814 19:42:59.398619 24732 solver.cpp:312] Iteration 42900 (60.4729 iter/s, 1.65363s/100 iter), loss = 0.0127771
I0814 19:42:59.398665 24732 solver.cpp:334]     Train net output #0: loss = 0.0127769 (* 1 = 0.0127769 loss)
I0814 19:42:59.398679 24732 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0814 19:43:01.019109 24732 solver.cpp:363] Sparsity after update:
I0814 19:43:01.020874 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:43:01.020884 24732 net.cpp:2192] conv1a_param_0(0.371) 
I0814 19:43:01.020891 24732 net.cpp:2192] conv1b_param_0(0.716) 
I0814 19:43:01.020895 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:43:01.020898 24732 net.cpp:2192] res2a_branch2a_param_0(0.772) 
I0814 19:43:01.020910 24732 net.cpp:2192] res2a_branch2b_param_0(0.696) 
I0814 19:43:01.020920 24732 net.cpp:2192] res3a_branch2a_param_0(0.776) 
I0814 19:43:01.020928 24732 net.cpp:2192] res3a_branch2b_param_0(0.735) 
I0814 19:43:01.020936 24732 net.cpp:2192] res4a_branch2a_param_0(0.78) 
I0814 19:43:01.020944 24732 net.cpp:2192] res4a_branch2b_param_0(0.778) 
I0814 19:43:01.020952 24732 net.cpp:2192] res5a_branch2a_param_0(0.746) 
I0814 19:43:01.020961 24732 net.cpp:2192] res5a_branch2b_param_0(0.779) 
I0814 19:43:01.020970 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.79219e+06/2.3599e+06) 0.759
I0814 19:43:01.020999 24732 solver.cpp:509] Iteration 43000, Testing net (#0)
I0814 19:43:01.834561 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915589
I0814 19:43:01.834581 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:43:01.834586 24732 solver.cpp:594]     Test net output #2: loss = 0.368681 (* 1 = 0.368681 loss)
I0814 19:43:01.834602 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813576s
I0814 19:43:01.850206 24783 solver.cpp:409] Finding and applying sparsity: 0.8
I0814 19:43:42.684126 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:43:42.686229 24732 solver.cpp:312] Iteration 43000 (2.31019 iter/s, 43.2864s/100 iter), loss = 0.000641299
I0814 19:43:42.686252 24732 solver.cpp:334]     Train net output #0: loss = 0.000641113 (* 1 = 0.000641113 loss)
I0814 19:43:42.686260 24732 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0814 19:43:44.578044 24732 solver.cpp:312] Iteration 43100 (52.861 iter/s, 1.89175s/100 iter), loss = 0.00202656
I0814 19:43:44.578069 24732 solver.cpp:334]     Train net output #0: loss = 0.00202638 (* 1 = 0.00202638 loss)
I0814 19:43:44.578075 24732 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0814 19:43:46.230785 24732 solver.cpp:312] Iteration 43200 (60.5074 iter/s, 1.65269s/100 iter), loss = 0.000817979
I0814 19:43:46.230841 24732 solver.cpp:334]     Train net output #0: loss = 0.000817792 (* 1 = 0.000817792 loss)
I0814 19:43:46.230856 24732 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0814 19:43:47.920801 24732 solver.cpp:312] Iteration 43300 (59.1729 iter/s, 1.68996s/100 iter), loss = 0.014249
I0814 19:43:47.920825 24732 solver.cpp:334]     Train net output #0: loss = 0.0142488 (* 1 = 0.0142488 loss)
I0814 19:43:47.920828 24732 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0814 19:43:49.535634 24732 solver.cpp:312] Iteration 43400 (61.9278 iter/s, 1.61478s/100 iter), loss = 0.00109175
I0814 19:43:49.535679 24732 solver.cpp:334]     Train net output #0: loss = 0.00109156 (* 1 = 0.00109156 loss)
I0814 19:43:49.535691 24732 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0814 19:43:51.194504 24732 solver.cpp:312] Iteration 43500 (60.2839 iter/s, 1.65882s/100 iter), loss = 0.000799006
I0814 19:43:51.194561 24732 solver.cpp:334]     Train net output #0: loss = 0.000798809 (* 1 = 0.000798809 loss)
I0814 19:43:51.194578 24732 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0814 19:43:52.875403 24732 solver.cpp:312] Iteration 43600 (59.4939 iter/s, 1.68085s/100 iter), loss = 0.00396464
I0814 19:43:52.875434 24732 solver.cpp:334]     Train net output #0: loss = 0.00396445 (* 1 = 0.00396445 loss)
I0814 19:43:52.875442 24732 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0814 19:43:54.567615 24732 solver.cpp:312] Iteration 43700 (59.0961 iter/s, 1.69216s/100 iter), loss = 0.0262592
I0814 19:43:54.567639 24732 solver.cpp:334]     Train net output #0: loss = 0.026259 (* 1 = 0.026259 loss)
I0814 19:43:54.567646 24732 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0814 19:43:56.186581 24732 solver.cpp:312] Iteration 43800 (61.7696 iter/s, 1.61892s/100 iter), loss = 0.00174655
I0814 19:43:56.186606 24732 solver.cpp:334]     Train net output #0: loss = 0.00174635 (* 1 = 0.00174635 loss)
I0814 19:43:56.186612 24732 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0814 19:43:57.823966 24732 solver.cpp:312] Iteration 43900 (61.0749 iter/s, 1.63733s/100 iter), loss = 0.00317649
I0814 19:43:57.824036 24732 solver.cpp:334]     Train net output #0: loss = 0.00317629 (* 1 = 0.00317629 loss)
I0814 19:43:57.824057 24732 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0814 19:43:58.425570 24716 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:43:59.486100 24732 solver.cpp:363] Sparsity after update:
I0814 19:43:59.487625 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:43:59.487633 24732 net.cpp:2192] conv1a_param_0(0.371) 
I0814 19:43:59.487642 24732 net.cpp:2192] conv1b_param_0(0.723) 
I0814 19:43:59.487646 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:43:59.487650 24732 net.cpp:2192] res2a_branch2a_param_0(0.788) 
I0814 19:43:59.487654 24732 net.cpp:2192] res2a_branch2b_param_0(0.701) 
I0814 19:43:59.487658 24732 net.cpp:2192] res3a_branch2a_param_0(0.792) 
I0814 19:43:59.487663 24732 net.cpp:2192] res3a_branch2b_param_0(0.741) 
I0814 19:43:59.487665 24732 net.cpp:2192] res4a_branch2a_param_0(0.799) 
I0814 19:43:59.487669 24732 net.cpp:2192] res4a_branch2b_param_0(0.796) 
I0814 19:43:59.487673 24732 net.cpp:2192] res5a_branch2a_param_0(0.764) 
I0814 19:43:59.487676 24732 net.cpp:2192] res5a_branch2b_param_0(0.799) 
I0814 19:43:59.487694 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.83555e+06/2.3599e+06) 0.778
I0814 19:43:59.487707 24732 solver.cpp:509] Iteration 44000, Testing net (#0)
I0814 19:44:00.291728 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910883
I0814 19:44:00.291748 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:44:00.291754 24732 solver.cpp:594]     Test net output #2: loss = 0.375525 (* 1 = 0.375525 loss)
I0814 19:44:00.291774 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.804037s
I0814 19:44:00.307719 24783 solver.cpp:409] Finding and applying sparsity: 0.82
I0814 19:44:43.451889 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:44:43.453974 24732 solver.cpp:312] Iteration 44000 (2.1916 iter/s, 45.6287s/100 iter), loss = 0.00289719
I0814 19:44:43.453994 24732 solver.cpp:334]     Train net output #0: loss = 0.00289699 (* 1 = 0.00289699 loss)
I0814 19:44:43.454002 24732 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0814 19:44:45.395800 24732 solver.cpp:312] Iteration 44100 (51.4995 iter/s, 1.94177s/100 iter), loss = 0.00316584
I0814 19:44:45.395823 24732 solver.cpp:334]     Train net output #0: loss = 0.00316564 (* 1 = 0.00316564 loss)
I0814 19:44:45.395828 24732 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0814 19:44:47.040112 24732 solver.cpp:312] Iteration 44200 (60.8177 iter/s, 1.64426s/100 iter), loss = 0.00374066
I0814 19:44:47.040176 24732 solver.cpp:334]     Train net output #0: loss = 0.00374048 (* 1 = 0.00374048 loss)
I0814 19:44:47.040194 24732 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0814 19:44:48.697098 24732 solver.cpp:312] Iteration 44300 (60.3524 iter/s, 1.65694s/100 iter), loss = 0.00155081
I0814 19:44:48.697149 24732 solver.cpp:334]     Train net output #0: loss = 0.00155064 (* 1 = 0.00155064 loss)
I0814 19:44:48.697165 24732 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0814 19:44:50.340046 24732 solver.cpp:312] Iteration 44400 (60.8681 iter/s, 1.6429s/100 iter), loss = 0.00750333
I0814 19:44:50.340072 24732 solver.cpp:334]     Train net output #0: loss = 0.00750316 (* 1 = 0.00750316 loss)
I0814 19:44:50.340078 24732 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0814 19:44:51.961629 24732 solver.cpp:312] Iteration 44500 (61.6701 iter/s, 1.62153s/100 iter), loss = 0.00136405
I0814 19:44:51.961659 24732 solver.cpp:334]     Train net output #0: loss = 0.00136388 (* 1 = 0.00136388 loss)
I0814 19:44:51.961666 24732 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0814 19:44:53.588526 24732 solver.cpp:312] Iteration 44600 (61.4685 iter/s, 1.62685s/100 iter), loss = 0.0017204
I0814 19:44:53.588551 24732 solver.cpp:334]     Train net output #0: loss = 0.00172023 (* 1 = 0.00172023 loss)
I0814 19:44:53.588557 24732 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0814 19:44:55.238241 24732 solver.cpp:312] Iteration 44700 (60.6185 iter/s, 1.64966s/100 iter), loss = 0.0013025
I0814 19:44:55.238266 24732 solver.cpp:334]     Train net output #0: loss = 0.00130233 (* 1 = 0.00130233 loss)
I0814 19:44:55.238272 24732 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0814 19:44:56.866319 24732 solver.cpp:312] Iteration 44800 (61.424 iter/s, 1.62803s/100 iter), loss = 0.00026684
I0814 19:44:56.866380 24732 solver.cpp:334]     Train net output #0: loss = 0.000266669 (* 1 = 0.000266669 loss)
I0814 19:44:56.866400 24732 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0814 19:44:58.496909 24732 solver.cpp:312] Iteration 44900 (61.3295 iter/s, 1.63054s/100 iter), loss = 0.0199795
I0814 19:44:58.496968 24732 solver.cpp:334]     Train net output #0: loss = 0.0199793 (* 1 = 0.0199793 loss)
I0814 19:44:58.496987 24732 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0814 19:45:00.118031 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:00.119702 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:00.119712 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:00.119719 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:00.119724 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:00.119729 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:00.119731 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:00.119735 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:00.119740 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:00.119742 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:00.119747 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:00.119750 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:00.119755 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:00.119758 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:00.119783 24732 solver.cpp:509] Iteration 45000, Testing net (#0)
I0814 19:45:00.942535 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.899413
I0814 19:45:00.942554 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:45:00.942561 24732 solver.cpp:594]     Test net output #2: loss = 0.429454 (* 1 = 0.429454 loss)
I0814 19:45:00.942579 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.822766s
I0814 19:45:00.958400 24732 solver.cpp:312] Iteration 45000 (40.6271 iter/s, 2.46141s/100 iter), loss = 0.00614304
I0814 19:45:00.958443 24732 solver.cpp:334]     Train net output #0: loss = 0.00614287 (* 1 = 0.00614287 loss)
I0814 19:45:00.958456 24732 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0814 19:45:02.578477 24732 solver.cpp:312] Iteration 45100 (61.7273 iter/s, 1.62003s/100 iter), loss = 0.00510173
I0814 19:45:02.578502 24732 solver.cpp:334]     Train net output #0: loss = 0.00510156 (* 1 = 0.00510156 loss)
I0814 19:45:02.578508 24732 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0814 19:45:04.249733 24732 solver.cpp:312] Iteration 45200 (59.8371 iter/s, 1.6712s/100 iter), loss = 0.00286821
I0814 19:45:04.249795 24732 solver.cpp:334]     Train net output #0: loss = 0.00286803 (* 1 = 0.00286803 loss)
I0814 19:45:04.249815 24732 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0814 19:45:05.877025 24732 solver.cpp:312] Iteration 45300 (61.4538 iter/s, 1.62724s/100 iter), loss = 0.000631504
I0814 19:45:05.877051 24732 solver.cpp:334]     Train net output #0: loss = 0.000631321 (* 1 = 0.000631321 loss)
I0814 19:45:05.877056 24732 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0814 19:45:07.492733 24732 solver.cpp:312] Iteration 45400 (61.8944 iter/s, 1.61565s/100 iter), loss = 0.0100706
I0814 19:45:07.493044 24732 solver.cpp:334]     Train net output #0: loss = 0.0100704 (* 1 = 0.0100704 loss)
I0814 19:45:07.493068 24732 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0814 19:45:09.102313 24732 solver.cpp:312] Iteration 45500 (62.13 iter/s, 1.60953s/100 iter), loss = 0.00429005
I0814 19:45:09.102337 24732 solver.cpp:334]     Train net output #0: loss = 0.00428986 (* 1 = 0.00428986 loss)
I0814 19:45:09.102344 24732 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0814 19:45:10.766691 24732 solver.cpp:312] Iteration 45600 (60.0844 iter/s, 1.66433s/100 iter), loss = 0.00164105
I0814 19:45:10.766716 24732 solver.cpp:334]     Train net output #0: loss = 0.00164086 (* 1 = 0.00164086 loss)
I0814 19:45:10.766721 24732 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0814 19:45:12.408849 24732 solver.cpp:312] Iteration 45700 (60.8974 iter/s, 1.64211s/100 iter), loss = 0.0057923
I0814 19:45:12.408874 24732 solver.cpp:334]     Train net output #0: loss = 0.00579212 (* 1 = 0.00579212 loss)
I0814 19:45:12.408880 24732 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0814 19:45:14.058796 24732 solver.cpp:312] Iteration 45800 (60.6099 iter/s, 1.6499s/100 iter), loss = 0.000685582
I0814 19:45:14.058871 24732 solver.cpp:334]     Train net output #0: loss = 0.000685394 (* 1 = 0.000685394 loss)
I0814 19:45:14.058878 24732 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0814 19:45:15.735945 24732 solver.cpp:312] Iteration 45900 (59.627 iter/s, 1.67709s/100 iter), loss = 0.00284487
I0814 19:45:15.735992 24732 solver.cpp:334]     Train net output #0: loss = 0.00284468 (* 1 = 0.00284468 loss)
I0814 19:45:15.736003 24732 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0814 19:45:17.380292 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:17.381755 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:17.381763 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:17.381780 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:17.381791 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:17.381800 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:17.381809 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:17.381817 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:17.381827 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:17.381836 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:17.381846 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:17.381855 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:17.381865 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:17.381875 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:17.381892 24732 solver.cpp:509] Iteration 46000, Testing net (#0)
I0814 19:45:18.189019 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906766
I0814 19:45:18.189038 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:45:18.189043 24732 solver.cpp:594]     Test net output #2: loss = 0.380803 (* 1 = 0.380803 loss)
I0814 19:45:18.189060 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.80714s
I0814 19:45:18.204962 24732 solver.cpp:312] Iteration 46000 (40.5031 iter/s, 2.46894s/100 iter), loss = 0.00646185
I0814 19:45:18.204977 24732 solver.cpp:334]     Train net output #0: loss = 0.00646166 (* 1 = 0.00646166 loss)
I0814 19:45:18.204980 24732 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0814 19:45:19.881498 24732 solver.cpp:312] Iteration 46100 (59.6487 iter/s, 1.67648s/100 iter), loss = 0.00628806
I0814 19:45:19.881561 24732 solver.cpp:334]     Train net output #0: loss = 0.00628787 (* 1 = 0.00628787 loss)
I0814 19:45:19.881580 24732 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0814 19:45:21.540662 24732 solver.cpp:312] Iteration 46200 (60.2733 iter/s, 1.65911s/100 iter), loss = 0.00107459
I0814 19:45:21.540729 24732 solver.cpp:334]     Train net output #0: loss = 0.0010744 (* 1 = 0.0010744 loss)
I0814 19:45:21.540750 24732 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0814 19:45:23.180186 24732 solver.cpp:312] Iteration 46300 (60.9954 iter/s, 1.63947s/100 iter), loss = 0.00255637
I0814 19:45:23.180209 24732 solver.cpp:334]     Train net output #0: loss = 0.00255618 (* 1 = 0.00255618 loss)
I0814 19:45:23.180215 24732 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0814 19:45:24.855367 24732 solver.cpp:312] Iteration 46400 (59.6968 iter/s, 1.67513s/100 iter), loss = 0.000989639
I0814 19:45:24.855393 24732 solver.cpp:334]     Train net output #0: loss = 0.000989447 (* 1 = 0.000989447 loss)
I0814 19:45:24.855399 24732 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0814 19:45:26.489683 24732 solver.cpp:312] Iteration 46500 (61.1897 iter/s, 1.63426s/100 iter), loss = 0.0096012
I0814 19:45:26.489713 24732 solver.cpp:334]     Train net output #0: loss = 0.00960101 (* 1 = 0.00960101 loss)
I0814 19:45:26.489720 24732 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0814 19:45:28.145165 24732 solver.cpp:312] Iteration 46600 (60.4073 iter/s, 1.65543s/100 iter), loss = 0.000646559
I0814 19:45:28.145229 24732 solver.cpp:334]     Train net output #0: loss = 0.000646367 (* 1 = 0.000646367 loss)
I0814 19:45:28.145267 24732 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0814 19:45:29.804136 24732 solver.cpp:312] Iteration 46700 (60.2804 iter/s, 1.65891s/100 iter), loss = 0.00427841
I0814 19:45:29.804159 24732 solver.cpp:334]     Train net output #0: loss = 0.00427821 (* 1 = 0.00427821 loss)
I0814 19:45:29.804165 24732 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0814 19:45:31.450904 24732 solver.cpp:312] Iteration 46800 (60.7269 iter/s, 1.64672s/100 iter), loss = 0.0141461
I0814 19:45:31.450966 24732 solver.cpp:334]     Train net output #0: loss = 0.0141459 (* 1 = 0.0141459 loss)
I0814 19:45:31.450985 24732 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0814 19:45:33.071557 24732 solver.cpp:312] Iteration 46900 (61.7055 iter/s, 1.6206s/100 iter), loss = 0.00131976
I0814 19:45:33.071581 24732 solver.cpp:334]     Train net output #0: loss = 0.00131956 (* 1 = 0.00131956 loss)
I0814 19:45:33.071588 24732 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0814 19:45:34.689520 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:34.691215 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:34.691224 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:34.691232 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:34.691243 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:34.691252 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:34.691262 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:34.691272 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:34.691279 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:34.691287 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:34.691295 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:34.691305 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:34.691315 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:34.691324 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:34.691342 24732 solver.cpp:509] Iteration 47000, Testing net (#0)
I0814 19:45:35.500025 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.899413
I0814 19:45:35.500043 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:45:35.500048 24732 solver.cpp:594]     Test net output #2: loss = 0.405145 (* 1 = 0.405145 loss)
I0814 19:45:35.500066 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.808695s
I0814 19:45:35.519748 24732 solver.cpp:312] Iteration 47000 (40.8477 iter/s, 2.44812s/100 iter), loss = 0.00161108
I0814 19:45:35.520150 24732 solver.cpp:334]     Train net output #0: loss = 0.00161088 (* 1 = 0.00161088 loss)
I0814 19:45:35.520161 24732 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0814 19:45:37.189707 24732 solver.cpp:312] Iteration 47100 (59.8836 iter/s, 1.66991s/100 iter), loss = 0.00546813
I0814 19:45:37.189774 24732 solver.cpp:334]     Train net output #0: loss = 0.00546794 (* 1 = 0.00546794 loss)
I0814 19:45:37.189792 24732 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0814 19:45:38.843749 24732 solver.cpp:312] Iteration 47200 (60.46 iter/s, 1.65399s/100 iter), loss = 0.000358653
I0814 19:45:38.843772 24732 solver.cpp:334]     Train net output #0: loss = 0.000358457 (* 1 = 0.000358457 loss)
I0814 19:45:38.843778 24732 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0814 19:45:40.493077 24732 solver.cpp:312] Iteration 47300 (60.6328 iter/s, 1.64927s/100 iter), loss = 0.00242342
I0814 19:45:40.493132 24732 solver.cpp:334]     Train net output #0: loss = 0.00242323 (* 1 = 0.00242323 loss)
I0814 19:45:40.493147 24732 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0814 19:45:42.146915 24732 solver.cpp:312] Iteration 47400 (60.4673 iter/s, 1.65379s/100 iter), loss = 0.00130676
I0814 19:45:42.146940 24732 solver.cpp:334]     Train net output #0: loss = 0.00130657 (* 1 = 0.00130657 loss)
I0814 19:45:42.146944 24732 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0814 19:45:43.780467 24732 solver.cpp:312] Iteration 47500 (61.2183 iter/s, 1.6335s/100 iter), loss = 0.00114526
I0814 19:45:43.780493 24732 solver.cpp:334]     Train net output #0: loss = 0.00114507 (* 1 = 0.00114507 loss)
I0814 19:45:43.780496 24732 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0814 19:45:45.448865 24732 solver.cpp:312] Iteration 47600 (59.9396 iter/s, 1.66835s/100 iter), loss = 0.000159153
I0814 19:45:45.448997 24732 solver.cpp:334]     Train net output #0: loss = 0.000158966 (* 1 = 0.000158966 loss)
I0814 19:45:45.449021 24732 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0814 19:45:47.134116 24732 solver.cpp:312] Iteration 47700 (59.3403 iter/s, 1.6852s/100 iter), loss = 0.00775853
I0814 19:45:47.134178 24732 solver.cpp:334]     Train net output #0: loss = 0.00775834 (* 1 = 0.00775834 loss)
I0814 19:45:47.134197 24732 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0814 19:45:48.783066 24732 solver.cpp:312] Iteration 47800 (60.6465 iter/s, 1.6489s/100 iter), loss = 0.00137352
I0814 19:45:48.783114 24732 solver.cpp:334]     Train net output #0: loss = 0.00137334 (* 1 = 0.00137334 loss)
I0814 19:45:48.783129 24732 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0814 19:45:50.400327 24732 solver.cpp:312] Iteration 47900 (61.8349 iter/s, 1.61721s/100 iter), loss = 0.000142569
I0814 19:45:50.400390 24732 solver.cpp:334]     Train net output #0: loss = 0.000142386 (* 1 = 0.000142386 loss)
I0814 19:45:50.400409 24732 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0814 19:45:52.045738 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:52.047408 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:52.047420 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:52.047441 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:52.047453 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:52.047466 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:52.047477 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:52.047489 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:52.047500 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:52.047511 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:52.047519 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:52.047528 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:52.047539 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:52.047550 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:52.047575 24732 solver.cpp:509] Iteration 48000, Testing net (#0)
I0814 19:45:52.079618 24730 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:45:52.886742 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905883
I0814 19:45:52.886762 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:45:52.886767 24732 solver.cpp:594]     Test net output #2: loss = 0.391059 (* 1 = 0.391059 loss)
I0814 19:45:52.886785 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83918s
I0814 19:45:52.905315 24732 solver.cpp:312] Iteration 48000 (39.9216 iter/s, 2.50491s/100 iter), loss = 0.00168554
I0814 19:45:52.905340 24732 solver.cpp:334]     Train net output #0: loss = 0.00168536 (* 1 = 0.00168536 loss)
I0814 19:45:52.905346 24732 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0814 19:45:54.521339 24732 solver.cpp:312] Iteration 48100 (61.8822 iter/s, 1.61597s/100 iter), loss = 0.00518548
I0814 19:45:54.521363 24732 solver.cpp:334]     Train net output #0: loss = 0.0051853 (* 1 = 0.0051853 loss)
I0814 19:45:54.521369 24732 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0814 19:45:56.203027 24732 solver.cpp:312] Iteration 48200 (59.4659 iter/s, 1.68164s/100 iter), loss = 0.00350444
I0814 19:45:56.203094 24732 solver.cpp:334]     Train net output #0: loss = 0.00350426 (* 1 = 0.00350426 loss)
I0814 19:45:56.203114 24732 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0814 19:45:57.866593 24732 solver.cpp:312] Iteration 48300 (60.1138 iter/s, 1.66351s/100 iter), loss = 0.00594211
I0814 19:45:57.866742 24732 solver.cpp:334]     Train net output #0: loss = 0.00594193 (* 1 = 0.00594193 loss)
I0814 19:45:57.866765 24732 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0814 19:45:59.510356 24732 solver.cpp:312] Iteration 48400 (60.8383 iter/s, 1.6437s/100 iter), loss = 0.00117012
I0814 19:45:59.510437 24732 solver.cpp:334]     Train net output #0: loss = 0.00116994 (* 1 = 0.00116994 loss)
I0814 19:45:59.510464 24732 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0814 19:46:01.116931 24732 solver.cpp:312] Iteration 48500 (62.2461 iter/s, 1.60653s/100 iter), loss = 0.000447479
I0814 19:46:01.116956 24732 solver.cpp:334]     Train net output #0: loss = 0.000447301 (* 1 = 0.000447301 loss)
I0814 19:46:01.116961 24732 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0814 19:46:02.765733 24732 solver.cpp:312] Iteration 48600 (60.6521 iter/s, 1.64875s/100 iter), loss = 0.00208838
I0814 19:46:02.765784 24732 solver.cpp:334]     Train net output #0: loss = 0.0020882 (* 1 = 0.0020882 loss)
I0814 19:46:02.765795 24732 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0814 19:46:04.425096 24732 solver.cpp:312] Iteration 48700 (60.266 iter/s, 1.65931s/100 iter), loss = 0.0002978
I0814 19:46:04.425145 24732 solver.cpp:334]     Train net output #0: loss = 0.000297624 (* 1 = 0.000297624 loss)
I0814 19:46:04.425158 24732 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0814 19:46:06.085487 24732 solver.cpp:312] Iteration 48800 (60.2287 iter/s, 1.66034s/100 iter), loss = 0.0012009
I0814 19:46:06.085512 24732 solver.cpp:334]     Train net output #0: loss = 0.00120072 (* 1 = 0.00120072 loss)
I0814 19:46:06.085517 24732 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0814 19:46:07.711839 24732 solver.cpp:312] Iteration 48900 (61.4893 iter/s, 1.6263s/100 iter), loss = 0.0009178
I0814 19:46:07.711869 24732 solver.cpp:334]     Train net output #0: loss = 0.000917628 (* 1 = 0.000917628 loss)
I0814 19:46:07.711874 24732 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0814 19:46:09.345917 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:09.347594 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:09.347604 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:09.347611 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:09.347615 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:09.347625 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:09.347641 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:09.347645 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:09.347650 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:09.347656 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:09.347661 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:09.347666 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:09.347673 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:09.347677 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:09.347692 24732 solver.cpp:509] Iteration 49000, Testing net (#0)
I0814 19:46:10.181808 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910001
I0814 19:46:10.181826 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:46:10.181831 24732 solver.cpp:594]     Test net output #2: loss = 0.374734 (* 1 = 0.374734 loss)
I0814 19:46:10.181849 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.834128s
I0814 19:46:10.198374 24732 solver.cpp:312] Iteration 49000 (40.2178 iter/s, 2.48646s/100 iter), loss = 0.00557617
I0814 19:46:10.198390 24732 solver.cpp:334]     Train net output #0: loss = 0.005576 (* 1 = 0.005576 loss)
I0814 19:46:10.198393 24732 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0814 19:46:11.828636 24732 solver.cpp:312] Iteration 49100 (61.3419 iter/s, 1.63021s/100 iter), loss = 0.00342955
I0814 19:46:11.828699 24732 solver.cpp:334]     Train net output #0: loss = 0.00342938 (* 1 = 0.00342938 loss)
I0814 19:46:11.828717 24732 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0814 19:46:13.504168 24732 solver.cpp:312] Iteration 49200 (59.6845 iter/s, 1.67548s/100 iter), loss = 0.00228195
I0814 19:46:13.504240 24732 solver.cpp:334]     Train net output #0: loss = 0.00228178 (* 1 = 0.00228178 loss)
I0814 19:46:13.504276 24732 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0814 19:46:15.157135 24732 solver.cpp:312] Iteration 49300 (60.4993 iter/s, 1.65291s/100 iter), loss = 0.000719808
I0814 19:46:15.157182 24732 solver.cpp:334]     Train net output #0: loss = 0.000719639 (* 1 = 0.000719639 loss)
I0814 19:46:15.157196 24732 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0814 19:46:16.792868 24732 solver.cpp:312] Iteration 49400 (61.1366 iter/s, 1.63568s/100 iter), loss = 0.0010356
I0814 19:46:16.793000 24732 solver.cpp:334]     Train net output #0: loss = 0.00103543 (* 1 = 0.00103543 loss)
I0814 19:46:16.793028 24732 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0814 19:46:18.433589 24732 solver.cpp:312] Iteration 49500 (60.9508 iter/s, 1.64067s/100 iter), loss = 0.00156729
I0814 19:46:18.433621 24732 solver.cpp:334]     Train net output #0: loss = 0.00156712 (* 1 = 0.00156712 loss)
I0814 19:46:18.433627 24732 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0814 19:46:20.089946 24732 solver.cpp:312] Iteration 49600 (60.3753 iter/s, 1.65631s/100 iter), loss = 0.00166838
I0814 19:46:20.089994 24732 solver.cpp:334]     Train net output #0: loss = 0.00166821 (* 1 = 0.00166821 loss)
I0814 19:46:20.090008 24732 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0814 19:46:21.798602 24732 solver.cpp:312] Iteration 49700 (58.5273 iter/s, 1.7086s/100 iter), loss = 0.000140487
I0814 19:46:21.798691 24732 solver.cpp:334]     Train net output #0: loss = 0.000140318 (* 1 = 0.000140318 loss)
I0814 19:46:21.798701 24732 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0814 19:46:23.439321 24732 solver.cpp:312] Iteration 49800 (60.951 iter/s, 1.64066s/100 iter), loss = 0.000340135
I0814 19:46:23.439370 24732 solver.cpp:334]     Train net output #0: loss = 0.000339965 (* 1 = 0.000339965 loss)
I0814 19:46:23.439383 24732 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0814 19:46:25.055382 24732 solver.cpp:312] Iteration 49900 (61.8808 iter/s, 1.61601s/100 iter), loss = 0.000991804
I0814 19:46:25.055449 24732 solver.cpp:334]     Train net output #0: loss = 0.000991636 (* 1 = 0.000991636 loss)
I0814 19:46:25.055476 24732 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0814 19:46:26.712857 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0814 19:46:26.720752 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0814 19:46:26.724236 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:26.725920 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:26.725929 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:26.725937 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:26.725950 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:26.725955 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:26.725963 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:26.725968 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:26.725976 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:26.725981 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:26.725989 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:26.725994 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:26.726002 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:26.726006 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:26.726022 24732 solver.cpp:509] Iteration 50000, Testing net (#0)
I0814 19:46:27.536077 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905295
I0814 19:46:27.536098 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:46:27.536106 24732 solver.cpp:594]     Test net output #2: loss = 0.390708 (* 1 = 0.390708 loss)
I0814 19:46:27.536149 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810081s
I0814 19:46:27.554985 24732 solver.cpp:312] Iteration 50000 (40.0076 iter/s, 2.49952s/100 iter), loss = 0.000394828
I0814 19:46:27.555007 24732 solver.cpp:334]     Train net output #0: loss = 0.00039466 (* 1 = 0.00039466 loss)
I0814 19:46:27.555014 24732 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0814 19:46:29.165817 24732 solver.cpp:312] Iteration 50100 (62.0817 iter/s, 1.61078s/100 iter), loss = 0.0026114
I0814 19:46:29.165841 24732 solver.cpp:334]     Train net output #0: loss = 0.00261123 (* 1 = 0.00261123 loss)
I0814 19:46:29.165869 24732 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0814 19:46:30.823281 24732 solver.cpp:312] Iteration 50200 (60.335 iter/s, 1.65741s/100 iter), loss = 0.0212677
I0814 19:46:30.823305 24732 solver.cpp:334]     Train net output #0: loss = 0.0212675 (* 1 = 0.0212675 loss)
I0814 19:46:30.823310 24732 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0814 19:46:32.489959 24732 solver.cpp:312] Iteration 50300 (60.0015 iter/s, 1.66663s/100 iter), loss = 0.000550282
I0814 19:46:32.489984 24732 solver.cpp:334]     Train net output #0: loss = 0.000550118 (* 1 = 0.000550118 loss)
I0814 19:46:32.489989 24732 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0814 19:46:34.138135 24732 solver.cpp:312] Iteration 50400 (60.6751 iter/s, 1.64812s/100 iter), loss = 0.00320841
I0814 19:46:34.138160 24732 solver.cpp:334]     Train net output #0: loss = 0.00320825 (* 1 = 0.00320825 loss)
I0814 19:46:34.138166 24732 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0814 19:46:35.756839 24732 solver.cpp:312] Iteration 50500 (61.7797 iter/s, 1.61865s/100 iter), loss = 0.00501047
I0814 19:46:35.756907 24732 solver.cpp:334]     Train net output #0: loss = 0.00501031 (* 1 = 0.00501031 loss)
I0814 19:46:35.756928 24732 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0814 19:46:37.398265 24732 solver.cpp:312] Iteration 50600 (60.9246 iter/s, 1.64137s/100 iter), loss = 0.00362599
I0814 19:46:37.398334 24732 solver.cpp:334]     Train net output #0: loss = 0.00362583 (* 1 = 0.00362583 loss)
I0814 19:46:37.398353 24732 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0814 19:46:39.034327 24732 solver.cpp:312] Iteration 50700 (61.1243 iter/s, 1.63601s/100 iter), loss = 0.000554803
I0814 19:46:39.034375 24732 solver.cpp:334]     Train net output #0: loss = 0.000554639 (* 1 = 0.000554639 loss)
I0814 19:46:39.034389 24732 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0814 19:46:40.696765 24732 solver.cpp:312] Iteration 50800 (60.1546 iter/s, 1.66238s/100 iter), loss = 0.00195463
I0814 19:46:40.696812 24732 solver.cpp:334]     Train net output #0: loss = 0.00195447 (* 1 = 0.00195447 loss)
I0814 19:46:40.696826 24732 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0814 19:46:42.307680 24732 solver.cpp:312] Iteration 50900 (62.0786 iter/s, 1.61086s/100 iter), loss = 0.000837453
I0814 19:46:42.307705 24732 solver.cpp:334]     Train net output #0: loss = 0.00083729 (* 1 = 0.00083729 loss)
I0814 19:46:42.307710 24732 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0814 19:46:43.940557 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:43.942268 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:43.942278 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:43.942286 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:43.942297 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:43.942307 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:43.942317 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:43.942324 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:43.942332 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:43.942340 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:43.942350 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:43.942359 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:43.942368 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:43.942378 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:43.942395 24732 solver.cpp:509] Iteration 51000, Testing net (#0)
I0814 19:46:44.759583 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907648
I0814 19:46:44.759603 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:46:44.759608 24732 solver.cpp:594]     Test net output #2: loss = 0.396453 (* 1 = 0.396453 loss)
I0814 19:46:44.759640 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.817216s
I0814 19:46:44.777479 24732 solver.cpp:312] Iteration 51000 (40.4903 iter/s, 2.46973s/100 iter), loss = 0.00299583
I0814 19:46:44.777498 24732 solver.cpp:334]     Train net output #0: loss = 0.00299567 (* 1 = 0.00299567 loss)
I0814 19:46:44.777503 24732 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0814 19:46:46.377229 24732 solver.cpp:312] Iteration 51100 (62.5119 iter/s, 1.5997s/100 iter), loss = 0.000207583
I0814 19:46:46.377254 24732 solver.cpp:334]     Train net output #0: loss = 0.000207419 (* 1 = 0.000207419 loss)
I0814 19:46:46.377260 24732 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0814 19:46:48.048111 24732 solver.cpp:312] Iteration 51200 (59.8505 iter/s, 1.67083s/100 iter), loss = 0.000346497
I0814 19:46:48.048197 24732 solver.cpp:334]     Train net output #0: loss = 0.000346334 (* 1 = 0.000346334 loss)
I0814 19:46:48.048204 24732 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0814 19:46:49.723917 24732 solver.cpp:312] Iteration 51300 (59.6747 iter/s, 1.67575s/100 iter), loss = 0.000380972
I0814 19:46:49.723976 24732 solver.cpp:334]     Train net output #0: loss = 0.000380807 (* 1 = 0.000380807 loss)
I0814 19:46:49.723994 24732 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0814 19:46:51.364089 24732 solver.cpp:312] Iteration 51400 (60.9712 iter/s, 1.64012s/100 iter), loss = 0.00129579
I0814 19:46:51.364114 24732 solver.cpp:334]     Train net output #0: loss = 0.00129562 (* 1 = 0.00129562 loss)
I0814 19:46:51.364120 24732 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0814 19:46:52.995292 24732 solver.cpp:312] Iteration 51500 (61.3064 iter/s, 1.63115s/100 iter), loss = 0.000211695
I0814 19:46:52.995316 24732 solver.cpp:334]     Train net output #0: loss = 0.00021153 (* 1 = 0.00021153 loss)
I0814 19:46:52.995322 24732 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0814 19:46:54.645207 24732 solver.cpp:312] Iteration 51600 (60.6111 iter/s, 1.64986s/100 iter), loss = 0.000746543
I0814 19:46:54.645232 24732 solver.cpp:334]     Train net output #0: loss = 0.000746378 (* 1 = 0.000746378 loss)
I0814 19:46:54.645237 24732 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0814 19:46:56.298024 24732 solver.cpp:312] Iteration 51700 (60.5047 iter/s, 1.65276s/100 iter), loss = 0.0202098
I0814 19:46:56.298173 24732 solver.cpp:334]     Train net output #0: loss = 0.0202096 (* 1 = 0.0202096 loss)
I0814 19:46:56.298193 24732 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0814 19:46:57.973520 24732 solver.cpp:312] Iteration 51800 (59.6857 iter/s, 1.67544s/100 iter), loss = 0.000722333
I0814 19:46:57.973549 24732 solver.cpp:334]     Train net output #0: loss = 0.00072217 (* 1 = 0.00072217 loss)
I0814 19:46:57.973556 24732 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0814 19:46:59.608155 24732 solver.cpp:312] Iteration 51900 (61.1777 iter/s, 1.63458s/100 iter), loss = 0.000391087
I0814 19:46:59.608177 24732 solver.cpp:334]     Train net output #0: loss = 0.000390924 (* 1 = 0.000390924 loss)
I0814 19:46:59.608183 24732 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0814 19:47:01.237748 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:01.239367 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:01.239377 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:01.239392 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:01.239398 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:01.239403 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:01.239410 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:01.239415 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:01.239418 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:01.239426 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:01.239430 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:01.239434 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:01.239441 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:01.239450 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:01.239461 24732 solver.cpp:509] Iteration 52000, Testing net (#0)
I0814 19:47:01.962494 24730 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:47:02.059015 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.901766
I0814 19:47:02.059034 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:47:02.059039 24732 solver.cpp:594]     Test net output #2: loss = 0.415919 (* 1 = 0.415919 loss)
I0814 19:47:02.059056 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819566s
I0814 19:47:02.076151 24732 solver.cpp:312] Iteration 52000 (40.5199 iter/s, 2.46792s/100 iter), loss = 0.000158553
I0814 19:47:02.076184 24732 solver.cpp:334]     Train net output #0: loss = 0.00015839 (* 1 = 0.00015839 loss)
I0814 19:47:02.076189 24732 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0814 19:47:03.753038 24732 solver.cpp:312] Iteration 52100 (59.6363 iter/s, 1.67683s/100 iter), loss = 0.000877061
I0814 19:47:03.753067 24732 solver.cpp:334]     Train net output #0: loss = 0.000876898 (* 1 = 0.000876898 loss)
I0814 19:47:03.753072 24732 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0814 19:47:05.421377 24732 solver.cpp:312] Iteration 52200 (59.9418 iter/s, 1.66829s/100 iter), loss = 0.00200717
I0814 19:47:05.421443 24732 solver.cpp:334]     Train net output #0: loss = 0.00200701 (* 1 = 0.00200701 loss)
I0814 19:47:05.421470 24732 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0814 19:47:07.104806 24732 solver.cpp:312] Iteration 52300 (59.4046 iter/s, 1.68337s/100 iter), loss = 0.00172062
I0814 19:47:07.104833 24732 solver.cpp:334]     Train net output #0: loss = 0.00172045 (* 1 = 0.00172045 loss)
I0814 19:47:07.104840 24732 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0814 19:47:08.771315 24732 solver.cpp:312] Iteration 52400 (60.0076 iter/s, 1.66646s/100 iter), loss = 0.000248581
I0814 19:47:08.771339 24732 solver.cpp:334]     Train net output #0: loss = 0.000248418 (* 1 = 0.000248418 loss)
I0814 19:47:08.771344 24732 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0814 19:47:10.403455 24732 solver.cpp:312] Iteration 52500 (61.2712 iter/s, 1.63209s/100 iter), loss = 0.000985062
I0814 19:47:10.403486 24732 solver.cpp:334]     Train net output #0: loss = 0.000984899 (* 1 = 0.000984899 loss)
I0814 19:47:10.403492 24732 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0814 19:47:12.045589 24732 solver.cpp:312] Iteration 52600 (60.8983 iter/s, 1.64208s/100 iter), loss = 0.000672462
I0814 19:47:12.045636 24732 solver.cpp:334]     Train net output #0: loss = 0.000672296 (* 1 = 0.000672296 loss)
I0814 19:47:12.045650 24732 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0814 19:47:13.696193 24732 solver.cpp:312] Iteration 52700 (60.5858 iter/s, 1.65055s/100 iter), loss = 0.000433707
I0814 19:47:13.696220 24732 solver.cpp:334]     Train net output #0: loss = 0.00043354 (* 1 = 0.00043354 loss)
I0814 19:47:13.696226 24732 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0814 19:47:15.352946 24732 solver.cpp:312] Iteration 52800 (60.3609 iter/s, 1.6567s/100 iter), loss = 0.00139823
I0814 19:47:15.353204 24732 solver.cpp:334]     Train net output #0: loss = 0.00139806 (* 1 = 0.00139806 loss)
I0814 19:47:15.353210 24732 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0814 19:47:17.011152 24732 solver.cpp:312] Iteration 52900 (60.3082 iter/s, 1.65815s/100 iter), loss = 0.000619081
I0814 19:47:17.011176 24732 solver.cpp:334]     Train net output #0: loss = 0.000618915 (* 1 = 0.000618915 loss)
I0814 19:47:17.011180 24732 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0814 19:47:18.617033 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:18.618651 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:18.618659 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:18.618664 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:18.618666 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:18.618670 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:18.618674 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:18.618676 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:18.618680 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:18.618690 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:18.618693 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:18.618697 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:18.618700 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:18.618703 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:18.618716 24732 solver.cpp:509] Iteration 53000, Testing net (#0)
I0814 19:47:19.434378 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911472
I0814 19:47:19.434399 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995294
I0814 19:47:19.434406 24732 solver.cpp:594]     Test net output #2: loss = 0.396804 (* 1 = 0.396804 loss)
I0814 19:47:19.434422 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815677s
I0814 19:47:19.450292 24732 solver.cpp:312] Iteration 53000 (40.9993 iter/s, 2.43907s/100 iter), loss = 0.000566136
I0814 19:47:19.450310 24732 solver.cpp:334]     Train net output #0: loss = 0.000565971 (* 1 = 0.000565971 loss)
I0814 19:47:19.450317 24732 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0814 19:47:21.083546 24732 solver.cpp:312] Iteration 53100 (61.2294 iter/s, 1.6332s/100 iter), loss = 0.00628255
I0814 19:47:21.083595 24732 solver.cpp:334]     Train net output #0: loss = 0.00628238 (* 1 = 0.00628238 loss)
I0814 19:47:21.083607 24732 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0814 19:47:22.775871 24732 solver.cpp:312] Iteration 53200 (59.0922 iter/s, 1.69227s/100 iter), loss = 0.000972055
I0814 19:47:22.775895 24732 solver.cpp:334]     Train net output #0: loss = 0.000971889 (* 1 = 0.000971889 loss)
I0814 19:47:22.775902 24732 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0814 19:47:24.406796 24732 solver.cpp:312] Iteration 53300 (61.3168 iter/s, 1.63087s/100 iter), loss = 0.000354552
I0814 19:47:24.406859 24732 solver.cpp:334]     Train net output #0: loss = 0.000354386 (* 1 = 0.000354386 loss)
I0814 19:47:24.406878 24732 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0814 19:47:26.058940 24732 solver.cpp:312] Iteration 53400 (60.5293 iter/s, 1.65209s/100 iter), loss = 0.000241542
I0814 19:47:26.058965 24732 solver.cpp:334]     Train net output #0: loss = 0.000241375 (* 1 = 0.000241375 loss)
I0814 19:47:26.058970 24732 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0814 19:47:27.696969 24732 solver.cpp:312] Iteration 53500 (61.051 iter/s, 1.63798s/100 iter), loss = 0.00137958
I0814 19:47:27.696992 24732 solver.cpp:334]     Train net output #0: loss = 0.00137942 (* 1 = 0.00137942 loss)
I0814 19:47:27.696996 24732 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0814 19:47:29.341034 24732 solver.cpp:312] Iteration 53600 (60.8268 iter/s, 1.64401s/100 iter), loss = 0.00105503
I0814 19:47:29.341081 24732 solver.cpp:334]     Train net output #0: loss = 0.00105487 (* 1 = 0.00105487 loss)
I0814 19:47:29.341094 24732 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0814 19:47:31.004618 24732 solver.cpp:312] Iteration 53700 (60.113 iter/s, 1.66353s/100 iter), loss = 0.000137793
I0814 19:47:31.004680 24732 solver.cpp:334]     Train net output #0: loss = 0.000137628 (* 1 = 0.000137628 loss)
I0814 19:47:31.004699 24732 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0814 19:47:32.705603 24732 solver.cpp:312] Iteration 53800 (58.7914 iter/s, 1.70093s/100 iter), loss = 0.00058894
I0814 19:47:32.705627 24732 solver.cpp:334]     Train net output #0: loss = 0.000588777 (* 1 = 0.000588777 loss)
I0814 19:47:32.705657 24732 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0814 19:47:34.350071 24732 solver.cpp:312] Iteration 53900 (60.8119 iter/s, 1.64442s/100 iter), loss = 0.00302605
I0814 19:47:34.350098 24732 solver.cpp:334]     Train net output #0: loss = 0.00302588 (* 1 = 0.00302588 loss)
I0814 19:47:34.350106 24732 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0814 19:47:35.992545 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:35.994345 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:35.994355 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:35.994364 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:35.994369 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:35.994374 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:35.994379 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:35.994382 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:35.994386 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:35.994390 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:35.994395 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:35.994400 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:35.994403 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:35.994408 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:35.994419 24732 solver.cpp:509] Iteration 54000, Testing net (#0)
I0814 19:47:36.809983 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911766
I0814 19:47:36.810003 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:47:36.810009 24732 solver.cpp:594]     Test net output #2: loss = 0.380741 (* 1 = 0.380741 loss)
I0814 19:47:36.810027 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815579s
I0814 19:47:36.828711 24732 solver.cpp:312] Iteration 54000 (40.3459 iter/s, 2.47856s/100 iter), loss = 0.000415275
I0814 19:47:36.828727 24732 solver.cpp:334]     Train net output #0: loss = 0.000415111 (* 1 = 0.000415111 loss)
I0814 19:47:36.828732 24732 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0814 19:47:38.450276 24732 solver.cpp:312] Iteration 54100 (61.6708 iter/s, 1.62151s/100 iter), loss = 0.000484623
I0814 19:47:38.450332 24732 solver.cpp:334]     Train net output #0: loss = 0.000484459 (* 1 = 0.000484459 loss)
I0814 19:47:38.450346 24732 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0814 19:47:40.112210 24732 solver.cpp:312] Iteration 54200 (60.1729 iter/s, 1.66188s/100 iter), loss = 0.000882286
I0814 19:47:40.112232 24732 solver.cpp:334]     Train net output #0: loss = 0.000882122 (* 1 = 0.000882122 loss)
I0814 19:47:40.112238 24732 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0814 19:47:41.756743 24732 solver.cpp:312] Iteration 54300 (60.8094 iter/s, 1.64448s/100 iter), loss = 0.00134849
I0814 19:47:41.756773 24732 solver.cpp:334]     Train net output #0: loss = 0.00134833 (* 1 = 0.00134833 loss)
I0814 19:47:41.756780 24732 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0814 19:47:43.399646 24732 solver.cpp:312] Iteration 54400 (60.8699 iter/s, 1.64285s/100 iter), loss = 0.001735
I0814 19:47:43.399668 24732 solver.cpp:334]     Train net output #0: loss = 0.00173484 (* 1 = 0.00173484 loss)
I0814 19:47:43.399672 24732 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0814 19:47:45.031991 24732 solver.cpp:312] Iteration 54500 (61.2635 iter/s, 1.63229s/100 iter), loss = 0.00100654
I0814 19:47:45.032024 24732 solver.cpp:334]     Train net output #0: loss = 0.00100638 (* 1 = 0.00100638 loss)
I0814 19:47:45.032032 24732 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0814 19:47:46.696625 24732 solver.cpp:312] Iteration 54600 (60.0751 iter/s, 1.66458s/100 iter), loss = 0.000652223
I0814 19:47:46.696647 24732 solver.cpp:334]     Train net output #0: loss = 0.000652057 (* 1 = 0.000652057 loss)
I0814 19:47:46.696668 24732 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0814 19:47:48.337177 24732 solver.cpp:312] Iteration 54700 (60.9571 iter/s, 1.6405s/100 iter), loss = 0.00235922
I0814 19:47:48.337357 24732 solver.cpp:334]     Train net output #0: loss = 0.00235906 (* 1 = 0.00235906 loss)
I0814 19:47:48.337448 24732 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0814 19:47:49.984449 24732 solver.cpp:312] Iteration 54800 (60.7083 iter/s, 1.64722s/100 iter), loss = 0.00113437
I0814 19:47:49.984589 24732 solver.cpp:334]     Train net output #0: loss = 0.0011342 (* 1 = 0.0011342 loss)
I0814 19:47:49.984611 24732 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0814 19:47:51.659538 24732 solver.cpp:312] Iteration 54900 (59.7002 iter/s, 1.67504s/100 iter), loss = 0.000390641
I0814 19:47:51.659561 24732 solver.cpp:334]     Train net output #0: loss = 0.000390475 (* 1 = 0.000390475 loss)
I0814 19:47:51.659567 24732 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0814 19:47:53.313954 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:53.315578 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:53.315587 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:53.315594 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:53.315598 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:53.315603 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:53.315608 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:53.315613 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:53.315615 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:53.315618 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:53.315623 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:53.315626 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:53.315630 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:53.315634 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:53.315644 24732 solver.cpp:509] Iteration 55000, Testing net (#0)
I0814 19:47:54.147377 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919413
I0814 19:47:54.147397 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:47:54.147405 24732 solver.cpp:594]     Test net output #2: loss = 0.3577 (* 1 = 0.3577 loss)
I0814 19:47:54.147423 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83175s
I0814 19:47:54.163492 24732 solver.cpp:312] Iteration 55000 (39.938 iter/s, 2.50388s/100 iter), loss = 0.000987634
I0814 19:47:54.163525 24732 solver.cpp:334]     Train net output #0: loss = 0.000987467 (* 1 = 0.000987467 loss)
I0814 19:47:54.163537 24732 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0814 19:47:55.813465 24732 solver.cpp:312] Iteration 55100 (60.6091 iter/s, 1.64992s/100 iter), loss = 0.00117576
I0814 19:47:55.813531 24732 solver.cpp:334]     Train net output #0: loss = 0.00117559 (* 1 = 0.00117559 loss)
I0814 19:47:55.813551 24732 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0814 19:47:57.434715 24732 solver.cpp:312] Iteration 55200 (61.6828 iter/s, 1.6212s/100 iter), loss = 0.00109533
I0814 19:47:57.434739 24732 solver.cpp:334]     Train net output #0: loss = 0.00109516 (* 1 = 0.00109516 loss)
I0814 19:47:57.434744 24732 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0814 19:47:59.115281 24732 solver.cpp:312] Iteration 55300 (59.5056 iter/s, 1.68051s/100 iter), loss = 0.000657295
I0814 19:47:59.115305 24732 solver.cpp:334]     Train net output #0: loss = 0.000657126 (* 1 = 0.000657126 loss)
I0814 19:47:59.115311 24732 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0814 19:48:00.773033 24732 solver.cpp:312] Iteration 55400 (60.3246 iter/s, 1.6577s/100 iter), loss = 0.00136124
I0814 19:48:00.773084 24732 solver.cpp:334]     Train net output #0: loss = 0.00136107 (* 1 = 0.00136107 loss)
I0814 19:48:00.773099 24732 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0814 19:48:02.405591 24732 solver.cpp:312] Iteration 55500 (61.2555 iter/s, 1.63251s/100 iter), loss = 0.0014403
I0814 19:48:02.405659 24732 solver.cpp:334]     Train net output #0: loss = 0.00144013 (* 1 = 0.00144013 loss)
I0814 19:48:02.405678 24732 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0814 19:48:04.053086 24732 solver.cpp:312] Iteration 55600 (60.7002 iter/s, 1.64744s/100 iter), loss = 0.000569539
I0814 19:48:04.053231 24732 solver.cpp:334]     Train net output #0: loss = 0.000569371 (* 1 = 0.000569371 loss)
I0814 19:48:04.053267 24732 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0814 19:48:05.686116 24732 solver.cpp:312] Iteration 55700 (61.2378 iter/s, 1.63298s/100 iter), loss = 0.000291419
I0814 19:48:05.686139 24732 solver.cpp:334]     Train net output #0: loss = 0.000291249 (* 1 = 0.000291249 loss)
I0814 19:48:05.686144 24732 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0814 19:48:07.356180 24732 solver.cpp:312] Iteration 55800 (59.8798 iter/s, 1.67001s/100 iter), loss = 0.0015388
I0814 19:48:07.356205 24732 solver.cpp:334]     Train net output #0: loss = 0.00153863 (* 1 = 0.00153863 loss)
I0814 19:48:07.356211 24732 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0814 19:48:09.024989 24732 solver.cpp:312] Iteration 55900 (59.9248 iter/s, 1.66876s/100 iter), loss = 0.000442932
I0814 19:48:09.025012 24732 solver.cpp:334]     Train net output #0: loss = 0.000442761 (* 1 = 0.000442761 loss)
I0814 19:48:09.025017 24732 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0814 19:48:10.624259 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:10.625927 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:10.625937 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:10.625944 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:10.625948 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:10.625952 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:10.625954 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:10.625965 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:10.625977 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:10.625984 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:10.625993 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:10.626000 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:10.626009 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:10.626019 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:10.626036 24732 solver.cpp:509] Iteration 56000, Testing net (#0)
I0814 19:48:11.441112 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917942
I0814 19:48:11.441131 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 19:48:11.441136 24732 solver.cpp:594]     Test net output #2: loss = 0.342924 (* 1 = 0.342924 loss)
I0814 19:48:11.441150 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815087s
I0814 19:48:11.457454 24732 solver.cpp:312] Iteration 56000 (41.1118 iter/s, 2.43239s/100 iter), loss = 0.00036006
I0814 19:48:11.457470 24732 solver.cpp:334]     Train net output #0: loss = 0.000359889 (* 1 = 0.000359889 loss)
I0814 19:48:11.457476 24732 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0814 19:48:12.395932 24716 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:48:13.123493 24732 solver.cpp:312] Iteration 56100 (60.0246 iter/s, 1.66598s/100 iter), loss = 0.00147961
I0814 19:48:13.123517 24732 solver.cpp:334]     Train net output #0: loss = 0.00147944 (* 1 = 0.00147944 loss)
I0814 19:48:13.123523 24732 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0814 19:48:14.772747 24732 solver.cpp:312] Iteration 56200 (60.6355 iter/s, 1.6492s/100 iter), loss = 0.00152217
I0814 19:48:14.772776 24732 solver.cpp:334]     Train net output #0: loss = 0.001522 (* 1 = 0.001522 loss)
I0814 19:48:14.772783 24732 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0814 19:48:16.403604 24732 solver.cpp:312] Iteration 56300 (61.3193 iter/s, 1.63081s/100 iter), loss = 0.000724058
I0814 19:48:16.403657 24732 solver.cpp:334]     Train net output #0: loss = 0.000723888 (* 1 = 0.000723888 loss)
I0814 19:48:16.403671 24732 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0814 19:48:18.049775 24732 solver.cpp:312] Iteration 56400 (60.7489 iter/s, 1.64612s/100 iter), loss = 0.00254141
I0814 19:48:18.049938 24732 solver.cpp:334]     Train net output #0: loss = 0.00254124 (* 1 = 0.00254124 loss)
I0814 19:48:18.049976 24732 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0814 19:48:19.721666 24732 solver.cpp:312] Iteration 56500 (59.8145 iter/s, 1.67184s/100 iter), loss = 0.00216156
I0814 19:48:19.721688 24732 solver.cpp:334]     Train net output #0: loss = 0.00216139 (* 1 = 0.00216139 loss)
I0814 19:48:19.721693 24732 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0814 19:48:21.347795 24732 solver.cpp:312] Iteration 56600 (61.4977 iter/s, 1.62608s/100 iter), loss = 0.00169523
I0814 19:48:21.347893 24732 solver.cpp:334]     Train net output #0: loss = 0.00169506 (* 1 = 0.00169506 loss)
I0814 19:48:21.347909 24732 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0814 19:48:22.968783 24732 solver.cpp:312] Iteration 56700 (61.6927 iter/s, 1.62094s/100 iter), loss = 0.000410143
I0814 19:48:22.968835 24732 solver.cpp:334]     Train net output #0: loss = 0.000409973 (* 1 = 0.000409973 loss)
I0814 19:48:22.968849 24732 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0814 19:48:24.634289 24732 solver.cpp:312] Iteration 56800 (60.0438 iter/s, 1.66545s/100 iter), loss = 0.000460702
I0814 19:48:24.634315 24732 solver.cpp:334]     Train net output #0: loss = 0.000460532 (* 1 = 0.000460532 loss)
I0814 19:48:24.634320 24732 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0814 19:48:26.263267 24732 solver.cpp:312] Iteration 56900 (61.3902 iter/s, 1.62893s/100 iter), loss = 0.000387435
I0814 19:48:26.263290 24732 solver.cpp:334]     Train net output #0: loss = 0.000387265 (* 1 = 0.000387265 loss)
I0814 19:48:26.263296 24732 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0814 19:48:27.871809 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:27.873345 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:27.873354 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:27.873363 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:27.873366 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:27.873379 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:27.873389 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:27.873397 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:27.873402 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:27.873406 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:27.873414 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:27.873422 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:27.873427 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:27.873435 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:27.873452 24732 solver.cpp:509] Iteration 57000, Testing net (#0)
I0814 19:48:28.702396 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.913824
I0814 19:48:28.702414 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:48:28.702419 24732 solver.cpp:594]     Test net output #2: loss = 0.356537 (* 1 = 0.356537 loss)
I0814 19:48:28.702433 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.828954s
I0814 19:48:28.718617 24732 solver.cpp:312] Iteration 57000 (40.7286 iter/s, 2.45528s/100 iter), loss = 0.000756286
I0814 19:48:28.718644 24732 solver.cpp:334]     Train net output #0: loss = 0.000756116 (* 1 = 0.000756116 loss)
I0814 19:48:28.718648 24732 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0814 19:48:30.344925 24732 solver.cpp:312] Iteration 57100 (61.4909 iter/s, 1.62626s/100 iter), loss = 0.00110375
I0814 19:48:30.344950 24732 solver.cpp:334]     Train net output #0: loss = 0.00110358 (* 1 = 0.00110358 loss)
I0814 19:48:30.344956 24732 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0814 19:48:31.963325 24732 solver.cpp:312] Iteration 57200 (61.7914 iter/s, 1.61835s/100 iter), loss = 0.00184722
I0814 19:48:31.963349 24732 solver.cpp:334]     Train net output #0: loss = 0.00184705 (* 1 = 0.00184705 loss)
I0814 19:48:31.963356 24732 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0814 19:48:33.585654 24732 solver.cpp:312] Iteration 57300 (61.6417 iter/s, 1.62228s/100 iter), loss = 0.00252326
I0814 19:48:33.585717 24732 solver.cpp:334]     Train net output #0: loss = 0.00252308 (* 1 = 0.00252308 loss)
I0814 19:48:33.585737 24732 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0814 19:48:35.215507 24732 solver.cpp:312] Iteration 57400 (61.3573 iter/s, 1.6298s/100 iter), loss = 0.000894318
I0814 19:48:35.215530 24732 solver.cpp:334]     Train net output #0: loss = 0.000894147 (* 1 = 0.000894147 loss)
I0814 19:48:35.215561 24732 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0814 19:48:36.879954 24732 solver.cpp:312] Iteration 57500 (60.082 iter/s, 1.66439s/100 iter), loss = 0.000937822
I0814 19:48:36.879987 24732 solver.cpp:334]     Train net output #0: loss = 0.000937651 (* 1 = 0.000937651 loss)
I0814 19:48:36.879994 24732 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0814 19:48:38.517138 24732 solver.cpp:312] Iteration 57600 (61.0824 iter/s, 1.63713s/100 iter), loss = 0.00163974
I0814 19:48:38.517163 24732 solver.cpp:334]     Train net output #0: loss = 0.00163956 (* 1 = 0.00163956 loss)
I0814 19:48:38.517168 24732 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0814 19:48:40.184813 24732 solver.cpp:312] Iteration 57700 (59.9656 iter/s, 1.66762s/100 iter), loss = 0.000532953
I0814 19:48:40.184839 24732 solver.cpp:334]     Train net output #0: loss = 0.00053278 (* 1 = 0.00053278 loss)
I0814 19:48:40.184844 24732 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0814 19:48:41.850992 24732 solver.cpp:312] Iteration 57800 (60.0194 iter/s, 1.66613s/100 iter), loss = 0.000564468
I0814 19:48:41.851233 24732 solver.cpp:334]     Train net output #0: loss = 0.000564295 (* 1 = 0.000564295 loss)
I0814 19:48:41.851238 24732 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0814 19:48:43.519412 24732 solver.cpp:312] Iteration 57900 (59.9389 iter/s, 1.66837s/100 iter), loss = 0.000786859
I0814 19:48:43.519474 24732 solver.cpp:334]     Train net output #0: loss = 0.000786685 (* 1 = 0.000786685 loss)
I0814 19:48:43.519495 24732 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0814 19:48:45.173496 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:45.175148 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:45.175158 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:45.175165 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:45.175169 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:45.175179 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:45.175189 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:45.175197 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:45.175206 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:45.175215 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:45.175222 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:45.175231 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:45.175240 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:45.175251 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:45.175267 24732 solver.cpp:509] Iteration 58000, Testing net (#0)
I0814 19:48:45.995615 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907354
I0814 19:48:45.995632 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:48:45.995640 24732 solver.cpp:594]     Test net output #2: loss = 0.397232 (* 1 = 0.397232 loss)
I0814 19:48:45.995761 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82036s
I0814 19:48:46.011548 24732 solver.cpp:312] Iteration 58000 (40.1274 iter/s, 2.49206s/100 iter), loss = 0.00265861
I0814 19:48:46.011564 24732 solver.cpp:334]     Train net output #0: loss = 0.00265843 (* 1 = 0.00265843 loss)
I0814 19:48:46.011569 24732 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0814 19:48:47.663524 24732 solver.cpp:312] Iteration 58100 (60.5356 iter/s, 1.65192s/100 iter), loss = 0.00976163
I0814 19:48:47.663555 24732 solver.cpp:334]     Train net output #0: loss = 0.00976146 (* 1 = 0.00976146 loss)
I0814 19:48:47.663563 24732 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0814 19:48:49.290735 24732 solver.cpp:312] Iteration 58200 (61.4569 iter/s, 1.62716s/100 iter), loss = 0.00060559
I0814 19:48:49.290802 24732 solver.cpp:334]     Train net output #0: loss = 0.000605417 (* 1 = 0.000605417 loss)
I0814 19:48:49.290822 24732 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0814 19:48:50.924865 24732 solver.cpp:312] Iteration 58300 (61.1966 iter/s, 1.63408s/100 iter), loss = 0.000936027
I0814 19:48:50.924893 24732 solver.cpp:334]     Train net output #0: loss = 0.000935853 (* 1 = 0.000935853 loss)
I0814 19:48:50.924901 24732 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0814 19:48:52.615259 24732 solver.cpp:312] Iteration 58400 (59.1596 iter/s, 1.69034s/100 iter), loss = 0.000307712
I0814 19:48:52.615329 24732 solver.cpp:334]     Train net output #0: loss = 0.000307538 (* 1 = 0.000307538 loss)
I0814 19:48:52.615334 24732 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0814 19:48:54.279206 24732 solver.cpp:312] Iteration 58500 (60.0999 iter/s, 1.6639s/100 iter), loss = 0.00497031
I0814 19:48:54.279266 24732 solver.cpp:334]     Train net output #0: loss = 0.00497014 (* 1 = 0.00497014 loss)
I0814 19:48:54.279284 24732 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0814 19:48:55.901024 24732 solver.cpp:312] Iteration 58600 (61.6611 iter/s, 1.62177s/100 iter), loss = 0.000380038
I0814 19:48:55.901075 24732 solver.cpp:334]     Train net output #0: loss = 0.000379865 (* 1 = 0.000379865 loss)
I0814 19:48:55.901089 24732 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0814 19:48:57.562435 24732 solver.cpp:312] Iteration 58700 (60.1918 iter/s, 1.66136s/100 iter), loss = 0.000670551
I0814 19:48:57.562466 24732 solver.cpp:334]     Train net output #0: loss = 0.000670378 (* 1 = 0.000670378 loss)
I0814 19:48:57.562472 24732 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0814 19:48:59.216187 24732 solver.cpp:312] Iteration 58800 (60.4704 iter/s, 1.6537s/100 iter), loss = 0.00192245
I0814 19:48:59.216215 24732 solver.cpp:334]     Train net output #0: loss = 0.00192227 (* 1 = 0.00192227 loss)
I0814 19:48:59.216222 24732 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0814 19:49:00.892971 24732 solver.cpp:312] Iteration 58900 (59.6398 iter/s, 1.67673s/100 iter), loss = 0.00160144
I0814 19:49:00.893020 24732 solver.cpp:334]     Train net output #0: loss = 0.00160127 (* 1 = 0.00160127 loss)
I0814 19:49:00.893035 24732 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0814 19:49:02.492782 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:02.494509 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:02.494518 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:02.494524 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:02.494525 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:02.494527 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:02.494529 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:02.494531 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:02.494534 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:02.494535 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:02.494539 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:02.494542 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:02.494546 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:02.494549 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:02.494560 24732 solver.cpp:509] Iteration 59000, Testing net (#0)
I0814 19:49:03.319313 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907354
I0814 19:49:03.319331 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:49:03.319336 24732 solver.cpp:594]     Test net output #2: loss = 0.385154 (* 1 = 0.385154 loss)
I0814 19:49:03.319351 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824763s
I0814 19:49:03.335222 24732 solver.cpp:312] Iteration 59000 (40.9471 iter/s, 2.44218s/100 iter), loss = 0.00155257
I0814 19:49:03.335238 24732 solver.cpp:334]     Train net output #0: loss = 0.0015524 (* 1 = 0.0015524 loss)
I0814 19:49:03.335243 24732 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0814 19:49:04.977433 24732 solver.cpp:312] Iteration 59100 (60.8955 iter/s, 1.64216s/100 iter), loss = 0.00065454
I0814 19:49:04.977675 24732 solver.cpp:334]     Train net output #0: loss = 0.000654368 (* 1 = 0.000654368 loss)
I0814 19:49:04.977679 24732 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0814 19:49:06.622251 24732 solver.cpp:312] Iteration 59200 (60.799 iter/s, 1.64476s/100 iter), loss = 0.00146386
I0814 19:49:06.622298 24732 solver.cpp:334]     Train net output #0: loss = 0.00146369 (* 1 = 0.00146369 loss)
I0814 19:49:06.622325 24732 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0814 19:49:08.287073 24732 solver.cpp:312] Iteration 59300 (60.0683 iter/s, 1.66477s/100 iter), loss = 0.000430182
I0814 19:49:08.287097 24732 solver.cpp:334]     Train net output #0: loss = 0.00043001 (* 1 = 0.00043001 loss)
I0814 19:49:08.287101 24732 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0814 19:49:09.972558 24732 solver.cpp:312] Iteration 59400 (59.332 iter/s, 1.68543s/100 iter), loss = 0.00262515
I0814 19:49:09.972618 24732 solver.cpp:334]     Train net output #0: loss = 0.00262498 (* 1 = 0.00262498 loss)
I0814 19:49:09.972635 24732 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0814 19:49:11.654566 24732 solver.cpp:312] Iteration 59500 (59.4547 iter/s, 1.68195s/100 iter), loss = 0.000638608
I0814 19:49:11.654589 24732 solver.cpp:334]     Train net output #0: loss = 0.000638436 (* 1 = 0.000638436 loss)
I0814 19:49:11.654594 24732 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0814 19:49:13.286159 24732 solver.cpp:312] Iteration 59600 (61.2918 iter/s, 1.63154s/100 iter), loss = 0.00144413
I0814 19:49:13.286206 24732 solver.cpp:334]     Train net output #0: loss = 0.00144396 (* 1 = 0.00144396 loss)
I0814 19:49:13.286221 24732 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0814 19:49:14.968873 24732 solver.cpp:312] Iteration 59700 (59.4297 iter/s, 1.68266s/100 iter), loss = 0.00114086
I0814 19:49:14.968899 24732 solver.cpp:334]     Train net output #0: loss = 0.00114069 (* 1 = 0.00114069 loss)
I0814 19:49:14.968905 24732 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0814 19:49:16.603477 24732 solver.cpp:312] Iteration 59800 (61.1789 iter/s, 1.63455s/100 iter), loss = 0.00365009
I0814 19:49:16.603505 24732 solver.cpp:334]     Train net output #0: loss = 0.00364992 (* 1 = 0.00364992 loss)
I0814 19:49:16.603513 24732 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0814 19:49:18.276494 24732 solver.cpp:312] Iteration 59900 (59.7741 iter/s, 1.67297s/100 iter), loss = 0.000228775
I0814 19:49:18.276571 24732 solver.cpp:334]     Train net output #0: loss = 0.000228603 (* 1 = 0.000228603 loss)
I0814 19:49:18.276592 24732 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0814 19:49:19.907140 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0814 19:49:19.915235 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0814 19:49:19.918814 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:19.920730 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:19.920742 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:19.920748 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:19.920753 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:19.920763 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:19.920773 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:19.920781 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:19.920789 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:19.920797 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:19.920805 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:19.920815 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:19.920825 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:19.920833 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:19.920851 24732 solver.cpp:509] Iteration 60000, Testing net (#0)
I0814 19:49:20.729743 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905295
I0814 19:49:20.729759 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:49:20.729764 24732 solver.cpp:594]     Test net output #2: loss = 0.398874 (* 1 = 0.398874 loss)
I0814 19:49:20.729802 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.808922s
I0814 19:49:20.745714 24732 solver.cpp:312] Iteration 60000 (40.4998 iter/s, 2.46915s/100 iter), loss = 0.00259431
I0814 19:49:20.745730 24732 solver.cpp:334]     Train net output #0: loss = 0.00259414 (* 1 = 0.00259414 loss)
I0814 19:49:20.745736 24732 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0814 19:49:22.410923 24732 solver.cpp:312] Iteration 60100 (60.0545 iter/s, 1.66515s/100 iter), loss = 0.00137172
I0814 19:49:22.410950 24732 solver.cpp:334]     Train net output #0: loss = 0.00137154 (* 1 = 0.00137154 loss)
I0814 19:49:22.410956 24732 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0814 19:49:24.044682 24732 solver.cpp:312] Iteration 60200 (61.2104 iter/s, 1.63371s/100 iter), loss = 0.000442115
I0814 19:49:24.044766 24732 solver.cpp:334]     Train net output #0: loss = 0.000441942 (* 1 = 0.000441942 loss)
I0814 19:49:24.044775 24732 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0814 19:49:25.691916 24732 solver.cpp:312] Iteration 60300 (60.7098 iter/s, 1.64718s/100 iter), loss = 0.00176699
I0814 19:49:25.691943 24732 solver.cpp:334]     Train net output #0: loss = 0.00176682 (* 1 = 0.00176682 loss)
I0814 19:49:25.691949 24732 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0814 19:49:27.350850 24732 solver.cpp:312] Iteration 60400 (60.2817 iter/s, 1.65888s/100 iter), loss = 0.000311345
I0814 19:49:27.350877 24732 solver.cpp:334]     Train net output #0: loss = 0.000311172 (* 1 = 0.000311172 loss)
I0814 19:49:27.350883 24732 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0814 19:49:28.989075 24732 solver.cpp:312] Iteration 60500 (61.0436 iter/s, 1.63817s/100 iter), loss = 0.00100377
I0814 19:49:28.989100 24732 solver.cpp:334]     Train net output #0: loss = 0.0010036 (* 1 = 0.0010036 loss)
I0814 19:49:28.989105 24732 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0814 19:49:30.625948 24732 solver.cpp:312] Iteration 60600 (61.0939 iter/s, 1.63682s/100 iter), loss = 0.00261663
I0814 19:49:30.625973 24732 solver.cpp:334]     Train net output #0: loss = 0.00261646 (* 1 = 0.00261646 loss)
I0814 19:49:30.625979 24732 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0814 19:49:30.674412 24716 data_reader.cpp:288] Starting prefetch of epoch 8
I0814 19:49:32.297677 24732 solver.cpp:312] Iteration 60700 (59.8202 iter/s, 1.67168s/100 iter), loss = 0.00090154
I0814 19:49:32.297727 24732 solver.cpp:334]     Train net output #0: loss = 0.000901367 (* 1 = 0.000901367 loss)
I0814 19:49:32.297741 24732 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0814 19:49:33.992772 24732 solver.cpp:312] Iteration 60800 (58.9956 iter/s, 1.69504s/100 iter), loss = 0.00182894
I0814 19:49:33.992802 24732 solver.cpp:334]     Train net output #0: loss = 0.00182877 (* 1 = 0.00182877 loss)
I0814 19:49:33.992810 24732 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0814 19:49:35.642485 24732 solver.cpp:312] Iteration 60900 (60.6185 iter/s, 1.64966s/100 iter), loss = 0.00158305
I0814 19:49:35.642510 24732 solver.cpp:334]     Train net output #0: loss = 0.00158288 (* 1 = 0.00158288 loss)
I0814 19:49:35.642515 24732 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0814 19:49:37.268610 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:37.270051 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:37.270062 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:37.270069 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:37.270073 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:37.270078 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:37.270082 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:37.270087 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:37.270090 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:37.270094 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:37.270098 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:37.270102 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:37.270107 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:37.270109 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:37.270119 24732 solver.cpp:509] Iteration 61000, Testing net (#0)
I0814 19:49:38.096077 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906472
I0814 19:49:38.096094 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:49:38.096099 24732 solver.cpp:594]     Test net output #2: loss = 0.40549 (* 1 = 0.40549 loss)
I0814 19:49:38.096117 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825969s
I0814 19:49:38.112051 24732 solver.cpp:312] Iteration 61000 (40.4941 iter/s, 2.46949s/100 iter), loss = 0.00127942
I0814 19:49:38.112083 24732 solver.cpp:334]     Train net output #0: loss = 0.00127924 (* 1 = 0.00127924 loss)
I0814 19:49:38.112089 24732 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0814 19:49:39.762760 24732 solver.cpp:312] Iteration 61100 (60.5819 iter/s, 1.65066s/100 iter), loss = 0.000589503
I0814 19:49:39.762781 24732 solver.cpp:334]     Train net output #0: loss = 0.000589332 (* 1 = 0.000589332 loss)
I0814 19:49:39.762787 24732 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0814 19:49:41.416812 24732 solver.cpp:312] Iteration 61200 (60.4596 iter/s, 1.654s/100 iter), loss = 0.00049441
I0814 19:49:41.416836 24732 solver.cpp:334]     Train net output #0: loss = 0.000494238 (* 1 = 0.000494238 loss)
I0814 19:49:41.416842 24732 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0814 19:49:43.094019 24732 solver.cpp:312] Iteration 61300 (59.6248 iter/s, 1.67715s/100 iter), loss = 0.000615211
I0814 19:49:43.094120 24732 solver.cpp:334]     Train net output #0: loss = 0.00061504 (* 1 = 0.00061504 loss)
I0814 19:49:43.094130 24732 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0814 19:49:44.740592 24732 solver.cpp:312] Iteration 61400 (60.7342 iter/s, 1.64652s/100 iter), loss = 0.00192736
I0814 19:49:44.740645 24732 solver.cpp:334]     Train net output #0: loss = 0.00192719 (* 1 = 0.00192719 loss)
I0814 19:49:44.740661 24732 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0814 19:49:46.397081 24732 solver.cpp:312] Iteration 61500 (60.3706 iter/s, 1.65644s/100 iter), loss = 0.00476211
I0814 19:49:46.397105 24732 solver.cpp:334]     Train net output #0: loss = 0.00476194 (* 1 = 0.00476194 loss)
I0814 19:49:46.397111 24732 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0814 19:49:48.028344 24732 solver.cpp:312] Iteration 61600 (61.3041 iter/s, 1.63121s/100 iter), loss = 0.00113627
I0814 19:49:48.028370 24732 solver.cpp:334]     Train net output #0: loss = 0.0011361 (* 1 = 0.0011361 loss)
I0814 19:49:48.028375 24732 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0814 19:49:49.669893 24732 solver.cpp:312] Iteration 61700 (60.9201 iter/s, 1.64149s/100 iter), loss = 0.000799573
I0814 19:49:49.669917 24732 solver.cpp:334]     Train net output #0: loss = 0.000799401 (* 1 = 0.000799401 loss)
I0814 19:49:49.669922 24732 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0814 19:49:51.330533 24732 solver.cpp:312] Iteration 61800 (60.2196 iter/s, 1.66059s/100 iter), loss = 0.000448428
I0814 19:49:51.330603 24732 solver.cpp:334]     Train net output #0: loss = 0.000448256 (* 1 = 0.000448256 loss)
I0814 19:49:51.330624 24732 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0814 19:49:52.993844 24732 solver.cpp:312] Iteration 61900 (60.1229 iter/s, 1.66326s/100 iter), loss = 0.000887192
I0814 19:49:52.993894 24732 solver.cpp:334]     Train net output #0: loss = 0.00088702 (* 1 = 0.00088702 loss)
I0814 19:49:52.993908 24732 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0814 19:49:54.636909 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:54.638463 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:54.638471 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:54.638481 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:54.638485 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:54.638489 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:54.638492 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:54.638497 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:54.638500 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:54.638505 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:54.638509 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:54.638514 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:54.638516 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:54.638521 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:54.638532 24732 solver.cpp:509] Iteration 62000, Testing net (#0)
I0814 19:49:55.476330 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906178
I0814 19:49:55.476348 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995294
I0814 19:49:55.476354 24732 solver.cpp:594]     Test net output #2: loss = 0.392083 (* 1 = 0.392083 loss)
I0814 19:49:55.476372 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.837811s
I0814 19:49:55.494695 24732 solver.cpp:312] Iteration 62000 (39.9876 iter/s, 2.50078s/100 iter), loss = 0.000428321
I0814 19:49:55.494711 24732 solver.cpp:334]     Train net output #0: loss = 0.000428148 (* 1 = 0.000428148 loss)
I0814 19:49:55.494716 24732 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0814 19:49:57.144325 24732 solver.cpp:312] Iteration 62100 (60.6217 iter/s, 1.64958s/100 iter), loss = 0.000326858
I0814 19:49:57.144372 24732 solver.cpp:334]     Train net output #0: loss = 0.000326687 (* 1 = 0.000326687 loss)
I0814 19:49:57.144385 24732 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0814 19:49:58.779141 24732 solver.cpp:312] Iteration 62200 (61.1709 iter/s, 1.63476s/100 iter), loss = 0.00236271
I0814 19:49:58.779189 24732 solver.cpp:334]     Train net output #0: loss = 0.00236254 (* 1 = 0.00236254 loss)
I0814 19:49:58.779204 24732 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0814 19:50:00.445168 24732 solver.cpp:312] Iteration 62300 (60.025 iter/s, 1.66597s/100 iter), loss = 0.00117782
I0814 19:50:00.445217 24732 solver.cpp:334]     Train net output #0: loss = 0.00117765 (* 1 = 0.00117765 loss)
I0814 19:50:00.445230 24732 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0814 19:50:02.125080 24732 solver.cpp:312] Iteration 62400 (59.5288 iter/s, 1.67986s/100 iter), loss = 0.000552193
I0814 19:50:02.125103 24732 solver.cpp:334]     Train net output #0: loss = 0.000552023 (* 1 = 0.000552023 loss)
I0814 19:50:02.125108 24732 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0814 19:50:03.789629 24732 solver.cpp:312] Iteration 62500 (60.0782 iter/s, 1.6645s/100 iter), loss = 0.000838799
I0814 19:50:03.789652 24732 solver.cpp:334]     Train net output #0: loss = 0.000838628 (* 1 = 0.000838628 loss)
I0814 19:50:03.789656 24732 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0814 19:50:05.468312 24732 solver.cpp:312] Iteration 62600 (59.5724 iter/s, 1.67863s/100 iter), loss = 0.0007496
I0814 19:50:05.468335 24732 solver.cpp:334]     Train net output #0: loss = 0.00074943 (* 1 = 0.00074943 loss)
I0814 19:50:05.468339 24732 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0814 19:50:07.097107 24732 solver.cpp:312] Iteration 62700 (61.397 iter/s, 1.62874s/100 iter), loss = 0.00127176
I0814 19:50:07.097134 24732 solver.cpp:334]     Train net output #0: loss = 0.00127159 (* 1 = 0.00127159 loss)
I0814 19:50:07.097141 24732 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0814 19:50:08.747892 24732 solver.cpp:312] Iteration 62800 (60.5891 iter/s, 1.65046s/100 iter), loss = 0.00196349
I0814 19:50:08.747949 24732 solver.cpp:334]     Train net output #0: loss = 0.00196332 (* 1 = 0.00196332 loss)
I0814 19:50:08.748013 24732 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0814 19:50:10.382938 24732 solver.cpp:312] Iteration 62900 (61.1621 iter/s, 1.635s/100 iter), loss = 0.000941342
I0814 19:50:10.382967 24732 solver.cpp:334]     Train net output #0: loss = 0.000941172 (* 1 = 0.000941172 loss)
I0814 19:50:10.382973 24732 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0814 19:50:12.064223 24732 solver.cpp:363] Sparsity after update:
I0814 19:50:12.065891 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:50:12.065901 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:50:12.065908 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:50:12.065920 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:50:12.065930 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:50:12.065940 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:50:12.065948 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:50:12.065956 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:50:12.065965 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:50:12.065974 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:50:12.065984 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:50:12.065992 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:50:12.066001 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:50:12.066020 24732 solver.cpp:509] Iteration 63000, Testing net (#0)
I0814 19:50:12.876919 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906178
I0814 19:50:12.876938 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:50:12.876943 24732 solver.cpp:594]     Test net output #2: loss = 0.391574 (* 1 = 0.391574 loss)
I0814 19:50:12.876960 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810913s
I0814 19:50:12.892778 24732 solver.cpp:312] Iteration 63000 (39.8443 iter/s, 2.50977s/100 iter), loss = 0.00362886
I0814 19:50:12.892794 24732 solver.cpp:334]     Train net output #0: loss = 0.0036287 (* 1 = 0.0036287 loss)
I0814 19:50:12.892801 24732 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0814 19:50:14.549154 24732 solver.cpp:312] Iteration 63100 (60.3747 iter/s, 1.65632s/100 iter), loss = 0.00053736
I0814 19:50:14.549235 24732 solver.cpp:334]     Train net output #0: loss = 0.000537191 (* 1 = 0.000537191 loss)
I0814 19:50:14.549243 24732 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0814 19:50:16.213310 24732 solver.cpp:312] Iteration 63200 (60.0925 iter/s, 1.6641s/100 iter), loss = 0.00127944
I0814 19:50:16.213340 24732 solver.cpp:334]     Train net output #0: loss = 0.00127927 (* 1 = 0.00127927 loss)
I0814 19:50:16.213574 24732 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0814 19:50:17.854145 24732 solver.cpp:312] Iteration 63300 (60.9465 iter/s, 1.64078s/100 iter), loss = 0.000680574
I0814 19:50:17.854173 24732 solver.cpp:334]     Train net output #0: loss = 0.000680404 (* 1 = 0.000680404 loss)
I0814 19:50:17.854180 24732 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0814 19:50:19.486371 24732 solver.cpp:312] Iteration 63400 (61.2679 iter/s, 1.63218s/100 iter), loss = 0.000565066
I0814 19:50:19.486420 24732 solver.cpp:334]     Train net output #0: loss = 0.000564896 (* 1 = 0.000564896 loss)
I0814 19:50:19.486434 24732 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0814 19:50:21.137411 24732 solver.cpp:312] Iteration 63500 (60.5699 iter/s, 1.65099s/100 iter), loss = 0.000795725
I0814 19:50:21.137467 24732 solver.cpp:334]     Train net output #0: loss = 0.000795555 (* 1 = 0.000795555 loss)
I0814 19:50:21.137483 24732 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0814 19:50:22.849174 24732 solver.cpp:312] Iteration 63600 (58.4211 iter/s, 1.71171s/100 iter), loss = 0.000704929
I0814 19:50:22.849200 24732 solver.cpp:334]     Train net output #0: loss = 0.000704759 (* 1 = 0.000704759 loss)
I0814 19:50:22.849223 24732 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0814 19:50:24.484115 24732 solver.cpp:312] Iteration 63700 (61.1662 iter/s, 1.63489s/100 iter), loss = 0.000765479
I0814 19:50:24.484150 24732 solver.cpp:334]     Train net output #0: loss = 0.000765308 (* 1 = 0.000765308 loss)
I0814 19:50:24.484158 24732 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0814 19:50:26.169351 24732 solver.cpp:312] Iteration 63800 (59.3407 iter/s, 1.68518s/100 iter), loss = 0.00073873
I0814 19:50:26.169436 24732 solver.cpp:334]     Train net output #0: loss = 0.000738558 (* 1 = 0.000738558 loss)
I0814 19:50:26.169443 24732 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0814 19:50:27.815717 24732 solver.cpp:312] Iteration 63900 (60.7418 iter/s, 1.64631s/100 iter), loss = 0.00074093
I0814 19:50:27.815740 24732 solver.cpp:334]     Train net output #0: loss = 0.000740759 (* 1 = 0.000740759 loss)
I0814 19:50:27.815745 24732 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0814 19:50:29.440295 24732 solver.cpp:312] Iteration 63999 (60.9408 iter/s, 1.62453s/99 iter), loss = 0.000398853
I0814 19:50:29.440321 24732 solver.cpp:334]     Train net output #0: loss = 0.000398682 (* 1 = 0.000398682 loss)
I0814 19:50:29.440327 24732 solver.cpp:363] Sparsity after update:
I0814 19:50:29.442262 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:50:29.442270 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:50:29.442276 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:50:29.442278 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:50:29.442289 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:50:29.442296 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:50:29.442303 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:50:29.442308 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:50:29.442312 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:50:29.442320 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:50:29.442324 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:50:29.442327 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:50:29.442335 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:50:29.442389 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 19:50:29.450320 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0814 19:50:29.458793 24732 solver.cpp:486] Iteration 64000, loss = 0.000476354
I0814 19:50:29.458817 24732 solver.cpp:509] Iteration 64000, Testing net (#0)
I0814 19:50:30.272869 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:50:30.272888 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:50:30.272894 24732 solver.cpp:594]     Test net output #2: loss = 0.417369 (* 1 = 0.417369 loss)
I0814 19:50:30.275840 24692 parallel.cpp:71] Root Solver performance on device 0: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275852 24692 parallel.cpp:76]      Solver performance on device 1: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275857 24692 parallel.cpp:76]      Solver performance on device 2: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275858 24692 parallel.cpp:79] Overall multi-GPU performance: 2055.51 img/sec
I0814 19:50:30.348296 24692 caffe.cpp:247] Optimization Done in 34m 18s
