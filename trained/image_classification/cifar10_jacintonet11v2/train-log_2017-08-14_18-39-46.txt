Logging output to training/cifar10_jacintonet11v2_2017-08-14_18-39-46/train-log_2017-08-14_18-39-46.txt
training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg
training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse
training/cifar10_jacintonet11v2_2017-08-14_18-39-46/test
training/cifar10_jacintonet11v2_2017-08-14_18-39-46/test_quantize
I0814 18:39:49.541561 10396 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 18:39:49 2017
I0814 18:39:49.541692 10396 caffe.cpp:611] CuDNN version: 6021
I0814 18:39:49.541695 10396 caffe.cpp:612] CuBLAS version: 8000
I0814 18:39:49.541697 10396 caffe.cpp:613] CUDA version: 8000
I0814 18:39:49.541699 10396 caffe.cpp:614] CUDA driver version: 8000
I0814 18:39:49.811841 10396 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 18:39:49.812417 10396 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 18:39:49.812942 10396 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0814 18:39:49.813460 10396 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0814 18:39:49.813469 10396 caffe.cpp:208] Using GPUs 0, 1, 2
I0814 18:39:49.813794 10396 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0814 18:39:49.814121 10396 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0814 18:39:49.814445 10396 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0814 18:39:49.814483 10396 solver.cpp:42] Solver data type: FLOAT
I0814 18:39:49.814515 10396 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
I0814 18:39:49.821110 10396 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/train.prototxt
I0814 18:39:49.821527 10396 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0814 18:39:49.821533 10396 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0814 18:39:49.821555 10396 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:39:49.821755 10396 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0814 18:39:49.821853 10396 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:49.821859 10396 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:49.821863 10396 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 18:39:49.821867 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:49.821923 10396 net.cpp:184] Created Layer data (0)
I0814 18:39:49.821929 10396 net.cpp:530] data -> data
I0814 18:39:49.821943 10396 net.cpp:530] data -> label
I0814 18:39:49.821965 10396 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:39:49.821980 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:39:49.822757 10436 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:39:49.823855 10396 data_layer.cpp:185] [0] ReshapePrefetch 22, 3, 32, 32
I0814 18:39:49.823918 10396 data_layer.cpp:209] [0] Output data size: 22, 3, 32, 32
I0814 18:39:49.823925 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:39:49.823942 10396 net.cpp:245] Setting up data
I0814 18:39:49.823951 10396 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0814 18:39:49.823958 10396 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0814 18:39:49.823966 10396 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 18:39:49.823971 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:49.823982 10396 net.cpp:184] Created Layer data/bias (1)
I0814 18:39:49.823987 10396 net.cpp:561] data/bias <- data
I0814 18:39:49.823994 10396 net.cpp:530] data/bias -> data/bias
I0814 18:39:49.825948 10396 net.cpp:245] Setting up data/bias
I0814 18:39:49.825959 10396 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0814 18:39:49.825969 10396 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 18:39:49.825974 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:49.825989 10396 net.cpp:184] Created Layer conv1a (2)
I0814 18:39:49.825994 10396 net.cpp:561] conv1a <- data/bias
I0814 18:39:49.825999 10396 net.cpp:530] conv1a -> conv1a
I0814 18:39:50.112067 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0814 18:39:50.112087 10396 net.cpp:245] Setting up conv1a
I0814 18:39:50.112094 10396 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0814 18:39:50.112105 10396 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 18:39:50.112112 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.112123 10396 net.cpp:184] Created Layer conv1a/bn (3)
I0814 18:39:50.112128 10396 net.cpp:561] conv1a/bn <- conv1a
I0814 18:39:50.112141 10396 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 18:39:50.112795 10396 net.cpp:245] Setting up conv1a/bn
I0814 18:39:50.112804 10396 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0814 18:39:50.112815 10396 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 18:39:50.112819 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.112826 10396 net.cpp:184] Created Layer conv1a/relu (4)
I0814 18:39:50.112830 10396 net.cpp:561] conv1a/relu <- conv1a
I0814 18:39:50.112834 10396 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 18:39:50.112850 10396 net.cpp:245] Setting up conv1a/relu
I0814 18:39:50.112855 10396 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0814 18:39:50.112859 10396 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 18:39:50.112862 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.112872 10396 net.cpp:184] Created Layer conv1b (5)
I0814 18:39:50.112876 10396 net.cpp:561] conv1b <- conv1a
I0814 18:39:50.112880 10396 net.cpp:530] conv1b -> conv1b
I0814 18:39:50.120013 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0814 18:39:50.120026 10396 net.cpp:245] Setting up conv1b
I0814 18:39:50.120033 10396 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0814 18:39:50.120041 10396 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 18:39:50.120045 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.120054 10396 net.cpp:184] Created Layer conv1b/bn (6)
I0814 18:39:50.120059 10396 net.cpp:561] conv1b/bn <- conv1b
I0814 18:39:50.120079 10396 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 18:39:50.120684 10396 net.cpp:245] Setting up conv1b/bn
I0814 18:39:50.120695 10396 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0814 18:39:50.120704 10396 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 18:39:50.120708 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.120714 10396 net.cpp:184] Created Layer conv1b/relu (7)
I0814 18:39:50.120718 10396 net.cpp:561] conv1b/relu <- conv1b
I0814 18:39:50.120723 10396 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 18:39:50.120728 10396 net.cpp:245] Setting up conv1b/relu
I0814 18:39:50.120733 10396 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0814 18:39:50.120736 10396 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 18:39:50.120740 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.120755 10396 net.cpp:184] Created Layer pool1 (8)
I0814 18:39:50.120759 10396 net.cpp:561] pool1 <- conv1b
I0814 18:39:50.120761 10396 net.cpp:530] pool1 -> pool1
I0814 18:39:50.120839 10396 net.cpp:245] Setting up pool1
I0814 18:39:50.120844 10396 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0814 18:39:50.120847 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 18:39:50.120849 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.120857 10396 net.cpp:184] Created Layer res2a_branch2a (9)
I0814 18:39:50.120859 10396 net.cpp:561] res2a_branch2a <- pool1
I0814 18:39:50.120862 10396 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 18:39:50.130894 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0814 18:39:50.130908 10396 net.cpp:245] Setting up res2a_branch2a
I0814 18:39:50.130911 10396 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0814 18:39:50.130918 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.130921 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.130926 10396 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0814 18:39:50.130928 10396 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 18:39:50.130930 10396 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 18:39:50.131548 10396 net.cpp:245] Setting up res2a_branch2a/bn
I0814 18:39:50.131556 10396 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0814 18:39:50.131562 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.131564 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.131569 10396 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0814 18:39:50.131572 10396 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 18:39:50.131573 10396 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 18:39:50.131577 10396 net.cpp:245] Setting up res2a_branch2a/relu
I0814 18:39:50.131580 10396 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0814 18:39:50.131584 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 18:39:50.131589 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.131600 10396 net.cpp:184] Created Layer res2a_branch2b (12)
I0814 18:39:50.131604 10396 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 18:39:50.131608 10396 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 18:39:50.138558 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0814 18:39:50.138571 10396 net.cpp:245] Setting up res2a_branch2b
I0814 18:39:50.138586 10396 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0814 18:39:50.138593 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.138599 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.138607 10396 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0814 18:39:50.138612 10396 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 18:39:50.138615 10396 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 18:39:50.139381 10396 net.cpp:245] Setting up res2a_branch2b/bn
I0814 18:39:50.139392 10396 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0814 18:39:50.139402 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.139407 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.139412 10396 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0814 18:39:50.139417 10396 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 18:39:50.139421 10396 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 18:39:50.139437 10396 net.cpp:245] Setting up res2a_branch2b/relu
I0814 18:39:50.139444 10396 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0814 18:39:50.139456 10396 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 18:39:50.139461 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.139467 10396 net.cpp:184] Created Layer pool2 (15)
I0814 18:39:50.139472 10396 net.cpp:561] pool2 <- res2a_branch2b
I0814 18:39:50.139477 10396 net.cpp:530] pool2 -> pool2
I0814 18:39:50.139561 10396 net.cpp:245] Setting up pool2
I0814 18:39:50.139569 10396 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0814 18:39:50.139580 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 18:39:50.139585 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.139595 10396 net.cpp:184] Created Layer res3a_branch2a (16)
I0814 18:39:50.139600 10396 net.cpp:561] res3a_branch2a <- pool2
I0814 18:39:50.139605 10396 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 18:39:50.151162 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.09G, req 0.01G)
I0814 18:39:50.151177 10396 net.cpp:245] Setting up res3a_branch2a
I0814 18:39:50.151182 10396 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0814 18:39:50.151188 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.151191 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.151198 10396 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0814 18:39:50.151201 10396 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 18:39:50.151204 10396 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 18:39:50.151818 10396 net.cpp:245] Setting up res3a_branch2a/bn
I0814 18:39:50.151826 10396 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0814 18:39:50.151835 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.151839 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.151842 10396 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0814 18:39:50.151845 10396 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 18:39:50.151847 10396 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 18:39:50.151852 10396 net.cpp:245] Setting up res3a_branch2a/relu
I0814 18:39:50.151855 10396 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0814 18:39:50.151859 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 18:39:50.151870 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.151878 10396 net.cpp:184] Created Layer res3a_branch2b (19)
I0814 18:39:50.151880 10396 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 18:39:50.151883 10396 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 18:39:50.156803 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0.01G)
I0814 18:39:50.156813 10396 net.cpp:245] Setting up res3a_branch2b
I0814 18:39:50.156817 10396 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0814 18:39:50.156821 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.156823 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.156827 10396 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0814 18:39:50.156829 10396 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 18:39:50.156833 10396 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 18:39:50.157418 10396 net.cpp:245] Setting up res3a_branch2b/bn
I0814 18:39:50.157424 10396 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0814 18:39:50.157429 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.157433 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.157436 10396 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0814 18:39:50.157438 10396 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 18:39:50.157440 10396 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 18:39:50.157444 10396 net.cpp:245] Setting up res3a_branch2b/relu
I0814 18:39:50.157447 10396 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0814 18:39:50.157449 10396 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 18:39:50.157452 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.157456 10396 net.cpp:184] Created Layer pool3 (22)
I0814 18:39:50.157459 10396 net.cpp:561] pool3 <- res3a_branch2b
I0814 18:39:50.157464 10396 net.cpp:530] pool3 -> pool3
I0814 18:39:50.157524 10396 net.cpp:245] Setting up pool3
I0814 18:39:50.157529 10396 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0814 18:39:50.157533 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 18:39:50.157537 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.157546 10396 net.cpp:184] Created Layer res4a_branch2a (23)
I0814 18:39:50.157549 10396 net.cpp:561] res4a_branch2a <- pool3
I0814 18:39:50.157553 10396 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 18:39:50.177336 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0.01G)
I0814 18:39:50.177356 10396 net.cpp:245] Setting up res4a_branch2a
I0814 18:39:50.177363 10396 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0814 18:39:50.177372 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.177377 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.177387 10396 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0814 18:39:50.177392 10396 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 18:39:50.177397 10396 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 18:39:50.178074 10396 net.cpp:245] Setting up res4a_branch2a/bn
I0814 18:39:50.178082 10396 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0814 18:39:50.178092 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.178103 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.178109 10396 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0814 18:39:50.178113 10396 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 18:39:50.178117 10396 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 18:39:50.178124 10396 net.cpp:245] Setting up res4a_branch2a/relu
I0814 18:39:50.178129 10396 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0814 18:39:50.178133 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 18:39:50.178138 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.178148 10396 net.cpp:184] Created Layer res4a_branch2b (26)
I0814 18:39:50.178151 10396 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 18:39:50.178155 10396 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 18:39:50.186637 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0.01G)
I0814 18:39:50.186650 10396 net.cpp:245] Setting up res4a_branch2b
I0814 18:39:50.186657 10396 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0814 18:39:50.186664 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.186668 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.186676 10396 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0814 18:39:50.186679 10396 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 18:39:50.186684 10396 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 18:39:50.187305 10396 net.cpp:245] Setting up res4a_branch2b/bn
I0814 18:39:50.187314 10396 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0814 18:39:50.187325 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.187328 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.187333 10396 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0814 18:39:50.187337 10396 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 18:39:50.187343 10396 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 18:39:50.187350 10396 net.cpp:245] Setting up res4a_branch2b/relu
I0814 18:39:50.187353 10396 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0814 18:39:50.187357 10396 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 18:39:50.187361 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.187367 10396 net.cpp:184] Created Layer pool4 (29)
I0814 18:39:50.187371 10396 net.cpp:561] pool4 <- res4a_branch2b
I0814 18:39:50.187376 10396 net.cpp:530] pool4 -> pool4
I0814 18:39:50.187436 10396 net.cpp:245] Setting up pool4
I0814 18:39:50.187441 10396 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0814 18:39:50.187445 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 18:39:50.187449 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.187458 10396 net.cpp:184] Created Layer res5a_branch2a (30)
I0814 18:39:50.187463 10396 net.cpp:561] res5a_branch2a <- pool4
I0814 18:39:50.187466 10396 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 18:39:50.230306 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0814 18:39:50.230324 10396 net.cpp:245] Setting up res5a_branch2a
I0814 18:39:50.230331 10396 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0814 18:39:50.230339 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.230343 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.230362 10396 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0814 18:39:50.230367 10396 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 18:39:50.230372 10396 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 18:39:50.231017 10396 net.cpp:245] Setting up res5a_branch2a/bn
I0814 18:39:50.231025 10396 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0814 18:39:50.231034 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.231039 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.231045 10396 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0814 18:39:50.231047 10396 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 18:39:50.231051 10396 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 18:39:50.231058 10396 net.cpp:245] Setting up res5a_branch2a/relu
I0814 18:39:50.231062 10396 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0814 18:39:50.231066 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 18:39:50.231070 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.231079 10396 net.cpp:184] Created Layer res5a_branch2b (33)
I0814 18:39:50.231083 10396 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 18:39:50.231087 10396 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 18:39:50.250644 10396 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0814 18:39:50.250661 10396 net.cpp:245] Setting up res5a_branch2b
I0814 18:39:50.250669 10396 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0814 18:39:50.250679 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.250684 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.250694 10396 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0814 18:39:50.250699 10396 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 18:39:50.250704 10396 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 18:39:50.251466 10396 net.cpp:245] Setting up res5a_branch2b/bn
I0814 18:39:50.251474 10396 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0814 18:39:50.251483 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.251487 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.251492 10396 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0814 18:39:50.251497 10396 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 18:39:50.251500 10396 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 18:39:50.251507 10396 net.cpp:245] Setting up res5a_branch2b/relu
I0814 18:39:50.251512 10396 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0814 18:39:50.251516 10396 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 18:39:50.251519 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.251525 10396 net.cpp:184] Created Layer pool5 (36)
I0814 18:39:50.251529 10396 net.cpp:561] pool5 <- res5a_branch2b
I0814 18:39:50.251534 10396 net.cpp:530] pool5 -> pool5
I0814 18:39:50.251567 10396 net.cpp:245] Setting up pool5
I0814 18:39:50.251574 10396 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0814 18:39:50.251579 10396 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 18:39:50.251582 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.251590 10396 net.cpp:184] Created Layer fc10 (37)
I0814 18:39:50.251595 10396 net.cpp:561] fc10 <- pool5
I0814 18:39:50.251598 10396 net.cpp:530] fc10 -> fc10
I0814 18:39:50.251883 10396 net.cpp:245] Setting up fc10
I0814 18:39:50.251890 10396 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0814 18:39:50.251896 10396 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 18:39:50.251900 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.251914 10396 net.cpp:184] Created Layer loss (38)
I0814 18:39:50.251917 10396 net.cpp:561] loss <- fc10
I0814 18:39:50.251921 10396 net.cpp:561] loss <- label
I0814 18:39:50.251926 10396 net.cpp:530] loss -> loss
I0814 18:39:50.252086 10396 net.cpp:245] Setting up loss
I0814 18:39:50.252094 10396 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0814 18:39:50.252097 10396 net.cpp:256]     with loss weight 1
I0814 18:39:50.252104 10396 net.cpp:323] loss needs backward computation.
I0814 18:39:50.252107 10396 net.cpp:323] fc10 needs backward computation.
I0814 18:39:50.252111 10396 net.cpp:323] pool5 needs backward computation.
I0814 18:39:50.252115 10396 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 18:39:50.252120 10396 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 18:39:50.252123 10396 net.cpp:323] res5a_branch2b needs backward computation.
I0814 18:39:50.252127 10396 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 18:39:50.252138 10396 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 18:39:50.252143 10396 net.cpp:323] res5a_branch2a needs backward computation.
I0814 18:39:50.252147 10396 net.cpp:323] pool4 needs backward computation.
I0814 18:39:50.252151 10396 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 18:39:50.252156 10396 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 18:39:50.252158 10396 net.cpp:323] res4a_branch2b needs backward computation.
I0814 18:39:50.252162 10396 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 18:39:50.252166 10396 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 18:39:50.252169 10396 net.cpp:323] res4a_branch2a needs backward computation.
I0814 18:39:50.252172 10396 net.cpp:323] pool3 needs backward computation.
I0814 18:39:50.252177 10396 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 18:39:50.252182 10396 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 18:39:50.252184 10396 net.cpp:323] res3a_branch2b needs backward computation.
I0814 18:39:50.252188 10396 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 18:39:50.252192 10396 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 18:39:50.252195 10396 net.cpp:323] res3a_branch2a needs backward computation.
I0814 18:39:50.252199 10396 net.cpp:323] pool2 needs backward computation.
I0814 18:39:50.252203 10396 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 18:39:50.252207 10396 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 18:39:50.252212 10396 net.cpp:323] res2a_branch2b needs backward computation.
I0814 18:39:50.252215 10396 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 18:39:50.252218 10396 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 18:39:50.252223 10396 net.cpp:323] res2a_branch2a needs backward computation.
I0814 18:39:50.252226 10396 net.cpp:323] pool1 needs backward computation.
I0814 18:39:50.252230 10396 net.cpp:323] conv1b/relu needs backward computation.
I0814 18:39:50.252233 10396 net.cpp:323] conv1b/bn needs backward computation.
I0814 18:39:50.252238 10396 net.cpp:323] conv1b needs backward computation.
I0814 18:39:50.252241 10396 net.cpp:323] conv1a/relu needs backward computation.
I0814 18:39:50.252245 10396 net.cpp:323] conv1a/bn needs backward computation.
I0814 18:39:50.252249 10396 net.cpp:323] conv1a needs backward computation.
I0814 18:39:50.252254 10396 net.cpp:325] data/bias does not need backward computation.
I0814 18:39:50.252259 10396 net.cpp:325] data does not need backward computation.
I0814 18:39:50.252261 10396 net.cpp:367] This network produces output loss
I0814 18:39:50.252296 10396 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0814 18:39:50.252300 10396 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0814 18:39:50.252303 10396 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0814 18:39:50.252307 10396 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0814 18:39:50.252312 10396 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0814 18:39:50.252315 10396 net.cpp:407] Network initialization done.
I0814 18:39:50.252663 10396 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/test.prototxt
W0814 18:39:50.252710 10396 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:39:50.252833 10396 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 18:39:50.252926 10396 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:50.252931 10396 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:50.252934 10396 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 18:39:50.252938 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.252950 10396 net.cpp:184] Created Layer data (0)
I0814 18:39:50.252954 10396 net.cpp:530] data -> data
I0814 18:39:50.252959 10396 net.cpp:530] data -> label
I0814 18:39:50.252969 10396 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:39:50.252976 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:39:50.253829 10438 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:39:50.253896 10396 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 32, 32
I0814 18:39:50.253962 10396 data_layer.cpp:209] (0) Output data size: 17, 3, 32, 32
I0814 18:39:50.253967 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:39:50.253981 10396 net.cpp:245] Setting up data
I0814 18:39:50.253986 10396 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0814 18:39:50.253989 10396 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0814 18:39:50.253993 10396 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 18:39:50.253996 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.254001 10396 net.cpp:184] Created Layer label_data_1_split (1)
I0814 18:39:50.254004 10396 net.cpp:561] label_data_1_split <- label
I0814 18:39:50.254007 10396 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 18:39:50.254016 10396 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 18:39:50.254019 10396 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 18:39:50.254076 10396 net.cpp:245] Setting up label_data_1_split
I0814 18:39:50.254081 10396 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:39:50.254083 10396 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:39:50.254086 10396 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:39:50.254088 10396 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 18:39:50.254091 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.254096 10396 net.cpp:184] Created Layer data/bias (2)
I0814 18:39:50.254098 10396 net.cpp:561] data/bias <- data
I0814 18:39:50.254101 10396 net.cpp:530] data/bias -> data/bias
I0814 18:39:50.254235 10396 net.cpp:245] Setting up data/bias
I0814 18:39:50.254240 10396 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0814 18:39:50.254245 10396 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 18:39:50.254247 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.254256 10396 net.cpp:184] Created Layer conv1a (3)
I0814 18:39:50.254259 10396 net.cpp:561] conv1a <- data/bias
I0814 18:39:50.254261 10396 net.cpp:530] conv1a -> conv1a
I0814 18:39:50.254611 10439 data_layer.cpp:97] (0) Parser threads: 1
I0814 18:39:50.254617 10439 data_layer.cpp:99] (0) Transformer threads: 1
I0814 18:39:50.257423 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0814 18:39:50.257432 10396 net.cpp:245] Setting up conv1a
I0814 18:39:50.257436 10396 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0814 18:39:50.257441 10396 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 18:39:50.257443 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.257452 10396 net.cpp:184] Created Layer conv1a/bn (4)
I0814 18:39:50.257455 10396 net.cpp:561] conv1a/bn <- conv1a
I0814 18:39:50.257457 10396 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 18:39:50.258085 10396 net.cpp:245] Setting up conv1a/bn
I0814 18:39:50.258092 10396 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0814 18:39:50.258098 10396 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 18:39:50.258101 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.258105 10396 net.cpp:184] Created Layer conv1a/relu (5)
I0814 18:39:50.258106 10396 net.cpp:561] conv1a/relu <- conv1a
I0814 18:39:50.258108 10396 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 18:39:50.258112 10396 net.cpp:245] Setting up conv1a/relu
I0814 18:39:50.258114 10396 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0814 18:39:50.258116 10396 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 18:39:50.258118 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.258126 10396 net.cpp:184] Created Layer conv1b (6)
I0814 18:39:50.258128 10396 net.cpp:561] conv1b <- conv1a
I0814 18:39:50.258131 10396 net.cpp:530] conv1b -> conv1b
I0814 18:39:50.261278 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0814 18:39:50.261287 10396 net.cpp:245] Setting up conv1b
I0814 18:39:50.261291 10396 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0814 18:39:50.261297 10396 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 18:39:50.261301 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.261304 10396 net.cpp:184] Created Layer conv1b/bn (7)
I0814 18:39:50.261313 10396 net.cpp:561] conv1b/bn <- conv1b
I0814 18:39:50.261317 10396 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 18:39:50.261984 10396 net.cpp:245] Setting up conv1b/bn
I0814 18:39:50.261996 10396 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0814 18:39:50.262006 10396 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 18:39:50.262011 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.262015 10396 net.cpp:184] Created Layer conv1b/relu (8)
I0814 18:39:50.262019 10396 net.cpp:561] conv1b/relu <- conv1b
I0814 18:39:50.262023 10396 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 18:39:50.262028 10396 net.cpp:245] Setting up conv1b/relu
I0814 18:39:50.262033 10396 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0814 18:39:50.262037 10396 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 18:39:50.262040 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.262046 10396 net.cpp:184] Created Layer pool1 (9)
I0814 18:39:50.262050 10396 net.cpp:561] pool1 <- conv1b
I0814 18:39:50.262055 10396 net.cpp:530] pool1 -> pool1
I0814 18:39:50.262203 10396 net.cpp:245] Setting up pool1
I0814 18:39:50.262213 10396 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0814 18:39:50.262219 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 18:39:50.262224 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.262233 10396 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 18:39:50.262238 10396 net.cpp:561] res2a_branch2a <- pool1
I0814 18:39:50.262243 10396 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 18:39:50.266264 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0814 18:39:50.266276 10396 net.cpp:245] Setting up res2a_branch2a
I0814 18:39:50.266281 10396 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0814 18:39:50.266288 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.266290 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.266296 10396 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 18:39:50.266299 10396 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 18:39:50.266302 10396 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 18:39:50.266991 10396 net.cpp:245] Setting up res2a_branch2a/bn
I0814 18:39:50.266999 10396 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0814 18:39:50.267004 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.267007 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.267011 10396 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 18:39:50.267014 10396 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 18:39:50.267015 10396 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 18:39:50.267022 10396 net.cpp:245] Setting up res2a_branch2a/relu
I0814 18:39:50.267024 10396 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0814 18:39:50.267026 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 18:39:50.267030 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.267045 10396 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 18:39:50.267047 10396 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 18:39:50.267050 10396 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 18:39:50.270359 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0814 18:39:50.270370 10396 net.cpp:245] Setting up res2a_branch2b
I0814 18:39:50.270382 10396 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0814 18:39:50.270387 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.270390 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.270395 10396 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 18:39:50.270398 10396 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 18:39:50.270401 10396 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 18:39:50.271064 10396 net.cpp:245] Setting up res2a_branch2b/bn
I0814 18:39:50.271071 10396 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0814 18:39:50.271077 10396 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.271080 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.271083 10396 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 18:39:50.271086 10396 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 18:39:50.271088 10396 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 18:39:50.271092 10396 net.cpp:245] Setting up res2a_branch2b/relu
I0814 18:39:50.271096 10396 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0814 18:39:50.271098 10396 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 18:39:50.271100 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.271106 10396 net.cpp:184] Created Layer pool2 (16)
I0814 18:39:50.271111 10396 net.cpp:561] pool2 <- res2a_branch2b
I0814 18:39:50.271114 10396 net.cpp:530] pool2 -> pool2
I0814 18:39:50.271183 10396 net.cpp:245] Setting up pool2
I0814 18:39:50.271188 10396 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0814 18:39:50.271190 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 18:39:50.271195 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.271203 10396 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 18:39:50.271208 10396 net.cpp:561] res3a_branch2a <- pool2
I0814 18:39:50.271210 10396 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 18:39:50.277293 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:39:50.277303 10396 net.cpp:245] Setting up res3a_branch2a
I0814 18:39:50.277307 10396 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0814 18:39:50.277312 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.277315 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.277319 10396 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 18:39:50.277321 10396 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 18:39:50.277324 10396 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 18:39:50.277976 10396 net.cpp:245] Setting up res3a_branch2a/bn
I0814 18:39:50.277984 10396 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0814 18:39:50.277992 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.277993 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.277997 10396 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 18:39:50.277998 10396 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 18:39:50.278002 10396 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 18:39:50.278004 10396 net.cpp:245] Setting up res3a_branch2a/relu
I0814 18:39:50.278007 10396 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0814 18:39:50.278009 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 18:39:50.278019 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.278026 10396 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 18:39:50.278029 10396 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 18:39:50.278031 10396 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 18:39:50.281338 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0814 18:39:50.281347 10396 net.cpp:245] Setting up res3a_branch2b
I0814 18:39:50.281350 10396 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0814 18:39:50.281355 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.281358 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.281363 10396 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 18:39:50.281365 10396 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 18:39:50.281368 10396 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 18:39:50.282011 10396 net.cpp:245] Setting up res3a_branch2b/bn
I0814 18:39:50.282017 10396 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0814 18:39:50.282023 10396 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.282027 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.282029 10396 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 18:39:50.282032 10396 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 18:39:50.282034 10396 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 18:39:50.282038 10396 net.cpp:245] Setting up res3a_branch2b/relu
I0814 18:39:50.282042 10396 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0814 18:39:50.282044 10396 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 18:39:50.282047 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.282053 10396 net.cpp:184] Created Layer pool3 (23)
I0814 18:39:50.282058 10396 net.cpp:561] pool3 <- res3a_branch2b
I0814 18:39:50.282061 10396 net.cpp:530] pool3 -> pool3
I0814 18:39:50.282132 10396 net.cpp:245] Setting up pool3
I0814 18:39:50.282137 10396 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0814 18:39:50.282141 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 18:39:50.282145 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.282153 10396 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 18:39:50.282157 10396 net.cpp:561] res4a_branch2a <- pool3
I0814 18:39:50.282160 10396 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 18:39:50.292865 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 18:39:50.292876 10396 net.cpp:245] Setting up res4a_branch2a
I0814 18:39:50.292879 10396 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0814 18:39:50.292883 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.292887 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.292892 10396 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 18:39:50.292894 10396 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 18:39:50.292898 10396 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 18:39:50.293781 10396 net.cpp:245] Setting up res4a_branch2a/bn
I0814 18:39:50.293790 10396 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0814 18:39:50.293797 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.293799 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.293814 10396 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 18:39:50.293818 10396 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 18:39:50.293823 10396 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 18:39:50.293828 10396 net.cpp:245] Setting up res4a_branch2a/relu
I0814 18:39:50.293833 10396 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0814 18:39:50.293836 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 18:39:50.293841 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.293857 10396 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 18:39:50.293860 10396 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 18:39:50.293864 10396 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 18:39:50.300331 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 18:39:50.300371 10396 net.cpp:245] Setting up res4a_branch2b
I0814 18:39:50.300393 10396 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0814 18:39:50.300406 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.300411 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.300426 10396 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 18:39:50.300431 10396 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 18:39:50.300436 10396 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 18:39:50.301465 10396 net.cpp:245] Setting up res4a_branch2b/bn
I0814 18:39:50.301483 10396 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0814 18:39:50.301497 10396 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.301503 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.301509 10396 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 18:39:50.301515 10396 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 18:39:50.301522 10396 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 18:39:50.301528 10396 net.cpp:245] Setting up res4a_branch2b/relu
I0814 18:39:50.301533 10396 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0814 18:39:50.301538 10396 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 18:39:50.301542 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.301549 10396 net.cpp:184] Created Layer pool4 (30)
I0814 18:39:50.301554 10396 net.cpp:561] pool4 <- res4a_branch2b
I0814 18:39:50.301559 10396 net.cpp:530] pool4 -> pool4
I0814 18:39:50.301659 10396 net.cpp:245] Setting up pool4
I0814 18:39:50.301667 10396 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0814 18:39:50.301672 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 18:39:50.301676 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.301688 10396 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 18:39:50.301693 10396 net.cpp:561] res5a_branch2a <- pool4
I0814 18:39:50.301698 10396 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 18:39:50.335475 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0814 18:39:50.335490 10396 net.cpp:245] Setting up res5a_branch2a
I0814 18:39:50.335496 10396 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0814 18:39:50.335502 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 18:39:50.335505 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.335528 10396 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 18:39:50.335532 10396 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 18:39:50.335536 10396 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 18:39:50.336243 10396 net.cpp:245] Setting up res5a_branch2a/bn
I0814 18:39:50.336251 10396 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0814 18:39:50.336256 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 18:39:50.336259 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.336262 10396 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 18:39:50.336266 10396 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 18:39:50.336267 10396 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 18:39:50.336272 10396 net.cpp:245] Setting up res5a_branch2a/relu
I0814 18:39:50.336274 10396 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0814 18:39:50.336277 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 18:39:50.336279 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.336284 10396 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 18:39:50.336287 10396 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 18:39:50.336290 10396 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 18:39:50.352457 10396 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0814 18:39:50.352468 10396 net.cpp:245] Setting up res5a_branch2b
I0814 18:39:50.352471 10396 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0814 18:39:50.352479 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 18:39:50.352483 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.352486 10396 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 18:39:50.352488 10396 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 18:39:50.352491 10396 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 18:39:50.353164 10396 net.cpp:245] Setting up res5a_branch2b/bn
I0814 18:39:50.353173 10396 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0814 18:39:50.353178 10396 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 18:39:50.353180 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353186 10396 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 18:39:50.353189 10396 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 18:39:50.353191 10396 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 18:39:50.353195 10396 net.cpp:245] Setting up res5a_branch2b/relu
I0814 18:39:50.353199 10396 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0814 18:39:50.353200 10396 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 18:39:50.353202 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353209 10396 net.cpp:184] Created Layer pool5 (37)
I0814 18:39:50.353212 10396 net.cpp:561] pool5 <- res5a_branch2b
I0814 18:39:50.353214 10396 net.cpp:530] pool5 -> pool5
I0814 18:39:50.353245 10396 net.cpp:245] Setting up pool5
I0814 18:39:50.353250 10396 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0814 18:39:50.353255 10396 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 18:39:50.353257 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353265 10396 net.cpp:184] Created Layer fc10 (38)
I0814 18:39:50.353267 10396 net.cpp:561] fc10 <- pool5
I0814 18:39:50.353271 10396 net.cpp:530] fc10 -> fc10
I0814 18:39:50.353549 10396 net.cpp:245] Setting up fc10
I0814 18:39:50.353566 10396 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0814 18:39:50.353572 10396 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0814 18:39:50.353576 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353584 10396 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0814 18:39:50.353586 10396 net.cpp:561] fc10_fc10_0_split <- fc10
I0814 18:39:50.353591 10396 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0814 18:39:50.353596 10396 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0814 18:39:50.353602 10396 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0814 18:39:50.353679 10396 net.cpp:245] Setting up fc10_fc10_0_split
I0814 18:39:50.353685 10396 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:39:50.353690 10396 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:39:50.353694 10396 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:39:50.353698 10396 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 18:39:50.353703 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353713 10396 net.cpp:184] Created Layer loss (40)
I0814 18:39:50.353718 10396 net.cpp:561] loss <- fc10_fc10_0_split_0
I0814 18:39:50.353723 10396 net.cpp:561] loss <- label_data_1_split_0
I0814 18:39:50.353727 10396 net.cpp:530] loss -> loss
I0814 18:39:50.353883 10396 net.cpp:245] Setting up loss
I0814 18:39:50.353890 10396 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 18:39:50.353894 10396 net.cpp:256]     with loss weight 1
I0814 18:39:50.353900 10396 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 18:39:50.353905 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353915 10396 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 18:39:50.353919 10396 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0814 18:39:50.353924 10396 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 18:39:50.353929 10396 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 18:39:50.353936 10396 net.cpp:245] Setting up accuracy/top1
I0814 18:39:50.353940 10396 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 18:39:50.353945 10396 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 18:39:50.353947 10396 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:39:50.353953 10396 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 18:39:50.353956 10396 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0814 18:39:50.353961 10396 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 18:39:50.353966 10396 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 18:39:50.353972 10396 net.cpp:245] Setting up accuracy/top5
I0814 18:39:50.353976 10396 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 18:39:50.353981 10396 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 18:39:50.353986 10396 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 18:39:50.353989 10396 net.cpp:323] loss needs backward computation.
I0814 18:39:50.353993 10396 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0814 18:39:50.353996 10396 net.cpp:323] fc10 needs backward computation.
I0814 18:39:50.354001 10396 net.cpp:323] pool5 needs backward computation.
I0814 18:39:50.354004 10396 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 18:39:50.354007 10396 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 18:39:50.354010 10396 net.cpp:323] res5a_branch2b needs backward computation.
I0814 18:39:50.354014 10396 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 18:39:50.354017 10396 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 18:39:50.354022 10396 net.cpp:323] res5a_branch2a needs backward computation.
I0814 18:39:50.354035 10396 net.cpp:323] pool4 needs backward computation.
I0814 18:39:50.354038 10396 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 18:39:50.354043 10396 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 18:39:50.354046 10396 net.cpp:323] res4a_branch2b needs backward computation.
I0814 18:39:50.354050 10396 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 18:39:50.354053 10396 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 18:39:50.354058 10396 net.cpp:323] res4a_branch2a needs backward computation.
I0814 18:39:50.354061 10396 net.cpp:323] pool3 needs backward computation.
I0814 18:39:50.354065 10396 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 18:39:50.354069 10396 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 18:39:50.354074 10396 net.cpp:323] res3a_branch2b needs backward computation.
I0814 18:39:50.354077 10396 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 18:39:50.354081 10396 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 18:39:50.354084 10396 net.cpp:323] res3a_branch2a needs backward computation.
I0814 18:39:50.354089 10396 net.cpp:323] pool2 needs backward computation.
I0814 18:39:50.354092 10396 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 18:39:50.354096 10396 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 18:39:50.354100 10396 net.cpp:323] res2a_branch2b needs backward computation.
I0814 18:39:50.354104 10396 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 18:39:50.354107 10396 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 18:39:50.354111 10396 net.cpp:323] res2a_branch2a needs backward computation.
I0814 18:39:50.354115 10396 net.cpp:323] pool1 needs backward computation.
I0814 18:39:50.354120 10396 net.cpp:323] conv1b/relu needs backward computation.
I0814 18:39:50.354123 10396 net.cpp:323] conv1b/bn needs backward computation.
I0814 18:39:50.354127 10396 net.cpp:323] conv1b needs backward computation.
I0814 18:39:50.354130 10396 net.cpp:323] conv1a/relu needs backward computation.
I0814 18:39:50.354135 10396 net.cpp:323] conv1a/bn needs backward computation.
I0814 18:39:50.354138 10396 net.cpp:323] conv1a needs backward computation.
I0814 18:39:50.354142 10396 net.cpp:325] data/bias does not need backward computation.
I0814 18:39:50.354147 10396 net.cpp:325] label_data_1_split does not need backward computation.
I0814 18:39:50.354152 10396 net.cpp:325] data does not need backward computation.
I0814 18:39:50.354156 10396 net.cpp:367] This network produces output accuracy/top1
I0814 18:39:50.354159 10396 net.cpp:367] This network produces output accuracy/top5
I0814 18:39:50.354162 10396 net.cpp:367] This network produces output loss
I0814 18:39:50.354197 10396 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0814 18:39:50.354202 10396 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0814 18:39:50.354204 10396 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0814 18:39:50.354207 10396 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 18:39:50.354212 10396 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 18:39:50.354215 10396 net.cpp:407] Network initialization done.
I0814 18:39:50.354276 10396 solver.cpp:56] Solver scaffolding done.
I0814 18:39:50.358371 10396 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0814 18:39:50.358381 10396 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0814 18:39:50.358384 10396 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0814 18:39:50.358386 10396 parallel.cpp:59] Starting Optimization
I0814 18:39:50.358391 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:39:50.358417 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:39:50.358433 10396 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:39:50.359093 10451 device_alternate.hpp:116] NVML initialized on thread 140165936875264
I0814 18:39:50.377971 10451 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 18:39:50.378034 10452 device_alternate.hpp:116] NVML initialized on thread 140165928482560
I0814 18:39:50.378907 10452 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 18:39:50.378962 10453 device_alternate.hpp:116] NVML initialized on thread 140165920089856
I0814 18:39:50.379781 10453 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 18:39:50.384420 10452 solver.cpp:42] Solver data type: FLOAT
W0814 18:39:50.384872 10452 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:39:50.385002 10452 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:50.385009 10452 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:50.385044 10452 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:39:50.385057 10452 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:39:50.391166 10453 solver.cpp:42] Solver data type: FLOAT
W0814 18:39:50.391777 10453 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:39:50.391800 10454 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:39:50.391880 10453 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:50.391896 10453 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:50.391935 10453 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:39:50.391950 10453 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:39:50.392779 10455 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:39:50.392915 10452 data_layer.cpp:185] [1] ReshapePrefetch 22, 3, 32, 32
I0814 18:39:50.394026 10453 data_layer.cpp:185] [2] ReshapePrefetch 22, 3, 32, 32
I0814 18:39:50.394227 10452 data_layer.cpp:209] [1] Output data size: 22, 3, 32, 32
I0814 18:39:50.394237 10452 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:39:50.394244 10453 data_layer.cpp:209] [2] Output data size: 22, 3, 32, 32
I0814 18:39:50.394251 10453 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:39:50.823902 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 18:39:50.840183 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 18:39:50.841848 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 18:39:50.849416 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 18:39:50.854476 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 8.21G, req 0G)
I0814 18:39:50.861412 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 18:39:50.863703 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 18:39:50.869792 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 18:39:50.877071 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 18:39:50.882418 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 18:39:50.883030 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 18:39:50.889619 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 18:39:50.906919 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 18:39:50.913259 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 18:39:50.916455 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 1 4 3  (limit 8.14G, req 0.01G)
I0814 18:39:50.924819 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0814 18:39:50.960798 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 18:39:50.972846 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 18:39:50.982321 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 18:39:50.984025 10452 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/test.prototxt
W0814 18:39:50.984076 10452 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:39:50.984167 10452 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:50.984172 10452 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:50.984191 10452 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:39:50.984201 10452 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:39:50.984936 10458 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:39:50.985018 10452 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 32, 32
I0814 18:39:50.985117 10452 data_layer.cpp:209] (1) Output data size: 17, 3, 32, 32
I0814 18:39:50.985123 10452 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:39:50.985898 10459 data_layer.cpp:97] (1) Parser threads: 1
I0814 18:39:50.985906 10459 data_layer.cpp:99] (1) Transformer threads: 1
I0814 18:39:50.988996 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 18:39:50.993538 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 18:39:50.997864 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 18:39:50.998528 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 18:39:50.999745 10453 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/test.prototxt
W0814 18:39:50.999796 10453 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:39:50.999893 10453 net.cpp:104] Using FLOAT as default forward math type
I0814 18:39:50.999899 10453 net.cpp:110] Using FLOAT as default backward math type
I0814 18:39:50.999918 10453 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:39:50.999927 10453 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:39:51.001021 10460 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:39:51.001524 10453 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 32, 32
I0814 18:39:51.001633 10453 data_layer.cpp:209] (2) Output data size: 17, 3, 32, 32
I0814 18:39:51.001639 10453 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:39:51.002398 10461 data_layer.cpp:97] (2) Parser threads: 1
I0814 18:39:51.002405 10461 data_layer.cpp:99] (2) Transformer threads: 1
I0814 18:39:51.003576 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 18:39:51.006417 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 18:39:51.012186 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 18:39:51.012718 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 18:39:51.017966 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 18:39:51.018803 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 18:39:51.023053 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 18:39:51.030503 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 18:39:51.030947 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 18:39:51.035485 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 18:39:51.037876 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 18:39:51.048251 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 18:39:51.054787 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 18:39:51.070842 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 18:39:51.087558 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 18:39:51.088632 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 18:39:51.091131 10452 solver.cpp:56] Solver scaffolding done.
I0814 18:39:51.105320 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 18:39:51.106868 10453 solver.cpp:56] Solver scaffolding done.
I0814 18:39:51.149130 10451 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0814 18:39:51.149155 10453 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0814 18:39:51.149183 10452 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0814 18:39:51.355933 10453 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:39:51.355933 10451 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:39:51.355933 10452 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:39:51.355953 10453 solver.cpp:439] Learning Rate Policy: poly
I0814 18:39:51.355957 10451 solver.cpp:439] Learning Rate Policy: poly
I0814 18:39:51.355963 10452 solver.cpp:439] Learning Rate Policy: poly
I0814 18:39:51.362339 10451 solver.cpp:227] Starting Optimization on GPU 0
I0814 18:39:51.362339 10453 solver.cpp:227] Starting Optimization on GPU 2
I0814 18:39:51.362391 10451 solver.cpp:509] Iteration 0, Testing net (#0)
I0814 18:39:51.362339 10452 solver.cpp:227] Starting Optimization on GPU 1
I0814 18:39:51.362421 10492 device_alternate.hpp:116] NVML initialized on thread 140165164668672
I0814 18:39:51.362447 10492 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 18:39:51.362460 10493 device_alternate.hpp:116] NVML initialized on thread 140165156275968
I0814 18:39:51.362470 10493 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 18:39:51.362699 10494 device_alternate.hpp:116] NVML initialized on thread 140165147883264
I0814 18:39:51.362730 10494 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 18:39:51.370357 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0814 18:39:51.375119 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0814 18:39:51.376451 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0814 18:39:51.376960 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0814 18:39:51.381806 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0814 18:39:51.382694 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:39:51.385715 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0814 18:39:51.388555 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 7.96G, req 0.01G)
I0814 18:39:51.389809 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:39:51.394613 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0814 18:39:51.395947 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0814 18:39:51.396347 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0814 18:39:51.401144 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0814 18:39:51.401532 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0814 18:39:51.403019 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0814 18:39:51.407915 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0814 18:39:51.408638 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0814 18:39:51.410404 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0814 18:39:51.413436 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0814 18:39:51.416812 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0814 18:39:51.418354 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0814 18:39:51.421033 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0814 18:39:51.423805 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0814 18:39:51.426048 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0814 18:39:51.430019 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0814 18:39:51.431066 10452 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0814 18:39:51.432898 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0814 18:39:51.438119 10453 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0814 18:39:51.439851 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0814 18:39:51.444249 10451 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0814 18:39:51.446528 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.176471
I0814 18:39:51.446537 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.470588
I0814 18:39:51.446544 10451 solver.cpp:594]     Test net output #2: loss = 71.9242 (* 1 = 71.9242 loss)
I0814 18:39:51.446552 10451 solver.cpp:254] [MultiGPU] Initial Test completed
I0814 18:39:51.446573 10451 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 18:39:51.455572 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0814 18:39:51.456221 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 0  (limit 7.88G, req 0.01G)
I0814 18:39:51.457500 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0814 18:39:51.464967 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0814 18:39:51.466501 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0814 18:39:51.466975 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.79G, req 0G)
I0814 18:39:51.475109 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0814 18:39:51.476930 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0814 18:39:51.478163 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0814 18:39:51.483178 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0814 18:39:51.486258 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0814 18:39:51.486573 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0814 18:39:51.493096 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0814 18:39:51.496953 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0814 18:39:51.498093 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0814 18:39:51.498497 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0814 18:39:51.504009 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0814 18:39:51.505338 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0814 18:39:51.513172 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0814 18:39:51.517796 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0814 18:39:51.519886 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0814 18:39:51.520347 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0814 18:39:51.526244 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0814 18:39:51.527724 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0814 18:39:51.538902 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0814 18:39:51.544909 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0814 18:39:51.545644 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0814 18:39:51.546919 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0814 18:39:51.553565 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0814 18:39:51.554309 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0814 18:39:51.585984 10456 data_layer.cpp:97] [1] Parser threads: 1
I0814 18:39:51.585999 10456 data_layer.cpp:99] [1] Transformer threads: 1
I0814 18:39:51.597975 10437 data_layer.cpp:97] [0] Parser threads: 1
I0814 18:39:51.597986 10437 data_layer.cpp:99] [0] Transformer threads: 1
I0814 18:39:51.598151 10457 data_layer.cpp:97] [2] Parser threads: 1
I0814 18:39:51.598163 10457 data_layer.cpp:99] [2] Transformer threads: 1
I0814 18:39:51.599684 10451 solver.cpp:317] Iteration 0 (0.15309 s), loss = 2.32328
I0814 18:39:51.599701 10451 solver.cpp:334]     Train net output #0: loss = 2.32328 (* 1 = 2.32328 loss)
I0814 18:39:51.599709 10451 sgd_solver.cpp:136] Iteration 0, lr = 0.1, m = 0.9
I0814 18:39:51.627612 10451 solver.cpp:317] Iteration 1 (0.0279183 s), loss = 2.19253
I0814 18:39:51.627660 10451 solver.cpp:334]     Train net output #0: loss = 2.19253 (* 1 = 2.19253 loss)
I0814 18:39:51.638320 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0814 18:39:51.638914 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.98G, req 0.01G)
I0814 18:39:51.639081 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0814 18:39:51.648144 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.649252 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 6 1 3  (limit 6.34G, req 0.01G)
I0814 18:39:51.649538 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.662787 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.663311 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.34G, req 0.01G)
I0814 18:39:51.669101 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.675125 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.675705 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.01G)
I0814 18:39:51.676407 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.685362 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0814 18:39:51.685724 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.01G)
I0814 18:39:51.687043 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0814 18:39:51.692951 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.34G, req 0.01G)
I0814 18:39:51.693127 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.693678 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:39:51.716511 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0814 18:39:51.718085 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0814 18:39:51.718931 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.02G)
I0814 18:39:51.724066 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0814 18:39:51.726078 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.02G)
I0814 18:39:51.726461 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0814 18:39:51.757967 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0814 18:39:51.760064 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 6 5 5  (limit 6.34G, req 0.03G)
I0814 18:39:51.762305 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0814 18:39:51.766517 10453 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0814 18:39:51.768065 10451 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.34G, req 0.03G)
I0814 18:39:51.770761 10452 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0814 18:39:51.783601 10451 solver.cpp:317] Iteration 2 (0.155979 s), loss = 2.16913
I0814 18:39:51.783612 10451 solver.cpp:334]     Train net output #0: loss = 2.16913 (* 1 = 2.16913 loss)
I0814 18:39:51.783627 10452 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:39:51.783639 10451 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:39:51.783790 10453 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:39:53.294811 10451 solver.cpp:312] Iteration 100 (64.8508 iter/s, 1.51116s/98 iter), loss = 2.5052
I0814 18:39:53.294872 10451 solver.cpp:334]     Train net output #0: loss = 2.5052 (* 1 = 2.5052 loss)
I0814 18:39:53.294889 10451 sgd_solver.cpp:136] Iteration 100, lr = 0.0998438, m = 0.9
I0814 18:39:54.871817 10451 solver.cpp:312] Iteration 200 (63.4133 iter/s, 1.57696s/100 iter), loss = 1.64632
I0814 18:39:54.871841 10451 solver.cpp:334]     Train net output #0: loss = 1.64632 (* 1 = 1.64632 loss)
I0814 18:39:54.871847 10451 sgd_solver.cpp:136] Iteration 200, lr = 0.0996875, m = 0.9
I0814 18:39:56.440872 10451 solver.cpp:312] Iteration 300 (63.7345 iter/s, 1.56901s/100 iter), loss = 1.21452
I0814 18:39:56.440935 10451 solver.cpp:334]     Train net output #0: loss = 1.21452 (* 1 = 1.21452 loss)
I0814 18:39:56.440953 10451 sgd_solver.cpp:136] Iteration 300, lr = 0.0995313, m = 0.9
I0814 18:39:57.978102 10451 solver.cpp:312] Iteration 400 (65.054 iter/s, 1.53718s/100 iter), loss = 1.19043
I0814 18:39:57.978147 10451 solver.cpp:334]     Train net output #0: loss = 1.19043 (* 1 = 1.19043 loss)
I0814 18:39:57.978159 10451 sgd_solver.cpp:136] Iteration 400, lr = 0.099375, m = 0.9
I0814 18:39:59.514072 10451 solver.cpp:312] Iteration 500 (65.1075 iter/s, 1.53592s/100 iter), loss = 1.51405
I0814 18:39:59.514096 10451 solver.cpp:334]     Train net output #0: loss = 1.51405 (* 1 = 1.51405 loss)
I0814 18:39:59.514102 10451 sgd_solver.cpp:136] Iteration 500, lr = 0.0992187, m = 0.9
I0814 18:40:01.058910 10451 solver.cpp:312] Iteration 600 (64.7336 iter/s, 1.54479s/100 iter), loss = 1.09533
I0814 18:40:01.058972 10451 solver.cpp:334]     Train net output #0: loss = 1.09533 (* 1 = 1.09533 loss)
I0814 18:40:01.058990 10451 sgd_solver.cpp:136] Iteration 600, lr = 0.0990625, m = 0.9
I0814 18:40:02.644903 10451 solver.cpp:312] Iteration 700 (63.0539 iter/s, 1.58594s/100 iter), loss = 0.990054
I0814 18:40:02.644949 10451 solver.cpp:334]     Train net output #0: loss = 0.990054 (* 1 = 0.990054 loss)
I0814 18:40:02.644958 10451 sgd_solver.cpp:136] Iteration 700, lr = 0.0989062, m = 0.9
I0814 18:40:03.470002 10436 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 18:40:04.168579 10451 solver.cpp:312] Iteration 800 (65.6329 iter/s, 1.52362s/100 iter), loss = 0.591562
I0814 18:40:04.168643 10451 solver.cpp:334]     Train net output #0: loss = 0.591562 (* 1 = 0.591562 loss)
I0814 18:40:04.168663 10451 sgd_solver.cpp:136] Iteration 800, lr = 0.09875, m = 0.9
I0814 18:40:05.733408 10451 solver.cpp:312] Iteration 900 (63.9066 iter/s, 1.56478s/100 iter), loss = 0.59057
I0814 18:40:05.733559 10451 solver.cpp:334]     Train net output #0: loss = 0.59057 (* 1 = 0.59057 loss)
I0814 18:40:05.733583 10451 sgd_solver.cpp:136] Iteration 900, lr = 0.0985937, m = 0.9
I0814 18:40:07.245594 10451 solver.cpp:509] Iteration 1000, Testing net (#0)
I0814 18:40:08.021127 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.553235
I0814 18:40:08.021147 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.927942
I0814 18:40:08.021152 10451 solver.cpp:594]     Test net output #2: loss = 1.36715 (* 1 = 1.36715 loss)
I0814 18:40:08.021169 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.775554s
I0814 18:40:08.036214 10451 solver.cpp:312] Iteration 1000 (43.4266 iter/s, 2.30274s/100 iter), loss = 1.28443
I0814 18:40:08.036247 10451 solver.cpp:334]     Train net output #0: loss = 1.28443 (* 1 = 1.28443 loss)
I0814 18:40:08.036257 10451 sgd_solver.cpp:136] Iteration 1000, lr = 0.0984375, m = 0.9
I0814 18:40:09.622818 10451 solver.cpp:312] Iteration 1100 (63.0296 iter/s, 1.58656s/100 iter), loss = 0.836491
I0814 18:40:09.622843 10451 solver.cpp:334]     Train net output #0: loss = 0.83649 (* 1 = 0.83649 loss)
I0814 18:40:09.622848 10451 sgd_solver.cpp:136] Iteration 1100, lr = 0.0982813, m = 0.9
I0814 18:40:11.160346 10451 solver.cpp:312] Iteration 1200 (65.0416 iter/s, 1.53748s/100 iter), loss = 0.687889
I0814 18:40:11.160372 10451 solver.cpp:334]     Train net output #0: loss = 0.687888 (* 1 = 0.687888 loss)
I0814 18:40:11.160377 10451 sgd_solver.cpp:136] Iteration 1200, lr = 0.098125, m = 0.9
I0814 18:40:12.731673 10451 solver.cpp:312] Iteration 1300 (63.6424 iter/s, 1.57128s/100 iter), loss = 0.693504
I0814 18:40:12.731732 10451 solver.cpp:334]     Train net output #0: loss = 0.693504 (* 1 = 0.693504 loss)
I0814 18:40:12.731750 10451 sgd_solver.cpp:136] Iteration 1300, lr = 0.0979687, m = 0.9
I0814 18:40:14.346971 10451 solver.cpp:312] Iteration 1400 (61.9101 iter/s, 1.61525s/100 iter), loss = 0.659862
I0814 18:40:14.346994 10451 solver.cpp:334]     Train net output #0: loss = 0.659862 (* 1 = 0.659862 loss)
I0814 18:40:14.347000 10451 sgd_solver.cpp:136] Iteration 1400, lr = 0.0978125, m = 0.9
I0814 18:40:15.920231 10451 solver.cpp:312] Iteration 1500 (63.5642 iter/s, 1.57321s/100 iter), loss = 0.458183
I0814 18:40:15.920253 10451 solver.cpp:334]     Train net output #0: loss = 0.458183 (* 1 = 0.458183 loss)
I0814 18:40:15.920259 10451 sgd_solver.cpp:136] Iteration 1500, lr = 0.0976562, m = 0.9
I0814 18:40:17.480980 10451 solver.cpp:312] Iteration 1600 (64.0736 iter/s, 1.5607s/100 iter), loss = 0.733794
I0814 18:40:17.481047 10451 solver.cpp:334]     Train net output #0: loss = 0.733794 (* 1 = 0.733794 loss)
I0814 18:40:17.481067 10451 sgd_solver.cpp:136] Iteration 1600, lr = 0.0975, m = 0.9
I0814 18:40:19.008615 10451 solver.cpp:312] Iteration 1700 (65.4629 iter/s, 1.52758s/100 iter), loss = 0.578921
I0814 18:40:19.008638 10451 solver.cpp:334]     Train net output #0: loss = 0.578921 (* 1 = 0.578921 loss)
I0814 18:40:19.008642 10451 sgd_solver.cpp:136] Iteration 1700, lr = 0.0973438, m = 0.9
I0814 18:40:20.594066 10451 solver.cpp:312] Iteration 1800 (63.0755 iter/s, 1.5854s/100 iter), loss = 0.372266
I0814 18:40:20.594172 10451 solver.cpp:334]     Train net output #0: loss = 0.372266 (* 1 = 0.372266 loss)
I0814 18:40:20.594179 10451 sgd_solver.cpp:136] Iteration 1800, lr = 0.0971875, m = 0.9
I0814 18:40:22.156193 10451 solver.cpp:312] Iteration 1900 (64.0174 iter/s, 1.56208s/100 iter), loss = 0.698815
I0814 18:40:22.156216 10451 solver.cpp:334]     Train net output #0: loss = 0.698815 (* 1 = 0.698815 loss)
I0814 18:40:22.156222 10451 sgd_solver.cpp:136] Iteration 1900, lr = 0.0970313, m = 0.9
I0814 18:40:23.748343 10451 solver.cpp:509] Iteration 2000, Testing net (#0)
I0814 18:40:24.545991 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.612941
I0814 18:40:24.546010 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.958236
I0814 18:40:24.546015 10451 solver.cpp:594]     Test net output #2: loss = 1.25721 (* 1 = 1.25721 loss)
I0814 18:40:24.546032 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.797666s
I0814 18:40:24.560982 10451 solver.cpp:312] Iteration 2000 (41.5849 iter/s, 2.40472s/100 iter), loss = 0.68712
I0814 18:40:24.560998 10451 solver.cpp:334]     Train net output #0: loss = 0.68712 (* 1 = 0.68712 loss)
I0814 18:40:24.561003 10451 sgd_solver.cpp:136] Iteration 2000, lr = 0.096875, m = 0.9
I0814 18:40:26.098135 10451 solver.cpp:312] Iteration 2100 (65.0576 iter/s, 1.5371s/100 iter), loss = 0.568798
I0814 18:40:26.098160 10451 solver.cpp:334]     Train net output #0: loss = 0.568798 (* 1 = 0.568798 loss)
I0814 18:40:26.098165 10451 sgd_solver.cpp:136] Iteration 2100, lr = 0.0967188, m = 0.9
I0814 18:40:27.609845 10451 solver.cpp:312] Iteration 2200 (66.1522 iter/s, 1.51166s/100 iter), loss = 0.645608
I0814 18:40:27.609869 10451 solver.cpp:334]     Train net output #0: loss = 0.645608 (* 1 = 0.645608 loss)
I0814 18:40:27.609874 10451 sgd_solver.cpp:136] Iteration 2200, lr = 0.0965625, m = 0.9
I0814 18:40:29.149340 10451 solver.cpp:312] Iteration 2300 (64.9585 iter/s, 1.53944s/100 iter), loss = 0.485628
I0814 18:40:29.149365 10451 solver.cpp:334]     Train net output #0: loss = 0.485628 (* 1 = 0.485628 loss)
I0814 18:40:29.149371 10451 sgd_solver.cpp:136] Iteration 2300, lr = 0.0964063, m = 0.9
I0814 18:40:30.701571 10451 solver.cpp:312] Iteration 2400 (64.4254 iter/s, 1.55218s/100 iter), loss = 0.37589
I0814 18:40:30.701620 10451 solver.cpp:334]     Train net output #0: loss = 0.37589 (* 1 = 0.37589 loss)
I0814 18:40:30.701632 10451 sgd_solver.cpp:136] Iteration 2400, lr = 0.09625, m = 0.9
I0814 18:40:32.235842 10451 solver.cpp:312] Iteration 2500 (65.1797 iter/s, 1.53422s/100 iter), loss = 0.193013
I0814 18:40:32.235911 10451 solver.cpp:334]     Train net output #0: loss = 0.193013 (* 1 = 0.193013 loss)
I0814 18:40:32.235935 10451 sgd_solver.cpp:136] Iteration 2500, lr = 0.0960938, m = 0.9
I0814 18:40:33.803396 10451 solver.cpp:312] Iteration 2600 (63.7957 iter/s, 1.5675s/100 iter), loss = 0.588975
I0814 18:40:33.803421 10451 solver.cpp:334]     Train net output #0: loss = 0.588975 (* 1 = 0.588975 loss)
I0814 18:40:33.803427 10451 sgd_solver.cpp:136] Iteration 2600, lr = 0.0959375, m = 0.9
I0814 18:40:35.314620 10451 solver.cpp:312] Iteration 2700 (66.1736 iter/s, 1.51118s/100 iter), loss = 0.274138
I0814 18:40:35.314646 10451 solver.cpp:334]     Train net output #0: loss = 0.274138 (* 1 = 0.274138 loss)
I0814 18:40:35.314651 10451 sgd_solver.cpp:136] Iteration 2700, lr = 0.0957813, m = 0.9
I0814 18:40:36.895692 10451 solver.cpp:312] Iteration 2800 (63.2502 iter/s, 1.58102s/100 iter), loss = 0.281997
I0814 18:40:36.895715 10451 solver.cpp:334]     Train net output #0: loss = 0.281997 (* 1 = 0.281997 loss)
I0814 18:40:36.895721 10451 sgd_solver.cpp:136] Iteration 2800, lr = 0.095625, m = 0.9
I0814 18:40:38.444293 10451 solver.cpp:312] Iteration 2900 (64.5765 iter/s, 1.54855s/100 iter), loss = 0.38425
I0814 18:40:38.444351 10451 solver.cpp:334]     Train net output #0: loss = 0.38425 (* 1 = 0.38425 loss)
I0814 18:40:38.444378 10451 sgd_solver.cpp:136] Iteration 2900, lr = 0.0954688, m = 0.9
I0814 18:40:40.003887 10451 solver.cpp:509] Iteration 3000, Testing net (#0)
I0814 18:40:40.813047 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.678235
I0814 18:40:40.813082 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.975
I0814 18:40:40.813088 10451 solver.cpp:594]     Test net output #2: loss = 0.943818 (* 1 = 0.943818 loss)
I0814 18:40:40.813107 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.809355s
I0814 18:40:40.832739 10451 solver.cpp:312] Iteration 3000 (41.8695 iter/s, 2.38838s/100 iter), loss = 0.624858
I0814 18:40:40.832778 10451 solver.cpp:334]     Train net output #0: loss = 0.624858 (* 1 = 0.624858 loss)
I0814 18:40:40.832798 10451 sgd_solver.cpp:136] Iteration 3000, lr = 0.0953125, m = 0.9
I0814 18:40:42.418480 10451 solver.cpp:312] Iteration 3100 (63.064 iter/s, 1.58569s/100 iter), loss = 0.124433
I0814 18:40:42.418506 10451 solver.cpp:334]     Train net output #0: loss = 0.124433 (* 1 = 0.124433 loss)
I0814 18:40:42.418514 10451 sgd_solver.cpp:136] Iteration 3100, lr = 0.0951563, m = 0.9
I0814 18:40:43.935494 10451 solver.cpp:312] Iteration 3200 (65.9212 iter/s, 1.51696s/100 iter), loss = 0.485795
I0814 18:40:43.935555 10451 solver.cpp:334]     Train net output #0: loss = 0.485795 (* 1 = 0.485795 loss)
I0814 18:40:43.935573 10451 sgd_solver.cpp:136] Iteration 3200, lr = 0.095, m = 0.9
I0814 18:40:45.522627 10451 solver.cpp:312] Iteration 3300 (63.0087 iter/s, 1.58708s/100 iter), loss = 0.0987551
I0814 18:40:45.522791 10451 solver.cpp:334]     Train net output #0: loss = 0.0987549 (* 1 = 0.0987549 loss)
I0814 18:40:45.522879 10451 sgd_solver.cpp:136] Iteration 3300, lr = 0.0948438, m = 0.9
I0814 18:40:47.119879 10451 solver.cpp:312] Iteration 3400 (62.6094 iter/s, 1.5972s/100 iter), loss = 0.358784
I0814 18:40:47.119902 10451 solver.cpp:334]     Train net output #0: loss = 0.358784 (* 1 = 0.358784 loss)
I0814 18:40:47.119906 10451 sgd_solver.cpp:136] Iteration 3400, lr = 0.0946875, m = 0.9
I0814 18:40:48.706596 10451 solver.cpp:312] Iteration 3500 (63.0253 iter/s, 1.58667s/100 iter), loss = 0.631892
I0814 18:40:48.706665 10451 solver.cpp:334]     Train net output #0: loss = 0.631892 (* 1 = 0.631892 loss)
I0814 18:40:48.706686 10451 sgd_solver.cpp:136] Iteration 3500, lr = 0.0945313, m = 0.9
I0814 18:40:50.278697 10451 solver.cpp:312] Iteration 3600 (63.6113 iter/s, 1.57205s/100 iter), loss = 0.336406
I0814 18:40:50.278743 10451 solver.cpp:334]     Train net output #0: loss = 0.336406 (* 1 = 0.336406 loss)
I0814 18:40:50.278755 10451 sgd_solver.cpp:136] Iteration 3600, lr = 0.094375, m = 0.9
I0814 18:40:51.820976 10451 solver.cpp:312] Iteration 3700 (64.8411 iter/s, 1.54223s/100 iter), loss = 0.136329
I0814 18:40:51.821054 10451 solver.cpp:334]     Train net output #0: loss = 0.136329 (* 1 = 0.136329 loss)
I0814 18:40:51.821059 10451 sgd_solver.cpp:136] Iteration 3700, lr = 0.0942188, m = 0.9
I0814 18:40:53.407397 10451 solver.cpp:312] Iteration 3800 (63.0371 iter/s, 1.58637s/100 iter), loss = 0.754933
I0814 18:40:53.407420 10451 solver.cpp:334]     Train net output #0: loss = 0.754932 (* 1 = 0.754932 loss)
I0814 18:40:53.407426 10451 sgd_solver.cpp:136] Iteration 3800, lr = 0.0940625, m = 0.9
I0814 18:40:54.962913 10451 solver.cpp:312] Iteration 3900 (64.2894 iter/s, 1.55547s/100 iter), loss = 0.846896
I0814 18:40:54.962975 10451 solver.cpp:334]     Train net output #0: loss = 0.846896 (* 1 = 0.846896 loss)
I0814 18:40:54.962991 10451 sgd_solver.cpp:136] Iteration 3900, lr = 0.0939062, m = 0.9
I0814 18:40:56.555378 10451 solver.cpp:509] Iteration 4000, Testing net (#0)
I0814 18:40:57.336838 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.691471
I0814 18:40:57.336858 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.977942
I0814 18:40:57.336863 10451 solver.cpp:594]     Test net output #2: loss = 0.923988 (* 1 = 0.923988 loss)
I0814 18:40:57.336877 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.781477s
I0814 18:40:57.351822 10451 solver.cpp:312] Iteration 4000 (41.8613 iter/s, 2.38884s/100 iter), loss = 0.352323
I0814 18:40:57.351855 10451 solver.cpp:334]     Train net output #0: loss = 0.352322 (* 1 = 0.352322 loss)
I0814 18:40:57.351867 10451 sgd_solver.cpp:136] Iteration 4000, lr = 0.09375, m = 0.9
I0814 18:40:58.901474 10451 solver.cpp:312] Iteration 4100 (64.5327 iter/s, 1.5496s/100 iter), loss = 0.211386
I0814 18:40:58.901499 10451 solver.cpp:334]     Train net output #0: loss = 0.211385 (* 1 = 0.211385 loss)
I0814 18:40:58.901504 10451 sgd_solver.cpp:136] Iteration 4100, lr = 0.0935938, m = 0.9
I0814 18:41:00.532707 10451 solver.cpp:312] Iteration 4200 (61.3053 iter/s, 1.63118s/100 iter), loss = 0.462415
I0814 18:41:00.532775 10451 solver.cpp:334]     Train net output #0: loss = 0.462415 (* 1 = 0.462415 loss)
I0814 18:41:00.532796 10451 sgd_solver.cpp:136] Iteration 4200, lr = 0.0934375, m = 0.9
I0814 18:41:02.082767 10451 solver.cpp:312] Iteration 4300 (64.5158 iter/s, 1.55001s/100 iter), loss = 0.174517
I0814 18:41:02.082793 10451 solver.cpp:334]     Train net output #0: loss = 0.174516 (* 1 = 0.174516 loss)
I0814 18:41:02.082800 10451 sgd_solver.cpp:136] Iteration 4300, lr = 0.0932813, m = 0.9
I0814 18:41:03.665261 10451 solver.cpp:312] Iteration 4400 (63.1933 iter/s, 1.58245s/100 iter), loss = 0.534613
I0814 18:41:03.665320 10451 solver.cpp:334]     Train net output #0: loss = 0.534612 (* 1 = 0.534612 loss)
I0814 18:41:03.665338 10451 sgd_solver.cpp:136] Iteration 4400, lr = 0.093125, m = 0.9
I0814 18:41:05.259289 10451 solver.cpp:312] Iteration 4500 (62.7362 iter/s, 1.59398s/100 iter), loss = 0.30862
I0814 18:41:05.259315 10451 solver.cpp:334]     Train net output #0: loss = 0.30862 (* 1 = 0.30862 loss)
I0814 18:41:05.259320 10451 sgd_solver.cpp:136] Iteration 4500, lr = 0.0929688, m = 0.9
I0814 18:41:06.836536 10451 solver.cpp:312] Iteration 4600 (63.4036 iter/s, 1.5772s/100 iter), loss = 0.147161
I0814 18:41:06.836599 10451 solver.cpp:334]     Train net output #0: loss = 0.147161 (* 1 = 0.147161 loss)
I0814 18:41:06.836617 10451 sgd_solver.cpp:136] Iteration 4600, lr = 0.0928125, m = 0.9
I0814 18:41:08.417214 10451 solver.cpp:312] Iteration 4700 (63.266 iter/s, 1.58063s/100 iter), loss = 0.385962
I0814 18:41:08.417271 10451 solver.cpp:334]     Train net output #0: loss = 0.385961 (* 1 = 0.385961 loss)
I0814 18:41:08.417290 10451 sgd_solver.cpp:136] Iteration 4700, lr = 0.0926562, m = 0.9
I0814 18:41:10.001161 10451 solver.cpp:312] Iteration 4800 (63.1354 iter/s, 1.5839s/100 iter), loss = 0.441463
I0814 18:41:10.001307 10451 solver.cpp:334]     Train net output #0: loss = 0.441462 (* 1 = 0.441462 loss)
I0814 18:41:10.001332 10451 sgd_solver.cpp:136] Iteration 4800, lr = 0.0925, m = 0.9
I0814 18:41:11.593833 10451 solver.cpp:312] Iteration 4900 (62.7897 iter/s, 1.59262s/100 iter), loss = 0.217581
I0814 18:41:11.593904 10451 solver.cpp:334]     Train net output #0: loss = 0.21758 (* 1 = 0.21758 loss)
I0814 18:41:11.593920 10451 sgd_solver.cpp:136] Iteration 4900, lr = 0.0923437, m = 0.9
I0814 18:41:13.138423 10451 solver.cpp:509] Iteration 5000, Testing net (#0)
I0814 18:41:13.814885 10438 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 18:41:13.944249 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.780295
I0814 18:41:13.944269 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.986471
I0814 18:41:13.944274 10451 solver.cpp:594]     Test net output #2: loss = 0.639085 (* 1 = 0.639085 loss)
I0814 18:41:13.944288 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.805841s
I0814 18:41:13.959228 10451 solver.cpp:312] Iteration 5000 (42.2775 iter/s, 2.36533s/100 iter), loss = 0.566563
I0814 18:41:13.959242 10451 solver.cpp:334]     Train net output #0: loss = 0.566563 (* 1 = 0.566563 loss)
I0814 18:41:13.959246 10451 sgd_solver.cpp:136] Iteration 5000, lr = 0.0921875, m = 0.9
I0814 18:41:15.538527 10451 solver.cpp:312] Iteration 5100 (63.3213 iter/s, 1.57925s/100 iter), loss = 0.172104
I0814 18:41:15.538552 10451 solver.cpp:334]     Train net output #0: loss = 0.172104 (* 1 = 0.172104 loss)
I0814 18:41:15.538558 10451 sgd_solver.cpp:136] Iteration 5100, lr = 0.0920313, m = 0.9
I0814 18:41:17.054474 10451 solver.cpp:312] Iteration 5200 (65.9674 iter/s, 1.5159s/100 iter), loss = 0.295581
I0814 18:41:17.054519 10451 solver.cpp:334]     Train net output #0: loss = 0.295581 (* 1 = 0.295581 loss)
I0814 18:41:17.054533 10451 sgd_solver.cpp:136] Iteration 5200, lr = 0.091875, m = 0.9
I0814 18:41:18.677471 10451 solver.cpp:312] Iteration 5300 (61.6164 iter/s, 1.62294s/100 iter), loss = 0.354646
I0814 18:41:18.677497 10451 solver.cpp:334]     Train net output #0: loss = 0.354645 (* 1 = 0.354645 loss)
I0814 18:41:18.677503 10451 sgd_solver.cpp:136] Iteration 5300, lr = 0.0917188, m = 0.9
I0814 18:41:20.271773 10451 solver.cpp:312] Iteration 5400 (62.7253 iter/s, 1.59425s/100 iter), loss = 0.3844
I0814 18:41:20.271836 10451 solver.cpp:334]     Train net output #0: loss = 0.384399 (* 1 = 0.384399 loss)
I0814 18:41:20.271855 10451 sgd_solver.cpp:136] Iteration 5400, lr = 0.0915625, m = 0.9
I0814 18:41:21.826975 10451 solver.cpp:312] Iteration 5500 (64.3024 iter/s, 1.55515s/100 iter), loss = 0.248296
I0814 18:41:21.827057 10451 solver.cpp:334]     Train net output #0: loss = 0.248296 (* 1 = 0.248296 loss)
I0814 18:41:21.827066 10451 sgd_solver.cpp:136] Iteration 5500, lr = 0.0914062, m = 0.9
I0814 18:41:23.410621 10451 solver.cpp:312] Iteration 5600 (63.1475 iter/s, 1.58359s/100 iter), loss = 0.207641
I0814 18:41:23.410645 10451 solver.cpp:334]     Train net output #0: loss = 0.20764 (* 1 = 0.20764 loss)
I0814 18:41:23.410650 10451 sgd_solver.cpp:136] Iteration 5600, lr = 0.09125, m = 0.9
I0814 18:41:24.970243 10451 solver.cpp:312] Iteration 5700 (64.1201 iter/s, 1.55957s/100 iter), loss = 0.251728
I0814 18:41:24.970269 10451 solver.cpp:334]     Train net output #0: loss = 0.251728 (* 1 = 0.251728 loss)
I0814 18:41:24.970274 10451 sgd_solver.cpp:136] Iteration 5700, lr = 0.0910937, m = 0.9
I0814 18:41:26.537359 10451 solver.cpp:312] Iteration 5800 (63.8136 iter/s, 1.56706s/100 iter), loss = 0.512637
I0814 18:41:26.537382 10451 solver.cpp:334]     Train net output #0: loss = 0.512637 (* 1 = 0.512637 loss)
I0814 18:41:26.537386 10451 sgd_solver.cpp:136] Iteration 5800, lr = 0.0909375, m = 0.9
I0814 18:41:28.074894 10451 solver.cpp:312] Iteration 5900 (65.0412 iter/s, 1.53749s/100 iter), loss = 0.321882
I0814 18:41:28.074916 10451 solver.cpp:334]     Train net output #0: loss = 0.321882 (* 1 = 0.321882 loss)
I0814 18:41:28.074920 10451 sgd_solver.cpp:136] Iteration 5900, lr = 0.0907812, m = 0.9
I0814 18:41:29.613937 10451 solver.cpp:509] Iteration 6000, Testing net (#0)
I0814 18:41:30.392645 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.817942
I0814 18:41:30.392665 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.988235
I0814 18:41:30.392673 10451 solver.cpp:594]     Test net output #2: loss = 0.568127 (* 1 = 0.568127 loss)
I0814 18:41:30.392690 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.778731s
I0814 18:41:30.411512 10451 solver.cpp:312] Iteration 6000 (42.7983 iter/s, 2.33654s/100 iter), loss = 0.226499
I0814 18:41:30.411559 10451 solver.cpp:334]     Train net output #0: loss = 0.226499 (* 1 = 0.226499 loss)
I0814 18:41:30.411573 10451 sgd_solver.cpp:136] Iteration 6000, lr = 0.090625, m = 0.9
I0814 18:41:31.964139 10451 solver.cpp:312] Iteration 6100 (64.409 iter/s, 1.55258s/100 iter), loss = 0.197113
I0814 18:41:31.964202 10451 solver.cpp:334]     Train net output #0: loss = 0.197112 (* 1 = 0.197112 loss)
I0814 18:41:31.964220 10451 sgd_solver.cpp:136] Iteration 6100, lr = 0.0904688, m = 0.9
I0814 18:41:33.564574 10451 solver.cpp:312] Iteration 6200 (62.4851 iter/s, 1.60038s/100 iter), loss = 0.41035
I0814 18:41:33.564636 10451 solver.cpp:334]     Train net output #0: loss = 0.410349 (* 1 = 0.410349 loss)
I0814 18:41:33.564656 10451 sgd_solver.cpp:136] Iteration 6200, lr = 0.0903125, m = 0.9
I0814 18:41:35.159553 10451 solver.cpp:312] Iteration 6300 (62.6987 iter/s, 1.59493s/100 iter), loss = 0.378435
I0814 18:41:35.159576 10451 solver.cpp:334]     Train net output #0: loss = 0.378435 (* 1 = 0.378435 loss)
I0814 18:41:35.159582 10451 sgd_solver.cpp:136] Iteration 6300, lr = 0.0901562, m = 0.9
I0814 18:41:36.694228 10451 solver.cpp:312] Iteration 6400 (65.1625 iter/s, 1.53463s/100 iter), loss = 0.439886
I0814 18:41:36.694288 10451 solver.cpp:334]     Train net output #0: loss = 0.439886 (* 1 = 0.439886 loss)
I0814 18:41:36.694315 10451 sgd_solver.cpp:136] Iteration 6400, lr = 0.09, m = 0.9
I0814 18:41:38.280915 10451 solver.cpp:312] Iteration 6500 (63.0264 iter/s, 1.58664s/100 iter), loss = 0.217451
I0814 18:41:38.280989 10451 solver.cpp:334]     Train net output #0: loss = 0.21745 (* 1 = 0.21745 loss)
I0814 18:41:38.281011 10451 sgd_solver.cpp:136] Iteration 6500, lr = 0.0898438, m = 0.9
I0814 18:41:39.870195 10451 solver.cpp:312] Iteration 6600 (62.9237 iter/s, 1.58923s/100 iter), loss = 0.241715
I0814 18:41:39.870220 10451 solver.cpp:334]     Train net output #0: loss = 0.241714 (* 1 = 0.241714 loss)
I0814 18:41:39.870225 10451 sgd_solver.cpp:136] Iteration 6600, lr = 0.0896875, m = 0.9
I0814 18:41:41.426816 10451 solver.cpp:312] Iteration 6700 (64.2438 iter/s, 1.55657s/100 iter), loss = 0.167078
I0814 18:41:41.426862 10451 solver.cpp:334]     Train net output #0: loss = 0.167078 (* 1 = 0.167078 loss)
I0814 18:41:41.426868 10451 sgd_solver.cpp:136] Iteration 6700, lr = 0.0895313, m = 0.9
I0814 18:41:43.009126 10451 solver.cpp:312] Iteration 6800 (63.2009 iter/s, 1.58226s/100 iter), loss = 0.244086
I0814 18:41:43.009188 10451 solver.cpp:334]     Train net output #0: loss = 0.244086 (* 1 = 0.244086 loss)
I0814 18:41:43.009207 10451 sgd_solver.cpp:136] Iteration 6800, lr = 0.089375, m = 0.9
I0814 18:41:44.591562 10451 solver.cpp:312] Iteration 6900 (63.1957 iter/s, 1.58239s/100 iter), loss = 0.279012
I0814 18:41:44.591609 10451 solver.cpp:334]     Train net output #0: loss = 0.279011 (* 1 = 0.279011 loss)
I0814 18:41:44.591620 10451 sgd_solver.cpp:136] Iteration 6900, lr = 0.0892188, m = 0.9
I0814 18:41:46.195098 10451 solver.cpp:509] Iteration 7000, Testing net (#0)
I0814 18:41:46.986227 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.762354
I0814 18:41:46.986244 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.985588
I0814 18:41:46.986249 10451 solver.cpp:594]     Test net output #2: loss = 0.790896 (* 1 = 0.790896 loss)
I0814 18:41:46.986265 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.791143s
I0814 18:41:47.001633 10451 solver.cpp:312] Iteration 7000 (41.4938 iter/s, 2.41s/100 iter), loss = 0.266948
I0814 18:41:47.001648 10451 solver.cpp:334]     Train net output #0: loss = 0.266948 (* 1 = 0.266948 loss)
I0814 18:41:47.001652 10451 sgd_solver.cpp:136] Iteration 7000, lr = 0.0890625, m = 0.9
I0814 18:41:48.573235 10451 solver.cpp:312] Iteration 7100 (63.6315 iter/s, 1.57155s/100 iter), loss = 0.125979
I0814 18:41:48.573297 10451 solver.cpp:334]     Train net output #0: loss = 0.125979 (* 1 = 0.125979 loss)
I0814 18:41:48.573317 10451 sgd_solver.cpp:136] Iteration 7100, lr = 0.0889063, m = 0.9
I0814 18:41:50.155082 10451 solver.cpp:312] Iteration 7200 (63.2193 iter/s, 1.58179s/100 iter), loss = 0.417545
I0814 18:41:50.155143 10451 solver.cpp:334]     Train net output #0: loss = 0.417545 (* 1 = 0.417545 loss)
I0814 18:41:50.155163 10451 sgd_solver.cpp:136] Iteration 7200, lr = 0.08875, m = 0.9
I0814 18:41:51.676388 10451 solver.cpp:312] Iteration 7300 (65.7351 iter/s, 1.52126s/100 iter), loss = 0.381239
I0814 18:41:51.676412 10451 solver.cpp:334]     Train net output #0: loss = 0.381238 (* 1 = 0.381238 loss)
I0814 18:41:51.676417 10451 sgd_solver.cpp:136] Iteration 7300, lr = 0.0885938, m = 0.9
I0814 18:41:53.289232 10451 solver.cpp:312] Iteration 7400 (62.0041 iter/s, 1.6128s/100 iter), loss = 0.0937643
I0814 18:41:53.289306 10451 solver.cpp:334]     Train net output #0: loss = 0.0937636 (* 1 = 0.0937636 loss)
I0814 18:41:53.289322 10451 sgd_solver.cpp:136] Iteration 7400, lr = 0.0884375, m = 0.9
I0814 18:41:54.847663 10451 solver.cpp:312] Iteration 7500 (64.1692 iter/s, 1.55838s/100 iter), loss = 0.0600123
I0814 18:41:54.847709 10451 solver.cpp:334]     Train net output #0: loss = 0.0600117 (* 1 = 0.0600117 loss)
I0814 18:41:54.847723 10451 sgd_solver.cpp:136] Iteration 7500, lr = 0.0882813, m = 0.9
I0814 18:41:56.385494 10451 solver.cpp:312] Iteration 7600 (65.0289 iter/s, 1.53778s/100 iter), loss = 0.281466
I0814 18:41:56.385562 10451 solver.cpp:334]     Train net output #0: loss = 0.281465 (* 1 = 0.281465 loss)
I0814 18:41:56.385591 10451 sgd_solver.cpp:136] Iteration 7600, lr = 0.088125, m = 0.9
I0814 18:41:57.929587 10451 solver.cpp:312] Iteration 7700 (64.765 iter/s, 1.54404s/100 iter), loss = 0.0670074
I0814 18:41:57.929612 10451 solver.cpp:334]     Train net output #0: loss = 0.0670067 (* 1 = 0.0670067 loss)
I0814 18:41:57.929618 10451 sgd_solver.cpp:136] Iteration 7700, lr = 0.0879688, m = 0.9
I0814 18:41:59.515604 10451 solver.cpp:312] Iteration 7800 (63.053 iter/s, 1.58597s/100 iter), loss = 0.364437
I0814 18:41:59.515647 10451 solver.cpp:334]     Train net output #0: loss = 0.364436 (* 1 = 0.364436 loss)
I0814 18:41:59.515660 10451 sgd_solver.cpp:136] Iteration 7800, lr = 0.0878125, m = 0.9
I0814 18:42:01.067558 10451 solver.cpp:312] Iteration 7900 (64.437 iter/s, 1.5519s/100 iter), loss = 0.307449
I0814 18:42:01.067582 10451 solver.cpp:334]     Train net output #0: loss = 0.307448 (* 1 = 0.307448 loss)
I0814 18:42:01.067589 10451 sgd_solver.cpp:136] Iteration 7900, lr = 0.0876563, m = 0.9
I0814 18:42:02.609865 10451 solver.cpp:509] Iteration 8000, Testing net (#0)
I0814 18:42:03.398140 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.772942
I0814 18:42:03.398160 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989706
I0814 18:42:03.398165 10451 solver.cpp:594]     Test net output #2: loss = 0.732179 (* 1 = 0.732179 loss)
I0814 18:42:03.398180 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.788294s
I0814 18:42:03.415071 10451 solver.cpp:312] Iteration 8000 (42.5995 iter/s, 2.34745s/100 iter), loss = 0.307844
I0814 18:42:03.415088 10451 solver.cpp:334]     Train net output #0: loss = 0.307843 (* 1 = 0.307843 loss)
I0814 18:42:03.415094 10451 sgd_solver.cpp:136] Iteration 8000, lr = 0.0875, m = 0.9
I0814 18:42:04.968680 10451 solver.cpp:312] Iteration 8100 (64.3685 iter/s, 1.55356s/100 iter), loss = 0.422737
I0814 18:42:04.968706 10451 solver.cpp:334]     Train net output #0: loss = 0.422736 (* 1 = 0.422736 loss)
I0814 18:42:04.968711 10451 sgd_solver.cpp:136] Iteration 8100, lr = 0.0873438, m = 0.9
I0814 18:42:06.531409 10451 solver.cpp:312] Iteration 8200 (63.9926 iter/s, 1.56268s/100 iter), loss = 0.0939769
I0814 18:42:06.531435 10451 solver.cpp:334]     Train net output #0: loss = 0.0939758 (* 1 = 0.0939758 loss)
I0814 18:42:06.531440 10451 sgd_solver.cpp:136] Iteration 8200, lr = 0.0871875, m = 0.9
I0814 18:42:08.141767 10451 solver.cpp:312] Iteration 8300 (62.1 iter/s, 1.61031s/100 iter), loss = 0.520569
I0814 18:42:08.141815 10451 solver.cpp:334]     Train net output #0: loss = 0.520568 (* 1 = 0.520568 loss)
I0814 18:42:08.141829 10451 sgd_solver.cpp:136] Iteration 8300, lr = 0.0870313, m = 0.9
I0814 18:42:09.793764 10451 solver.cpp:312] Iteration 8400 (60.5348 iter/s, 1.65194s/100 iter), loss = 0.0569333
I0814 18:42:09.793788 10451 solver.cpp:334]     Train net output #0: loss = 0.056932 (* 1 = 0.056932 loss)
I0814 18:42:09.793793 10451 sgd_solver.cpp:136] Iteration 8400, lr = 0.086875, m = 0.9
I0814 18:42:11.361608 10451 solver.cpp:312] Iteration 8500 (63.7838 iter/s, 1.5678s/100 iter), loss = 0.360311
I0814 18:42:11.361654 10451 solver.cpp:334]     Train net output #0: loss = 0.36031 (* 1 = 0.36031 loss)
I0814 18:42:11.361665 10451 sgd_solver.cpp:136] Iteration 8500, lr = 0.0867188, m = 0.9
I0814 18:42:13.033681 10451 solver.cpp:312] Iteration 8600 (59.8079 iter/s, 1.67202s/100 iter), loss = 0.141279
I0814 18:42:13.033720 10451 solver.cpp:334]     Train net output #0: loss = 0.141278 (* 1 = 0.141278 loss)
I0814 18:42:13.033726 10451 sgd_solver.cpp:136] Iteration 8600, lr = 0.0865625, m = 0.9
I0814 18:42:14.703764 10451 solver.cpp:312] Iteration 8700 (59.8791 iter/s, 1.67003s/100 iter), loss = 0.258948
I0814 18:42:14.703789 10451 solver.cpp:334]     Train net output #0: loss = 0.258947 (* 1 = 0.258947 loss)
I0814 18:42:14.703794 10451 sgd_solver.cpp:136] Iteration 8700, lr = 0.0864063, m = 0.9
I0814 18:42:16.388916 10451 solver.cpp:312] Iteration 8800 (59.3437 iter/s, 1.6851s/100 iter), loss = 0.316296
I0814 18:42:16.388941 10451 solver.cpp:334]     Train net output #0: loss = 0.316295 (* 1 = 0.316295 loss)
I0814 18:42:16.388945 10451 sgd_solver.cpp:136] Iteration 8800, lr = 0.08625, m = 0.9
I0814 18:42:18.097640 10451 solver.cpp:312] Iteration 8900 (58.5251 iter/s, 1.70867s/100 iter), loss = 0.216375
I0814 18:42:18.097663 10451 solver.cpp:334]     Train net output #0: loss = 0.216374 (* 1 = 0.216374 loss)
I0814 18:42:18.097674 10451 sgd_solver.cpp:136] Iteration 8900, lr = 0.0860937, m = 0.9
I0814 18:42:19.738920 10451 solver.cpp:509] Iteration 9000, Testing net (#0)
I0814 18:42:20.559623 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.682059
I0814 18:42:20.559641 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.967353
I0814 18:42:20.559646 10451 solver.cpp:594]     Test net output #2: loss = 1.38103 (* 1 = 1.38103 loss)
I0814 18:42:20.559661 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.820718s
I0814 18:42:20.577886 10451 solver.cpp:312] Iteration 9000 (40.3198 iter/s, 2.48017s/100 iter), loss = 0.155422
I0814 18:42:20.577904 10451 solver.cpp:334]     Train net output #0: loss = 0.15542 (* 1 = 0.15542 loss)
I0814 18:42:20.577909 10451 sgd_solver.cpp:136] Iteration 9000, lr = 0.0859375, m = 0.9
I0814 18:42:22.005168 10436 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 18:42:22.210474 10451 solver.cpp:312] Iteration 9100 (61.2544 iter/s, 1.63254s/100 iter), loss = 0.29371
I0814 18:42:22.210497 10451 solver.cpp:334]     Train net output #0: loss = 0.293708 (* 1 = 0.293708 loss)
I0814 18:42:22.210503 10451 sgd_solver.cpp:136] Iteration 9100, lr = 0.0857813, m = 0.9
I0814 18:42:23.834656 10451 solver.cpp:312] Iteration 9200 (61.5714 iter/s, 1.62413s/100 iter), loss = 0.387051
I0814 18:42:23.834746 10451 solver.cpp:334]     Train net output #0: loss = 0.387049 (* 1 = 0.387049 loss)
I0814 18:42:23.834763 10451 sgd_solver.cpp:136] Iteration 9200, lr = 0.085625, m = 0.9
I0814 18:42:25.492686 10451 solver.cpp:312] Iteration 9300 (60.3145 iter/s, 1.65797s/100 iter), loss = 0.194589
I0814 18:42:25.492707 10451 solver.cpp:334]     Train net output #0: loss = 0.194587 (* 1 = 0.194587 loss)
I0814 18:42:25.492710 10451 sgd_solver.cpp:136] Iteration 9300, lr = 0.0854688, m = 0.9
I0814 18:42:27.129860 10451 solver.cpp:312] Iteration 9400 (61.0828 iter/s, 1.63712s/100 iter), loss = 0.40611
I0814 18:42:27.129885 10451 solver.cpp:334]     Train net output #0: loss = 0.406108 (* 1 = 0.406108 loss)
I0814 18:42:27.129891 10451 sgd_solver.cpp:136] Iteration 9400, lr = 0.0853125, m = 0.9
I0814 18:42:28.810974 10451 solver.cpp:312] Iteration 9500 (59.4863 iter/s, 1.68106s/100 iter), loss = 0.136916
I0814 18:42:28.810999 10451 solver.cpp:334]     Train net output #0: loss = 0.136915 (* 1 = 0.136915 loss)
I0814 18:42:28.811004 10451 sgd_solver.cpp:136] Iteration 9500, lr = 0.0851563, m = 0.9
I0814 18:42:30.444152 10451 solver.cpp:312] Iteration 9600 (61.2322 iter/s, 1.63313s/100 iter), loss = 0.282278
I0814 18:42:30.444290 10451 solver.cpp:334]     Train net output #0: loss = 0.282276 (* 1 = 0.282276 loss)
I0814 18:42:30.444309 10451 sgd_solver.cpp:136] Iteration 9600, lr = 0.085, m = 0.9
I0814 18:42:32.056471 10451 solver.cpp:312] Iteration 9700 (62.0245 iter/s, 1.61227s/100 iter), loss = 0.184055
I0814 18:42:32.056535 10451 solver.cpp:334]     Train net output #0: loss = 0.184054 (* 1 = 0.184054 loss)
I0814 18:42:32.056556 10451 sgd_solver.cpp:136] Iteration 9700, lr = 0.0848437, m = 0.9
I0814 18:42:33.752081 10451 solver.cpp:312] Iteration 9800 (58.9778 iter/s, 1.69555s/100 iter), loss = 0.0626927
I0814 18:42:33.752127 10451 solver.cpp:334]     Train net output #0: loss = 0.0626912 (* 1 = 0.0626912 loss)
I0814 18:42:33.752146 10451 sgd_solver.cpp:136] Iteration 9800, lr = 0.0846875, m = 0.9
I0814 18:42:35.394700 10451 solver.cpp:312] Iteration 9900 (60.8804 iter/s, 1.64256s/100 iter), loss = 0.0313934
I0814 18:42:35.394722 10451 solver.cpp:334]     Train net output #0: loss = 0.0313921 (* 1 = 0.0313921 loss)
I0814 18:42:35.394727 10451 sgd_solver.cpp:136] Iteration 9900, lr = 0.0845312, m = 0.9
I0814 18:42:37.043603 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_10000.caffemodel
I0814 18:42:37.060371 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_10000.solverstate
I0814 18:42:37.064101 10451 solver.cpp:509] Iteration 10000, Testing net (#0)
I0814 18:42:37.448909 10459 blocking_queue.cpp:40] Waiting for datum
I0814 18:42:37.888200 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.79206
I0814 18:42:37.888221 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.988529
I0814 18:42:37.888227 10451 solver.cpp:594]     Test net output #2: loss = 0.65715 (* 1 = 0.65715 loss)
I0814 18:42:37.888248 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.824122s
I0814 18:42:37.908818 10451 solver.cpp:312] Iteration 10000 (39.7765 iter/s, 2.51405s/100 iter), loss = 0.164429
I0814 18:42:37.908834 10451 solver.cpp:334]     Train net output #0: loss = 0.164428 (* 1 = 0.164428 loss)
I0814 18:42:37.908840 10451 sgd_solver.cpp:136] Iteration 10000, lr = 0.084375, m = 0.9
I0814 18:42:39.556673 10451 solver.cpp:312] Iteration 10100 (60.687 iter/s, 1.6478s/100 iter), loss = 0.238966
I0814 18:42:39.556720 10451 solver.cpp:334]     Train net output #0: loss = 0.238965 (* 1 = 0.238965 loss)
I0814 18:42:39.556730 10451 sgd_solver.cpp:136] Iteration 10100, lr = 0.0842188, m = 0.9
I0814 18:42:41.252315 10451 solver.cpp:312] Iteration 10200 (58.9764 iter/s, 1.69559s/100 iter), loss = 0.106979
I0814 18:42:41.252338 10451 solver.cpp:334]     Train net output #0: loss = 0.106978 (* 1 = 0.106978 loss)
I0814 18:42:41.252344 10451 sgd_solver.cpp:136] Iteration 10200, lr = 0.0840625, m = 0.9
I0814 18:42:42.969885 10451 solver.cpp:312] Iteration 10300 (58.2236 iter/s, 1.71752s/100 iter), loss = 0.229476
I0814 18:42:42.969910 10451 solver.cpp:334]     Train net output #0: loss = 0.229475 (* 1 = 0.229475 loss)
I0814 18:42:42.969915 10451 sgd_solver.cpp:136] Iteration 10300, lr = 0.0839063, m = 0.9
I0814 18:42:44.644680 10451 solver.cpp:312] Iteration 10400 (59.7108 iter/s, 1.67474s/100 iter), loss = 0.1535
I0814 18:42:44.644704 10451 solver.cpp:334]     Train net output #0: loss = 0.153498 (* 1 = 0.153498 loss)
I0814 18:42:44.644711 10451 sgd_solver.cpp:136] Iteration 10400, lr = 0.08375, m = 0.9
I0814 18:42:46.298245 10451 solver.cpp:312] Iteration 10500 (60.4772 iter/s, 1.65352s/100 iter), loss = 0.267838
I0814 18:42:46.298306 10451 solver.cpp:334]     Train net output #0: loss = 0.267837 (* 1 = 0.267837 loss)
I0814 18:42:46.298326 10451 sgd_solver.cpp:136] Iteration 10500, lr = 0.0835937, m = 0.9
I0814 18:42:47.949879 10451 solver.cpp:312] Iteration 10600 (60.548 iter/s, 1.65158s/100 iter), loss = 0.427453
I0814 18:42:47.949905 10451 solver.cpp:334]     Train net output #0: loss = 0.427452 (* 1 = 0.427452 loss)
I0814 18:42:47.949913 10451 sgd_solver.cpp:136] Iteration 10600, lr = 0.0834375, m = 0.9
I0814 18:42:49.559361 10451 solver.cpp:312] Iteration 10700 (62.1339 iter/s, 1.60943s/100 iter), loss = 0.179563
I0814 18:42:49.559386 10451 solver.cpp:334]     Train net output #0: loss = 0.179561 (* 1 = 0.179561 loss)
I0814 18:42:49.559391 10451 sgd_solver.cpp:136] Iteration 10700, lr = 0.0832812, m = 0.9
I0814 18:42:51.157871 10451 solver.cpp:312] Iteration 10800 (62.5602 iter/s, 1.59846s/100 iter), loss = 0.354537
I0814 18:42:51.157933 10451 solver.cpp:334]     Train net output #0: loss = 0.354535 (* 1 = 0.354535 loss)
I0814 18:42:51.157953 10451 sgd_solver.cpp:136] Iteration 10800, lr = 0.083125, m = 0.9
I0814 18:42:52.800031 10451 solver.cpp:312] Iteration 10900 (60.8974 iter/s, 1.64211s/100 iter), loss = 0.102046
I0814 18:42:52.800079 10451 solver.cpp:334]     Train net output #0: loss = 0.102044 (* 1 = 0.102044 loss)
I0814 18:42:52.800094 10451 sgd_solver.cpp:136] Iteration 10900, lr = 0.0829687, m = 0.9
I0814 18:42:54.376484 10451 solver.cpp:509] Iteration 11000, Testing net (#0)
I0814 18:42:55.194793 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.764119
I0814 18:42:55.194813 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.988235
I0814 18:42:55.194820 10451 solver.cpp:594]     Test net output #2: loss = 0.865669 (* 1 = 0.865669 loss)
I0814 18:42:55.194836 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.818329s
I0814 18:42:55.210435 10451 solver.cpp:312] Iteration 11000 (41.488 iter/s, 2.41033s/100 iter), loss = 0.144268
I0814 18:42:55.210451 10451 solver.cpp:334]     Train net output #0: loss = 0.144266 (* 1 = 0.144266 loss)
I0814 18:42:55.210456 10451 sgd_solver.cpp:136] Iteration 11000, lr = 0.0828125, m = 0.9
I0814 18:42:56.799346 10451 solver.cpp:312] Iteration 11100 (62.9383 iter/s, 1.58886s/100 iter), loss = 0.0752977
I0814 18:42:56.799368 10451 solver.cpp:334]     Train net output #0: loss = 0.0752961 (* 1 = 0.0752961 loss)
I0814 18:42:56.799373 10451 sgd_solver.cpp:136] Iteration 11100, lr = 0.0826563, m = 0.9
I0814 18:42:58.411156 10451 solver.cpp:312] Iteration 11200 (62.0439 iter/s, 1.61176s/100 iter), loss = 0.140167
I0814 18:42:58.411181 10451 solver.cpp:334]     Train net output #0: loss = 0.140165 (* 1 = 0.140165 loss)
I0814 18:42:58.411187 10451 sgd_solver.cpp:136] Iteration 11200, lr = 0.0825, m = 0.9
I0814 18:42:59.996963 10451 solver.cpp:312] Iteration 11300 (63.0614 iter/s, 1.58576s/100 iter), loss = 0.0883709
I0814 18:42:59.996986 10451 solver.cpp:334]     Train net output #0: loss = 0.0883693 (* 1 = 0.0883693 loss)
I0814 18:42:59.996991 10451 sgd_solver.cpp:136] Iteration 11300, lr = 0.0823437, m = 0.9
I0814 18:43:01.648751 10451 solver.cpp:312] Iteration 11400 (60.5425 iter/s, 1.65173s/100 iter), loss = 0.0467752
I0814 18:43:01.648777 10451 solver.cpp:334]     Train net output #0: loss = 0.0467737 (* 1 = 0.0467737 loss)
I0814 18:43:01.648784 10451 sgd_solver.cpp:136] Iteration 11400, lr = 0.0821875, m = 0.9
I0814 18:43:03.298830 10451 solver.cpp:312] Iteration 11500 (60.6051 iter/s, 1.65003s/100 iter), loss = 0.298181
I0814 18:43:03.298894 10451 solver.cpp:334]     Train net output #0: loss = 0.29818 (* 1 = 0.29818 loss)
I0814 18:43:03.298913 10451 sgd_solver.cpp:136] Iteration 11500, lr = 0.0820312, m = 0.9
I0814 18:43:04.937732 10451 solver.cpp:312] Iteration 11600 (61.0184 iter/s, 1.63885s/100 iter), loss = 0.364951
I0814 18:43:04.937782 10451 solver.cpp:334]     Train net output #0: loss = 0.364949 (* 1 = 0.364949 loss)
I0814 18:43:04.937795 10451 sgd_solver.cpp:136] Iteration 11600, lr = 0.081875, m = 0.9
I0814 18:43:06.612839 10451 solver.cpp:312] Iteration 11700 (59.6996 iter/s, 1.67505s/100 iter), loss = 0.174459
I0814 18:43:06.612864 10451 solver.cpp:334]     Train net output #0: loss = 0.174458 (* 1 = 0.174458 loss)
I0814 18:43:06.612869 10451 sgd_solver.cpp:136] Iteration 11700, lr = 0.0817188, m = 0.9
I0814 18:43:08.244334 10451 solver.cpp:312] Iteration 11800 (61.2955 iter/s, 1.63144s/100 iter), loss = 0.0788061
I0814 18:43:08.244385 10451 solver.cpp:334]     Train net output #0: loss = 0.0788044 (* 1 = 0.0788044 loss)
I0814 18:43:08.244398 10451 sgd_solver.cpp:136] Iteration 11800, lr = 0.0815625, m = 0.9
I0814 18:43:09.912720 10451 solver.cpp:312] Iteration 11900 (59.9401 iter/s, 1.66833s/100 iter), loss = 0.157115
I0814 18:43:09.912750 10451 solver.cpp:334]     Train net output #0: loss = 0.157114 (* 1 = 0.157114 loss)
I0814 18:43:09.912757 10451 sgd_solver.cpp:136] Iteration 11900, lr = 0.0814063, m = 0.9
I0814 18:43:11.491199 10451 solver.cpp:509] Iteration 12000, Testing net (#0)
I0814 18:43:12.314981 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.737648
I0814 18:43:12.315001 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.98
I0814 18:43:12.315006 10451 solver.cpp:594]     Test net output #2: loss = 1.00164 (* 1 = 1.00164 loss)
I0814 18:43:12.315096 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.823873s
I0814 18:43:12.330720 10451 solver.cpp:312] Iteration 12000 (41.3577 iter/s, 2.41793s/100 iter), loss = 0.045548
I0814 18:43:12.330749 10451 solver.cpp:334]     Train net output #0: loss = 0.0455463 (* 1 = 0.0455463 loss)
I0814 18:43:12.330775 10451 sgd_solver.cpp:136] Iteration 12000, lr = 0.08125, m = 0.9
I0814 18:43:13.936175 10451 solver.cpp:312] Iteration 12100 (62.2898 iter/s, 1.6054s/100 iter), loss = 0.0464011
I0814 18:43:13.936200 10451 solver.cpp:334]     Train net output #0: loss = 0.0463995 (* 1 = 0.0463995 loss)
I0814 18:43:13.936205 10451 sgd_solver.cpp:136] Iteration 12100, lr = 0.0810938, m = 0.9
I0814 18:43:15.583387 10451 solver.cpp:312] Iteration 12200 (60.7106 iter/s, 1.64716s/100 iter), loss = 0.117088
I0814 18:43:15.583411 10451 solver.cpp:334]     Train net output #0: loss = 0.117087 (* 1 = 0.117087 loss)
I0814 18:43:15.583416 10451 sgd_solver.cpp:136] Iteration 12200, lr = 0.0809375, m = 0.9
I0814 18:43:17.200248 10451 solver.cpp:312] Iteration 12300 (61.8502 iter/s, 1.61681s/100 iter), loss = 0.190491
I0814 18:43:17.200274 10451 solver.cpp:334]     Train net output #0: loss = 0.190489 (* 1 = 0.190489 loss)
I0814 18:43:17.200280 10451 sgd_solver.cpp:136] Iteration 12300, lr = 0.0807813, m = 0.9
I0814 18:43:18.808799 10451 solver.cpp:312] Iteration 12400 (62.1698 iter/s, 1.6085s/100 iter), loss = 0.201828
I0814 18:43:18.808825 10451 solver.cpp:334]     Train net output #0: loss = 0.201827 (* 1 = 0.201827 loss)
I0814 18:43:18.808830 10451 sgd_solver.cpp:136] Iteration 12400, lr = 0.080625, m = 0.9
I0814 18:43:20.406674 10451 solver.cpp:312] Iteration 12500 (62.5851 iter/s, 1.59782s/100 iter), loss = 0.0807709
I0814 18:43:20.406697 10451 solver.cpp:334]     Train net output #0: loss = 0.0807695 (* 1 = 0.0807695 loss)
I0814 18:43:20.406702 10451 sgd_solver.cpp:136] Iteration 12500, lr = 0.0804688, m = 0.9
I0814 18:43:22.025337 10451 solver.cpp:312] Iteration 12600 (61.7813 iter/s, 1.61861s/100 iter), loss = 0.317818
I0814 18:43:22.025360 10451 solver.cpp:334]     Train net output #0: loss = 0.317817 (* 1 = 0.317817 loss)
I0814 18:43:22.025365 10451 sgd_solver.cpp:136] Iteration 12600, lr = 0.0803125, m = 0.9
I0814 18:43:23.662329 10451 solver.cpp:312] Iteration 12700 (61.0895 iter/s, 1.63694s/100 iter), loss = 0.145552
I0814 18:43:23.662377 10451 solver.cpp:334]     Train net output #0: loss = 0.145551 (* 1 = 0.145551 loss)
I0814 18:43:23.662390 10451 sgd_solver.cpp:136] Iteration 12700, lr = 0.0801563, m = 0.9
I0814 18:43:25.286761 10451 solver.cpp:312] Iteration 12800 (61.5619 iter/s, 1.62438s/100 iter), loss = 0.241692
I0814 18:43:25.286813 10451 solver.cpp:334]     Train net output #0: loss = 0.24169 (* 1 = 0.24169 loss)
I0814 18:43:25.286818 10451 sgd_solver.cpp:136] Iteration 12800, lr = 0.08, m = 0.9
I0814 18:43:26.869617 10451 solver.cpp:312] Iteration 12900 (63.1791 iter/s, 1.5828s/100 iter), loss = 0.0441889
I0814 18:43:26.869643 10451 solver.cpp:334]     Train net output #0: loss = 0.0441876 (* 1 = 0.0441876 loss)
I0814 18:43:26.869649 10451 sgd_solver.cpp:136] Iteration 12900, lr = 0.0798438, m = 0.9
I0814 18:43:28.460041 10451 solver.cpp:509] Iteration 13000, Testing net (#0)
I0814 18:43:29.278812 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.82853
I0814 18:43:29.278838 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.990588
I0814 18:43:29.278847 10451 solver.cpp:594]     Test net output #2: loss = 0.59088 (* 1 = 0.59088 loss)
I0814 18:43:29.278872 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.818807s
I0814 18:43:29.296460 10451 solver.cpp:312] Iteration 13000 (41.2071 iter/s, 2.42677s/100 iter), loss = 0.0993569
I0814 18:43:29.296739 10451 solver.cpp:334]     Train net output #0: loss = 0.0993555 (* 1 = 0.0993555 loss)
I0814 18:43:29.296747 10451 sgd_solver.cpp:136] Iteration 13000, lr = 0.0796875, m = 0.9
I0814 18:43:30.927083 10451 solver.cpp:312] Iteration 13100 (61.3282 iter/s, 1.63057s/100 iter), loss = 0.261867
I0814 18:43:30.927129 10451 solver.cpp:334]     Train net output #0: loss = 0.261866 (* 1 = 0.261866 loss)
I0814 18:43:30.927141 10451 sgd_solver.cpp:136] Iteration 13100, lr = 0.0795313, m = 0.9
I0814 18:43:32.556668 10451 solver.cpp:312] Iteration 13200 (61.3674 iter/s, 1.62953s/100 iter), loss = 0.254124
I0814 18:43:32.556747 10451 solver.cpp:334]     Train net output #0: loss = 0.254123 (* 1 = 0.254123 loss)
I0814 18:43:32.556756 10451 sgd_solver.cpp:136] Iteration 13200, lr = 0.079375, m = 0.9
I0814 18:43:34.177989 10451 solver.cpp:312] Iteration 13300 (61.68 iter/s, 1.62127s/100 iter), loss = 0.0747326
I0814 18:43:34.178036 10451 solver.cpp:334]     Train net output #0: loss = 0.0747312 (* 1 = 0.0747312 loss)
I0814 18:43:34.178048 10451 sgd_solver.cpp:136] Iteration 13300, lr = 0.0792188, m = 0.9
I0814 18:43:35.830471 10451 solver.cpp:312] Iteration 13400 (60.5169 iter/s, 1.65243s/100 iter), loss = 0.0421931
I0814 18:43:35.830497 10451 solver.cpp:334]     Train net output #0: loss = 0.0421917 (* 1 = 0.0421917 loss)
I0814 18:43:35.830500 10451 sgd_solver.cpp:136] Iteration 13400, lr = 0.0790625, m = 0.9
I0814 18:43:37.406162 10451 solver.cpp:312] Iteration 13500 (63.4661 iter/s, 1.57564s/100 iter), loss = 0.152179
I0814 18:43:37.406185 10451 solver.cpp:334]     Train net output #0: loss = 0.152178 (* 1 = 0.152178 loss)
I0814 18:43:37.406191 10451 sgd_solver.cpp:136] Iteration 13500, lr = 0.0789063, m = 0.9
I0814 18:43:39.045737 10451 solver.cpp:312] Iteration 13600 (60.9934 iter/s, 1.63952s/100 iter), loss = 0.0251957
I0814 18:43:39.045759 10451 solver.cpp:334]     Train net output #0: loss = 0.0251941 (* 1 = 0.0251941 loss)
I0814 18:43:39.045765 10451 sgd_solver.cpp:136] Iteration 13600, lr = 0.07875, m = 0.9
I0814 18:43:39.584765 10436 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 18:43:40.660521 10451 solver.cpp:312] Iteration 13700 (61.9297 iter/s, 1.61473s/100 iter), loss = 0.164027
I0814 18:43:40.660542 10451 solver.cpp:334]     Train net output #0: loss = 0.164026 (* 1 = 0.164026 loss)
I0814 18:43:40.660548 10451 sgd_solver.cpp:136] Iteration 13700, lr = 0.0785938, m = 0.9
I0814 18:43:42.255220 10451 solver.cpp:312] Iteration 13800 (62.7098 iter/s, 1.59465s/100 iter), loss = 0.0417317
I0814 18:43:42.255244 10451 solver.cpp:334]     Train net output #0: loss = 0.0417302 (* 1 = 0.0417302 loss)
I0814 18:43:42.255250 10451 sgd_solver.cpp:136] Iteration 13800, lr = 0.0784375, m = 0.9
I0814 18:43:43.862349 10451 solver.cpp:312] Iteration 13900 (62.2248 iter/s, 1.60708s/100 iter), loss = 0.0977415
I0814 18:43:43.862373 10451 solver.cpp:334]     Train net output #0: loss = 0.09774 (* 1 = 0.09774 loss)
I0814 18:43:43.862380 10451 sgd_solver.cpp:136] Iteration 13900, lr = 0.0782812, m = 0.9
I0814 18:43:45.499514 10451 solver.cpp:509] Iteration 14000, Testing net (#0)
I0814 18:43:46.314064 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.77353
I0814 18:43:46.314085 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.969412
I0814 18:43:46.314090 10451 solver.cpp:594]     Test net output #2: loss = 0.967167 (* 1 = 0.967167 loss)
I0814 18:43:46.314105 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.814565s
I0814 18:43:46.330008 10451 solver.cpp:312] Iteration 14000 (40.5254 iter/s, 2.46759s/100 iter), loss = 0.0452544
I0814 18:43:46.330024 10451 solver.cpp:334]     Train net output #0: loss = 0.0452529 (* 1 = 0.0452529 loss)
I0814 18:43:46.330029 10451 sgd_solver.cpp:136] Iteration 14000, lr = 0.078125, m = 0.9
I0814 18:43:47.973815 10451 solver.cpp:312] Iteration 14100 (60.8365 iter/s, 1.64375s/100 iter), loss = 0.112046
I0814 18:43:47.973841 10451 solver.cpp:334]     Train net output #0: loss = 0.112044 (* 1 = 0.112044 loss)
I0814 18:43:47.973847 10451 sgd_solver.cpp:136] Iteration 14100, lr = 0.0779688, m = 0.9
I0814 18:43:49.573822 10451 solver.cpp:312] Iteration 14200 (62.5017 iter/s, 1.59996s/100 iter), loss = 0.0277375
I0814 18:43:49.573845 10451 solver.cpp:334]     Train net output #0: loss = 0.0277361 (* 1 = 0.0277361 loss)
I0814 18:43:49.573851 10451 sgd_solver.cpp:136] Iteration 14200, lr = 0.0778125, m = 0.9
I0814 18:43:51.202647 10451 solver.cpp:312] Iteration 14300 (61.3959 iter/s, 1.62877s/100 iter), loss = 0.091118
I0814 18:43:51.202672 10451 solver.cpp:334]     Train net output #0: loss = 0.0911165 (* 1 = 0.0911165 loss)
I0814 18:43:51.202677 10451 sgd_solver.cpp:136] Iteration 14300, lr = 0.0776563, m = 0.9
I0814 18:43:52.814159 10451 solver.cpp:312] Iteration 14400 (62.0555 iter/s, 1.61146s/100 iter), loss = 0.248979
I0814 18:43:52.814213 10451 solver.cpp:334]     Train net output #0: loss = 0.248978 (* 1 = 0.248978 loss)
I0814 18:43:52.814224 10451 sgd_solver.cpp:136] Iteration 14400, lr = 0.0775, m = 0.9
I0814 18:43:54.461863 10451 solver.cpp:312] Iteration 14500 (60.6925 iter/s, 1.64765s/100 iter), loss = 0.0455427
I0814 18:43:54.461913 10451 solver.cpp:334]     Train net output #0: loss = 0.0455412 (* 1 = 0.0455412 loss)
I0814 18:43:54.461927 10451 sgd_solver.cpp:136] Iteration 14500, lr = 0.0773438, m = 0.9
I0814 18:43:56.119771 10451 solver.cpp:312] Iteration 14600 (60.3189 iter/s, 1.65786s/100 iter), loss = 0.209466
I0814 18:43:56.119876 10451 solver.cpp:334]     Train net output #0: loss = 0.209465 (* 1 = 0.209465 loss)
I0814 18:43:56.119889 10451 sgd_solver.cpp:136] Iteration 14600, lr = 0.0771875, m = 0.9
I0814 18:43:57.740177 10451 solver.cpp:312] Iteration 14700 (61.7148 iter/s, 1.62036s/100 iter), loss = 0.259719
I0814 18:43:57.740201 10451 solver.cpp:334]     Train net output #0: loss = 0.259718 (* 1 = 0.259718 loss)
I0814 18:43:57.740206 10451 sgd_solver.cpp:136] Iteration 14700, lr = 0.0770312, m = 0.9
I0814 18:43:59.362427 10451 solver.cpp:312] Iteration 14800 (61.6448 iter/s, 1.6222s/100 iter), loss = 0.138027
I0814 18:43:59.362450 10451 solver.cpp:334]     Train net output #0: loss = 0.138025 (* 1 = 0.138025 loss)
I0814 18:43:59.362457 10451 sgd_solver.cpp:136] Iteration 14800, lr = 0.076875, m = 0.9
I0814 18:44:01.012573 10451 solver.cpp:312] Iteration 14900 (60.6026 iter/s, 1.6501s/100 iter), loss = 0.111309
I0814 18:44:01.012600 10451 solver.cpp:334]     Train net output #0: loss = 0.111308 (* 1 = 0.111308 loss)
I0814 18:44:01.012607 10451 sgd_solver.cpp:136] Iteration 14900, lr = 0.0767187, m = 0.9
I0814 18:44:02.652148 10451 solver.cpp:509] Iteration 15000, Testing net (#0)
I0814 18:44:03.489470 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.78559
I0814 18:44:03.489488 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.974412
I0814 18:44:03.489493 10451 solver.cpp:594]     Test net output #2: loss = 0.934723 (* 1 = 0.934723 loss)
I0814 18:44:03.489508 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.837338s
I0814 18:44:03.505002 10451 solver.cpp:312] Iteration 15000 (40.1228 iter/s, 2.49235s/100 iter), loss = 0.0478968
I0814 18:44:03.505020 10451 solver.cpp:334]     Train net output #0: loss = 0.0478953 (* 1 = 0.0478953 loss)
I0814 18:44:03.505026 10451 sgd_solver.cpp:136] Iteration 15000, lr = 0.0765625, m = 0.9
I0814 18:44:05.123206 10451 solver.cpp:312] Iteration 15100 (61.7989 iter/s, 1.61815s/100 iter), loss = 0.0737568
I0814 18:44:05.123251 10451 solver.cpp:334]     Train net output #0: loss = 0.0737554 (* 1 = 0.0737554 loss)
I0814 18:44:05.123265 10451 sgd_solver.cpp:136] Iteration 15100, lr = 0.0764063, m = 0.9
I0814 18:44:06.732506 10451 solver.cpp:312] Iteration 15200 (62.1409 iter/s, 1.60925s/100 iter), loss = 0.117436
I0814 18:44:06.732530 10451 solver.cpp:334]     Train net output #0: loss = 0.117435 (* 1 = 0.117435 loss)
I0814 18:44:06.732535 10451 sgd_solver.cpp:136] Iteration 15200, lr = 0.07625, m = 0.9
I0814 18:44:08.373384 10451 solver.cpp:312] Iteration 15300 (60.9449 iter/s, 1.64083s/100 iter), loss = 0.0917573
I0814 18:44:08.373409 10451 solver.cpp:334]     Train net output #0: loss = 0.0917559 (* 1 = 0.0917559 loss)
I0814 18:44:08.373414 10451 sgd_solver.cpp:136] Iteration 15300, lr = 0.0760938, m = 0.9
I0814 18:44:10.018666 10451 solver.cpp:312] Iteration 15400 (60.7818 iter/s, 1.64523s/100 iter), loss = 0.285941
I0814 18:44:10.018689 10451 solver.cpp:334]     Train net output #0: loss = 0.285939 (* 1 = 0.285939 loss)
I0814 18:44:10.018695 10451 sgd_solver.cpp:136] Iteration 15400, lr = 0.0759375, m = 0.9
I0814 18:44:11.671264 10451 solver.cpp:312] Iteration 15500 (60.5126 iter/s, 1.65255s/100 iter), loss = 0.0659411
I0814 18:44:11.671326 10451 solver.cpp:334]     Train net output #0: loss = 0.0659397 (* 1 = 0.0659397 loss)
I0814 18:44:11.671344 10451 sgd_solver.cpp:136] Iteration 15500, lr = 0.0757812, m = 0.9
I0814 18:44:13.316429 10451 solver.cpp:312] Iteration 15600 (60.7861 iter/s, 1.64511s/100 iter), loss = 0.186997
I0814 18:44:13.316478 10451 solver.cpp:334]     Train net output #0: loss = 0.186995 (* 1 = 0.186995 loss)
I0814 18:44:13.316493 10451 sgd_solver.cpp:136] Iteration 15600, lr = 0.075625, m = 0.9
I0814 18:44:14.942283 10451 solver.cpp:312] Iteration 15700 (61.508 iter/s, 1.6258s/100 iter), loss = 0.218465
I0814 18:44:14.942307 10451 solver.cpp:334]     Train net output #0: loss = 0.218463 (* 1 = 0.218463 loss)
I0814 18:44:14.942312 10451 sgd_solver.cpp:136] Iteration 15700, lr = 0.0754687, m = 0.9
I0814 18:44:16.556831 10451 solver.cpp:312] Iteration 15800 (61.9388 iter/s, 1.6145s/100 iter), loss = 0.198966
I0814 18:44:16.557001 10451 solver.cpp:334]     Train net output #0: loss = 0.198965 (* 1 = 0.198965 loss)
I0814 18:44:16.557025 10451 sgd_solver.cpp:136] Iteration 15800, lr = 0.0753125, m = 0.9
I0814 18:44:18.187975 10451 solver.cpp:312] Iteration 15900 (61.3087 iter/s, 1.63109s/100 iter), loss = 0.139731
I0814 18:44:18.188032 10451 solver.cpp:334]     Train net output #0: loss = 0.139729 (* 1 = 0.139729 loss)
I0814 18:44:18.188050 10451 sgd_solver.cpp:136] Iteration 15900, lr = 0.0751562, m = 0.9
I0814 18:44:19.805476 10451 solver.cpp:509] Iteration 16000, Testing net (#0)
I0814 18:44:20.642119 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.799413
I0814 18:44:20.642137 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.985883
I0814 18:44:20.642141 10451 solver.cpp:594]     Test net output #2: loss = 0.749197 (* 1 = 0.749197 loss)
I0814 18:44:20.642163 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.836662s
I0814 18:44:20.659468 10451 solver.cpp:312] Iteration 16000 (40.4625 iter/s, 2.47142s/100 iter), loss = 0.0290861
I0814 18:44:20.659488 10451 solver.cpp:334]     Train net output #0: loss = 0.0290846 (* 1 = 0.0290846 loss)
I0814 18:44:20.659494 10451 sgd_solver.cpp:136] Iteration 16000, lr = 0.075, m = 0.9
I0814 18:44:22.299613 10451 solver.cpp:312] Iteration 16100 (60.9722 iter/s, 1.64009s/100 iter), loss = 0.33422
I0814 18:44:22.299638 10451 solver.cpp:334]     Train net output #0: loss = 0.334219 (* 1 = 0.334219 loss)
I0814 18:44:22.299644 10451 sgd_solver.cpp:136] Iteration 16100, lr = 0.0748438, m = 0.9
I0814 18:44:23.929388 10451 solver.cpp:312] Iteration 16200 (61.3601 iter/s, 1.62972s/100 iter), loss = 0.200306
I0814 18:44:23.929414 10451 solver.cpp:334]     Train net output #0: loss = 0.200305 (* 1 = 0.200305 loss)
I0814 18:44:23.929421 10451 sgd_solver.cpp:136] Iteration 16200, lr = 0.0746875, m = 0.9
I0814 18:44:25.571362 10451 solver.cpp:312] Iteration 16300 (60.9042 iter/s, 1.64192s/100 iter), loss = 0.0135765
I0814 18:44:25.571388 10451 solver.cpp:334]     Train net output #0: loss = 0.013575 (* 1 = 0.013575 loss)
I0814 18:44:25.571393 10451 sgd_solver.cpp:136] Iteration 16300, lr = 0.0745312, m = 0.9
I0814 18:44:27.183315 10451 solver.cpp:312] Iteration 16400 (62.0385 iter/s, 1.6119s/100 iter), loss = 0.0856932
I0814 18:44:27.183413 10451 solver.cpp:334]     Train net output #0: loss = 0.0856917 (* 1 = 0.0856917 loss)
I0814 18:44:27.183429 10451 sgd_solver.cpp:136] Iteration 16400, lr = 0.074375, m = 0.9
I0814 18:44:28.807698 10451 solver.cpp:312] Iteration 16500 (61.5639 iter/s, 1.62433s/100 iter), loss = 0.0741544
I0814 18:44:28.807745 10451 solver.cpp:334]     Train net output #0: loss = 0.0741529 (* 1 = 0.0741529 loss)
I0814 18:44:28.807760 10451 sgd_solver.cpp:136] Iteration 16500, lr = 0.0742188, m = 0.9
I0814 18:44:30.445211 10451 solver.cpp:312] Iteration 16600 (61.07 iter/s, 1.63746s/100 iter), loss = 0.330168
I0814 18:44:30.445238 10451 solver.cpp:334]     Train net output #0: loss = 0.330166 (* 1 = 0.330166 loss)
I0814 18:44:30.445245 10451 sgd_solver.cpp:136] Iteration 16600, lr = 0.0740625, m = 0.9
I0814 18:44:32.089239 10451 solver.cpp:312] Iteration 16700 (60.8282 iter/s, 1.64397s/100 iter), loss = 0.171333
I0814 18:44:32.089308 10451 solver.cpp:334]     Train net output #0: loss = 0.171331 (* 1 = 0.171331 loss)
I0814 18:44:32.089339 10451 sgd_solver.cpp:136] Iteration 16700, lr = 0.0739063, m = 0.9
I0814 18:44:33.715647 10451 solver.cpp:312] Iteration 16800 (61.4872 iter/s, 1.62636s/100 iter), loss = 0.0531785
I0814 18:44:33.715672 10451 solver.cpp:334]     Train net output #0: loss = 0.0531769 (* 1 = 0.0531769 loss)
I0814 18:44:33.715677 10451 sgd_solver.cpp:136] Iteration 16800, lr = 0.07375, m = 0.9
I0814 18:44:35.338398 10451 solver.cpp:312] Iteration 16900 (61.6257 iter/s, 1.6227s/100 iter), loss = 0.128129
I0814 18:44:35.338446 10451 solver.cpp:334]     Train net output #0: loss = 0.128127 (* 1 = 0.128127 loss)
I0814 18:44:35.338459 10451 sgd_solver.cpp:136] Iteration 16900, lr = 0.0735938, m = 0.9
I0814 18:44:36.930523 10451 solver.cpp:509] Iteration 17000, Testing net (#0)
I0814 18:44:37.750562 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.820295
I0814 18:44:37.750581 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.985588
I0814 18:44:37.750586 10451 solver.cpp:594]     Test net output #2: loss = 0.74981 (* 1 = 0.74981 loss)
I0814 18:44:37.750602 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.820056s
I0814 18:44:37.767980 10451 solver.cpp:312] Iteration 17000 (41.1605 iter/s, 2.42951s/100 iter), loss = 0.143703
I0814 18:44:37.767998 10451 solver.cpp:334]     Train net output #0: loss = 0.143701 (* 1 = 0.143701 loss)
I0814 18:44:37.768003 10451 sgd_solver.cpp:136] Iteration 17000, lr = 0.0734375, m = 0.9
I0814 18:44:39.388905 10451 solver.cpp:312] Iteration 17100 (61.6951 iter/s, 1.62087s/100 iter), loss = 0.112766
I0814 18:44:39.388952 10451 solver.cpp:334]     Train net output #0: loss = 0.112764 (* 1 = 0.112764 loss)
I0814 18:44:39.388968 10451 sgd_solver.cpp:136] Iteration 17100, lr = 0.0732813, m = 0.9
I0814 18:44:41.020293 10451 solver.cpp:312] Iteration 17200 (61.2995 iter/s, 1.63133s/100 iter), loss = 0.0443328
I0814 18:44:41.020339 10451 solver.cpp:334]     Train net output #0: loss = 0.0443313 (* 1 = 0.0443313 loss)
I0814 18:44:41.020354 10451 sgd_solver.cpp:136] Iteration 17200, lr = 0.073125, m = 0.9
I0814 18:44:42.618517 10451 solver.cpp:312] Iteration 17300 (62.5715 iter/s, 1.59817s/100 iter), loss = 0.0679909
I0814 18:44:42.618572 10451 solver.cpp:334]     Train net output #0: loss = 0.0679893 (* 1 = 0.0679893 loss)
I0814 18:44:42.618587 10451 sgd_solver.cpp:136] Iteration 17300, lr = 0.0729688, m = 0.9
I0814 18:44:44.229044 10451 solver.cpp:312] Iteration 17400 (62.0933 iter/s, 1.61048s/100 iter), loss = 0.0466631
I0814 18:44:44.229094 10451 solver.cpp:334]     Train net output #0: loss = 0.0466614 (* 1 = 0.0466614 loss)
I0814 18:44:44.229105 10451 sgd_solver.cpp:136] Iteration 17400, lr = 0.0728125, m = 0.9
I0814 18:44:45.881207 10451 solver.cpp:312] Iteration 17500 (60.5286 iter/s, 1.65211s/100 iter), loss = 0.0399758
I0814 18:44:45.881233 10451 solver.cpp:334]     Train net output #0: loss = 0.0399742 (* 1 = 0.0399742 loss)
I0814 18:44:45.881239 10451 sgd_solver.cpp:136] Iteration 17500, lr = 0.0726563, m = 0.9
I0814 18:44:47.481632 10451 solver.cpp:312] Iteration 17600 (62.4854 iter/s, 1.60037s/100 iter), loss = 0.0310081
I0814 18:44:47.481676 10451 solver.cpp:334]     Train net output #0: loss = 0.0310066 (* 1 = 0.0310066 loss)
I0814 18:44:47.481683 10451 sgd_solver.cpp:136] Iteration 17600, lr = 0.0725, m = 0.9
I0814 18:44:49.104802 10451 solver.cpp:312] Iteration 17700 (61.6099 iter/s, 1.62312s/100 iter), loss = 0.0206702
I0814 18:44:49.104848 10451 solver.cpp:334]     Train net output #0: loss = 0.0206687 (* 1 = 0.0206687 loss)
I0814 18:44:49.104862 10451 sgd_solver.cpp:136] Iteration 17700, lr = 0.0723438, m = 0.9
I0814 18:44:50.704496 10451 solver.cpp:312] Iteration 17800 (62.5139 iter/s, 1.59964s/100 iter), loss = 0.093697
I0814 18:44:50.704524 10451 solver.cpp:334]     Train net output #0: loss = 0.0936956 (* 1 = 0.0936956 loss)
I0814 18:44:50.704530 10451 sgd_solver.cpp:136] Iteration 17800, lr = 0.0721875, m = 0.9
I0814 18:44:52.308497 10451 solver.cpp:312] Iteration 17900 (62.3459 iter/s, 1.60395s/100 iter), loss = 0.223889
I0814 18:44:52.308555 10451 solver.cpp:334]     Train net output #0: loss = 0.223887 (* 1 = 0.223887 loss)
I0814 18:44:52.308570 10451 sgd_solver.cpp:136] Iteration 17900, lr = 0.0720313, m = 0.9
I0814 18:44:53.923688 10451 solver.cpp:509] Iteration 18000, Testing net (#0)
I0814 18:44:54.401392 10438 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 18:44:54.739018 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.805884
I0814 18:44:54.739037 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.986471
I0814 18:44:54.739042 10451 solver.cpp:594]     Test net output #2: loss = 0.756222 (* 1 = 0.756222 loss)
I0814 18:44:54.739058 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.815347s
I0814 18:44:54.754614 10451 solver.cpp:312] Iteration 18000 (40.8824 iter/s, 2.44604s/100 iter), loss = 0.029643
I0814 18:44:54.754631 10451 solver.cpp:334]     Train net output #0: loss = 0.0296416 (* 1 = 0.0296416 loss)
I0814 18:44:54.754637 10451 sgd_solver.cpp:136] Iteration 18000, lr = 0.071875, m = 0.9
I0814 18:44:56.347685 10451 solver.cpp:312] Iteration 18100 (62.7739 iter/s, 1.59302s/100 iter), loss = 0.00368932
I0814 18:44:56.347733 10451 solver.cpp:334]     Train net output #0: loss = 0.00368797 (* 1 = 0.00368797 loss)
I0814 18:44:56.347748 10451 sgd_solver.cpp:136] Iteration 18100, lr = 0.0717188, m = 0.9
I0814 18:44:57.991847 10451 solver.cpp:312] Iteration 18200 (60.8231 iter/s, 1.64411s/100 iter), loss = 0.00725249
I0814 18:44:57.991925 10451 solver.cpp:334]     Train net output #0: loss = 0.00725117 (* 1 = 0.00725117 loss)
I0814 18:44:57.991933 10451 sgd_solver.cpp:136] Iteration 18200, lr = 0.0715625, m = 0.9
I0814 18:44:59.577795 10451 solver.cpp:312] Iteration 18300 (63.0559 iter/s, 1.5859s/100 iter), loss = 0.354245
I0814 18:44:59.577890 10451 solver.cpp:334]     Train net output #0: loss = 0.354244 (* 1 = 0.354244 loss)
I0814 18:44:59.577898 10451 sgd_solver.cpp:136] Iteration 18300, lr = 0.0714063, m = 0.9
I0814 18:45:01.194073 10451 solver.cpp:312] Iteration 18400 (61.8724 iter/s, 1.61623s/100 iter), loss = 0.125627
I0814 18:45:01.194098 10451 solver.cpp:334]     Train net output #0: loss = 0.125626 (* 1 = 0.125626 loss)
I0814 18:45:01.194104 10451 sgd_solver.cpp:136] Iteration 18400, lr = 0.07125, m = 0.9
I0814 18:45:02.806784 10451 solver.cpp:312] Iteration 18500 (62.0093 iter/s, 1.61266s/100 iter), loss = 0.0937057
I0814 18:45:02.806807 10451 solver.cpp:334]     Train net output #0: loss = 0.0937045 (* 1 = 0.0937045 loss)
I0814 18:45:02.806812 10451 sgd_solver.cpp:136] Iteration 18500, lr = 0.0710938, m = 0.9
I0814 18:45:04.404680 10451 solver.cpp:312] Iteration 18600 (62.5843 iter/s, 1.59785s/100 iter), loss = 0.260423
I0814 18:45:04.404703 10451 solver.cpp:334]     Train net output #0: loss = 0.260422 (* 1 = 0.260422 loss)
I0814 18:45:04.404707 10451 sgd_solver.cpp:136] Iteration 18600, lr = 0.0709375, m = 0.9
I0814 18:45:05.986974 10451 solver.cpp:312] Iteration 18700 (63.2014 iter/s, 1.58224s/100 iter), loss = 0.187294
I0814 18:45:05.987001 10451 solver.cpp:334]     Train net output #0: loss = 0.187293 (* 1 = 0.187293 loss)
I0814 18:45:05.987009 10451 sgd_solver.cpp:136] Iteration 18700, lr = 0.0707813, m = 0.9
I0814 18:45:07.631041 10451 solver.cpp:312] Iteration 18800 (60.8268 iter/s, 1.64401s/100 iter), loss = 0.0238274
I0814 18:45:07.631083 10451 solver.cpp:334]     Train net output #0: loss = 0.0238261 (* 1 = 0.0238261 loss)
I0814 18:45:07.631089 10451 sgd_solver.cpp:136] Iteration 18800, lr = 0.070625, m = 0.9
I0814 18:45:09.256136 10451 solver.cpp:312] Iteration 18900 (61.5368 iter/s, 1.62504s/100 iter), loss = 0.162358
I0814 18:45:09.256204 10451 solver.cpp:334]     Train net output #0: loss = 0.162356 (* 1 = 0.162356 loss)
I0814 18:45:09.256234 10451 sgd_solver.cpp:136] Iteration 18900, lr = 0.0704687, m = 0.9
I0814 18:45:10.887142 10451 solver.cpp:509] Iteration 19000, Testing net (#0)
I0814 18:45:11.704962 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.82706
I0814 18:45:11.704978 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.988235
I0814 18:45:11.704983 10451 solver.cpp:594]     Test net output #2: loss = 0.667615 (* 1 = 0.667615 loss)
I0814 18:45:11.705000 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.817835s
I0814 18:45:11.720535 10451 solver.cpp:312] Iteration 19000 (40.579 iter/s, 2.46433s/100 iter), loss = 0.227152
I0814 18:45:11.720554 10451 solver.cpp:334]     Train net output #0: loss = 0.22715 (* 1 = 0.22715 loss)
I0814 18:45:11.720561 10451 sgd_solver.cpp:136] Iteration 19000, lr = 0.0703125, m = 0.9
I0814 18:45:13.368896 10451 solver.cpp:312] Iteration 19100 (60.6683 iter/s, 1.64831s/100 iter), loss = 0.154841
I0814 18:45:13.368944 10451 solver.cpp:334]     Train net output #0: loss = 0.15484 (* 1 = 0.15484 loss)
I0814 18:45:13.368958 10451 sgd_solver.cpp:136] Iteration 19100, lr = 0.0701563, m = 0.9
I0814 18:45:14.991855 10451 solver.cpp:312] Iteration 19200 (61.6178 iter/s, 1.62291s/100 iter), loss = 0.0331433
I0814 18:45:14.991881 10451 solver.cpp:334]     Train net output #0: loss = 0.0331422 (* 1 = 0.0331422 loss)
I0814 18:45:14.991888 10451 sgd_solver.cpp:136] Iteration 19200, lr = 0.07, m = 0.9
I0814 18:45:16.611850 10451 solver.cpp:312] Iteration 19300 (61.7306 iter/s, 1.61994s/100 iter), loss = 0.0327034
I0814 18:45:16.611877 10451 solver.cpp:334]     Train net output #0: loss = 0.0327023 (* 1 = 0.0327023 loss)
I0814 18:45:16.611884 10451 sgd_solver.cpp:136] Iteration 19300, lr = 0.0698438, m = 0.9
I0814 18:45:18.255129 10451 solver.cpp:312] Iteration 19400 (60.8558 iter/s, 1.64323s/100 iter), loss = 0.114478
I0814 18:45:18.255188 10451 solver.cpp:334]     Train net output #0: loss = 0.114476 (* 1 = 0.114476 loss)
I0814 18:45:18.255203 10451 sgd_solver.cpp:136] Iteration 19400, lr = 0.0696875, m = 0.9
I0814 18:45:19.892213 10451 solver.cpp:312] Iteration 19500 (61.0862 iter/s, 1.63703s/100 iter), loss = 0.0220742
I0814 18:45:19.892343 10451 solver.cpp:334]     Train net output #0: loss = 0.0220731 (* 1 = 0.0220731 loss)
I0814 18:45:19.892359 10451 sgd_solver.cpp:136] Iteration 19500, lr = 0.0695313, m = 0.9
I0814 18:45:21.488608 10451 solver.cpp:312] Iteration 19600 (62.6431 iter/s, 1.59634s/100 iter), loss = 0.0620249
I0814 18:45:21.488688 10451 solver.cpp:334]     Train net output #0: loss = 0.0620238 (* 1 = 0.0620238 loss)
I0814 18:45:21.488709 10451 sgd_solver.cpp:136] Iteration 19600, lr = 0.069375, m = 0.9
I0814 18:45:23.120695 10451 solver.cpp:312] Iteration 19700 (61.2732 iter/s, 1.63203s/100 iter), loss = 0.0285975
I0814 18:45:23.120853 10451 solver.cpp:334]     Train net output #0: loss = 0.0285964 (* 1 = 0.0285964 loss)
I0814 18:45:23.120934 10451 sgd_solver.cpp:136] Iteration 19700, lr = 0.0692187, m = 0.9
I0814 18:45:24.780223 10451 solver.cpp:312] Iteration 19800 (60.26 iter/s, 1.65947s/100 iter), loss = 0.154363
I0814 18:45:24.780270 10451 solver.cpp:334]     Train net output #0: loss = 0.154362 (* 1 = 0.154362 loss)
I0814 18:45:24.780282 10451 sgd_solver.cpp:136] Iteration 19800, lr = 0.0690625, m = 0.9
I0814 18:45:26.402451 10451 solver.cpp:312] Iteration 19900 (61.6456 iter/s, 1.62217s/100 iter), loss = 0.125881
I0814 18:45:26.402510 10451 solver.cpp:334]     Train net output #0: loss = 0.125879 (* 1 = 0.125879 loss)
I0814 18:45:26.402526 10451 sgd_solver.cpp:136] Iteration 19900, lr = 0.0689062, m = 0.9
I0814 18:45:28.050727 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_20000.caffemodel
I0814 18:45:28.060282 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_20000.solverstate
I0814 18:45:28.065045 10451 solver.cpp:509] Iteration 20000, Testing net (#0)
I0814 18:45:28.870872 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.744707
I0814 18:45:28.870890 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.982941
I0814 18:45:28.870896 10451 solver.cpp:594]     Test net output #2: loss = 1.17378 (* 1 = 1.17378 loss)
I0814 18:45:28.870914 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.805846s
I0814 18:45:28.886544 10451 solver.cpp:312] Iteration 20000 (40.2573 iter/s, 2.48402s/100 iter), loss = 0.027179
I0814 18:45:28.886562 10451 solver.cpp:334]     Train net output #0: loss = 0.0271778 (* 1 = 0.0271778 loss)
I0814 18:45:28.886569 10451 sgd_solver.cpp:136] Iteration 20000, lr = 0.06875, m = 0.9
I0814 18:45:30.490424 10451 solver.cpp:312] Iteration 20100 (62.3508 iter/s, 1.60383s/100 iter), loss = 0.0696251
I0814 18:45:30.490447 10451 solver.cpp:334]     Train net output #0: loss = 0.069624 (* 1 = 0.069624 loss)
I0814 18:45:30.490453 10451 sgd_solver.cpp:136] Iteration 20100, lr = 0.0685938, m = 0.9
I0814 18:45:32.135009 10451 solver.cpp:312] Iteration 20200 (60.8075 iter/s, 1.64453s/100 iter), loss = 0.0663001
I0814 18:45:32.135154 10451 solver.cpp:334]     Train net output #0: loss = 0.0662989 (* 1 = 0.0662989 loss)
I0814 18:45:32.135177 10451 sgd_solver.cpp:136] Iteration 20200, lr = 0.0684375, m = 0.9
I0814 18:45:33.779783 10451 solver.cpp:312] Iteration 20300 (60.8006 iter/s, 1.64472s/100 iter), loss = 0.0625814
I0814 18:45:33.779850 10451 solver.cpp:334]     Train net output #0: loss = 0.0625803 (* 1 = 0.0625803 loss)
I0814 18:45:33.779875 10451 sgd_solver.cpp:136] Iteration 20300, lr = 0.0682813, m = 0.9
I0814 18:45:35.394346 10451 solver.cpp:312] Iteration 20400 (61.9383 iter/s, 1.61451s/100 iter), loss = 0.238864
I0814 18:45:35.394409 10451 solver.cpp:334]     Train net output #0: loss = 0.238863 (* 1 = 0.238863 loss)
I0814 18:45:35.394428 10451 sgd_solver.cpp:136] Iteration 20400, lr = 0.068125, m = 0.9
I0814 18:45:37.034440 10451 solver.cpp:312] Iteration 20500 (60.974 iter/s, 1.64004s/100 iter), loss = 0.181434
I0814 18:45:37.034462 10451 solver.cpp:334]     Train net output #0: loss = 0.181433 (* 1 = 0.181433 loss)
I0814 18:45:37.034468 10451 sgd_solver.cpp:136] Iteration 20500, lr = 0.0679687, m = 0.9
I0814 18:45:38.610610 10451 solver.cpp:312] Iteration 20600 (63.447 iter/s, 1.57612s/100 iter), loss = 0.0422956
I0814 18:45:38.610632 10451 solver.cpp:334]     Train net output #0: loss = 0.0422945 (* 1 = 0.0422945 loss)
I0814 18:45:38.610637 10451 sgd_solver.cpp:136] Iteration 20600, lr = 0.0678125, m = 0.9
I0814 18:45:40.215598 10451 solver.cpp:312] Iteration 20700 (62.3077 iter/s, 1.60494s/100 iter), loss = 0.101729
I0814 18:45:40.215667 10451 solver.cpp:334]     Train net output #0: loss = 0.101728 (* 1 = 0.101728 loss)
I0814 18:45:40.215687 10451 sgd_solver.cpp:136] Iteration 20700, lr = 0.0676562, m = 0.9
I0814 18:45:41.886473 10451 solver.cpp:312] Iteration 20800 (59.8507 iter/s, 1.67082s/100 iter), loss = 0.0312345
I0814 18:45:41.886540 10451 solver.cpp:334]     Train net output #0: loss = 0.0312333 (* 1 = 0.0312333 loss)
I0814 18:45:41.886560 10451 sgd_solver.cpp:136] Iteration 20800, lr = 0.0675, m = 0.9
I0814 18:45:43.469056 10451 solver.cpp:312] Iteration 20900 (63.1898 iter/s, 1.58253s/100 iter), loss = 0.0757367
I0814 18:45:43.469105 10451 solver.cpp:334]     Train net output #0: loss = 0.0757354 (* 1 = 0.0757354 loss)
I0814 18:45:43.469118 10451 sgd_solver.cpp:136] Iteration 20900, lr = 0.0673437, m = 0.9
I0814 18:45:45.052381 10451 solver.cpp:509] Iteration 21000, Testing net (#0)
I0814 18:45:45.892338 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.843531
I0814 18:45:45.892356 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.991471
I0814 18:45:45.892361 10451 solver.cpp:594]     Test net output #2: loss = 0.572196 (* 1 = 0.572196 loss)
I0814 18:45:45.892390 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.839985s
I0814 18:45:45.909785 10451 solver.cpp:312] Iteration 21000 (40.9726 iter/s, 2.44066s/100 iter), loss = 0.0723213
I0814 18:45:45.909802 10451 solver.cpp:334]     Train net output #0: loss = 0.07232 (* 1 = 0.07232 loss)
I0814 18:45:45.909808 10451 sgd_solver.cpp:136] Iteration 21000, lr = 0.0671875, m = 0.9
I0814 18:45:47.532778 10451 solver.cpp:312] Iteration 21100 (61.6167 iter/s, 1.62294s/100 iter), loss = 0.0402658
I0814 18:45:47.532827 10451 solver.cpp:334]     Train net output #0: loss = 0.0402645 (* 1 = 0.0402645 loss)
I0814 18:45:47.532847 10451 sgd_solver.cpp:136] Iteration 21100, lr = 0.0670313, m = 0.9
I0814 18:45:49.194260 10451 solver.cpp:312] Iteration 21200 (60.189 iter/s, 1.66143s/100 iter), loss = 0.0827765
I0814 18:45:49.194286 10451 solver.cpp:334]     Train net output #0: loss = 0.0827753 (* 1 = 0.0827753 loss)
I0814 18:45:49.194293 10451 sgd_solver.cpp:136] Iteration 21200, lr = 0.066875, m = 0.9
I0814 18:45:50.815351 10451 solver.cpp:312] Iteration 21300 (61.6888 iter/s, 1.62104s/100 iter), loss = 0.106158
I0814 18:45:50.815376 10451 solver.cpp:334]     Train net output #0: loss = 0.106157 (* 1 = 0.106157 loss)
I0814 18:45:50.815382 10451 sgd_solver.cpp:136] Iteration 21300, lr = 0.0667187, m = 0.9
I0814 18:45:52.391729 10451 solver.cpp:312] Iteration 21400 (63.4385 iter/s, 1.57633s/100 iter), loss = 0.0130171
I0814 18:45:52.391778 10451 solver.cpp:334]     Train net output #0: loss = 0.0130158 (* 1 = 0.0130158 loss)
I0814 18:45:52.391791 10451 sgd_solver.cpp:136] Iteration 21400, lr = 0.0665625, m = 0.9
I0814 18:45:54.011489 10451 solver.cpp:312] Iteration 21500 (61.7395 iter/s, 1.61971s/100 iter), loss = 0.0945474
I0814 18:45:54.011538 10451 solver.cpp:334]     Train net output #0: loss = 0.0945462 (* 1 = 0.0945462 loss)
I0814 18:45:54.011550 10451 sgd_solver.cpp:136] Iteration 21500, lr = 0.0664062, m = 0.9
I0814 18:45:55.636242 10451 solver.cpp:312] Iteration 21600 (61.5498 iter/s, 1.6247s/100 iter), loss = 0.0195708
I0814 18:45:55.636265 10451 solver.cpp:334]     Train net output #0: loss = 0.0195696 (* 1 = 0.0195696 loss)
I0814 18:45:55.636270 10451 sgd_solver.cpp:136] Iteration 21600, lr = 0.06625, m = 0.9
I0814 18:45:57.256575 10451 solver.cpp:312] Iteration 21700 (61.7176 iter/s, 1.62028s/100 iter), loss = 0.14687
I0814 18:45:57.256635 10451 solver.cpp:334]     Train net output #0: loss = 0.146869 (* 1 = 0.146869 loss)
I0814 18:45:57.256655 10451 sgd_solver.cpp:136] Iteration 21700, lr = 0.0660938, m = 0.9
I0814 18:45:58.878281 10451 solver.cpp:312] Iteration 21800 (61.6653 iter/s, 1.62166s/100 iter), loss = 0.147751
I0814 18:45:58.878373 10451 solver.cpp:334]     Train net output #0: loss = 0.14775 (* 1 = 0.14775 loss)
I0814 18:45:58.878381 10451 sgd_solver.cpp:136] Iteration 21800, lr = 0.0659375, m = 0.9
I0814 18:46:00.502902 10451 solver.cpp:312] Iteration 21900 (61.5549 iter/s, 1.62457s/100 iter), loss = 0.231411
I0814 18:46:00.502924 10451 solver.cpp:334]     Train net output #0: loss = 0.231409 (* 1 = 0.231409 loss)
I0814 18:46:00.502930 10451 sgd_solver.cpp:136] Iteration 21900, lr = 0.0657813, m = 0.9
I0814 18:46:02.062724 10451 solver.cpp:509] Iteration 22000, Testing net (#0)
I0814 18:46:02.482765 10438 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 18:46:02.889092 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.819119
I0814 18:46:02.889111 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.987059
I0814 18:46:02.889117 10451 solver.cpp:594]     Test net output #2: loss = 0.704304 (* 1 = 0.704304 loss)
I0814 18:46:02.889132 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.826384s
I0814 18:46:02.907668 10451 solver.cpp:312] Iteration 22000 (41.5853 iter/s, 2.4047s/100 iter), loss = 0.0388929
I0814 18:46:02.907685 10451 solver.cpp:334]     Train net output #0: loss = 0.0388916 (* 1 = 0.0388916 loss)
I0814 18:46:02.907690 10451 sgd_solver.cpp:136] Iteration 22000, lr = 0.065625, m = 0.9
I0814 18:46:04.503993 10451 solver.cpp:312] Iteration 22100 (62.6459 iter/s, 1.59627s/100 iter), loss = 0.139702
I0814 18:46:04.504151 10451 solver.cpp:334]     Train net output #0: loss = 0.139701 (* 1 = 0.139701 loss)
I0814 18:46:04.504179 10451 sgd_solver.cpp:136] Iteration 22100, lr = 0.0654688, m = 0.9
I0814 18:46:06.129202 10451 solver.cpp:312] Iteration 22200 (61.5324 iter/s, 1.62516s/100 iter), loss = 0.171414
I0814 18:46:06.129226 10451 solver.cpp:334]     Train net output #0: loss = 0.171412 (* 1 = 0.171412 loss)
I0814 18:46:06.129232 10451 sgd_solver.cpp:136] Iteration 22200, lr = 0.0653125, m = 0.9
I0814 18:46:07.777119 10451 solver.cpp:312] Iteration 22300 (60.6847 iter/s, 1.64786s/100 iter), loss = 0.0128292
I0814 18:46:07.777170 10451 solver.cpp:334]     Train net output #0: loss = 0.0128279 (* 1 = 0.0128279 loss)
I0814 18:46:07.777184 10451 sgd_solver.cpp:136] Iteration 22300, lr = 0.0651563, m = 0.9
I0814 18:46:09.382447 10451 solver.cpp:312] Iteration 22400 (62.2944 iter/s, 1.60528s/100 iter), loss = 0.0537215
I0814 18:46:09.382494 10451 solver.cpp:334]     Train net output #0: loss = 0.0537203 (* 1 = 0.0537203 loss)
I0814 18:46:09.382506 10451 sgd_solver.cpp:136] Iteration 22400, lr = 0.065, m = 0.9
I0814 18:46:10.955663 10451 solver.cpp:312] Iteration 22500 (63.5661 iter/s, 1.57317s/100 iter), loss = 0.0642078
I0814 18:46:10.955688 10451 solver.cpp:334]     Train net output #0: loss = 0.0642065 (* 1 = 0.0642065 loss)
I0814 18:46:10.955693 10451 sgd_solver.cpp:136] Iteration 22500, lr = 0.0648438, m = 0.9
I0814 18:46:12.596853 10451 solver.cpp:312] Iteration 22600 (60.9332 iter/s, 1.64114s/100 iter), loss = 0.076145
I0814 18:46:12.596879 10451 solver.cpp:334]     Train net output #0: loss = 0.0761438 (* 1 = 0.0761438 loss)
I0814 18:46:12.596885 10451 sgd_solver.cpp:136] Iteration 22600, lr = 0.0646875, m = 0.9
I0814 18:46:14.237812 10451 solver.cpp:312] Iteration 22700 (60.942 iter/s, 1.6409s/100 iter), loss = 0.0630133
I0814 18:46:14.237862 10451 solver.cpp:334]     Train net output #0: loss = 0.0630121 (* 1 = 0.0630121 loss)
I0814 18:46:14.237875 10451 sgd_solver.cpp:136] Iteration 22700, lr = 0.0645313, m = 0.9
I0814 18:46:15.884608 10451 solver.cpp:312] Iteration 22800 (60.7257 iter/s, 1.64675s/100 iter), loss = 0.104703
I0814 18:46:15.884652 10451 solver.cpp:334]     Train net output #0: loss = 0.104702 (* 1 = 0.104702 loss)
I0814 18:46:15.884663 10451 sgd_solver.cpp:136] Iteration 22800, lr = 0.064375, m = 0.9
I0814 18:46:17.485180 10451 solver.cpp:312] Iteration 22900 (62.4796 iter/s, 1.60052s/100 iter), loss = 0.0100016
I0814 18:46:17.485208 10451 solver.cpp:334]     Train net output #0: loss = 0.0100004 (* 1 = 0.0100004 loss)
I0814 18:46:17.485215 10451 sgd_solver.cpp:136] Iteration 22900, lr = 0.0642188, m = 0.9
I0814 18:46:19.081270 10451 solver.cpp:509] Iteration 23000, Testing net (#0)
I0814 18:46:19.904260 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.673235
I0814 18:46:19.904279 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.955883
I0814 18:46:19.904286 10451 solver.cpp:594]     Test net output #2: loss = 1.53669 (* 1 = 1.53669 loss)
I0814 18:46:19.904304 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.823011s
I0814 18:46:19.919862 10451 solver.cpp:312] Iteration 23000 (41.0743 iter/s, 2.43461s/100 iter), loss = 0.222563
I0814 18:46:19.919890 10451 solver.cpp:334]     Train net output #0: loss = 0.222562 (* 1 = 0.222562 loss)
I0814 18:46:19.919901 10451 sgd_solver.cpp:136] Iteration 23000, lr = 0.0640625, m = 0.9
I0814 18:46:21.528087 10451 solver.cpp:312] Iteration 23100 (62.1824 iter/s, 1.60817s/100 iter), loss = 0.0627405
I0814 18:46:21.528111 10451 solver.cpp:334]     Train net output #0: loss = 0.0627393 (* 1 = 0.0627393 loss)
I0814 18:46:21.528117 10451 sgd_solver.cpp:136] Iteration 23100, lr = 0.0639063, m = 0.9
I0814 18:46:23.172240 10451 solver.cpp:312] Iteration 23200 (60.8235 iter/s, 1.6441s/100 iter), loss = 0.0163003
I0814 18:46:23.172266 10451 solver.cpp:334]     Train net output #0: loss = 0.0162992 (* 1 = 0.0162992 loss)
I0814 18:46:23.172271 10451 sgd_solver.cpp:136] Iteration 23200, lr = 0.06375, m = 0.9
I0814 18:46:24.800444 10451 solver.cpp:312] Iteration 23300 (61.4192 iter/s, 1.62816s/100 iter), loss = 0.030798
I0814 18:46:24.800470 10451 solver.cpp:334]     Train net output #0: loss = 0.0307969 (* 1 = 0.0307969 loss)
I0814 18:46:24.800477 10451 sgd_solver.cpp:136] Iteration 23300, lr = 0.0635938, m = 0.9
I0814 18:46:26.464419 10451 solver.cpp:312] Iteration 23400 (60.0989 iter/s, 1.66393s/100 iter), loss = 0.057242
I0814 18:46:26.464489 10451 solver.cpp:334]     Train net output #0: loss = 0.0572409 (* 1 = 0.0572409 loss)
I0814 18:46:26.464509 10451 sgd_solver.cpp:136] Iteration 23400, lr = 0.0634375, m = 0.9
I0814 18:46:28.057186 10451 solver.cpp:312] Iteration 23500 (62.7859 iter/s, 1.59272s/100 iter), loss = 0.0026719
I0814 18:46:28.057212 10451 solver.cpp:334]     Train net output #0: loss = 0.00267075 (* 1 = 0.00267075 loss)
I0814 18:46:28.057219 10451 sgd_solver.cpp:136] Iteration 23500, lr = 0.0632813, m = 0.9
I0814 18:46:29.699753 10451 solver.cpp:312] Iteration 23600 (60.8822 iter/s, 1.64252s/100 iter), loss = 0.0895933
I0814 18:46:29.699857 10451 solver.cpp:334]     Train net output #0: loss = 0.0895921 (* 1 = 0.0895921 loss)
I0814 18:46:29.699870 10451 sgd_solver.cpp:136] Iteration 23600, lr = 0.063125, m = 0.9
I0814 18:46:31.338162 10451 solver.cpp:312] Iteration 23700 (61.0368 iter/s, 1.63836s/100 iter), loss = 0.0393265
I0814 18:46:31.338212 10451 solver.cpp:334]     Train net output #0: loss = 0.0393255 (* 1 = 0.0393255 loss)
I0814 18:46:31.338227 10451 sgd_solver.cpp:136] Iteration 23700, lr = 0.0629688, m = 0.9
I0814 18:46:33.003410 10451 solver.cpp:312] Iteration 23800 (60.0529 iter/s, 1.6652s/100 iter), loss = 0.0941757
I0814 18:46:33.003435 10451 solver.cpp:334]     Train net output #0: loss = 0.0941746 (* 1 = 0.0941746 loss)
I0814 18:46:33.003442 10451 sgd_solver.cpp:136] Iteration 23800, lr = 0.0628125, m = 0.9
I0814 18:46:34.575440 10451 solver.cpp:312] Iteration 23900 (63.6139 iter/s, 1.57198s/100 iter), loss = 0.110839
I0814 18:46:34.575465 10451 solver.cpp:334]     Train net output #0: loss = 0.110838 (* 1 = 0.110838 loss)
I0814 18:46:34.575470 10451 sgd_solver.cpp:136] Iteration 23900, lr = 0.0626562, m = 0.9
I0814 18:46:36.181872 10451 solver.cpp:509] Iteration 24000, Testing net (#0)
I0814 18:46:36.996721 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.801472
I0814 18:46:36.996739 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.987059
I0814 18:46:36.996745 10451 solver.cpp:594]     Test net output #2: loss = 0.761519 (* 1 = 0.761519 loss)
I0814 18:46:36.996767 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.814869s
I0814 18:46:37.015897 10451 solver.cpp:312] Iteration 24000 (40.9772 iter/s, 2.44038s/100 iter), loss = 0.126281
I0814 18:46:37.015928 10451 solver.cpp:334]     Train net output #0: loss = 0.126279 (* 1 = 0.126279 loss)
I0814 18:46:37.015935 10451 sgd_solver.cpp:136] Iteration 24000, lr = 0.0625, m = 0.9
I0814 18:46:38.631508 10451 solver.cpp:312] Iteration 24100 (61.8979 iter/s, 1.61556s/100 iter), loss = 0.0786973
I0814 18:46:38.631750 10451 solver.cpp:334]     Train net output #0: loss = 0.0786962 (* 1 = 0.0786962 loss)
I0814 18:46:38.631757 10451 sgd_solver.cpp:136] Iteration 24100, lr = 0.0623438, m = 0.9
I0814 18:46:40.259683 10451 solver.cpp:312] Iteration 24200 (61.4205 iter/s, 1.62812s/100 iter), loss = 0.050549
I0814 18:46:40.259732 10451 solver.cpp:334]     Train net output #0: loss = 0.0505479 (* 1 = 0.0505479 loss)
I0814 18:46:40.259748 10451 sgd_solver.cpp:136] Iteration 24200, lr = 0.0621875, m = 0.9
I0814 18:46:41.896401 10451 solver.cpp:312] Iteration 24300 (61.0998 iter/s, 1.63667s/100 iter), loss = 0.00538185
I0814 18:46:41.896427 10451 solver.cpp:334]     Train net output #0: loss = 0.00538072 (* 1 = 0.00538072 loss)
I0814 18:46:41.896435 10451 sgd_solver.cpp:136] Iteration 24300, lr = 0.0620313, m = 0.9
I0814 18:46:43.510818 10451 solver.cpp:312] Iteration 24400 (61.9438 iter/s, 1.61437s/100 iter), loss = 0.0184136
I0814 18:46:43.510843 10451 solver.cpp:334]     Train net output #0: loss = 0.0184124 (* 1 = 0.0184124 loss)
I0814 18:46:43.510849 10451 sgd_solver.cpp:136] Iteration 24400, lr = 0.061875, m = 0.9
I0814 18:46:45.117569 10451 solver.cpp:312] Iteration 24500 (62.2393 iter/s, 1.6067s/100 iter), loss = 0.0496733
I0814 18:46:45.117597 10451 solver.cpp:334]     Train net output #0: loss = 0.0496722 (* 1 = 0.0496722 loss)
I0814 18:46:45.117604 10451 sgd_solver.cpp:136] Iteration 24500, lr = 0.0617188, m = 0.9
I0814 18:46:46.776991 10451 solver.cpp:312] Iteration 24600 (60.2638 iter/s, 1.65937s/100 iter), loss = 0.0528936
I0814 18:46:46.777048 10451 solver.cpp:334]     Train net output #0: loss = 0.0528925 (* 1 = 0.0528925 loss)
I0814 18:46:46.777074 10451 sgd_solver.cpp:136] Iteration 24600, lr = 0.0615625, m = 0.9
I0814 18:46:48.391196 10451 solver.cpp:312] Iteration 24700 (61.9519 iter/s, 1.61415s/100 iter), loss = 0.0132825
I0814 18:46:48.391245 10451 solver.cpp:334]     Train net output #0: loss = 0.0132814 (* 1 = 0.0132814 loss)
I0814 18:46:48.391259 10451 sgd_solver.cpp:136] Iteration 24700, lr = 0.0614063, m = 0.9
I0814 18:46:50.040156 10451 solver.cpp:312] Iteration 24800 (60.6461 iter/s, 1.64891s/100 iter), loss = 0.0904861
I0814 18:46:50.040195 10451 solver.cpp:334]     Train net output #0: loss = 0.090485 (* 1 = 0.090485 loss)
I0814 18:46:50.040200 10451 sgd_solver.cpp:136] Iteration 24800, lr = 0.06125, m = 0.9
I0814 18:46:51.633901 10451 solver.cpp:312] Iteration 24900 (62.7473 iter/s, 1.59369s/100 iter), loss = 0.0627828
I0814 18:46:51.633975 10451 solver.cpp:334]     Train net output #0: loss = 0.0627816 (* 1 = 0.0627816 loss)
I0814 18:46:51.633997 10451 sgd_solver.cpp:136] Iteration 24900, lr = 0.0610937, m = 0.9
I0814 18:46:53.251240 10451 solver.cpp:509] Iteration 25000, Testing net (#0)
I0814 18:46:54.068444 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.846178
I0814 18:46:54.068461 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989412
I0814 18:46:54.068466 10451 solver.cpp:594]     Test net output #2: loss = 0.611982 (* 1 = 0.611982 loss)
I0814 18:46:54.068483 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.817221s
I0814 18:46:54.084005 10451 solver.cpp:312] Iteration 25000 (40.8158 iter/s, 2.45003s/100 iter), loss = 0.0205969
I0814 18:46:54.084022 10451 solver.cpp:334]     Train net output #0: loss = 0.0205957 (* 1 = 0.0205957 loss)
I0814 18:46:54.084025 10451 sgd_solver.cpp:136] Iteration 25000, lr = 0.0609375, m = 0.9
I0814 18:46:55.678484 10451 solver.cpp:312] Iteration 25100 (62.7184 iter/s, 1.59443s/100 iter), loss = 0.041005
I0814 18:46:55.678530 10451 solver.cpp:334]     Train net output #0: loss = 0.0410038 (* 1 = 0.0410038 loss)
I0814 18:46:55.678544 10451 sgd_solver.cpp:136] Iteration 25100, lr = 0.0607813, m = 0.9
I0814 18:46:57.315088 10451 solver.cpp:312] Iteration 25200 (61.1042 iter/s, 1.63655s/100 iter), loss = 0.150859
I0814 18:46:57.315114 10451 solver.cpp:334]     Train net output #0: loss = 0.150858 (* 1 = 0.150858 loss)
I0814 18:46:57.315120 10451 sgd_solver.cpp:136] Iteration 25200, lr = 0.060625, m = 0.9
I0814 18:46:58.934146 10451 solver.cpp:312] Iteration 25300 (61.7661 iter/s, 1.61901s/100 iter), loss = 0.0692633
I0814 18:46:58.934195 10451 solver.cpp:334]     Train net output #0: loss = 0.0692622 (* 1 = 0.0692622 loss)
I0814 18:46:58.934211 10451 sgd_solver.cpp:136] Iteration 25300, lr = 0.0604688, m = 0.9
I0814 18:47:00.514853 10451 solver.cpp:312] Iteration 25400 (63.2647 iter/s, 1.58066s/100 iter), loss = 0.0245713
I0814 18:47:00.515120 10451 solver.cpp:334]     Train net output #0: loss = 0.0245702 (* 1 = 0.0245702 loss)
I0814 18:47:00.515141 10451 sgd_solver.cpp:136] Iteration 25400, lr = 0.0603125, m = 0.9
I0814 18:47:02.112560 10451 solver.cpp:312] Iteration 25500 (62.5917 iter/s, 1.59766s/100 iter), loss = 0.0525974
I0814 18:47:02.112582 10451 solver.cpp:334]     Train net output #0: loss = 0.0525963 (* 1 = 0.0525963 loss)
I0814 18:47:02.112588 10451 sgd_solver.cpp:136] Iteration 25500, lr = 0.0601563, m = 0.9
I0814 18:47:03.742482 10451 solver.cpp:312] Iteration 25600 (61.3546 iter/s, 1.62987s/100 iter), loss = 0.0877739
I0814 18:47:03.742509 10451 solver.cpp:334]     Train net output #0: loss = 0.0877728 (* 1 = 0.0877728 loss)
I0814 18:47:03.742516 10451 sgd_solver.cpp:136] Iteration 25600, lr = 0.06, m = 0.9
I0814 18:47:05.366109 10451 solver.cpp:312] Iteration 25700 (61.5923 iter/s, 1.62358s/100 iter), loss = 0.0297554
I0814 18:47:05.366276 10451 solver.cpp:334]     Train net output #0: loss = 0.0297543 (* 1 = 0.0297543 loss)
I0814 18:47:05.366298 10451 sgd_solver.cpp:136] Iteration 25700, lr = 0.0598437, m = 0.9
I0814 18:47:06.971418 10451 solver.cpp:312] Iteration 25800 (62.2953 iter/s, 1.60526s/100 iter), loss = 0.0321142
I0814 18:47:06.971488 10451 solver.cpp:334]     Train net output #0: loss = 0.032113 (* 1 = 0.032113 loss)
I0814 18:47:06.971508 10451 sgd_solver.cpp:136] Iteration 25800, lr = 0.0596875, m = 0.9
I0814 18:47:08.598374 10451 solver.cpp:312] Iteration 25900 (61.4665 iter/s, 1.6269s/100 iter), loss = 0.0137708
I0814 18:47:08.598440 10451 solver.cpp:334]     Train net output #0: loss = 0.0137696 (* 1 = 0.0137696 loss)
I0814 18:47:08.598460 10451 sgd_solver.cpp:136] Iteration 25900, lr = 0.0595312, m = 0.9
I0814 18:47:10.212738 10451 solver.cpp:509] Iteration 26000, Testing net (#0)
I0814 18:47:11.027925 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.82206
I0814 18:47:11.027940 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.985588
I0814 18:47:11.027945 10451 solver.cpp:594]     Test net output #2: loss = 0.69271 (* 1 = 0.69271 loss)
I0814 18:47:11.027962 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.815202s
I0814 18:47:11.043562 10451 solver.cpp:312] Iteration 26000 (40.8978 iter/s, 2.44512s/100 iter), loss = 0.300729
I0814 18:47:11.043581 10451 solver.cpp:334]     Train net output #0: loss = 0.300728 (* 1 = 0.300728 loss)
I0814 18:47:11.043584 10451 sgd_solver.cpp:136] Iteration 26000, lr = 0.059375, m = 0.9
I0814 18:47:12.684507 10451 solver.cpp:312] Iteration 26100 (60.9424 iter/s, 1.64089s/100 iter), loss = 0.0509249
I0814 18:47:12.684567 10451 solver.cpp:334]     Train net output #0: loss = 0.0509238 (* 1 = 0.0509238 loss)
I0814 18:47:12.684586 10451 sgd_solver.cpp:136] Iteration 26100, lr = 0.0592188, m = 0.9
I0814 18:47:14.333937 10451 solver.cpp:312] Iteration 26200 (60.6289 iter/s, 1.64938s/100 iter), loss = 0.0556359
I0814 18:47:14.333961 10451 solver.cpp:334]     Train net output #0: loss = 0.0556347 (* 1 = 0.0556347 loss)
I0814 18:47:14.333967 10451 sgd_solver.cpp:136] Iteration 26200, lr = 0.0590625, m = 0.9
I0814 18:47:15.973160 10451 solver.cpp:312] Iteration 26300 (61.0064 iter/s, 1.63917s/100 iter), loss = 0.00731394
I0814 18:47:15.973182 10451 solver.cpp:334]     Train net output #0: loss = 0.00731271 (* 1 = 0.00731271 loss)
I0814 18:47:15.973187 10451 sgd_solver.cpp:136] Iteration 26300, lr = 0.0589063, m = 0.9
I0814 18:47:17.579797 10451 solver.cpp:312] Iteration 26400 (62.2437 iter/s, 1.60659s/100 iter), loss = 0.0622092
I0814 18:47:17.579848 10451 solver.cpp:334]     Train net output #0: loss = 0.062208 (* 1 = 0.062208 loss)
I0814 18:47:17.579864 10451 sgd_solver.cpp:136] Iteration 26400, lr = 0.05875, m = 0.9
I0814 18:47:19.215502 10451 solver.cpp:312] Iteration 26500 (61.1376 iter/s, 1.63565s/100 iter), loss = 0.0254935
I0814 18:47:19.215528 10451 solver.cpp:334]     Train net output #0: loss = 0.0254924 (* 1 = 0.0254924 loss)
I0814 18:47:19.215535 10451 sgd_solver.cpp:136] Iteration 26500, lr = 0.0585938, m = 0.9
I0814 18:47:19.414412 10436 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 18:47:20.834434 10451 solver.cpp:312] Iteration 26600 (61.771 iter/s, 1.61888s/100 iter), loss = 0.0740183
I0814 18:47:20.834480 10451 solver.cpp:334]     Train net output #0: loss = 0.0740172 (* 1 = 0.0740172 loss)
I0814 18:47:20.834491 10451 sgd_solver.cpp:136] Iteration 26600, lr = 0.0584375, m = 0.9
I0814 18:47:22.429409 10451 solver.cpp:312] Iteration 26700 (62.6988 iter/s, 1.59493s/100 iter), loss = 0.21666
I0814 18:47:22.429435 10451 solver.cpp:334]     Train net output #0: loss = 0.216659 (* 1 = 0.216659 loss)
I0814 18:47:22.429440 10451 sgd_solver.cpp:136] Iteration 26700, lr = 0.0582813, m = 0.9
I0814 18:47:24.017623 10451 solver.cpp:312] Iteration 26800 (62.9659 iter/s, 1.58816s/100 iter), loss = 0.124589
I0814 18:47:24.017649 10451 solver.cpp:334]     Train net output #0: loss = 0.124587 (* 1 = 0.124587 loss)
I0814 18:47:24.017655 10451 sgd_solver.cpp:136] Iteration 26800, lr = 0.058125, m = 0.9
I0814 18:47:25.630852 10451 solver.cpp:312] Iteration 26900 (61.9894 iter/s, 1.61318s/100 iter), loss = 0.141671
I0814 18:47:25.630877 10451 solver.cpp:334]     Train net output #0: loss = 0.14167 (* 1 = 0.14167 loss)
I0814 18:47:25.630883 10451 sgd_solver.cpp:136] Iteration 26900, lr = 0.0579687, m = 0.9
I0814 18:47:27.265558 10451 solver.cpp:509] Iteration 27000, Testing net (#0)
I0814 18:47:28.101840 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.813825
I0814 18:47:28.101857 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.985883
I0814 18:47:28.101862 10451 solver.cpp:594]     Test net output #2: loss = 0.690739 (* 1 = 0.690739 loss)
I0814 18:47:28.101877 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.836296s
I0814 18:47:28.119418 10451 solver.cpp:312] Iteration 27000 (40.1849 iter/s, 2.4885s/100 iter), loss = 0.0122398
I0814 18:47:28.119436 10451 solver.cpp:334]     Train net output #0: loss = 0.0122387 (* 1 = 0.0122387 loss)
I0814 18:47:28.119441 10451 sgd_solver.cpp:136] Iteration 27000, lr = 0.0578125, m = 0.9
I0814 18:47:29.759593 10451 solver.cpp:312] Iteration 27100 (60.9711 iter/s, 1.64012s/100 iter), loss = 0.0148728
I0814 18:47:29.759657 10451 solver.cpp:334]     Train net output #0: loss = 0.0148718 (* 1 = 0.0148718 loss)
I0814 18:47:29.759677 10451 sgd_solver.cpp:136] Iteration 27100, lr = 0.0576563, m = 0.9
I0814 18:47:31.435333 10451 solver.cpp:312] Iteration 27200 (59.677 iter/s, 1.67569s/100 iter), loss = 0.0304983
I0814 18:47:31.435462 10451 solver.cpp:334]     Train net output #0: loss = 0.0304973 (* 1 = 0.0304973 loss)
I0814 18:47:31.435483 10451 sgd_solver.cpp:136] Iteration 27200, lr = 0.0575, m = 0.9
I0814 18:47:33.044389 10451 solver.cpp:312] Iteration 27300 (62.1503 iter/s, 1.609s/100 iter), loss = 0.0388495
I0814 18:47:33.044453 10451 solver.cpp:334]     Train net output #0: loss = 0.0388484 (* 1 = 0.0388484 loss)
I0814 18:47:33.044473 10451 sgd_solver.cpp:136] Iteration 27300, lr = 0.0573438, m = 0.9
I0814 18:47:34.657492 10451 solver.cpp:312] Iteration 27400 (61.9941 iter/s, 1.61306s/100 iter), loss = 0.0154386
I0814 18:47:34.657539 10451 solver.cpp:334]     Train net output #0: loss = 0.0154375 (* 1 = 0.0154375 loss)
I0814 18:47:34.657552 10451 sgd_solver.cpp:136] Iteration 27400, lr = 0.0571875, m = 0.9
I0814 18:47:36.297762 10451 solver.cpp:312] Iteration 27500 (60.9675 iter/s, 1.64022s/100 iter), loss = 0.0197837
I0814 18:47:36.297812 10451 solver.cpp:334]     Train net output #0: loss = 0.0197826 (* 1 = 0.0197826 loss)
I0814 18:47:36.297827 10451 sgd_solver.cpp:136] Iteration 27500, lr = 0.0570313, m = 0.9
I0814 18:47:37.919636 10451 solver.cpp:312] Iteration 27600 (61.659 iter/s, 1.62182s/100 iter), loss = 0.040675
I0814 18:47:37.919661 10451 solver.cpp:334]     Train net output #0: loss = 0.0406739 (* 1 = 0.0406739 loss)
I0814 18:47:37.919667 10451 sgd_solver.cpp:136] Iteration 27600, lr = 0.056875, m = 0.9
I0814 18:47:39.511689 10451 solver.cpp:312] Iteration 27700 (62.8139 iter/s, 1.592s/100 iter), loss = 0.0184342
I0814 18:47:39.511756 10451 solver.cpp:334]     Train net output #0: loss = 0.018433 (* 1 = 0.018433 loss)
I0814 18:47:39.511776 10451 sgd_solver.cpp:136] Iteration 27700, lr = 0.0567187, m = 0.9
I0814 18:47:41.122467 10451 solver.cpp:312] Iteration 27800 (62.0839 iter/s, 1.61072s/100 iter), loss = 0.0131205
I0814 18:47:41.122490 10451 solver.cpp:334]     Train net output #0: loss = 0.0131193 (* 1 = 0.0131193 loss)
I0814 18:47:41.122496 10451 sgd_solver.cpp:136] Iteration 27800, lr = 0.0565625, m = 0.9
I0814 18:47:42.724033 10451 solver.cpp:312] Iteration 27900 (62.4408 iter/s, 1.60152s/100 iter), loss = 0.0672494
I0814 18:47:42.724079 10451 solver.cpp:334]     Train net output #0: loss = 0.0672481 (* 1 = 0.0672481 loss)
I0814 18:47:42.724094 10451 sgd_solver.cpp:136] Iteration 27900, lr = 0.0564062, m = 0.9
I0814 18:47:44.335402 10451 solver.cpp:509] Iteration 28000, Testing net (#0)
I0814 18:47:45.168401 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.767354
I0814 18:47:45.168418 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.979706
I0814 18:47:45.168426 10451 solver.cpp:594]     Test net output #2: loss = 1.02632 (* 1 = 1.02632 loss)
I0814 18:47:45.168442 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.833017s
I0814 18:47:45.196828 10451 solver.cpp:312] Iteration 28000 (40.4413 iter/s, 2.47272s/100 iter), loss = 0.0260342
I0814 18:47:45.196856 10451 solver.cpp:334]     Train net output #0: loss = 0.026033 (* 1 = 0.026033 loss)
I0814 18:47:45.196861 10451 sgd_solver.cpp:136] Iteration 28000, lr = 0.05625, m = 0.9
I0814 18:47:46.808748 10451 solver.cpp:312] Iteration 28100 (62.0397 iter/s, 1.61187s/100 iter), loss = 0.014554
I0814 18:47:46.808773 10451 solver.cpp:334]     Train net output #0: loss = 0.0145528 (* 1 = 0.0145528 loss)
I0814 18:47:46.808779 10451 sgd_solver.cpp:136] Iteration 28100, lr = 0.0560938, m = 0.9
I0814 18:47:48.438803 10451 solver.cpp:312] Iteration 28200 (61.3495 iter/s, 1.63001s/100 iter), loss = 0.0090445
I0814 18:47:48.438829 10451 solver.cpp:334]     Train net output #0: loss = 0.00904335 (* 1 = 0.00904335 loss)
I0814 18:47:48.438835 10451 sgd_solver.cpp:136] Iteration 28200, lr = 0.0559375, m = 0.9
I0814 18:47:50.030465 10451 solver.cpp:312] Iteration 28300 (62.8293 iter/s, 1.59161s/100 iter), loss = 0.0349209
I0814 18:47:50.030488 10451 solver.cpp:334]     Train net output #0: loss = 0.0349198 (* 1 = 0.0349198 loss)
I0814 18:47:50.030493 10451 sgd_solver.cpp:136] Iteration 28300, lr = 0.0557813, m = 0.9
I0814 18:47:51.664898 10451 solver.cpp:312] Iteration 28400 (61.1852 iter/s, 1.63438s/100 iter), loss = 0.0254241
I0814 18:47:51.664981 10451 solver.cpp:334]     Train net output #0: loss = 0.025423 (* 1 = 0.025423 loss)
I0814 18:47:51.665004 10451 sgd_solver.cpp:136] Iteration 28400, lr = 0.055625, m = 0.9
I0814 18:47:53.289290 10451 solver.cpp:312] Iteration 28500 (61.5634 iter/s, 1.62434s/100 iter), loss = 0.0543113
I0814 18:47:53.289340 10451 solver.cpp:334]     Train net output #0: loss = 0.0543102 (* 1 = 0.0543102 loss)
I0814 18:47:53.289350 10451 sgd_solver.cpp:136] Iteration 28500, lr = 0.0554687, m = 0.9
I0814 18:47:54.934948 10451 solver.cpp:312] Iteration 28600 (60.7679 iter/s, 1.64561s/100 iter), loss = 0.0701881
I0814 18:47:54.934975 10451 solver.cpp:334]     Train net output #0: loss = 0.070187 (* 1 = 0.070187 loss)
I0814 18:47:54.934983 10451 sgd_solver.cpp:136] Iteration 28600, lr = 0.0553125, m = 0.9
I0814 18:47:56.550459 10451 solver.cpp:312] Iteration 28700 (61.9019 iter/s, 1.61546s/100 iter), loss = 0.00852625
I0814 18:47:56.550487 10451 solver.cpp:334]     Train net output #0: loss = 0.00852507 (* 1 = 0.00852507 loss)
I0814 18:47:56.550493 10451 sgd_solver.cpp:136] Iteration 28700, lr = 0.0551562, m = 0.9
I0814 18:47:58.171772 10451 solver.cpp:312] Iteration 28800 (61.6804 iter/s, 1.62126s/100 iter), loss = 0.0221278
I0814 18:47:58.171797 10451 solver.cpp:334]     Train net output #0: loss = 0.0221266 (* 1 = 0.0221266 loss)
I0814 18:47:58.171802 10451 sgd_solver.cpp:136] Iteration 28800, lr = 0.055, m = 0.9
I0814 18:47:59.803843 10451 solver.cpp:312] Iteration 28900 (61.2736 iter/s, 1.63202s/100 iter), loss = 0.122922
I0814 18:47:59.803894 10451 solver.cpp:334]     Train net output #0: loss = 0.122921 (* 1 = 0.122921 loss)
I0814 18:47:59.803908 10451 sgd_solver.cpp:136] Iteration 28900, lr = 0.0548437, m = 0.9
I0814 18:48:01.430172 10451 solver.cpp:509] Iteration 29000, Testing net (#0)
I0814 18:48:02.259057 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.839707
I0814 18:48:02.259125 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989706
I0814 18:48:02.259135 10451 solver.cpp:594]     Test net output #2: loss = 0.662421 (* 1 = 0.662421 loss)
I0814 18:48:02.259151 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.828956s
I0814 18:48:02.276635 10451 solver.cpp:312] Iteration 29000 (40.4413 iter/s, 2.47272s/100 iter), loss = 0.0165589
I0814 18:48:02.276652 10451 solver.cpp:334]     Train net output #0: loss = 0.0165577 (* 1 = 0.0165577 loss)
I0814 18:48:02.276659 10451 sgd_solver.cpp:136] Iteration 29000, lr = 0.0546875, m = 0.9
I0814 18:48:03.916640 10451 solver.cpp:312] Iteration 29100 (60.9774 iter/s, 1.63995s/100 iter), loss = 0.0485322
I0814 18:48:03.916687 10451 solver.cpp:334]     Train net output #0: loss = 0.048531 (* 1 = 0.048531 loss)
I0814 18:48:03.916694 10451 sgd_solver.cpp:136] Iteration 29100, lr = 0.0545313, m = 0.9
I0814 18:48:05.544513 10451 solver.cpp:312] Iteration 29200 (61.4319 iter/s, 1.62782s/100 iter), loss = 0.0586282
I0814 18:48:05.544615 10451 solver.cpp:334]     Train net output #0: loss = 0.058627 (* 1 = 0.058627 loss)
I0814 18:48:05.544625 10451 sgd_solver.cpp:136] Iteration 29200, lr = 0.054375, m = 0.9
I0814 18:48:07.215697 10451 solver.cpp:312] Iteration 29300 (59.8396 iter/s, 1.67114s/100 iter), loss = 0.0599032
I0814 18:48:07.215744 10451 solver.cpp:334]     Train net output #0: loss = 0.059902 (* 1 = 0.059902 loss)
I0814 18:48:07.215754 10451 sgd_solver.cpp:136] Iteration 29300, lr = 0.0542188, m = 0.9
I0814 18:48:08.868568 10451 solver.cpp:312] Iteration 29400 (60.5027 iter/s, 1.65282s/100 iter), loss = 0.0132989
I0814 18:48:08.868592 10451 solver.cpp:334]     Train net output #0: loss = 0.0132976 (* 1 = 0.0132976 loss)
I0814 18:48:08.868597 10451 sgd_solver.cpp:136] Iteration 29400, lr = 0.0540625, m = 0.9
I0814 18:48:10.472684 10451 solver.cpp:312] Iteration 29500 (62.3416 iter/s, 1.60406s/100 iter), loss = 0.0111242
I0814 18:48:10.472744 10451 solver.cpp:334]     Train net output #0: loss = 0.011123 (* 1 = 0.011123 loss)
I0814 18:48:10.472761 10451 sgd_solver.cpp:136] Iteration 29500, lr = 0.0539063, m = 0.9
I0814 18:48:12.081426 10451 solver.cpp:312] Iteration 29600 (62.1623 iter/s, 1.60869s/100 iter), loss = 0.0608884
I0814 18:48:12.081454 10451 solver.cpp:334]     Train net output #0: loss = 0.0608871 (* 1 = 0.0608871 loss)
I0814 18:48:12.081461 10451 sgd_solver.cpp:136] Iteration 29600, lr = 0.05375, m = 0.9
I0814 18:48:13.688263 10451 solver.cpp:312] Iteration 29700 (62.2361 iter/s, 1.60679s/100 iter), loss = 0.123161
I0814 18:48:13.688325 10451 solver.cpp:334]     Train net output #0: loss = 0.12316 (* 1 = 0.12316 loss)
I0814 18:48:13.688344 10451 sgd_solver.cpp:136] Iteration 29700, lr = 0.0535938, m = 0.9
I0814 18:48:15.361147 10451 solver.cpp:312] Iteration 29800 (59.7788 iter/s, 1.67283s/100 iter), loss = 0.085298
I0814 18:48:15.361197 10451 solver.cpp:334]     Train net output #0: loss = 0.0852969 (* 1 = 0.0852969 loss)
I0814 18:48:15.361212 10451 sgd_solver.cpp:136] Iteration 29800, lr = 0.0534375, m = 0.9
I0814 18:48:16.975670 10451 solver.cpp:312] Iteration 29900 (61.9398 iter/s, 1.61447s/100 iter), loss = 0.106392
I0814 18:48:16.975718 10451 solver.cpp:334]     Train net output #0: loss = 0.106391 (* 1 = 0.106391 loss)
I0814 18:48:16.975729 10451 sgd_solver.cpp:136] Iteration 29900, lr = 0.0532812, m = 0.9
I0814 18:48:18.596055 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_30000.caffemodel
I0814 18:48:18.605531 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_30000.solverstate
I0814 18:48:18.610255 10451 solver.cpp:509] Iteration 30000, Testing net (#0)
I0814 18:48:19.419006 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.865295
I0814 18:48:19.419025 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993235
I0814 18:48:19.419030 10451 solver.cpp:594]     Test net output #2: loss = 0.494331 (* 1 = 0.494331 loss)
I0814 18:48:19.419068 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.808791s
I0814 18:48:19.434702 10451 solver.cpp:312] Iteration 30000 (40.6676 iter/s, 2.45896s/100 iter), loss = 0.0279261
I0814 18:48:19.434720 10451 solver.cpp:334]     Train net output #0: loss = 0.027925 (* 1 = 0.027925 loss)
I0814 18:48:19.434725 10451 sgd_solver.cpp:136] Iteration 30000, lr = 0.053125, m = 0.9
I0814 18:48:21.040349 10451 solver.cpp:312] Iteration 30100 (62.2823 iter/s, 1.60559s/100 iter), loss = 0.0567074
I0814 18:48:21.040392 10451 solver.cpp:334]     Train net output #0: loss = 0.0567064 (* 1 = 0.0567064 loss)
I0814 18:48:21.040405 10451 sgd_solver.cpp:136] Iteration 30100, lr = 0.0529688, m = 0.9
I0814 18:48:22.632340 10451 solver.cpp:312] Iteration 30200 (62.8163 iter/s, 1.59194s/100 iter), loss = 0.0150462
I0814 18:48:22.632367 10451 solver.cpp:334]     Train net output #0: loss = 0.0150452 (* 1 = 0.0150452 loss)
I0814 18:48:22.632374 10451 sgd_solver.cpp:136] Iteration 30200, lr = 0.0528125, m = 0.9
I0814 18:48:24.271828 10451 solver.cpp:312] Iteration 30300 (60.9965 iter/s, 1.63944s/100 iter), loss = 0.0230297
I0814 18:48:24.271989 10451 solver.cpp:334]     Train net output #0: loss = 0.0230286 (* 1 = 0.0230286 loss)
I0814 18:48:24.272013 10451 sgd_solver.cpp:136] Iteration 30300, lr = 0.0526563, m = 0.9
I0814 18:48:25.895375 10451 solver.cpp:312] Iteration 30400 (61.5956 iter/s, 1.62349s/100 iter), loss = 0.00905156
I0814 18:48:25.895419 10451 solver.cpp:334]     Train net output #0: loss = 0.00905052 (* 1 = 0.00905052 loss)
I0814 18:48:25.895432 10451 sgd_solver.cpp:136] Iteration 30400, lr = 0.0525, m = 0.9
I0814 18:48:27.500071 10451 solver.cpp:312] Iteration 30500 (62.319 iter/s, 1.60465s/100 iter), loss = 0.030611
I0814 18:48:27.500128 10451 solver.cpp:334]     Train net output #0: loss = 0.0306099 (* 1 = 0.0306099 loss)
I0814 18:48:27.500151 10451 sgd_solver.cpp:136] Iteration 30500, lr = 0.0523438, m = 0.9
I0814 18:48:29.081715 10451 solver.cpp:312] Iteration 30600 (63.2272 iter/s, 1.5816s/100 iter), loss = 0.0848983
I0814 18:48:29.081786 10451 solver.cpp:334]     Train net output #0: loss = 0.0848972 (* 1 = 0.0848972 loss)
I0814 18:48:29.081807 10451 sgd_solver.cpp:136] Iteration 30600, lr = 0.0521875, m = 0.9
I0814 18:48:30.672725 10451 solver.cpp:312] Iteration 30700 (62.8552 iter/s, 1.59096s/100 iter), loss = 0.0894887
I0814 18:48:30.672792 10451 solver.cpp:334]     Train net output #0: loss = 0.0894875 (* 1 = 0.0894875 loss)
I0814 18:48:30.672812 10451 sgd_solver.cpp:136] Iteration 30700, lr = 0.0520312, m = 0.9
I0814 18:48:32.324086 10451 solver.cpp:312] Iteration 30800 (60.558 iter/s, 1.65131s/100 iter), loss = 0.0707794
I0814 18:48:32.324203 10451 solver.cpp:334]     Train net output #0: loss = 0.0707782 (* 1 = 0.0707782 loss)
I0814 18:48:32.324220 10451 sgd_solver.cpp:136] Iteration 30800, lr = 0.051875, m = 0.9
I0814 18:48:33.986016 10451 solver.cpp:312] Iteration 30900 (60.173 iter/s, 1.66188s/100 iter), loss = 0.0251768
I0814 18:48:33.986079 10451 solver.cpp:334]     Train net output #0: loss = 0.0251756 (* 1 = 0.0251756 loss)
I0814 18:48:33.986099 10451 sgd_solver.cpp:136] Iteration 30900, lr = 0.0517187, m = 0.9
I0814 18:48:35.544252 10451 solver.cpp:509] Iteration 31000, Testing net (#0)
I0814 18:48:35.844640 10438 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 18:48:36.365428 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.846472
I0814 18:48:36.365448 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992647
I0814 18:48:36.365453 10451 solver.cpp:594]     Test net output #2: loss = 0.620303 (* 1 = 0.620303 loss)
I0814 18:48:36.365468 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.821195s
I0814 18:48:36.392905 10451 solver.cpp:312] Iteration 31000 (41.5487 iter/s, 2.40682s/100 iter), loss = 0.0358153
I0814 18:48:36.392940 10451 solver.cpp:334]     Train net output #0: loss = 0.0358142 (* 1 = 0.0358142 loss)
I0814 18:48:36.392951 10451 sgd_solver.cpp:136] Iteration 31000, lr = 0.0515625, m = 0.9
I0814 18:48:38.034559 10451 solver.cpp:312] Iteration 31100 (60.9161 iter/s, 1.6416s/100 iter), loss = 0.0824393
I0814 18:48:38.034621 10451 solver.cpp:334]     Train net output #0: loss = 0.0824382 (* 1 = 0.0824382 loss)
I0814 18:48:38.034638 10451 sgd_solver.cpp:136] Iteration 31100, lr = 0.0514063, m = 0.9
I0814 18:48:39.620373 10451 solver.cpp:312] Iteration 31200 (63.0611 iter/s, 1.58576s/100 iter), loss = 0.0278474
I0814 18:48:39.620395 10451 solver.cpp:334]     Train net output #0: loss = 0.0278463 (* 1 = 0.0278463 loss)
I0814 18:48:39.620400 10451 sgd_solver.cpp:136] Iteration 31200, lr = 0.05125, m = 0.9
I0814 18:48:41.216352 10451 solver.cpp:312] Iteration 31300 (62.6594 iter/s, 1.59593s/100 iter), loss = 0.008993
I0814 18:48:41.216421 10451 solver.cpp:334]     Train net output #0: loss = 0.00899187 (* 1 = 0.00899187 loss)
I0814 18:48:41.216435 10451 sgd_solver.cpp:136] Iteration 31300, lr = 0.0510938, m = 0.9
I0814 18:48:42.824124 10451 solver.cpp:312] Iteration 31400 (62.1998 iter/s, 1.60772s/100 iter), loss = 0.0120592
I0814 18:48:42.824179 10451 solver.cpp:334]     Train net output #0: loss = 0.0120581 (* 1 = 0.0120581 loss)
I0814 18:48:42.824193 10451 sgd_solver.cpp:136] Iteration 31400, lr = 0.0509375, m = 0.9
I0814 18:48:44.410578 10451 solver.cpp:312] Iteration 31500 (63.0358 iter/s, 1.5864s/100 iter), loss = 0.0820099
I0814 18:48:44.410630 10451 solver.cpp:334]     Train net output #0: loss = 0.0820088 (* 1 = 0.0820088 loss)
I0814 18:48:44.410647 10451 sgd_solver.cpp:136] Iteration 31500, lr = 0.0507812, m = 0.9
I0814 18:48:46.032886 10451 solver.cpp:312] Iteration 31600 (61.6425 iter/s, 1.62226s/100 iter), loss = 0.0560098
I0814 18:48:46.032909 10451 solver.cpp:334]     Train net output #0: loss = 0.0560087 (* 1 = 0.0560087 loss)
I0814 18:48:46.032915 10451 sgd_solver.cpp:136] Iteration 31600, lr = 0.050625, m = 0.9
I0814 18:48:47.651834 10451 solver.cpp:312] Iteration 31700 (61.7705 iter/s, 1.6189s/100 iter), loss = 0.0117635
I0814 18:48:47.651895 10451 solver.cpp:334]     Train net output #0: loss = 0.0117624 (* 1 = 0.0117624 loss)
I0814 18:48:47.651913 10451 sgd_solver.cpp:136] Iteration 31700, lr = 0.0504688, m = 0.9
I0814 18:48:49.321336 10451 solver.cpp:312] Iteration 31800 (59.9 iter/s, 1.66945s/100 iter), loss = 0.00448515
I0814 18:48:49.321405 10451 solver.cpp:334]     Train net output #0: loss = 0.00448405 (* 1 = 0.00448405 loss)
I0814 18:48:49.321429 10451 sgd_solver.cpp:136] Iteration 31800, lr = 0.0503125, m = 0.9
I0814 18:48:50.949584 10451 solver.cpp:312] Iteration 31900 (61.4175 iter/s, 1.6282s/100 iter), loss = 0.0477318
I0814 18:48:50.949652 10451 solver.cpp:334]     Train net output #0: loss = 0.0477307 (* 1 = 0.0477307 loss)
I0814 18:48:50.949672 10451 sgd_solver.cpp:136] Iteration 31900, lr = 0.0501562, m = 0.9
I0814 18:48:52.518962 10451 solver.cpp:509] Iteration 32000, Testing net (#0)
I0814 18:48:53.365865 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.832354
I0814 18:48:53.365886 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.988824
I0814 18:48:53.365893 10451 solver.cpp:594]     Test net output #2: loss = 0.695846 (* 1 = 0.695846 loss)
I0814 18:48:53.365919 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.846932s
I0814 18:48:53.384780 10451 solver.cpp:312] Iteration 32000 (41.0656 iter/s, 2.43513s/100 iter), loss = 0.0147003
I0814 18:48:53.384811 10451 solver.cpp:334]     Train net output #0: loss = 0.0146992 (* 1 = 0.0146992 loss)
I0814 18:48:53.384825 10451 sgd_solver.cpp:136] Iteration 32000, lr = 0.05, m = 0.9
I0814 18:48:55.000095 10451 solver.cpp:312] Iteration 32100 (61.9094 iter/s, 1.61526s/100 iter), loss = 0.00240377
I0814 18:48:55.000160 10451 solver.cpp:334]     Train net output #0: loss = 0.00240266 (* 1 = 0.00240266 loss)
I0814 18:48:55.000180 10451 sgd_solver.cpp:136] Iteration 32100, lr = 0.0498438, m = 0.9
I0814 18:48:56.632297 10451 solver.cpp:312] Iteration 32200 (61.2689 iter/s, 1.63215s/100 iter), loss = 0.145529
I0814 18:48:56.632361 10451 solver.cpp:334]     Train net output #0: loss = 0.145528 (* 1 = 0.145528 loss)
I0814 18:48:56.632382 10451 sgd_solver.cpp:136] Iteration 32200, lr = 0.0496875, m = 0.9
I0814 18:48:58.260125 10451 solver.cpp:312] Iteration 32300 (61.4334 iter/s, 1.62778s/100 iter), loss = 0.0562469
I0814 18:48:58.260155 10451 solver.cpp:334]     Train net output #0: loss = 0.0562458 (* 1 = 0.0562458 loss)
I0814 18:48:58.260161 10451 sgd_solver.cpp:136] Iteration 32300, lr = 0.0495313, m = 0.9
I0814 18:48:59.895892 10451 solver.cpp:312] Iteration 32400 (61.1353 iter/s, 1.63572s/100 iter), loss = 0.0730606
I0814 18:48:59.895936 10451 solver.cpp:334]     Train net output #0: loss = 0.0730595 (* 1 = 0.0730595 loss)
I0814 18:48:59.895951 10451 sgd_solver.cpp:136] Iteration 32400, lr = 0.049375, m = 0.9
I0814 18:49:01.503353 10451 solver.cpp:312] Iteration 32500 (62.2118 iter/s, 1.60741s/100 iter), loss = 0.126334
I0814 18:49:01.503381 10451 solver.cpp:334]     Train net output #0: loss = 0.126332 (* 1 = 0.126332 loss)
I0814 18:49:01.503388 10451 sgd_solver.cpp:136] Iteration 32500, lr = 0.0492188, m = 0.9
I0814 18:49:03.103260 10451 solver.cpp:312] Iteration 32600 (62.5056 iter/s, 1.59986s/100 iter), loss = 0.105557
I0814 18:49:03.103354 10451 solver.cpp:334]     Train net output #0: loss = 0.105556 (* 1 = 0.105556 loss)
I0814 18:49:03.103368 10451 sgd_solver.cpp:136] Iteration 32600, lr = 0.0490625, m = 0.9
I0814 18:49:04.709373 10451 solver.cpp:312] Iteration 32700 (62.264 iter/s, 1.60606s/100 iter), loss = 0.0270023
I0814 18:49:04.709424 10451 solver.cpp:334]     Train net output #0: loss = 0.0270013 (* 1 = 0.0270013 loss)
I0814 18:49:04.709437 10451 sgd_solver.cpp:136] Iteration 32700, lr = 0.0489062, m = 0.9
I0814 18:49:06.323611 10451 solver.cpp:312] Iteration 32800 (61.9507 iter/s, 1.61419s/100 iter), loss = 0.114115
I0814 18:49:06.323659 10451 solver.cpp:334]     Train net output #0: loss = 0.114114 (* 1 = 0.114114 loss)
I0814 18:49:06.323673 10451 sgd_solver.cpp:136] Iteration 32800, lr = 0.04875, m = 0.9
I0814 18:49:07.936669 10451 solver.cpp:312] Iteration 32900 (61.9961 iter/s, 1.61301s/100 iter), loss = 0.0109862
I0814 18:49:07.936928 10451 solver.cpp:334]     Train net output #0: loss = 0.0109852 (* 1 = 0.0109852 loss)
I0814 18:49:07.936942 10451 sgd_solver.cpp:136] Iteration 32900, lr = 0.0485937, m = 0.9
I0814 18:49:09.560148 10451 solver.cpp:509] Iteration 33000, Testing net (#0)
I0814 18:49:10.385915 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.837942
I0814 18:49:10.385932 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992647
I0814 18:49:10.385937 10451 solver.cpp:594]     Test net output #2: loss = 0.615405 (* 1 = 0.615405 loss)
I0814 18:49:10.385959 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.825789s
I0814 18:49:10.401703 10451 solver.cpp:312] Iteration 33000 (40.5686 iter/s, 2.46496s/100 iter), loss = 0.0430765
I0814 18:49:10.401733 10451 solver.cpp:334]     Train net output #0: loss = 0.0430755 (* 1 = 0.0430755 loss)
I0814 18:49:10.401744 10451 sgd_solver.cpp:136] Iteration 33000, lr = 0.0484375, m = 0.9
I0814 18:49:11.988018 10451 solver.cpp:312] Iteration 33100 (63.0412 iter/s, 1.58626s/100 iter), loss = 0.271976
I0814 18:49:11.988044 10451 solver.cpp:334]     Train net output #0: loss = 0.271975 (* 1 = 0.271975 loss)
I0814 18:49:11.988049 10451 sgd_solver.cpp:136] Iteration 33100, lr = 0.0482813, m = 0.9
I0814 18:49:13.667670 10451 solver.cpp:312] Iteration 33200 (59.5379 iter/s, 1.6796s/100 iter), loss = 0.00432047
I0814 18:49:13.667696 10451 solver.cpp:334]     Train net output #0: loss = 0.00431939 (* 1 = 0.00431939 loss)
I0814 18:49:13.667702 10451 sgd_solver.cpp:136] Iteration 33200, lr = 0.048125, m = 0.9
I0814 18:49:15.290233 10451 solver.cpp:312] Iteration 33300 (61.6328 iter/s, 1.62251s/100 iter), loss = 0.1046
I0814 18:49:15.290293 10451 solver.cpp:334]     Train net output #0: loss = 0.104599 (* 1 = 0.104599 loss)
I0814 18:49:15.290311 10451 sgd_solver.cpp:136] Iteration 33300, lr = 0.0479688, m = 0.9
I0814 18:49:16.929900 10451 solver.cpp:312] Iteration 33400 (60.9898 iter/s, 1.63962s/100 iter), loss = 0.0341524
I0814 18:49:16.929965 10451 solver.cpp:334]     Train net output #0: loss = 0.0341514 (* 1 = 0.0341514 loss)
I0814 18:49:16.929986 10451 sgd_solver.cpp:136] Iteration 33400, lr = 0.0478125, m = 0.9
I0814 18:49:18.495329 10451 solver.cpp:312] Iteration 33500 (63.8821 iter/s, 1.56538s/100 iter), loss = 0.136702
I0814 18:49:18.495353 10451 solver.cpp:334]     Train net output #0: loss = 0.136701 (* 1 = 0.136701 loss)
I0814 18:49:18.495358 10451 sgd_solver.cpp:136] Iteration 33500, lr = 0.0476562, m = 0.9
I0814 18:49:20.140719 10451 solver.cpp:312] Iteration 33600 (60.7778 iter/s, 1.64534s/100 iter), loss = 0.0164037
I0814 18:49:20.140902 10451 solver.cpp:334]     Train net output #0: loss = 0.0164027 (* 1 = 0.0164027 loss)
I0814 18:49:20.140964 10451 sgd_solver.cpp:136] Iteration 33600, lr = 0.0475, m = 0.9
I0814 18:49:21.774711 10451 solver.cpp:312] Iteration 33700 (61.2017 iter/s, 1.63394s/100 iter), loss = 0.0105642
I0814 18:49:21.774780 10451 solver.cpp:334]     Train net output #0: loss = 0.0105631 (* 1 = 0.0105631 loss)
I0814 18:49:21.774798 10451 sgd_solver.cpp:136] Iteration 33700, lr = 0.0473437, m = 0.9
I0814 18:49:23.371799 10451 solver.cpp:312] Iteration 33800 (62.6159 iter/s, 1.59704s/100 iter), loss = 0.0359925
I0814 18:49:23.371837 10451 solver.cpp:334]     Train net output #0: loss = 0.0359915 (* 1 = 0.0359915 loss)
I0814 18:49:23.371845 10451 sgd_solver.cpp:136] Iteration 33800, lr = 0.0471875, m = 0.9
I0814 18:49:24.982461 10451 solver.cpp:312] Iteration 33900 (62.0882 iter/s, 1.61061s/100 iter), loss = 0.012779
I0814 18:49:24.982487 10451 solver.cpp:334]     Train net output #0: loss = 0.012778 (* 1 = 0.012778 loss)
I0814 18:49:24.982492 10451 sgd_solver.cpp:136] Iteration 33900, lr = 0.0470312, m = 0.9
I0814 18:49:26.591557 10451 solver.cpp:509] Iteration 34000, Testing net (#0)
I0814 18:49:27.424793 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.849413
I0814 18:49:27.424813 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993824
I0814 18:49:27.424819 10451 solver.cpp:594]     Test net output #2: loss = 0.621647 (* 1 = 0.621647 loss)
I0814 18:49:27.424836 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.833257s
I0814 18:49:27.445243 10451 solver.cpp:312] Iteration 34000 (40.6057 iter/s, 2.46271s/100 iter), loss = 0.0236849
I0814 18:49:27.445291 10451 solver.cpp:334]     Train net output #0: loss = 0.0236839 (* 1 = 0.0236839 loss)
I0814 18:49:27.445300 10451 sgd_solver.cpp:136] Iteration 34000, lr = 0.046875, m = 0.9
I0814 18:49:29.080417 10451 solver.cpp:312] Iteration 34100 (61.1575 iter/s, 1.63512s/100 iter), loss = 0.0189975
I0814 18:49:29.080440 10451 solver.cpp:334]     Train net output #0: loss = 0.0189965 (* 1 = 0.0189965 loss)
I0814 18:49:29.080446 10451 sgd_solver.cpp:136] Iteration 34100, lr = 0.0467188, m = 0.9
I0814 18:49:30.723062 10451 solver.cpp:312] Iteration 34200 (60.8793 iter/s, 1.6426s/100 iter), loss = 0.021231
I0814 18:49:30.723126 10451 solver.cpp:334]     Train net output #0: loss = 0.0212301 (* 1 = 0.0212301 loss)
I0814 18:49:30.723145 10451 sgd_solver.cpp:136] Iteration 34200, lr = 0.0465625, m = 0.9
I0814 18:49:32.320770 10451 solver.cpp:312] Iteration 34300 (62.5917 iter/s, 1.59766s/100 iter), loss = 0.0233357
I0814 18:49:32.320792 10451 solver.cpp:334]     Train net output #0: loss = 0.0233348 (* 1 = 0.0233348 loss)
I0814 18:49:32.320797 10451 sgd_solver.cpp:136] Iteration 34300, lr = 0.0464063, m = 0.9
I0814 18:49:33.920626 10451 solver.cpp:312] Iteration 34400 (62.5075 iter/s, 1.59981s/100 iter), loss = 0.0839406
I0814 18:49:33.920687 10451 solver.cpp:334]     Train net output #0: loss = 0.0839396 (* 1 = 0.0839396 loss)
I0814 18:49:33.920694 10451 sgd_solver.cpp:136] Iteration 34400, lr = 0.04625, m = 0.9
I0814 18:49:35.562902 10451 solver.cpp:312] Iteration 34500 (60.893 iter/s, 1.64222s/100 iter), loss = 0.0207696
I0814 18:49:35.562965 10451 solver.cpp:334]     Train net output #0: loss = 0.0207687 (* 1 = 0.0207687 loss)
I0814 18:49:35.562983 10451 sgd_solver.cpp:136] Iteration 34500, lr = 0.0460938, m = 0.9
I0814 18:49:37.158835 10451 solver.cpp:312] Iteration 34600 (62.6612 iter/s, 1.59588s/100 iter), loss = 0.0152108
I0814 18:49:37.158859 10451 solver.cpp:334]     Train net output #0: loss = 0.0152098 (* 1 = 0.0152098 loss)
I0814 18:49:37.158862 10451 sgd_solver.cpp:136] Iteration 34600, lr = 0.0459375, m = 0.9
I0814 18:49:38.773162 10451 solver.cpp:312] Iteration 34700 (61.9472 iter/s, 1.61428s/100 iter), loss = 0.00515084
I0814 18:49:38.773213 10451 solver.cpp:334]     Train net output #0: loss = 0.00514984 (* 1 = 0.00514984 loss)
I0814 18:49:38.773227 10451 sgd_solver.cpp:136] Iteration 34700, lr = 0.0457813, m = 0.9
I0814 18:49:40.352954 10451 solver.cpp:312] Iteration 34800 (63.3017 iter/s, 1.57974s/100 iter), loss = 0.0182675
I0814 18:49:40.353178 10451 solver.cpp:334]     Train net output #0: loss = 0.0182665 (* 1 = 0.0182665 loss)
I0814 18:49:40.353257 10451 sgd_solver.cpp:136] Iteration 34800, lr = 0.045625, m = 0.9
I0814 18:49:41.940711 10451 solver.cpp:312] Iteration 34900 (62.9838 iter/s, 1.58771s/100 iter), loss = 0.00600049
I0814 18:49:41.940732 10451 solver.cpp:334]     Train net output #0: loss = 0.00599946 (* 1 = 0.00599946 loss)
I0814 18:49:41.940738 10451 sgd_solver.cpp:136] Iteration 34900, lr = 0.0454687, m = 0.9
I0814 18:49:43.534709 10451 solver.cpp:509] Iteration 35000, Testing net (#0)
I0814 18:49:43.758369 10438 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 18:49:44.349493 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.830884
I0814 18:49:44.349514 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989706
I0814 18:49:44.349519 10451 solver.cpp:594]     Test net output #2: loss = 0.638632 (* 1 = 0.638632 loss)
I0814 18:49:44.349537 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.814806s
I0814 18:49:44.365080 10451 solver.cpp:312] Iteration 35000 (41.249 iter/s, 2.4243s/100 iter), loss = 0.011998
I0814 18:49:44.365097 10451 solver.cpp:334]     Train net output #0: loss = 0.011997 (* 1 = 0.011997 loss)
I0814 18:49:44.365103 10451 sgd_solver.cpp:136] Iteration 35000, lr = 0.0453125, m = 0.9
I0814 18:49:45.997088 10451 solver.cpp:312] Iteration 35100 (61.2762 iter/s, 1.63196s/100 iter), loss = 0.0718785
I0814 18:49:45.997113 10451 solver.cpp:334]     Train net output #0: loss = 0.0718774 (* 1 = 0.0718774 loss)
I0814 18:49:45.997119 10451 sgd_solver.cpp:136] Iteration 35100, lr = 0.0451563, m = 0.9
I0814 18:49:47.634649 10451 solver.cpp:312] Iteration 35200 (61.0683 iter/s, 1.63751s/100 iter), loss = 0.00875828
I0814 18:49:47.634678 10451 solver.cpp:334]     Train net output #0: loss = 0.00875724 (* 1 = 0.00875724 loss)
I0814 18:49:47.634686 10451 sgd_solver.cpp:136] Iteration 35200, lr = 0.045, m = 0.9
I0814 18:49:49.248739 10451 solver.cpp:312] Iteration 35300 (61.9564 iter/s, 1.61404s/100 iter), loss = 0.00456172
I0814 18:49:49.248764 10451 solver.cpp:334]     Train net output #0: loss = 0.00456071 (* 1 = 0.00456071 loss)
I0814 18:49:49.248769 10451 sgd_solver.cpp:136] Iteration 35300, lr = 0.0448438, m = 0.9
I0814 18:49:50.829383 10451 solver.cpp:312] Iteration 35400 (63.2673 iter/s, 1.5806s/100 iter), loss = 0.0229
I0814 18:49:50.829430 10451 solver.cpp:334]     Train net output #0: loss = 0.0228989 (* 1 = 0.0228989 loss)
I0814 18:49:50.829442 10451 sgd_solver.cpp:136] Iteration 35400, lr = 0.0446875, m = 0.9
I0814 18:49:52.461930 10451 solver.cpp:312] Iteration 35500 (61.2559 iter/s, 1.6325s/100 iter), loss = 0.109765
I0814 18:49:52.461956 10451 solver.cpp:334]     Train net output #0: loss = 0.109764 (* 1 = 0.109764 loss)
I0814 18:49:52.461961 10451 sgd_solver.cpp:136] Iteration 35500, lr = 0.0445313, m = 0.9
I0814 18:49:54.087716 10451 solver.cpp:312] Iteration 35600 (61.5105 iter/s, 1.62574s/100 iter), loss = 0.0853621
I0814 18:49:54.087764 10451 solver.cpp:334]     Train net output #0: loss = 0.0853611 (* 1 = 0.0853611 loss)
I0814 18:49:54.087776 10451 sgd_solver.cpp:136] Iteration 35600, lr = 0.044375, m = 0.9
I0814 18:49:55.684978 10451 solver.cpp:312] Iteration 35700 (62.6091 iter/s, 1.59721s/100 iter), loss = 0.132078
I0814 18:49:55.685004 10451 solver.cpp:334]     Train net output #0: loss = 0.132077 (* 1 = 0.132077 loss)
I0814 18:49:55.685009 10451 sgd_solver.cpp:136] Iteration 35700, lr = 0.0442187, m = 0.9
I0814 18:49:57.292235 10451 solver.cpp:312] Iteration 35800 (62.2198 iter/s, 1.60721s/100 iter), loss = 0.287182
I0814 18:49:57.292259 10451 solver.cpp:334]     Train net output #0: loss = 0.287181 (* 1 = 0.287181 loss)
I0814 18:49:57.292265 10451 sgd_solver.cpp:136] Iteration 35800, lr = 0.0440625, m = 0.9
I0814 18:49:58.874491 10451 solver.cpp:312] Iteration 35900 (63.2028 iter/s, 1.58221s/100 iter), loss = 0.00830345
I0814 18:49:58.874541 10451 solver.cpp:334]     Train net output #0: loss = 0.00830234 (* 1 = 0.00830234 loss)
I0814 18:49:58.874554 10451 sgd_solver.cpp:136] Iteration 35900, lr = 0.0439062, m = 0.9
I0814 18:50:00.469429 10451 solver.cpp:509] Iteration 36000, Testing net (#0)
I0814 18:50:01.285873 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.816766
I0814 18:50:01.285893 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989412
I0814 18:50:01.285898 10451 solver.cpp:594]     Test net output #2: loss = 0.742305 (* 1 = 0.742305 loss)
I0814 18:50:01.285913 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.816463s
I0814 18:50:01.303599 10451 solver.cpp:312] Iteration 36000 (41.1685 iter/s, 2.42904s/100 iter), loss = 0.0140326
I0814 18:50:01.303618 10451 solver.cpp:334]     Train net output #0: loss = 0.0140314 (* 1 = 0.0140314 loss)
I0814 18:50:01.303624 10451 sgd_solver.cpp:136] Iteration 36000, lr = 0.04375, m = 0.9
I0814 18:50:02.893963 10451 solver.cpp:312] Iteration 36100 (62.8808 iter/s, 1.59031s/100 iter), loss = 0.0218012
I0814 18:50:02.893987 10451 solver.cpp:334]     Train net output #0: loss = 0.0218 (* 1 = 0.0218 loss)
I0814 18:50:02.893992 10451 sgd_solver.cpp:136] Iteration 36100, lr = 0.0435938, m = 0.9
I0814 18:50:04.523744 10451 solver.cpp:312] Iteration 36200 (61.3597 iter/s, 1.62973s/100 iter), loss = 0.0106119
I0814 18:50:04.523846 10451 solver.cpp:334]     Train net output #0: loss = 0.0106108 (* 1 = 0.0106108 loss)
I0814 18:50:04.523859 10451 sgd_solver.cpp:136] Iteration 36200, lr = 0.0434375, m = 0.9
I0814 18:50:06.105769 10451 solver.cpp:312] Iteration 36300 (63.2122 iter/s, 1.58197s/100 iter), loss = 0.0305873
I0814 18:50:06.105794 10451 solver.cpp:334]     Train net output #0: loss = 0.0305862 (* 1 = 0.0305862 loss)
I0814 18:50:06.105799 10451 sgd_solver.cpp:136] Iteration 36300, lr = 0.0432813, m = 0.9
I0814 18:50:07.685899 10451 solver.cpp:312] Iteration 36400 (63.2879 iter/s, 1.58008s/100 iter), loss = 0.0481877
I0814 18:50:07.685961 10451 solver.cpp:334]     Train net output #0: loss = 0.0481866 (* 1 = 0.0481866 loss)
I0814 18:50:07.685981 10451 sgd_solver.cpp:136] Iteration 36400, lr = 0.043125, m = 0.9
I0814 18:50:09.321249 10451 solver.cpp:312] Iteration 36500 (61.151 iter/s, 1.6353s/100 iter), loss = 0.0366159
I0814 18:50:09.321393 10451 solver.cpp:334]     Train net output #0: loss = 0.0366148 (* 1 = 0.0366148 loss)
I0814 18:50:09.321410 10451 sgd_solver.cpp:136] Iteration 36500, lr = 0.0429688, m = 0.9
I0814 18:50:10.960520 10451 solver.cpp:312] Iteration 36600 (61.0045 iter/s, 1.63922s/100 iter), loss = 0.0297357
I0814 18:50:10.960542 10451 solver.cpp:334]     Train net output #0: loss = 0.0297346 (* 1 = 0.0297346 loss)
I0814 18:50:10.960548 10451 sgd_solver.cpp:136] Iteration 36600, lr = 0.0428125, m = 0.9
I0814 18:50:12.532585 10451 solver.cpp:312] Iteration 36700 (63.6125 iter/s, 1.57202s/100 iter), loss = 0.0475045
I0814 18:50:12.532613 10451 solver.cpp:334]     Train net output #0: loss = 0.0475034 (* 1 = 0.0475034 loss)
I0814 18:50:12.532618 10451 sgd_solver.cpp:136] Iteration 36700, lr = 0.0426563, m = 0.9
I0814 18:50:14.164801 10451 solver.cpp:312] Iteration 36800 (61.2684 iter/s, 1.63216s/100 iter), loss = 0.0030108
I0814 18:50:14.164872 10451 solver.cpp:334]     Train net output #0: loss = 0.00300968 (* 1 = 0.00300968 loss)
I0814 18:50:14.164893 10451 sgd_solver.cpp:136] Iteration 36800, lr = 0.0425, m = 0.9
I0814 18:50:15.797813 10451 solver.cpp:312] Iteration 36900 (61.2384 iter/s, 1.63296s/100 iter), loss = 0.164065
I0814 18:50:15.797884 10451 solver.cpp:334]     Train net output #0: loss = 0.164063 (* 1 = 0.164063 loss)
I0814 18:50:15.797907 10451 sgd_solver.cpp:136] Iteration 36900, lr = 0.0423437, m = 0.9
I0814 18:50:17.431097 10451 solver.cpp:509] Iteration 37000, Testing net (#0)
I0814 18:50:18.259825 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.865295
I0814 18:50:18.259845 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 18:50:18.259850 10451 solver.cpp:594]     Test net output #2: loss = 0.460349 (* 1 = 0.460349 loss)
I0814 18:50:18.259865 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.828745s
I0814 18:50:18.280473 10451 solver.cpp:312] Iteration 37000 (40.2805 iter/s, 2.48259s/100 iter), loss = 0.0130029
I0814 18:50:18.280491 10451 solver.cpp:334]     Train net output #0: loss = 0.0130018 (* 1 = 0.0130018 loss)
I0814 18:50:18.280495 10451 sgd_solver.cpp:136] Iteration 37000, lr = 0.0421875, m = 0.9
I0814 18:50:19.899005 10451 solver.cpp:312] Iteration 37100 (61.7864 iter/s, 1.61848s/100 iter), loss = 0.0284992
I0814 18:50:19.899052 10451 solver.cpp:334]     Train net output #0: loss = 0.0284981 (* 1 = 0.0284981 loss)
I0814 18:50:19.899063 10451 sgd_solver.cpp:136] Iteration 37100, lr = 0.0420313, m = 0.9
I0814 18:50:21.506511 10451 solver.cpp:312] Iteration 37200 (62.21 iter/s, 1.60746s/100 iter), loss = 0.052587
I0814 18:50:21.506541 10451 solver.cpp:334]     Train net output #0: loss = 0.0525859 (* 1 = 0.0525859 loss)
I0814 18:50:21.506546 10451 sgd_solver.cpp:136] Iteration 37200, lr = 0.041875, m = 0.9
I0814 18:50:23.100685 10451 solver.cpp:312] Iteration 37300 (62.7304 iter/s, 1.59412s/100 iter), loss = 0.00367564
I0814 18:50:23.100745 10451 solver.cpp:334]     Train net output #0: loss = 0.00367454 (* 1 = 0.00367454 loss)
I0814 18:50:23.100764 10451 sgd_solver.cpp:136] Iteration 37300, lr = 0.0417188, m = 0.9
I0814 18:50:24.712740 10451 solver.cpp:312] Iteration 37400 (62.0346 iter/s, 1.612s/100 iter), loss = 0.00318111
I0814 18:50:24.712783 10451 solver.cpp:334]     Train net output #0: loss = 0.00318 (* 1 = 0.00318 loss)
I0814 18:50:24.712790 10451 sgd_solver.cpp:136] Iteration 37400, lr = 0.0415625, m = 0.9
I0814 18:50:26.362512 10451 solver.cpp:312] Iteration 37500 (60.6162 iter/s, 1.64972s/100 iter), loss = 0.00733955
I0814 18:50:26.362537 10451 solver.cpp:334]     Train net output #0: loss = 0.00733843 (* 1 = 0.00733843 loss)
I0814 18:50:26.362543 10451 sgd_solver.cpp:136] Iteration 37500, lr = 0.0414063, m = 0.9
I0814 18:50:28.016585 10451 solver.cpp:312] Iteration 37600 (60.4587 iter/s, 1.65402s/100 iter), loss = 0.177557
I0814 18:50:28.016726 10451 solver.cpp:334]     Train net output #0: loss = 0.177556 (* 1 = 0.177556 loss)
I0814 18:50:28.016744 10451 sgd_solver.cpp:136] Iteration 37600, lr = 0.04125, m = 0.9
I0814 18:50:29.622297 10451 solver.cpp:312] Iteration 37700 (62.2797 iter/s, 1.60566s/100 iter), loss = 0.0137741
I0814 18:50:29.622324 10451 solver.cpp:334]     Train net output #0: loss = 0.013773 (* 1 = 0.013773 loss)
I0814 18:50:29.622331 10451 sgd_solver.cpp:136] Iteration 37700, lr = 0.0410937, m = 0.9
I0814 18:50:31.264061 10451 solver.cpp:312] Iteration 37800 (60.912 iter/s, 1.64171s/100 iter), loss = 0.0487533
I0814 18:50:31.264108 10451 solver.cpp:334]     Train net output #0: loss = 0.0487522 (* 1 = 0.0487522 loss)
I0814 18:50:31.264120 10451 sgd_solver.cpp:136] Iteration 37800, lr = 0.0409375, m = 0.9
I0814 18:50:32.894942 10451 solver.cpp:312] Iteration 37900 (61.3183 iter/s, 1.63083s/100 iter), loss = 0.0671668
I0814 18:50:32.894966 10451 solver.cpp:334]     Train net output #0: loss = 0.0671657 (* 1 = 0.0671657 loss)
I0814 18:50:32.894971 10451 sgd_solver.cpp:136] Iteration 37900, lr = 0.0407812, m = 0.9
I0814 18:50:34.543927 10451 solver.cpp:509] Iteration 38000, Testing net (#0)
I0814 18:50:35.358865 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.839707
I0814 18:50:35.358882 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.991765
I0814 18:50:35.358886 10451 solver.cpp:594]     Test net output #2: loss = 0.735563 (* 1 = 0.735563 loss)
I0814 18:50:35.358906 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.814957s
I0814 18:50:35.376727 10451 solver.cpp:312] Iteration 38000 (40.2947 iter/s, 2.48171s/100 iter), loss = 0.0135817
I0814 18:50:35.376745 10451 solver.cpp:334]     Train net output #0: loss = 0.0135805 (* 1 = 0.0135805 loss)
I0814 18:50:35.376751 10451 sgd_solver.cpp:136] Iteration 38000, lr = 0.040625, m = 0.9
I0814 18:50:36.992604 10451 solver.cpp:312] Iteration 38100 (61.8879 iter/s, 1.61583s/100 iter), loss = 0.00928718
I0814 18:50:36.992648 10451 solver.cpp:334]     Train net output #0: loss = 0.00928604 (* 1 = 0.00928604 loss)
I0814 18:50:36.992660 10451 sgd_solver.cpp:136] Iteration 38100, lr = 0.0404688, m = 0.9
I0814 18:50:38.616257 10451 solver.cpp:312] Iteration 38200 (61.5915 iter/s, 1.6236s/100 iter), loss = 0.0153909
I0814 18:50:38.616283 10451 solver.cpp:334]     Train net output #0: loss = 0.0153898 (* 1 = 0.0153898 loss)
I0814 18:50:38.616289 10451 sgd_solver.cpp:136] Iteration 38200, lr = 0.0403125, m = 0.9
I0814 18:50:40.219811 10451 solver.cpp:312] Iteration 38300 (62.3633 iter/s, 1.60351s/100 iter), loss = 0.0146341
I0814 18:50:40.219871 10451 solver.cpp:334]     Train net output #0: loss = 0.0146329 (* 1 = 0.0146329 loss)
I0814 18:50:40.219889 10451 sgd_solver.cpp:136] Iteration 38300, lr = 0.0401563, m = 0.9
I0814 18:50:41.830943 10451 solver.cpp:312] Iteration 38400 (62.0701 iter/s, 1.61108s/100 iter), loss = 0.0283117
I0814 18:50:41.830993 10451 solver.cpp:334]     Train net output #0: loss = 0.0283105 (* 1 = 0.0283105 loss)
I0814 18:50:41.831007 10451 sgd_solver.cpp:136] Iteration 38400, lr = 0.04, m = 0.9
I0814 18:50:43.482955 10451 solver.cpp:312] Iteration 38500 (60.5341 iter/s, 1.65196s/100 iter), loss = 0.0651713
I0814 18:50:43.483006 10451 solver.cpp:334]     Train net output #0: loss = 0.0651702 (* 1 = 0.0651702 loss)
I0814 18:50:43.483019 10451 sgd_solver.cpp:136] Iteration 38500, lr = 0.0398437, m = 0.9
I0814 18:50:45.148651 10451 solver.cpp:312] Iteration 38600 (60.0367 iter/s, 1.66565s/100 iter), loss = 0.0219931
I0814 18:50:45.148699 10451 solver.cpp:334]     Train net output #0: loss = 0.0219919 (* 1 = 0.0219919 loss)
I0814 18:50:45.148712 10451 sgd_solver.cpp:136] Iteration 38600, lr = 0.0396875, m = 0.9
I0814 18:50:46.722004 10451 solver.cpp:312] Iteration 38700 (63.5606 iter/s, 1.5733s/100 iter), loss = 0.0231006
I0814 18:50:46.722028 10451 solver.cpp:334]     Train net output #0: loss = 0.0230994 (* 1 = 0.0230994 loss)
I0814 18:50:46.722033 10451 sgd_solver.cpp:136] Iteration 38700, lr = 0.0395312, m = 0.9
I0814 18:50:48.349359 10451 solver.cpp:312] Iteration 38800 (61.4512 iter/s, 1.62731s/100 iter), loss = 0.00887007
I0814 18:50:48.349382 10451 solver.cpp:334]     Train net output #0: loss = 0.00886893 (* 1 = 0.00886893 loss)
I0814 18:50:48.349386 10451 sgd_solver.cpp:136] Iteration 38800, lr = 0.039375, m = 0.9
I0814 18:50:49.974524 10451 solver.cpp:312] Iteration 38900 (61.5345 iter/s, 1.62511s/100 iter), loss = 0.0196538
I0814 18:50:49.974566 10451 solver.cpp:334]     Train net output #0: loss = 0.0196527 (* 1 = 0.0196527 loss)
I0814 18:50:49.974572 10451 sgd_solver.cpp:136] Iteration 38900, lr = 0.0392187, m = 0.9
I0814 18:50:51.604910 10451 solver.cpp:509] Iteration 39000, Testing net (#0)
I0814 18:50:52.425328 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.843531
I0814 18:50:52.425348 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992647
I0814 18:50:52.425351 10451 solver.cpp:594]     Test net output #2: loss = 0.684127 (* 1 = 0.684127 loss)
I0814 18:50:52.425365 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.820435s
I0814 18:50:52.442770 10451 solver.cpp:312] Iteration 39000 (40.5156 iter/s, 2.46818s/100 iter), loss = 0.00276099
I0814 18:50:52.442802 10451 solver.cpp:334]     Train net output #0: loss = 0.00275985 (* 1 = 0.00275985 loss)
I0814 18:50:52.442829 10451 sgd_solver.cpp:136] Iteration 39000, lr = 0.0390625, m = 0.9
I0814 18:50:54.074367 10451 solver.cpp:312] Iteration 39100 (61.2917 iter/s, 1.63154s/100 iter), loss = 0.0155792
I0814 18:50:54.074390 10451 solver.cpp:334]     Train net output #0: loss = 0.0155781 (* 1 = 0.0155781 loss)
I0814 18:50:54.074395 10451 sgd_solver.cpp:136] Iteration 39100, lr = 0.0389063, m = 0.9
I0814 18:50:55.682654 10451 solver.cpp:312] Iteration 39200 (62.1799 iter/s, 1.60824s/100 iter), loss = 0.0111963
I0814 18:50:55.682718 10451 solver.cpp:334]     Train net output #0: loss = 0.0111952 (* 1 = 0.0111952 loss)
I0814 18:50:55.682739 10451 sgd_solver.cpp:136] Iteration 39200, lr = 0.03875, m = 0.9
I0814 18:50:57.306162 10451 solver.cpp:312] Iteration 39300 (61.5968 iter/s, 1.62346s/100 iter), loss = 0.00411076
I0814 18:50:57.306228 10451 solver.cpp:334]     Train net output #0: loss = 0.00410958 (* 1 = 0.00410958 loss)
I0814 18:50:57.306249 10451 sgd_solver.cpp:136] Iteration 39300, lr = 0.0385938, m = 0.9
I0814 18:50:58.761765 10436 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 18:50:58.919410 10451 solver.cpp:312] Iteration 39400 (61.9887 iter/s, 1.6132s/100 iter), loss = 0.0460615
I0814 18:50:58.919459 10451 solver.cpp:334]     Train net output #0: loss = 0.0460604 (* 1 = 0.0460604 loss)
I0814 18:50:58.919473 10451 sgd_solver.cpp:136] Iteration 39400, lr = 0.0384375, m = 0.9
I0814 18:51:00.569964 10451 solver.cpp:312] Iteration 39500 (60.5876 iter/s, 1.6505s/100 iter), loss = 0.0373622
I0814 18:51:00.569990 10451 solver.cpp:334]     Train net output #0: loss = 0.0373611 (* 1 = 0.0373611 loss)
I0814 18:51:00.569996 10451 sgd_solver.cpp:136] Iteration 39500, lr = 0.0382813, m = 0.9
I0814 18:51:02.183873 10451 solver.cpp:312] Iteration 39600 (61.9632 iter/s, 1.61386s/100 iter), loss = 0.0260769
I0814 18:51:02.183935 10451 solver.cpp:334]     Train net output #0: loss = 0.0260757 (* 1 = 0.0260757 loss)
I0814 18:51:02.183954 10451 sgd_solver.cpp:136] Iteration 39600, lr = 0.038125, m = 0.9
I0814 18:51:03.811596 10451 solver.cpp:312] Iteration 39700 (61.4373 iter/s, 1.62768s/100 iter), loss = 0.0189345
I0814 18:51:03.811619 10451 solver.cpp:334]     Train net output #0: loss = 0.0189333 (* 1 = 0.0189333 loss)
I0814 18:51:03.811625 10451 sgd_solver.cpp:136] Iteration 39700, lr = 0.0379688, m = 0.9
I0814 18:51:05.411988 10451 solver.cpp:312] Iteration 39800 (62.4866 iter/s, 1.60034s/100 iter), loss = 0.0664255
I0814 18:51:05.412096 10451 solver.cpp:334]     Train net output #0: loss = 0.0664243 (* 1 = 0.0664243 loss)
I0814 18:51:05.412117 10451 sgd_solver.cpp:136] Iteration 39800, lr = 0.0378125, m = 0.9
I0814 18:51:07.032528 10451 solver.cpp:312] Iteration 39900 (61.7098 iter/s, 1.62049s/100 iter), loss = 0.171182
I0814 18:51:07.032593 10451 solver.cpp:334]     Train net output #0: loss = 0.171181 (* 1 = 0.171181 loss)
I0814 18:51:07.032613 10451 sgd_solver.cpp:136] Iteration 39900, lr = 0.0376562, m = 0.9
I0814 18:51:08.613867 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_40000.caffemodel
I0814 18:51:08.621873 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_40000.solverstate
I0814 18:51:08.625613 10451 solver.cpp:509] Iteration 40000, Testing net (#0)
I0814 18:51:09.457631 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.807942
I0814 18:51:09.457651 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.984412
I0814 18:51:09.457656 10451 solver.cpp:594]     Test net output #2: loss = 0.882284 (* 1 = 0.882284 loss)
I0814 18:51:09.457681 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.832042s
I0814 18:51:09.476699 10451 solver.cpp:312] Iteration 40000 (40.9148 iter/s, 2.4441s/100 iter), loss = 0.0124523
I0814 18:51:09.476717 10451 solver.cpp:334]     Train net output #0: loss = 0.0124512 (* 1 = 0.0124512 loss)
I0814 18:51:09.476723 10451 sgd_solver.cpp:136] Iteration 40000, lr = 0.0375, m = 0.9
I0814 18:51:11.112432 10451 solver.cpp:312] Iteration 40100 (61.1366 iter/s, 1.63568s/100 iter), loss = 0.00379068
I0814 18:51:11.112486 10451 solver.cpp:334]     Train net output #0: loss = 0.00378953 (* 1 = 0.00378953 loss)
I0814 18:51:11.112504 10451 sgd_solver.cpp:136] Iteration 40100, lr = 0.0373438, m = 0.9
I0814 18:51:12.714201 10451 solver.cpp:312] Iteration 40200 (62.433 iter/s, 1.60172s/100 iter), loss = 0.00168164
I0814 18:51:12.714253 10451 solver.cpp:334]     Train net output #0: loss = 0.00168047 (* 1 = 0.00168047 loss)
I0814 18:51:12.714267 10451 sgd_solver.cpp:136] Iteration 40200, lr = 0.0371875, m = 0.9
I0814 18:51:14.352242 10451 solver.cpp:312] Iteration 40300 (61.0503 iter/s, 1.63799s/100 iter), loss = 0.0571777
I0814 18:51:14.352267 10451 solver.cpp:334]     Train net output #0: loss = 0.0571765 (* 1 = 0.0571765 loss)
I0814 18:51:14.352272 10451 sgd_solver.cpp:136] Iteration 40300, lr = 0.0370313, m = 0.9
I0814 18:51:15.978581 10451 solver.cpp:312] Iteration 40400 (61.4898 iter/s, 1.62629s/100 iter), loss = 0.044027
I0814 18:51:15.978605 10451 solver.cpp:334]     Train net output #0: loss = 0.0440259 (* 1 = 0.0440259 loss)
I0814 18:51:15.978612 10451 sgd_solver.cpp:136] Iteration 40400, lr = 0.036875, m = 0.9
I0814 18:51:17.604154 10451 solver.cpp:312] Iteration 40500 (61.5186 iter/s, 1.62552s/100 iter), loss = 0.0101699
I0814 18:51:17.604182 10451 solver.cpp:334]     Train net output #0: loss = 0.0101687 (* 1 = 0.0101687 loss)
I0814 18:51:17.604188 10451 sgd_solver.cpp:136] Iteration 40500, lr = 0.0367188, m = 0.9
I0814 18:51:19.185962 10451 solver.cpp:312] Iteration 40600 (63.2207 iter/s, 1.58176s/100 iter), loss = 0.000730814
I0814 18:51:19.185987 10451 solver.cpp:334]     Train net output #0: loss = 0.000729586 (* 1 = 0.000729586 loss)
I0814 18:51:19.185993 10451 sgd_solver.cpp:136] Iteration 40600, lr = 0.0365625, m = 0.9
I0814 18:51:20.790719 10451 solver.cpp:312] Iteration 40700 (62.3166 iter/s, 1.60471s/100 iter), loss = 0.00220906
I0814 18:51:20.790743 10451 solver.cpp:334]     Train net output #0: loss = 0.00220781 (* 1 = 0.00220781 loss)
I0814 18:51:20.790748 10451 sgd_solver.cpp:136] Iteration 40700, lr = 0.0364062, m = 0.9
I0814 18:51:22.374222 10451 solver.cpp:312] Iteration 40800 (63.1532 iter/s, 1.58345s/100 iter), loss = 0.00279215
I0814 18:51:22.374269 10451 solver.cpp:334]     Train net output #0: loss = 0.0027909 (* 1 = 0.0027909 loss)
I0814 18:51:22.374280 10451 sgd_solver.cpp:136] Iteration 40800, lr = 0.03625, m = 0.9
I0814 18:51:23.962272 10451 solver.cpp:312] Iteration 40900 (62.9722 iter/s, 1.588s/100 iter), loss = 0.00667532
I0814 18:51:23.962296 10451 solver.cpp:334]     Train net output #0: loss = 0.00667411 (* 1 = 0.00667411 loss)
I0814 18:51:23.962301 10451 sgd_solver.cpp:136] Iteration 40900, lr = 0.0360937, m = 0.9
I0814 18:51:25.556411 10451 solver.cpp:509] Iteration 41000, Testing net (#0)
I0814 18:51:26.391418 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.881766
I0814 18:51:26.391438 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.991177
I0814 18:51:26.391443 10451 solver.cpp:594]     Test net output #2: loss = 0.49656 (* 1 = 0.49656 loss)
I0814 18:51:26.391458 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.835026s
I0814 18:51:26.408761 10451 solver.cpp:312] Iteration 41000 (40.876 iter/s, 2.44642s/100 iter), loss = 0.0294485
I0814 18:51:26.408778 10451 solver.cpp:334]     Train net output #0: loss = 0.0294473 (* 1 = 0.0294473 loss)
I0814 18:51:26.408784 10451 sgd_solver.cpp:136] Iteration 41000, lr = 0.0359375, m = 0.9
I0814 18:51:28.001758 10451 solver.cpp:312] Iteration 41100 (62.7767 iter/s, 1.59295s/100 iter), loss = 0.0179635
I0814 18:51:28.001782 10451 solver.cpp:334]     Train net output #0: loss = 0.0179623 (* 1 = 0.0179623 loss)
I0814 18:51:28.001788 10451 sgd_solver.cpp:136] Iteration 41100, lr = 0.0357813, m = 0.9
I0814 18:51:29.602717 10451 solver.cpp:312] Iteration 41200 (62.4645 iter/s, 1.60091s/100 iter), loss = 0.00240601
I0814 18:51:29.602741 10451 solver.cpp:334]     Train net output #0: loss = 0.00240479 (* 1 = 0.00240479 loss)
I0814 18:51:29.602744 10451 sgd_solver.cpp:136] Iteration 41200, lr = 0.035625, m = 0.9
I0814 18:51:31.231551 10451 solver.cpp:312] Iteration 41300 (61.3955 iter/s, 1.62878s/100 iter), loss = 0.0119284
I0814 18:51:31.231575 10451 solver.cpp:334]     Train net output #0: loss = 0.0119271 (* 1 = 0.0119271 loss)
I0814 18:51:31.231580 10451 sgd_solver.cpp:136] Iteration 41300, lr = 0.0354688, m = 0.9
I0814 18:51:32.837158 10451 solver.cpp:312] Iteration 41400 (62.2836 iter/s, 1.60556s/100 iter), loss = 0.16754
I0814 18:51:32.837182 10451 solver.cpp:334]     Train net output #0: loss = 0.167539 (* 1 = 0.167539 loss)
I0814 18:51:32.837188 10451 sgd_solver.cpp:136] Iteration 41400, lr = 0.0353125, m = 0.9
I0814 18:51:34.456197 10451 solver.cpp:312] Iteration 41500 (61.7669 iter/s, 1.61899s/100 iter), loss = 0.0748028
I0814 18:51:34.456220 10451 solver.cpp:334]     Train net output #0: loss = 0.0748016 (* 1 = 0.0748016 loss)
I0814 18:51:34.456224 10451 sgd_solver.cpp:136] Iteration 41500, lr = 0.0351562, m = 0.9
I0814 18:51:36.056588 10451 solver.cpp:312] Iteration 41600 (62.4866 iter/s, 1.60034s/100 iter), loss = 0.0687077
I0814 18:51:36.056694 10451 solver.cpp:334]     Train net output #0: loss = 0.0687065 (* 1 = 0.0687065 loss)
I0814 18:51:36.056715 10451 sgd_solver.cpp:136] Iteration 41600, lr = 0.035, m = 0.9
I0814 18:51:37.683518 10451 solver.cpp:312] Iteration 41700 (61.4675 iter/s, 1.62688s/100 iter), loss = 0.00337201
I0814 18:51:37.683570 10451 solver.cpp:334]     Train net output #0: loss = 0.00337079 (* 1 = 0.00337079 loss)
I0814 18:51:37.683583 10451 sgd_solver.cpp:136] Iteration 41700, lr = 0.0348438, m = 0.9
I0814 18:51:39.281250 10451 solver.cpp:312] Iteration 41800 (62.5905 iter/s, 1.59769s/100 iter), loss = 0.0311508
I0814 18:51:39.281275 10451 solver.cpp:334]     Train net output #0: loss = 0.0311495 (* 1 = 0.0311495 loss)
I0814 18:51:39.281281 10451 sgd_solver.cpp:136] Iteration 41800, lr = 0.0346875, m = 0.9
I0814 18:51:40.863584 10451 solver.cpp:312] Iteration 41900 (63.1999 iter/s, 1.58228s/100 iter), loss = 0.00378588
I0814 18:51:40.863608 10451 solver.cpp:334]     Train net output #0: loss = 0.00378466 (* 1 = 0.00378466 loss)
I0814 18:51:40.863613 10451 sgd_solver.cpp:136] Iteration 41900, lr = 0.0345312, m = 0.9
I0814 18:51:42.471477 10451 solver.cpp:509] Iteration 42000, Testing net (#0)
I0814 18:51:43.292425 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.875884
I0814 18:51:43.292445 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993235
I0814 18:51:43.292453 10451 solver.cpp:594]     Test net output #2: loss = 0.516559 (* 1 = 0.516559 loss)
I0814 18:51:43.292469 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.820969s
I0814 18:51:43.308943 10451 solver.cpp:312] Iteration 42000 (40.8949 iter/s, 2.44529s/100 iter), loss = 0.0140624
I0814 18:51:43.308959 10451 solver.cpp:334]     Train net output #0: loss = 0.0140612 (* 1 = 0.0140612 loss)
I0814 18:51:43.308965 10451 sgd_solver.cpp:136] Iteration 42000, lr = 0.034375, m = 0.9
I0814 18:51:44.935335 10451 solver.cpp:312] Iteration 42100 (61.4877 iter/s, 1.62634s/100 iter), loss = 0.00130629
I0814 18:51:44.935398 10451 solver.cpp:334]     Train net output #0: loss = 0.00130508 (* 1 = 0.00130508 loss)
I0814 18:51:44.935416 10451 sgd_solver.cpp:136] Iteration 42100, lr = 0.0342188, m = 0.9
I0814 18:51:46.603922 10451 solver.cpp:312] Iteration 42200 (59.9327 iter/s, 1.66854s/100 iter), loss = 0.0121697
I0814 18:51:46.603947 10451 solver.cpp:334]     Train net output #0: loss = 0.0121685 (* 1 = 0.0121685 loss)
I0814 18:51:46.603953 10451 sgd_solver.cpp:136] Iteration 42200, lr = 0.0340625, m = 0.9
I0814 18:51:48.228363 10451 solver.cpp:312] Iteration 42300 (61.5616 iter/s, 1.62439s/100 iter), loss = 0.00853712
I0814 18:51:48.228411 10451 solver.cpp:334]     Train net output #0: loss = 0.00853589 (* 1 = 0.00853589 loss)
I0814 18:51:48.228426 10451 sgd_solver.cpp:136] Iteration 42300, lr = 0.0339063, m = 0.9
I0814 18:51:49.841279 10451 solver.cpp:312] Iteration 42400 (62.0015 iter/s, 1.61286s/100 iter), loss = 0.0219785
I0814 18:51:49.841424 10451 solver.cpp:334]     Train net output #0: loss = 0.0219773 (* 1 = 0.0219773 loss)
I0814 18:51:49.841442 10451 sgd_solver.cpp:136] Iteration 42400, lr = 0.03375, m = 0.9
I0814 18:51:51.462061 10451 solver.cpp:312] Iteration 42500 (61.7004 iter/s, 1.62074s/100 iter), loss = 0.0180231
I0814 18:51:51.462124 10451 solver.cpp:334]     Train net output #0: loss = 0.0180219 (* 1 = 0.0180219 loss)
I0814 18:51:51.462142 10451 sgd_solver.cpp:136] Iteration 42500, lr = 0.0335938, m = 0.9
I0814 18:51:53.085363 10451 solver.cpp:312] Iteration 42600 (61.6048 iter/s, 1.62325s/100 iter), loss = 0.0046968
I0814 18:51:53.085412 10451 solver.cpp:334]     Train net output #0: loss = 0.00469558 (* 1 = 0.00469558 loss)
I0814 18:51:53.085427 10451 sgd_solver.cpp:136] Iteration 42600, lr = 0.0334375, m = 0.9
I0814 18:51:54.716687 10451 solver.cpp:312] Iteration 42700 (61.3018 iter/s, 1.63127s/100 iter), loss = 0.0192307
I0814 18:51:54.716733 10451 solver.cpp:334]     Train net output #0: loss = 0.0192295 (* 1 = 0.0192295 loss)
I0814 18:51:54.716753 10451 sgd_solver.cpp:136] Iteration 42700, lr = 0.0332812, m = 0.9
I0814 18:51:56.334280 10451 solver.cpp:312] Iteration 42800 (61.8222 iter/s, 1.61754s/100 iter), loss = 0.00405898
I0814 18:51:56.334321 10451 solver.cpp:334]     Train net output #0: loss = 0.00405776 (* 1 = 0.00405776 loss)
I0814 18:51:56.334327 10451 sgd_solver.cpp:136] Iteration 42800, lr = 0.033125, m = 0.9
I0814 18:51:57.978305 10451 solver.cpp:312] Iteration 42900 (60.8281 iter/s, 1.64398s/100 iter), loss = 0.0852655
I0814 18:51:57.978368 10451 solver.cpp:334]     Train net output #0: loss = 0.0852642 (* 1 = 0.0852642 loss)
I0814 18:51:57.978385 10451 sgd_solver.cpp:136] Iteration 42900, lr = 0.0329687, m = 0.9
I0814 18:51:59.581404 10451 solver.cpp:509] Iteration 43000, Testing net (#0)
I0814 18:52:00.398563 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902942
I0814 18:52:00.398584 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992647
I0814 18:52:00.398591 10451 solver.cpp:594]     Test net output #2: loss = 0.394826 (* 1 = 0.394826 loss)
I0814 18:52:00.398607 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.817181s
I0814 18:52:00.417124 10451 solver.cpp:312] Iteration 43000 (41.0046 iter/s, 2.43875s/100 iter), loss = 0.0366942
I0814 18:52:00.417143 10451 solver.cpp:334]     Train net output #0: loss = 0.036693 (* 1 = 0.036693 loss)
I0814 18:52:00.417148 10451 sgd_solver.cpp:136] Iteration 43000, lr = 0.0328125, m = 0.9
I0814 18:52:02.028239 10451 solver.cpp:312] Iteration 43100 (62.0709 iter/s, 1.61106s/100 iter), loss = 0.00117044
I0814 18:52:02.028286 10451 solver.cpp:334]     Train net output #0: loss = 0.00116922 (* 1 = 0.00116922 loss)
I0814 18:52:02.028301 10451 sgd_solver.cpp:136] Iteration 43100, lr = 0.0326563, m = 0.9
I0814 18:52:03.656743 10451 solver.cpp:312] Iteration 43200 (61.4079 iter/s, 1.62845s/100 iter), loss = 0.0132958
I0814 18:52:03.656765 10451 solver.cpp:334]     Train net output #0: loss = 0.0132946 (* 1 = 0.0132946 loss)
I0814 18:52:03.656769 10451 sgd_solver.cpp:136] Iteration 43200, lr = 0.0325, m = 0.9
I0814 18:52:05.275022 10451 solver.cpp:312] Iteration 43300 (61.7959 iter/s, 1.61823s/100 iter), loss = 0.0137503
I0814 18:52:05.275048 10451 solver.cpp:334]     Train net output #0: loss = 0.0137491 (* 1 = 0.0137491 loss)
I0814 18:52:05.275054 10451 sgd_solver.cpp:136] Iteration 43300, lr = 0.0323438, m = 0.9
I0814 18:52:06.887450 10451 solver.cpp:312] Iteration 43400 (62.0201 iter/s, 1.61238s/100 iter), loss = 0.039078
I0814 18:52:06.887677 10451 solver.cpp:334]     Train net output #0: loss = 0.0390768 (* 1 = 0.0390768 loss)
I0814 18:52:06.887688 10451 sgd_solver.cpp:136] Iteration 43400, lr = 0.0321875, m = 0.9
I0814 18:52:08.522092 10451 solver.cpp:312] Iteration 43500 (61.1774 iter/s, 1.63459s/100 iter), loss = 0.00291157
I0814 18:52:08.522159 10451 solver.cpp:334]     Train net output #0: loss = 0.00291035 (* 1 = 0.00291035 loss)
I0814 18:52:08.522178 10451 sgd_solver.cpp:136] Iteration 43500, lr = 0.0320312, m = 0.9
I0814 18:52:10.165441 10451 solver.cpp:312] Iteration 43600 (60.8533 iter/s, 1.6433s/100 iter), loss = 0.0510304
I0814 18:52:10.165508 10451 solver.cpp:334]     Train net output #0: loss = 0.0510292 (* 1 = 0.0510292 loss)
I0814 18:52:10.165527 10451 sgd_solver.cpp:136] Iteration 43600, lr = 0.031875, m = 0.9
I0814 18:52:11.788556 10451 solver.cpp:312] Iteration 43700 (61.6118 iter/s, 1.62307s/100 iter), loss = 0.00586816
I0814 18:52:11.788579 10451 solver.cpp:334]     Train net output #0: loss = 0.00586692 (* 1 = 0.00586692 loss)
I0814 18:52:11.788585 10451 sgd_solver.cpp:136] Iteration 43700, lr = 0.0317187, m = 0.9
I0814 18:52:13.368582 10451 solver.cpp:312] Iteration 43800 (63.292 iter/s, 1.57998s/100 iter), loss = 0.0241371
I0814 18:52:13.368607 10451 solver.cpp:334]     Train net output #0: loss = 0.0241359 (* 1 = 0.0241359 loss)
I0814 18:52:13.368612 10451 sgd_solver.cpp:136] Iteration 43800, lr = 0.0315625, m = 0.9
I0814 18:52:15.009043 10451 solver.cpp:312] Iteration 43900 (60.9604 iter/s, 1.64041s/100 iter), loss = 0.0037756
I0814 18:52:15.009099 10451 solver.cpp:334]     Train net output #0: loss = 0.00377437 (* 1 = 0.00377437 loss)
I0814 18:52:15.009114 10451 sgd_solver.cpp:136] Iteration 43900, lr = 0.0314062, m = 0.9
I0814 18:52:15.577735 10436 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 18:52:16.627359 10451 solver.cpp:509] Iteration 44000, Testing net (#0)
I0814 18:52:17.456171 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.899413
I0814 18:52:17.456190 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992647
I0814 18:52:17.456195 10451 solver.cpp:594]     Test net output #2: loss = 0.417456 (* 1 = 0.417456 loss)
I0814 18:52:17.456210 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.828829s
I0814 18:52:17.471879 10451 solver.cpp:312] Iteration 44000 (40.6048 iter/s, 2.46276s/100 iter), loss = 0.0263823
I0814 18:52:17.471895 10451 solver.cpp:334]     Train net output #0: loss = 0.0263811 (* 1 = 0.0263811 loss)
I0814 18:52:17.471900 10451 sgd_solver.cpp:136] Iteration 44000, lr = 0.03125, m = 0.9
I0814 18:52:19.136248 10451 solver.cpp:312] Iteration 44100 (60.0846 iter/s, 1.66432s/100 iter), loss = 0.00904561
I0814 18:52:19.136382 10451 solver.cpp:334]     Train net output #0: loss = 0.00904439 (* 1 = 0.00904439 loss)
I0814 18:52:19.136401 10451 sgd_solver.cpp:136] Iteration 44100, lr = 0.0310938, m = 0.9
I0814 18:52:20.752770 10451 solver.cpp:312] Iteration 44200 (61.8632 iter/s, 1.61647s/100 iter), loss = 0.00138242
I0814 18:52:20.752818 10451 solver.cpp:334]     Train net output #0: loss = 0.0013812 (* 1 = 0.0013812 loss)
I0814 18:52:20.752831 10451 sgd_solver.cpp:136] Iteration 44200, lr = 0.0309375, m = 0.9
I0814 18:52:22.396009 10451 solver.cpp:312] Iteration 44300 (60.8573 iter/s, 1.64319s/100 iter), loss = 0.001634
I0814 18:52:22.396039 10451 solver.cpp:334]     Train net output #0: loss = 0.00163278 (* 1 = 0.00163278 loss)
I0814 18:52:22.396046 10451 sgd_solver.cpp:136] Iteration 44300, lr = 0.0307813, m = 0.9
I0814 18:52:24.014899 10451 solver.cpp:312] Iteration 44400 (61.7726 iter/s, 1.61884s/100 iter), loss = 0.00267269
I0814 18:52:24.014968 10451 solver.cpp:334]     Train net output #0: loss = 0.00267146 (* 1 = 0.00267146 loss)
I0814 18:52:24.014991 10451 sgd_solver.cpp:136] Iteration 44400, lr = 0.030625, m = 0.9
I0814 18:52:25.662688 10451 solver.cpp:312] Iteration 44500 (60.6893 iter/s, 1.64774s/100 iter), loss = 0.00531626
I0814 18:52:25.662713 10451 solver.cpp:334]     Train net output #0: loss = 0.00531503 (* 1 = 0.00531503 loss)
I0814 18:52:25.662719 10451 sgd_solver.cpp:136] Iteration 44500, lr = 0.0304688, m = 0.9
I0814 18:52:27.310803 10451 solver.cpp:312] Iteration 44600 (60.6772 iter/s, 1.64807s/100 iter), loss = 0.0118297
I0814 18:52:27.310858 10451 solver.cpp:334]     Train net output #0: loss = 0.0118285 (* 1 = 0.0118285 loss)
I0814 18:52:27.310873 10451 sgd_solver.cpp:136] Iteration 44600, lr = 0.0303125, m = 0.9
I0814 18:52:28.942615 10451 solver.cpp:312] Iteration 44700 (61.2834 iter/s, 1.63176s/100 iter), loss = 0.00679046
I0814 18:52:28.942641 10451 solver.cpp:334]     Train net output #0: loss = 0.00678924 (* 1 = 0.00678924 loss)
I0814 18:52:28.942646 10451 sgd_solver.cpp:136] Iteration 44700, lr = 0.0301562, m = 0.9
I0814 18:52:30.543246 10451 solver.cpp:312] Iteration 44800 (62.4772 iter/s, 1.60058s/100 iter), loss = 0.00202586
I0814 18:52:30.543293 10451 solver.cpp:334]     Train net output #0: loss = 0.00202464 (* 1 = 0.00202464 loss)
I0814 18:52:30.543306 10451 sgd_solver.cpp:136] Iteration 44800, lr = 0.03, m = 0.9
I0814 18:52:32.160542 10451 solver.cpp:312] Iteration 44900 (61.8336 iter/s, 1.61724s/100 iter), loss = 0.0124467
I0814 18:52:32.160567 10451 solver.cpp:334]     Train net output #0: loss = 0.0124455 (* 1 = 0.0124455 loss)
I0814 18:52:32.160573 10451 sgd_solver.cpp:136] Iteration 44900, lr = 0.0298437, m = 0.9
I0814 18:52:33.750669 10451 solver.cpp:509] Iteration 45000, Testing net (#0)
I0814 18:52:34.567961 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909119
I0814 18:52:34.567981 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:52:34.567986 10451 solver.cpp:594]     Test net output #2: loss = 0.365954 (* 1 = 0.365954 loss)
I0814 18:52:34.568004 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.817311s
I0814 18:52:34.583636 10451 solver.cpp:312] Iteration 45000 (41.2707 iter/s, 2.42302s/100 iter), loss = 0.000105146
I0814 18:52:34.583665 10451 solver.cpp:334]     Train net output #0: loss = 0.000103918 (* 1 = 0.000103918 loss)
I0814 18:52:34.583676 10451 sgd_solver.cpp:136] Iteration 45000, lr = 0.0296875, m = 0.9
I0814 18:52:36.220041 10451 solver.cpp:312] Iteration 45100 (61.1114 iter/s, 1.63636s/100 iter), loss = 0.00087603
I0814 18:52:36.220064 10451 solver.cpp:334]     Train net output #0: loss = 0.000874811 (* 1 = 0.000874811 loss)
I0814 18:52:36.220072 10451 sgd_solver.cpp:136] Iteration 45100, lr = 0.0295313, m = 0.9
I0814 18:52:37.868669 10451 solver.cpp:312] Iteration 45200 (60.6585 iter/s, 1.64857s/100 iter), loss = 0.000562392
I0814 18:52:37.868780 10451 solver.cpp:334]     Train net output #0: loss = 0.000561175 (* 1 = 0.000561175 loss)
I0814 18:52:37.868798 10451 sgd_solver.cpp:136] Iteration 45200, lr = 0.029375, m = 0.9
I0814 18:52:39.503159 10451 solver.cpp:312] Iteration 45300 (61.183 iter/s, 1.63444s/100 iter), loss = 0.000995024
I0814 18:52:39.503252 10451 solver.cpp:334]     Train net output #0: loss = 0.000993808 (* 1 = 0.000993808 loss)
I0814 18:52:39.503260 10451 sgd_solver.cpp:136] Iteration 45300, lr = 0.0292188, m = 0.9
I0814 18:52:41.121129 10451 solver.cpp:312] Iteration 45400 (61.8077 iter/s, 1.61792s/100 iter), loss = 0.013548
I0814 18:52:41.121152 10451 solver.cpp:334]     Train net output #0: loss = 0.0135468 (* 1 = 0.0135468 loss)
I0814 18:52:41.121158 10451 sgd_solver.cpp:136] Iteration 45400, lr = 0.0290625, m = 0.9
I0814 18:52:42.730648 10451 solver.cpp:312] Iteration 45500 (62.1323 iter/s, 1.60947s/100 iter), loss = 0.00162457
I0814 18:52:42.730672 10451 solver.cpp:334]     Train net output #0: loss = 0.00162335 (* 1 = 0.00162335 loss)
I0814 18:52:42.730679 10451 sgd_solver.cpp:136] Iteration 45500, lr = 0.0289063, m = 0.9
I0814 18:52:44.376734 10451 solver.cpp:312] Iteration 45600 (60.752 iter/s, 1.64604s/100 iter), loss = 0.00594894
I0814 18:52:44.376760 10451 solver.cpp:334]     Train net output #0: loss = 0.00594772 (* 1 = 0.00594772 loss)
I0814 18:52:44.376766 10451 sgd_solver.cpp:136] Iteration 45600, lr = 0.02875, m = 0.9
I0814 18:52:45.999230 10451 solver.cpp:312] Iteration 45700 (61.6353 iter/s, 1.62245s/100 iter), loss = 0.00241306
I0814 18:52:45.999258 10451 solver.cpp:334]     Train net output #0: loss = 0.00241183 (* 1 = 0.00241183 loss)
I0814 18:52:45.999264 10451 sgd_solver.cpp:136] Iteration 45700, lr = 0.0285937, m = 0.9
I0814 18:52:47.624460 10451 solver.cpp:312] Iteration 45800 (61.5316 iter/s, 1.62518s/100 iter), loss = 0.00164167
I0814 18:52:47.624485 10451 solver.cpp:334]     Train net output #0: loss = 0.00164045 (* 1 = 0.00164045 loss)
I0814 18:52:47.624490 10451 sgd_solver.cpp:136] Iteration 45800, lr = 0.0284375, m = 0.9
I0814 18:52:49.233278 10451 solver.cpp:312] Iteration 45900 (62.1594 iter/s, 1.60877s/100 iter), loss = 0.0202958
I0814 18:52:49.233304 10451 solver.cpp:334]     Train net output #0: loss = 0.0202946 (* 1 = 0.0202946 loss)
I0814 18:52:49.233309 10451 sgd_solver.cpp:136] Iteration 45900, lr = 0.0282812, m = 0.9
I0814 18:52:50.843459 10451 solver.cpp:509] Iteration 46000, Testing net (#0)
I0814 18:52:51.672091 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915295
I0814 18:52:51.672111 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 18:52:51.672116 10451 solver.cpp:594]     Test net output #2: loss = 0.311863 (* 1 = 0.311863 loss)
I0814 18:52:51.672138 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.828652s
I0814 18:52:51.687741 10451 solver.cpp:312] Iteration 46000 (40.7433 iter/s, 2.45439s/100 iter), loss = 0.00736032
I0814 18:52:51.687757 10451 solver.cpp:334]     Train net output #0: loss = 0.00735909 (* 1 = 0.00735909 loss)
I0814 18:52:51.687762 10451 sgd_solver.cpp:136] Iteration 46000, lr = 0.028125, m = 0.9
I0814 18:52:53.345029 10451 solver.cpp:312] Iteration 46100 (60.3415 iter/s, 1.65723s/100 iter), loss = 0.00109126
I0814 18:52:53.345098 10451 solver.cpp:334]     Train net output #0: loss = 0.00109003 (* 1 = 0.00109003 loss)
I0814 18:52:53.345118 10451 sgd_solver.cpp:136] Iteration 46100, lr = 0.0279688, m = 0.9
I0814 18:52:54.959347 10451 solver.cpp:312] Iteration 46200 (61.9474 iter/s, 1.61427s/100 iter), loss = 0.00144412
I0814 18:52:54.959409 10451 solver.cpp:334]     Train net output #0: loss = 0.00144289 (* 1 = 0.00144289 loss)
I0814 18:52:54.959429 10451 sgd_solver.cpp:136] Iteration 46200, lr = 0.0278125, m = 0.9
I0814 18:52:56.540159 10451 solver.cpp:312] Iteration 46300 (63.2607 iter/s, 1.58076s/100 iter), loss = 0.000453224
I0814 18:52:56.540226 10451 solver.cpp:334]     Train net output #0: loss = 0.000451996 (* 1 = 0.000451996 loss)
I0814 18:52:56.540246 10451 sgd_solver.cpp:136] Iteration 46300, lr = 0.0276563, m = 0.9
I0814 18:52:58.195893 10451 solver.cpp:312] Iteration 46400 (60.398 iter/s, 1.65568s/100 iter), loss = 0.00120206
I0814 18:52:58.195958 10451 solver.cpp:334]     Train net output #0: loss = 0.00120083 (* 1 = 0.00120083 loss)
I0814 18:52:58.195978 10451 sgd_solver.cpp:136] Iteration 46400, lr = 0.0275, m = 0.9
I0814 18:52:59.824237 10451 solver.cpp:312] Iteration 46500 (61.414 iter/s, 1.62829s/100 iter), loss = 0.0010247
I0814 18:52:59.824298 10451 solver.cpp:334]     Train net output #0: loss = 0.00102348 (* 1 = 0.00102348 loss)
I0814 18:52:59.824316 10451 sgd_solver.cpp:136] Iteration 46500, lr = 0.0273438, m = 0.9
I0814 18:53:01.425729 10451 solver.cpp:312] Iteration 46600 (62.4436 iter/s, 1.60144s/100 iter), loss = 0.000866297
I0814 18:53:01.425788 10451 solver.cpp:334]     Train net output #0: loss = 0.000865067 (* 1 = 0.000865067 loss)
I0814 18:53:01.425807 10451 sgd_solver.cpp:136] Iteration 46600, lr = 0.0271875, m = 0.9
I0814 18:53:03.009379 10451 solver.cpp:312] Iteration 46700 (63.1473 iter/s, 1.5836s/100 iter), loss = 0.00257616
I0814 18:53:03.009523 10451 solver.cpp:334]     Train net output #0: loss = 0.00257493 (* 1 = 0.00257493 loss)
I0814 18:53:03.009542 10451 sgd_solver.cpp:136] Iteration 46700, lr = 0.0270312, m = 0.9
I0814 18:53:04.633821 10451 solver.cpp:312] Iteration 46800 (61.5615 iter/s, 1.62439s/100 iter), loss = 0.000689873
I0814 18:53:04.633868 10451 solver.cpp:334]     Train net output #0: loss = 0.000688642 (* 1 = 0.000688642 loss)
I0814 18:53:04.633880 10451 sgd_solver.cpp:136] Iteration 46800, lr = 0.026875, m = 0.9
I0814 18:53:06.256296 10451 solver.cpp:312] Iteration 46900 (61.636 iter/s, 1.62243s/100 iter), loss = 0.000566735
I0814 18:53:06.256362 10451 solver.cpp:334]     Train net output #0: loss = 0.000565504 (* 1 = 0.000565504 loss)
I0814 18:53:06.256381 10451 sgd_solver.cpp:136] Iteration 46900, lr = 0.0267187, m = 0.9
I0814 18:53:07.883736 10451 solver.cpp:509] Iteration 47000, Testing net (#0)
I0814 18:53:08.708876 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91706
I0814 18:53:08.708895 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 18:53:08.708900 10451 solver.cpp:594]     Test net output #2: loss = 0.318635 (* 1 = 0.318635 loss)
I0814 18:53:08.708915 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.825158s
I0814 18:53:08.724726 10451 solver.cpp:312] Iteration 47000 (40.5127 iter/s, 2.46836s/100 iter), loss = 0.00420694
I0814 18:53:08.724745 10451 solver.cpp:334]     Train net output #0: loss = 0.00420571 (* 1 = 0.00420571 loss)
I0814 18:53:08.724750 10451 sgd_solver.cpp:136] Iteration 47000, lr = 0.0265625, m = 0.9
I0814 18:53:10.326845 10451 solver.cpp:312] Iteration 47100 (62.4193 iter/s, 1.60207s/100 iter), loss = 0.0022052
I0814 18:53:10.326917 10451 solver.cpp:334]     Train net output #0: loss = 0.00220397 (* 1 = 0.00220397 loss)
I0814 18:53:10.326938 10451 sgd_solver.cpp:136] Iteration 47100, lr = 0.0264063, m = 0.9
I0814 18:53:11.907449 10451 solver.cpp:312] Iteration 47200 (63.2689 iter/s, 1.58055s/100 iter), loss = 0.000568174
I0814 18:53:11.907502 10451 solver.cpp:334]     Train net output #0: loss = 0.000566938 (* 1 = 0.000566938 loss)
I0814 18:53:11.907515 10451 sgd_solver.cpp:136] Iteration 47200, lr = 0.02625, m = 0.9
I0814 18:53:13.527763 10451 solver.cpp:312] Iteration 47300 (61.7183 iter/s, 1.62026s/100 iter), loss = 0.00090965
I0814 18:53:13.527812 10451 solver.cpp:334]     Train net output #0: loss = 0.000908414 (* 1 = 0.000908414 loss)
I0814 18:53:13.527827 10451 sgd_solver.cpp:136] Iteration 47300, lr = 0.0260938, m = 0.9
I0814 18:53:15.164391 10451 solver.cpp:312] Iteration 47400 (61.1031 iter/s, 1.63658s/100 iter), loss = 0.000767824
I0814 18:53:15.164450 10451 solver.cpp:334]     Train net output #0: loss = 0.000766588 (* 1 = 0.000766588 loss)
I0814 18:53:15.164469 10451 sgd_solver.cpp:136] Iteration 47400, lr = 0.0259375, m = 0.9
I0814 18:53:16.777034 10451 solver.cpp:312] Iteration 47500 (62.012 iter/s, 1.61259s/100 iter), loss = 0.00259016
I0814 18:53:16.777107 10451 solver.cpp:334]     Train net output #0: loss = 0.00258892 (* 1 = 0.00258892 loss)
I0814 18:53:16.777129 10451 sgd_solver.cpp:136] Iteration 47500, lr = 0.0257812, m = 0.9
I0814 18:53:18.404006 10451 solver.cpp:312] Iteration 47600 (61.4657 iter/s, 1.62692s/100 iter), loss = 0.000281997
I0814 18:53:18.404050 10451 solver.cpp:334]     Train net output #0: loss = 0.000280759 (* 1 = 0.000280759 loss)
I0814 18:53:18.404062 10451 sgd_solver.cpp:136] Iteration 47600, lr = 0.025625, m = 0.9
I0814 18:53:20.032513 10451 solver.cpp:312] Iteration 47700 (61.4079 iter/s, 1.62845s/100 iter), loss = 0.00115148
I0814 18:53:20.032537 10451 solver.cpp:334]     Train net output #0: loss = 0.00115025 (* 1 = 0.00115025 loss)
I0814 18:53:20.032543 10451 sgd_solver.cpp:136] Iteration 47700, lr = 0.0254687, m = 0.9
I0814 18:53:21.641963 10451 solver.cpp:312] Iteration 47800 (62.1349 iter/s, 1.6094s/100 iter), loss = 0.000972823
I0814 18:53:21.642030 10451 solver.cpp:334]     Train net output #0: loss = 0.000971587 (* 1 = 0.000971587 loss)
I0814 18:53:21.642050 10451 sgd_solver.cpp:136] Iteration 47800, lr = 0.0253125, m = 0.9
I0814 18:53:23.290139 10451 solver.cpp:312] Iteration 47900 (60.675 iter/s, 1.64812s/100 iter), loss = 0.000272515
I0814 18:53:23.290164 10451 solver.cpp:334]     Train net output #0: loss = 0.00027128 (* 1 = 0.00027128 loss)
I0814 18:53:23.290170 10451 sgd_solver.cpp:136] Iteration 47900, lr = 0.0251562, m = 0.9
I0814 18:53:24.851459 10451 solver.cpp:509] Iteration 48000, Testing net (#0)
I0814 18:53:24.882599 10438 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 18:53:25.688210 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919118
I0814 18:53:25.688230 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:53:25.688235 10451 solver.cpp:594]     Test net output #2: loss = 0.298945 (* 1 = 0.298945 loss)
I0814 18:53:25.688251 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.83677s
I0814 18:53:25.709908 10451 solver.cpp:312] Iteration 48000 (41.3274 iter/s, 2.4197s/100 iter), loss = 0.000495195
I0814 18:53:25.709936 10451 solver.cpp:334]     Train net output #0: loss = 0.000493957 (* 1 = 0.000493957 loss)
I0814 18:53:25.709941 10451 sgd_solver.cpp:136] Iteration 48000, lr = 0.025, m = 0.9
I0814 18:53:27.340843 10451 solver.cpp:312] Iteration 48100 (61.3164 iter/s, 1.63088s/100 iter), loss = 0.00406636
I0814 18:53:27.340867 10451 solver.cpp:334]     Train net output #0: loss = 0.00406512 (* 1 = 0.00406512 loss)
I0814 18:53:27.340873 10451 sgd_solver.cpp:136] Iteration 48100, lr = 0.0248438, m = 0.9
I0814 18:53:28.928972 10451 solver.cpp:312] Iteration 48200 (62.9691 iter/s, 1.58808s/100 iter), loss = 0.00129514
I0814 18:53:28.928997 10451 solver.cpp:334]     Train net output #0: loss = 0.00129391 (* 1 = 0.00129391 loss)
I0814 18:53:28.929003 10451 sgd_solver.cpp:136] Iteration 48200, lr = 0.0246875, m = 0.9
I0814 18:53:30.544404 10451 solver.cpp:312] Iteration 48300 (61.9049 iter/s, 1.61538s/100 iter), loss = 0.00145908
I0814 18:53:30.544466 10451 solver.cpp:334]     Train net output #0: loss = 0.00145784 (* 1 = 0.00145784 loss)
I0814 18:53:30.544493 10451 sgd_solver.cpp:136] Iteration 48300, lr = 0.0245313, m = 0.9
I0814 18:53:32.158380 10451 solver.cpp:312] Iteration 48400 (61.9607 iter/s, 1.61393s/100 iter), loss = 0.00140065
I0814 18:53:32.158401 10451 solver.cpp:334]     Train net output #0: loss = 0.00139941 (* 1 = 0.00139941 loss)
I0814 18:53:32.158407 10451 sgd_solver.cpp:136] Iteration 48400, lr = 0.024375, m = 0.9
I0814 18:53:33.756410 10451 solver.cpp:312] Iteration 48500 (62.5789 iter/s, 1.59798s/100 iter), loss = 0.000401453
I0814 18:53:33.756435 10451 solver.cpp:334]     Train net output #0: loss = 0.000400218 (* 1 = 0.000400218 loss)
I0814 18:53:33.756441 10451 sgd_solver.cpp:136] Iteration 48500, lr = 0.0242188, m = 0.9
I0814 18:53:35.365556 10451 solver.cpp:312] Iteration 48600 (62.1467 iter/s, 1.6091s/100 iter), loss = 0.00178565
I0814 18:53:35.365603 10451 solver.cpp:334]     Train net output #0: loss = 0.00178441 (* 1 = 0.00178441 loss)
I0814 18:53:35.365615 10451 sgd_solver.cpp:136] Iteration 48600, lr = 0.0240625, m = 0.9
I0814 18:53:36.968639 10451 solver.cpp:312] Iteration 48700 (62.3817 iter/s, 1.60303s/100 iter), loss = 0.000970848
I0814 18:53:36.968665 10451 solver.cpp:334]     Train net output #0: loss = 0.000969612 (* 1 = 0.000969612 loss)
I0814 18:53:36.968673 10451 sgd_solver.cpp:136] Iteration 48700, lr = 0.0239062, m = 0.9
I0814 18:53:38.575392 10451 solver.cpp:312] Iteration 48800 (62.2393 iter/s, 1.6067s/100 iter), loss = 0.000798375
I0814 18:53:38.575515 10451 solver.cpp:334]     Train net output #0: loss = 0.000797139 (* 1 = 0.000797139 loss)
I0814 18:53:38.575534 10451 sgd_solver.cpp:136] Iteration 48800, lr = 0.02375, m = 0.9
I0814 18:53:40.149404 10451 solver.cpp:312] Iteration 48900 (63.534 iter/s, 1.57396s/100 iter), loss = 0.00106245
I0814 18:53:40.149453 10451 solver.cpp:334]     Train net output #0: loss = 0.00106121 (* 1 = 0.00106121 loss)
I0814 18:53:40.149466 10451 sgd_solver.cpp:136] Iteration 48900, lr = 0.0235937, m = 0.9
I0814 18:53:41.719640 10451 solver.cpp:509] Iteration 49000, Testing net (#0)
I0814 18:53:42.541822 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919707
I0814 18:53:42.541842 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 18:53:42.541847 10451 solver.cpp:594]     Test net output #2: loss = 0.297661 (* 1 = 0.297661 loss)
I0814 18:53:42.541862 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.822199s
I0814 18:53:42.557442 10451 solver.cpp:312] Iteration 49000 (41.5287 iter/s, 2.40797s/100 iter), loss = 0.00294959
I0814 18:53:42.557461 10451 solver.cpp:334]     Train net output #0: loss = 0.00294835 (* 1 = 0.00294835 loss)
I0814 18:53:42.557466 10451 sgd_solver.cpp:136] Iteration 49000, lr = 0.0234375, m = 0.9
I0814 18:53:44.120275 10451 solver.cpp:312] Iteration 49100 (63.9884 iter/s, 1.56278s/100 iter), loss = 0.00307807
I0814 18:53:44.120334 10451 solver.cpp:334]     Train net output #0: loss = 0.00307684 (* 1 = 0.00307684 loss)
I0814 18:53:44.120352 10451 sgd_solver.cpp:136] Iteration 49100, lr = 0.0232813, m = 0.9
I0814 18:53:45.758668 10451 solver.cpp:312] Iteration 49200 (61.0373 iter/s, 1.63834s/100 iter), loss = 0.00102918
I0814 18:53:45.758694 10451 solver.cpp:334]     Train net output #0: loss = 0.00102795 (* 1 = 0.00102795 loss)
I0814 18:53:45.758700 10451 sgd_solver.cpp:136] Iteration 49200, lr = 0.023125, m = 0.9
I0814 18:53:47.380723 10451 solver.cpp:312] Iteration 49300 (61.6522 iter/s, 1.622s/100 iter), loss = 0.000471866
I0814 18:53:47.380748 10451 solver.cpp:334]     Train net output #0: loss = 0.000470632 (* 1 = 0.000470632 loss)
I0814 18:53:47.380753 10451 sgd_solver.cpp:136] Iteration 49300, lr = 0.0229688, m = 0.9
I0814 18:53:49.016607 10451 solver.cpp:312] Iteration 49400 (61.1309 iter/s, 1.63583s/100 iter), loss = 0.000497134
I0814 18:53:49.016680 10451 solver.cpp:334]     Train net output #0: loss = 0.0004959 (* 1 = 0.0004959 loss)
I0814 18:53:49.016700 10451 sgd_solver.cpp:136] Iteration 49400, lr = 0.0228125, m = 0.9
I0814 18:53:50.624868 10451 solver.cpp:312] Iteration 49500 (62.1808 iter/s, 1.60821s/100 iter), loss = 0.00160473
I0814 18:53:50.625030 10451 solver.cpp:334]     Train net output #0: loss = 0.0016035 (* 1 = 0.0016035 loss)
I0814 18:53:50.625051 10451 sgd_solver.cpp:136] Iteration 49500, lr = 0.0226563, m = 0.9
I0814 18:53:52.252451 10451 solver.cpp:312] Iteration 49600 (61.4427 iter/s, 1.62753s/100 iter), loss = 0.000892904
I0814 18:53:52.252514 10451 solver.cpp:334]     Train net output #0: loss = 0.00089167 (* 1 = 0.00089167 loss)
I0814 18:53:52.252532 10451 sgd_solver.cpp:136] Iteration 49600, lr = 0.0225, m = 0.9
I0814 18:53:53.842883 10451 solver.cpp:312] Iteration 49700 (62.878 iter/s, 1.59038s/100 iter), loss = 0.000289856
I0814 18:53:53.842907 10451 solver.cpp:334]     Train net output #0: loss = 0.000288621 (* 1 = 0.000288621 loss)
I0814 18:53:53.842913 10451 sgd_solver.cpp:136] Iteration 49700, lr = 0.0223437, m = 0.9
I0814 18:53:55.486316 10451 solver.cpp:312] Iteration 49800 (60.85 iter/s, 1.64338s/100 iter), loss = 0.000511818
I0814 18:53:55.486366 10451 solver.cpp:334]     Train net output #0: loss = 0.000510583 (* 1 = 0.000510583 loss)
I0814 18:53:55.486378 10451 sgd_solver.cpp:136] Iteration 49800, lr = 0.0221875, m = 0.9
I0814 18:53:57.075842 10451 solver.cpp:312] Iteration 49900 (62.9139 iter/s, 1.58947s/100 iter), loss = 0.000839467
I0814 18:53:57.075866 10451 solver.cpp:334]     Train net output #0: loss = 0.000838232 (* 1 = 0.000838232 loss)
I0814 18:53:57.075872 10451 sgd_solver.cpp:136] Iteration 49900, lr = 0.0220312, m = 0.9
I0814 18:53:58.675328 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_50000.caffemodel
I0814 18:53:58.683293 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_50000.solverstate
I0814 18:53:58.686866 10451 solver.cpp:509] Iteration 50000, Testing net (#0)
I0814 18:53:59.486847 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917354
I0814 18:53:59.486876 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:53:59.486884 10451 solver.cpp:594]     Test net output #2: loss = 0.310996 (* 1 = 0.310996 loss)
I0814 18:53:59.486907 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.800017s
I0814 18:53:59.509707 10451 solver.cpp:312] Iteration 50000 (41.0881 iter/s, 2.43379s/100 iter), loss = 0.00032694
I0814 18:53:59.509734 10451 solver.cpp:334]     Train net output #0: loss = 0.000325705 (* 1 = 0.000325705 loss)
I0814 18:53:59.509739 10451 sgd_solver.cpp:136] Iteration 50000, lr = 0.021875, m = 0.9
I0814 18:54:01.136447 10451 solver.cpp:312] Iteration 50100 (61.4745 iter/s, 1.62669s/100 iter), loss = 0.000703579
I0814 18:54:01.136473 10451 solver.cpp:334]     Train net output #0: loss = 0.000702344 (* 1 = 0.000702344 loss)
I0814 18:54:01.136481 10451 sgd_solver.cpp:136] Iteration 50100, lr = 0.0217188, m = 0.9
I0814 18:54:02.759368 10451 solver.cpp:312] Iteration 50200 (61.6191 iter/s, 1.62287s/100 iter), loss = 0.00170066
I0814 18:54:02.759393 10451 solver.cpp:334]     Train net output #0: loss = 0.00169943 (* 1 = 0.00169943 loss)
I0814 18:54:02.759399 10451 sgd_solver.cpp:136] Iteration 50200, lr = 0.0215625, m = 0.9
I0814 18:54:04.390023 10451 solver.cpp:312] Iteration 50300 (61.327 iter/s, 1.6306s/100 iter), loss = 0.000491016
I0814 18:54:04.390085 10451 solver.cpp:334]     Train net output #0: loss = 0.000489781 (* 1 = 0.000489781 loss)
I0814 18:54:04.390103 10451 sgd_solver.cpp:136] Iteration 50300, lr = 0.0214063, m = 0.9
I0814 18:54:06.020542 10451 solver.cpp:312] Iteration 50400 (61.332 iter/s, 1.63047s/100 iter), loss = 0.000467951
I0814 18:54:06.020568 10451 solver.cpp:334]     Train net output #0: loss = 0.000466717 (* 1 = 0.000466717 loss)
I0814 18:54:06.020573 10451 sgd_solver.cpp:136] Iteration 50400, lr = 0.02125, m = 0.9
I0814 18:54:07.625085 10451 solver.cpp:312] Iteration 50500 (62.3248 iter/s, 1.6045s/100 iter), loss = 0.00106298
I0814 18:54:07.625232 10451 solver.cpp:334]     Train net output #0: loss = 0.00106174 (* 1 = 0.00106174 loss)
I0814 18:54:07.625260 10451 sgd_solver.cpp:136] Iteration 50500, lr = 0.0210938, m = 0.9
I0814 18:54:09.233101 10451 solver.cpp:312] Iteration 50600 (62.1903 iter/s, 1.60797s/100 iter), loss = 0.00110135
I0814 18:54:09.233206 10451 solver.cpp:334]     Train net output #0: loss = 0.00110012 (* 1 = 0.00110012 loss)
I0814 18:54:09.233219 10451 sgd_solver.cpp:136] Iteration 50600, lr = 0.0209375, m = 0.9
I0814 18:54:10.852448 10451 solver.cpp:312] Iteration 50700 (61.7552 iter/s, 1.6193s/100 iter), loss = 0.00217056
I0814 18:54:10.852473 10451 solver.cpp:334]     Train net output #0: loss = 0.00216933 (* 1 = 0.00216933 loss)
I0814 18:54:10.852479 10451 sgd_solver.cpp:136] Iteration 50700, lr = 0.0207812, m = 0.9
I0814 18:54:12.447604 10451 solver.cpp:312] Iteration 50800 (62.6916 iter/s, 1.59511s/100 iter), loss = 0.000804358
I0814 18:54:12.447634 10451 solver.cpp:334]     Train net output #0: loss = 0.000803124 (* 1 = 0.000803124 loss)
I0814 18:54:12.447640 10451 sgd_solver.cpp:136] Iteration 50800, lr = 0.020625, m = 0.9
I0814 18:54:14.082401 10451 solver.cpp:312] Iteration 50900 (61.1715 iter/s, 1.63475s/100 iter), loss = 0.000998344
I0814 18:54:14.082546 10451 solver.cpp:334]     Train net output #0: loss = 0.000997111 (* 1 = 0.000997111 loss)
I0814 18:54:14.082571 10451 sgd_solver.cpp:136] Iteration 50900, lr = 0.0204687, m = 0.9
I0814 18:54:15.717260 10451 solver.cpp:509] Iteration 51000, Testing net (#0)
I0814 18:54:16.546891 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912354
I0814 18:54:16.546911 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:54:16.546916 10451 solver.cpp:594]     Test net output #2: loss = 0.321833 (* 1 = 0.321833 loss)
I0814 18:54:16.546929 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.829648s
I0814 18:54:16.567836 10451 solver.cpp:312] Iteration 51000 (40.2356 iter/s, 2.48536s/100 iter), loss = 0.00075457
I0814 18:54:16.567853 10451 solver.cpp:334]     Train net output #0: loss = 0.000753338 (* 1 = 0.000753338 loss)
I0814 18:54:16.567857 10451 sgd_solver.cpp:136] Iteration 51000, lr = 0.0203125, m = 0.9
I0814 18:54:18.231674 10451 solver.cpp:312] Iteration 51100 (60.1038 iter/s, 1.66379s/100 iter), loss = 0.000562066
I0814 18:54:18.231698 10451 solver.cpp:334]     Train net output #0: loss = 0.000560835 (* 1 = 0.000560835 loss)
I0814 18:54:18.231703 10451 sgd_solver.cpp:136] Iteration 51100, lr = 0.0201563, m = 0.9
I0814 18:54:19.863257 10451 solver.cpp:312] Iteration 51200 (61.292 iter/s, 1.63153s/100 iter), loss = 0.000749945
I0814 18:54:19.863304 10451 solver.cpp:334]     Train net output #0: loss = 0.000748714 (* 1 = 0.000748714 loss)
I0814 18:54:19.863317 10451 sgd_solver.cpp:136] Iteration 51200, lr = 0.02, m = 0.9
I0814 18:54:21.466930 10451 solver.cpp:312] Iteration 51300 (62.3588 iter/s, 1.60362s/100 iter), loss = 0.00117396
I0814 18:54:21.466956 10451 solver.cpp:334]     Train net output #0: loss = 0.00117273 (* 1 = 0.00117273 loss)
I0814 18:54:21.466962 10451 sgd_solver.cpp:136] Iteration 51300, lr = 0.0198438, m = 0.9
I0814 18:54:23.077049 10451 solver.cpp:312] Iteration 51400 (62.1092 iter/s, 1.61007s/100 iter), loss = 0.00267232
I0814 18:54:23.077096 10451 solver.cpp:334]     Train net output #0: loss = 0.00267109 (* 1 = 0.00267109 loss)
I0814 18:54:23.077111 10451 sgd_solver.cpp:136] Iteration 51400, lr = 0.0196875, m = 0.9
I0814 18:54:24.701618 10451 solver.cpp:312] Iteration 51500 (61.5566 iter/s, 1.62452s/100 iter), loss = 0.000394848
I0814 18:54:24.701644 10451 solver.cpp:334]     Train net output #0: loss = 0.000393617 (* 1 = 0.000393617 loss)
I0814 18:54:24.701650 10451 sgd_solver.cpp:136] Iteration 51500, lr = 0.0195312, m = 0.9
I0814 18:54:26.305460 10451 solver.cpp:312] Iteration 51600 (62.3521 iter/s, 1.60379s/100 iter), loss = 0.00115834
I0814 18:54:26.305483 10451 solver.cpp:334]     Train net output #0: loss = 0.00115711 (* 1 = 0.00115711 loss)
I0814 18:54:26.305487 10451 sgd_solver.cpp:136] Iteration 51600, lr = 0.019375, m = 0.9
I0814 18:54:27.940460 10451 solver.cpp:312] Iteration 51700 (61.1639 iter/s, 1.63495s/100 iter), loss = 0.000353252
I0814 18:54:27.940488 10451 solver.cpp:334]     Train net output #0: loss = 0.000352022 (* 1 = 0.000352022 loss)
I0814 18:54:27.940495 10451 sgd_solver.cpp:136] Iteration 51700, lr = 0.0192187, m = 0.9
I0814 18:54:29.568994 10451 solver.cpp:312] Iteration 51800 (61.4069 iter/s, 1.62848s/100 iter), loss = 0.00143266
I0814 18:54:29.569056 10451 solver.cpp:334]     Train net output #0: loss = 0.00143143 (* 1 = 0.00143143 loss)
I0814 18:54:29.569075 10451 sgd_solver.cpp:136] Iteration 51800, lr = 0.0190625, m = 0.9
I0814 18:54:31.187793 10451 solver.cpp:312] Iteration 51900 (61.7761 iter/s, 1.61875s/100 iter), loss = 0.000557461
I0814 18:54:31.187816 10451 solver.cpp:334]     Train net output #0: loss = 0.00055623 (* 1 = 0.00055623 loss)
I0814 18:54:31.187824 10451 sgd_solver.cpp:136] Iteration 51900, lr = 0.0189062, m = 0.9
I0814 18:54:32.806725 10451 solver.cpp:509] Iteration 52000, Testing net (#0)
I0814 18:54:33.531756 10438 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 18:54:33.622151 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911177
I0814 18:54:33.622174 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 18:54:33.622181 10451 solver.cpp:594]     Test net output #2: loss = 0.317098 (* 1 = 0.317098 loss)
I0814 18:54:33.622198 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.81545s
I0814 18:54:33.663556 10451 solver.cpp:312] Iteration 52000 (40.3927 iter/s, 2.47569s/100 iter), loss = 0.000271434
I0814 18:54:33.663605 10451 solver.cpp:334]     Train net output #0: loss = 0.000270203 (* 1 = 0.000270203 loss)
I0814 18:54:33.663616 10451 sgd_solver.cpp:136] Iteration 52000, lr = 0.01875, m = 0.9
I0814 18:54:35.259913 10451 solver.cpp:312] Iteration 52100 (62.6446 iter/s, 1.59631s/100 iter), loss = 0.00146658
I0814 18:54:35.259938 10451 solver.cpp:334]     Train net output #0: loss = 0.00146535 (* 1 = 0.00146535 loss)
I0814 18:54:35.259945 10451 sgd_solver.cpp:136] Iteration 52100, lr = 0.0185938, m = 0.9
I0814 18:54:36.881520 10451 solver.cpp:312] Iteration 52200 (61.6692 iter/s, 1.62156s/100 iter), loss = 0.000917433
I0814 18:54:36.881567 10451 solver.cpp:334]     Train net output #0: loss = 0.000916202 (* 1 = 0.000916202 loss)
I0814 18:54:36.881578 10451 sgd_solver.cpp:136] Iteration 52200, lr = 0.0184375, m = 0.9
I0814 18:54:38.533223 10451 solver.cpp:312] Iteration 52300 (60.5453 iter/s, 1.65165s/100 iter), loss = 0.00238159
I0814 18:54:38.533291 10451 solver.cpp:334]     Train net output #0: loss = 0.00238036 (* 1 = 0.00238036 loss)
I0814 18:54:38.533311 10451 sgd_solver.cpp:136] Iteration 52300, lr = 0.0182813, m = 0.9
I0814 18:54:40.144783 10451 solver.cpp:312] Iteration 52400 (62.0536 iter/s, 1.61151s/100 iter), loss = 0.00113295
I0814 18:54:40.144865 10451 solver.cpp:334]     Train net output #0: loss = 0.00113172 (* 1 = 0.00113172 loss)
I0814 18:54:40.144871 10451 sgd_solver.cpp:136] Iteration 52400, lr = 0.018125, m = 0.9
I0814 18:54:41.766054 10451 solver.cpp:312] Iteration 52500 (61.6819 iter/s, 1.62122s/100 iter), loss = 0.000821069
I0814 18:54:41.766104 10451 solver.cpp:334]     Train net output #0: loss = 0.000819838 (* 1 = 0.000819838 loss)
I0814 18:54:41.766119 10451 sgd_solver.cpp:136] Iteration 52500, lr = 0.0179687, m = 0.9
I0814 18:54:43.402146 10451 solver.cpp:312] Iteration 52600 (61.1231 iter/s, 1.63604s/100 iter), loss = 0.000957264
I0814 18:54:43.402196 10451 solver.cpp:334]     Train net output #0: loss = 0.000956032 (* 1 = 0.000956032 loss)
I0814 18:54:43.402209 10451 sgd_solver.cpp:136] Iteration 52600, lr = 0.0178125, m = 0.9
I0814 18:54:45.011947 10451 solver.cpp:312] Iteration 52700 (62.1213 iter/s, 1.60975s/100 iter), loss = 0.000501108
I0814 18:54:45.011992 10451 solver.cpp:334]     Train net output #0: loss = 0.000499876 (* 1 = 0.000499876 loss)
I0814 18:54:45.012006 10451 sgd_solver.cpp:136] Iteration 52700, lr = 0.0176562, m = 0.9
I0814 18:54:46.603376 10451 solver.cpp:312] Iteration 52800 (62.8386 iter/s, 1.59138s/100 iter), loss = 0.000739691
I0814 18:54:46.603433 10451 solver.cpp:334]     Train net output #0: loss = 0.000738459 (* 1 = 0.000738459 loss)
I0814 18:54:46.603447 10451 sgd_solver.cpp:136] Iteration 52800, lr = 0.0175, m = 0.9
I0814 18:54:48.205332 10451 solver.cpp:312] Iteration 52900 (62.4258 iter/s, 1.6019s/100 iter), loss = 0.000399633
I0814 18:54:48.205366 10451 solver.cpp:334]     Train net output #0: loss = 0.000398402 (* 1 = 0.000398402 loss)
I0814 18:54:48.205374 10451 sgd_solver.cpp:136] Iteration 52900, lr = 0.0173437, m = 0.9
I0814 18:54:49.824395 10451 solver.cpp:509] Iteration 53000, Testing net (#0)
I0814 18:54:50.642603 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915295
I0814 18:54:50.642626 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 18:54:50.642632 10451 solver.cpp:594]     Test net output #2: loss = 0.312248 (* 1 = 0.312248 loss)
I0814 18:54:50.642671 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.818254s
I0814 18:54:50.658247 10451 solver.cpp:312] Iteration 53000 (40.7689 iter/s, 2.45285s/100 iter), loss = 0.000322833
I0814 18:54:50.658263 10451 solver.cpp:334]     Train net output #0: loss = 0.000321601 (* 1 = 0.000321601 loss)
I0814 18:54:50.658268 10451 sgd_solver.cpp:136] Iteration 53000, lr = 0.0171875, m = 0.9
I0814 18:54:52.281255 10451 solver.cpp:312] Iteration 53100 (61.6159 iter/s, 1.62296s/100 iter), loss = 0.000510215
I0814 18:54:52.281299 10451 solver.cpp:334]     Train net output #0: loss = 0.000508984 (* 1 = 0.000508984 loss)
I0814 18:54:52.281313 10451 sgd_solver.cpp:136] Iteration 53100, lr = 0.0170313, m = 0.9
I0814 18:54:53.911402 10451 solver.cpp:312] Iteration 53200 (61.346 iter/s, 1.6301s/100 iter), loss = 0.000336807
I0814 18:54:53.911427 10451 solver.cpp:334]     Train net output #0: loss = 0.000335576 (* 1 = 0.000335576 loss)
I0814 18:54:53.911433 10451 sgd_solver.cpp:136] Iteration 53200, lr = 0.016875, m = 0.9
I0814 18:54:55.533953 10451 solver.cpp:312] Iteration 53300 (61.6332 iter/s, 1.6225s/100 iter), loss = 0.00118072
I0814 18:54:55.533977 10451 solver.cpp:334]     Train net output #0: loss = 0.00117949 (* 1 = 0.00117949 loss)
I0814 18:54:55.533983 10451 sgd_solver.cpp:136] Iteration 53300, lr = 0.0167188, m = 0.9
I0814 18:54:57.160763 10451 solver.cpp:312] Iteration 53400 (61.4718 iter/s, 1.62676s/100 iter), loss = 0.000591931
I0814 18:54:57.160789 10451 solver.cpp:334]     Train net output #0: loss = 0.000590699 (* 1 = 0.000590699 loss)
I0814 18:54:57.160794 10451 sgd_solver.cpp:136] Iteration 53400, lr = 0.0165625, m = 0.9
I0814 18:54:58.783751 10451 solver.cpp:312] Iteration 53500 (61.6167 iter/s, 1.62294s/100 iter), loss = 0.00239152
I0814 18:54:58.783776 10451 solver.cpp:334]     Train net output #0: loss = 0.00239029 (* 1 = 0.00239029 loss)
I0814 18:54:58.783782 10451 sgd_solver.cpp:136] Iteration 53500, lr = 0.0164063, m = 0.9
I0814 18:55:00.420122 10451 solver.cpp:312] Iteration 53600 (61.1126 iter/s, 1.63632s/100 iter), loss = 0.00101008
I0814 18:55:00.420171 10451 solver.cpp:334]     Train net output #0: loss = 0.00100885 (* 1 = 0.00100885 loss)
I0814 18:55:00.420186 10451 sgd_solver.cpp:136] Iteration 53600, lr = 0.01625, m = 0.9
I0814 18:55:02.008635 10451 solver.cpp:312] Iteration 53700 (62.954 iter/s, 1.58846s/100 iter), loss = 0.000510479
I0814 18:55:02.008689 10451 solver.cpp:334]     Train net output #0: loss = 0.000509247 (* 1 = 0.000509247 loss)
I0814 18:55:02.008708 10451 sgd_solver.cpp:136] Iteration 53700, lr = 0.0160937, m = 0.9
I0814 18:55:03.611337 10451 solver.cpp:312] Iteration 53800 (62.3965 iter/s, 1.60265s/100 iter), loss = 0.000755453
I0814 18:55:03.611383 10451 solver.cpp:334]     Train net output #0: loss = 0.000754221 (* 1 = 0.000754221 loss)
I0814 18:55:03.611397 10451 sgd_solver.cpp:136] Iteration 53800, lr = 0.0159375, m = 0.9
I0814 18:55:05.244680 10451 solver.cpp:312] Iteration 53900 (61.2259 iter/s, 1.6333s/100 iter), loss = 0.00269955
I0814 18:55:05.244735 10451 solver.cpp:334]     Train net output #0: loss = 0.00269832 (* 1 = 0.00269832 loss)
I0814 18:55:05.244750 10451 sgd_solver.cpp:136] Iteration 53900, lr = 0.0157812, m = 0.9
I0814 18:55:06.820300 10451 solver.cpp:509] Iteration 54000, Testing net (#0)
I0814 18:55:07.632517 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917354
I0814 18:55:07.632535 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 18:55:07.632542 10451 solver.cpp:594]     Test net output #2: loss = 0.286956 (* 1 = 0.286956 loss)
I0814 18:55:07.632560 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.812427s
I0814 18:55:07.648859 10451 solver.cpp:312] Iteration 54000 (41.5954 iter/s, 2.40411s/100 iter), loss = 0.000950624
I0814 18:55:07.648890 10451 solver.cpp:334]     Train net output #0: loss = 0.000949391 (* 1 = 0.000949391 loss)
I0814 18:55:07.648900 10451 sgd_solver.cpp:136] Iteration 54000, lr = 0.015625, m = 0.9
I0814 18:55:09.285346 10451 solver.cpp:312] Iteration 54100 (61.1085 iter/s, 1.63643s/100 iter), loss = 0.000910629
I0814 18:55:09.285464 10451 solver.cpp:334]     Train net output #0: loss = 0.000909396 (* 1 = 0.000909396 loss)
I0814 18:55:09.285480 10451 sgd_solver.cpp:136] Iteration 54100, lr = 0.0154688, m = 0.9
I0814 18:55:10.884905 10451 solver.cpp:312] Iteration 54200 (62.5191 iter/s, 1.59951s/100 iter), loss = 0.00130849
I0814 18:55:10.885026 10451 solver.cpp:334]     Train net output #0: loss = 0.00130726 (* 1 = 0.00130726 loss)
I0814 18:55:10.885044 10451 sgd_solver.cpp:136] Iteration 54200, lr = 0.0153125, m = 0.9
I0814 18:55:12.490061 10451 solver.cpp:312] Iteration 54300 (62.3011 iter/s, 1.60511s/100 iter), loss = 0.00361174
I0814 18:55:12.490084 10451 solver.cpp:334]     Train net output #0: loss = 0.00361051 (* 1 = 0.00361051 loss)
I0814 18:55:12.490090 10451 sgd_solver.cpp:136] Iteration 54300, lr = 0.0151563, m = 0.9
I0814 18:55:14.134016 10451 solver.cpp:312] Iteration 54400 (60.8308 iter/s, 1.64391s/100 iter), loss = 0.000510048
I0814 18:55:14.134042 10451 solver.cpp:334]     Train net output #0: loss = 0.000508817 (* 1 = 0.000508817 loss)
I0814 18:55:14.134047 10451 sgd_solver.cpp:136] Iteration 54400, lr = 0.015, m = 0.9
I0814 18:55:15.799777 10451 solver.cpp:312] Iteration 54500 (60.0345 iter/s, 1.66571s/100 iter), loss = 0.00109913
I0814 18:55:15.799803 10451 solver.cpp:334]     Train net output #0: loss = 0.0010979 (* 1 = 0.0010979 loss)
I0814 18:55:15.799808 10451 sgd_solver.cpp:136] Iteration 54500, lr = 0.0148437, m = 0.9
I0814 18:55:17.430318 10451 solver.cpp:312] Iteration 54600 (61.3311 iter/s, 1.63049s/100 iter), loss = 0.000547417
I0814 18:55:17.430363 10451 solver.cpp:334]     Train net output #0: loss = 0.000546185 (* 1 = 0.000546185 loss)
I0814 18:55:17.430371 10451 sgd_solver.cpp:136] Iteration 54600, lr = 0.0146875, m = 0.9
I0814 18:55:18.996803 10451 solver.cpp:312] Iteration 54700 (63.8392 iter/s, 1.56644s/100 iter), loss = 0.00375039
I0814 18:55:18.996830 10451 solver.cpp:334]     Train net output #0: loss = 0.00374916 (* 1 = 0.00374916 loss)
I0814 18:55:18.996836 10451 sgd_solver.cpp:136] Iteration 54700, lr = 0.0145312, m = 0.9
I0814 18:55:20.652500 10451 solver.cpp:312] Iteration 54800 (60.3993 iter/s, 1.65565s/100 iter), loss = 0.00116787
I0814 18:55:20.652547 10451 solver.cpp:334]     Train net output #0: loss = 0.00116664 (* 1 = 0.00116664 loss)
I0814 18:55:20.652559 10451 sgd_solver.cpp:136] Iteration 54800, lr = 0.014375, m = 0.9
I0814 18:55:22.294411 10451 solver.cpp:312] Iteration 54900 (60.9065 iter/s, 1.64186s/100 iter), loss = 0.000564652
I0814 18:55:22.294456 10451 solver.cpp:334]     Train net output #0: loss = 0.00056342 (* 1 = 0.00056342 loss)
I0814 18:55:22.294468 10451 sgd_solver.cpp:136] Iteration 54900, lr = 0.0142187, m = 0.9
I0814 18:55:23.896700 10451 solver.cpp:509] Iteration 55000, Testing net (#0)
I0814 18:55:24.711472 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.924707
I0814 18:55:24.711494 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:55:24.711499 10451 solver.cpp:594]     Test net output #2: loss = 0.259994 (* 1 = 0.259994 loss)
I0814 18:55:24.711524 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.814802s
I0814 18:55:24.729013 10451 solver.cpp:312] Iteration 55000 (41.0756 iter/s, 2.43453s/100 iter), loss = 0.00134337
I0814 18:55:24.729049 10451 solver.cpp:334]     Train net output #0: loss = 0.00134214 (* 1 = 0.00134214 loss)
I0814 18:55:24.729063 10451 sgd_solver.cpp:136] Iteration 55000, lr = 0.0140625, m = 0.9
I0814 18:55:26.363684 10451 solver.cpp:312] Iteration 55100 (61.1764 iter/s, 1.63462s/100 iter), loss = 0.00159734
I0814 18:55:26.363732 10451 solver.cpp:334]     Train net output #0: loss = 0.00159611 (* 1 = 0.00159611 loss)
I0814 18:55:26.363746 10451 sgd_solver.cpp:136] Iteration 55100, lr = 0.0139063, m = 0.9
I0814 18:55:27.986346 10451 solver.cpp:312] Iteration 55200 (61.6291 iter/s, 1.62261s/100 iter), loss = 0.00182078
I0814 18:55:27.986394 10451 solver.cpp:334]     Train net output #0: loss = 0.00181955 (* 1 = 0.00181955 loss)
I0814 18:55:27.986407 10451 sgd_solver.cpp:136] Iteration 55200, lr = 0.01375, m = 0.9
I0814 18:55:29.567176 10451 solver.cpp:312] Iteration 55300 (63.2597 iter/s, 1.58078s/100 iter), loss = 0.000906301
I0814 18:55:29.567200 10451 solver.cpp:334]     Train net output #0: loss = 0.000905069 (* 1 = 0.000905069 loss)
I0814 18:55:29.567205 10451 sgd_solver.cpp:136] Iteration 55300, lr = 0.0135938, m = 0.9
I0814 18:55:31.204169 10451 solver.cpp:312] Iteration 55400 (61.0896 iter/s, 1.63694s/100 iter), loss = 0.000625996
I0814 18:55:31.204216 10451 solver.cpp:334]     Train net output #0: loss = 0.000624764 (* 1 = 0.000624764 loss)
I0814 18:55:31.204228 10451 sgd_solver.cpp:136] Iteration 55400, lr = 0.0134375, m = 0.9
I0814 18:55:32.852048 10451 solver.cpp:312] Iteration 55500 (60.6858 iter/s, 1.64783s/100 iter), loss = 0.000557931
I0814 18:55:32.852193 10451 solver.cpp:334]     Train net output #0: loss = 0.000556698 (* 1 = 0.000556698 loss)
I0814 18:55:32.852210 10451 sgd_solver.cpp:136] Iteration 55500, lr = 0.0132813, m = 0.9
I0814 18:55:34.465461 10451 solver.cpp:312] Iteration 55600 (61.9824 iter/s, 1.61336s/100 iter), loss = 0.000669917
I0814 18:55:34.465492 10451 solver.cpp:334]     Train net output #0: loss = 0.000668685 (* 1 = 0.000668685 loss)
I0814 18:55:34.465499 10451 sgd_solver.cpp:136] Iteration 55600, lr = 0.013125, m = 0.9
I0814 18:55:36.097889 10451 solver.cpp:312] Iteration 55700 (61.2604 iter/s, 1.63238s/100 iter), loss = 0.000473318
I0814 18:55:36.097936 10451 solver.cpp:334]     Train net output #0: loss = 0.000472087 (* 1 = 0.000472087 loss)
I0814 18:55:36.097950 10451 sgd_solver.cpp:136] Iteration 55700, lr = 0.0129687, m = 0.9
I0814 18:55:37.770625 10451 solver.cpp:312] Iteration 55800 (59.7841 iter/s, 1.67269s/100 iter), loss = 0.00122252
I0814 18:55:37.770674 10451 solver.cpp:334]     Train net output #0: loss = 0.00122129 (* 1 = 0.00122129 loss)
I0814 18:55:37.770686 10451 sgd_solver.cpp:136] Iteration 55800, lr = 0.0128125, m = 0.9
I0814 18:55:39.423409 10451 solver.cpp:312] Iteration 55900 (60.5058 iter/s, 1.65273s/100 iter), loss = 0.00107546
I0814 18:55:39.423436 10451 solver.cpp:334]     Train net output #0: loss = 0.00107423 (* 1 = 0.00107423 loss)
I0814 18:55:39.423444 10451 sgd_solver.cpp:136] Iteration 55900, lr = 0.0126562, m = 0.9
I0814 18:55:41.064301 10451 solver.cpp:509] Iteration 56000, Testing net (#0)
I0814 18:55:41.886852 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.927354
I0814 18:55:41.886871 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:55:41.886876 10451 solver.cpp:594]     Test net output #2: loss = 0.258227 (* 1 = 0.258227 loss)
I0814 18:55:41.886891 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.822568s
I0814 18:55:41.904646 10451 solver.cpp:312] Iteration 56000 (40.3036 iter/s, 2.48117s/100 iter), loss = 0.000564412
I0814 18:55:41.904664 10451 solver.cpp:334]     Train net output #0: loss = 0.00056318 (* 1 = 0.00056318 loss)
I0814 18:55:41.904670 10451 sgd_solver.cpp:136] Iteration 56000, lr = 0.0125, m = 0.9
I0814 18:55:42.812085 10436 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 18:55:43.531683 10451 solver.cpp:312] Iteration 56100 (61.4634 iter/s, 1.62699s/100 iter), loss = 0.00166363
I0814 18:55:43.531740 10451 solver.cpp:334]     Train net output #0: loss = 0.0016624 (* 1 = 0.0016624 loss)
I0814 18:55:43.531754 10451 sgd_solver.cpp:136] Iteration 56100, lr = 0.0123438, m = 0.9
I0814 18:55:45.116880 10451 solver.cpp:312] Iteration 56200 (63.0856 iter/s, 1.58515s/100 iter), loss = 0.000565143
I0814 18:55:45.116904 10451 solver.cpp:334]     Train net output #0: loss = 0.000563911 (* 1 = 0.000563911 loss)
I0814 18:55:45.116907 10451 sgd_solver.cpp:136] Iteration 56200, lr = 0.0121875, m = 0.9
I0814 18:55:46.757547 10451 solver.cpp:312] Iteration 56300 (60.9527 iter/s, 1.64062s/100 iter), loss = 0.00117087
I0814 18:55:46.757571 10451 solver.cpp:334]     Train net output #0: loss = 0.00116963 (* 1 = 0.00116963 loss)
I0814 18:55:46.757577 10451 sgd_solver.cpp:136] Iteration 56300, lr = 0.0120313, m = 0.9
I0814 18:55:48.397936 10451 solver.cpp:312] Iteration 56400 (60.9629 iter/s, 1.64034s/100 iter), loss = 0.00128762
I0814 18:55:48.397985 10451 solver.cpp:334]     Train net output #0: loss = 0.00128639 (* 1 = 0.00128639 loss)
I0814 18:55:48.398000 10451 sgd_solver.cpp:136] Iteration 56400, lr = 0.011875, m = 0.9
I0814 18:55:49.990844 10451 solver.cpp:312] Iteration 56500 (62.7802 iter/s, 1.59286s/100 iter), loss = 0.000556568
I0814 18:55:49.990917 10451 solver.cpp:334]     Train net output #0: loss = 0.000555336 (* 1 = 0.000555336 loss)
I0814 18:55:49.990942 10451 sgd_solver.cpp:136] Iteration 56500, lr = 0.0117188, m = 0.9
I0814 18:55:51.601828 10451 solver.cpp:312] Iteration 56600 (62.0758 iter/s, 1.61093s/100 iter), loss = 0.00181939
I0814 18:55:51.601897 10451 solver.cpp:334]     Train net output #0: loss = 0.00181815 (* 1 = 0.00181815 loss)
I0814 18:55:51.601917 10451 sgd_solver.cpp:136] Iteration 56600, lr = 0.0115625, m = 0.9
I0814 18:55:53.270568 10451 solver.cpp:312] Iteration 56700 (59.9273 iter/s, 1.66869s/100 iter), loss = 0.000480917
I0814 18:55:53.270632 10451 solver.cpp:334]     Train net output #0: loss = 0.000479686 (* 1 = 0.000479686 loss)
I0814 18:55:53.270653 10451 sgd_solver.cpp:136] Iteration 56700, lr = 0.0114062, m = 0.9
I0814 18:55:54.895768 10451 solver.cpp:312] Iteration 56800 (61.5327 iter/s, 1.62515s/100 iter), loss = 0.000449994
I0814 18:55:54.895794 10451 solver.cpp:334]     Train net output #0: loss = 0.000448763 (* 1 = 0.000448763 loss)
I0814 18:55:54.895800 10451 sgd_solver.cpp:136] Iteration 56800, lr = 0.01125, m = 0.9
I0814 18:55:56.554152 10451 solver.cpp:312] Iteration 56900 (60.3017 iter/s, 1.65833s/100 iter), loss = 0.00108461
I0814 18:55:56.554177 10451 solver.cpp:334]     Train net output #0: loss = 0.00108338 (* 1 = 0.00108338 loss)
I0814 18:55:56.554183 10451 sgd_solver.cpp:136] Iteration 56900, lr = 0.0110937, m = 0.9
I0814 18:55:58.171747 10451 solver.cpp:509] Iteration 57000, Testing net (#0)
I0814 18:55:58.996601 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.924118
I0814 18:55:58.996620 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 18:55:58.996626 10451 solver.cpp:594]     Test net output #2: loss = 0.25858 (* 1 = 0.25858 loss)
I0814 18:55:58.996640 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.824873s
I0814 18:55:59.020314 10451 solver.cpp:312] Iteration 57000 (40.5499 iter/s, 2.4661s/100 iter), loss = 0.00314952
I0814 18:55:59.020356 10451 solver.cpp:334]     Train net output #0: loss = 0.00314829 (* 1 = 0.00314829 loss)
I0814 18:55:59.020364 10451 sgd_solver.cpp:136] Iteration 57000, lr = 0.0109375, m = 0.9
I0814 18:56:00.634039 10451 solver.cpp:312] Iteration 57100 (61.9704 iter/s, 1.61367s/100 iter), loss = 0.000750812
I0814 18:56:00.634093 10451 solver.cpp:334]     Train net output #0: loss = 0.00074958 (* 1 = 0.00074958 loss)
I0814 18:56:00.634109 10451 sgd_solver.cpp:136] Iteration 57100, lr = 0.0107813, m = 0.9
I0814 18:56:02.210853 10451 solver.cpp:312] Iteration 57200 (63.4209 iter/s, 1.57677s/100 iter), loss = 0.00177745
I0814 18:56:02.210922 10451 solver.cpp:334]     Train net output #0: loss = 0.00177621 (* 1 = 0.00177621 loss)
I0814 18:56:02.210948 10451 sgd_solver.cpp:136] Iteration 57200, lr = 0.010625, m = 0.9
I0814 18:56:03.805660 10451 solver.cpp:312] Iteration 57300 (62.7054 iter/s, 1.59476s/100 iter), loss = 0.00195043
I0814 18:56:03.805685 10451 solver.cpp:334]     Train net output #0: loss = 0.0019492 (* 1 = 0.0019492 loss)
I0814 18:56:03.805691 10451 sgd_solver.cpp:136] Iteration 57300, lr = 0.0104688, m = 0.9
I0814 18:56:05.430621 10451 solver.cpp:312] Iteration 57400 (61.5418 iter/s, 1.62491s/100 iter), loss = 0.00105527
I0814 18:56:05.430667 10451 solver.cpp:334]     Train net output #0: loss = 0.00105404 (* 1 = 0.00105404 loss)
I0814 18:56:05.430678 10451 sgd_solver.cpp:136] Iteration 57400, lr = 0.0103125, m = 0.9
I0814 18:56:07.086958 10451 solver.cpp:312] Iteration 57500 (60.376 iter/s, 1.65629s/100 iter), loss = 0.00108823
I0814 18:56:07.087023 10451 solver.cpp:334]     Train net output #0: loss = 0.00108699 (* 1 = 0.00108699 loss)
I0814 18:56:07.087041 10451 sgd_solver.cpp:136] Iteration 57500, lr = 0.0101563, m = 0.9
I0814 18:56:08.686594 10451 solver.cpp:312] Iteration 57600 (62.5162 iter/s, 1.59959s/100 iter), loss = 0.000894665
I0814 18:56:08.686652 10451 solver.cpp:334]     Train net output #0: loss = 0.000893434 (* 1 = 0.000893434 loss)
I0814 18:56:08.686671 10451 sgd_solver.cpp:136] Iteration 57600, lr = 0.01, m = 0.9
I0814 18:56:10.320752 10451 solver.cpp:312] Iteration 57700 (61.1954 iter/s, 1.63411s/100 iter), loss = 0.00124147
I0814 18:56:10.320797 10451 solver.cpp:334]     Train net output #0: loss = 0.00124024 (* 1 = 0.00124024 loss)
I0814 18:56:10.320811 10451 sgd_solver.cpp:136] Iteration 57700, lr = 0.00984375, m = 0.9
I0814 18:56:11.951863 10451 solver.cpp:312] Iteration 57800 (61.3099 iter/s, 1.63106s/100 iter), loss = 0.00169928
I0814 18:56:11.951969 10451 solver.cpp:334]     Train net output #0: loss = 0.00169805 (* 1 = 0.00169805 loss)
I0814 18:56:11.951984 10451 sgd_solver.cpp:136] Iteration 57800, lr = 0.0096875, m = 0.9
I0814 18:56:13.576314 10451 solver.cpp:312] Iteration 57900 (61.561 iter/s, 1.6244s/100 iter), loss = 0.00106803
I0814 18:56:13.576341 10451 solver.cpp:334]     Train net output #0: loss = 0.0010668 (* 1 = 0.0010668 loss)
I0814 18:56:13.576347 10451 sgd_solver.cpp:136] Iteration 57900, lr = 0.00953125, m = 0.9
I0814 18:56:15.164325 10451 solver.cpp:509] Iteration 58000, Testing net (#0)
I0814 18:56:15.992182 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.918824
I0814 18:56:15.992202 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 18:56:15.992207 10451 solver.cpp:594]     Test net output #2: loss = 0.277742 (* 1 = 0.277742 loss)
I0814 18:56:15.992223 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.827875s
I0814 18:56:16.007686 10451 solver.cpp:312] Iteration 58000 (41.1302 iter/s, 2.4313s/100 iter), loss = 0.00234378
I0814 18:56:16.007705 10451 solver.cpp:334]     Train net output #0: loss = 0.00234254 (* 1 = 0.00234254 loss)
I0814 18:56:16.007711 10451 sgd_solver.cpp:136] Iteration 58000, lr = 0.009375, m = 0.9
I0814 18:56:17.582916 10451 solver.cpp:312] Iteration 58100 (63.485 iter/s, 1.57518s/100 iter), loss = 0.0032052
I0814 18:56:17.582944 10451 solver.cpp:334]     Train net output #0: loss = 0.00320397 (* 1 = 0.00320397 loss)
I0814 18:56:17.582949 10451 sgd_solver.cpp:136] Iteration 58100, lr = 0.00921875, m = 0.9
I0814 18:56:19.205978 10451 solver.cpp:312] Iteration 58200 (61.6138 iter/s, 1.62301s/100 iter), loss = 0.000705798
I0814 18:56:19.206041 10451 solver.cpp:334]     Train net output #0: loss = 0.000704566 (* 1 = 0.000704566 loss)
I0814 18:56:19.206060 10451 sgd_solver.cpp:136] Iteration 58200, lr = 0.0090625, m = 0.9
I0814 18:56:20.825124 10451 solver.cpp:312] Iteration 58300 (61.7628 iter/s, 1.6191s/100 iter), loss = 0.00125104
I0814 18:56:20.825148 10451 solver.cpp:334]     Train net output #0: loss = 0.00124981 (* 1 = 0.00124981 loss)
I0814 18:56:20.825155 10451 sgd_solver.cpp:136] Iteration 58300, lr = 0.00890625, m = 0.9
I0814 18:56:22.462705 10451 solver.cpp:312] Iteration 58400 (61.0676 iter/s, 1.63753s/100 iter), loss = 0.000794407
I0814 18:56:22.462729 10451 solver.cpp:334]     Train net output #0: loss = 0.000793176 (* 1 = 0.000793176 loss)
I0814 18:56:22.462735 10451 sgd_solver.cpp:136] Iteration 58400, lr = 0.00875, m = 0.9
I0814 18:56:24.072718 10451 solver.cpp:312] Iteration 58500 (62.1131 iter/s, 1.60997s/100 iter), loss = 0.00209288
I0814 18:56:24.072796 10451 solver.cpp:334]     Train net output #0: loss = 0.00209165 (* 1 = 0.00209165 loss)
I0814 18:56:24.072818 10451 sgd_solver.cpp:136] Iteration 58500, lr = 0.00859375, m = 0.9
I0814 18:56:25.645290 10451 solver.cpp:312] Iteration 58600 (63.5921 iter/s, 1.57252s/100 iter), loss = 0.000244313
I0814 18:56:25.645354 10451 solver.cpp:334]     Train net output #0: loss = 0.000243081 (* 1 = 0.000243081 loss)
I0814 18:56:25.645372 10451 sgd_solver.cpp:136] Iteration 58600, lr = 0.0084375, m = 0.9
I0814 18:56:27.266263 10451 solver.cpp:312] Iteration 58700 (61.6932 iter/s, 1.62092s/100 iter), loss = 0.00109152
I0814 18:56:27.266311 10451 solver.cpp:334]     Train net output #0: loss = 0.00109029 (* 1 = 0.00109029 loss)
I0814 18:56:27.266325 10451 sgd_solver.cpp:136] Iteration 58700, lr = 0.00828125, m = 0.9
I0814 18:56:28.886451 10451 solver.cpp:312] Iteration 58800 (61.7231 iter/s, 1.62014s/100 iter), loss = 0.00117142
I0814 18:56:28.886473 10451 solver.cpp:334]     Train net output #0: loss = 0.00117019 (* 1 = 0.00117019 loss)
I0814 18:56:28.886479 10451 sgd_solver.cpp:136] Iteration 58800, lr = 0.008125, m = 0.9
I0814 18:56:30.462716 10451 solver.cpp:312] Iteration 58900 (63.443 iter/s, 1.57622s/100 iter), loss = 0.000894161
I0814 18:56:30.462777 10451 solver.cpp:334]     Train net output #0: loss = 0.000892929 (* 1 = 0.000892929 loss)
I0814 18:56:30.462795 10451 sgd_solver.cpp:136] Iteration 58900, lr = 0.00796875, m = 0.9
I0814 18:56:32.051406 10451 solver.cpp:509] Iteration 59000, Testing net (#0)
I0814 18:56:32.877914 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.922648
I0814 18:56:32.877935 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 18:56:32.877940 10451 solver.cpp:594]     Test net output #2: loss = 0.273025 (* 1 = 0.273025 loss)
I0814 18:56:32.877956 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.826527s
I0814 18:56:32.893761 10451 solver.cpp:312] Iteration 59000 (41.1357 iter/s, 2.43098s/100 iter), loss = 0.000754704
I0814 18:56:32.893780 10451 solver.cpp:334]     Train net output #0: loss = 0.000753471 (* 1 = 0.000753471 loss)
I0814 18:56:32.893786 10451 sgd_solver.cpp:136] Iteration 59000, lr = 0.0078125, m = 0.9
I0814 18:56:34.493808 10451 solver.cpp:312] Iteration 59100 (62.5001 iter/s, 1.6s/100 iter), loss = 0.00126008
I0814 18:56:34.493871 10451 solver.cpp:334]     Train net output #0: loss = 0.00125885 (* 1 = 0.00125885 loss)
I0814 18:56:34.493891 10451 sgd_solver.cpp:136] Iteration 59100, lr = 0.00765625, m = 0.9
I0814 18:56:36.111426 10451 solver.cpp:312] Iteration 59200 (61.8212 iter/s, 1.61757s/100 iter), loss = 0.000922871
I0814 18:56:36.111449 10451 solver.cpp:334]     Train net output #0: loss = 0.000921639 (* 1 = 0.000921639 loss)
I0814 18:56:36.111455 10451 sgd_solver.cpp:136] Iteration 59200, lr = 0.0075, m = 0.9
I0814 18:56:37.768780 10451 solver.cpp:312] Iteration 59300 (60.339 iter/s, 1.6573s/100 iter), loss = 0.000581206
I0814 18:56:37.768806 10451 solver.cpp:334]     Train net output #0: loss = 0.000579973 (* 1 = 0.000579973 loss)
I0814 18:56:37.768812 10451 sgd_solver.cpp:136] Iteration 59300, lr = 0.00734375, m = 0.9
I0814 18:56:39.379773 10451 solver.cpp:312] Iteration 59400 (62.0755 iter/s, 1.61094s/100 iter), loss = 0.00216083
I0814 18:56:39.379796 10451 solver.cpp:334]     Train net output #0: loss = 0.00215959 (* 1 = 0.00215959 loss)
I0814 18:56:39.379801 10451 sgd_solver.cpp:136] Iteration 59400, lr = 0.0071875, m = 0.9
I0814 18:56:40.977162 10451 solver.cpp:312] Iteration 59500 (62.604 iter/s, 1.59734s/100 iter), loss = 0.000483285
I0814 18:56:40.977210 10451 solver.cpp:334]     Train net output #0: loss = 0.000482052 (* 1 = 0.000482052 loss)
I0814 18:56:40.977221 10451 sgd_solver.cpp:136] Iteration 59500, lr = 0.00703125, m = 0.9
I0814 18:56:42.627460 10451 solver.cpp:312] Iteration 59600 (60.597 iter/s, 1.65025s/100 iter), loss = 0.00109495
I0814 18:56:42.627581 10451 solver.cpp:334]     Train net output #0: loss = 0.00109371 (* 1 = 0.00109371 loss)
I0814 18:56:42.627599 10451 sgd_solver.cpp:136] Iteration 59600, lr = 0.006875, m = 0.9
I0814 18:56:44.263106 10451 solver.cpp:312] Iteration 59700 (61.1398 iter/s, 1.6356s/100 iter), loss = 0.00246691
I0814 18:56:44.263167 10451 solver.cpp:334]     Train net output #0: loss = 0.00246567 (* 1 = 0.00246567 loss)
I0814 18:56:44.263186 10451 sgd_solver.cpp:136] Iteration 59700, lr = 0.00671875, m = 0.9
I0814 18:56:45.923985 10451 solver.cpp:312] Iteration 59800 (60.2109 iter/s, 1.66083s/100 iter), loss = 0.00158699
I0814 18:56:45.924010 10451 solver.cpp:334]     Train net output #0: loss = 0.00158575 (* 1 = 0.00158575 loss)
I0814 18:56:45.924015 10451 sgd_solver.cpp:136] Iteration 59800, lr = 0.0065625, m = 0.9
I0814 18:56:47.527837 10451 solver.cpp:312] Iteration 59900 (62.3517 iter/s, 1.6038s/100 iter), loss = 0.0006376
I0814 18:56:47.527884 10451 solver.cpp:334]     Train net output #0: loss = 0.000636366 (* 1 = 0.000636366 loss)
I0814 18:56:47.527896 10451 sgd_solver.cpp:136] Iteration 59900, lr = 0.00640625, m = 0.9
I0814 18:56:49.135640 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_60000.caffemodel
I0814 18:56:49.143610 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_60000.solverstate
I0814 18:56:49.147171 10451 solver.cpp:509] Iteration 60000, Testing net (#0)
I0814 18:56:49.948936 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.921177
I0814 18:56:49.948956 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 18:56:49.948961 10451 solver.cpp:594]     Test net output #2: loss = 0.282779 (* 1 = 0.282779 loss)
I0814 18:56:49.948982 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.801787s
I0814 18:56:49.968080 10451 solver.cpp:312] Iteration 60000 (40.9807 iter/s, 2.44017s/100 iter), loss = 0.00285988
I0814 18:56:49.968107 10451 solver.cpp:334]     Train net output #0: loss = 0.00285865 (* 1 = 0.00285865 loss)
I0814 18:56:49.968112 10451 sgd_solver.cpp:136] Iteration 60000, lr = 0.00625, m = 0.9
I0814 18:56:51.593371 10451 solver.cpp:312] Iteration 60100 (61.5293 iter/s, 1.62524s/100 iter), loss = 0.00131081
I0814 18:56:51.593395 10451 solver.cpp:334]     Train net output #0: loss = 0.00130958 (* 1 = 0.00130958 loss)
I0814 18:56:51.593400 10451 sgd_solver.cpp:136] Iteration 60100, lr = 0.00609375, m = 0.9
I0814 18:56:53.183264 10451 solver.cpp:312] Iteration 60200 (62.8993 iter/s, 1.58984s/100 iter), loss = 0.000997263
I0814 18:56:53.183323 10451 solver.cpp:334]     Train net output #0: loss = 0.00099603 (* 1 = 0.00099603 loss)
I0814 18:56:53.183341 10451 sgd_solver.cpp:136] Iteration 60200, lr = 0.0059375, m = 0.9
I0814 18:56:54.812763 10451 solver.cpp:312] Iteration 60300 (61.3704 iter/s, 1.62945s/100 iter), loss = 0.00188993
I0814 18:56:54.812788 10451 solver.cpp:334]     Train net output #0: loss = 0.0018887 (* 1 = 0.0018887 loss)
I0814 18:56:54.812794 10451 sgd_solver.cpp:136] Iteration 60300, lr = 0.00578125, m = 0.9
I0814 18:56:56.428689 10451 solver.cpp:312] Iteration 60400 (61.886 iter/s, 1.61588s/100 iter), loss = 0.000784928
I0814 18:56:56.428771 10451 solver.cpp:334]     Train net output #0: loss = 0.000783694 (* 1 = 0.000783694 loss)
I0814 18:56:56.428797 10451 sgd_solver.cpp:136] Iteration 60400, lr = 0.005625, m = 0.9
I0814 18:56:58.044888 10451 solver.cpp:312] Iteration 60500 (61.8753 iter/s, 1.61615s/100 iter), loss = 0.00123573
I0814 18:56:58.044910 10451 solver.cpp:334]     Train net output #0: loss = 0.0012345 (* 1 = 0.0012345 loss)
I0814 18:56:58.044916 10451 sgd_solver.cpp:136] Iteration 60500, lr = 0.00546875, m = 0.9
I0814 18:56:59.644196 10451 solver.cpp:312] Iteration 60600 (62.5289 iter/s, 1.59926s/100 iter), loss = 0.00208535
I0814 18:56:59.644265 10451 solver.cpp:334]     Train net output #0: loss = 0.00208411 (* 1 = 0.00208411 loss)
I0814 18:56:59.644300 10451 sgd_solver.cpp:136] Iteration 60600, lr = 0.0053125, m = 0.9
I0814 18:56:59.696897 10436 data_reader.cpp:288] Starting prefetch of epoch 8
I0814 18:57:01.246330 10451 solver.cpp:312] Iteration 60700 (62.4188 iter/s, 1.60208s/100 iter), loss = 0.000623473
I0814 18:57:01.246376 10451 solver.cpp:334]     Train net output #0: loss = 0.00062224 (* 1 = 0.00062224 loss)
I0814 18:57:01.246388 10451 sgd_solver.cpp:136] Iteration 60700, lr = 0.00515625, m = 0.9
I0814 18:57:02.826195 10451 solver.cpp:312] Iteration 60800 (63.2985 iter/s, 1.57982s/100 iter), loss = 0.004545
I0814 18:57:02.826220 10451 solver.cpp:334]     Train net output #0: loss = 0.00454377 (* 1 = 0.00454377 loss)
I0814 18:57:02.826225 10451 sgd_solver.cpp:136] Iteration 60800, lr = 0.005, m = 0.9
I0814 18:57:04.451730 10451 solver.cpp:312] Iteration 60900 (61.52 iter/s, 1.62549s/100 iter), loss = 0.00166931
I0814 18:57:04.451752 10451 solver.cpp:334]     Train net output #0: loss = 0.00166808 (* 1 = 0.00166808 loss)
I0814 18:57:04.451756 10451 sgd_solver.cpp:136] Iteration 60900, lr = 0.00484375, m = 0.9
I0814 18:57:06.023166 10451 solver.cpp:509] Iteration 61000, Testing net (#0)
I0814 18:57:06.838989 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917648
I0814 18:57:06.839011 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 18:57:06.839016 10451 solver.cpp:594]     Test net output #2: loss = 0.295314 (* 1 = 0.295314 loss)
I0814 18:57:06.839046 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.815856s
I0814 18:57:06.854648 10451 solver.cpp:312] Iteration 61000 (41.6172 iter/s, 2.40285s/100 iter), loss = 0.00107245
I0814 18:57:06.854678 10451 solver.cpp:334]     Train net output #0: loss = 0.00107122 (* 1 = 0.00107122 loss)
I0814 18:57:06.854689 10451 sgd_solver.cpp:136] Iteration 61000, lr = 0.0046875, m = 0.9
I0814 18:57:08.531898 10451 solver.cpp:312] Iteration 61100 (59.6232 iter/s, 1.6772s/100 iter), loss = 0.00103812
I0814 18:57:08.531985 10451 solver.cpp:334]     Train net output #0: loss = 0.00103689 (* 1 = 0.00103689 loss)
I0814 18:57:08.531993 10451 sgd_solver.cpp:136] Iteration 61100, lr = 0.00453125, m = 0.9
I0814 18:57:10.144480 10451 solver.cpp:312] Iteration 61200 (62.0144 iter/s, 1.61253s/100 iter), loss = 0.000756713
I0814 18:57:10.144508 10451 solver.cpp:334]     Train net output #0: loss = 0.000755479 (* 1 = 0.000755479 loss)
I0814 18:57:10.144515 10451 sgd_solver.cpp:136] Iteration 61200, lr = 0.004375, m = 0.9
I0814 18:57:11.748689 10451 solver.cpp:312] Iteration 61300 (62.3378 iter/s, 1.60416s/100 iter), loss = 0.000556967
I0814 18:57:11.748713 10451 solver.cpp:334]     Train net output #0: loss = 0.000555734 (* 1 = 0.000555734 loss)
I0814 18:57:11.748716 10451 sgd_solver.cpp:136] Iteration 61300, lr = 0.00421875, m = 0.9
I0814 18:57:13.339221 10451 solver.cpp:312] Iteration 61400 (62.8741 iter/s, 1.59048s/100 iter), loss = 0.00132137
I0814 18:57:13.339329 10451 solver.cpp:334]     Train net output #0: loss = 0.00132014 (* 1 = 0.00132014 loss)
I0814 18:57:13.339347 10451 sgd_solver.cpp:136] Iteration 61400, lr = 0.0040625, m = 0.9
I0814 18:57:14.979758 10451 solver.cpp:312] Iteration 61500 (60.9574 iter/s, 1.64049s/100 iter), loss = 0.00348396
I0814 18:57:14.979782 10451 solver.cpp:334]     Train net output #0: loss = 0.00348273 (* 1 = 0.00348273 loss)
I0814 18:57:14.979789 10451 sgd_solver.cpp:136] Iteration 61500, lr = 0.00390625, m = 0.9
I0814 18:57:16.604085 10451 solver.cpp:312] Iteration 61600 (61.5658 iter/s, 1.62428s/100 iter), loss = 0.00147628
I0814 18:57:16.604110 10451 solver.cpp:334]     Train net output #0: loss = 0.00147505 (* 1 = 0.00147505 loss)
I0814 18:57:16.604116 10451 sgd_solver.cpp:136] Iteration 61600, lr = 0.00375, m = 0.9
I0814 18:57:18.232282 10451 solver.cpp:312] Iteration 61700 (61.4195 iter/s, 1.62815s/100 iter), loss = 0.000346502
I0814 18:57:18.232306 10451 solver.cpp:334]     Train net output #0: loss = 0.000345268 (* 1 = 0.000345268 loss)
I0814 18:57:18.232312 10451 sgd_solver.cpp:136] Iteration 61700, lr = 0.00359375, m = 0.9
I0814 18:57:19.853334 10451 solver.cpp:312] Iteration 61800 (61.6903 iter/s, 1.621s/100 iter), loss = 0.00034867
I0814 18:57:19.853361 10451 solver.cpp:334]     Train net output #0: loss = 0.000347436 (* 1 = 0.000347436 loss)
I0814 18:57:19.853369 10451 sgd_solver.cpp:136] Iteration 61800, lr = 0.0034375, m = 0.9
I0814 18:57:21.479785 10451 solver.cpp:312] Iteration 61900 (61.4854 iter/s, 1.6264s/100 iter), loss = 0.00107245
I0814 18:57:21.479852 10451 solver.cpp:334]     Train net output #0: loss = 0.00107122 (* 1 = 0.00107122 loss)
I0814 18:57:21.479874 10451 sgd_solver.cpp:136] Iteration 61900, lr = 0.00328125, m = 0.9
I0814 18:57:23.051357 10451 solver.cpp:509] Iteration 62000, Testing net (#0)
I0814 18:57:23.883425 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.924413
I0814 18:57:23.883443 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 18:57:23.883447 10451 solver.cpp:594]     Test net output #2: loss = 0.285738 (* 1 = 0.285738 loss)
I0814 18:57:23.883463 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.832085s
I0814 18:57:23.898879 10451 solver.cpp:312] Iteration 62000 (41.339 iter/s, 2.41902s/100 iter), loss = 0.00153246
I0814 18:57:23.898895 10451 solver.cpp:334]     Train net output #0: loss = 0.00153122 (* 1 = 0.00153122 loss)
I0814 18:57:23.898901 10451 sgd_solver.cpp:136] Iteration 62000, lr = 0.003125, m = 0.9
I0814 18:57:25.521047 10451 solver.cpp:312] Iteration 62100 (61.6479 iter/s, 1.62212s/100 iter), loss = 0.000606953
I0814 18:57:25.521071 10451 solver.cpp:334]     Train net output #0: loss = 0.000605718 (* 1 = 0.000605718 loss)
I0814 18:57:25.521076 10451 sgd_solver.cpp:136] Iteration 62100, lr = 0.00296875, m = 0.9
I0814 18:57:27.171756 10451 solver.cpp:312] Iteration 62200 (60.5819 iter/s, 1.65066s/100 iter), loss = 0.000650115
I0814 18:57:27.171782 10451 solver.cpp:334]     Train net output #0: loss = 0.00064888 (* 1 = 0.00064888 loss)
I0814 18:57:27.171787 10451 sgd_solver.cpp:136] Iteration 62200, lr = 0.0028125, m = 0.9
I0814 18:57:28.783681 10451 solver.cpp:312] Iteration 62300 (62.0394 iter/s, 1.61188s/100 iter), loss = 0.00155333
I0814 18:57:28.783710 10451 solver.cpp:334]     Train net output #0: loss = 0.00155209 (* 1 = 0.00155209 loss)
I0814 18:57:28.783715 10451 sgd_solver.cpp:136] Iteration 62300, lr = 0.00265625, m = 0.9
I0814 18:57:30.386093 10451 solver.cpp:312] Iteration 62400 (62.4079 iter/s, 1.60236s/100 iter), loss = 0.000725817
I0814 18:57:30.386167 10451 solver.cpp:334]     Train net output #0: loss = 0.000724582 (* 1 = 0.000724582 loss)
I0814 18:57:30.386184 10451 sgd_solver.cpp:136] Iteration 62400, lr = 0.0025, m = 0.9
I0814 18:57:31.995882 10451 solver.cpp:312] Iteration 62500 (62.1218 iter/s, 1.60974s/100 iter), loss = 0.000801346
I0814 18:57:31.995932 10451 solver.cpp:334]     Train net output #0: loss = 0.000800111 (* 1 = 0.000800111 loss)
I0814 18:57:31.995949 10451 sgd_solver.cpp:136] Iteration 62500, lr = 0.00234375, m = 0.9
I0814 18:57:33.621520 10451 solver.cpp:312] Iteration 62600 (61.5162 iter/s, 1.62559s/100 iter), loss = 0.00141416
I0814 18:57:33.621546 10451 solver.cpp:334]     Train net output #0: loss = 0.00141293 (* 1 = 0.00141293 loss)
I0814 18:57:33.621552 10451 sgd_solver.cpp:136] Iteration 62600, lr = 0.0021875, m = 0.9
I0814 18:57:35.206928 10451 solver.cpp:312] Iteration 62700 (63.0772 iter/s, 1.58536s/100 iter), loss = 0.00109758
I0814 18:57:35.206976 10451 solver.cpp:334]     Train net output #0: loss = 0.00109634 (* 1 = 0.00109634 loss)
I0814 18:57:35.206987 10451 sgd_solver.cpp:136] Iteration 62700, lr = 0.00203125, m = 0.9
I0814 18:57:36.841001 10451 solver.cpp:312] Iteration 62800 (61.1986 iter/s, 1.63403s/100 iter), loss = 0.00219834
I0814 18:57:36.841025 10451 solver.cpp:334]     Train net output #0: loss = 0.0021971 (* 1 = 0.0021971 loss)
I0814 18:57:36.841030 10451 sgd_solver.cpp:136] Iteration 62800, lr = 0.001875, m = 0.9
I0814 18:57:38.473928 10451 solver.cpp:312] Iteration 62900 (61.2416 iter/s, 1.63288s/100 iter), loss = 0.000779434
I0814 18:57:38.474017 10451 solver.cpp:334]     Train net output #0: loss = 0.000778199 (* 1 = 0.000778199 loss)
I0814 18:57:38.474026 10451 sgd_solver.cpp:136] Iteration 62900, lr = 0.00171875, m = 0.9
I0814 18:57:40.071972 10451 solver.cpp:509] Iteration 63000, Testing net (#0)
I0814 18:57:40.887034 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920883
I0814 18:57:40.887053 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 18:57:40.887058 10451 solver.cpp:594]     Test net output #2: loss = 0.297442 (* 1 = 0.297442 loss)
I0814 18:57:40.887073 10451 solver.cpp:264] [MultiGPU] Tests completed in 0.81508s
I0814 18:57:40.902688 10451 solver.cpp:312] Iteration 63000 (41.1744 iter/s, 2.42869s/100 iter), loss = 0.000700657
I0814 18:57:40.902704 10451 solver.cpp:334]     Train net output #0: loss = 0.000699421 (* 1 = 0.000699421 loss)
I0814 18:57:40.902710 10451 sgd_solver.cpp:136] Iteration 63000, lr = 0.0015625, m = 0.9
I0814 18:57:42.499683 10451 solver.cpp:312] Iteration 63100 (62.6196 iter/s, 1.59694s/100 iter), loss = 0.00127757
I0814 18:57:42.499727 10451 solver.cpp:334]     Train net output #0: loss = 0.00127633 (* 1 = 0.00127633 loss)
I0814 18:57:42.499740 10451 sgd_solver.cpp:136] Iteration 63100, lr = 0.00140625, m = 0.9
I0814 18:57:44.164033 10451 solver.cpp:312] Iteration 63200 (60.0854 iter/s, 1.6643s/100 iter), loss = 0.00167401
I0814 18:57:44.164119 10451 solver.cpp:334]     Train net output #0: loss = 0.00167277 (* 1 = 0.00167277 loss)
I0814 18:57:44.164127 10451 sgd_solver.cpp:136] Iteration 63200, lr = 0.00125, m = 0.9
I0814 18:57:45.743232 10451 solver.cpp:312] Iteration 63300 (63.325 iter/s, 1.57915s/100 iter), loss = 0.00170525
I0814 18:57:45.743257 10451 solver.cpp:334]     Train net output #0: loss = 0.00170402 (* 1 = 0.00170402 loss)
I0814 18:57:45.743263 10451 sgd_solver.cpp:136] Iteration 63300, lr = 0.00109375, m = 0.9
I0814 18:57:47.361192 10451 solver.cpp:312] Iteration 63400 (61.8082 iter/s, 1.61791s/100 iter), loss = 0.000515989
I0814 18:57:47.361217 10451 solver.cpp:334]     Train net output #0: loss = 0.000514755 (* 1 = 0.000514755 loss)
I0814 18:57:47.361222 10451 sgd_solver.cpp:136] Iteration 63400, lr = 0.000937498, m = 0.9
I0814 18:57:48.979198 10451 solver.cpp:312] Iteration 63500 (61.8063 iter/s, 1.61796s/100 iter), loss = 0.000633863
I0814 18:57:48.979254 10451 solver.cpp:334]     Train net output #0: loss = 0.000632628 (* 1 = 0.000632628 loss)
I0814 18:57:48.979269 10451 sgd_solver.cpp:136] Iteration 63500, lr = 0.00078125, m = 0.9
I0814 18:57:50.587890 10451 solver.cpp:312] Iteration 63600 (62.1642 iter/s, 1.60864s/100 iter), loss = 0.00161503
I0814 18:57:50.587918 10451 solver.cpp:334]     Train net output #0: loss = 0.0016138 (* 1 = 0.0016138 loss)
I0814 18:57:50.587924 10451 sgd_solver.cpp:136] Iteration 63600, lr = 0.000625002, m = 0.9
I0814 18:57:52.151053 10451 solver.cpp:312] Iteration 63700 (63.9749 iter/s, 1.56311s/100 iter), loss = 0.00182812
I0814 18:57:52.151077 10451 solver.cpp:334]     Train net output #0: loss = 0.00182689 (* 1 = 0.00182689 loss)
I0814 18:57:52.151083 10451 sgd_solver.cpp:136] Iteration 63700, lr = 0.000468749, m = 0.9
I0814 18:57:53.803895 10451 solver.cpp:312] Iteration 63800 (60.5037 iter/s, 1.65279s/100 iter), loss = 0.00115639
I0814 18:57:53.803923 10451 solver.cpp:334]     Train net output #0: loss = 0.00115516 (* 1 = 0.00115516 loss)
I0814 18:57:53.803930 10451 sgd_solver.cpp:136] Iteration 63800, lr = 0.000312501, m = 0.9
I0814 18:57:55.416045 10451 solver.cpp:312] Iteration 63900 (62.0309 iter/s, 1.6121s/100 iter), loss = 0.00106957
I0814 18:57:55.416261 10451 solver.cpp:334]     Train net output #0: loss = 0.00106833 (* 1 = 0.00106833 loss)
I0814 18:57:55.416268 10451 sgd_solver.cpp:136] Iteration 63900, lr = 0.000156248, m = 0.9
I0814 18:57:57.030429 10451 solver.cpp:312] Iteration 63999 (61.3255 iter/s, 1.61434s/99 iter), loss = 0.00052641
I0814 18:57:57.030472 10451 solver.cpp:334]     Train net output #0: loss = 0.000525176 (* 1 = 0.000525176 loss)
I0814 18:57:57.030827 10451 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 18:57:57.040349 10451 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_64000.solverstate
I0814 18:57:57.050317 10451 solver.cpp:486] Iteration 64000, loss = 0.000863708
I0814 18:57:57.050343 10451 solver.cpp:509] Iteration 64000, Testing net (#0)
I0814 18:57:57.854610 10451 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915001
I0814 18:57:57.854629 10451 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 18:57:57.854635 10451 solver.cpp:594]     Test net output #2: loss = 0.311565 (* 1 = 0.311565 loss)
I0814 18:57:57.857556 10396 parallel.cpp:71] Root Solver performance on device 0: 58.97 * 22 = 1297 img/sec (64000 itr in 1085 sec)
I0814 18:57:57.857568 10396 parallel.cpp:76]      Solver performance on device 1: 58.97 * 22 = 1297 img/sec (64000 itr in 1085 sec)
I0814 18:57:57.857573 10396 parallel.cpp:76]      Solver performance on device 2: 58.97 * 22 = 1297 img/sec (64000 itr in 1085 sec)
I0814 18:57:57.857578 10396 parallel.cpp:79] Overall multi-GPU performance: 3892.15 img/sec
I0814 18:57:57.935192 10396 caffe.cpp:247] Optimization Done in 18m 8s
I0814 18:57:58.809798   556 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 18:57:58 2017
I0814 18:57:58.809919   556 caffe.cpp:611] CuDNN version: 6021
I0814 18:57:58.809923   556 caffe.cpp:612] CuBLAS version: 8000
I0814 18:57:58.809926   556 caffe.cpp:613] CUDA version: 8000
I0814 18:57:58.809927   556 caffe.cpp:614] CUDA driver version: 8000
I0814 18:57:59.060483   556 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 18:57:59.061066   556 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 18:57:59.061597   556 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0814 18:57:59.062113   556 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0814 18:57:59.062120   556 caffe.cpp:208] Using GPUs 0, 1, 2
I0814 18:57:59.062443   556 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0814 18:57:59.062764   556 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0814 18:57:59.063086   556 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0814 18:57:59.063123   556 solver.cpp:42] Solver data type: FLOAT
I0814 18:57:59.063151   556 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
I0814 18:57:59.069975   556 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/train.prototxt
I0814 18:57:59.070391   556 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0814 18:57:59.070399   556 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0814 18:57:59.070420   556 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:57:59.070603   556 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0814 18:57:59.070704   556 net.cpp:104] Using FLOAT as default forward math type
I0814 18:57:59.070708   556 net.cpp:110] Using FLOAT as default backward math type
I0814 18:57:59.070711   556 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 18:57:59.070714   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.070757   556 net.cpp:184] Created Layer data (0)
I0814 18:57:59.070762   556 net.cpp:530] data -> data
I0814 18:57:59.070771   556 net.cpp:530] data -> label
I0814 18:57:59.070791   556 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:57:59.070809   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:57:59.071651   592 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:57:59.072685   556 data_layer.cpp:185] [0] ReshapePrefetch 22, 3, 32, 32
I0814 18:57:59.072747   556 data_layer.cpp:209] [0] Output data size: 22, 3, 32, 32
I0814 18:57:59.072752   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:57:59.072770   556 net.cpp:245] Setting up data
I0814 18:57:59.072777   556 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0814 18:57:59.072782   556 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0814 18:57:59.072788   556 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 18:57:59.072793   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.072803   556 net.cpp:184] Created Layer data/bias (1)
I0814 18:57:59.072808   556 net.cpp:561] data/bias <- data
I0814 18:57:59.072814   556 net.cpp:530] data/bias -> data/bias
I0814 18:57:59.074765   556 net.cpp:245] Setting up data/bias
I0814 18:57:59.074775   556 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0814 18:57:59.074784   556 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 18:57:59.074787   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.074800   556 net.cpp:184] Created Layer conv1a (2)
I0814 18:57:59.074805   556 net.cpp:561] conv1a <- data/bias
I0814 18:57:59.074806   556 net.cpp:530] conv1a -> conv1a
I0814 18:57:59.363154   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0814 18:57:59.363173   556 net.cpp:245] Setting up conv1a
I0814 18:57:59.363179   556 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0814 18:57:59.363188   556 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 18:57:59.363193   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.363204   556 net.cpp:184] Created Layer conv1a/bn (3)
I0814 18:57:59.363206   556 net.cpp:561] conv1a/bn <- conv1a
I0814 18:57:59.363210   556 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 18:57:59.363850   556 net.cpp:245] Setting up conv1a/bn
I0814 18:57:59.363858   556 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0814 18:57:59.363867   556 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 18:57:59.363869   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.363874   556 net.cpp:184] Created Layer conv1a/relu (4)
I0814 18:57:59.363876   556 net.cpp:561] conv1a/relu <- conv1a
I0814 18:57:59.363878   556 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 18:57:59.363891   556 net.cpp:245] Setting up conv1a/relu
I0814 18:57:59.363895   556 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0814 18:57:59.363898   556 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 18:57:59.363899   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.363906   556 net.cpp:184] Created Layer conv1b (5)
I0814 18:57:59.363909   556 net.cpp:561] conv1b <- conv1a
I0814 18:57:59.363910   556 net.cpp:530] conv1b -> conv1b
I0814 18:57:59.370620   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0814 18:57:59.370630   556 net.cpp:245] Setting up conv1b
I0814 18:57:59.370635   556 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0814 18:57:59.370641   556 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 18:57:59.370645   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.370651   556 net.cpp:184] Created Layer conv1b/bn (6)
I0814 18:57:59.370653   556 net.cpp:561] conv1b/bn <- conv1b
I0814 18:57:59.370664   556 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 18:57:59.371253   556 net.cpp:245] Setting up conv1b/bn
I0814 18:57:59.371263   556 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0814 18:57:59.371269   556 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 18:57:59.371273   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.371276   556 net.cpp:184] Created Layer conv1b/relu (7)
I0814 18:57:59.371279   556 net.cpp:561] conv1b/relu <- conv1b
I0814 18:57:59.371281   556 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 18:57:59.371285   556 net.cpp:245] Setting up conv1b/relu
I0814 18:57:59.371289   556 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0814 18:57:59.371291   556 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 18:57:59.371294   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.371300   556 net.cpp:184] Created Layer pool1 (8)
I0814 18:57:59.371302   556 net.cpp:561] pool1 <- conv1b
I0814 18:57:59.371305   556 net.cpp:530] pool1 -> pool1
I0814 18:57:59.371378   556 net.cpp:245] Setting up pool1
I0814 18:57:59.371382   556 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0814 18:57:59.371386   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 18:57:59.371388   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.371397   556 net.cpp:184] Created Layer res2a_branch2a (9)
I0814 18:57:59.371400   556 net.cpp:561] res2a_branch2a <- pool1
I0814 18:57:59.371403   556 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 18:57:59.380489   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0814 18:57:59.380501   556 net.cpp:245] Setting up res2a_branch2a
I0814 18:57:59.380506   556 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0814 18:57:59.380513   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.380517   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.380522   556 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0814 18:57:59.380523   556 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 18:57:59.380527   556 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 18:57:59.381132   556 net.cpp:245] Setting up res2a_branch2a/bn
I0814 18:57:59.381140   556 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0814 18:57:59.381145   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.381148   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.381151   556 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0814 18:57:59.381155   556 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 18:57:59.381156   556 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 18:57:59.381160   556 net.cpp:245] Setting up res2a_branch2a/relu
I0814 18:57:59.381162   556 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0814 18:57:59.381165   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 18:57:59.381166   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.381175   556 net.cpp:184] Created Layer res2a_branch2b (12)
I0814 18:57:59.381177   556 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 18:57:59.381181   556 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 18:57:59.387694   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0814 18:57:59.387706   556 net.cpp:245] Setting up res2a_branch2b
I0814 18:57:59.387718   556 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0814 18:57:59.387723   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.387727   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.387732   556 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0814 18:57:59.387735   556 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 18:57:59.387737   556 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 18:57:59.388346   556 net.cpp:245] Setting up res2a_branch2b/bn
I0814 18:57:59.388353   556 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0814 18:57:59.388360   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.388363   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.388367   556 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0814 18:57:59.388370   556 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 18:57:59.388373   556 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 18:57:59.388377   556 net.cpp:245] Setting up res2a_branch2b/relu
I0814 18:57:59.388381   556 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0814 18:57:59.388382   556 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 18:57:59.388386   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.388389   556 net.cpp:184] Created Layer pool2 (15)
I0814 18:57:59.388392   556 net.cpp:561] pool2 <- res2a_branch2b
I0814 18:57:59.388394   556 net.cpp:530] pool2 -> pool2
I0814 18:57:59.388453   556 net.cpp:245] Setting up pool2
I0814 18:57:59.388456   556 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0814 18:57:59.388459   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 18:57:59.388463   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.388470   556 net.cpp:184] Created Layer res3a_branch2a (16)
I0814 18:57:59.388473   556 net.cpp:561] res3a_branch2a <- pool2
I0814 18:57:59.388476   556 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 18:57:59.399266   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0814 18:57:59.399308   556 net.cpp:245] Setting up res3a_branch2a
I0814 18:57:59.399317   556 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0814 18:57:59.399327   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.399334   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.399350   556 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0814 18:57:59.399355   556 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 18:57:59.399363   556 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 18:57:59.400300   556 net.cpp:245] Setting up res3a_branch2a/bn
I0814 18:57:59.400329   556 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0814 18:57:59.400348   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.403084   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.403108   556 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0814 18:57:59.403115   556 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 18:57:59.403126   556 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 18:57:59.403139   556 net.cpp:245] Setting up res3a_branch2a/relu
I0814 18:57:59.403147   556 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0814 18:57:59.403154   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 18:57:59.403172   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.403189   556 net.cpp:184] Created Layer res3a_branch2b (19)
I0814 18:57:59.403194   556 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 18:57:59.403198   556 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 18:57:59.408903   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0814 18:57:59.408927   556 net.cpp:245] Setting up res3a_branch2b
I0814 18:57:59.408936   556 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0814 18:57:59.408947   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.408952   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.408964   556 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0814 18:57:59.408967   556 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 18:57:59.408972   556 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 18:57:59.409687   556 net.cpp:245] Setting up res3a_branch2b/bn
I0814 18:57:59.409696   556 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0814 18:57:59.409703   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.409706   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.409713   556 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0814 18:57:59.409716   556 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 18:57:59.409719   556 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 18:57:59.409724   556 net.cpp:245] Setting up res3a_branch2b/relu
I0814 18:57:59.409729   556 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0814 18:57:59.409731   556 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 18:57:59.409734   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.409739   556 net.cpp:184] Created Layer pool3 (22)
I0814 18:57:59.409740   556 net.cpp:561] pool3 <- res3a_branch2b
I0814 18:57:59.409742   556 net.cpp:530] pool3 -> pool3
I0814 18:57:59.409803   556 net.cpp:245] Setting up pool3
I0814 18:57:59.409808   556 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0814 18:57:59.409811   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 18:57:59.409813   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.409822   556 net.cpp:184] Created Layer res4a_branch2a (23)
I0814 18:57:59.409826   556 net.cpp:561] res4a_branch2a <- pool3
I0814 18:57:59.409827   556 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 18:57:59.428618   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0814 18:57:59.428637   556 net.cpp:245] Setting up res4a_branch2a
I0814 18:57:59.428642   556 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0814 18:57:59.428648   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.428653   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.428660   556 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0814 18:57:59.428663   556 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 18:57:59.428668   556 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 18:57:59.429316   556 net.cpp:245] Setting up res4a_branch2a/bn
I0814 18:57:59.429322   556 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0814 18:57:59.429328   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.429342   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.429345   556 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0814 18:57:59.429347   556 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 18:57:59.429349   556 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 18:57:59.429353   556 net.cpp:245] Setting up res4a_branch2a/relu
I0814 18:57:59.429356   556 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0814 18:57:59.429358   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 18:57:59.429363   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.429373   556 net.cpp:184] Created Layer res4a_branch2b (26)
I0814 18:57:59.429374   556 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 18:57:59.429378   556 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 18:57:59.437366   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0814 18:57:59.437377   556 net.cpp:245] Setting up res4a_branch2b
I0814 18:57:59.437381   556 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0814 18:57:59.437386   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.437389   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.437394   556 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0814 18:57:59.437397   556 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 18:57:59.437399   556 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 18:57:59.438014   556 net.cpp:245] Setting up res4a_branch2b/bn
I0814 18:57:59.438021   556 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0814 18:57:59.438030   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.438032   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.438035   556 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0814 18:57:59.438037   556 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 18:57:59.438040   556 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 18:57:59.438043   556 net.cpp:245] Setting up res4a_branch2b/relu
I0814 18:57:59.438045   556 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0814 18:57:59.438047   556 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 18:57:59.438050   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.438053   556 net.cpp:184] Created Layer pool4 (29)
I0814 18:57:59.438055   556 net.cpp:561] pool4 <- res4a_branch2b
I0814 18:57:59.438057   556 net.cpp:530] pool4 -> pool4
I0814 18:57:59.438120   556 net.cpp:245] Setting up pool4
I0814 18:57:59.438125   556 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0814 18:57:59.438127   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 18:57:59.438129   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.438134   556 net.cpp:184] Created Layer res5a_branch2a (30)
I0814 18:57:59.438138   556 net.cpp:561] res5a_branch2a <- pool4
I0814 18:57:59.438139   556 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 18:57:59.479249   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0814 18:57:59.479266   556 net.cpp:245] Setting up res5a_branch2a
I0814 18:57:59.479272   556 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0814 18:57:59.479277   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.479281   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.479302   556 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0814 18:57:59.479305   556 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 18:57:59.479310   556 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 18:57:59.479945   556 net.cpp:245] Setting up res5a_branch2a/bn
I0814 18:57:59.479954   556 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0814 18:57:59.479959   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.479961   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.479966   556 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0814 18:57:59.479969   556 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 18:57:59.479970   556 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 18:57:59.479974   556 net.cpp:245] Setting up res5a_branch2a/relu
I0814 18:57:59.479977   556 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0814 18:57:59.479979   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 18:57:59.479981   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.479988   556 net.cpp:184] Created Layer res5a_branch2b (33)
I0814 18:57:59.479990   556 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 18:57:59.479993   556 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 18:57:59.498662   556 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0814 18:57:59.498679   556 net.cpp:245] Setting up res5a_branch2b
I0814 18:57:59.498684   556 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0814 18:57:59.498693   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.498697   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.498703   556 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0814 18:57:59.498706   556 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 18:57:59.498709   556 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 18:57:59.499342   556 net.cpp:245] Setting up res5a_branch2b/bn
I0814 18:57:59.499349   556 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0814 18:57:59.499356   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.499357   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.499361   556 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0814 18:57:59.499363   556 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 18:57:59.499366   556 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 18:57:59.499369   556 net.cpp:245] Setting up res5a_branch2b/relu
I0814 18:57:59.499372   556 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0814 18:57:59.499373   556 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 18:57:59.499377   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.499379   556 net.cpp:184] Created Layer pool5 (36)
I0814 18:57:59.499382   556 net.cpp:561] pool5 <- res5a_branch2b
I0814 18:57:59.499384   556 net.cpp:530] pool5 -> pool5
I0814 18:57:59.499413   556 net.cpp:245] Setting up pool5
I0814 18:57:59.499418   556 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0814 18:57:59.499420   556 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 18:57:59.499423   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.499428   556 net.cpp:184] Created Layer fc10 (37)
I0814 18:57:59.499429   556 net.cpp:561] fc10 <- pool5
I0814 18:57:59.499440   556 net.cpp:530] fc10 -> fc10
I0814 18:57:59.499701   556 net.cpp:245] Setting up fc10
I0814 18:57:59.499707   556 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0814 18:57:59.499711   556 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 18:57:59.499713   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.499725   556 net.cpp:184] Created Layer loss (38)
I0814 18:57:59.499727   556 net.cpp:561] loss <- fc10
I0814 18:57:59.499730   556 net.cpp:561] loss <- label
I0814 18:57:59.499734   556 net.cpp:530] loss -> loss
I0814 18:57:59.499877   556 net.cpp:245] Setting up loss
I0814 18:57:59.499883   556 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0814 18:57:59.499886   556 net.cpp:256]     with loss weight 1
I0814 18:57:59.499889   556 net.cpp:323] loss needs backward computation.
I0814 18:57:59.499891   556 net.cpp:323] fc10 needs backward computation.
I0814 18:57:59.499893   556 net.cpp:323] pool5 needs backward computation.
I0814 18:57:59.499896   556 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 18:57:59.499897   556 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 18:57:59.499898   556 net.cpp:323] res5a_branch2b needs backward computation.
I0814 18:57:59.499900   556 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 18:57:59.499903   556 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 18:57:59.499907   556 net.cpp:323] res5a_branch2a needs backward computation.
I0814 18:57:59.499910   556 net.cpp:323] pool4 needs backward computation.
I0814 18:57:59.499913   556 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 18:57:59.499917   556 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 18:57:59.499920   556 net.cpp:323] res4a_branch2b needs backward computation.
I0814 18:57:59.499923   556 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 18:57:59.499927   556 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 18:57:59.499929   556 net.cpp:323] res4a_branch2a needs backward computation.
I0814 18:57:59.499933   556 net.cpp:323] pool3 needs backward computation.
I0814 18:57:59.499935   556 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 18:57:59.499939   556 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 18:57:59.499941   556 net.cpp:323] res3a_branch2b needs backward computation.
I0814 18:57:59.499945   556 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 18:57:59.499948   556 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 18:57:59.499950   556 net.cpp:323] res3a_branch2a needs backward computation.
I0814 18:57:59.499954   556 net.cpp:323] pool2 needs backward computation.
I0814 18:57:59.499958   556 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 18:57:59.499961   556 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 18:57:59.499964   556 net.cpp:323] res2a_branch2b needs backward computation.
I0814 18:57:59.499967   556 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 18:57:59.499971   556 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 18:57:59.499974   556 net.cpp:323] res2a_branch2a needs backward computation.
I0814 18:57:59.499977   556 net.cpp:323] pool1 needs backward computation.
I0814 18:57:59.499980   556 net.cpp:323] conv1b/relu needs backward computation.
I0814 18:57:59.499984   556 net.cpp:323] conv1b/bn needs backward computation.
I0814 18:57:59.499986   556 net.cpp:323] conv1b needs backward computation.
I0814 18:57:59.499990   556 net.cpp:323] conv1a/relu needs backward computation.
I0814 18:57:59.499994   556 net.cpp:323] conv1a/bn needs backward computation.
I0814 18:57:59.499996   556 net.cpp:323] conv1a needs backward computation.
I0814 18:57:59.500000   556 net.cpp:325] data/bias does not need backward computation.
I0814 18:57:59.500005   556 net.cpp:325] data does not need backward computation.
I0814 18:57:59.500006   556 net.cpp:367] This network produces output loss
I0814 18:57:59.500041   556 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0814 18:57:59.500044   556 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0814 18:57:59.500046   556 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0814 18:57:59.500049   556 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0814 18:57:59.500052   556 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0814 18:57:59.500056   556 net.cpp:407] Network initialization done.
I0814 18:57:59.500416   556 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/test.prototxt
W0814 18:57:59.500460   556 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:57:59.500581   556 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 18:57:59.500689   556 net.cpp:104] Using FLOAT as default forward math type
I0814 18:57:59.500694   556 net.cpp:110] Using FLOAT as default backward math type
I0814 18:57:59.500696   556 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 18:57:59.500699   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.500711   556 net.cpp:184] Created Layer data (0)
I0814 18:57:59.500715   556 net.cpp:530] data -> data
I0814 18:57:59.500718   556 net.cpp:530] data -> label
I0814 18:57:59.500726   556 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:57:59.500735   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:57:59.501469   611 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:57:59.501526   556 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 32, 32
I0814 18:57:59.501590   556 data_layer.cpp:209] (0) Output data size: 17, 3, 32, 32
I0814 18:57:59.501593   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:57:59.501605   556 net.cpp:245] Setting up data
I0814 18:57:59.501610   556 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0814 18:57:59.501611   556 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0814 18:57:59.501615   556 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 18:57:59.501616   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.501621   556 net.cpp:184] Created Layer label_data_1_split (1)
I0814 18:57:59.501622   556 net.cpp:561] label_data_1_split <- label
I0814 18:57:59.501626   556 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 18:57:59.501636   556 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 18:57:59.501638   556 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 18:57:59.501698   556 net.cpp:245] Setting up label_data_1_split
I0814 18:57:59.501701   556 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:57:59.501704   556 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:57:59.501708   556 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 18:57:59.501709   556 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 18:57:59.501713   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.501716   556 net.cpp:184] Created Layer data/bias (2)
I0814 18:57:59.501720   556 net.cpp:561] data/bias <- data
I0814 18:57:59.501724   556 net.cpp:530] data/bias -> data/bias
I0814 18:57:59.501852   556 net.cpp:245] Setting up data/bias
I0814 18:57:59.501858   556 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0814 18:57:59.501865   556 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 18:57:59.501869   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.501879   556 net.cpp:184] Created Layer conv1a (3)
I0814 18:57:59.501883   556 net.cpp:561] conv1a <- data/bias
I0814 18:57:59.501888   556 net.cpp:530] conv1a -> conv1a
I0814 18:57:59.502287   612 data_layer.cpp:97] (0) Parser threads: 1
I0814 18:57:59.502295   612 data_layer.cpp:99] (0) Transformer threads: 1
I0814 18:57:59.504931   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0814 18:57:59.504942   556 net.cpp:245] Setting up conv1a
I0814 18:57:59.504948   556 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0814 18:57:59.504957   556 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 18:57:59.504961   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.504969   556 net.cpp:184] Created Layer conv1a/bn (4)
I0814 18:57:59.504972   556 net.cpp:561] conv1a/bn <- conv1a
I0814 18:57:59.504976   556 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 18:57:59.505594   556 net.cpp:245] Setting up conv1a/bn
I0814 18:57:59.505601   556 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0814 18:57:59.505611   556 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 18:57:59.505615   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.505620   556 net.cpp:184] Created Layer conv1a/relu (5)
I0814 18:57:59.505625   556 net.cpp:561] conv1a/relu <- conv1a
I0814 18:57:59.505628   556 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 18:57:59.505635   556 net.cpp:245] Setting up conv1a/relu
I0814 18:57:59.505640   556 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0814 18:57:59.505643   556 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 18:57:59.505647   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.505655   556 net.cpp:184] Created Layer conv1b (6)
I0814 18:57:59.505658   556 net.cpp:561] conv1b <- conv1a
I0814 18:57:59.505661   556 net.cpp:530] conv1b -> conv1b
I0814 18:57:59.508611   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0814 18:57:59.508621   556 net.cpp:245] Setting up conv1b
I0814 18:57:59.508627   556 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0814 18:57:59.508636   556 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 18:57:59.508641   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.508647   556 net.cpp:184] Created Layer conv1b/bn (7)
I0814 18:57:59.508656   556 net.cpp:561] conv1b/bn <- conv1b
I0814 18:57:59.508661   556 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 18:57:59.509280   556 net.cpp:245] Setting up conv1b/bn
I0814 18:57:59.509287   556 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0814 18:57:59.509296   556 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 18:57:59.509300   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.509305   556 net.cpp:184] Created Layer conv1b/relu (8)
I0814 18:57:59.509310   556 net.cpp:561] conv1b/relu <- conv1b
I0814 18:57:59.509315   556 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 18:57:59.509320   556 net.cpp:245] Setting up conv1b/relu
I0814 18:57:59.509325   556 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0814 18:57:59.509328   556 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 18:57:59.509332   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.509337   556 net.cpp:184] Created Layer pool1 (9)
I0814 18:57:59.509341   556 net.cpp:561] pool1 <- conv1b
I0814 18:57:59.509344   556 net.cpp:530] pool1 -> pool1
I0814 18:57:59.509405   556 net.cpp:245] Setting up pool1
I0814 18:57:59.509412   556 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0814 18:57:59.509415   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 18:57:59.509419   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.509428   556 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 18:57:59.509431   556 net.cpp:561] res2a_branch2a <- pool1
I0814 18:57:59.509435   556 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 18:57:59.512919   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0814 18:57:59.512930   556 net.cpp:245] Setting up res2a_branch2a
I0814 18:57:59.512936   556 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0814 18:57:59.512944   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.512949   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.512955   556 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 18:57:59.512959   556 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 18:57:59.512964   556 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 18:57:59.513597   556 net.cpp:245] Setting up res2a_branch2a/bn
I0814 18:57:59.513604   556 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0814 18:57:59.513612   556 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.513625   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.513630   556 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 18:57:59.513639   556 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 18:57:59.513645   556 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 18:57:59.513659   556 net.cpp:245] Setting up res2a_branch2a/relu
I0814 18:57:59.513664   556 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0814 18:57:59.513671   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 18:57:59.513676   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.513689   556 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 18:57:59.513692   556 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 18:57:59.513696   556 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 18:57:59.516703   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0814 18:57:59.516716   556 net.cpp:245] Setting up res2a_branch2b
I0814 18:57:59.516731   556 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0814 18:57:59.516736   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.516739   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.516746   556 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 18:57:59.516747   556 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 18:57:59.516752   556 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 18:57:59.517403   556 net.cpp:245] Setting up res2a_branch2b/bn
I0814 18:57:59.517410   556 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0814 18:57:59.517416   556 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.517418   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.517421   556 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 18:57:59.517423   556 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 18:57:59.517426   556 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 18:57:59.517429   556 net.cpp:245] Setting up res2a_branch2b/relu
I0814 18:57:59.517432   556 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0814 18:57:59.517434   556 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 18:57:59.517436   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.517441   556 net.cpp:184] Created Layer pool2 (16)
I0814 18:57:59.517443   556 net.cpp:561] pool2 <- res2a_branch2b
I0814 18:57:59.517448   556 net.cpp:530] pool2 -> pool2
I0814 18:57:59.517524   556 net.cpp:245] Setting up pool2
I0814 18:57:59.517529   556 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0814 18:57:59.517534   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 18:57:59.517542   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.517554   556 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 18:57:59.517563   556 net.cpp:561] res3a_branch2a <- pool2
I0814 18:57:59.517570   556 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 18:57:59.523484   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:57:59.523495   556 net.cpp:245] Setting up res3a_branch2a
I0814 18:57:59.523500   556 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0814 18:57:59.523506   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.523512   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.523520   556 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 18:57:59.523524   556 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 18:57:59.523530   556 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 18:57:59.524165   556 net.cpp:245] Setting up res3a_branch2a/bn
I0814 18:57:59.524173   556 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0814 18:57:59.524181   556 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.524183   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.524186   556 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 18:57:59.524188   556 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 18:57:59.524191   556 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 18:57:59.524195   556 net.cpp:245] Setting up res3a_branch2a/relu
I0814 18:57:59.524197   556 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0814 18:57:59.524199   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 18:57:59.524214   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.524224   556 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 18:57:59.524229   556 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 18:57:59.524231   556 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 18:57:59.527416   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0814 18:57:59.527426   556 net.cpp:245] Setting up res3a_branch2b
I0814 18:57:59.527431   556 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0814 18:57:59.527437   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.527442   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.527449   556 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 18:57:59.527454   556 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 18:57:59.527459   556 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 18:57:59.528077   556 net.cpp:245] Setting up res3a_branch2b/bn
I0814 18:57:59.528085   556 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0814 18:57:59.528095   556 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.528098   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.528103   556 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 18:57:59.528108   556 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 18:57:59.528112   556 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 18:57:59.528120   556 net.cpp:245] Setting up res3a_branch2b/relu
I0814 18:57:59.528125   556 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0814 18:57:59.528128   556 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 18:57:59.528138   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.528144   556 net.cpp:184] Created Layer pool3 (23)
I0814 18:57:59.528146   556 net.cpp:561] pool3 <- res3a_branch2b
I0814 18:57:59.528149   556 net.cpp:530] pool3 -> pool3
I0814 18:57:59.528216   556 net.cpp:245] Setting up pool3
I0814 18:57:59.528221   556 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0814 18:57:59.528226   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 18:57:59.528231   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.528240   556 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 18:57:59.528244   556 net.cpp:561] res4a_branch2a <- pool3
I0814 18:57:59.528247   556 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 18:57:59.538830   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 18:57:59.538838   556 net.cpp:245] Setting up res4a_branch2a
I0814 18:57:59.538842   556 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0814 18:57:59.538846   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.538849   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.538853   556 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 18:57:59.538856   556 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 18:57:59.538858   556 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 18:57:59.539486   556 net.cpp:245] Setting up res4a_branch2a/bn
I0814 18:57:59.539494   556 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0814 18:57:59.539499   556 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.539501   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.539510   556 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 18:57:59.539513   556 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 18:57:59.539515   556 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 18:57:59.539520   556 net.cpp:245] Setting up res4a_branch2a/relu
I0814 18:57:59.539522   556 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0814 18:57:59.539525   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 18:57:59.539528   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.539539   556 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 18:57:59.539544   556 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 18:57:59.539547   556 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 18:57:59.545004   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 18:57:59.545017   556 net.cpp:245] Setting up res4a_branch2b
I0814 18:57:59.545022   556 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0814 18:57:59.545029   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.545034   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.545044   556 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 18:57:59.545050   556 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 18:57:59.545053   556 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 18:57:59.545740   556 net.cpp:245] Setting up res4a_branch2b/bn
I0814 18:57:59.545747   556 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0814 18:57:59.545756   556 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.545763   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.545766   556 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 18:57:59.545770   556 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 18:57:59.545775   556 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 18:57:59.545783   556 net.cpp:245] Setting up res4a_branch2b/relu
I0814 18:57:59.545788   556 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0814 18:57:59.545791   556 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 18:57:59.545796   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.545804   556 net.cpp:184] Created Layer pool4 (30)
I0814 18:57:59.545809   556 net.cpp:561] pool4 <- res4a_branch2b
I0814 18:57:59.545814   556 net.cpp:530] pool4 -> pool4
I0814 18:57:59.545882   556 net.cpp:245] Setting up pool4
I0814 18:57:59.545887   556 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0814 18:57:59.545892   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 18:57:59.545895   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.545905   556 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 18:57:59.545909   556 net.cpp:561] res5a_branch2a <- pool4
I0814 18:57:59.545912   556 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 18:57:59.576697   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0814 18:57:59.576719   556 net.cpp:245] Setting up res5a_branch2a
I0814 18:57:59.576726   556 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0814 18:57:59.576735   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 18:57:59.576741   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.576767   556 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 18:57:59.576772   556 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 18:57:59.576777   556 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 18:57:59.577735   556 net.cpp:245] Setting up res5a_branch2a/bn
I0814 18:57:59.577745   556 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0814 18:57:59.577754   556 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 18:57:59.577759   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.577764   556 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 18:57:59.577767   556 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 18:57:59.577771   556 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 18:57:59.577778   556 net.cpp:245] Setting up res5a_branch2a/relu
I0814 18:57:59.577782   556 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0814 18:57:59.577786   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 18:57:59.577790   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.577800   556 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 18:57:59.577805   556 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 18:57:59.577808   556 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 18:57:59.596637   556 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0814 18:57:59.596652   556 net.cpp:245] Setting up res5a_branch2b
I0814 18:57:59.596657   556 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0814 18:57:59.596665   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 18:57:59.596669   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.596676   556 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 18:57:59.596678   556 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 18:57:59.596683   556 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 18:57:59.597337   556 net.cpp:245] Setting up res5a_branch2b/bn
I0814 18:57:59.597344   556 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0814 18:57:59.597349   556 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 18:57:59.597352   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597357   556 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 18:57:59.597358   556 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 18:57:59.597362   556 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 18:57:59.597364   556 net.cpp:245] Setting up res5a_branch2b/relu
I0814 18:57:59.597368   556 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0814 18:57:59.597369   556 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 18:57:59.597371   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597376   556 net.cpp:184] Created Layer pool5 (37)
I0814 18:57:59.597378   556 net.cpp:561] pool5 <- res5a_branch2b
I0814 18:57:59.597381   556 net.cpp:530] pool5 -> pool5
I0814 18:57:59.597409   556 net.cpp:245] Setting up pool5
I0814 18:57:59.597412   556 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0814 18:57:59.597415   556 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 18:57:59.597417   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597421   556 net.cpp:184] Created Layer fc10 (38)
I0814 18:57:59.597424   556 net.cpp:561] fc10 <- pool5
I0814 18:57:59.597425   556 net.cpp:530] fc10 -> fc10
I0814 18:57:59.597683   556 net.cpp:245] Setting up fc10
I0814 18:57:59.597698   556 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0814 18:57:59.597702   556 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0814 18:57:59.597704   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597707   556 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0814 18:57:59.597709   556 net.cpp:561] fc10_fc10_0_split <- fc10
I0814 18:57:59.597712   556 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0814 18:57:59.597715   556 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0814 18:57:59.597718   556 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0814 18:57:59.597779   556 net.cpp:245] Setting up fc10_fc10_0_split
I0814 18:57:59.597784   556 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:57:59.597786   556 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:57:59.597789   556 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 18:57:59.597790   556 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 18:57:59.597792   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597797   556 net.cpp:184] Created Layer loss (40)
I0814 18:57:59.597800   556 net.cpp:561] loss <- fc10_fc10_0_split_0
I0814 18:57:59.597801   556 net.cpp:561] loss <- label_data_1_split_0
I0814 18:57:59.597805   556 net.cpp:530] loss -> loss
I0814 18:57:59.597936   556 net.cpp:245] Setting up loss
I0814 18:57:59.597942   556 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 18:57:59.597944   556 net.cpp:256]     with loss weight 1
I0814 18:57:59.597947   556 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 18:57:59.597950   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597956   556 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 18:57:59.597959   556 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0814 18:57:59.597960   556 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 18:57:59.597964   556 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 18:57:59.597967   556 net.cpp:245] Setting up accuracy/top1
I0814 18:57:59.597970   556 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 18:57:59.597971   556 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 18:57:59.597973   556 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 18:57:59.597976   556 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 18:57:59.597980   556 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0814 18:57:59.597981   556 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 18:57:59.597985   556 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 18:57:59.597988   556 net.cpp:245] Setting up accuracy/top5
I0814 18:57:59.597990   556 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 18:57:59.597993   556 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 18:57:59.597996   556 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 18:57:59.597998   556 net.cpp:323] loss needs backward computation.
I0814 18:57:59.598001   556 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0814 18:57:59.598002   556 net.cpp:323] fc10 needs backward computation.
I0814 18:57:59.598004   556 net.cpp:323] pool5 needs backward computation.
I0814 18:57:59.598007   556 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 18:57:59.598008   556 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 18:57:59.598011   556 net.cpp:323] res5a_branch2b needs backward computation.
I0814 18:57:59.598012   556 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 18:57:59.598016   556 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 18:57:59.598016   556 net.cpp:323] res5a_branch2a needs backward computation.
I0814 18:57:59.598026   556 net.cpp:323] pool4 needs backward computation.
I0814 18:57:59.598029   556 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 18:57:59.598031   556 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 18:57:59.598033   556 net.cpp:323] res4a_branch2b needs backward computation.
I0814 18:57:59.598036   556 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 18:57:59.598037   556 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 18:57:59.598039   556 net.cpp:323] res4a_branch2a needs backward computation.
I0814 18:57:59.598042   556 net.cpp:323] pool3 needs backward computation.
I0814 18:57:59.598044   556 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 18:57:59.598047   556 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 18:57:59.598048   556 net.cpp:323] res3a_branch2b needs backward computation.
I0814 18:57:59.598050   556 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 18:57:59.598052   556 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 18:57:59.598055   556 net.cpp:323] res3a_branch2a needs backward computation.
I0814 18:57:59.598057   556 net.cpp:323] pool2 needs backward computation.
I0814 18:57:59.598058   556 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 18:57:59.598062   556 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 18:57:59.598063   556 net.cpp:323] res2a_branch2b needs backward computation.
I0814 18:57:59.598065   556 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 18:57:59.598067   556 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 18:57:59.598069   556 net.cpp:323] res2a_branch2a needs backward computation.
I0814 18:57:59.598071   556 net.cpp:323] pool1 needs backward computation.
I0814 18:57:59.598074   556 net.cpp:323] conv1b/relu needs backward computation.
I0814 18:57:59.598076   556 net.cpp:323] conv1b/bn needs backward computation.
I0814 18:57:59.598078   556 net.cpp:323] conv1b needs backward computation.
I0814 18:57:59.598080   556 net.cpp:323] conv1a/relu needs backward computation.
I0814 18:57:59.598083   556 net.cpp:323] conv1a/bn needs backward computation.
I0814 18:57:59.598085   556 net.cpp:323] conv1a needs backward computation.
I0814 18:57:59.598088   556 net.cpp:325] data/bias does not need backward computation.
I0814 18:57:59.598090   556 net.cpp:325] label_data_1_split does not need backward computation.
I0814 18:57:59.598093   556 net.cpp:325] data does not need backward computation.
I0814 18:57:59.598096   556 net.cpp:367] This network produces output accuracy/top1
I0814 18:57:59.598098   556 net.cpp:367] This network produces output accuracy/top5
I0814 18:57:59.598101   556 net.cpp:367] This network produces output loss
I0814 18:57:59.598126   556 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0814 18:57:59.598129   556 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0814 18:57:59.598131   556 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0814 18:57:59.598134   556 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 18:57:59.598135   556 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 18:57:59.598137   556 net.cpp:407] Network initialization done.
I0814 18:57:59.598188   556 solver.cpp:56] Solver scaffolding done.
I0814 18:57:59.602416   556 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-14_18-39-46/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 18:57:59.607378   556 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 18:57:59.607405   556 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 18:57:59.607445   556 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 18:57:59.607460   556 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.607775   556 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 18:57:59.607794   556 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 18:57:59.607806   556 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.608001   556 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 18:57:59.608008   556 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 18:57:59.608012   556 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.608031   556 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.608229   556 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.608237   556 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.608252   556 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.608434   556 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.608440   556 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 18:57:59.608444   556 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.608487   556 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.608656   556 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.608662   556 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.608690   556 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.608844   556 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.608850   556 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 18:57:59.608855   556 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.608976   556 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.609136   556 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.609143   556 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.609212   556 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.609370   556 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.609376   556 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 18:57:59.609380   556 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.609763   556 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.609933   556 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.609941   556 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.610136   556 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.610299   556 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.610306   556 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 18:57:59.610309   556 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 18:57:59.610322   556 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 18:57:59.613785   556 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 18:57:59.613813   556 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 18:57:59.613847   556 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 18:57:59.613863   556 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.614171   556 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 18:57:59.614177   556 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 18:57:59.614188   556 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.614394   556 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 18:57:59.614401   556 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 18:57:59.614405   556 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.614423   556 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.614622   556 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.614629   556 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.614644   556 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.614827   556 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.614835   556 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 18:57:59.614837   556 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.614909   556 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.615118   556 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.615124   556 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.615164   556 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.615350   556 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.615356   556 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 18:57:59.615361   556 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.615578   556 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.615769   556 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.615777   556 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.615892   556 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.616052   556 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.616060   556 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 18:57:59.616063   556 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 18:57:59.616472   556 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 18:57:59.616606   556 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 18:57:59.616611   556 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 18:57:59.616791   556 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 18:57:59.616914   556 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 18:57:59.616919   556 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 18:57:59.616920   556 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 18:57:59.616931   556 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 18:57:59.617017   556 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0814 18:57:59.617022   556 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0814 18:57:59.617025   556 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0814 18:57:59.617027   556 parallel.cpp:59] Starting Optimization
I0814 18:57:59.617030   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 18:57:59.617063   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:57:59.617081   556 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:57:59.617736   613 device_alternate.hpp:116] NVML initialized on thread 139686397118208
I0814 18:57:59.630224   613 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 18:57:59.630278   614 device_alternate.hpp:116] NVML initialized on thread 139686388725504
I0814 18:57:59.631216   614 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 18:57:59.631230   615 device_alternate.hpp:116] NVML initialized on thread 139686380332800
I0814 18:57:59.631891   615 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 18:57:59.635761   614 solver.cpp:42] Solver data type: FLOAT
W0814 18:57:59.636374   614 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:57:59.636497   614 net.cpp:104] Using FLOAT as default forward math type
I0814 18:57:59.636505   614 net.cpp:110] Using FLOAT as default backward math type
I0814 18:57:59.636553   614 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:57:59.636569   614 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:57:59.639961   615 solver.cpp:42] Solver data type: FLOAT
W0814 18:57:59.640400   615 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 18:57:59.640487   615 net.cpp:104] Using FLOAT as default forward math type
I0814 18:57:59.640494   615 net.cpp:110] Using FLOAT as default backward math type
I0814 18:57:59.640525   615 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 18:57:59.640537   615 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:57:59.640677   616 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:57:59.641551   617 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 18:57:59.641700   614 data_layer.cpp:185] [1] ReshapePrefetch 22, 3, 32, 32
I0814 18:57:59.642566   615 data_layer.cpp:185] [2] ReshapePrefetch 22, 3, 32, 32
I0814 18:57:59.642638   614 data_layer.cpp:209] [1] Output data size: 22, 3, 32, 32
I0814 18:57:59.642647   614 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:57:59.642693   615 data_layer.cpp:209] [2] Output data size: 22, 3, 32, 32
I0814 18:57:59.642711   615 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:58:00.106839   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 18:58:00.128237   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 18:58:00.130591   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 18:58:00.139853   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 18:58:00.140468   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 18:58:00.150177   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 18:58:00.154572   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 18:58:00.163355   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 18:58:00.164436   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 18:58:00.172821   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 18:58:00.177237   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 18:58:00.184353   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 18:58:00.197270   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 18:58:00.207002   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 18:58:00.207504   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0814 18:58:00.215893   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0814 18:58:00.252426   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 18:58:00.259016   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 18:58:00.272644   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 18:58:00.274224   614 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/test.prototxt
W0814 18:58:00.274274   614 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:58:00.274351   614 net.cpp:104] Using FLOAT as default forward math type
I0814 18:58:00.274356   614 net.cpp:110] Using FLOAT as default backward math type
I0814 18:58:00.274374   614 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:58:00.274380   614 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:58:00.275141   620 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:58:00.275256   614 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 32, 32
I0814 18:58:00.275410   614 data_layer.cpp:209] (1) Output data size: 17, 3, 32, 32
I0814 18:58:00.275418   614 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 18:58:00.276432   621 data_layer.cpp:97] (1) Parser threads: 1
I0814 18:58:00.276439   621 data_layer.cpp:99] (1) Transformer threads: 1
I0814 18:58:00.280073   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 18:58:00.282663   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 18:58:00.284327   615 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/test.prototxt
W0814 18:58:00.284374   615 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 18:58:00.284453   615 net.cpp:104] Using FLOAT as default forward math type
I0814 18:58:00.284457   615 net.cpp:110] Using FLOAT as default backward math type
I0814 18:58:00.284472   615 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 18:58:00.284479   615 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:58:00.284564   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 18:58:00.285249   622 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 18:58:00.285310   615 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 32, 32
I0814 18:58:00.285444   615 data_layer.cpp:209] (2) Output data size: 17, 3, 32, 32
I0814 18:58:00.285449   615 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 18:58:00.286301   623 data_layer.cpp:97] (2) Parser threads: 1
I0814 18:58:00.286310   623 data_layer.cpp:99] (2) Transformer threads: 1
I0814 18:58:00.289921   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 18:58:00.290402   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 18:58:00.294618   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 18:58:00.295430   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 18:58:00.299952   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 18:58:00.303792   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 18:58:00.305091   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 18:58:00.308926   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 18:58:00.330965   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 18:58:00.349627   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 18:58:00.350803   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 18:58:00.356366   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 18:58:00.364092   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 18:58:00.370630   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 18:58:00.389578   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 18:58:00.405167   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 18:58:00.409523   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 18:58:00.411995   614 solver.cpp:56] Solver scaffolding done.
I0814 18:58:00.428007   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 18:58:00.429550   615 solver.cpp:56] Solver scaffolding done.
I0814 18:58:00.474635   614 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0814 18:58:00.474660   615 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0814 18:58:00.474660   613 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0814 18:58:00.655833   615 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:58:00.655853   615 solver.cpp:439] Learning Rate Policy: poly
I0814 18:58:00.655860   613 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:58:00.655869   613 solver.cpp:439] Learning Rate Policy: poly
I0814 18:58:00.655889   614 solver.cpp:438] Solving jacintonet11v2_train
I0814 18:58:00.655897   614 solver.cpp:439] Learning Rate Policy: poly
I0814 18:58:00.662636   614 solver.cpp:227] Starting Optimization on GPU 1
I0814 18:58:00.662642   615 solver.cpp:227] Starting Optimization on GPU 2
I0814 18:58:00.662672   613 solver.cpp:227] Starting Optimization on GPU 0
I0814 18:58:00.662794   613 solver.cpp:509] Iteration 0, Testing net (#0)
I0814 18:58:00.662796   640 device_alternate.hpp:116] NVML initialized on thread 139685616518912
I0814 18:58:00.662834   640 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 18:58:00.663596   641 device_alternate.hpp:116] NVML initialized on thread 139685624911616
I0814 18:58:00.663611   641 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 18:58:00.663621   642 device_alternate.hpp:116] NVML initialized on thread 139685608126208
I0814 18:58:00.663642   642 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 18:58:00.672150   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0814 18:58:00.672612   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0814 18:58:00.677130   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0814 18:58:00.677780   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0814 18:58:00.680330   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0814 18:58:00.684980   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:58:00.686468   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 18:58:00.689206   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0814 18:58:00.690688   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0814 18:58:00.691998   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0814 18:58:00.697806   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0814 18:58:00.698791   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0814 18:58:00.699388   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0814 18:58:00.704150   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0814 18:58:00.705171   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0814 18:58:00.705519   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0814 18:58:00.711588   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0814 18:58:00.714012   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0814 18:58:00.714648   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0814 18:58:00.716493   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0814 18:58:00.721395   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0814 18:58:00.721773   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0814 18:58:00.724592   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0814 18:58:00.729332   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0814 18:58:00.732103   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0814 18:58:00.732488   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0814 18:58:00.736973   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0814 18:58:00.737758   614 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0814 18:58:00.738447   615 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0814 18:58:00.743052   613 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0814 18:58:00.745635   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 1
I0814 18:58:00.745646   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0814 18:58:00.745651   613 solver.cpp:594]     Test net output #2: loss = 0.0157617 (* 1 = 0.0157617 loss)
I0814 18:58:00.745656   613 solver.cpp:254] [MultiGPU] Initial Test completed
I0814 18:58:00.745671   613 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 18:58:00.755549   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0814 18:58:00.755846   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0814 18:58:00.756314   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0814 18:58:00.764144   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0814 18:58:00.765630   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0814 18:58:00.766041   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.79G, req 0G)
I0814 18:58:00.774771   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0814 18:58:00.776578   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0814 18:58:00.777418   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0814 18:58:00.783592   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0814 18:58:00.785780   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0814 18:58:00.786193   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0814 18:58:00.794153   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0814 18:58:00.797289   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0814 18:58:00.797729   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0814 18:58:00.800206   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0814 18:58:00.804668   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0814 18:58:00.804935   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0814 18:58:00.814416   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0814 18:58:00.819767   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0814 18:58:00.820945   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0814 18:58:00.821667   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0814 18:58:00.827654   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0814 18:58:00.829885   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0814 18:58:00.840168   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0814 18:58:00.845576   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0814 18:58:00.848228   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0814 18:58:00.848793   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0814 18:58:00.853555   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0814 18:58:00.856276   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0814 18:58:00.886109   618 data_layer.cpp:97] [1] Parser threads: 1
I0814 18:58:00.886127   618 data_layer.cpp:99] [1] Transformer threads: 1
I0814 18:58:00.895926   619 data_layer.cpp:97] [2] Parser threads: 1
I0814 18:58:00.895938   619 data_layer.cpp:99] [2] Transformer threads: 1
I0814 18:58:00.899823   595 data_layer.cpp:97] [0] Parser threads: 1
I0814 18:58:00.899838   595 data_layer.cpp:99] [0] Transformer threads: 1
I0814 18:58:00.905618   613 solver.cpp:317] Iteration 0 (0.159937 s), loss = 0.000548466
I0814 18:58:00.905635   613 solver.cpp:334]     Train net output #0: loss = 0.000548466 (* 1 = 0.000548466 loss)
I0814 18:58:00.905642   613 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0814 18:58:00.934574   613 solver.cpp:317] Iteration 1 (0.0289444 s), loss = 0.00309271
I0814 18:58:00.934600   613 solver.cpp:334]     Train net output #0: loss = 0.00309271 (* 1 = 0.00309271 loss)
I0814 18:58:00.950670   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.98G, req 0.01G)
I0814 18:58:00.951123   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0814 18:58:00.951709   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0814 18:58:00.959046   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.34G, req 0.01G)
I0814 18:58:00.962069   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.962230   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.971765   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 1  (limit 6.34G, req 0.01G)
I0814 18:58:00.976080   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.976320   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.978543   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.01G)
I0814 18:58:00.983597   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.984035   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.987265   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.01G)
I0814 18:58:00.991374   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.01G)
I0814 18:58:00.993340   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0814 18:58:00.993840   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0814 18:58:00.998183   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:00.998622   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0814 18:58:01.013521   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.02G)
I0814 18:58:01.020581   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.02G)
I0814 18:58:01.022765   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0814 18:58:01.023008   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0814 18:58:01.030165   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0814 18:58:01.031278   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0814 18:58:01.054518   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.34G, req 0.03G)
I0814 18:58:01.062980   613 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.34G, req 0.03G)
I0814 18:58:01.066308   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0814 18:58:01.070242   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0814 18:58:01.076056   614 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0814 18:58:01.078339   615 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0814 18:58:01.091893   613 solver.cpp:317] Iteration 2 (0.15731 s), loss = 0.00064818
I0814 18:58:01.091922   613 solver.cpp:334]     Train net output #0: loss = 0.00064818 (* 1 = 0.00064818 loss)
I0814 18:58:01.091962   613 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:58:01.091964   614 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:58:01.092200   615 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 18:58:02.694392   613 solver.cpp:312] Iteration 100 (61.1564 iter/s, 1.60245s/98 iter), loss = 0.00096959
I0814 18:58:02.694568   613 solver.cpp:334]     Train net output #0: loss = 0.00096959 (* 1 = 0.00096959 loss)
I0814 18:58:02.694641   613 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0814 18:58:04.294579   613 solver.cpp:312] Iteration 200 (62.4946 iter/s, 1.60014s/100 iter), loss = 0.00222569
I0814 18:58:04.294603   613 solver.cpp:334]     Train net output #0: loss = 0.00222569 (* 1 = 0.00222569 loss)
I0814 18:58:04.294608   613 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0814 18:58:05.923637   613 solver.cpp:312] Iteration 300 (61.3869 iter/s, 1.62901s/100 iter), loss = 0.000549344
I0814 18:58:05.923661   613 solver.cpp:334]     Train net output #0: loss = 0.000549344 (* 1 = 0.000549344 loss)
I0814 18:58:05.923667   613 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0814 18:58:07.560977   613 solver.cpp:312] Iteration 400 (61.0766 iter/s, 1.63729s/100 iter), loss = 0.000610687
I0814 18:58:07.561024   613 solver.cpp:334]     Train net output #0: loss = 0.000610687 (* 1 = 0.000610687 loss)
I0814 18:58:07.561036   613 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0814 18:58:09.142563   613 solver.cpp:312] Iteration 500 (63.2297 iter/s, 1.58154s/100 iter), loss = 0.00207446
I0814 18:58:09.142587   613 solver.cpp:334]     Train net output #0: loss = 0.00207446 (* 1 = 0.00207446 loss)
I0814 18:58:09.142593   613 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0814 18:58:10.761804   613 solver.cpp:312] Iteration 600 (61.7591 iter/s, 1.61919s/100 iter), loss = 0.00105794
I0814 18:58:10.761828   613 solver.cpp:334]     Train net output #0: loss = 0.00105794 (* 1 = 0.00105794 loss)
I0814 18:58:10.761834   613 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0814 18:58:12.366839   613 solver.cpp:312] Iteration 700 (62.3059 iter/s, 1.60498s/100 iter), loss = 0.000622428
I0814 18:58:12.366897   613 solver.cpp:334]     Train net output #0: loss = 0.000622429 (* 1 = 0.000622429 loss)
I0814 18:58:12.366922   613 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0814 18:58:13.231541   592 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 18:58:13.976851   613 solver.cpp:312] Iteration 800 (62.1132 iter/s, 1.60996s/100 iter), loss = 0.00091661
I0814 18:58:13.976914   613 solver.cpp:334]     Train net output #0: loss = 0.000916611 (* 1 = 0.000916611 loss)
I0814 18:58:13.976933   613 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0814 18:58:15.602308   613 solver.cpp:312] Iteration 900 (61.523 iter/s, 1.62541s/100 iter), loss = 0.000823716
I0814 18:58:15.602355   613 solver.cpp:334]     Train net output #0: loss = 0.000823718 (* 1 = 0.000823718 loss)
I0814 18:58:15.602366   613 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0814 18:58:17.208395   613 solver.cpp:509] Iteration 1000, Testing net (#0)
I0814 18:58:18.045778   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914413
I0814 18:58:18.045796   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 18:58:18.045804   613 solver.cpp:594]     Test net output #2: loss = 0.307134 (* 1 = 0.307134 loss)
I0814 18:58:18.045822   613 solver.cpp:264] [MultiGPU] Tests completed in 0.837406s
I0814 18:58:18.061470   613 solver.cpp:312] Iteration 1000 (40.6654 iter/s, 2.45909s/100 iter), loss = 0.00108904
I0814 18:58:18.061487   613 solver.cpp:334]     Train net output #0: loss = 0.00108904 (* 1 = 0.00108904 loss)
I0814 18:58:18.061493   613 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0814 18:58:19.709295   613 solver.cpp:312] Iteration 1100 (60.6879 iter/s, 1.64777s/100 iter), loss = 0.000795235
I0814 18:58:19.709342   613 solver.cpp:334]     Train net output #0: loss = 0.000795236 (* 1 = 0.000795236 loss)
I0814 18:58:19.709354   613 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0814 18:58:21.293243   613 solver.cpp:312] Iteration 1200 (63.1353 iter/s, 1.5839s/100 iter), loss = 0.00101821
I0814 18:58:21.293269   613 solver.cpp:334]     Train net output #0: loss = 0.00101821 (* 1 = 0.00101821 loss)
I0814 18:58:21.293275   613 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0814 18:58:22.885113   613 solver.cpp:312] Iteration 1300 (62.8211 iter/s, 1.59182s/100 iter), loss = 0.00055598
I0814 18:58:22.885295   613 solver.cpp:334]     Train net output #0: loss = 0.000555982 (* 1 = 0.000555982 loss)
I0814 18:58:22.885390   613 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0814 18:58:24.511551   613 solver.cpp:312] Iteration 1400 (61.4859 iter/s, 1.62639s/100 iter), loss = 0.00180588
I0814 18:58:24.511597   613 solver.cpp:334]     Train net output #0: loss = 0.00180588 (* 1 = 0.00180588 loss)
I0814 18:58:24.511610   613 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0814 18:58:26.133632   613 solver.cpp:312] Iteration 1500 (61.6511 iter/s, 1.62203s/100 iter), loss = 0.00105548
I0814 18:58:26.133657   613 solver.cpp:334]     Train net output #0: loss = 0.00105548 (* 1 = 0.00105548 loss)
I0814 18:58:26.133662   613 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0814 18:58:27.763067   613 solver.cpp:312] Iteration 1600 (61.3728 iter/s, 1.62939s/100 iter), loss = 0.000856867
I0814 18:58:27.763092   613 solver.cpp:334]     Train net output #0: loss = 0.000856868 (* 1 = 0.000856868 loss)
I0814 18:58:27.763098   613 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0814 18:58:29.404562   613 solver.cpp:312] Iteration 1700 (60.9219 iter/s, 1.64145s/100 iter), loss = 0.000658037
I0814 18:58:29.404639   613 solver.cpp:334]     Train net output #0: loss = 0.000658039 (* 1 = 0.000658039 loss)
I0814 18:58:29.404651   613 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0814 18:58:30.991829   613 solver.cpp:312] Iteration 1800 (63.0033 iter/s, 1.58722s/100 iter), loss = 0.00117118
I0814 18:58:30.991876   613 solver.cpp:334]     Train net output #0: loss = 0.00117118 (* 1 = 0.00117118 loss)
I0814 18:58:30.991888   613 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0814 18:58:32.586118   613 solver.cpp:312] Iteration 1900 (62.7258 iter/s, 1.59424s/100 iter), loss = 0.00158526
I0814 18:58:32.586168   613 solver.cpp:334]     Train net output #0: loss = 0.00158526 (* 1 = 0.00158526 loss)
I0814 18:58:32.586180   613 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0814 18:58:34.226788   613 solver.cpp:509] Iteration 2000, Testing net (#0)
I0814 18:58:35.040240   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91206
I0814 18:58:35.040256   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 18:58:35.040263   613 solver.cpp:594]     Test net output #2: loss = 0.306596 (* 1 = 0.306596 loss)
I0814 18:58:35.040282   613 solver.cpp:264] [MultiGPU] Tests completed in 0.813471s
I0814 18:58:35.055939   613 solver.cpp:312] Iteration 2000 (40.4899 iter/s, 2.46975s/100 iter), loss = 0.000787644
I0814 18:58:35.055956   613 solver.cpp:334]     Train net output #0: loss = 0.000787645 (* 1 = 0.000787645 loss)
I0814 18:58:35.055963   613 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0814 18:58:36.668682   613 solver.cpp:312] Iteration 2100 (62.0081 iter/s, 1.61269s/100 iter), loss = 0.00090322
I0814 18:58:36.668706   613 solver.cpp:334]     Train net output #0: loss = 0.000903221 (* 1 = 0.000903221 loss)
I0814 18:58:36.668711   613 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0814 18:58:38.286005   613 solver.cpp:312] Iteration 2200 (61.8324 iter/s, 1.61727s/100 iter), loss = 0.000534815
I0814 18:58:38.286069   613 solver.cpp:334]     Train net output #0: loss = 0.000534816 (* 1 = 0.000534816 loss)
I0814 18:58:38.286089   613 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0814 18:58:39.902894   613 solver.cpp:312] Iteration 2300 (61.849 iter/s, 1.61684s/100 iter), loss = 0.00118112
I0814 18:58:39.902920   613 solver.cpp:334]     Train net output #0: loss = 0.00118112 (* 1 = 0.00118112 loss)
I0814 18:58:39.902928   613 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0814 18:58:41.535922   613 solver.cpp:312] Iteration 2400 (61.238 iter/s, 1.63297s/100 iter), loss = 0.00106256
I0814 18:58:41.536255   613 solver.cpp:334]     Train net output #0: loss = 0.00106256 (* 1 = 0.00106256 loss)
I0814 18:58:41.536273   613 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0814 18:58:43.173405   613 solver.cpp:312] Iteration 2500 (61.0712 iter/s, 1.63743s/100 iter), loss = 0.000910575
I0814 18:58:43.173430   613 solver.cpp:334]     Train net output #0: loss = 0.000910576 (* 1 = 0.000910576 loss)
I0814 18:58:43.173436   613 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0814 18:58:44.829327   613 solver.cpp:312] Iteration 2600 (60.3911 iter/s, 1.65587s/100 iter), loss = 0.00105894
I0814 18:58:44.829352   613 solver.cpp:334]     Train net output #0: loss = 0.00105894 (* 1 = 0.00105894 loss)
I0814 18:58:44.829358   613 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0814 18:58:46.421227   613 solver.cpp:312] Iteration 2700 (62.82 iter/s, 1.59185s/100 iter), loss = 0.000250533
I0814 18:58:46.421291   613 solver.cpp:334]     Train net output #0: loss = 0.000250535 (* 1 = 0.000250535 loss)
I0814 18:58:46.421310   613 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0814 18:58:48.067821   613 solver.cpp:312] Iteration 2800 (60.7332 iter/s, 1.64655s/100 iter), loss = 0.00105618
I0814 18:58:48.067845   613 solver.cpp:334]     Train net output #0: loss = 0.00105618 (* 1 = 0.00105618 loss)
I0814 18:58:48.067850   613 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0814 18:58:49.724416   613 solver.cpp:312] Iteration 2900 (60.3667 iter/s, 1.65654s/100 iter), loss = 0.000324261
I0814 18:58:49.724474   613 solver.cpp:334]     Train net output #0: loss = 0.000324263 (* 1 = 0.000324263 loss)
I0814 18:58:49.724493   613 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0814 18:58:51.330724   613 solver.cpp:509] Iteration 3000, Testing net (#0)
I0814 18:58:52.141471   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914119
I0814 18:58:52.141490   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 18:58:52.141497   613 solver.cpp:594]     Test net output #2: loss = 0.301904 (* 1 = 0.301904 loss)
I0814 18:58:52.141515   613 solver.cpp:264] [MultiGPU] Tests completed in 0.810769s
I0814 18:58:52.157130   613 solver.cpp:312] Iteration 3000 (41.1075 iter/s, 2.43265s/100 iter), loss = 0.000712715
I0814 18:58:52.157160   613 solver.cpp:334]     Train net output #0: loss = 0.000712717 (* 1 = 0.000712717 loss)
I0814 18:58:52.157172   613 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0814 18:58:53.792516   613 solver.cpp:312] Iteration 3100 (61.1496 iter/s, 1.63533s/100 iter), loss = 0.000373582
I0814 18:58:53.792542   613 solver.cpp:334]     Train net output #0: loss = 0.000373585 (* 1 = 0.000373585 loss)
I0814 18:58:53.792548   613 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0814 18:58:55.432726   613 solver.cpp:312] Iteration 3200 (60.9697 iter/s, 1.64016s/100 iter), loss = 0.000884733
I0814 18:58:55.432754   613 solver.cpp:334]     Train net output #0: loss = 0.000884735 (* 1 = 0.000884735 loss)
I0814 18:58:55.432761   613 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0814 18:58:57.039791   613 solver.cpp:312] Iteration 3300 (62.2272 iter/s, 1.60702s/100 iter), loss = 0.000937286
I0814 18:58:57.039841   613 solver.cpp:334]     Train net output #0: loss = 0.000937289 (* 1 = 0.000937289 loss)
I0814 18:58:57.039855   613 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0814 18:58:58.696647   613 solver.cpp:312] Iteration 3400 (60.357 iter/s, 1.65681s/100 iter), loss = 0.000920437
I0814 18:58:58.696696   613 solver.cpp:334]     Train net output #0: loss = 0.000920439 (* 1 = 0.000920439 loss)
I0814 18:58:58.696710   613 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0814 18:59:00.363884   613 solver.cpp:312] Iteration 3500 (59.9813 iter/s, 1.66719s/100 iter), loss = 0.00241712
I0814 18:59:00.363986   613 solver.cpp:334]     Train net output #0: loss = 0.00241713 (* 1 = 0.00241713 loss)
I0814 18:59:00.363992   613 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0814 18:59:02.020522   613 solver.cpp:312] Iteration 3600 (60.3651 iter/s, 1.65659s/100 iter), loss = 0.00146104
I0814 18:59:02.020550   613 solver.cpp:334]     Train net output #0: loss = 0.00146105 (* 1 = 0.00146105 loss)
I0814 18:59:02.020555   613 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0814 18:59:03.623626   613 solver.cpp:312] Iteration 3700 (62.3809 iter/s, 1.60305s/100 iter), loss = 0.00051913
I0814 18:59:03.623693   613 solver.cpp:334]     Train net output #0: loss = 0.000519133 (* 1 = 0.000519133 loss)
I0814 18:59:03.623715   613 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0814 18:59:05.234933   613 solver.cpp:312] Iteration 3800 (62.0632 iter/s, 1.61126s/100 iter), loss = 0.00115885
I0814 18:59:05.234958   613 solver.cpp:334]     Train net output #0: loss = 0.00115885 (* 1 = 0.00115885 loss)
I0814 18:59:05.234964   613 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0814 18:59:06.874492   613 solver.cpp:312] Iteration 3900 (60.9939 iter/s, 1.63951s/100 iter), loss = 0.00281817
I0814 18:59:06.874517   613 solver.cpp:334]     Train net output #0: loss = 0.00281817 (* 1 = 0.00281817 loss)
I0814 18:59:06.874523   613 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0814 18:59:08.492553   613 solver.cpp:509] Iteration 4000, Testing net (#0)
I0814 18:59:09.310694   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919118
I0814 18:59:09.310714   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 18:59:09.310719   613 solver.cpp:594]     Test net output #2: loss = 0.285206 (* 1 = 0.285206 loss)
I0814 18:59:09.310732   613 solver.cpp:264] [MultiGPU] Tests completed in 0.81816s
I0814 18:59:09.332589   613 solver.cpp:312] Iteration 4000 (40.683 iter/s, 2.45803s/100 iter), loss = 0.000766689
I0814 18:59:09.332607   613 solver.cpp:334]     Train net output #0: loss = 0.000766691 (* 1 = 0.000766691 loss)
I0814 18:59:09.332612   613 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0814 18:59:10.934617   613 solver.cpp:312] Iteration 4100 (62.4228 iter/s, 1.60198s/100 iter), loss = 0.000855702
I0814 18:59:10.934682   613 solver.cpp:334]     Train net output #0: loss = 0.000855704 (* 1 = 0.000855704 loss)
I0814 18:59:10.934701   613 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0814 18:59:12.590566   613 solver.cpp:312] Iteration 4200 (60.3902 iter/s, 1.6559s/100 iter), loss = 0.000800785
I0814 18:59:12.590613   613 solver.cpp:334]     Train net output #0: loss = 0.000800788 (* 1 = 0.000800788 loss)
I0814 18:59:12.590626   613 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0814 18:59:14.180943   613 solver.cpp:312] Iteration 4300 (62.8801 iter/s, 1.59033s/100 iter), loss = 0.00231759
I0814 18:59:14.180969   613 solver.cpp:334]     Train net output #0: loss = 0.00231759 (* 1 = 0.00231759 loss)
I0814 18:59:14.180974   613 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0814 18:59:15.820266   613 solver.cpp:312] Iteration 4400 (61.0027 iter/s, 1.63927s/100 iter), loss = 0.000542646
I0814 18:59:15.820333   613 solver.cpp:334]     Train net output #0: loss = 0.000542649 (* 1 = 0.000542649 loss)
I0814 18:59:15.820353   613 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0814 18:59:17.466584   613 solver.cpp:312] Iteration 4500 (60.7435 iter/s, 1.64627s/100 iter), loss = 0.000854967
I0814 18:59:17.466631   613 solver.cpp:334]     Train net output #0: loss = 0.00085497 (* 1 = 0.00085497 loss)
I0814 18:59:17.466644   613 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0814 18:59:19.120606   613 solver.cpp:312] Iteration 4600 (60.4604 iter/s, 1.65398s/100 iter), loss = 0.000815887
I0814 18:59:19.120631   613 solver.cpp:334]     Train net output #0: loss = 0.00081589 (* 1 = 0.00081589 loss)
I0814 18:59:19.120637   613 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0814 18:59:20.732702   613 solver.cpp:312] Iteration 4700 (62.033 iter/s, 1.61205s/100 iter), loss = 0.00404256
I0814 18:59:20.732725   613 solver.cpp:334]     Train net output #0: loss = 0.00404256 (* 1 = 0.00404256 loss)
I0814 18:59:20.732729   613 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0814 18:59:22.367946   613 solver.cpp:312] Iteration 4800 (61.1548 iter/s, 1.6352s/100 iter), loss = 0.00112103
I0814 18:59:22.368017   613 solver.cpp:334]     Train net output #0: loss = 0.00112103 (* 1 = 0.00112103 loss)
I0814 18:59:22.368037   613 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0814 18:59:23.989509   613 solver.cpp:312] Iteration 4900 (61.6709 iter/s, 1.62151s/100 iter), loss = 0.00161254
I0814 18:59:23.989533   613 solver.cpp:334]     Train net output #0: loss = 0.00161254 (* 1 = 0.00161254 loss)
I0814 18:59:23.989539   613 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0814 18:59:25.600283   613 solver.cpp:509] Iteration 5000, Testing net (#0)
I0814 18:59:26.272680   611 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 18:59:26.409451   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.925295
I0814 18:59:26.409471   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 18:59:26.409476   613 solver.cpp:594]     Test net output #2: loss = 0.258845 (* 1 = 0.258845 loss)
I0814 18:59:26.409490   613 solver.cpp:264] [MultiGPU] Tests completed in 0.809186s
I0814 18:59:26.425143   613 solver.cpp:312] Iteration 5000 (41.0582 iter/s, 2.43557s/100 iter), loss = 0.00130248
I0814 18:59:26.425158   613 solver.cpp:334]     Train net output #0: loss = 0.00130249 (* 1 = 0.00130249 loss)
I0814 18:59:26.425163   613 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0814 18:59:28.022670   613 solver.cpp:312] Iteration 5100 (62.5987 iter/s, 1.59748s/100 iter), loss = 0.00145321
I0814 18:59:28.022734   613 solver.cpp:334]     Train net output #0: loss = 0.00145322 (* 1 = 0.00145322 loss)
I0814 18:59:28.022755   613 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0814 18:59:29.635462   613 solver.cpp:312] Iteration 5200 (62.0063 iter/s, 1.61274s/100 iter), loss = 0.00149784
I0814 18:59:29.635489   613 solver.cpp:334]     Train net output #0: loss = 0.00149785 (* 1 = 0.00149785 loss)
I0814 18:59:29.635496   613 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0814 18:59:31.265836   613 solver.cpp:312] Iteration 5300 (61.3374 iter/s, 1.63033s/100 iter), loss = 0.00119783
I0814 18:59:31.265918   613 solver.cpp:334]     Train net output #0: loss = 0.00119784 (* 1 = 0.00119784 loss)
I0814 18:59:31.265924   613 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0814 18:59:32.920838   613 solver.cpp:312] Iteration 5400 (60.4247 iter/s, 1.65495s/100 iter), loss = 0.000558035
I0814 18:59:32.920897   613 solver.cpp:334]     Train net output #0: loss = 0.000558037 (* 1 = 0.000558037 loss)
I0814 18:59:32.920917   613 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0814 18:59:34.554991   613 solver.cpp:312] Iteration 5500 (61.1958 iter/s, 1.6341s/100 iter), loss = 0.000801629
I0814 18:59:34.555038   613 solver.cpp:334]     Train net output #0: loss = 0.000801631 (* 1 = 0.000801631 loss)
I0814 18:59:34.555053   613 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0814 18:59:36.168826   613 solver.cpp:312] Iteration 5600 (61.9662 iter/s, 1.61378s/100 iter), loss = 0.000841402
I0814 18:59:36.168850   613 solver.cpp:334]     Train net output #0: loss = 0.000841404 (* 1 = 0.000841404 loss)
I0814 18:59:36.168854   613 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0814 18:59:37.786312   613 solver.cpp:312] Iteration 5700 (61.8261 iter/s, 1.61744s/100 iter), loss = 0.000567414
I0814 18:59:37.786337   613 solver.cpp:334]     Train net output #0: loss = 0.000567416 (* 1 = 0.000567416 loss)
I0814 18:59:37.786342   613 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0814 18:59:39.411339   613 solver.cpp:312] Iteration 5800 (61.5394 iter/s, 1.62498s/100 iter), loss = 0.000803597
I0814 18:59:39.411363   613 solver.cpp:334]     Train net output #0: loss = 0.0008036 (* 1 = 0.0008036 loss)
I0814 18:59:39.411370   613 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0814 18:59:41.038350   613 solver.cpp:312] Iteration 5900 (61.4641 iter/s, 1.62697s/100 iter), loss = 0.00183941
I0814 18:59:41.038375   613 solver.cpp:334]     Train net output #0: loss = 0.00183941 (* 1 = 0.00183941 loss)
I0814 18:59:41.038381   613 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0814 18:59:42.631839   613 solver.cpp:509] Iteration 6000, Testing net (#0)
I0814 18:59:43.448930   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.928236
I0814 18:59:43.448949   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 18:59:43.448954   613 solver.cpp:594]     Test net output #2: loss = 0.25561 (* 1 = 0.25561 loss)
I0814 18:59:43.448971   613 solver.cpp:264] [MultiGPU] Tests completed in 0.81711s
I0814 18:59:43.464515   613 solver.cpp:312] Iteration 6000 (41.2185 iter/s, 2.42609s/100 iter), loss = 0.000732102
I0814 18:59:43.464534   613 solver.cpp:334]     Train net output #0: loss = 0.000732104 (* 1 = 0.000732104 loss)
I0814 18:59:43.464540   613 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0814 18:59:45.100119   613 solver.cpp:312] Iteration 6100 (61.1413 iter/s, 1.63555s/100 iter), loss = 0.00225511
I0814 18:59:45.100172   613 solver.cpp:334]     Train net output #0: loss = 0.00225511 (* 1 = 0.00225511 loss)
I0814 18:59:45.100184   613 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0814 18:59:46.694612   613 solver.cpp:312] Iteration 6200 (62.7177 iter/s, 1.59445s/100 iter), loss = 0.000762816
I0814 18:59:46.694636   613 solver.cpp:334]     Train net output #0: loss = 0.000762818 (* 1 = 0.000762818 loss)
I0814 18:59:46.694641   613 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0814 18:59:48.315784   613 solver.cpp:312] Iteration 6300 (61.6858 iter/s, 1.62112s/100 iter), loss = 0.000955307
I0814 18:59:48.315834   613 solver.cpp:334]     Train net output #0: loss = 0.000955309 (* 1 = 0.000955309 loss)
I0814 18:59:48.315855   613 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0814 18:59:49.928176   613 solver.cpp:312] Iteration 6400 (62.0214 iter/s, 1.61235s/100 iter), loss = 0.00260718
I0814 18:59:49.928237   613 solver.cpp:334]     Train net output #0: loss = 0.00260719 (* 1 = 0.00260719 loss)
I0814 18:59:49.928256   613 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0814 18:59:51.541084   613 solver.cpp:312] Iteration 6500 (62.0019 iter/s, 1.61285s/100 iter), loss = 0.00083582
I0814 18:59:51.541108   613 solver.cpp:334]     Train net output #0: loss = 0.000835823 (* 1 = 0.000835823 loss)
I0814 18:59:51.541115   613 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0814 18:59:53.110280   613 solver.cpp:312] Iteration 6600 (63.7289 iter/s, 1.56915s/100 iter), loss = 0.00268273
I0814 18:59:53.110340   613 solver.cpp:334]     Train net output #0: loss = 0.00268274 (* 1 = 0.00268274 loss)
I0814 18:59:53.110359   613 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0814 18:59:54.720000   613 solver.cpp:312] Iteration 6700 (62.1244 iter/s, 1.60967s/100 iter), loss = 0.00100576
I0814 18:59:54.720043   613 solver.cpp:334]     Train net output #0: loss = 0.00100577 (* 1 = 0.00100577 loss)
I0814 18:59:54.720054   613 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0814 18:59:56.327491   613 solver.cpp:312] Iteration 6800 (62.2107 iter/s, 1.60744s/100 iter), loss = 0.00122988
I0814 18:59:56.327515   613 solver.cpp:334]     Train net output #0: loss = 0.00122988 (* 1 = 0.00122988 loss)
I0814 18:59:56.327522   613 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0814 18:59:57.915350   613 solver.cpp:312] Iteration 6900 (62.9797 iter/s, 1.58781s/100 iter), loss = 0.000633278
I0814 18:59:57.915411   613 solver.cpp:334]     Train net output #0: loss = 0.000633281 (* 1 = 0.000633281 loss)
I0814 18:59:57.915429   613 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0814 18:59:59.496975   613 solver.cpp:509] Iteration 7000, Testing net (#0)
I0814 19:00:00.318768   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.92853
I0814 19:00:00.318786   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:00:00.318794   613 solver.cpp:594]     Test net output #2: loss = 0.259493 (* 1 = 0.259493 loss)
I0814 19:00:00.318811   613 solver.cpp:264] [MultiGPU] Tests completed in 0.821812s
I0814 19:00:00.334638   613 solver.cpp:312] Iteration 7000 (41.3356 iter/s, 2.41922s/100 iter), loss = 0.00120422
I0814 19:00:00.334657   613 solver.cpp:334]     Train net output #0: loss = 0.00120423 (* 1 = 0.00120423 loss)
I0814 19:00:00.334662   613 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0814 19:00:01.932751   613 solver.cpp:312] Iteration 7100 (62.5758 iter/s, 1.59806s/100 iter), loss = 0.000656834
I0814 19:00:01.932831   613 solver.cpp:334]     Train net output #0: loss = 0.000656837 (* 1 = 0.000656837 loss)
I0814 19:00:01.932838   613 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0814 19:00:03.559361   613 solver.cpp:312] Iteration 7200 (61.4794 iter/s, 1.62656s/100 iter), loss = 0.00135278
I0814 19:00:03.559384   613 solver.cpp:334]     Train net output #0: loss = 0.00135279 (* 1 = 0.00135279 loss)
I0814 19:00:03.559391   613 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0814 19:00:05.222133   613 solver.cpp:312] Iteration 7300 (60.1424 iter/s, 1.66272s/100 iter), loss = 0.00144445
I0814 19:00:05.222182   613 solver.cpp:334]     Train net output #0: loss = 0.00144445 (* 1 = 0.00144445 loss)
I0814 19:00:05.222193   613 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0814 19:00:06.819478   613 solver.cpp:312] Iteration 7400 (62.6059 iter/s, 1.59729s/100 iter), loss = 0.000636561
I0814 19:00:06.819524   613 solver.cpp:334]     Train net output #0: loss = 0.000636564 (* 1 = 0.000636564 loss)
I0814 19:00:06.819536   613 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0814 19:00:08.423743   613 solver.cpp:312] Iteration 7500 (62.3357 iter/s, 1.60422s/100 iter), loss = 0.0004935
I0814 19:00:08.423810   613 solver.cpp:334]     Train net output #0: loss = 0.000493504 (* 1 = 0.000493504 loss)
I0814 19:00:08.423828   613 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0814 19:00:10.040758   613 solver.cpp:312] Iteration 7600 (61.8443 iter/s, 1.61696s/100 iter), loss = 0.00115649
I0814 19:00:10.040807   613 solver.cpp:334]     Train net output #0: loss = 0.00115649 (* 1 = 0.00115649 loss)
I0814 19:00:10.040946   613 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0814 19:00:11.665786   613 solver.cpp:312] Iteration 7700 (61.5393 iter/s, 1.62498s/100 iter), loss = 0.000344601
I0814 19:00:11.665845   613 solver.cpp:334]     Train net output #0: loss = 0.000344605 (* 1 = 0.000344605 loss)
I0814 19:00:11.665863   613 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0814 19:00:13.334347   613 solver.cpp:312] Iteration 7800 (59.9337 iter/s, 1.66851s/100 iter), loss = 0.00147152
I0814 19:00:13.334391   613 solver.cpp:334]     Train net output #0: loss = 0.00147152 (* 1 = 0.00147152 loss)
I0814 19:00:13.334403   613 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0814 19:00:14.916340   613 solver.cpp:312] Iteration 7900 (63.2133 iter/s, 1.58195s/100 iter), loss = 0.00125078
I0814 19:00:14.916368   613 solver.cpp:334]     Train net output #0: loss = 0.00125079 (* 1 = 0.00125079 loss)
I0814 19:00:14.916375   613 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0814 19:00:16.519829   613 solver.cpp:509] Iteration 8000, Testing net (#0)
I0814 19:00:17.332904   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920001
I0814 19:00:17.332924   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:00:17.332929   613 solver.cpp:594]     Test net output #2: loss = 0.280132 (* 1 = 0.280132 loss)
I0814 19:00:17.332947   613 solver.cpp:264] [MultiGPU] Tests completed in 0.813096s
I0814 19:00:17.348517   613 solver.cpp:312] Iteration 8000 (41.1166 iter/s, 2.43211s/100 iter), loss = 0.00241033
I0814 19:00:17.348536   613 solver.cpp:334]     Train net output #0: loss = 0.00241033 (* 1 = 0.00241033 loss)
I0814 19:00:17.348543   613 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0814 19:00:18.940136   613 solver.cpp:312] Iteration 8100 (62.8313 iter/s, 1.59156s/100 iter), loss = 0.00254658
I0814 19:00:18.940182   613 solver.cpp:334]     Train net output #0: loss = 0.00254658 (* 1 = 0.00254658 loss)
I0814 19:00:18.940193   613 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0814 19:00:20.604782   613 solver.cpp:312] Iteration 8200 (60.0746 iter/s, 1.6646s/100 iter), loss = 0.000689749
I0814 19:00:20.604938   613 solver.cpp:334]     Train net output #0: loss = 0.000689751 (* 1 = 0.000689751 loss)
I0814 19:00:20.605029   613 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0814 19:00:22.213068   613 solver.cpp:312] Iteration 8300 (62.1799 iter/s, 1.60824s/100 iter), loss = 0.00152416
I0814 19:00:22.213093   613 solver.cpp:334]     Train net output #0: loss = 0.00152416 (* 1 = 0.00152416 loss)
I0814 19:00:22.213101   613 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0814 19:00:23.856964   613 solver.cpp:312] Iteration 8400 (60.8329 iter/s, 1.64385s/100 iter), loss = 0.00169297
I0814 19:00:23.856988   613 solver.cpp:334]     Train net output #0: loss = 0.00169297 (* 1 = 0.00169297 loss)
I0814 19:00:23.856994   613 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0814 19:00:25.446161   613 solver.cpp:312] Iteration 8500 (62.9268 iter/s, 1.58915s/100 iter), loss = 0.00153697
I0814 19:00:25.446188   613 solver.cpp:334]     Train net output #0: loss = 0.00153697 (* 1 = 0.00153697 loss)
I0814 19:00:25.446194   613 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0814 19:00:27.056836   613 solver.cpp:312] Iteration 8600 (62.0877 iter/s, 1.61063s/100 iter), loss = 0.000643072
I0814 19:00:27.056860   613 solver.cpp:334]     Train net output #0: loss = 0.000643074 (* 1 = 0.000643074 loss)
I0814 19:00:27.056865   613 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0814 19:00:28.691506   613 solver.cpp:312] Iteration 8700 (61.1762 iter/s, 1.63462s/100 iter), loss = 0.00097274
I0814 19:00:28.691534   613 solver.cpp:334]     Train net output #0: loss = 0.000972742 (* 1 = 0.000972742 loss)
I0814 19:00:28.691540   613 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0814 19:00:30.338904   613 solver.cpp:312] Iteration 8800 (60.7037 iter/s, 1.64735s/100 iter), loss = 0.002728
I0814 19:00:30.338929   613 solver.cpp:334]     Train net output #0: loss = 0.002728 (* 1 = 0.002728 loss)
I0814 19:00:30.338935   613 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0814 19:00:31.950960   613 solver.cpp:312] Iteration 8900 (62.0345 iter/s, 1.61201s/100 iter), loss = 0.0010851
I0814 19:00:31.951051   613 solver.cpp:334]     Train net output #0: loss = 0.00108511 (* 1 = 0.00108511 loss)
I0814 19:00:31.951068   613 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0814 19:00:33.545852   613 solver.cpp:509] Iteration 9000, Testing net (#0)
I0814 19:00:34.355777   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920001
I0814 19:00:34.355795   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:00:34.355799   613 solver.cpp:594]     Test net output #2: loss = 0.275584 (* 1 = 0.275584 loss)
I0814 19:00:34.355814   613 solver.cpp:264] [MultiGPU] Tests completed in 0.809941s
I0814 19:00:34.371256   613 solver.cpp:312] Iteration 9000 (41.3184 iter/s, 2.42023s/100 iter), loss = 0.0010081
I0814 19:00:34.371274   613 solver.cpp:334]     Train net output #0: loss = 0.0010081 (* 1 = 0.0010081 loss)
I0814 19:00:34.371279   613 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0814 19:00:35.772261   592 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:00:35.974222   613 solver.cpp:312] Iteration 9100 (62.3864 iter/s, 1.60291s/100 iter), loss = 0.00175818
I0814 19:00:35.974268   613 solver.cpp:334]     Train net output #0: loss = 0.00175818 (* 1 = 0.00175818 loss)
I0814 19:00:35.974280   613 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0814 19:00:37.564599   613 solver.cpp:312] Iteration 9200 (62.8801 iter/s, 1.59033s/100 iter), loss = 0.00146192
I0814 19:00:37.564652   613 solver.cpp:334]     Train net output #0: loss = 0.00146192 (* 1 = 0.00146192 loss)
I0814 19:00:37.564666   613 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0814 19:00:39.182272   613 solver.cpp:312] Iteration 9300 (61.8191 iter/s, 1.61762s/100 iter), loss = 0.000442209
I0814 19:00:39.182297   613 solver.cpp:334]     Train net output #0: loss = 0.00044221 (* 1 = 0.00044221 loss)
I0814 19:00:39.182303   613 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0814 19:00:40.806073   613 solver.cpp:312] Iteration 9400 (61.5858 iter/s, 1.62375s/100 iter), loss = 0.00222224
I0814 19:00:40.806100   613 solver.cpp:334]     Train net output #0: loss = 0.00222224 (* 1 = 0.00222224 loss)
I0814 19:00:40.806107   613 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0814 19:00:42.422749   613 solver.cpp:312] Iteration 9500 (61.8572 iter/s, 1.61663s/100 iter), loss = 0.000476131
I0814 19:00:42.422775   613 solver.cpp:334]     Train net output #0: loss = 0.000476132 (* 1 = 0.000476132 loss)
I0814 19:00:42.422781   613 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0814 19:00:44.072320   613 solver.cpp:312] Iteration 9600 (60.6237 iter/s, 1.64952s/100 iter), loss = 0.00124401
I0814 19:00:44.072347   613 solver.cpp:334]     Train net output #0: loss = 0.00124401 (* 1 = 0.00124401 loss)
I0814 19:00:44.072355   613 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0814 19:00:45.709556   613 solver.cpp:312] Iteration 9700 (61.0804 iter/s, 1.63719s/100 iter), loss = 0.00352153
I0814 19:00:45.709602   613 solver.cpp:334]     Train net output #0: loss = 0.00352153 (* 1 = 0.00352153 loss)
I0814 19:00:45.709614   613 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0814 19:00:47.320477   613 solver.cpp:312] Iteration 9800 (62.0782 iter/s, 1.61087s/100 iter), loss = 0.00116342
I0814 19:00:47.320502   613 solver.cpp:334]     Train net output #0: loss = 0.00116342 (* 1 = 0.00116342 loss)
I0814 19:00:47.320508   613 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0814 19:00:48.951493   613 solver.cpp:312] Iteration 9900 (61.3133 iter/s, 1.63097s/100 iter), loss = 0.000878821
I0814 19:00:48.951557   613 solver.cpp:334]     Train net output #0: loss = 0.000878822 (* 1 = 0.000878822 loss)
I0814 19:00:48.951576   613 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0814 19:00:50.558339   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_10000.caffemodel
I0814 19:00:50.573385   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_10000.solverstate
I0814 19:00:50.579520   613 solver.cpp:509] Iteration 10000, Testing net (#0)
I0814 19:00:51.388346   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920295
I0814 19:00:51.388365   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:00:51.388370   613 solver.cpp:594]     Test net output #2: loss = 0.28255 (* 1 = 0.28255 loss)
I0814 19:00:51.388386   613 solver.cpp:264] [MultiGPU] Tests completed in 0.808842s
I0814 19:00:51.407680   613 solver.cpp:312] Iteration 10000 (40.7147 iter/s, 2.45612s/100 iter), loss = 0.0023508
I0814 19:00:51.407698   613 solver.cpp:334]     Train net output #0: loss = 0.0023508 (* 1 = 0.0023508 loss)
I0814 19:00:51.407704   613 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0814 19:00:53.005925   613 solver.cpp:312] Iteration 10100 (62.5706 iter/s, 1.59819s/100 iter), loss = 0.00207654
I0814 19:00:53.005949   613 solver.cpp:334]     Train net output #0: loss = 0.00207654 (* 1 = 0.00207654 loss)
I0814 19:00:53.005954   613 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0814 19:00:54.655638   613 solver.cpp:312] Iteration 10200 (60.6184 iter/s, 1.64966s/100 iter), loss = 0.00217249
I0814 19:00:54.655663   613 solver.cpp:334]     Train net output #0: loss = 0.00217249 (* 1 = 0.00217249 loss)
I0814 19:00:54.655668   613 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0814 19:00:56.284055   613 solver.cpp:312] Iteration 10300 (61.4113 iter/s, 1.62837s/100 iter), loss = 0.00213185
I0814 19:00:56.284126   613 solver.cpp:334]     Train net output #0: loss = 0.00213185 (* 1 = 0.00213185 loss)
I0814 19:00:56.284155   613 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0814 19:00:57.873436   613 solver.cpp:312] Iteration 10400 (62.9194 iter/s, 1.58934s/100 iter), loss = 0.00123549
I0814 19:00:57.873503   613 solver.cpp:334]     Train net output #0: loss = 0.00123549 (* 1 = 0.00123549 loss)
I0814 19:00:57.873523   613 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0814 19:00:59.523696   613 solver.cpp:312] Iteration 10500 (60.5984 iter/s, 1.65021s/100 iter), loss = 0.00176174
I0814 19:00:59.523721   613 solver.cpp:334]     Train net output #0: loss = 0.00176174 (* 1 = 0.00176174 loss)
I0814 19:00:59.523727   613 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0814 19:01:01.150777   613 solver.cpp:312] Iteration 10600 (61.4615 iter/s, 1.62703s/100 iter), loss = 0.00185199
I0814 19:01:01.150835   613 solver.cpp:334]     Train net output #0: loss = 0.00185199 (* 1 = 0.00185199 loss)
I0814 19:01:01.150853   613 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0814 19:01:02.766182   613 solver.cpp:312] Iteration 10700 (61.9058 iter/s, 1.61536s/100 iter), loss = 0.00171822
I0814 19:01:02.766286   613 solver.cpp:334]     Train net output #0: loss = 0.00171823 (* 1 = 0.00171823 loss)
I0814 19:01:02.766304   613 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0814 19:01:04.369999   613 solver.cpp:312] Iteration 10800 (62.3532 iter/s, 1.60377s/100 iter), loss = 0.00409551
I0814 19:01:04.370070   613 solver.cpp:334]     Train net output #0: loss = 0.00409552 (* 1 = 0.00409552 loss)
I0814 19:01:04.370101   613 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0814 19:01:05.980542   613 solver.cpp:312] Iteration 10900 (62.0928 iter/s, 1.61049s/100 iter), loss = 0.00212342
I0814 19:01:05.980568   613 solver.cpp:334]     Train net output #0: loss = 0.00212343 (* 1 = 0.00212343 loss)
I0814 19:01:05.980574   613 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0814 19:01:07.593227   613 solver.cpp:509] Iteration 11000, Testing net (#0)
I0814 19:01:08.422883   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.916177
I0814 19:01:08.422902   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:01:08.422907   613 solver.cpp:594]     Test net output #2: loss = 0.296183 (* 1 = 0.296183 loss)
I0814 19:01:08.422924   613 solver.cpp:264] [MultiGPU] Tests completed in 0.829684s
I0814 19:01:08.440649   613 solver.cpp:312] Iteration 11000 (40.6498 iter/s, 2.46004s/100 iter), loss = 0.00128561
I0814 19:01:08.440681   613 solver.cpp:334]     Train net output #0: loss = 0.00128562 (* 1 = 0.00128562 loss)
I0814 19:01:08.440696   613 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0814 19:01:10.035934   613 solver.cpp:312] Iteration 11100 (62.6867 iter/s, 1.59524s/100 iter), loss = 0.00254285
I0814 19:01:10.035981   613 solver.cpp:334]     Train net output #0: loss = 0.00254286 (* 1 = 0.00254286 loss)
I0814 19:01:10.035993   613 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0814 19:01:11.685240   613 solver.cpp:312] Iteration 11200 (60.6334 iter/s, 1.64926s/100 iter), loss = 0.000756026
I0814 19:01:11.685266   613 solver.cpp:334]     Train net output #0: loss = 0.000756032 (* 1 = 0.000756032 loss)
I0814 19:01:11.685274   613 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0814 19:01:13.280048   613 solver.cpp:312] Iteration 11300 (62.7055 iter/s, 1.59476s/100 iter), loss = 0.000932795
I0814 19:01:13.280077   613 solver.cpp:334]     Train net output #0: loss = 0.000932802 (* 1 = 0.000932802 loss)
I0814 19:01:13.280083   613 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0814 19:01:14.921511   613 solver.cpp:312] Iteration 11400 (60.9232 iter/s, 1.64141s/100 iter), loss = 0.0019065
I0814 19:01:14.921579   613 solver.cpp:334]     Train net output #0: loss = 0.00190651 (* 1 = 0.00190651 loss)
I0814 19:01:14.921599   613 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0814 19:01:16.534878   613 solver.cpp:312] Iteration 11500 (61.9841 iter/s, 1.61332s/100 iter), loss = 0.00301876
I0814 19:01:16.534940   613 solver.cpp:334]     Train net output #0: loss = 0.00301876 (* 1 = 0.00301876 loss)
I0814 19:01:16.534958   613 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0814 19:01:18.156390   613 solver.cpp:312] Iteration 11600 (61.6727 iter/s, 1.62146s/100 iter), loss = 0.00316448
I0814 19:01:18.156437   613 solver.cpp:334]     Train net output #0: loss = 0.00316449 (* 1 = 0.00316449 loss)
I0814 19:01:18.156450   613 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0814 19:01:19.756758   613 solver.cpp:312] Iteration 11700 (62.4876 iter/s, 1.60032s/100 iter), loss = 0.000728933
I0814 19:01:19.756811   613 solver.cpp:334]     Train net output #0: loss = 0.00072894 (* 1 = 0.00072894 loss)
I0814 19:01:19.756829   613 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0814 19:01:21.369840   613 solver.cpp:312] Iteration 11800 (61.995 iter/s, 1.61303s/100 iter), loss = 0.000616483
I0814 19:01:21.369894   613 solver.cpp:334]     Train net output #0: loss = 0.000616489 (* 1 = 0.000616489 loss)
I0814 19:01:21.369907   613 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0814 19:01:23.010448   613 solver.cpp:312] Iteration 11900 (60.9548 iter/s, 1.64056s/100 iter), loss = 0.000982335
I0814 19:01:23.010545   613 solver.cpp:334]     Train net output #0: loss = 0.000982341 (* 1 = 0.000982341 loss)
I0814 19:01:23.010553   613 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0814 19:01:24.632519   613 solver.cpp:509] Iteration 12000, Testing net (#0)
I0814 19:01:25.447366   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919413
I0814 19:01:25.447386   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:01:25.447391   613 solver.cpp:594]     Test net output #2: loss = 0.286347 (* 1 = 0.286347 loss)
I0814 19:01:25.447405   613 solver.cpp:264] [MultiGPU] Tests completed in 0.814867s
I0814 19:01:25.462883   613 solver.cpp:312] Iteration 12000 (40.777 iter/s, 2.45236s/100 iter), loss = 0.000947756
I0814 19:01:25.462899   613 solver.cpp:334]     Train net output #0: loss = 0.000947762 (* 1 = 0.000947762 loss)
I0814 19:01:25.462903   613 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0814 19:01:27.072866   613 solver.cpp:312] Iteration 12100 (62.1144 iter/s, 1.60993s/100 iter), loss = 0.000586014
I0814 19:01:27.072895   613 solver.cpp:334]     Train net output #0: loss = 0.00058602 (* 1 = 0.00058602 loss)
I0814 19:01:27.072901   613 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0814 19:01:28.684233   613 solver.cpp:312] Iteration 12200 (62.0611 iter/s, 1.61132s/100 iter), loss = 0.000906608
I0814 19:01:28.684259   613 solver.cpp:334]     Train net output #0: loss = 0.000906614 (* 1 = 0.000906614 loss)
I0814 19:01:28.684267   613 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0814 19:01:30.263794   613 solver.cpp:312] Iteration 12300 (63.3106 iter/s, 1.57951s/100 iter), loss = 0.00084128
I0814 19:01:30.263819   613 solver.cpp:334]     Train net output #0: loss = 0.000841287 (* 1 = 0.000841287 loss)
I0814 19:01:30.263825   613 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0814 19:01:31.923334   613 solver.cpp:312] Iteration 12400 (60.2595 iter/s, 1.65949s/100 iter), loss = 0.000783493
I0814 19:01:31.923358   613 solver.cpp:334]     Train net output #0: loss = 0.000783501 (* 1 = 0.000783501 loss)
I0814 19:01:31.923364   613 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0814 19:01:33.538316   613 solver.cpp:312] Iteration 12500 (61.922 iter/s, 1.61493s/100 iter), loss = 0.000894408
I0814 19:01:33.538401   613 solver.cpp:334]     Train net output #0: loss = 0.000894416 (* 1 = 0.000894416 loss)
I0814 19:01:33.538408   613 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0814 19:01:35.144990   613 solver.cpp:312] Iteration 12600 (62.2423 iter/s, 1.60662s/100 iter), loss = 0.00241254
I0814 19:01:35.145015   613 solver.cpp:334]     Train net output #0: loss = 0.00241255 (* 1 = 0.00241255 loss)
I0814 19:01:35.145020   613 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0814 19:01:36.746534   613 solver.cpp:312] Iteration 12700 (62.4417 iter/s, 1.6015s/100 iter), loss = 0.00268598
I0814 19:01:36.746558   613 solver.cpp:334]     Train net output #0: loss = 0.00268598 (* 1 = 0.00268598 loss)
I0814 19:01:36.746564   613 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0814 19:01:38.410475   613 solver.cpp:312] Iteration 12800 (60.1002 iter/s, 1.66389s/100 iter), loss = 0.00307565
I0814 19:01:38.410544   613 solver.cpp:334]     Train net output #0: loss = 0.00307566 (* 1 = 0.00307566 loss)
I0814 19:01:38.410567   613 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0814 19:01:40.081960   613 solver.cpp:312] Iteration 12900 (59.8288 iter/s, 1.67144s/100 iter), loss = 0.00121009
I0814 19:01:40.081989   613 solver.cpp:334]     Train net output #0: loss = 0.0012101 (* 1 = 0.0012101 loss)
I0814 19:01:40.081995   613 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0814 19:01:41.684594   613 solver.cpp:509] Iteration 13000, Testing net (#0)
I0814 19:01:42.509568   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914707
I0814 19:01:42.509588   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:01:42.509593   613 solver.cpp:594]     Test net output #2: loss = 0.300487 (* 1 = 0.300487 loss)
I0814 19:01:42.509611   613 solver.cpp:264] [MultiGPU] Tests completed in 0.824994s
I0814 19:01:42.526510   613 solver.cpp:312] Iteration 13000 (40.9085 iter/s, 2.44448s/100 iter), loss = 0.00118577
I0814 19:01:42.526530   613 solver.cpp:334]     Train net output #0: loss = 0.00118578 (* 1 = 0.00118578 loss)
I0814 19:01:42.526536   613 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0814 19:01:44.131896   613 solver.cpp:312] Iteration 13100 (62.2922 iter/s, 1.60534s/100 iter), loss = 0.00265075
I0814 19:01:44.131920   613 solver.cpp:334]     Train net output #0: loss = 0.00265076 (* 1 = 0.00265076 loss)
I0814 19:01:44.131925   613 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0814 19:01:45.766261   613 solver.cpp:312] Iteration 13200 (61.1877 iter/s, 1.63432s/100 iter), loss = 0.00332237
I0814 19:01:45.766309   613 solver.cpp:334]     Train net output #0: loss = 0.00332238 (* 1 = 0.00332238 loss)
I0814 19:01:45.766320   613 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0814 19:01:47.361692   613 solver.cpp:312] Iteration 13300 (62.681 iter/s, 1.59538s/100 iter), loss = 0.00146
I0814 19:01:47.361717   613 solver.cpp:334]     Train net output #0: loss = 0.00146001 (* 1 = 0.00146001 loss)
I0814 19:01:47.361722   613 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0814 19:01:48.973592   613 solver.cpp:312] Iteration 13400 (62.0404 iter/s, 1.61185s/100 iter), loss = 0.000782068
I0814 19:01:48.973619   613 solver.cpp:334]     Train net output #0: loss = 0.000782077 (* 1 = 0.000782077 loss)
I0814 19:01:48.973626   613 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0814 19:01:50.612776   613 solver.cpp:312] Iteration 13500 (61.0079 iter/s, 1.63913s/100 iter), loss = 0.00214357
I0814 19:01:50.612803   613 solver.cpp:334]     Train net output #0: loss = 0.00214358 (* 1 = 0.00214358 loss)
I0814 19:01:50.612810   613 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0814 19:01:52.240870   613 solver.cpp:312] Iteration 13600 (61.4235 iter/s, 1.62804s/100 iter), loss = 0.00199094
I0814 19:01:52.240900   613 solver.cpp:334]     Train net output #0: loss = 0.00199095 (* 1 = 0.00199095 loss)
I0814 19:01:52.240906   613 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0814 19:01:52.774317   592 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:01:53.828558   613 solver.cpp:312] Iteration 13700 (62.9866 iter/s, 1.58764s/100 iter), loss = 0.0016825
I0814 19:01:53.828583   613 solver.cpp:334]     Train net output #0: loss = 0.00168251 (* 1 = 0.00168251 loss)
I0814 19:01:53.828590   613 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0814 19:01:55.437085   613 solver.cpp:312] Iteration 13800 (62.1705 iter/s, 1.60848s/100 iter), loss = 0.00191537
I0814 19:01:55.437110   613 solver.cpp:334]     Train net output #0: loss = 0.00191538 (* 1 = 0.00191538 loss)
I0814 19:01:55.437116   613 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0814 19:01:57.056506   613 solver.cpp:312] Iteration 13900 (61.7523 iter/s, 1.61937s/100 iter), loss = 0.00177273
I0814 19:01:57.056532   613 solver.cpp:334]     Train net output #0: loss = 0.00177274 (* 1 = 0.00177274 loss)
I0814 19:01:57.056537   613 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0814 19:01:58.669126   613 solver.cpp:509] Iteration 14000, Testing net (#0)
I0814 19:01:59.487828   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910295
I0814 19:01:59.487848   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:01:59.487853   613 solver.cpp:594]     Test net output #2: loss = 0.322272 (* 1 = 0.322272 loss)
I0814 19:01:59.487867   613 solver.cpp:264] [MultiGPU] Tests completed in 0.818723s
I0814 19:01:59.505007   613 solver.cpp:312] Iteration 14000 (40.8424 iter/s, 2.44843s/100 iter), loss = 0.00211855
I0814 19:01:59.505201   613 solver.cpp:334]     Train net output #0: loss = 0.00211856 (* 1 = 0.00211856 loss)
I0814 19:01:59.505208   613 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0814 19:02:01.122784   613 solver.cpp:312] Iteration 14100 (61.8152 iter/s, 1.61772s/100 iter), loss = 0.00135588
I0814 19:02:01.122807   613 solver.cpp:334]     Train net output #0: loss = 0.00135589 (* 1 = 0.00135589 loss)
I0814 19:02:01.122812   613 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0814 19:02:02.722257   613 solver.cpp:312] Iteration 14200 (62.5225 iter/s, 1.59942s/100 iter), loss = 0.00100908
I0814 19:02:02.722282   613 solver.cpp:334]     Train net output #0: loss = 0.00100909 (* 1 = 0.00100909 loss)
I0814 19:02:02.722288   613 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0814 19:02:04.365736   613 solver.cpp:312] Iteration 14300 (60.8483 iter/s, 1.64343s/100 iter), loss = 0.00267836
I0814 19:02:04.365875   613 solver.cpp:334]     Train net output #0: loss = 0.00267837 (* 1 = 0.00267837 loss)
I0814 19:02:04.365885   613 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0814 19:02:06.014319   613 solver.cpp:312] Iteration 14400 (60.66 iter/s, 1.64853s/100 iter), loss = 0.00425318
I0814 19:02:06.014380   613 solver.cpp:334]     Train net output #0: loss = 0.00425319 (* 1 = 0.00425319 loss)
I0814 19:02:06.014398   613 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0814 19:02:07.665529   613 solver.cpp:312] Iteration 14500 (60.5637 iter/s, 1.65115s/100 iter), loss = 0.000804562
I0814 19:02:07.665555   613 solver.cpp:334]     Train net output #0: loss = 0.00080457 (* 1 = 0.00080457 loss)
I0814 19:02:07.665561   613 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0814 19:02:09.242305   613 solver.cpp:312] Iteration 14600 (63.4224 iter/s, 1.57673s/100 iter), loss = 0.0016726
I0814 19:02:09.242332   613 solver.cpp:334]     Train net output #0: loss = 0.00167261 (* 1 = 0.00167261 loss)
I0814 19:02:09.242338   613 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0814 19:02:10.906086   613 solver.cpp:312] Iteration 14700 (60.1059 iter/s, 1.66373s/100 iter), loss = 0.0019093
I0814 19:02:10.906134   613 solver.cpp:334]     Train net output #0: loss = 0.00190931 (* 1 = 0.00190931 loss)
I0814 19:02:10.906147   613 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0814 19:02:12.545008   613 solver.cpp:312] Iteration 14800 (61.0177 iter/s, 1.63887s/100 iter), loss = 0.00179903
I0814 19:02:12.545037   613 solver.cpp:334]     Train net output #0: loss = 0.00179904 (* 1 = 0.00179904 loss)
I0814 19:02:12.545043   613 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0814 19:02:14.181891   613 solver.cpp:312] Iteration 14900 (61.0936 iter/s, 1.63683s/100 iter), loss = 0.00176115
I0814 19:02:14.181916   613 solver.cpp:334]     Train net output #0: loss = 0.00176116 (* 1 = 0.00176116 loss)
I0814 19:02:14.181922   613 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0814 19:02:15.753820   613 solver.cpp:509] Iteration 15000, Testing net (#0)
I0814 19:02:16.578467   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909119
I0814 19:02:16.578485   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:02:16.578490   613 solver.cpp:594]     Test net output #2: loss = 0.314472 (* 1 = 0.314472 loss)
I0814 19:02:16.578507   613 solver.cpp:264] [MultiGPU] Tests completed in 0.824664s
I0814 19:02:16.594023   613 solver.cpp:312] Iteration 15000 (41.4583 iter/s, 2.41206s/100 iter), loss = 0.000777235
I0814 19:02:16.594041   613 solver.cpp:334]     Train net output #0: loss = 0.00077724 (* 1 = 0.00077724 loss)
I0814 19:02:16.594046   613 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0814 19:02:18.199842   613 solver.cpp:312] Iteration 15100 (62.2755 iter/s, 1.60577s/100 iter), loss = 0.00185174
I0814 19:02:18.199867   613 solver.cpp:334]     Train net output #0: loss = 0.00185174 (* 1 = 0.00185174 loss)
I0814 19:02:18.199872   613 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0814 19:02:19.796512   613 solver.cpp:312] Iteration 15200 (62.6324 iter/s, 1.59662s/100 iter), loss = 0.000854524
I0814 19:02:19.796561   613 solver.cpp:334]     Train net output #0: loss = 0.000854529 (* 1 = 0.000854529 loss)
I0814 19:02:19.796574   613 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0814 19:02:21.439632   613 solver.cpp:312] Iteration 15300 (60.8617 iter/s, 1.64307s/100 iter), loss = 0.00110343
I0814 19:02:21.439680   613 solver.cpp:334]     Train net output #0: loss = 0.00110343 (* 1 = 0.00110343 loss)
I0814 19:02:21.439692   613 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0814 19:02:23.060503   613 solver.cpp:312] Iteration 15400 (61.6971 iter/s, 1.62082s/100 iter), loss = 0.00551649
I0814 19:02:23.060576   613 solver.cpp:334]     Train net output #0: loss = 0.0055165 (* 1 = 0.0055165 loss)
I0814 19:02:23.060600   613 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0814 19:02:24.678741   613 solver.cpp:312] Iteration 15500 (61.7976 iter/s, 1.61819s/100 iter), loss = 0.00109347
I0814 19:02:24.678774   613 solver.cpp:334]     Train net output #0: loss = 0.00109347 (* 1 = 0.00109347 loss)
I0814 19:02:24.678782   613 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0814 19:02:26.284601   613 solver.cpp:312] Iteration 15600 (62.2738 iter/s, 1.60581s/100 iter), loss = 0.00153432
I0814 19:02:26.284626   613 solver.cpp:334]     Train net output #0: loss = 0.00153432 (* 1 = 0.00153432 loss)
I0814 19:02:26.284632   613 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0814 19:02:27.918694   613 solver.cpp:312] Iteration 15700 (61.1979 iter/s, 1.63404s/100 iter), loss = 0.00775528
I0814 19:02:27.918718   613 solver.cpp:334]     Train net output #0: loss = 0.00775528 (* 1 = 0.00775528 loss)
I0814 19:02:27.918725   613 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0814 19:02:29.571770   613 solver.cpp:312] Iteration 15800 (60.4951 iter/s, 1.65303s/100 iter), loss = 0.00118675
I0814 19:02:29.571837   613 solver.cpp:334]     Train net output #0: loss = 0.00118675 (* 1 = 0.00118675 loss)
I0814 19:02:29.571856   613 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0814 19:02:31.235224   613 solver.cpp:312] Iteration 15900 (60.1177 iter/s, 1.6634s/100 iter), loss = 0.00723424
I0814 19:02:31.235251   613 solver.cpp:334]     Train net output #0: loss = 0.00723424 (* 1 = 0.00723424 loss)
I0814 19:02:31.235257   613 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0814 19:02:32.830828   613 solver.cpp:509] Iteration 16000, Testing net (#0)
I0814 19:02:33.646445   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910295
I0814 19:02:33.646464   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:02:33.646469   613 solver.cpp:594]     Test net output #2: loss = 0.309762 (* 1 = 0.309762 loss)
I0814 19:02:33.646483   613 solver.cpp:264] [MultiGPU] Tests completed in 0.815633s
I0814 19:02:33.662111   613 solver.cpp:312] Iteration 16000 (41.2062 iter/s, 2.42682s/100 iter), loss = 0.0019195
I0814 19:02:33.662129   613 solver.cpp:334]     Train net output #0: loss = 0.0019195 (* 1 = 0.0019195 loss)
I0814 19:02:33.662135   613 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0814 19:02:35.252976   613 solver.cpp:312] Iteration 16100 (62.861 iter/s, 1.59081s/100 iter), loss = 0.00118147
I0814 19:02:35.253023   613 solver.cpp:334]     Train net output #0: loss = 0.00118147 (* 1 = 0.00118147 loss)
I0814 19:02:35.253029   613 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0814 19:02:36.864543   613 solver.cpp:312] Iteration 16200 (62.0533 iter/s, 1.61152s/100 iter), loss = 0.00292831
I0814 19:02:36.864588   613 solver.cpp:334]     Train net output #0: loss = 0.00292831 (* 1 = 0.00292831 loss)
I0814 19:02:36.864599   613 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0814 19:02:38.505540   613 solver.cpp:312] Iteration 16300 (60.9404 iter/s, 1.64095s/100 iter), loss = 0.00397588
I0814 19:02:38.505587   613 solver.cpp:334]     Train net output #0: loss = 0.00397588 (* 1 = 0.00397588 loss)
I0814 19:02:38.505599   613 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0814 19:02:40.167719   613 solver.cpp:312] Iteration 16400 (60.1638 iter/s, 1.66213s/100 iter), loss = 0.00162321
I0814 19:02:40.167800   613 solver.cpp:334]     Train net output #0: loss = 0.00162321 (* 1 = 0.00162321 loss)
I0814 19:02:40.167807   613 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0814 19:02:41.788164   613 solver.cpp:312] Iteration 16500 (61.7133 iter/s, 1.6204s/100 iter), loss = 0.0013623
I0814 19:02:41.788192   613 solver.cpp:334]     Train net output #0: loss = 0.0013623 (* 1 = 0.0013623 loss)
I0814 19:02:41.788199   613 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0814 19:02:43.392880   613 solver.cpp:312] Iteration 16600 (62.3183 iter/s, 1.60467s/100 iter), loss = 0.00175429
I0814 19:02:43.392931   613 solver.cpp:334]     Train net output #0: loss = 0.00175429 (* 1 = 0.00175429 loss)
I0814 19:02:43.392946   613 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0814 19:02:45.049950   613 solver.cpp:312] Iteration 16700 (60.3494 iter/s, 1.65702s/100 iter), loss = 0.00445534
I0814 19:02:45.049999   613 solver.cpp:334]     Train net output #0: loss = 0.00445534 (* 1 = 0.00445534 loss)
I0814 19:02:45.050012   613 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0814 19:02:46.685770   613 solver.cpp:312] Iteration 16800 (61.1332 iter/s, 1.63577s/100 iter), loss = 0.00359188
I0814 19:02:46.685820   613 solver.cpp:334]     Train net output #0: loss = 0.00359188 (* 1 = 0.00359188 loss)
I0814 19:02:46.685832   613 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0814 19:02:48.291832   613 solver.cpp:312] Iteration 16900 (62.2662 iter/s, 1.60601s/100 iter), loss = 0.00337016
I0814 19:02:48.291862   613 solver.cpp:334]     Train net output #0: loss = 0.00337016 (* 1 = 0.00337016 loss)
I0814 19:02:48.291868   613 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0814 19:02:49.916357   613 solver.cpp:509] Iteration 17000, Testing net (#0)
I0814 19:02:50.731360   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.918236
I0814 19:02:50.731379   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:02:50.731386   613 solver.cpp:594]     Test net output #2: loss = 0.282802 (* 1 = 0.282802 loss)
I0814 19:02:50.731415   613 solver.cpp:264] [MultiGPU] Tests completed in 0.815037s
I0814 19:02:50.747025   613 solver.cpp:312] Iteration 17000 (40.7311 iter/s, 2.45513s/100 iter), loss = 0.00310713
I0814 19:02:50.747040   613 solver.cpp:334]     Train net output #0: loss = 0.00310713 (* 1 = 0.00310713 loss)
I0814 19:02:50.747046   613 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0814 19:02:52.351989   613 solver.cpp:312] Iteration 17100 (62.3086 iter/s, 1.60491s/100 iter), loss = 0.0036535
I0814 19:02:52.352037   613 solver.cpp:334]     Train net output #0: loss = 0.0036535 (* 1 = 0.0036535 loss)
I0814 19:02:52.352051   613 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0814 19:02:53.989886   613 solver.cpp:312] Iteration 17200 (61.0558 iter/s, 1.63784s/100 iter), loss = 0.000626812
I0814 19:02:53.989912   613 solver.cpp:334]     Train net output #0: loss = 0.000626813 (* 1 = 0.000626813 loss)
I0814 19:02:53.989917   613 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0814 19:02:55.571626   613 solver.cpp:312] Iteration 17300 (63.2234 iter/s, 1.58169s/100 iter), loss = 0.00146118
I0814 19:02:55.571651   613 solver.cpp:334]     Train net output #0: loss = 0.00146118 (* 1 = 0.00146118 loss)
I0814 19:02:55.571657   613 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0814 19:02:57.198526   613 solver.cpp:312] Iteration 17400 (61.4684 iter/s, 1.62685s/100 iter), loss = 0.00112822
I0814 19:02:57.198689   613 solver.cpp:334]     Train net output #0: loss = 0.00112822 (* 1 = 0.00112822 loss)
I0814 19:02:57.198772   613 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0814 19:02:58.795130   613 solver.cpp:312] Iteration 17500 (62.635 iter/s, 1.59655s/100 iter), loss = 0.00461505
I0814 19:02:58.795192   613 solver.cpp:334]     Train net output #0: loss = 0.00461505 (* 1 = 0.00461505 loss)
I0814 19:02:58.795209   613 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0814 19:03:00.467663   613 solver.cpp:312] Iteration 17600 (59.7913 iter/s, 1.67248s/100 iter), loss = 0.00241077
I0814 19:03:00.467803   613 solver.cpp:334]     Train net output #0: loss = 0.00241077 (* 1 = 0.00241077 loss)
I0814 19:03:00.467820   613 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0814 19:03:02.104017   613 solver.cpp:312] Iteration 17700 (61.1134 iter/s, 1.6363s/100 iter), loss = 0.0015215
I0814 19:03:02.104043   613 solver.cpp:334]     Train net output #0: loss = 0.0015215 (* 1 = 0.0015215 loss)
I0814 19:03:02.104050   613 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0814 19:03:03.740682   613 solver.cpp:312] Iteration 17800 (61.1018 iter/s, 1.63661s/100 iter), loss = 0.00227391
I0814 19:03:03.740742   613 solver.cpp:334]     Train net output #0: loss = 0.00227391 (* 1 = 0.00227391 loss)
I0814 19:03:03.740764   613 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0814 19:03:05.343962   613 solver.cpp:312] Iteration 17900 (62.374 iter/s, 1.60323s/100 iter), loss = 0.00459324
I0814 19:03:05.344090   613 solver.cpp:334]     Train net output #0: loss = 0.00459324 (* 1 = 0.00459324 loss)
I0814 19:03:05.344110   613 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0814 19:03:06.942507   613 solver.cpp:509] Iteration 18000, Testing net (#0)
I0814 19:03:07.419322   611 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:03:07.757176   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.927354
I0814 19:03:07.757201   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:03:07.757211   613 solver.cpp:594]     Test net output #2: loss = 0.266419 (* 1 = 0.266419 loss)
I0814 19:03:07.757236   613 solver.cpp:264] [MultiGPU] Tests completed in 0.814704s
I0814 19:03:07.781983   613 solver.cpp:312] Iteration 18000 (41.0181 iter/s, 2.43795s/100 iter), loss = 0.000737331
I0814 19:03:07.782014   613 solver.cpp:334]     Train net output #0: loss = 0.000737331 (* 1 = 0.000737331 loss)
I0814 19:03:07.782021   613 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0814 19:03:09.409538   613 solver.cpp:312] Iteration 18100 (61.4438 iter/s, 1.6275s/100 iter), loss = 0.0010484
I0814 19:03:09.409564   613 solver.cpp:334]     Train net output #0: loss = 0.0010484 (* 1 = 0.0010484 loss)
I0814 19:03:09.409570   613 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0814 19:03:11.050189   613 solver.cpp:312] Iteration 18200 (60.9532 iter/s, 1.6406s/100 iter), loss = 0.00127794
I0814 19:03:11.050211   613 solver.cpp:334]     Train net output #0: loss = 0.00127794 (* 1 = 0.00127794 loss)
I0814 19:03:11.050217   613 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0814 19:03:12.679476   613 solver.cpp:312] Iteration 18300 (61.3784 iter/s, 1.62924s/100 iter), loss = 0.00375784
I0814 19:03:12.679500   613 solver.cpp:334]     Train net output #0: loss = 0.00375784 (* 1 = 0.00375784 loss)
I0814 19:03:12.679505   613 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0814 19:03:14.302444   613 solver.cpp:312] Iteration 18400 (61.6175 iter/s, 1.62292s/100 iter), loss = 0.0021986
I0814 19:03:14.302472   613 solver.cpp:334]     Train net output #0: loss = 0.0021986 (* 1 = 0.0021986 loss)
I0814 19:03:14.302479   613 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0814 19:03:15.896833   613 solver.cpp:312] Iteration 18500 (62.7218 iter/s, 1.59434s/100 iter), loss = 0.000585724
I0814 19:03:15.896893   613 solver.cpp:334]     Train net output #0: loss = 0.000585723 (* 1 = 0.000585723 loss)
I0814 19:03:15.896911   613 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0814 19:03:17.500366   613 solver.cpp:312] Iteration 18600 (62.3641 iter/s, 1.60349s/100 iter), loss = 0.00354955
I0814 19:03:17.500427   613 solver.cpp:334]     Train net output #0: loss = 0.00354955 (* 1 = 0.00354955 loss)
I0814 19:03:17.500445   613 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0814 19:03:19.124641   613 solver.cpp:312] Iteration 18700 (61.5678 iter/s, 1.62423s/100 iter), loss = 0.00254444
I0814 19:03:19.124665   613 solver.cpp:334]     Train net output #0: loss = 0.00254444 (* 1 = 0.00254444 loss)
I0814 19:03:19.124668   613 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0814 19:03:20.750239   613 solver.cpp:312] Iteration 18800 (61.5178 iter/s, 1.62555s/100 iter), loss = 0.00104108
I0814 19:03:20.750268   613 solver.cpp:334]     Train net output #0: loss = 0.00104108 (* 1 = 0.00104108 loss)
I0814 19:03:20.750275   613 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0814 19:03:22.331162   613 solver.cpp:312] Iteration 18900 (63.2561 iter/s, 1.58087s/100 iter), loss = 0.00155515
I0814 19:03:22.331187   613 solver.cpp:334]     Train net output #0: loss = 0.00155515 (* 1 = 0.00155515 loss)
I0814 19:03:22.331193   613 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0814 19:03:23.932219   613 solver.cpp:509] Iteration 19000, Testing net (#0)
I0814 19:03:24.751765   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.92353
I0814 19:03:24.751781   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:03:24.751803   613 solver.cpp:594]     Test net output #2: loss = 0.27743 (* 1 = 0.27743 loss)
I0814 19:03:24.751823   613 solver.cpp:264] [MultiGPU] Tests completed in 0.819583s
I0814 19:03:24.772634   613 solver.cpp:312] Iteration 19000 (40.9601 iter/s, 2.4414s/100 iter), loss = 0.00470685
I0814 19:03:24.772660   613 solver.cpp:334]     Train net output #0: loss = 0.00470686 (* 1 = 0.00470686 loss)
I0814 19:03:24.772666   613 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0814 19:03:26.405289   613 solver.cpp:312] Iteration 19100 (61.2516 iter/s, 1.63261s/100 iter), loss = 0.00294124
I0814 19:03:26.405313   613 solver.cpp:334]     Train net output #0: loss = 0.00294124 (* 1 = 0.00294124 loss)
I0814 19:03:26.405318   613 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0814 19:03:28.027969   613 solver.cpp:312] Iteration 19200 (61.6283 iter/s, 1.62263s/100 iter), loss = 0.000795837
I0814 19:03:28.028029   613 solver.cpp:334]     Train net output #0: loss = 0.000795837 (* 1 = 0.000795837 loss)
I0814 19:03:28.028048   613 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0814 19:03:29.644533   613 solver.cpp:312] Iteration 19300 (61.8616 iter/s, 1.61651s/100 iter), loss = 0.000762076
I0814 19:03:29.644562   613 solver.cpp:334]     Train net output #0: loss = 0.000762077 (* 1 = 0.000762077 loss)
I0814 19:03:29.644568   613 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0814 19:03:31.244894   613 solver.cpp:312] Iteration 19400 (62.4879 iter/s, 1.60031s/100 iter), loss = 0.00231931
I0814 19:03:31.244917   613 solver.cpp:334]     Train net output #0: loss = 0.00231931 (* 1 = 0.00231931 loss)
I0814 19:03:31.244923   613 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0814 19:03:32.828857   613 solver.cpp:312] Iteration 19500 (63.1346 iter/s, 1.58392s/100 iter), loss = 0.0022329
I0814 19:03:32.828923   613 solver.cpp:334]     Train net output #0: loss = 0.0022329 (* 1 = 0.0022329 loss)
I0814 19:03:32.828945   613 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0814 19:03:34.432029   613 solver.cpp:312] Iteration 19600 (62.3783 iter/s, 1.60312s/100 iter), loss = 0.00570968
I0814 19:03:34.432054   613 solver.cpp:334]     Train net output #0: loss = 0.00570968 (* 1 = 0.00570968 loss)
I0814 19:03:34.432060   613 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0814 19:03:36.048465   613 solver.cpp:312] Iteration 19700 (61.8663 iter/s, 1.61639s/100 iter), loss = 0.000734632
I0814 19:03:36.048537   613 solver.cpp:334]     Train net output #0: loss = 0.000734632 (* 1 = 0.000734632 loss)
I0814 19:03:36.048544   613 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0814 19:03:37.664813   613 solver.cpp:312] Iteration 19800 (61.8697 iter/s, 1.6163s/100 iter), loss = 0.00238147
I0814 19:03:37.664836   613 solver.cpp:334]     Train net output #0: loss = 0.00238147 (* 1 = 0.00238147 loss)
I0814 19:03:37.664844   613 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0814 19:03:39.303618   613 solver.cpp:312] Iteration 19900 (61.0219 iter/s, 1.63876s/100 iter), loss = 0.00173495
I0814 19:03:39.303679   613 solver.cpp:334]     Train net output #0: loss = 0.00173495 (* 1 = 0.00173495 loss)
I0814 19:03:39.303697   613 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0814 19:03:40.900918   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_20000.caffemodel
I0814 19:03:40.910614   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_20000.solverstate
I0814 19:03:40.914304   613 solver.cpp:509] Iteration 20000, Testing net (#0)
I0814 19:03:41.712262   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.918824
I0814 19:03:41.712280   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:03:41.712285   613 solver.cpp:594]     Test net output #2: loss = 0.28781 (* 1 = 0.28781 loss)
I0814 19:03:41.712306   613 solver.cpp:264] [MultiGPU] Tests completed in 0.797976s
I0814 19:03:41.730294   613 solver.cpp:312] Iteration 20000 (41.2098 iter/s, 2.42661s/100 iter), loss = 0.000906871
I0814 19:03:41.730315   613 solver.cpp:334]     Train net output #0: loss = 0.000906869 (* 1 = 0.000906869 loss)
I0814 19:03:41.730321   613 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0814 19:03:43.371384   613 solver.cpp:312] Iteration 20100 (60.937 iter/s, 1.64104s/100 iter), loss = 0.00163634
I0814 19:03:43.371409   613 solver.cpp:334]     Train net output #0: loss = 0.00163633 (* 1 = 0.00163633 loss)
I0814 19:03:43.371415   613 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0814 19:03:44.994832   613 solver.cpp:312] Iteration 20200 (61.5993 iter/s, 1.62339s/100 iter), loss = 0.00267018
I0814 19:03:44.994901   613 solver.cpp:334]     Train net output #0: loss = 0.00267018 (* 1 = 0.00267018 loss)
I0814 19:03:44.994925   613 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0814 19:03:46.600505   613 solver.cpp:312] Iteration 20300 (62.281 iter/s, 1.60563s/100 iter), loss = 0.00179529
I0814 19:03:46.600535   613 solver.cpp:334]     Train net output #0: loss = 0.00179529 (* 1 = 0.00179529 loss)
I0814 19:03:46.600543   613 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0814 19:03:48.211993   613 solver.cpp:312] Iteration 20400 (62.0564 iter/s, 1.61144s/100 iter), loss = 0.00372159
I0814 19:03:48.212062   613 solver.cpp:334]     Train net output #0: loss = 0.00372159 (* 1 = 0.00372159 loss)
I0814 19:03:48.212082   613 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0814 19:03:49.822804   613 solver.cpp:312] Iteration 20500 (62.0824 iter/s, 1.61076s/100 iter), loss = 0.00281403
I0814 19:03:49.822829   613 solver.cpp:334]     Train net output #0: loss = 0.00281402 (* 1 = 0.00281402 loss)
I0814 19:03:49.822835   613 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0814 19:03:51.430668   613 solver.cpp:312] Iteration 20600 (62.1962 iter/s, 1.60782s/100 iter), loss = 0.00183386
I0814 19:03:51.430693   613 solver.cpp:334]     Train net output #0: loss = 0.00183386 (* 1 = 0.00183386 loss)
I0814 19:03:51.430698   613 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0814 19:03:53.056679   613 solver.cpp:312] Iteration 20700 (61.5022 iter/s, 1.62596s/100 iter), loss = 0.0033849
I0814 19:03:53.056741   613 solver.cpp:334]     Train net output #0: loss = 0.0033849 (* 1 = 0.0033849 loss)
I0814 19:03:53.056772   613 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0814 19:03:54.667013   613 solver.cpp:312] Iteration 20800 (62.1008 iter/s, 1.61028s/100 iter), loss = 0.00102706
I0814 19:03:54.667155   613 solver.cpp:334]     Train net output #0: loss = 0.00102706 (* 1 = 0.00102706 loss)
I0814 19:03:54.667173   613 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0814 19:03:56.288794   613 solver.cpp:312] Iteration 20900 (61.6625 iter/s, 1.62173s/100 iter), loss = 0.00151879
I0814 19:03:56.288818   613 solver.cpp:334]     Train net output #0: loss = 0.00151879 (* 1 = 0.00151879 loss)
I0814 19:03:56.288823   613 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0814 19:03:57.896621   613 solver.cpp:509] Iteration 21000, Testing net (#0)
I0814 19:03:58.717090   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919413
I0814 19:03:58.717108   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:03:58.717115   613 solver.cpp:594]     Test net output #2: loss = 0.293822 (* 1 = 0.293822 loss)
I0814 19:03:58.717133   613 solver.cpp:264] [MultiGPU] Tests completed in 0.820489s
I0814 19:03:58.736523   613 solver.cpp:312] Iteration 21000 (40.8553 iter/s, 2.44766s/100 iter), loss = 0.00344774
I0814 19:03:58.736541   613 solver.cpp:334]     Train net output #0: loss = 0.00344774 (* 1 = 0.00344774 loss)
I0814 19:03:58.736546   613 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0814 19:04:00.374426   613 solver.cpp:312] Iteration 21100 (61.0556 iter/s, 1.63785s/100 iter), loss = 0.00048228
I0814 19:04:00.374476   613 solver.cpp:334]     Train net output #0: loss = 0.000482281 (* 1 = 0.000482281 loss)
I0814 19:04:00.374490   613 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0814 19:04:01.988874   613 solver.cpp:312] Iteration 21200 (61.9426 iter/s, 1.6144s/100 iter), loss = 0.00248236
I0814 19:04:01.988900   613 solver.cpp:334]     Train net output #0: loss = 0.00248236 (* 1 = 0.00248236 loss)
I0814 19:04:01.988906   613 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0814 19:04:03.573149   613 solver.cpp:312] Iteration 21300 (63.1223 iter/s, 1.58423s/100 iter), loss = 0.00168004
I0814 19:04:03.573212   613 solver.cpp:334]     Train net output #0: loss = 0.00168004 (* 1 = 0.00168004 loss)
I0814 19:04:03.573232   613 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0814 19:04:05.207016   613 solver.cpp:312] Iteration 21400 (61.2063 iter/s, 1.63382s/100 iter), loss = 0.00240321
I0814 19:04:05.207041   613 solver.cpp:334]     Train net output #0: loss = 0.00240322 (* 1 = 0.00240322 loss)
I0814 19:04:05.207047   613 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0814 19:04:06.837733   613 solver.cpp:312] Iteration 21500 (61.3247 iter/s, 1.63066s/100 iter), loss = 0.00129845
I0814 19:04:06.837801   613 solver.cpp:334]     Train net output #0: loss = 0.00129845 (* 1 = 0.00129845 loss)
I0814 19:04:06.837806   613 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0814 19:04:08.452221   613 solver.cpp:312] Iteration 21600 (61.9409 iter/s, 1.61444s/100 iter), loss = 0.000980934
I0814 19:04:08.452267   613 solver.cpp:334]     Train net output #0: loss = 0.000980937 (* 1 = 0.000980937 loss)
I0814 19:04:08.452280   613 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0814 19:04:10.038586   613 solver.cpp:312] Iteration 21700 (63.0392 iter/s, 1.58631s/100 iter), loss = 0.00176299
I0814 19:04:10.038843   613 solver.cpp:334]     Train net output #0: loss = 0.001763 (* 1 = 0.001763 loss)
I0814 19:04:10.038851   613 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0814 19:04:11.695492   613 solver.cpp:312] Iteration 21800 (60.3554 iter/s, 1.65685s/100 iter), loss = 0.00176023
I0814 19:04:11.695521   613 solver.cpp:334]     Train net output #0: loss = 0.00176023 (* 1 = 0.00176023 loss)
I0814 19:04:11.695528   613 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0814 19:04:13.363286   613 solver.cpp:312] Iteration 21900 (59.9612 iter/s, 1.66774s/100 iter), loss = 0.000521508
I0814 19:04:13.363309   613 solver.cpp:334]     Train net output #0: loss = 0.00052151 (* 1 = 0.00052151 loss)
I0814 19:04:13.363317   613 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0814 19:04:14.989109   613 solver.cpp:509] Iteration 22000, Testing net (#0)
I0814 19:04:15.404855   611 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:04:15.798588   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917648
I0814 19:04:15.798611   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:04:15.798619   613 solver.cpp:594]     Test net output #2: loss = 0.294372 (* 1 = 0.294372 loss)
I0814 19:04:15.798648   613 solver.cpp:264] [MultiGPU] Tests completed in 0.809512s
I0814 19:04:15.817553   613 solver.cpp:312] Iteration 22000 (40.7465 iter/s, 2.4542s/100 iter), loss = 0.00435823
I0814 19:04:15.817579   613 solver.cpp:334]     Train net output #0: loss = 0.00435823 (* 1 = 0.00435823 loss)
I0814 19:04:15.817584   613 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0814 19:04:17.445997   613 solver.cpp:312] Iteration 22100 (61.4103 iter/s, 1.62839s/100 iter), loss = 0.00471555
I0814 19:04:17.446140   613 solver.cpp:334]     Train net output #0: loss = 0.00471555 (* 1 = 0.00471555 loss)
I0814 19:04:17.446159   613 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0814 19:04:19.078338   613 solver.cpp:312] Iteration 22200 (61.2634 iter/s, 1.63229s/100 iter), loss = 0.00286048
I0814 19:04:19.078362   613 solver.cpp:334]     Train net output #0: loss = 0.00286048 (* 1 = 0.00286048 loss)
I0814 19:04:19.078368   613 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0814 19:04:20.689061   613 solver.cpp:312] Iteration 22300 (62.0858 iter/s, 1.61067s/100 iter), loss = 0.00219214
I0814 19:04:20.689111   613 solver.cpp:334]     Train net output #0: loss = 0.00219214 (* 1 = 0.00219214 loss)
I0814 19:04:20.689124   613 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0814 19:04:22.324229   613 solver.cpp:312] Iteration 22400 (61.1577 iter/s, 1.63512s/100 iter), loss = 0.00215813
I0814 19:04:22.324276   613 solver.cpp:334]     Train net output #0: loss = 0.00215813 (* 1 = 0.00215813 loss)
I0814 19:04:22.324288   613 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0814 19:04:23.972817   613 solver.cpp:312] Iteration 22500 (60.6599 iter/s, 1.64854s/100 iter), loss = 0.00330128
I0814 19:04:23.972885   613 solver.cpp:334]     Train net output #0: loss = 0.00330128 (* 1 = 0.00330128 loss)
I0814 19:04:23.972908   613 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0814 19:04:25.616348   613 solver.cpp:312] Iteration 22600 (60.8463 iter/s, 1.64348s/100 iter), loss = 0.00053988
I0814 19:04:25.616426   613 solver.cpp:334]     Train net output #0: loss = 0.000539878 (* 1 = 0.000539878 loss)
I0814 19:04:25.616459   613 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0814 19:04:27.241888   613 solver.cpp:312] Iteration 22700 (61.5198 iter/s, 1.62549s/100 iter), loss = 0.00686422
I0814 19:04:27.241948   613 solver.cpp:334]     Train net output #0: loss = 0.00686422 (* 1 = 0.00686422 loss)
I0814 19:04:27.241966   613 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0814 19:04:28.870021   613 solver.cpp:312] Iteration 22800 (61.422 iter/s, 1.62808s/100 iter), loss = 0.00274646
I0814 19:04:28.870080   613 solver.cpp:334]     Train net output #0: loss = 0.00274646 (* 1 = 0.00274646 loss)
I0814 19:04:28.870098   613 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0814 19:04:30.511370   613 solver.cpp:312] Iteration 22900 (60.9273 iter/s, 1.6413s/100 iter), loss = 0.000264519
I0814 19:04:30.511427   613 solver.cpp:334]     Train net output #0: loss = 0.000264518 (* 1 = 0.000264518 loss)
I0814 19:04:30.511445   613 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0814 19:04:32.120968   613 solver.cpp:509] Iteration 23000, Testing net (#0)
I0814 19:04:32.944051   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.916177
I0814 19:04:32.944070   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:04:32.944077   613 solver.cpp:594]     Test net output #2: loss = 0.291836 (* 1 = 0.291836 loss)
I0814 19:04:32.944095   613 solver.cpp:264] [MultiGPU] Tests completed in 0.823107s
I0814 19:04:32.964614   613 solver.cpp:312] Iteration 23000 (40.7635 iter/s, 2.45318s/100 iter), loss = 0.00219191
I0814 19:04:32.964633   613 solver.cpp:334]     Train net output #0: loss = 0.00219191 (* 1 = 0.00219191 loss)
I0814 19:04:32.964638   613 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0814 19:04:34.585244   613 solver.cpp:312] Iteration 23100 (61.7064 iter/s, 1.62058s/100 iter), loss = 0.00441107
I0814 19:04:34.585268   613 solver.cpp:334]     Train net output #0: loss = 0.00441107 (* 1 = 0.00441107 loss)
I0814 19:04:34.585273   613 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0814 19:04:36.201262   613 solver.cpp:312] Iteration 23200 (61.8825 iter/s, 1.61597s/100 iter), loss = 0.00173958
I0814 19:04:36.201289   613 solver.cpp:334]     Train net output #0: loss = 0.00173958 (* 1 = 0.00173958 loss)
I0814 19:04:36.201295   613 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0814 19:04:37.789973   613 solver.cpp:312] Iteration 23300 (62.946 iter/s, 1.58866s/100 iter), loss = 0.0031079
I0814 19:04:37.790181   613 solver.cpp:334]     Train net output #0: loss = 0.0031079 (* 1 = 0.0031079 loss)
I0814 19:04:37.790205   613 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0814 19:04:39.444963   613 solver.cpp:312] Iteration 23400 (60.4251 iter/s, 1.65494s/100 iter), loss = 0.00281051
I0814 19:04:39.445009   613 solver.cpp:334]     Train net output #0: loss = 0.00281051 (* 1 = 0.00281051 loss)
I0814 19:04:39.445022   613 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0814 19:04:41.064201   613 solver.cpp:312] Iteration 23500 (61.7594 iter/s, 1.61919s/100 iter), loss = 0.000432662
I0814 19:04:41.064252   613 solver.cpp:334]     Train net output #0: loss = 0.000432662 (* 1 = 0.000432662 loss)
I0814 19:04:41.064266   613 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0814 19:04:42.675755   613 solver.cpp:312] Iteration 23600 (62.0539 iter/s, 1.6115s/100 iter), loss = 0.00194008
I0814 19:04:42.675778   613 solver.cpp:334]     Train net output #0: loss = 0.00194008 (* 1 = 0.00194008 loss)
I0814 19:04:42.675783   613 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0814 19:04:44.284289   613 solver.cpp:312] Iteration 23700 (62.1702 iter/s, 1.60849s/100 iter), loss = 0.00201476
I0814 19:04:44.284315   613 solver.cpp:334]     Train net output #0: loss = 0.00201476 (* 1 = 0.00201476 loss)
I0814 19:04:44.284322   613 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0814 19:04:45.944391   613 solver.cpp:312] Iteration 23800 (60.2391 iter/s, 1.66005s/100 iter), loss = 0.00134619
I0814 19:04:45.944450   613 solver.cpp:334]     Train net output #0: loss = 0.0013462 (* 1 = 0.0013462 loss)
I0814 19:04:45.944468   613 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0814 19:04:47.528363   613 solver.cpp:312] Iteration 23900 (63.1344 iter/s, 1.58392s/100 iter), loss = 0.00208318
I0814 19:04:47.528424   613 solver.cpp:334]     Train net output #0: loss = 0.00208318 (* 1 = 0.00208318 loss)
I0814 19:04:47.528442   613 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0814 19:04:49.133083   613 solver.cpp:509] Iteration 24000, Testing net (#0)
I0814 19:04:49.953320   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91353
I0814 19:04:49.953339   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:04:49.953343   613 solver.cpp:594]     Test net output #2: loss = 0.303969 (* 1 = 0.303969 loss)
I0814 19:04:49.953359   613 solver.cpp:264] [MultiGPU] Tests completed in 0.820255s
I0814 19:04:49.969040   613 solver.cpp:312] Iteration 24000 (40.9734 iter/s, 2.44061s/100 iter), loss = 0.00240469
I0814 19:04:49.969056   613 solver.cpp:334]     Train net output #0: loss = 0.00240469 (* 1 = 0.00240469 loss)
I0814 19:04:49.969059   613 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0814 19:04:51.595706   613 solver.cpp:312] Iteration 24100 (61.4775 iter/s, 1.62661s/100 iter), loss = 0.00447908
I0814 19:04:51.595757   613 solver.cpp:334]     Train net output #0: loss = 0.00447908 (* 1 = 0.00447908 loss)
I0814 19:04:51.595769   613 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0814 19:04:53.215042   613 solver.cpp:312] Iteration 24200 (61.7556 iter/s, 1.61929s/100 iter), loss = 0.00180773
I0814 19:04:53.215101   613 solver.cpp:334]     Train net output #0: loss = 0.00180773 (* 1 = 0.00180773 loss)
I0814 19:04:53.215117   613 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0814 19:04:54.868645   613 solver.cpp:312] Iteration 24300 (60.4759 iter/s, 1.65355s/100 iter), loss = 0.00156146
I0814 19:04:54.868667   613 solver.cpp:334]     Train net output #0: loss = 0.00156146 (* 1 = 0.00156146 loss)
I0814 19:04:54.868672   613 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0814 19:04:56.461805   613 solver.cpp:312] Iteration 24400 (62.7702 iter/s, 1.59311s/100 iter), loss = 0.00068046
I0814 19:04:56.461854   613 solver.cpp:334]     Train net output #0: loss = 0.00068046 (* 1 = 0.00068046 loss)
I0814 19:04:56.461866   613 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0814 19:04:58.116303   613 solver.cpp:312] Iteration 24500 (60.4431 iter/s, 1.65445s/100 iter), loss = 0.00273359
I0814 19:04:58.116349   613 solver.cpp:334]     Train net output #0: loss = 0.00273359 (* 1 = 0.00273359 loss)
I0814 19:04:58.116360   613 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0814 19:04:59.689074   613 solver.cpp:312] Iteration 24600 (63.5841 iter/s, 1.57272s/100 iter), loss = 0.00144755
I0814 19:04:59.689122   613 solver.cpp:334]     Train net output #0: loss = 0.00144755 (* 1 = 0.00144755 loss)
I0814 19:04:59.689136   613 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0814 19:05:01.303139   613 solver.cpp:312] Iteration 24700 (61.9572 iter/s, 1.61402s/100 iter), loss = 0.000753807
I0814 19:05:01.303186   613 solver.cpp:334]     Train net output #0: loss = 0.000753808 (* 1 = 0.000753808 loss)
I0814 19:05:01.303198   613 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0814 19:05:02.908033   613 solver.cpp:312] Iteration 24800 (62.3113 iter/s, 1.60484s/100 iter), loss = 0.00184208
I0814 19:05:02.908080   613 solver.cpp:334]     Train net output #0: loss = 0.00184208 (* 1 = 0.00184208 loss)
I0814 19:05:02.908097   613 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0814 19:05:04.530117   613 solver.cpp:312] Iteration 24900 (61.651 iter/s, 1.62203s/100 iter), loss = 0.00120535
I0814 19:05:04.530141   613 solver.cpp:334]     Train net output #0: loss = 0.00120535 (* 1 = 0.00120535 loss)
I0814 19:05:04.530148   613 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0814 19:05:06.097812   613 solver.cpp:509] Iteration 25000, Testing net (#0)
I0814 19:05:06.921210   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911471
I0814 19:05:06.921226   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:05:06.921231   613 solver.cpp:594]     Test net output #2: loss = 0.316788 (* 1 = 0.316788 loss)
I0814 19:05:06.921252   613 solver.cpp:264] [MultiGPU] Tests completed in 0.823416s
I0814 19:05:06.936925   613 solver.cpp:312] Iteration 25000 (41.5499 iter/s, 2.40674s/100 iter), loss = 0.00108883
I0814 19:05:06.936944   613 solver.cpp:334]     Train net output #0: loss = 0.00108883 (* 1 = 0.00108883 loss)
I0814 19:05:06.936949   613 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0814 19:05:08.543546   613 solver.cpp:312] Iteration 25100 (62.2444 iter/s, 1.60657s/100 iter), loss = 0.00245606
I0814 19:05:08.543660   613 solver.cpp:334]     Train net output #0: loss = 0.00245606 (* 1 = 0.00245606 loss)
I0814 19:05:08.543678   613 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0814 19:05:10.178711   613 solver.cpp:312] Iteration 25200 (61.1577 iter/s, 1.63512s/100 iter), loss = 0.0063464
I0814 19:05:10.178758   613 solver.cpp:334]     Train net output #0: loss = 0.0063464 (* 1 = 0.0063464 loss)
I0814 19:05:10.178769   613 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0814 19:05:11.804309   613 solver.cpp:312] Iteration 25300 (61.5179 iter/s, 1.62554s/100 iter), loss = 0.000997024
I0814 19:05:11.804355   613 solver.cpp:334]     Train net output #0: loss = 0.000997022 (* 1 = 0.000997022 loss)
I0814 19:05:11.804373   613 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0814 19:05:13.454617   613 solver.cpp:312] Iteration 25400 (60.5966 iter/s, 1.65026s/100 iter), loss = 0.00138587
I0814 19:05:13.454661   613 solver.cpp:334]     Train net output #0: loss = 0.00138587 (* 1 = 0.00138587 loss)
I0814 19:05:13.454674   613 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0814 19:05:15.046357   613 solver.cpp:312] Iteration 25500 (62.8262 iter/s, 1.59169s/100 iter), loss = 0.00241851
I0814 19:05:15.046383   613 solver.cpp:334]     Train net output #0: loss = 0.00241851 (* 1 = 0.00241851 loss)
I0814 19:05:15.046389   613 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0814 19:05:16.655443   613 solver.cpp:312] Iteration 25600 (62.1491 iter/s, 1.60903s/100 iter), loss = 0.00251582
I0814 19:05:16.655490   613 solver.cpp:334]     Train net output #0: loss = 0.00251582 (* 1 = 0.00251582 loss)
I0814 19:05:16.655505   613 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0814 19:05:18.313510   613 solver.cpp:312] Iteration 25700 (60.313 iter/s, 1.65802s/100 iter), loss = 0.00182482
I0814 19:05:18.313557   613 solver.cpp:334]     Train net output #0: loss = 0.00182482 (* 1 = 0.00182482 loss)
I0814 19:05:18.313568   613 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0814 19:05:19.944317   613 solver.cpp:312] Iteration 25800 (61.3211 iter/s, 1.63076s/100 iter), loss = 0.00243829
I0814 19:05:19.944360   613 solver.cpp:334]     Train net output #0: loss = 0.00243829 (* 1 = 0.00243829 loss)
I0814 19:05:19.944372   613 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0814 19:05:21.564703   613 solver.cpp:312] Iteration 25900 (61.7156 iter/s, 1.62034s/100 iter), loss = 0.00236606
I0814 19:05:21.564730   613 solver.cpp:334]     Train net output #0: loss = 0.00236607 (* 1 = 0.00236607 loss)
I0814 19:05:21.564738   613 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0814 19:05:23.161217   613 solver.cpp:509] Iteration 26000, Testing net (#0)
I0814 19:05:23.990511   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907648
I0814 19:05:23.990531   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993529
I0814 19:05:23.990536   613 solver.cpp:594]     Test net output #2: loss = 0.338682 (* 1 = 0.338682 loss)
I0814 19:05:23.990703   613 solver.cpp:264] [MultiGPU] Tests completed in 0.829313s
I0814 19:05:24.006377   613 solver.cpp:312] Iteration 26000 (40.9567 iter/s, 2.4416s/100 iter), loss = 0.00200366
I0814 19:05:24.006395   613 solver.cpp:334]     Train net output #0: loss = 0.00200367 (* 1 = 0.00200367 loss)
I0814 19:05:24.006402   613 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0814 19:05:25.603566   613 solver.cpp:312] Iteration 26100 (62.612 iter/s, 1.59714s/100 iter), loss = 0.00150677
I0814 19:05:25.603615   613 solver.cpp:334]     Train net output #0: loss = 0.00150678 (* 1 = 0.00150678 loss)
I0814 19:05:25.603626   613 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0814 19:05:27.210198   613 solver.cpp:312] Iteration 26200 (62.2439 iter/s, 1.60658s/100 iter), loss = 0.00192631
I0814 19:05:27.210223   613 solver.cpp:334]     Train net output #0: loss = 0.00192632 (* 1 = 0.00192632 loss)
I0814 19:05:27.210229   613 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0814 19:05:28.845508   613 solver.cpp:312] Iteration 26300 (61.1523 iter/s, 1.63526s/100 iter), loss = 0.0007196
I0814 19:05:28.845556   613 solver.cpp:334]     Train net output #0: loss = 0.000719603 (* 1 = 0.000719603 loss)
I0814 19:05:28.845567   613 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0814 19:05:30.481977   613 solver.cpp:312] Iteration 26400 (61.1091 iter/s, 1.63642s/100 iter), loss = 0.0020366
I0814 19:05:30.482002   613 solver.cpp:334]     Train net output #0: loss = 0.0020366 (* 1 = 0.0020366 loss)
I0814 19:05:30.482008   613 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0814 19:05:32.083915   613 solver.cpp:312] Iteration 26500 (62.4263 iter/s, 1.60189s/100 iter), loss = 0.00100681
I0814 19:05:32.083937   613 solver.cpp:334]     Train net output #0: loss = 0.00100681 (* 1 = 0.00100681 loss)
I0814 19:05:32.083941   613 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0814 19:05:32.298688   592 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:05:33.718333   613 solver.cpp:312] Iteration 26600 (61.1857 iter/s, 1.63437s/100 iter), loss = 0.00245127
I0814 19:05:33.718380   613 solver.cpp:334]     Train net output #0: loss = 0.00245127 (* 1 = 0.00245127 loss)
I0814 19:05:33.718391   613 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0814 19:05:35.332358   613 solver.cpp:312] Iteration 26700 (61.9587 iter/s, 1.61398s/100 iter), loss = 0.000862415
I0814 19:05:35.332404   613 solver.cpp:334]     Train net output #0: loss = 0.000862418 (* 1 = 0.000862418 loss)
I0814 19:05:35.332417   613 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0814 19:05:36.955605   613 solver.cpp:312] Iteration 26800 (61.6068 iter/s, 1.6232s/100 iter), loss = 0.00213367
I0814 19:05:36.955627   613 solver.cpp:334]     Train net output #0: loss = 0.00213367 (* 1 = 0.00213367 loss)
I0814 19:05:36.955632   613 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0814 19:05:38.558969   613 solver.cpp:312] Iteration 26900 (62.3708 iter/s, 1.60331s/100 iter), loss = 0.00146487
I0814 19:05:38.559089   613 solver.cpp:334]     Train net output #0: loss = 0.00146487 (* 1 = 0.00146487 loss)
I0814 19:05:38.559105   613 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0814 19:05:40.185906   613 solver.cpp:509] Iteration 27000, Testing net (#0)
I0814 19:05:41.022527   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.904119
I0814 19:05:41.022547   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:05:41.022552   613 solver.cpp:594]     Test net output #2: loss = 0.35173 (* 1 = 0.35173 loss)
I0814 19:05:41.022570   613 solver.cpp:264] [MultiGPU] Tests completed in 0.836641s
I0814 19:05:41.038179   613 solver.cpp:312] Iteration 27000 (40.3366 iter/s, 2.47914s/100 iter), loss = 0.000517777
I0814 19:05:41.038219   613 solver.cpp:334]     Train net output #0: loss = 0.00051778 (* 1 = 0.00051778 loss)
I0814 19:05:41.038233   613 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0814 19:05:42.643024   613 solver.cpp:312] Iteration 27100 (62.3133 iter/s, 1.60479s/100 iter), loss = 0.000853854
I0814 19:05:42.643051   613 solver.cpp:334]     Train net output #0: loss = 0.000853857 (* 1 = 0.000853857 loss)
I0814 19:05:42.643056   613 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0814 19:05:44.294350   613 solver.cpp:312] Iteration 27200 (60.5592 iter/s, 1.65128s/100 iter), loss = 0.00395417
I0814 19:05:44.294397   613 solver.cpp:334]     Train net output #0: loss = 0.00395417 (* 1 = 0.00395417 loss)
I0814 19:05:44.294409   613 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0814 19:05:45.929885   613 solver.cpp:312] Iteration 27300 (61.144 iter/s, 1.63548s/100 iter), loss = 0.00226241
I0814 19:05:45.929908   613 solver.cpp:334]     Train net output #0: loss = 0.00226241 (* 1 = 0.00226241 loss)
I0814 19:05:45.929913   613 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0814 19:05:47.568505   613 solver.cpp:312] Iteration 27400 (61.0288 iter/s, 1.63857s/100 iter), loss = 0.000639817
I0814 19:05:47.568565   613 solver.cpp:334]     Train net output #0: loss = 0.00063982 (* 1 = 0.00063982 loss)
I0814 19:05:47.568583   613 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0814 19:05:49.195760   613 solver.cpp:312] Iteration 27500 (61.4551 iter/s, 1.6272s/100 iter), loss = 0.00121468
I0814 19:05:49.195827   613 solver.cpp:334]     Train net output #0: loss = 0.00121469 (* 1 = 0.00121469 loss)
I0814 19:05:49.195848   613 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0814 19:05:50.834017   613 solver.cpp:312] Iteration 27600 (61.0423 iter/s, 1.63821s/100 iter), loss = 0.00087925
I0814 19:05:50.834089   613 solver.cpp:334]     Train net output #0: loss = 0.000879252 (* 1 = 0.000879252 loss)
I0814 19:05:50.834115   613 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0814 19:05:52.489537   613 solver.cpp:312] Iteration 27700 (60.4058 iter/s, 1.65547s/100 iter), loss = 0.000493639
I0814 19:05:52.489560   613 solver.cpp:334]     Train net output #0: loss = 0.000493642 (* 1 = 0.000493642 loss)
I0814 19:05:52.489567   613 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0814 19:05:54.103617   613 solver.cpp:312] Iteration 27800 (61.9566 iter/s, 1.61403s/100 iter), loss = 0.00111701
I0814 19:05:54.103642   613 solver.cpp:334]     Train net output #0: loss = 0.00111701 (* 1 = 0.00111701 loss)
I0814 19:05:54.103648   613 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0814 19:05:55.677261   613 solver.cpp:312] Iteration 27900 (63.5487 iter/s, 1.5736s/100 iter), loss = 0.00106433
I0814 19:05:55.677309   613 solver.cpp:334]     Train net output #0: loss = 0.00106433 (* 1 = 0.00106433 loss)
I0814 19:05:55.677325   613 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0814 19:05:57.294170   613 solver.cpp:509] Iteration 28000, Testing net (#0)
I0814 19:05:58.108824   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.900295
I0814 19:05:58.108841   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:05:58.108847   613 solver.cpp:594]     Test net output #2: loss = 0.365981 (* 1 = 0.365981 loss)
I0814 19:05:58.108878   613 solver.cpp:264] [MultiGPU] Tests completed in 0.814686s
I0814 19:05:58.129106   613 solver.cpp:312] Iteration 28000 (40.7868 iter/s, 2.45177s/100 iter), loss = 0.000745689
I0814 19:05:58.129149   613 solver.cpp:334]     Train net output #0: loss = 0.000745693 (* 1 = 0.000745693 loss)
I0814 19:05:58.129160   613 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0814 19:05:59.748340   613 solver.cpp:312] Iteration 28100 (61.7595 iter/s, 1.61918s/100 iter), loss = 0.000732339
I0814 19:05:59.748365   613 solver.cpp:334]     Train net output #0: loss = 0.000732342 (* 1 = 0.000732342 loss)
I0814 19:05:59.748371   613 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0814 19:06:01.421178   613 solver.cpp:312] Iteration 28200 (59.7804 iter/s, 1.67279s/100 iter), loss = 0.000563306
I0814 19:06:01.421206   613 solver.cpp:334]     Train net output #0: loss = 0.000563308 (* 1 = 0.000563308 loss)
I0814 19:06:01.421213   613 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0814 19:06:03.076673   613 solver.cpp:312] Iteration 28300 (60.4068 iter/s, 1.65544s/100 iter), loss = 0.00223918
I0814 19:06:03.076743   613 solver.cpp:334]     Train net output #0: loss = 0.00223918 (* 1 = 0.00223918 loss)
I0814 19:06:03.076764   613 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0814 19:06:04.704172   613 solver.cpp:312] Iteration 28400 (61.4458 iter/s, 1.62745s/100 iter), loss = 0.00152164
I0814 19:06:04.704200   613 solver.cpp:334]     Train net output #0: loss = 0.00152164 (* 1 = 0.00152164 loss)
I0814 19:06:04.704206   613 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0814 19:06:06.317081   613 solver.cpp:312] Iteration 28500 (62.0017 iter/s, 1.61286s/100 iter), loss = 0.0019488
I0814 19:06:06.317104   613 solver.cpp:334]     Train net output #0: loss = 0.0019488 (* 1 = 0.0019488 loss)
I0814 19:06:06.317108   613 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0814 19:06:07.919008   613 solver.cpp:312] Iteration 28600 (62.4268 iter/s, 1.60188s/100 iter), loss = 0.0022939
I0814 19:06:07.919031   613 solver.cpp:334]     Train net output #0: loss = 0.0022939 (* 1 = 0.0022939 loss)
I0814 19:06:07.919039   613 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0814 19:06:09.560844   613 solver.cpp:312] Iteration 28700 (60.9091 iter/s, 1.64179s/100 iter), loss = 0.000315197
I0814 19:06:09.560916   613 solver.cpp:334]     Train net output #0: loss = 0.000315198 (* 1 = 0.000315198 loss)
I0814 19:06:09.560922   613 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0814 19:06:11.188757   613 solver.cpp:312] Iteration 28800 (61.4304 iter/s, 1.62786s/100 iter), loss = 0.00195202
I0814 19:06:11.188781   613 solver.cpp:334]     Train net output #0: loss = 0.00195202 (* 1 = 0.00195202 loss)
I0814 19:06:11.188784   613 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0814 19:06:12.822639   613 solver.cpp:312] Iteration 28900 (61.2056 iter/s, 1.63384s/100 iter), loss = 0.0100725
I0814 19:06:12.822703   613 solver.cpp:334]     Train net output #0: loss = 0.0100725 (* 1 = 0.0100725 loss)
I0814 19:06:12.822723   613 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0814 19:06:14.447767   613 solver.cpp:509] Iteration 29000, Testing net (#0)
I0814 19:06:15.257841   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907354
I0814 19:06:15.257859   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:06:15.257864   613 solver.cpp:594]     Test net output #2: loss = 0.32872 (* 1 = 0.32872 loss)
I0814 19:06:15.257879   613 solver.cpp:264] [MultiGPU] Tests completed in 0.810091s
I0814 19:06:15.273560   613 solver.cpp:312] Iteration 29000 (40.8022 iter/s, 2.45085s/100 iter), loss = 0.000663055
I0814 19:06:15.273576   613 solver.cpp:334]     Train net output #0: loss = 0.000663057 (* 1 = 0.000663057 loss)
I0814 19:06:15.273579   613 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0814 19:06:16.915151   613 solver.cpp:312] Iteration 29100 (60.9184 iter/s, 1.64154s/100 iter), loss = 0.000427868
I0814 19:06:16.915179   613 solver.cpp:334]     Train net output #0: loss = 0.000427868 (* 1 = 0.000427868 loss)
I0814 19:06:16.915184   613 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0814 19:06:18.568418   613 solver.cpp:312] Iteration 29200 (60.488 iter/s, 1.65322s/100 iter), loss = 0.00145948
I0814 19:06:18.568442   613 solver.cpp:334]     Train net output #0: loss = 0.00145948 (* 1 = 0.00145948 loss)
I0814 19:06:18.568446   613 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0814 19:06:20.160897   613 solver.cpp:312] Iteration 29300 (62.7972 iter/s, 1.59243s/100 iter), loss = 0.00129487
I0814 19:06:20.160961   613 solver.cpp:334]     Train net output #0: loss = 0.00129487 (* 1 = 0.00129487 loss)
I0814 19:06:20.160980   613 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0814 19:06:21.750012   613 solver.cpp:312] Iteration 29400 (62.9299 iter/s, 1.58907s/100 iter), loss = 0.00302093
I0814 19:06:21.750058   613 solver.cpp:334]     Train net output #0: loss = 0.00302093 (* 1 = 0.00302093 loss)
I0814 19:06:21.750069   613 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0814 19:06:23.384806   613 solver.cpp:312] Iteration 29500 (61.1717 iter/s, 1.63474s/100 iter), loss = 0.00114148
I0814 19:06:23.384853   613 solver.cpp:334]     Train net output #0: loss = 0.00114148 (* 1 = 0.00114148 loss)
I0814 19:06:23.384866   613 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0814 19:06:25.010274   613 solver.cpp:312] Iteration 29600 (61.5225 iter/s, 1.62542s/100 iter), loss = 0.00158248
I0814 19:06:25.010298   613 solver.cpp:334]     Train net output #0: loss = 0.00158248 (* 1 = 0.00158248 loss)
I0814 19:06:25.010304   613 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0814 19:06:26.611616   613 solver.cpp:312] Iteration 29700 (62.4496 iter/s, 1.60129s/100 iter), loss = 0.0169916
I0814 19:06:26.611675   613 solver.cpp:334]     Train net output #0: loss = 0.0169916 (* 1 = 0.0169916 loss)
I0814 19:06:26.611692   613 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0814 19:06:28.188993   613 solver.cpp:312] Iteration 29800 (63.3983 iter/s, 1.57733s/100 iter), loss = 0.00105313
I0814 19:06:28.189019   613 solver.cpp:334]     Train net output #0: loss = 0.00105313 (* 1 = 0.00105313 loss)
I0814 19:06:28.189025   613 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0814 19:06:29.783895   613 solver.cpp:312] Iteration 29900 (62.7017 iter/s, 1.59485s/100 iter), loss = 0.000953969
I0814 19:06:29.783921   613 solver.cpp:334]     Train net output #0: loss = 0.00095397 (* 1 = 0.00095397 loss)
I0814 19:06:29.783926   613 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0814 19:06:31.417229   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_30000.caffemodel
I0814 19:06:31.426682   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_30000.solverstate
I0814 19:06:31.431435   613 solver.cpp:509] Iteration 30000, Testing net (#0)
I0814 19:06:32.229717   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920001
I0814 19:06:32.229738   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:06:32.229743   613 solver.cpp:594]     Test net output #2: loss = 0.299295 (* 1 = 0.299295 loss)
I0814 19:06:32.229759   613 solver.cpp:264] [MultiGPU] Tests completed in 0.798302s
I0814 19:06:32.246217   613 solver.cpp:312] Iteration 30000 (40.6132 iter/s, 2.46225s/100 iter), loss = 0.0012747
I0814 19:06:32.246232   613 solver.cpp:334]     Train net output #0: loss = 0.0012747 (* 1 = 0.0012747 loss)
I0814 19:06:32.246238   613 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0814 19:06:33.866674   613 solver.cpp:312] Iteration 30100 (61.7129 iter/s, 1.62041s/100 iter), loss = 0.00116765
I0814 19:06:33.866699   613 solver.cpp:334]     Train net output #0: loss = 0.00116766 (* 1 = 0.00116766 loss)
I0814 19:06:33.866705   613 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0814 19:06:35.477090   613 solver.cpp:312] Iteration 30200 (62.0977 iter/s, 1.61037s/100 iter), loss = 0.00232306
I0814 19:06:35.477155   613 solver.cpp:334]     Train net output #0: loss = 0.00232306 (* 1 = 0.00232306 loss)
I0814 19:06:35.477176   613 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0814 19:06:37.088768   613 solver.cpp:312] Iteration 30300 (62.0491 iter/s, 1.61163s/100 iter), loss = 0.0010685
I0814 19:06:37.088795   613 solver.cpp:334]     Train net output #0: loss = 0.0010685 (* 1 = 0.0010685 loss)
I0814 19:06:37.088800   613 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0814 19:06:38.693588   613 solver.cpp:312] Iteration 30400 (62.3142 iter/s, 1.60477s/100 iter), loss = 0.000610738
I0814 19:06:38.693611   613 solver.cpp:334]     Train net output #0: loss = 0.00061074 (* 1 = 0.00061074 loss)
I0814 19:06:38.693616   613 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0814 19:06:40.293716   613 solver.cpp:312] Iteration 30500 (62.4969 iter/s, 1.60008s/100 iter), loss = 0.0022117
I0814 19:06:40.293800   613 solver.cpp:334]     Train net output #0: loss = 0.00221171 (* 1 = 0.00221171 loss)
I0814 19:06:40.293807   613 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0814 19:06:41.916584   613 solver.cpp:312] Iteration 30600 (61.6211 iter/s, 1.62282s/100 iter), loss = 0.0012603
I0814 19:06:41.916728   613 solver.cpp:334]     Train net output #0: loss = 0.0012603 (* 1 = 0.0012603 loss)
I0814 19:06:41.916751   613 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0814 19:06:43.525964   613 solver.cpp:312] Iteration 30700 (62.1377 iter/s, 1.60933s/100 iter), loss = 0.000680323
I0814 19:06:43.526041   613 solver.cpp:334]     Train net output #0: loss = 0.00068033 (* 1 = 0.00068033 loss)
I0814 19:06:43.526068   613 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0814 19:06:45.154989   613 solver.cpp:312] Iteration 30800 (61.3883 iter/s, 1.62897s/100 iter), loss = 0.000910119
I0814 19:06:45.155050   613 solver.cpp:334]     Train net output #0: loss = 0.000910126 (* 1 = 0.000910126 loss)
I0814 19:06:45.155068   613 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0814 19:06:46.769093   613 solver.cpp:312] Iteration 30900 (61.9558 iter/s, 1.61405s/100 iter), loss = 0.00429211
I0814 19:06:46.769119   613 solver.cpp:334]     Train net output #0: loss = 0.00429212 (* 1 = 0.00429212 loss)
I0814 19:06:46.769124   613 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0814 19:06:48.387125   613 solver.cpp:509] Iteration 31000, Testing net (#0)
I0814 19:06:48.672564   611 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:06:49.196532   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.922354
I0814 19:06:49.196552   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:06:49.196557   613 solver.cpp:594]     Test net output #2: loss = 0.281071 (* 1 = 0.281071 loss)
I0814 19:06:49.196573   613 solver.cpp:264] [MultiGPU] Tests completed in 0.809428s
I0814 19:06:49.212363   613 solver.cpp:312] Iteration 31000 (40.9299 iter/s, 2.4432s/100 iter), loss = 0.00109493
I0814 19:06:49.212379   613 solver.cpp:334]     Train net output #0: loss = 0.00109494 (* 1 = 0.00109494 loss)
I0814 19:06:49.212385   613 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0814 19:06:50.834684   613 solver.cpp:312] Iteration 31100 (61.6421 iter/s, 1.62227s/100 iter), loss = 0.00312401
I0814 19:06:50.834731   613 solver.cpp:334]     Train net output #0: loss = 0.00312401 (* 1 = 0.00312401 loss)
I0814 19:06:50.834743   613 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0814 19:06:52.478286   613 solver.cpp:312] Iteration 31200 (60.8439 iter/s, 1.64355s/100 iter), loss = 0.00161411
I0814 19:06:52.478456   613 solver.cpp:334]     Train net output #0: loss = 0.00161412 (* 1 = 0.00161412 loss)
I0814 19:06:52.478466   613 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0814 19:06:54.088609   613 solver.cpp:312] Iteration 31300 (62.1012 iter/s, 1.61028s/100 iter), loss = 0.00232459
I0814 19:06:54.088668   613 solver.cpp:334]     Train net output #0: loss = 0.0023246 (* 1 = 0.0023246 loss)
I0814 19:06:54.088685   613 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0814 19:06:55.722692   613 solver.cpp:312] Iteration 31400 (61.1983 iter/s, 1.63403s/100 iter), loss = 0.00221242
I0814 19:06:55.722736   613 solver.cpp:334]     Train net output #0: loss = 0.00221243 (* 1 = 0.00221243 loss)
I0814 19:06:55.722748   613 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0814 19:06:57.333865   613 solver.cpp:312] Iteration 31500 (62.0684 iter/s, 1.61113s/100 iter), loss = 0.00101202
I0814 19:06:57.333892   613 solver.cpp:334]     Train net output #0: loss = 0.00101203 (* 1 = 0.00101203 loss)
I0814 19:06:57.333899   613 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0814 19:06:58.931884   613 solver.cpp:312] Iteration 31600 (62.5795 iter/s, 1.59797s/100 iter), loss = 0.00398866
I0814 19:06:58.931910   613 solver.cpp:334]     Train net output #0: loss = 0.00398866 (* 1 = 0.00398866 loss)
I0814 19:06:58.931933   613 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0814 19:07:00.527154   613 solver.cpp:312] Iteration 31700 (62.6872 iter/s, 1.59522s/100 iter), loss = 0.000412395
I0814 19:07:00.527204   613 solver.cpp:334]     Train net output #0: loss = 0.000412401 (* 1 = 0.000412401 loss)
I0814 19:07:00.527217   613 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0814 19:07:02.133250   613 solver.cpp:312] Iteration 31800 (62.2646 iter/s, 1.60605s/100 iter), loss = 0.0015609
I0814 19:07:02.133296   613 solver.cpp:334]     Train net output #0: loss = 0.00156091 (* 1 = 0.00156091 loss)
I0814 19:07:02.133307   613 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0814 19:07:03.730506   613 solver.cpp:312] Iteration 31900 (62.6092 iter/s, 1.59721s/100 iter), loss = 0.00103219
I0814 19:07:03.730531   613 solver.cpp:334]     Train net output #0: loss = 0.00103219 (* 1 = 0.00103219 loss)
I0814 19:07:03.730537   613 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0814 19:07:05.354261   613 solver.cpp:509] Iteration 32000, Testing net (#0)
I0814 19:07:06.167038   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.908825
I0814 19:07:06.167057   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:07:06.167062   613 solver.cpp:594]     Test net output #2: loss = 0.334485 (* 1 = 0.334485 loss)
I0814 19:07:06.167076   613 solver.cpp:264] [MultiGPU] Tests completed in 0.812793s
I0814 19:07:06.182662   613 solver.cpp:312] Iteration 32000 (40.7816 iter/s, 2.45209s/100 iter), loss = 0.00355713
I0814 19:07:06.182695   613 solver.cpp:334]     Train net output #0: loss = 0.00355714 (* 1 = 0.00355714 loss)
I0814 19:07:06.182708   613 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0814 19:07:07.782886   613 solver.cpp:312] Iteration 32100 (62.4932 iter/s, 1.60017s/100 iter), loss = 0.00259734
I0814 19:07:07.782934   613 solver.cpp:334]     Train net output #0: loss = 0.00259735 (* 1 = 0.00259735 loss)
I0814 19:07:07.782948   613 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0814 19:07:09.398686   613 solver.cpp:312] Iteration 32200 (61.8908 iter/s, 1.61575s/100 iter), loss = 0.00564189
I0814 19:07:09.398715   613 solver.cpp:334]     Train net output #0: loss = 0.00564189 (* 1 = 0.00564189 loss)
I0814 19:07:09.398721   613 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0814 19:07:11.039486   613 solver.cpp:312] Iteration 32300 (60.9477 iter/s, 1.64075s/100 iter), loss = 0.00361107
I0814 19:07:11.039592   613 solver.cpp:334]     Train net output #0: loss = 0.00361107 (* 1 = 0.00361107 loss)
I0814 19:07:11.039611   613 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0814 19:07:12.643287   613 solver.cpp:312] Iteration 32400 (62.3538 iter/s, 1.60375s/100 iter), loss = 0.0013609
I0814 19:07:12.643352   613 solver.cpp:334]     Train net output #0: loss = 0.0013609 (* 1 = 0.0013609 loss)
I0814 19:07:12.643370   613 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0814 19:07:14.302295   613 solver.cpp:312] Iteration 32500 (60.2788 iter/s, 1.65896s/100 iter), loss = 0.000625696
I0814 19:07:14.302325   613 solver.cpp:334]     Train net output #0: loss = 0.000625699 (* 1 = 0.000625699 loss)
I0814 19:07:14.302331   613 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0814 19:07:15.904858   613 solver.cpp:312] Iteration 32600 (62.4019 iter/s, 1.60252s/100 iter), loss = 0.0044096
I0814 19:07:15.904927   613 solver.cpp:334]     Train net output #0: loss = 0.0044096 (* 1 = 0.0044096 loss)
I0814 19:07:15.904947   613 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0814 19:07:17.558605   613 solver.cpp:312] Iteration 32700 (60.4706 iter/s, 1.6537s/100 iter), loss = 0.000515654
I0814 19:07:17.558658   613 solver.cpp:334]     Train net output #0: loss = 0.000515655 (* 1 = 0.000515655 loss)
I0814 19:07:17.558671   613 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0814 19:07:19.188149   613 solver.cpp:312] Iteration 32800 (61.3688 iter/s, 1.62949s/100 iter), loss = 0.00848426
I0814 19:07:19.188208   613 solver.cpp:334]     Train net output #0: loss = 0.00848426 (* 1 = 0.00848426 loss)
I0814 19:07:19.188227   613 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0814 19:07:20.784354   613 solver.cpp:312] Iteration 32900 (62.6504 iter/s, 1.59616s/100 iter), loss = 0.102276
I0814 19:07:20.784415   613 solver.cpp:334]     Train net output #0: loss = 0.102276 (* 1 = 0.102276 loss)
I0814 19:07:20.784433   613 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0814 19:07:22.357062   613 solver.cpp:509] Iteration 33000, Testing net (#0)
I0814 19:07:23.175706   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.735295
I0814 19:07:23.175721   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.965589
I0814 19:07:23.175726   613 solver.cpp:594]     Test net output #2: loss = 1.77748 (* 1 = 1.77748 loss)
I0814 19:07:23.175752   613 solver.cpp:264] [MultiGPU] Tests completed in 0.818661s
I0814 19:07:23.191292   613 solver.cpp:312] Iteration 33000 (41.5478 iter/s, 2.40687s/100 iter), loss = 0.206587
I0814 19:07:23.191314   613 solver.cpp:334]     Train net output #0: loss = 0.206587 (* 1 = 0.206587 loss)
I0814 19:07:23.191321   613 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0814 19:07:24.834992   613 solver.cpp:312] Iteration 33100 (60.8402 iter/s, 1.64365s/100 iter), loss = 0.0545392
I0814 19:07:24.835052   613 solver.cpp:334]     Train net output #0: loss = 0.0545393 (* 1 = 0.0545393 loss)
I0814 19:07:24.835077   613 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0814 19:07:26.429656   613 solver.cpp:312] Iteration 33200 (62.7111 iter/s, 1.59461s/100 iter), loss = 0.0138729
I0814 19:07:26.429680   613 solver.cpp:334]     Train net output #0: loss = 0.0138729 (* 1 = 0.0138729 loss)
I0814 19:07:26.429687   613 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0814 19:07:28.091325   613 solver.cpp:312] Iteration 33300 (60.1823 iter/s, 1.66162s/100 iter), loss = 0.260078
I0814 19:07:28.091385   613 solver.cpp:334]     Train net output #0: loss = 0.260078 (* 1 = 0.260078 loss)
I0814 19:07:28.091403   613 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0814 19:07:29.700553   613 solver.cpp:312] Iteration 33400 (62.1434 iter/s, 1.60918s/100 iter), loss = 0.0040168
I0814 19:07:29.700577   613 solver.cpp:334]     Train net output #0: loss = 0.00401665 (* 1 = 0.00401665 loss)
I0814 19:07:29.700583   613 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0814 19:07:31.310570   613 solver.cpp:312] Iteration 33500 (62.1129 iter/s, 1.60997s/100 iter), loss = 0.195826
I0814 19:07:31.310592   613 solver.cpp:334]     Train net output #0: loss = 0.195826 (* 1 = 0.195826 loss)
I0814 19:07:31.310598   613 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0814 19:07:32.894371   613 solver.cpp:312] Iteration 33600 (63.1413 iter/s, 1.58375s/100 iter), loss = 0.0206433
I0814 19:07:32.894418   613 solver.cpp:334]     Train net output #0: loss = 0.0206433 (* 1 = 0.0206433 loss)
I0814 19:07:32.894429   613 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0814 19:07:34.499771   613 solver.cpp:312] Iteration 33700 (62.2916 iter/s, 1.60535s/100 iter), loss = 0.0796808
I0814 19:07:34.499833   613 solver.cpp:334]     Train net output #0: loss = 0.0796808 (* 1 = 0.0796808 loss)
I0814 19:07:34.499851   613 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0814 19:07:36.119072   613 solver.cpp:312] Iteration 33800 (61.7569 iter/s, 1.61925s/100 iter), loss = 0.0639974
I0814 19:07:36.119134   613 solver.cpp:334]     Train net output #0: loss = 0.0639974 (* 1 = 0.0639974 loss)
I0814 19:07:36.119151   613 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0814 19:07:37.738488   613 solver.cpp:312] Iteration 33900 (61.7525 iter/s, 1.61937s/100 iter), loss = 0.0214644
I0814 19:07:37.738549   613 solver.cpp:334]     Train net output #0: loss = 0.0214644 (* 1 = 0.0214644 loss)
I0814 19:07:37.738569   613 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0814 19:07:39.284940   613 solver.cpp:509] Iteration 34000, Testing net (#0)
I0814 19:07:40.104893   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.806178
I0814 19:07:40.104912   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.987059
I0814 19:07:40.104918   613 solver.cpp:594]     Test net output #2: loss = 0.77042 (* 1 = 0.77042 loss)
I0814 19:07:40.104935   613 solver.cpp:264] [MultiGPU] Tests completed in 0.819974s
I0814 19:07:40.120582   613 solver.cpp:312] Iteration 34000 (41.9811 iter/s, 2.38202s/100 iter), loss = 0.0285851
I0814 19:07:40.120615   613 solver.cpp:334]     Train net output #0: loss = 0.0285851 (* 1 = 0.0285851 loss)
I0814 19:07:40.120625   613 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0814 19:07:41.728358   613 solver.cpp:312] Iteration 34100 (62.1997 iter/s, 1.60772s/100 iter), loss = 0.0332484
I0814 19:07:41.728474   613 solver.cpp:334]     Train net output #0: loss = 0.0332484 (* 1 = 0.0332484 loss)
I0814 19:07:41.728492   613 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0814 19:07:43.343317   613 solver.cpp:312] Iteration 34200 (61.923 iter/s, 1.61491s/100 iter), loss = 0.238849
I0814 19:07:43.343364   613 solver.cpp:334]     Train net output #0: loss = 0.238849 (* 1 = 0.238849 loss)
I0814 19:07:43.343375   613 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0814 19:07:44.953410   613 solver.cpp:312] Iteration 34300 (62.1101 iter/s, 1.61005s/100 iter), loss = 0.0479735
I0814 19:07:44.953434   613 solver.cpp:334]     Train net output #0: loss = 0.0479734 (* 1 = 0.0479734 loss)
I0814 19:07:44.953440   613 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0814 19:07:46.582744   613 solver.cpp:312] Iteration 34400 (61.3767 iter/s, 1.62928s/100 iter), loss = 0.0742773
I0814 19:07:46.582768   613 solver.cpp:334]     Train net output #0: loss = 0.0742772 (* 1 = 0.0742772 loss)
I0814 19:07:46.582774   613 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0814 19:07:48.177839   613 solver.cpp:312] Iteration 34500 (62.6941 iter/s, 1.59505s/100 iter), loss = 0.0499547
I0814 19:07:48.177861   613 solver.cpp:334]     Train net output #0: loss = 0.0499546 (* 1 = 0.0499546 loss)
I0814 19:07:48.177866   613 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0814 19:07:49.823281   613 solver.cpp:312] Iteration 34600 (60.7759 iter/s, 1.64539s/100 iter), loss = 0.0116858
I0814 19:07:49.823338   613 solver.cpp:334]     Train net output #0: loss = 0.0116858 (* 1 = 0.0116858 loss)
I0814 19:07:49.823357   613 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0814 19:07:51.492287   613 solver.cpp:312] Iteration 34700 (59.9177 iter/s, 1.66896s/100 iter), loss = 0.0527371
I0814 19:07:51.492352   613 solver.cpp:334]     Train net output #0: loss = 0.052737 (* 1 = 0.052737 loss)
I0814 19:07:51.492372   613 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0814 19:07:53.117347   613 solver.cpp:312] Iteration 34800 (61.538 iter/s, 1.62501s/100 iter), loss = 0.0356135
I0814 19:07:53.117370   613 solver.cpp:334]     Train net output #0: loss = 0.0356134 (* 1 = 0.0356134 loss)
I0814 19:07:53.117377   613 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0814 19:07:54.706791   613 solver.cpp:312] Iteration 34900 (62.917 iter/s, 1.5894s/100 iter), loss = 0.0897899
I0814 19:07:54.706822   613 solver.cpp:334]     Train net output #0: loss = 0.0897899 (* 1 = 0.0897899 loss)
I0814 19:07:54.706830   613 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0814 19:07:56.313321   613 solver.cpp:509] Iteration 35000, Testing net (#0)
I0814 19:07:56.548751   611 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:07:57.148259   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.881766
I0814 19:07:57.148282   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992059
I0814 19:07:57.148288   613 solver.cpp:594]     Test net output #2: loss = 0.420559 (* 1 = 0.420559 loss)
I0814 19:07:57.148305   613 solver.cpp:264] [MultiGPU] Tests completed in 0.834962s
I0814 19:07:57.163691   613 solver.cpp:312] Iteration 35000 (40.7029 iter/s, 2.45683s/100 iter), loss = 0.143601
I0814 19:07:57.163720   613 solver.cpp:334]     Train net output #0: loss = 0.143601 (* 1 = 0.143601 loss)
I0814 19:07:57.163733   613 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0814 19:07:58.734797   613 solver.cpp:312] Iteration 35100 (63.6515 iter/s, 1.57105s/100 iter), loss = 0.105247
I0814 19:07:58.734822   613 solver.cpp:334]     Train net output #0: loss = 0.105247 (* 1 = 0.105247 loss)
I0814 19:07:58.734828   613 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0814 19:08:00.345036   613 solver.cpp:312] Iteration 35200 (62.1045 iter/s, 1.61019s/100 iter), loss = 0.0268567
I0814 19:08:00.345063   613 solver.cpp:334]     Train net output #0: loss = 0.0268567 (* 1 = 0.0268567 loss)
I0814 19:08:00.345070   613 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0814 19:08:02.004587   613 solver.cpp:312] Iteration 35300 (60.2591 iter/s, 1.6595s/100 iter), loss = 0.0032801
I0814 19:08:02.004799   613 solver.cpp:334]     Train net output #0: loss = 0.00328004 (* 1 = 0.00328004 loss)
I0814 19:08:02.004809   613 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0814 19:08:03.632575   613 solver.cpp:312] Iteration 35400 (61.4274 iter/s, 1.62794s/100 iter), loss = 0.0127292
I0814 19:08:03.632602   613 solver.cpp:334]     Train net output #0: loss = 0.0127291 (* 1 = 0.0127291 loss)
I0814 19:08:03.632609   613 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0814 19:08:05.254626   613 solver.cpp:312] Iteration 35500 (61.6522 iter/s, 1.622s/100 iter), loss = 0.0331928
I0814 19:08:05.254675   613 solver.cpp:334]     Train net output #0: loss = 0.0331927 (* 1 = 0.0331927 loss)
I0814 19:08:05.254691   613 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0814 19:08:06.887248   613 solver.cpp:312] Iteration 35600 (61.253 iter/s, 1.63257s/100 iter), loss = 0.0484342
I0814 19:08:06.887272   613 solver.cpp:334]     Train net output #0: loss = 0.0484341 (* 1 = 0.0484341 loss)
I0814 19:08:06.887277   613 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0814 19:08:08.542246   613 solver.cpp:312] Iteration 35700 (60.425 iter/s, 1.65494s/100 iter), loss = 0.033302
I0814 19:08:08.542317   613 solver.cpp:334]     Train net output #0: loss = 0.0333018 (* 1 = 0.0333018 loss)
I0814 19:08:08.542340   613 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0814 19:08:10.188094   613 solver.cpp:312] Iteration 35800 (60.7607 iter/s, 1.6458s/100 iter), loss = 0.206288
I0814 19:08:10.188144   613 solver.cpp:334]     Train net output #0: loss = 0.206288 (* 1 = 0.206288 loss)
I0814 19:08:10.188158   613 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0814 19:08:11.757380   613 solver.cpp:312] Iteration 35900 (63.7253 iter/s, 1.56924s/100 iter), loss = 0.0475682
I0814 19:08:11.757503   613 solver.cpp:334]     Train net output #0: loss = 0.0475681 (* 1 = 0.0475681 loss)
I0814 19:08:11.757521   613 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0814 19:08:13.366859   613 solver.cpp:509] Iteration 36000, Testing net (#0)
I0814 19:08:14.179399   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.894119
I0814 19:08:14.179417   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993235
I0814 19:08:14.179422   613 solver.cpp:594]     Test net output #2: loss = 0.37666 (* 1 = 0.37666 loss)
I0814 19:08:14.179446   613 solver.cpp:264] [MultiGPU] Tests completed in 0.812563s
I0814 19:08:14.197743   613 solver.cpp:312] Iteration 36000 (40.9787 iter/s, 2.44029s/100 iter), loss = 0.00646078
I0814 19:08:14.197970   613 solver.cpp:334]     Train net output #0: loss = 0.00646061 (* 1 = 0.00646061 loss)
I0814 19:08:14.197984   613 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0814 19:08:15.810365   613 solver.cpp:312] Iteration 36100 (62.0128 iter/s, 1.61257s/100 iter), loss = 0.00177366
I0814 19:08:15.810392   613 solver.cpp:334]     Train net output #0: loss = 0.00177349 (* 1 = 0.00177349 loss)
I0814 19:08:15.810398   613 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0814 19:08:17.455663   613 solver.cpp:312] Iteration 36200 (60.7811 iter/s, 1.64525s/100 iter), loss = 0.0215986
I0814 19:08:17.455710   613 solver.cpp:334]     Train net output #0: loss = 0.0215984 (* 1 = 0.0215984 loss)
I0814 19:08:17.455723   613 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0814 19:08:19.059888   613 solver.cpp:312] Iteration 36300 (62.3373 iter/s, 1.60417s/100 iter), loss = 0.00072427
I0814 19:08:19.059937   613 solver.cpp:334]     Train net output #0: loss = 0.000724099 (* 1 = 0.000724099 loss)
I0814 19:08:19.059952   613 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0814 19:08:20.647841   613 solver.cpp:312] Iteration 36400 (62.9761 iter/s, 1.5879s/100 iter), loss = 0.00117935
I0814 19:08:20.647869   613 solver.cpp:334]     Train net output #0: loss = 0.0011792 (* 1 = 0.0011792 loss)
I0814 19:08:20.647874   613 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0814 19:08:22.253224   613 solver.cpp:312] Iteration 36500 (62.2923 iter/s, 1.60533s/100 iter), loss = 0.0838403
I0814 19:08:22.253252   613 solver.cpp:334]     Train net output #0: loss = 0.0838401 (* 1 = 0.0838401 loss)
I0814 19:08:22.253257   613 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0814 19:08:23.874516   613 solver.cpp:312] Iteration 36600 (61.6811 iter/s, 1.62124s/100 iter), loss = 0.016702
I0814 19:08:23.874590   613 solver.cpp:334]     Train net output #0: loss = 0.0167019 (* 1 = 0.0167019 loss)
I0814 19:08:23.874613   613 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0814 19:08:25.491875   613 solver.cpp:312] Iteration 36700 (61.831 iter/s, 1.61731s/100 iter), loss = 0.057909
I0814 19:08:25.491900   613 solver.cpp:334]     Train net output #0: loss = 0.0579088 (* 1 = 0.0579088 loss)
I0814 19:08:25.491906   613 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0814 19:08:27.119096   613 solver.cpp:312] Iteration 36800 (61.4563 iter/s, 1.62717s/100 iter), loss = 0.00279752
I0814 19:08:27.119123   613 solver.cpp:334]     Train net output #0: loss = 0.00279735 (* 1 = 0.00279735 loss)
I0814 19:08:27.119128   613 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0814 19:08:28.737275   613 solver.cpp:312] Iteration 36900 (61.7998 iter/s, 1.61813s/100 iter), loss = 0.0050897
I0814 19:08:28.737321   613 solver.cpp:334]     Train net output #0: loss = 0.00508954 (* 1 = 0.00508954 loss)
I0814 19:08:28.737332   613 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0814 19:08:30.363559   613 solver.cpp:509] Iteration 37000, Testing net (#0)
I0814 19:08:31.175101   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.888825
I0814 19:08:31.175118   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.993824
I0814 19:08:31.175123   613 solver.cpp:594]     Test net output #2: loss = 0.460336 (* 1 = 0.460336 loss)
I0814 19:08:31.175150   613 solver.cpp:264] [MultiGPU] Tests completed in 0.811569s
I0814 19:08:31.190873   613 solver.cpp:312] Iteration 37000 (40.7576 iter/s, 2.45353s/100 iter), loss = 0.0206861
I0814 19:08:31.190891   613 solver.cpp:334]     Train net output #0: loss = 0.020686 (* 1 = 0.020686 loss)
I0814 19:08:31.190896   613 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0814 19:08:32.804273   613 solver.cpp:312] Iteration 37100 (61.9828 iter/s, 1.61335s/100 iter), loss = 0.000446452
I0814 19:08:32.804296   613 solver.cpp:334]     Train net output #0: loss = 0.000446295 (* 1 = 0.000446295 loss)
I0814 19:08:32.804301   613 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0814 19:08:34.452745   613 solver.cpp:312] Iteration 37200 (60.6642 iter/s, 1.64842s/100 iter), loss = 0.0076685
I0814 19:08:34.452767   613 solver.cpp:334]     Train net output #0: loss = 0.00766833 (* 1 = 0.00766833 loss)
I0814 19:08:34.452771   613 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0814 19:08:36.075112   613 solver.cpp:312] Iteration 37300 (61.6402 iter/s, 1.62232s/100 iter), loss = 0.00514031
I0814 19:08:36.075143   613 solver.cpp:334]     Train net output #0: loss = 0.00514014 (* 1 = 0.00514014 loss)
I0814 19:08:36.075150   613 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0814 19:08:37.659490   613 solver.cpp:312] Iteration 37400 (63.1182 iter/s, 1.58433s/100 iter), loss = 0.00871886
I0814 19:08:37.659517   613 solver.cpp:334]     Train net output #0: loss = 0.0087187 (* 1 = 0.0087187 loss)
I0814 19:08:37.659523   613 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0814 19:08:39.293591   613 solver.cpp:312] Iteration 37500 (61.1975 iter/s, 1.63405s/100 iter), loss = 0.00666315
I0814 19:08:39.293649   613 solver.cpp:334]     Train net output #0: loss = 0.00666298 (* 1 = 0.00666298 loss)
I0814 19:08:39.293668   613 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0814 19:08:40.891549   613 solver.cpp:312] Iteration 37600 (62.5818 iter/s, 1.59791s/100 iter), loss = 0.0070395
I0814 19:08:40.891610   613 solver.cpp:334]     Train net output #0: loss = 0.00703934 (* 1 = 0.00703934 loss)
I0814 19:08:40.891628   613 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0814 19:08:42.568646   613 solver.cpp:312] Iteration 37700 (59.6286 iter/s, 1.67705s/100 iter), loss = 0.00196609
I0814 19:08:42.568769   613 solver.cpp:334]     Train net output #0: loss = 0.00196591 (* 1 = 0.00196591 loss)
I0814 19:08:42.568790   613 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0814 19:08:44.150415   613 solver.cpp:312] Iteration 37800 (63.2223 iter/s, 1.58172s/100 iter), loss = 0.00154227
I0814 19:08:44.150439   613 solver.cpp:334]     Train net output #0: loss = 0.00154208 (* 1 = 0.00154208 loss)
I0814 19:08:44.150444   613 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0814 19:08:45.748406   613 solver.cpp:312] Iteration 37900 (62.5805 iter/s, 1.59794s/100 iter), loss = 0.0196866
I0814 19:08:45.748431   613 solver.cpp:334]     Train net output #0: loss = 0.0196864 (* 1 = 0.0196864 loss)
I0814 19:08:45.748437   613 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0814 19:08:47.365270   613 solver.cpp:509] Iteration 38000, Testing net (#0)
I0814 19:08:48.175565   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.898236
I0814 19:08:48.175585   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:08:48.175590   613 solver.cpp:594]     Test net output #2: loss = 0.406222 (* 1 = 0.406222 loss)
I0814 19:08:48.175608   613 solver.cpp:264] [MultiGPU] Tests completed in 0.810315s
I0814 19:08:48.191131   613 solver.cpp:312] Iteration 38000 (40.939 iter/s, 2.44266s/100 iter), loss = 0.00469881
I0814 19:08:48.191149   613 solver.cpp:334]     Train net output #0: loss = 0.00469862 (* 1 = 0.00469862 loss)
I0814 19:08:48.191154   613 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0814 19:08:49.797472   613 solver.cpp:312] Iteration 38100 (62.2552 iter/s, 1.60629s/100 iter), loss = 0.00378725
I0814 19:08:49.797497   613 solver.cpp:334]     Train net output #0: loss = 0.00378706 (* 1 = 0.00378706 loss)
I0814 19:08:49.797503   613 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0814 19:08:51.406745   613 solver.cpp:312] Iteration 38200 (62.1417 iter/s, 1.60922s/100 iter), loss = 0.00249077
I0814 19:08:51.406806   613 solver.cpp:334]     Train net output #0: loss = 0.00249056 (* 1 = 0.00249056 loss)
I0814 19:08:51.406824   613 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0814 19:08:53.004896   613 solver.cpp:312] Iteration 38300 (62.5744 iter/s, 1.5981s/100 iter), loss = 0.00416665
I0814 19:08:53.004921   613 solver.cpp:334]     Train net output #0: loss = 0.00416645 (* 1 = 0.00416645 loss)
I0814 19:08:53.004927   613 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0814 19:08:54.606511   613 solver.cpp:312] Iteration 38400 (62.4388 iter/s, 1.60157s/100 iter), loss = 0.000731407
I0814 19:08:54.606575   613 solver.cpp:334]     Train net output #0: loss = 0.000731209 (* 1 = 0.000731209 loss)
I0814 19:08:54.606595   613 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0814 19:08:56.242810   613 solver.cpp:312] Iteration 38500 (61.1153 iter/s, 1.63625s/100 iter), loss = 0.12351
I0814 19:08:56.242832   613 solver.cpp:334]     Train net output #0: loss = 0.12351 (* 1 = 0.12351 loss)
I0814 19:08:56.242836   613 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0814 19:08:57.876981   613 solver.cpp:312] Iteration 38600 (61.1951 iter/s, 1.63412s/100 iter), loss = 0.00438004
I0814 19:08:57.877005   613 solver.cpp:334]     Train net output #0: loss = 0.00437984 (* 1 = 0.00437984 loss)
I0814 19:08:57.877012   613 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0814 19:08:59.442773   613 solver.cpp:312] Iteration 38700 (63.8674 iter/s, 1.56575s/100 iter), loss = 0.00191789
I0814 19:08:59.442823   613 solver.cpp:334]     Train net output #0: loss = 0.00191769 (* 1 = 0.00191769 loss)
I0814 19:08:59.442837   613 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0814 19:09:01.075417   613 solver.cpp:312] Iteration 38800 (61.2525 iter/s, 1.63259s/100 iter), loss = 0.00138349
I0814 19:09:01.075462   613 solver.cpp:334]     Train net output #0: loss = 0.00138329 (* 1 = 0.00138329 loss)
I0814 19:09:01.075471   613 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0814 19:09:02.706985   613 solver.cpp:312] Iteration 38900 (61.2923 iter/s, 1.63153s/100 iter), loss = 0.00213671
I0814 19:09:02.707010   613 solver.cpp:334]     Train net output #0: loss = 0.00213652 (* 1 = 0.00213652 loss)
I0814 19:09:02.707015   613 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0814 19:09:04.290182   613 solver.cpp:509] Iteration 39000, Testing net (#0)
I0814 19:09:05.100741   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902648
I0814 19:09:05.100759   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995294
I0814 19:09:05.100764   613 solver.cpp:594]     Test net output #2: loss = 0.379448 (* 1 = 0.379448 loss)
I0814 19:09:05.100838   613 solver.cpp:264] [MultiGPU] Tests completed in 0.810633s
I0814 19:09:05.119067   613 solver.cpp:312] Iteration 39000 (41.4593 iter/s, 2.41201s/100 iter), loss = 0.00151517
I0814 19:09:05.119108   613 solver.cpp:334]     Train net output #0: loss = 0.00151498 (* 1 = 0.00151498 loss)
I0814 19:09:05.119117   613 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0814 19:09:06.733034   613 solver.cpp:312] Iteration 39100 (61.9609 iter/s, 1.61392s/100 iter), loss = 0.00159203
I0814 19:09:06.733062   613 solver.cpp:334]     Train net output #0: loss = 0.00159184 (* 1 = 0.00159184 loss)
I0814 19:09:06.733067   613 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0814 19:09:08.344581   613 solver.cpp:312] Iteration 39200 (62.0541 iter/s, 1.6115s/100 iter), loss = 0.00267104
I0814 19:09:08.344604   613 solver.cpp:334]     Train net output #0: loss = 0.00267085 (* 1 = 0.00267085 loss)
I0814 19:09:08.344609   613 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0814 19:09:09.959564   613 solver.cpp:312] Iteration 39300 (61.922 iter/s, 1.61494s/100 iter), loss = 0.00185432
I0814 19:09:09.959628   613 solver.cpp:334]     Train net output #0: loss = 0.00185412 (* 1 = 0.00185412 loss)
I0814 19:09:09.959648   613 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0814 19:09:11.413118   592 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:09:11.582645   613 solver.cpp:312] Iteration 39400 (61.6132 iter/s, 1.62303s/100 iter), loss = 0.0383216
I0814 19:09:11.582705   613 solver.cpp:334]     Train net output #0: loss = 0.0383214 (* 1 = 0.0383214 loss)
I0814 19:09:11.582722   613 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0814 19:09:13.171697   613 solver.cpp:312] Iteration 39500 (62.9325 iter/s, 1.589s/100 iter), loss = 0.000563638
I0814 19:09:13.171778   613 solver.cpp:334]     Train net output #0: loss = 0.000563439 (* 1 = 0.000563439 loss)
I0814 19:09:13.171790   613 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0814 19:09:14.798892   613 solver.cpp:312] Iteration 39600 (61.4573 iter/s, 1.62715s/100 iter), loss = 0.00294112
I0814 19:09:14.798936   613 solver.cpp:334]     Train net output #0: loss = 0.00294092 (* 1 = 0.00294092 loss)
I0814 19:09:14.798949   613 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0814 19:09:16.394758   613 solver.cpp:312] Iteration 39700 (62.6638 iter/s, 1.59582s/100 iter), loss = 0.00109256
I0814 19:09:16.394783   613 solver.cpp:334]     Train net output #0: loss = 0.00109237 (* 1 = 0.00109237 loss)
I0814 19:09:16.394788   613 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0814 19:09:18.014926   613 solver.cpp:312] Iteration 39800 (61.7239 iter/s, 1.62012s/100 iter), loss = 0.00394184
I0814 19:09:18.014988   613 solver.cpp:334]     Train net output #0: loss = 0.00394164 (* 1 = 0.00394164 loss)
I0814 19:09:18.015008   613 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0814 19:09:19.621235   613 solver.cpp:312] Iteration 39900 (62.2564 iter/s, 1.60626s/100 iter), loss = 0.000960186
I0814 19:09:19.621297   613 solver.cpp:334]     Train net output #0: loss = 0.000959984 (* 1 = 0.000959984 loss)
I0814 19:09:19.621315   613 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0814 19:09:21.243870   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_40000.caffemodel
I0814 19:09:21.251791   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_40000.solverstate
I0814 19:09:21.255410   613 solver.cpp:509] Iteration 40000, Testing net (#0)
I0814 19:09:22.057953   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902942
I0814 19:09:22.057971   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:09:22.057977   613 solver.cpp:594]     Test net output #2: loss = 0.358747 (* 1 = 0.358747 loss)
I0814 19:09:22.058028   613 solver.cpp:264] [MultiGPU] Tests completed in 0.802596s
I0814 19:09:22.073787   613 solver.cpp:312] Iteration 40000 (40.775 iter/s, 2.45248s/100 iter), loss = 0.000631083
I0814 19:09:22.073804   613 solver.cpp:334]     Train net output #0: loss = 0.000630882 (* 1 = 0.000630882 loss)
I0814 19:09:22.073810   613 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0814 19:09:23.695361   613 solver.cpp:312] Iteration 40100 (61.6705 iter/s, 1.62152s/100 iter), loss = 0.00163009
I0814 19:09:23.695410   613 solver.cpp:334]     Train net output #0: loss = 0.00162989 (* 1 = 0.00162989 loss)
I0814 19:09:23.695423   613 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0814 19:09:25.315073   613 solver.cpp:312] Iteration 40200 (61.7412 iter/s, 1.61966s/100 iter), loss = 0.000181896
I0814 19:09:25.315099   613 solver.cpp:334]     Train net output #0: loss = 0.000181699 (* 1 = 0.000181699 loss)
I0814 19:09:25.315104   613 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0814 19:09:26.943068   613 solver.cpp:312] Iteration 40300 (61.4271 iter/s, 1.62795s/100 iter), loss = 0.00129886
I0814 19:09:26.943119   613 solver.cpp:334]     Train net output #0: loss = 0.00129866 (* 1 = 0.00129866 loss)
I0814 19:09:26.943132   613 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0814 19:09:28.563990   613 solver.cpp:312] Iteration 40400 (61.6952 iter/s, 1.62087s/100 iter), loss = 0.00357933
I0814 19:09:28.564015   613 solver.cpp:334]     Train net output #0: loss = 0.00357914 (* 1 = 0.00357914 loss)
I0814 19:09:28.564020   613 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0814 19:09:30.172710   613 solver.cpp:312] Iteration 40500 (62.1631 iter/s, 1.60867s/100 iter), loss = 0.0020413
I0814 19:09:30.172735   613 solver.cpp:334]     Train net output #0: loss = 0.00204111 (* 1 = 0.00204111 loss)
I0814 19:09:30.172757   613 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0814 19:09:31.766396   613 solver.cpp:312] Iteration 40600 (62.7497 iter/s, 1.59363s/100 iter), loss = 0.000145992
I0814 19:09:31.766536   613 solver.cpp:334]     Train net output #0: loss = 0.000145805 (* 1 = 0.000145805 loss)
I0814 19:09:31.766554   613 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0814 19:09:33.400741   613 solver.cpp:312] Iteration 40700 (61.1889 iter/s, 1.63428s/100 iter), loss = 0.00343533
I0814 19:09:33.400842   613 solver.cpp:334]     Train net output #0: loss = 0.00343514 (* 1 = 0.00343514 loss)
I0814 19:09:33.400879   613 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0814 19:09:35.022871   613 solver.cpp:312] Iteration 40800 (61.6492 iter/s, 1.62208s/100 iter), loss = 0.00113448
I0814 19:09:35.022927   613 solver.cpp:334]     Train net output #0: loss = 0.00113429 (* 1 = 0.00113429 loss)
I0814 19:09:35.022945   613 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0814 19:09:36.647507   613 solver.cpp:312] Iteration 40900 (61.554 iter/s, 1.62459s/100 iter), loss = 0.00135423
I0814 19:09:36.647572   613 solver.cpp:334]     Train net output #0: loss = 0.00135403 (* 1 = 0.00135403 loss)
I0814 19:09:36.647591   613 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0814 19:09:38.236651   613 solver.cpp:509] Iteration 41000, Testing net (#0)
I0814 19:09:39.055471   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906177
I0814 19:09:39.055492   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:09:39.055500   613 solver.cpp:594]     Test net output #2: loss = 0.363845 (* 1 = 0.363845 loss)
I0814 19:09:39.055519   613 solver.cpp:264] [MultiGPU] Tests completed in 0.818847s
I0814 19:09:39.071223   613 solver.cpp:312] Iteration 41000 (41.2601 iter/s, 2.42365s/100 iter), loss = 0.00100691
I0814 19:09:39.071240   613 solver.cpp:334]     Train net output #0: loss = 0.00100672 (* 1 = 0.00100672 loss)
I0814 19:09:39.071244   613 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0814 19:09:40.711818   613 solver.cpp:312] Iteration 41100 (60.9554 iter/s, 1.64054s/100 iter), loss = 0.00126491
I0814 19:09:40.711843   613 solver.cpp:334]     Train net output #0: loss = 0.00126472 (* 1 = 0.00126472 loss)
I0814 19:09:40.711849   613 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0814 19:09:42.340250   613 solver.cpp:312] Iteration 41200 (61.4106 iter/s, 1.62838s/100 iter), loss = 0.000263502
I0814 19:09:42.340275   613 solver.cpp:334]     Train net output #0: loss = 0.000263308 (* 1 = 0.000263308 loss)
I0814 19:09:42.340279   613 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0814 19:09:43.998565   613 solver.cpp:312] Iteration 41300 (60.304 iter/s, 1.65826s/100 iter), loss = 0.00360963
I0814 19:09:43.998920   613 solver.cpp:334]     Train net output #0: loss = 0.00360944 (* 1 = 0.00360944 loss)
I0814 19:09:43.998942   613 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0814 19:09:45.647939   613 solver.cpp:312] Iteration 41400 (60.631 iter/s, 1.64932s/100 iter), loss = 0.00111865
I0814 19:09:45.647987   613 solver.cpp:334]     Train net output #0: loss = 0.00111846 (* 1 = 0.00111846 loss)
I0814 19:09:45.648000   613 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0814 19:09:47.269423   613 solver.cpp:312] Iteration 41500 (61.6737 iter/s, 1.62144s/100 iter), loss = 0.00346085
I0814 19:09:47.269448   613 solver.cpp:334]     Train net output #0: loss = 0.00346065 (* 1 = 0.00346065 loss)
I0814 19:09:47.269454   613 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0814 19:09:48.858846   613 solver.cpp:312] Iteration 41600 (62.9179 iter/s, 1.58937s/100 iter), loss = 0.00293498
I0814 19:09:48.858911   613 solver.cpp:334]     Train net output #0: loss = 0.00293478 (* 1 = 0.00293478 loss)
I0814 19:09:48.858928   613 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0814 19:09:50.494462   613 solver.cpp:312] Iteration 41700 (61.1409 iter/s, 1.63557s/100 iter), loss = 0.00701026
I0814 19:09:50.494487   613 solver.cpp:334]     Train net output #0: loss = 0.00701006 (* 1 = 0.00701006 loss)
I0814 19:09:50.494493   613 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0814 19:09:52.137043   613 solver.cpp:312] Iteration 41800 (60.8818 iter/s, 1.64253s/100 iter), loss = 0.00462833
I0814 19:09:52.137128   613 solver.cpp:334]     Train net output #0: loss = 0.00462813 (* 1 = 0.00462813 loss)
I0814 19:09:52.137150   613 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0814 19:09:53.788580   613 solver.cpp:312] Iteration 41900 (60.5515 iter/s, 1.65149s/100 iter), loss = 0.00151235
I0814 19:09:53.788606   613 solver.cpp:334]     Train net output #0: loss = 0.00151215 (* 1 = 0.00151215 loss)
I0814 19:09:53.788612   613 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0814 19:09:55.370249   613 solver.cpp:509] Iteration 42000, Testing net (#0)
I0814 19:09:56.183568   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.916472
I0814 19:09:56.183585   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:09:56.183593   613 solver.cpp:594]     Test net output #2: loss = 0.317332 (* 1 = 0.317332 loss)
I0814 19:09:56.183696   613 solver.cpp:264] [MultiGPU] Tests completed in 0.813426s
I0814 19:09:56.201138   613 solver.cpp:312] Iteration 42000 (41.451 iter/s, 2.41249s/100 iter), loss = 0.000720609
I0814 19:09:56.201165   613 solver.cpp:334]     Train net output #0: loss = 0.000720412 (* 1 = 0.000720412 loss)
I0814 19:09:56.201170   613 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0814 19:09:57.824607   613 solver.cpp:312] Iteration 42100 (61.5984 iter/s, 1.62342s/100 iter), loss = 0.00112608
I0814 19:09:57.824633   613 solver.cpp:334]     Train net output #0: loss = 0.00112588 (* 1 = 0.00112588 loss)
I0814 19:09:57.824640   613 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0814 19:09:59.426971   613 solver.cpp:312] Iteration 42200 (62.4098 iter/s, 1.60231s/100 iter), loss = 0.000295132
I0814 19:09:59.426996   613 solver.cpp:334]     Train net output #0: loss = 0.000294934 (* 1 = 0.000294934 loss)
I0814 19:09:59.427001   613 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0814 19:10:01.046291   613 solver.cpp:312] Iteration 42300 (61.7563 iter/s, 1.61927s/100 iter), loss = 0.000434836
I0814 19:10:01.046471   613 solver.cpp:334]     Train net output #0: loss = 0.000434638 (* 1 = 0.000434638 loss)
I0814 19:10:01.046566   613 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0814 19:10:02.678210   613 solver.cpp:312] Iteration 42400 (61.2795 iter/s, 1.63187s/100 iter), loss = 0.00242478
I0814 19:10:02.678236   613 solver.cpp:334]     Train net output #0: loss = 0.00242458 (* 1 = 0.00242458 loss)
I0814 19:10:02.678242   613 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0814 19:10:04.262579   613 solver.cpp:312] Iteration 42500 (63.1186 iter/s, 1.58432s/100 iter), loss = 0.00152347
I0814 19:10:04.262639   613 solver.cpp:334]     Train net output #0: loss = 0.00152327 (* 1 = 0.00152327 loss)
I0814 19:10:04.262657   613 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0814 19:10:05.872861   613 solver.cpp:312] Iteration 42600 (62.1029 iter/s, 1.61023s/100 iter), loss = 0.00199565
I0814 19:10:05.873078   613 solver.cpp:334]     Train net output #0: loss = 0.00199545 (* 1 = 0.00199545 loss)
I0814 19:10:05.873196   613 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0814 19:10:07.510170   613 solver.cpp:312] Iteration 42700 (61.0777 iter/s, 1.63726s/100 iter), loss = 0.00183584
I0814 19:10:07.510216   613 solver.cpp:334]     Train net output #0: loss = 0.00183564 (* 1 = 0.00183564 loss)
I0814 19:10:07.510227   613 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0814 19:10:09.110486   613 solver.cpp:312] Iteration 42800 (62.4897 iter/s, 1.60026s/100 iter), loss = 0.000779117
I0814 19:10:09.110510   613 solver.cpp:334]     Train net output #0: loss = 0.000778921 (* 1 = 0.000778921 loss)
I0814 19:10:09.110515   613 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0814 19:10:10.712728   613 solver.cpp:312] Iteration 42900 (62.4145 iter/s, 1.60219s/100 iter), loss = 0.0023665
I0814 19:10:10.712797   613 solver.cpp:334]     Train net output #0: loss = 0.00236631 (* 1 = 0.00236631 loss)
I0814 19:10:10.712816   613 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0814 19:10:12.351410   613 solver.cpp:509] Iteration 43000, Testing net (#0)
I0814 19:10:13.190018   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.921766
I0814 19:10:13.190035   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.998235
I0814 19:10:13.190040   613 solver.cpp:594]     Test net output #2: loss = 0.290765 (* 1 = 0.290765 loss)
I0814 19:10:13.190057   613 solver.cpp:264] [MultiGPU] Tests completed in 0.838623s
I0814 19:10:13.208822   613 solver.cpp:312] Iteration 43000 (40.0638 iter/s, 2.49602s/100 iter), loss = 0.000730149
I0814 19:10:13.208850   613 solver.cpp:334]     Train net output #0: loss = 0.000729954 (* 1 = 0.000729954 loss)
I0814 19:10:13.208856   613 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0814 19:10:14.791419   613 solver.cpp:312] Iteration 43100 (63.1893 iter/s, 1.58255s/100 iter), loss = 0.000725857
I0814 19:10:14.791514   613 solver.cpp:334]     Train net output #0: loss = 0.000725663 (* 1 = 0.000725663 loss)
I0814 19:10:14.791527   613 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0814 19:10:16.429031   613 solver.cpp:312] Iteration 43200 (61.0664 iter/s, 1.63756s/100 iter), loss = 0.00492749
I0814 19:10:16.429055   613 solver.cpp:334]     Train net output #0: loss = 0.00492729 (* 1 = 0.00492729 loss)
I0814 19:10:16.429061   613 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0814 19:10:18.054787   613 solver.cpp:312] Iteration 43300 (61.5117 iter/s, 1.62571s/100 iter), loss = 0.00227864
I0814 19:10:18.054811   613 solver.cpp:334]     Train net output #0: loss = 0.00227844 (* 1 = 0.00227844 loss)
I0814 19:10:18.054816   613 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0814 19:10:19.658825   613 solver.cpp:312] Iteration 43400 (62.3446 iter/s, 1.60399s/100 iter), loss = 0.00108539
I0814 19:10:19.658850   613 solver.cpp:334]     Train net output #0: loss = 0.0010852 (* 1 = 0.0010852 loss)
I0814 19:10:19.658856   613 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0814 19:10:21.284073   613 solver.cpp:312] Iteration 43500 (61.5312 iter/s, 1.62519s/100 iter), loss = 0.000225047
I0814 19:10:21.284121   613 solver.cpp:334]     Train net output #0: loss = 0.00022485 (* 1 = 0.00022485 loss)
I0814 19:10:21.284142   613 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0814 19:10:22.918233   613 solver.cpp:312] Iteration 43600 (61.1954 iter/s, 1.63411s/100 iter), loss = 0.000195396
I0814 19:10:22.918259   613 solver.cpp:334]     Train net output #0: loss = 0.000195199 (* 1 = 0.000195199 loss)
I0814 19:10:22.918265   613 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0814 19:10:24.555605   613 solver.cpp:312] Iteration 43700 (61.0753 iter/s, 1.63732s/100 iter), loss = 0.00091494
I0814 19:10:24.555665   613 solver.cpp:334]     Train net output #0: loss = 0.00091474 (* 1 = 0.00091474 loss)
I0814 19:10:24.555685   613 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0814 19:10:26.201346   613 solver.cpp:312] Iteration 43800 (60.7648 iter/s, 1.64569s/100 iter), loss = 0.0006925
I0814 19:10:26.201370   613 solver.cpp:334]     Train net output #0: loss = 0.000692301 (* 1 = 0.000692301 loss)
I0814 19:10:26.201375   613 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0814 19:10:27.804654   613 solver.cpp:312] Iteration 43900 (62.3731 iter/s, 1.60326s/100 iter), loss = 0.000834943
I0814 19:10:27.804678   613 solver.cpp:334]     Train net output #0: loss = 0.000834743 (* 1 = 0.000834743 loss)
I0814 19:10:27.804682   613 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0814 19:10:28.408018   592 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:10:29.401788   613 solver.cpp:509] Iteration 44000, Testing net (#0)
I0814 19:10:30.225769   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.926766
I0814 19:10:30.225791   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:10:30.225796   613 solver.cpp:594]     Test net output #2: loss = 0.276727 (* 1 = 0.276727 loss)
I0814 19:10:30.225823   613 solver.cpp:264] [MultiGPU] Tests completed in 0.824013s
I0814 19:10:30.242105   613 solver.cpp:312] Iteration 44000 (41.0276 iter/s, 2.43738s/100 iter), loss = 0.000927302
I0814 19:10:30.242123   613 solver.cpp:334]     Train net output #0: loss = 0.000927103 (* 1 = 0.000927103 loss)
I0814 19:10:30.242127   613 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0814 19:10:31.828783   613 solver.cpp:312] Iteration 44100 (63.0269 iter/s, 1.58662s/100 iter), loss = 0.001005
I0814 19:10:31.828832   613 solver.cpp:334]     Train net output #0: loss = 0.0010048 (* 1 = 0.0010048 loss)
I0814 19:10:31.828846   613 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0814 19:10:33.426414   613 solver.cpp:312] Iteration 44200 (62.5947 iter/s, 1.59758s/100 iter), loss = 0.000923711
I0814 19:10:33.426440   613 solver.cpp:334]     Train net output #0: loss = 0.000923514 (* 1 = 0.000923514 loss)
I0814 19:10:33.427999   613 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0814 19:10:35.028414   613 solver.cpp:312] Iteration 44300 (62.4239 iter/s, 1.60195s/100 iter), loss = 0.000169598
I0814 19:10:35.028457   613 solver.cpp:334]     Train net output #0: loss = 0.000169401 (* 1 = 0.000169401 loss)
I0814 19:10:35.028470   613 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0814 19:10:36.624832   613 solver.cpp:312] Iteration 44400 (62.6422 iter/s, 1.59637s/100 iter), loss = 0.00291703
I0814 19:10:36.624861   613 solver.cpp:334]     Train net output #0: loss = 0.00291683 (* 1 = 0.00291683 loss)
I0814 19:10:36.624867   613 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0814 19:10:38.240097   613 solver.cpp:312] Iteration 44500 (61.9114 iter/s, 1.61521s/100 iter), loss = 0.00086036
I0814 19:10:38.240149   613 solver.cpp:334]     Train net output #0: loss = 0.000860163 (* 1 = 0.000860163 loss)
I0814 19:10:38.240162   613 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0814 19:10:39.864621   613 solver.cpp:312] Iteration 44600 (61.5584 iter/s, 1.62447s/100 iter), loss = 0.0012991
I0814 19:10:39.864645   613 solver.cpp:334]     Train net output #0: loss = 0.0012989 (* 1 = 0.0012989 loss)
I0814 19:10:39.864650   613 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0814 19:10:41.496698   613 solver.cpp:312] Iteration 44700 (61.2736 iter/s, 1.63203s/100 iter), loss = 0.00137271
I0814 19:10:41.496723   613 solver.cpp:334]     Train net output #0: loss = 0.00137251 (* 1 = 0.00137251 loss)
I0814 19:10:41.496729   613 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0814 19:10:43.091781   613 solver.cpp:312] Iteration 44800 (62.6946 iter/s, 1.59503s/100 iter), loss = 0.00130456
I0814 19:10:43.091850   613 solver.cpp:334]     Train net output #0: loss = 0.00130436 (* 1 = 0.00130436 loss)
I0814 19:10:43.091873   613 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0814 19:10:44.717278   613 solver.cpp:312] Iteration 44900 (61.5216 iter/s, 1.62545s/100 iter), loss = 0.000756068
I0814 19:10:44.717336   613 solver.cpp:334]     Train net output #0: loss = 0.000755871 (* 1 = 0.000755871 loss)
I0814 19:10:44.717355   613 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0814 19:10:46.333463   613 solver.cpp:509] Iteration 45000, Testing net (#0)
I0814 19:10:47.147727   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917648
I0814 19:10:47.147745   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:10:47.147750   613 solver.cpp:594]     Test net output #2: loss = 0.305562 (* 1 = 0.305562 loss)
I0814 19:10:47.147764   613 solver.cpp:264] [MultiGPU] Tests completed in 0.814278s
I0814 19:10:47.163338   613 solver.cpp:312] Iteration 45000 (40.8833 iter/s, 2.44599s/100 iter), loss = 0.000281358
I0814 19:10:47.163355   613 solver.cpp:334]     Train net output #0: loss = 0.00028116 (* 1 = 0.00028116 loss)
I0814 19:10:47.163360   613 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0814 19:10:48.822198   613 solver.cpp:312] Iteration 45100 (60.2844 iter/s, 1.6588s/100 iter), loss = 0.00089973
I0814 19:10:48.822223   613 solver.cpp:334]     Train net output #0: loss = 0.000899532 (* 1 = 0.000899532 loss)
I0814 19:10:48.822229   613 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0814 19:10:50.456260   613 solver.cpp:312] Iteration 45200 (61.1991 iter/s, 1.63401s/100 iter), loss = 0.000943092
I0814 19:10:50.456406   613 solver.cpp:334]     Train net output #0: loss = 0.000942893 (* 1 = 0.000942893 loss)
I0814 19:10:50.456423   613 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0814 19:10:52.102867   613 solver.cpp:312] Iteration 45300 (60.7328 iter/s, 1.64656s/100 iter), loss = 0.000480841
I0814 19:10:52.102913   613 solver.cpp:334]     Train net output #0: loss = 0.000480643 (* 1 = 0.000480643 loss)
I0814 19:10:52.102926   613 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0814 19:10:53.703881   613 solver.cpp:312] Iteration 45400 (62.4624 iter/s, 1.60096s/100 iter), loss = 0.00328456
I0814 19:10:53.703925   613 solver.cpp:334]     Train net output #0: loss = 0.00328436 (* 1 = 0.00328436 loss)
I0814 19:10:53.703936   613 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0814 19:10:55.365870   613 solver.cpp:312] Iteration 45500 (60.1708 iter/s, 1.66194s/100 iter), loss = 0.00136303
I0814 19:10:55.365895   613 solver.cpp:334]     Train net output #0: loss = 0.00136283 (* 1 = 0.00136283 loss)
I0814 19:10:55.365900   613 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0814 19:10:57.016520   613 solver.cpp:312] Iteration 45600 (60.5842 iter/s, 1.6506s/100 iter), loss = 0.00122352
I0814 19:10:57.016546   613 solver.cpp:334]     Train net output #0: loss = 0.00122333 (* 1 = 0.00122333 loss)
I0814 19:10:57.016551   613 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0814 19:10:58.646961   613 solver.cpp:312] Iteration 45700 (61.335 iter/s, 1.63039s/100 iter), loss = 0.00177652
I0814 19:10:58.646986   613 solver.cpp:334]     Train net output #0: loss = 0.00177633 (* 1 = 0.00177633 loss)
I0814 19:10:58.646991   613 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0814 19:11:00.234938   613 solver.cpp:312] Iteration 45800 (62.9751 iter/s, 1.58793s/100 iter), loss = 0.000674201
I0814 19:11:00.234961   613 solver.cpp:334]     Train net output #0: loss = 0.000674005 (* 1 = 0.000674005 loss)
I0814 19:11:00.234968   613 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0814 19:11:01.856184   613 solver.cpp:312] Iteration 45900 (61.6828 iter/s, 1.6212s/100 iter), loss = 0.0025055
I0814 19:11:01.856243   613 solver.cpp:334]     Train net output #0: loss = 0.0025053 (* 1 = 0.0025053 loss)
I0814 19:11:01.856261   613 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0814 19:11:03.427736   613 solver.cpp:509] Iteration 46000, Testing net (#0)
I0814 19:11:04.237390   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.920883
I0814 19:11:04.237406   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:11:04.237411   613 solver.cpp:594]     Test net output #2: loss = 0.295255 (* 1 = 0.295255 loss)
I0814 19:11:04.237433   613 solver.cpp:264] [MultiGPU] Tests completed in 0.809674s
I0814 19:11:04.253439   613 solver.cpp:312] Iteration 46000 (41.7156 iter/s, 2.39719s/100 iter), loss = 0.00137337
I0814 19:11:04.253471   613 solver.cpp:334]     Train net output #0: loss = 0.00137318 (* 1 = 0.00137318 loss)
I0814 19:11:04.253479   613 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0814 19:11:05.886731   613 solver.cpp:312] Iteration 46100 (61.2281 iter/s, 1.63324s/100 iter), loss = 0.000649925
I0814 19:11:05.886756   613 solver.cpp:334]     Train net output #0: loss = 0.00064973 (* 1 = 0.00064973 loss)
I0814 19:11:05.886762   613 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0814 19:11:07.521942   613 solver.cpp:312] Iteration 46200 (61.1561 iter/s, 1.63516s/100 iter), loss = 0.00113563
I0814 19:11:07.521987   613 solver.cpp:334]     Train net output #0: loss = 0.00113543 (* 1 = 0.00113543 loss)
I0814 19:11:07.522001   613 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0814 19:11:09.146766   613 solver.cpp:312] Iteration 46300 (61.547 iter/s, 1.62477s/100 iter), loss = 0.000672932
I0814 19:11:09.146814   613 solver.cpp:334]     Train net output #0: loss = 0.000672736 (* 1 = 0.000672736 loss)
I0814 19:11:09.146827   613 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0814 19:11:10.760692   613 solver.cpp:312] Iteration 46400 (61.9627 iter/s, 1.61387s/100 iter), loss = 0.000989048
I0814 19:11:10.760720   613 solver.cpp:334]     Train net output #0: loss = 0.000988852 (* 1 = 0.000988852 loss)
I0814 19:11:10.760725   613 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0814 19:11:12.400082   613 solver.cpp:312] Iteration 46500 (61.0003 iter/s, 1.63934s/100 iter), loss = 0.000460505
I0814 19:11:12.400108   613 solver.cpp:334]     Train net output #0: loss = 0.00046031 (* 1 = 0.00046031 loss)
I0814 19:11:12.400115   613 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0814 19:11:14.012054   613 solver.cpp:312] Iteration 46600 (62.0378 iter/s, 1.61192s/100 iter), loss = 0.000382233
I0814 19:11:14.012079   613 solver.cpp:334]     Train net output #0: loss = 0.000382038 (* 1 = 0.000382038 loss)
I0814 19:11:14.012084   613 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0814 19:11:15.655050   613 solver.cpp:312] Iteration 46700 (60.8664 iter/s, 1.64294s/100 iter), loss = 0.00150081
I0814 19:11:15.655099   613 solver.cpp:334]     Train net output #0: loss = 0.00150062 (* 1 = 0.00150062 loss)
I0814 19:11:15.655112   613 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0814 19:11:17.258934   613 solver.cpp:312] Iteration 46800 (62.3506 iter/s, 1.60383s/100 iter), loss = 0.000535131
I0814 19:11:17.259011   613 solver.cpp:334]     Train net output #0: loss = 0.000534936 (* 1 = 0.000534936 loss)
I0814 19:11:17.259018   613 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0814 19:11:18.879930   613 solver.cpp:312] Iteration 46900 (61.6924 iter/s, 1.62095s/100 iter), loss = 0.000372487
I0814 19:11:18.879997   613 solver.cpp:334]     Train net output #0: loss = 0.000372292 (* 1 = 0.000372292 loss)
I0814 19:11:18.880018   613 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0814 19:11:20.479991   613 solver.cpp:509] Iteration 47000, Testing net (#0)
I0814 19:11:21.295418   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915589
I0814 19:11:21.295434   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:11:21.295441   613 solver.cpp:594]     Test net output #2: loss = 0.322146 (* 1 = 0.322146 loss)
I0814 19:11:21.295460   613 solver.cpp:264] [MultiGPU] Tests completed in 0.815446s
I0814 19:11:21.311031   613 solver.cpp:312] Iteration 47000 (41.1348 iter/s, 2.43103s/100 iter), loss = 0.00237561
I0814 19:11:21.311048   613 solver.cpp:334]     Train net output #0: loss = 0.00237542 (* 1 = 0.00237542 loss)
I0814 19:11:21.311054   613 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0814 19:11:22.960857   613 solver.cpp:312] Iteration 47100 (60.6145 iter/s, 1.64977s/100 iter), loss = 0.00282767
I0814 19:11:22.960886   613 solver.cpp:334]     Train net output #0: loss = 0.00282748 (* 1 = 0.00282748 loss)
I0814 19:11:22.960891   613 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0814 19:11:24.595844   613 solver.cpp:312] Iteration 47200 (61.1644 iter/s, 1.63494s/100 iter), loss = 0.000968134
I0814 19:11:24.595906   613 solver.cpp:334]     Train net output #0: loss = 0.00096794 (* 1 = 0.00096794 loss)
I0814 19:11:24.595932   613 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0814 19:11:26.220190   613 solver.cpp:312] Iteration 47300 (61.5653 iter/s, 1.62429s/100 iter), loss = 0.000658779
I0814 19:11:26.220290   613 solver.cpp:334]     Train net output #0: loss = 0.000658586 (* 1 = 0.000658586 loss)
I0814 19:11:26.220297   613 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0814 19:11:27.822966   613 solver.cpp:312] Iteration 47400 (62.3936 iter/s, 1.60273s/100 iter), loss = 0.000510129
I0814 19:11:27.823030   613 solver.cpp:334]     Train net output #0: loss = 0.000509935 (* 1 = 0.000509935 loss)
I0814 19:11:27.823050   613 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0814 19:11:29.434669   613 solver.cpp:312] Iteration 47500 (62.0481 iter/s, 1.61165s/100 iter), loss = 0.0011101
I0814 19:11:29.434728   613 solver.cpp:334]     Train net output #0: loss = 0.0011099 (* 1 = 0.0011099 loss)
I0814 19:11:29.434746   613 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0814 19:11:31.058871   613 solver.cpp:312] Iteration 47600 (61.5708 iter/s, 1.62415s/100 iter), loss = 0.0002729
I0814 19:11:31.058894   613 solver.cpp:334]     Train net output #0: loss = 0.000272705 (* 1 = 0.000272705 loss)
I0814 19:11:31.058900   613 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0814 19:11:32.730572   613 solver.cpp:312] Iteration 47700 (59.8212 iter/s, 1.67165s/100 iter), loss = 0.00322666
I0814 19:11:32.730597   613 solver.cpp:334]     Train net output #0: loss = 0.00322647 (* 1 = 0.00322647 loss)
I0814 19:11:32.730603   613 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0814 19:11:34.367100   613 solver.cpp:312] Iteration 47800 (61.1068 iter/s, 1.63648s/100 iter), loss = 0.00155126
I0814 19:11:34.367126   613 solver.cpp:334]     Train net output #0: loss = 0.00155107 (* 1 = 0.00155107 loss)
I0814 19:11:34.367131   613 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0814 19:11:36.015431   613 solver.cpp:312] Iteration 47900 (60.6694 iter/s, 1.64828s/100 iter), loss = 0.000147609
I0814 19:11:36.015458   613 solver.cpp:334]     Train net output #0: loss = 0.000147414 (* 1 = 0.000147414 loss)
I0814 19:11:36.015465   613 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0814 19:11:37.589439   613 solver.cpp:509] Iteration 48000, Testing net (#0)
I0814 19:11:37.620172   611 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:11:38.407500   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911472
I0814 19:11:38.407516   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:11:38.407522   613 solver.cpp:594]     Test net output #2: loss = 0.321733 (* 1 = 0.321733 loss)
I0814 19:11:38.407536   613 solver.cpp:264] [MultiGPU] Tests completed in 0.818075s
I0814 19:11:38.423218   613 solver.cpp:312] Iteration 48000 (41.5331 iter/s, 2.40772s/100 iter), loss = 0.00145816
I0814 19:11:38.423236   613 solver.cpp:334]     Train net output #0: loss = 0.00145796 (* 1 = 0.00145796 loss)
I0814 19:11:38.423243   613 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0814 19:11:40.034956   613 solver.cpp:312] Iteration 48100 (62.0468 iter/s, 1.61169s/100 iter), loss = 0.00205142
I0814 19:11:40.034978   613 solver.cpp:334]     Train net output #0: loss = 0.00205123 (* 1 = 0.00205123 loss)
I0814 19:11:40.034982   613 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0814 19:11:41.644170   613 solver.cpp:312] Iteration 48200 (62.1442 iter/s, 1.60916s/100 iter), loss = 0.00149214
I0814 19:11:41.644193   613 solver.cpp:334]     Train net output #0: loss = 0.00149194 (* 1 = 0.00149194 loss)
I0814 19:11:41.644199   613 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0814 19:11:43.219826   613 solver.cpp:312] Iteration 48300 (63.4676 iter/s, 1.57561s/100 iter), loss = 0.00283488
I0814 19:11:43.219849   613 solver.cpp:334]     Train net output #0: loss = 0.00283469 (* 1 = 0.00283469 loss)
I0814 19:11:43.219854   613 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0814 19:11:44.836551   613 solver.cpp:312] Iteration 48400 (61.8554 iter/s, 1.61667s/100 iter), loss = 0.000683112
I0814 19:11:44.836603   613 solver.cpp:334]     Train net output #0: loss = 0.000682918 (* 1 = 0.000682918 loss)
I0814 19:11:44.836618   613 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0814 19:11:46.491102   613 solver.cpp:312] Iteration 48500 (60.4412 iter/s, 1.6545s/100 iter), loss = 0.00055069
I0814 19:11:46.491150   613 solver.cpp:334]     Train net output #0: loss = 0.000550496 (* 1 = 0.000550496 loss)
I0814 19:11:46.491163   613 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0814 19:11:48.118222   613 solver.cpp:312] Iteration 48600 (61.4604 iter/s, 1.62707s/100 iter), loss = 0.00152892
I0814 19:11:48.118333   613 solver.cpp:334]     Train net output #0: loss = 0.00152872 (* 1 = 0.00152872 loss)
I0814 19:11:48.118351   613 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0814 19:11:49.711868   613 solver.cpp:312] Iteration 48700 (62.7513 iter/s, 1.59359s/100 iter), loss = 0.000505909
I0814 19:11:49.711930   613 solver.cpp:334]     Train net output #0: loss = 0.000505713 (* 1 = 0.000505713 loss)
I0814 19:11:49.711949   613 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0814 19:11:51.338914   613 solver.cpp:312] Iteration 48800 (61.4629 iter/s, 1.627s/100 iter), loss = 0.000574905
I0814 19:11:51.338938   613 solver.cpp:334]     Train net output #0: loss = 0.00057471 (* 1 = 0.00057471 loss)
I0814 19:11:51.338943   613 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0814 19:11:52.971025   613 solver.cpp:312] Iteration 48900 (61.2723 iter/s, 1.63206s/100 iter), loss = 0.000342894
I0814 19:11:52.971089   613 solver.cpp:334]     Train net output #0: loss = 0.000342698 (* 1 = 0.000342698 loss)
I0814 19:11:52.971118   613 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0814 19:11:54.545794   613 solver.cpp:509] Iteration 49000, Testing net (#0)
I0814 19:11:55.359479   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.916177
I0814 19:11:55.359498   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:11:55.359503   613 solver.cpp:594]     Test net output #2: loss = 0.314725 (* 1 = 0.314725 loss)
I0814 19:11:55.359519   613 solver.cpp:264] [MultiGPU] Tests completed in 0.813701s
I0814 19:11:55.375043   613 solver.cpp:312] Iteration 49000 (41.5982 iter/s, 2.40395s/100 iter), loss = 0.00297777
I0814 19:11:55.375075   613 solver.cpp:334]     Train net output #0: loss = 0.00297758 (* 1 = 0.00297758 loss)
I0814 19:11:55.375087   613 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0814 19:11:56.987402   613 solver.cpp:312] Iteration 49100 (62.0229 iter/s, 1.61231s/100 iter), loss = 0.00277521
I0814 19:11:56.987428   613 solver.cpp:334]     Train net output #0: loss = 0.00277502 (* 1 = 0.00277502 loss)
I0814 19:11:56.987433   613 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0814 19:11:58.590071   613 solver.cpp:312] Iteration 49200 (62.398 iter/s, 1.60262s/100 iter), loss = 0.00125179
I0814 19:11:58.590140   613 solver.cpp:334]     Train net output #0: loss = 0.00125159 (* 1 = 0.00125159 loss)
I0814 19:11:58.590162   613 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0814 19:12:00.221596   613 solver.cpp:312] Iteration 49300 (61.2942 iter/s, 1.63147s/100 iter), loss = 0.000314942
I0814 19:12:00.221622   613 solver.cpp:334]     Train net output #0: loss = 0.000314747 (* 1 = 0.000314747 loss)
I0814 19:12:00.221627   613 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0814 19:12:01.857727   613 solver.cpp:312] Iteration 49400 (61.1219 iter/s, 1.63607s/100 iter), loss = 0.000704738
I0814 19:12:01.857775   613 solver.cpp:334]     Train net output #0: loss = 0.000704543 (* 1 = 0.000704543 loss)
I0814 19:12:01.857789   613 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0814 19:12:03.521880   613 solver.cpp:312] Iteration 49500 (60.0924 iter/s, 1.6641s/100 iter), loss = 0.000926789
I0814 19:12:03.521930   613 solver.cpp:334]     Train net output #0: loss = 0.000926593 (* 1 = 0.000926593 loss)
I0814 19:12:03.521944   613 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0814 19:12:05.153570   613 solver.cpp:312] Iteration 49600 (61.2881 iter/s, 1.63164s/100 iter), loss = 0.00110798
I0814 19:12:05.153614   613 solver.cpp:334]     Train net output #0: loss = 0.00110779 (* 1 = 0.00110779 loss)
I0814 19:12:05.153720   613 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0814 19:12:06.749265   613 solver.cpp:312] Iteration 49700 (62.6707 iter/s, 1.59564s/100 iter), loss = 0.000155018
I0814 19:12:06.749325   613 solver.cpp:334]     Train net output #0: loss = 0.000154823 (* 1 = 0.000154823 loss)
I0814 19:12:06.749342   613 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0814 19:12:08.407992   613 solver.cpp:312] Iteration 49800 (60.289 iter/s, 1.65868s/100 iter), loss = 0.000403863
I0814 19:12:08.408054   613 solver.cpp:334]     Train net output #0: loss = 0.000403669 (* 1 = 0.000403669 loss)
I0814 19:12:08.408073   613 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0814 19:12:10.065203   613 solver.cpp:312] Iteration 49900 (60.3442 iter/s, 1.65716s/100 iter), loss = 0.000469129
I0814 19:12:10.065230   613 solver.cpp:334]     Train net output #0: loss = 0.000468935 (* 1 = 0.000468935 loss)
I0814 19:12:10.065237   613 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0814 19:12:11.668686   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_50000.caffemodel
I0814 19:12:11.676733   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_50000.solverstate
I0814 19:12:11.680392   613 solver.cpp:509] Iteration 50000, Testing net (#0)
I0814 19:12:12.484498   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.913236
I0814 19:12:12.484518   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:12:12.484522   613 solver.cpp:594]     Test net output #2: loss = 0.336716 (* 1 = 0.336716 loss)
I0814 19:12:12.484536   613 solver.cpp:264] [MultiGPU] Tests completed in 0.804122s
I0814 19:12:12.500227   613 solver.cpp:312] Iteration 50000 (41.0686 iter/s, 2.43495s/100 iter), loss = 0.000987399
I0814 19:12:12.500247   613 solver.cpp:334]     Train net output #0: loss = 0.000987205 (* 1 = 0.000987205 loss)
I0814 19:12:12.500252   613 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0814 19:12:14.127074   613 solver.cpp:312] Iteration 50100 (61.4707 iter/s, 1.62679s/100 iter), loss = 0.00187994
I0814 19:12:14.127099   613 solver.cpp:334]     Train net output #0: loss = 0.00187974 (* 1 = 0.00187974 loss)
I0814 19:12:14.127104   613 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0814 19:12:15.711103   613 solver.cpp:312] Iteration 50200 (63.1322 iter/s, 1.58398s/100 iter), loss = 0.00256797
I0814 19:12:15.711151   613 solver.cpp:334]     Train net output #0: loss = 0.00256777 (* 1 = 0.00256777 loss)
I0814 19:12:15.711163   613 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0814 19:12:17.310173   613 solver.cpp:312] Iteration 50300 (62.5383 iter/s, 1.59902s/100 iter), loss = 0.000531755
I0814 19:12:17.310216   613 solver.cpp:334]     Train net output #0: loss = 0.000531559 (* 1 = 0.000531559 loss)
I0814 19:12:17.310230   613 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0814 19:12:19.050264   613 solver.cpp:312] Iteration 50400 (57.4701 iter/s, 1.74004s/100 iter), loss = 0.0012119
I0814 19:12:19.050374   613 solver.cpp:334]     Train net output #0: loss = 0.00121171 (* 1 = 0.00121171 loss)
I0814 19:12:19.050392   613 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0814 19:12:20.737241   613 solver.cpp:312] Iteration 50500 (59.2794 iter/s, 1.68693s/100 iter), loss = 0.0012757
I0814 19:12:20.737293   613 solver.cpp:334]     Train net output #0: loss = 0.0012755 (* 1 = 0.0012755 loss)
I0814 19:12:20.737308   613 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0814 19:12:22.345952   613 solver.cpp:312] Iteration 50600 (62.1636 iter/s, 1.60866s/100 iter), loss = 0.00135308
I0814 19:12:22.345978   613 solver.cpp:334]     Train net output #0: loss = 0.00135288 (* 1 = 0.00135288 loss)
I0814 19:12:22.345981   613 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0814 19:12:23.978142   613 solver.cpp:312] Iteration 50700 (61.2694 iter/s, 1.63214s/100 iter), loss = 0.000444065
I0814 19:12:23.978163   613 solver.cpp:334]     Train net output #0: loss = 0.00044387 (* 1 = 0.00044387 loss)
I0814 19:12:23.978168   613 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0814 19:12:25.606401   613 solver.cpp:312] Iteration 50800 (61.4171 iter/s, 1.62821s/100 iter), loss = 0.000914598
I0814 19:12:25.606449   613 solver.cpp:334]     Train net output #0: loss = 0.000914403 (* 1 = 0.000914403 loss)
I0814 19:12:25.606462   613 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0814 19:12:27.225606   613 solver.cpp:312] Iteration 50900 (61.7606 iter/s, 1.61915s/100 iter), loss = 0.00063187
I0814 19:12:27.225653   613 solver.cpp:334]     Train net output #0: loss = 0.000631676 (* 1 = 0.000631676 loss)
I0814 19:12:27.225666   613 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0814 19:12:28.821693   613 solver.cpp:509] Iteration 51000, Testing net (#0)
I0814 19:12:29.636981   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910001
I0814 19:12:29.636998   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:12:29.637003   613 solver.cpp:594]     Test net output #2: loss = 0.351211 (* 1 = 0.351211 loss)
I0814 19:12:29.637018   613 solver.cpp:264] [MultiGPU] Tests completed in 0.815303s
I0814 19:12:29.656695   613 solver.cpp:312] Iteration 51000 (41.135 iter/s, 2.43102s/100 iter), loss = 0.00157698
I0814 19:12:29.656713   613 solver.cpp:334]     Train net output #0: loss = 0.00157679 (* 1 = 0.00157679 loss)
I0814 19:12:29.656716   613 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0814 19:12:31.259333   613 solver.cpp:312] Iteration 51100 (62.3991 iter/s, 1.60259s/100 iter), loss = 0.000660087
I0814 19:12:31.259358   613 solver.cpp:334]     Train net output #0: loss = 0.000659892 (* 1 = 0.000659892 loss)
I0814 19:12:31.259363   613 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0814 19:12:32.840857   613 solver.cpp:312] Iteration 51200 (63.2321 iter/s, 1.58147s/100 iter), loss = 0.00916277
I0814 19:12:32.840901   613 solver.cpp:334]     Train net output #0: loss = 0.00916258 (* 1 = 0.00916258 loss)
I0814 19:12:32.840914   613 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0814 19:12:34.445493   613 solver.cpp:312] Iteration 51300 (62.3214 iter/s, 1.60458s/100 iter), loss = 0.00129054
I0814 19:12:34.445518   613 solver.cpp:334]     Train net output #0: loss = 0.00129035 (* 1 = 0.00129035 loss)
I0814 19:12:34.445523   613 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0814 19:12:36.063861   613 solver.cpp:312] Iteration 51400 (61.7925 iter/s, 1.61832s/100 iter), loss = 0.0014402
I0814 19:12:36.063927   613 solver.cpp:334]     Train net output #0: loss = 0.00144001 (* 1 = 0.00144001 loss)
I0814 19:12:36.063947   613 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0814 19:12:37.697497   613 solver.cpp:312] Iteration 51500 (61.2151 iter/s, 1.63358s/100 iter), loss = 0.000564373
I0814 19:12:37.697547   613 solver.cpp:334]     Train net output #0: loss = 0.000564177 (* 1 = 0.000564177 loss)
I0814 19:12:37.697561   613 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0814 19:12:39.288358   613 solver.cpp:312] Iteration 51600 (62.8611 iter/s, 1.59081s/100 iter), loss = 0.000885502
I0814 19:12:39.288383   613 solver.cpp:334]     Train net output #0: loss = 0.000885306 (* 1 = 0.000885306 loss)
I0814 19:12:39.288388   613 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0814 19:12:40.921553   613 solver.cpp:312] Iteration 51700 (61.2316 iter/s, 1.63314s/100 iter), loss = 0.000484147
I0814 19:12:40.921622   613 solver.cpp:334]     Train net output #0: loss = 0.00048395 (* 1 = 0.00048395 loss)
I0814 19:12:40.921639   613 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0814 19:12:42.569620   613 solver.cpp:312] Iteration 51800 (60.6791 iter/s, 1.64801s/100 iter), loss = 0.00100339
I0814 19:12:42.569665   613 solver.cpp:334]     Train net output #0: loss = 0.0010032 (* 1 = 0.0010032 loss)
I0814 19:12:42.569677   613 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0814 19:12:44.211886   613 solver.cpp:312] Iteration 51900 (60.8933 iter/s, 1.64222s/100 iter), loss = 0.000678729
I0814 19:12:44.211949   613 solver.cpp:334]     Train net output #0: loss = 0.000678533 (* 1 = 0.000678533 loss)
I0814 19:12:44.211969   613 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0814 19:12:45.817054   613 solver.cpp:509] Iteration 52000, Testing net (#0)
I0814 19:12:46.557572   611 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:12:46.647630   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909413
I0814 19:12:46.647650   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:12:46.647655   613 solver.cpp:594]     Test net output #2: loss = 0.352891 (* 1 = 0.352891 loss)
I0814 19:12:46.647670   613 solver.cpp:264] [MultiGPU] Tests completed in 0.830595s
I0814 19:12:46.663411   613 solver.cpp:312] Iteration 52000 (40.7921 iter/s, 2.45145s/100 iter), loss = 0.000248034
I0814 19:12:46.663427   613 solver.cpp:334]     Train net output #0: loss = 0.000247838 (* 1 = 0.000247838 loss)
I0814 19:12:46.663434   613 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0814 19:12:48.300037   613 solver.cpp:312] Iteration 52100 (61.1034 iter/s, 1.63657s/100 iter), loss = 0.000967355
I0814 19:12:48.300096   613 solver.cpp:334]     Train net output #0: loss = 0.000967158 (* 1 = 0.000967158 loss)
I0814 19:12:48.300112   613 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0814 19:12:49.882455   613 solver.cpp:312] Iteration 52200 (63.1964 iter/s, 1.58237s/100 iter), loss = 0.00126357
I0814 19:12:49.882566   613 solver.cpp:334]     Train net output #0: loss = 0.00126338 (* 1 = 0.00126338 loss)
I0814 19:12:49.882583   613 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0814 19:12:51.504448   613 solver.cpp:312] Iteration 52300 (61.6545 iter/s, 1.62194s/100 iter), loss = 0.00224237
I0814 19:12:51.504578   613 solver.cpp:334]     Train net output #0: loss = 0.00224218 (* 1 = 0.00224218 loss)
I0814 19:12:51.504596   613 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0814 19:12:53.117820   613 solver.cpp:312] Iteration 52400 (61.9839 iter/s, 1.61332s/100 iter), loss = 0.000889893
I0814 19:12:53.117889   613 solver.cpp:334]     Train net output #0: loss = 0.000889698 (* 1 = 0.000889698 loss)
I0814 19:12:53.117913   613 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0814 19:12:54.736692   613 solver.cpp:312] Iteration 52500 (61.7733 iter/s, 1.61882s/100 iter), loss = 0.00168973
I0814 19:12:54.736958   613 solver.cpp:334]     Train net output #0: loss = 0.00168953 (* 1 = 0.00168953 loss)
I0814 19:12:54.736964   613 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0814 19:12:56.325379   613 solver.cpp:312] Iteration 52600 (62.947 iter/s, 1.58864s/100 iter), loss = 0.00093692
I0814 19:12:56.325405   613 solver.cpp:334]     Train net output #0: loss = 0.000936724 (* 1 = 0.000936724 loss)
I0814 19:12:56.325412   613 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0814 19:12:57.965597   613 solver.cpp:312] Iteration 52700 (60.9694 iter/s, 1.64017s/100 iter), loss = 0.000245013
I0814 19:12:57.965642   613 solver.cpp:334]     Train net output #0: loss = 0.000244818 (* 1 = 0.000244818 loss)
I0814 19:12:57.965653   613 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0814 19:12:59.624940   613 solver.cpp:312] Iteration 52800 (60.2667 iter/s, 1.65929s/100 iter), loss = 0.000550303
I0814 19:12:59.625011   613 solver.cpp:334]     Train net output #0: loss = 0.000550109 (* 1 = 0.000550109 loss)
I0814 19:12:59.625038   613 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0814 19:13:01.254320   613 solver.cpp:312] Iteration 52900 (61.3751 iter/s, 1.62933s/100 iter), loss = 0.00138242
I0814 19:13:01.254384   613 solver.cpp:334]     Train net output #0: loss = 0.00138223 (* 1 = 0.00138223 loss)
I0814 19:13:01.254405   613 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0814 19:13:02.842218   613 solver.cpp:509] Iteration 53000, Testing net (#0)
I0814 19:13:03.676280   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912942
I0814 19:13:03.676301   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:13:03.676304   613 solver.cpp:594]     Test net output #2: loss = 0.33903 (* 1 = 0.33903 loss)
I0814 19:13:03.676319   613 solver.cpp:264] [MultiGPU] Tests completed in 0.83408s
I0814 19:13:03.692448   613 solver.cpp:312] Iteration 53000 (41.0162 iter/s, 2.43806s/100 iter), loss = 0.000588241
I0814 19:13:03.692469   613 solver.cpp:334]     Train net output #0: loss = 0.000588047 (* 1 = 0.000588047 loss)
I0814 19:13:03.692477   613 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0814 19:13:05.296648   613 solver.cpp:312] Iteration 53100 (62.3383 iter/s, 1.60415s/100 iter), loss = 0.00102272
I0814 19:13:05.296699   613 solver.cpp:334]     Train net output #0: loss = 0.00102252 (* 1 = 0.00102252 loss)
I0814 19:13:05.296707   613 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0814 19:13:06.933173   613 solver.cpp:312] Iteration 53200 (61.1072 iter/s, 1.63647s/100 iter), loss = 0.000212771
I0814 19:13:06.933198   613 solver.cpp:334]     Train net output #0: loss = 0.000212576 (* 1 = 0.000212576 loss)
I0814 19:13:06.933203   613 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0814 19:13:08.547691   613 solver.cpp:312] Iteration 53300 (61.9398 iter/s, 1.61447s/100 iter), loss = 0.000728705
I0814 19:13:08.547745   613 solver.cpp:334]     Train net output #0: loss = 0.000728511 (* 1 = 0.000728511 loss)
I0814 19:13:08.547760   613 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0814 19:13:10.169041   613 solver.cpp:312] Iteration 53400 (61.6789 iter/s, 1.6213s/100 iter), loss = 0.000756696
I0814 19:13:10.169087   613 solver.cpp:334]     Train net output #0: loss = 0.000756501 (* 1 = 0.000756501 loss)
I0814 19:13:10.169100   613 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0814 19:13:11.781137   613 solver.cpp:312] Iteration 53500 (62.033 iter/s, 1.61205s/100 iter), loss = 0.00268828
I0814 19:13:11.781203   613 solver.cpp:334]     Train net output #0: loss = 0.00268809 (* 1 = 0.00268809 loss)
I0814 19:13:11.781224   613 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0814 19:13:13.399871   613 solver.cpp:312] Iteration 53600 (61.7786 iter/s, 1.61868s/100 iter), loss = 0.000763351
I0814 19:13:13.399894   613 solver.cpp:334]     Train net output #0: loss = 0.000763155 (* 1 = 0.000763155 loss)
I0814 19:13:13.399899   613 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0814 19:13:15.052706   613 solver.cpp:312] Iteration 53700 (60.5041 iter/s, 1.65278s/100 iter), loss = 0.000423735
I0814 19:13:15.052729   613 solver.cpp:334]     Train net output #0: loss = 0.000423539 (* 1 = 0.000423539 loss)
I0814 19:13:15.052736   613 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0814 19:13:16.672395   613 solver.cpp:312] Iteration 53800 (61.742 iter/s, 1.61964s/100 iter), loss = 0.000823352
I0814 19:13:16.672441   613 solver.cpp:334]     Train net output #0: loss = 0.000823156 (* 1 = 0.000823156 loss)
I0814 19:13:16.672452   613 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0814 19:13:18.287758   613 solver.cpp:312] Iteration 53900 (61.9076 iter/s, 1.61531s/100 iter), loss = 0.00432964
I0814 19:13:18.287786   613 solver.cpp:334]     Train net output #0: loss = 0.00432945 (* 1 = 0.00432945 loss)
I0814 19:13:18.287792   613 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0814 19:13:19.900918   613 solver.cpp:509] Iteration 54000, Testing net (#0)
I0814 19:13:20.184164   621 blocking_queue.cpp:40] Waiting for datum
I0814 19:13:20.723284   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917648
I0814 19:13:20.723304   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:13:20.723309   613 solver.cpp:594]     Test net output #2: loss = 0.30761 (* 1 = 0.30761 loss)
I0814 19:13:20.723325   613 solver.cpp:264] [MultiGPU] Tests completed in 0.822384s
I0814 19:13:20.740979   613 solver.cpp:312] Iteration 54000 (40.7639 iter/s, 2.45315s/100 iter), loss = 0.000643323
I0814 19:13:20.740995   613 solver.cpp:334]     Train net output #0: loss = 0.000643129 (* 1 = 0.000643129 loss)
I0814 19:13:20.741001   613 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0814 19:13:22.328306   613 solver.cpp:312] Iteration 54100 (63.0011 iter/s, 1.58727s/100 iter), loss = 0.000745889
I0814 19:13:22.328368   613 solver.cpp:334]     Train net output #0: loss = 0.000745695 (* 1 = 0.000745695 loss)
I0814 19:13:22.328385   613 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0814 19:13:23.977360   613 solver.cpp:312] Iteration 54200 (60.6427 iter/s, 1.649s/100 iter), loss = 0.000538481
I0814 19:13:23.977385   613 solver.cpp:334]     Train net output #0: loss = 0.000538287 (* 1 = 0.000538287 loss)
I0814 19:13:23.977391   613 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0814 19:13:25.609180   613 solver.cpp:312] Iteration 54300 (61.2833 iter/s, 1.63177s/100 iter), loss = 0.000933999
I0814 19:13:25.609207   613 solver.cpp:334]     Train net output #0: loss = 0.000933804 (* 1 = 0.000933804 loss)
I0814 19:13:25.609215   613 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0814 19:13:27.247759   613 solver.cpp:312] Iteration 54400 (61.0302 iter/s, 1.63853s/100 iter), loss = 0.000624677
I0814 19:13:27.247784   613 solver.cpp:334]     Train net output #0: loss = 0.000624481 (* 1 = 0.000624481 loss)
I0814 19:13:27.247790   613 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0814 19:13:28.852190   613 solver.cpp:312] Iteration 54500 (62.3294 iter/s, 1.60438s/100 iter), loss = 0.000566006
I0814 19:13:28.852216   613 solver.cpp:334]     Train net output #0: loss = 0.000565811 (* 1 = 0.000565811 loss)
I0814 19:13:28.852222   613 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0814 19:13:30.460671   613 solver.cpp:312] Iteration 54600 (62.1723 iter/s, 1.60843s/100 iter), loss = 0.00056443
I0814 19:13:30.460697   613 solver.cpp:334]     Train net output #0: loss = 0.000564235 (* 1 = 0.000564235 loss)
I0814 19:13:30.460705   613 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0814 19:13:32.068125   613 solver.cpp:312] Iteration 54700 (62.2122 iter/s, 1.6074s/100 iter), loss = 0.00651327
I0814 19:13:32.068156   613 solver.cpp:334]     Train net output #0: loss = 0.00651307 (* 1 = 0.00651307 loss)
I0814 19:13:32.068162   613 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0814 19:13:33.709308   613 solver.cpp:312] Iteration 54800 (60.9336 iter/s, 1.64113s/100 iter), loss = 0.00120459
I0814 19:13:33.709333   613 solver.cpp:334]     Train net output #0: loss = 0.0012044 (* 1 = 0.0012044 loss)
I0814 19:13:33.709339   613 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0814 19:13:35.283931   613 solver.cpp:312] Iteration 54900 (63.5092 iter/s, 1.57457s/100 iter), loss = 0.000286021
I0814 19:13:35.283958   613 solver.cpp:334]     Train net output #0: loss = 0.000285827 (* 1 = 0.000285827 loss)
I0814 19:13:35.283965   613 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0814 19:13:36.910495   613 solver.cpp:509] Iteration 55000, Testing net (#0)
I0814 19:13:37.727433   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.92353
I0814 19:13:37.727453   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 19:13:37.727459   613 solver.cpp:594]     Test net output #2: loss = 0.283337 (* 1 = 0.283337 loss)
I0814 19:13:37.727478   613 solver.cpp:264] [MultiGPU] Tests completed in 0.81696s
I0814 19:13:37.743332   613 solver.cpp:312] Iteration 55000 (40.6615 iter/s, 2.45933s/100 iter), loss = 0.00130107
I0814 19:13:37.743360   613 solver.cpp:334]     Train net output #0: loss = 0.00130088 (* 1 = 0.00130088 loss)
I0814 19:13:37.743366   613 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0814 19:13:39.406357   613 solver.cpp:312] Iteration 55100 (60.1334 iter/s, 1.66297s/100 iter), loss = 0.00069349
I0814 19:13:39.406502   613 solver.cpp:334]     Train net output #0: loss = 0.000693295 (* 1 = 0.000693295 loss)
I0814 19:13:39.406577   613 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0814 19:13:41.045289   613 solver.cpp:312] Iteration 55200 (61.0172 iter/s, 1.63888s/100 iter), loss = 0.00163812
I0814 19:13:41.045310   613 solver.cpp:334]     Train net output #0: loss = 0.00163792 (* 1 = 0.00163792 loss)
I0814 19:13:41.045315   613 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0814 19:13:42.660694   613 solver.cpp:312] Iteration 55300 (61.9059 iter/s, 1.61536s/100 iter), loss = 0.00109261
I0814 19:13:42.660758   613 solver.cpp:334]     Train net output #0: loss = 0.00109241 (* 1 = 0.00109241 loss)
I0814 19:13:42.660776   613 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0814 19:13:44.268653   613 solver.cpp:312] Iteration 55400 (62.1926 iter/s, 1.60791s/100 iter), loss = 0.000434343
I0814 19:13:44.268730   613 solver.cpp:334]     Train net output #0: loss = 0.000434149 (* 1 = 0.000434149 loss)
I0814 19:13:44.268756   613 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0814 19:13:45.876451   613 solver.cpp:312] Iteration 55500 (62.1988 iter/s, 1.60775s/100 iter), loss = 0.000768183
I0814 19:13:45.876477   613 solver.cpp:334]     Train net output #0: loss = 0.000767989 (* 1 = 0.000767989 loss)
I0814 19:13:45.876482   613 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0814 19:13:47.490258   613 solver.cpp:312] Iteration 55600 (61.9671 iter/s, 1.61376s/100 iter), loss = 0.00075796
I0814 19:13:47.490319   613 solver.cpp:334]     Train net output #0: loss = 0.000757766 (* 1 = 0.000757766 loss)
I0814 19:13:47.490339   613 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0814 19:13:49.112522   613 solver.cpp:312] Iteration 55700 (61.6442 iter/s, 1.62221s/100 iter), loss = 0.000269589
I0814 19:13:49.112545   613 solver.cpp:334]     Train net output #0: loss = 0.000269396 (* 1 = 0.000269396 loss)
I0814 19:13:49.112548   613 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0814 19:13:50.708226   613 solver.cpp:312] Iteration 55800 (62.6702 iter/s, 1.59566s/100 iter), loss = 0.000683465
I0814 19:13:50.708353   613 solver.cpp:334]     Train net output #0: loss = 0.000683272 (* 1 = 0.000683272 loss)
I0814 19:13:50.708372   613 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0814 19:13:52.347543   613 solver.cpp:312] Iteration 55900 (61.003 iter/s, 1.63926s/100 iter), loss = 0.00142602
I0814 19:13:52.347568   613 solver.cpp:334]     Train net output #0: loss = 0.00142582 (* 1 = 0.00142582 loss)
I0814 19:13:52.347574   613 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0814 19:13:53.927754   613 solver.cpp:509] Iteration 56000, Testing net (#0)
I0814 19:13:54.739979   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.928824
I0814 19:13:54.739997   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:13:54.740005   613 solver.cpp:594]     Test net output #2: loss = 0.272097 (* 1 = 0.272097 loss)
I0814 19:13:54.740021   613 solver.cpp:264] [MultiGPU] Tests completed in 0.812245s
I0814 19:13:54.755614   613 solver.cpp:312] Iteration 56000 (41.5282 iter/s, 2.408s/100 iter), loss = 0.000504016
I0814 19:13:54.755630   613 solver.cpp:334]     Train net output #0: loss = 0.000503824 (* 1 = 0.000503824 loss)
I0814 19:13:54.755635   613 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0814 19:13:55.691763   592 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:13:56.368233   613 solver.cpp:312] Iteration 56100 (62.0129 iter/s, 1.61257s/100 iter), loss = 0.00139256
I0814 19:13:56.368288   613 solver.cpp:334]     Train net output #0: loss = 0.00139237 (* 1 = 0.00139237 loss)
I0814 19:13:56.368301   613 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0814 19:13:58.015171   613 solver.cpp:312] Iteration 56200 (60.7206 iter/s, 1.64689s/100 iter), loss = 0.00132154
I0814 19:13:58.015219   613 solver.cpp:334]     Train net output #0: loss = 0.00132135 (* 1 = 0.00132135 loss)
I0814 19:13:58.015234   613 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0814 19:13:59.639606   613 solver.cpp:312] Iteration 56300 (61.5617 iter/s, 1.62439s/100 iter), loss = 0.00138546
I0814 19:13:59.639631   613 solver.cpp:334]     Train net output #0: loss = 0.00138527 (* 1 = 0.00138527 loss)
I0814 19:13:59.639636   613 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0814 19:14:01.250710   613 solver.cpp:312] Iteration 56400 (62.0712 iter/s, 1.61105s/100 iter), loss = 0.00222113
I0814 19:14:01.250851   613 solver.cpp:334]     Train net output #0: loss = 0.00222094 (* 1 = 0.00222094 loss)
I0814 19:14:01.250871   613 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0814 19:14:02.852708   613 solver.cpp:312] Iteration 56500 (62.424 iter/s, 1.60195s/100 iter), loss = 0.00057469
I0814 19:14:02.852733   613 solver.cpp:334]     Train net output #0: loss = 0.000574497 (* 1 = 0.000574497 loss)
I0814 19:14:02.852740   613 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0814 19:14:04.492292   613 solver.cpp:312] Iteration 56600 (60.9928 iter/s, 1.63954s/100 iter), loss = 0.000937809
I0814 19:14:04.492316   613 solver.cpp:334]     Train net output #0: loss = 0.000937617 (* 1 = 0.000937617 loss)
I0814 19:14:04.492321   613 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0814 19:14:06.135954   613 solver.cpp:312] Iteration 56700 (60.8416 iter/s, 1.64361s/100 iter), loss = 0.000194073
I0814 19:14:06.135982   613 solver.cpp:334]     Train net output #0: loss = 0.00019388 (* 1 = 0.00019388 loss)
I0814 19:14:06.135987   613 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0814 19:14:07.740455   613 solver.cpp:312] Iteration 56800 (62.3266 iter/s, 1.60445s/100 iter), loss = 0.000555627
I0814 19:14:07.740479   613 solver.cpp:334]     Train net output #0: loss = 0.000555434 (* 1 = 0.000555434 loss)
I0814 19:14:07.740485   613 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0814 19:14:09.362591   613 solver.cpp:312] Iteration 56900 (61.649 iter/s, 1.62209s/100 iter), loss = 0.00085945
I0814 19:14:09.362653   613 solver.cpp:334]     Train net output #0: loss = 0.000859258 (* 1 = 0.000859258 loss)
I0814 19:14:09.362685   613 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0814 19:14:10.985059   613 solver.cpp:509] Iteration 57000, Testing net (#0)
I0814 19:14:11.819135   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.926766
I0814 19:14:11.819154   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:14:11.819159   613 solver.cpp:594]     Test net output #2: loss = 0.287946 (* 1 = 0.287946 loss)
I0814 19:14:11.819173   613 solver.cpp:264] [MultiGPU] Tests completed in 0.834093s
I0814 19:14:11.836732   613 solver.cpp:312] Iteration 57000 (40.4192 iter/s, 2.47407s/100 iter), loss = 0.00205957
I0814 19:14:11.836751   613 solver.cpp:334]     Train net output #0: loss = 0.00205938 (* 1 = 0.00205938 loss)
I0814 19:14:11.836757   613 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0814 19:14:13.467335   613 solver.cpp:312] Iteration 57100 (61.3291 iter/s, 1.63055s/100 iter), loss = 0.00130063
I0814 19:14:13.467360   613 solver.cpp:334]     Train net output #0: loss = 0.00130044 (* 1 = 0.00130044 loss)
I0814 19:14:13.467366   613 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0814 19:14:15.108762   613 solver.cpp:312] Iteration 57200 (60.9244 iter/s, 1.64138s/100 iter), loss = 0.00238271
I0814 19:14:15.108808   613 solver.cpp:334]     Train net output #0: loss = 0.00238252 (* 1 = 0.00238252 loss)
I0814 19:14:15.108820   613 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0814 19:14:16.732713   613 solver.cpp:312] Iteration 57300 (61.5802 iter/s, 1.6239s/100 iter), loss = 0.00636473
I0814 19:14:16.733021   613 solver.cpp:334]     Train net output #0: loss = 0.00636454 (* 1 = 0.00636454 loss)
I0814 19:14:16.733036   613 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0814 19:14:18.335400   613 solver.cpp:312] Iteration 57400 (62.3971 iter/s, 1.60264s/100 iter), loss = 0.000838508
I0814 19:14:18.335425   613 solver.cpp:334]     Train net output #0: loss = 0.000838316 (* 1 = 0.000838316 loss)
I0814 19:14:18.335431   613 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0814 19:14:19.996417   613 solver.cpp:312] Iteration 57500 (60.206 iter/s, 1.66096s/100 iter), loss = 0.000250811
I0814 19:14:19.996444   613 solver.cpp:334]     Train net output #0: loss = 0.000250619 (* 1 = 0.000250619 loss)
I0814 19:14:19.996453   613 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0814 19:14:21.623462   613 solver.cpp:312] Iteration 57600 (61.4629 iter/s, 1.627s/100 iter), loss = 0.00287358
I0814 19:14:21.623548   613 solver.cpp:334]     Train net output #0: loss = 0.00287339 (* 1 = 0.00287339 loss)
I0814 19:14:21.623555   613 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0814 19:14:23.223930   613 solver.cpp:312] Iteration 57700 (62.4837 iter/s, 1.60042s/100 iter), loss = 0.000693637
I0814 19:14:23.223954   613 solver.cpp:334]     Train net output #0: loss = 0.000693444 (* 1 = 0.000693444 loss)
I0814 19:14:23.223960   613 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0814 19:14:24.871544   613 solver.cpp:312] Iteration 57800 (60.6957 iter/s, 1.64756s/100 iter), loss = 0.000866125
I0814 19:14:24.871598   613 solver.cpp:334]     Train net output #0: loss = 0.000865931 (* 1 = 0.000865931 loss)
I0814 19:14:24.871610   613 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0814 19:14:26.527604   613 solver.cpp:312] Iteration 57900 (60.3861 iter/s, 1.65601s/100 iter), loss = 0.000593252
I0814 19:14:26.527751   613 solver.cpp:334]     Train net output #0: loss = 0.000593059 (* 1 = 0.000593059 loss)
I0814 19:14:26.527828   613 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0814 19:14:28.133599   613 solver.cpp:509] Iteration 58000, Testing net (#0)
I0814 19:14:28.964867   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.918236
I0814 19:14:28.964885   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:14:28.964890   613 solver.cpp:594]     Test net output #2: loss = 0.319302 (* 1 = 0.319302 loss)
I0814 19:14:28.964906   613 solver.cpp:264] [MultiGPU] Tests completed in 0.831284s
I0814 19:14:28.980962   613 solver.cpp:312] Iteration 58000 (40.7617 iter/s, 2.45329s/100 iter), loss = 0.00253755
I0814 19:14:28.980979   613 solver.cpp:334]     Train net output #0: loss = 0.00253735 (* 1 = 0.00253735 loss)
I0814 19:14:28.980984   613 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0814 19:14:30.606820   613 solver.cpp:312] Iteration 58100 (61.5079 iter/s, 1.62581s/100 iter), loss = 0.00608398
I0814 19:14:30.607029   613 solver.cpp:334]     Train net output #0: loss = 0.00608378 (* 1 = 0.00608378 loss)
I0814 19:14:30.607092   613 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0814 19:14:32.246266   613 solver.cpp:312] Iteration 58200 (60.9982 iter/s, 1.63939s/100 iter), loss = 0.000571924
I0814 19:14:32.246292   613 solver.cpp:334]     Train net output #0: loss = 0.000571731 (* 1 = 0.000571731 loss)
I0814 19:14:32.246299   613 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0814 19:14:33.851166   613 solver.cpp:312] Iteration 58300 (62.3111 iter/s, 1.60485s/100 iter), loss = 0.000714348
I0814 19:14:33.851189   613 solver.cpp:334]     Train net output #0: loss = 0.000714155 (* 1 = 0.000714155 loss)
I0814 19:14:33.851193   613 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0814 19:14:35.468067   613 solver.cpp:312] Iteration 58400 (61.8485 iter/s, 1.61685s/100 iter), loss = 0.00192373
I0814 19:14:35.468091   613 solver.cpp:334]     Train net output #0: loss = 0.00192353 (* 1 = 0.00192353 loss)
I0814 19:14:35.468097   613 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0814 19:14:37.101158   613 solver.cpp:312] Iteration 58500 (61.2355 iter/s, 1.63304s/100 iter), loss = 0.00132546
I0814 19:14:37.101204   613 solver.cpp:334]     Train net output #0: loss = 0.00132527 (* 1 = 0.00132527 loss)
I0814 19:14:37.101217   613 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0814 19:14:38.717130   613 solver.cpp:312] Iteration 58600 (61.8841 iter/s, 1.61592s/100 iter), loss = 0.000223666
I0814 19:14:38.717175   613 solver.cpp:334]     Train net output #0: loss = 0.000223471 (* 1 = 0.000223471 loss)
I0814 19:14:38.717186   613 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0814 19:14:40.342741   613 solver.cpp:312] Iteration 58700 (61.5173 iter/s, 1.62556s/100 iter), loss = 0.000490509
I0814 19:14:40.342768   613 solver.cpp:334]     Train net output #0: loss = 0.000490314 (* 1 = 0.000490314 loss)
I0814 19:14:40.342773   613 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0814 19:14:41.986130   613 solver.cpp:312] Iteration 58800 (60.8518 iter/s, 1.64334s/100 iter), loss = 0.000481489
I0814 19:14:41.986155   613 solver.cpp:334]     Train net output #0: loss = 0.000481295 (* 1 = 0.000481295 loss)
I0814 19:14:41.986158   613 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0814 19:14:43.662510   613 solver.cpp:312] Iteration 58900 (59.6542 iter/s, 1.67633s/100 iter), loss = 0.000873105
I0814 19:14:43.662536   613 solver.cpp:334]     Train net output #0: loss = 0.000872912 (* 1 = 0.000872912 loss)
I0814 19:14:43.662544   613 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0814 19:14:45.262933   613 solver.cpp:509] Iteration 59000, Testing net (#0)
I0814 19:14:46.075053   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915883
I0814 19:14:46.075070   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:14:46.075075   613 solver.cpp:594]     Test net output #2: loss = 0.312787 (* 1 = 0.312787 loss)
I0814 19:14:46.075091   613 solver.cpp:264] [MultiGPU] Tests completed in 0.812137s
I0814 19:14:46.090919   613 solver.cpp:312] Iteration 59000 (41.1804 iter/s, 2.42834s/100 iter), loss = 0.000907719
I0814 19:14:46.090935   613 solver.cpp:334]     Train net output #0: loss = 0.000907526 (* 1 = 0.000907526 loss)
I0814 19:14:46.090941   613 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0814 19:14:47.729897   613 solver.cpp:312] Iteration 59100 (61.0156 iter/s, 1.63892s/100 iter), loss = 0.00181612
I0814 19:14:47.729923   613 solver.cpp:334]     Train net output #0: loss = 0.00181593 (* 1 = 0.00181593 loss)
I0814 19:14:47.729928   613 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0814 19:14:49.349674   613 solver.cpp:312] Iteration 59200 (61.7388 iter/s, 1.61973s/100 iter), loss = 0.000910037
I0814 19:14:49.349704   613 solver.cpp:334]     Train net output #0: loss = 0.000909844 (* 1 = 0.000909844 loss)
I0814 19:14:49.349711   613 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0814 19:14:50.962692   613 solver.cpp:312] Iteration 59300 (61.9976 iter/s, 1.61296s/100 iter), loss = 0.000566687
I0814 19:14:50.962716   613 solver.cpp:334]     Train net output #0: loss = 0.000566493 (* 1 = 0.000566493 loss)
I0814 19:14:50.962721   613 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0814 19:14:52.540957   613 solver.cpp:312] Iteration 59400 (63.3625 iter/s, 1.57822s/100 iter), loss = 0.00306824
I0814 19:14:52.541030   613 solver.cpp:334]     Train net output #0: loss = 0.00306804 (* 1 = 0.00306804 loss)
I0814 19:14:52.541038   613 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0814 19:14:54.151327   613 solver.cpp:312] Iteration 59500 (62.0995 iter/s, 1.61032s/100 iter), loss = 0.000278579
I0814 19:14:54.151356   613 solver.cpp:334]     Train net output #0: loss = 0.000278384 (* 1 = 0.000278384 loss)
I0814 19:14:54.151363   613 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0814 19:14:55.784466   613 solver.cpp:312] Iteration 59600 (61.2336 iter/s, 1.63309s/100 iter), loss = 0.00138084
I0814 19:14:55.784612   613 solver.cpp:334]     Train net output #0: loss = 0.00138065 (* 1 = 0.00138065 loss)
I0814 19:14:55.784709   613 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0814 19:14:57.398236   613 solver.cpp:312] Iteration 59700 (61.9686 iter/s, 1.61372s/100 iter), loss = 0.0029104
I0814 19:14:57.398282   613 solver.cpp:334]     Train net output #0: loss = 0.0029102 (* 1 = 0.0029102 loss)
I0814 19:14:57.398293   613 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0814 19:14:59.029199   613 solver.cpp:312] Iteration 59800 (61.3153 iter/s, 1.63091s/100 iter), loss = 0.00193224
I0814 19:14:59.029247   613 solver.cpp:334]     Train net output #0: loss = 0.00193204 (* 1 = 0.00193204 loss)
I0814 19:14:59.029263   613 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0814 19:15:00.661589   613 solver.cpp:312] Iteration 59900 (61.2619 iter/s, 1.63234s/100 iter), loss = 0.00061853
I0814 19:15:00.661639   613 solver.cpp:334]     Train net output #0: loss = 0.000618336 (* 1 = 0.000618336 loss)
I0814 19:15:00.661653   613 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0814 19:15:02.252465   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_60000.caffemodel
I0814 19:15:02.260396   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_60000.solverstate
I0814 19:15:02.263861   613 solver.cpp:509] Iteration 60000, Testing net (#0)
I0814 19:15:03.071303   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914119
I0814 19:15:03.071321   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:15:03.071326   613 solver.cpp:594]     Test net output #2: loss = 0.332166 (* 1 = 0.332166 loss)
I0814 19:15:03.071341   613 solver.cpp:264] [MultiGPU] Tests completed in 0.807457s
I0814 19:15:03.086921   613 solver.cpp:312] Iteration 60000 (41.2326 iter/s, 2.42526s/100 iter), loss = 0.00160001
I0814 19:15:03.086937   613 solver.cpp:334]     Train net output #0: loss = 0.00159981 (* 1 = 0.00159981 loss)
I0814 19:15:03.086941   613 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0814 19:15:04.703081   613 solver.cpp:312] Iteration 60100 (61.877 iter/s, 1.61611s/100 iter), loss = 0.00154639
I0814 19:15:04.703107   613 solver.cpp:334]     Train net output #0: loss = 0.00154619 (* 1 = 0.00154619 loss)
I0814 19:15:04.703112   613 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0814 19:15:06.278401   613 solver.cpp:312] Iteration 60200 (63.4812 iter/s, 1.57527s/100 iter), loss = 0.000553183
I0814 19:15:06.278475   613 solver.cpp:334]     Train net output #0: loss = 0.00055299 (* 1 = 0.00055299 loss)
I0814 19:15:06.278501   613 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0814 19:15:07.905796   613 solver.cpp:312] Iteration 60300 (61.4498 iter/s, 1.62734s/100 iter), loss = 0.0018664
I0814 19:15:07.905951   613 solver.cpp:334]     Train net output #0: loss = 0.0018662 (* 1 = 0.0018662 loss)
I0814 19:15:07.905977   613 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0814 19:15:09.544441   613 solver.cpp:312] Iteration 60400 (61.0279 iter/s, 1.6386s/100 iter), loss = 0.00028911
I0814 19:15:09.544466   613 solver.cpp:334]     Train net output #0: loss = 0.000288916 (* 1 = 0.000288916 loss)
I0814 19:15:09.544497   613 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0814 19:15:11.162394   613 solver.cpp:312] Iteration 60500 (61.8085 iter/s, 1.6179s/100 iter), loss = 0.00134083
I0814 19:15:11.162458   613 solver.cpp:334]     Train net output #0: loss = 0.00134063 (* 1 = 0.00134063 loss)
I0814 19:15:11.162478   613 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0814 19:15:12.801955   613 solver.cpp:312] Iteration 60600 (60.9939 iter/s, 1.63951s/100 iter), loss = 0.00305291
I0814 19:15:12.802022   613 solver.cpp:334]     Train net output #0: loss = 0.00305271 (* 1 = 0.00305271 loss)
I0814 19:15:12.802047   613 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0814 19:15:12.860913   592 data_reader.cpp:288] Starting prefetch of epoch 8
I0814 19:15:14.401856   613 solver.cpp:312] Iteration 60700 (62.5058 iter/s, 1.59985s/100 iter), loss = 0.00130965
I0814 19:15:14.402112   613 solver.cpp:334]     Train net output #0: loss = 0.00130946 (* 1 = 0.00130946 loss)
I0814 19:15:14.402118   613 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0814 19:15:16.041265   613 solver.cpp:312] Iteration 60800 (60.9996 iter/s, 1.63936s/100 iter), loss = 0.00204671
I0814 19:15:16.041287   613 solver.cpp:334]     Train net output #0: loss = 0.00204652 (* 1 = 0.00204652 loss)
I0814 19:15:16.041293   613 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0814 19:15:17.673768   613 solver.cpp:312] Iteration 60900 (61.2575 iter/s, 1.63245s/100 iter), loss = 0.00169358
I0814 19:15:17.673790   613 solver.cpp:334]     Train net output #0: loss = 0.00169339 (* 1 = 0.00169339 loss)
I0814 19:15:17.673794   613 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0814 19:15:19.252734   613 solver.cpp:509] Iteration 61000, Testing net (#0)
I0814 19:15:20.083407   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909413
I0814 19:15:20.083425   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:15:20.083431   613 solver.cpp:594]     Test net output #2: loss = 0.341405 (* 1 = 0.341405 loss)
I0814 19:15:20.083480   613 solver.cpp:264] [MultiGPU] Tests completed in 0.830724s
I0814 19:15:20.098951   613 solver.cpp:312] Iteration 61000 (41.2352 iter/s, 2.42511s/100 iter), loss = 0.000427757
I0814 19:15:20.098968   613 solver.cpp:334]     Train net output #0: loss = 0.000427564 (* 1 = 0.000427564 loss)
I0814 19:15:20.098973   613 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0814 19:15:21.705335   613 solver.cpp:312] Iteration 61100 (62.2536 iter/s, 1.60633s/100 iter), loss = 0.00085722
I0814 19:15:21.705358   613 solver.cpp:334]     Train net output #0: loss = 0.000857027 (* 1 = 0.000857027 loss)
I0814 19:15:21.705363   613 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0814 19:15:23.297592   613 solver.cpp:312] Iteration 61200 (62.8059 iter/s, 1.59221s/100 iter), loss = 0.000820572
I0814 19:15:23.297737   613 solver.cpp:334]     Train net output #0: loss = 0.000820379 (* 1 = 0.000820379 loss)
I0814 19:15:23.297745   613 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0814 19:15:24.953053   613 solver.cpp:312] Iteration 61300 (60.408 iter/s, 1.65541s/100 iter), loss = 0.001401
I0814 19:15:24.953122   613 solver.cpp:334]     Train net output #0: loss = 0.00140081 (* 1 = 0.00140081 loss)
I0814 19:15:24.953145   613 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0814 19:15:26.575523   613 solver.cpp:312] Iteration 61400 (61.6363 iter/s, 1.62242s/100 iter), loss = 0.00141695
I0814 19:15:26.575546   613 solver.cpp:334]     Train net output #0: loss = 0.00141675 (* 1 = 0.00141675 loss)
I0814 19:15:26.575552   613 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0814 19:15:28.229060   613 solver.cpp:312] Iteration 61500 (60.4784 iter/s, 1.65348s/100 iter), loss = 0.0028573
I0814 19:15:28.229107   613 solver.cpp:334]     Train net output #0: loss = 0.0028571 (* 1 = 0.0028571 loss)
I0814 19:15:28.229120   613 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0814 19:15:29.844717   613 solver.cpp:312] Iteration 61600 (61.8962 iter/s, 1.61561s/100 iter), loss = 0.0024157
I0814 19:15:29.844741   613 solver.cpp:334]     Train net output #0: loss = 0.00241551 (* 1 = 0.00241551 loss)
I0814 19:15:29.844748   613 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0814 19:15:31.423151   613 solver.cpp:312] Iteration 61700 (63.356 iter/s, 1.57838s/100 iter), loss = 0.000199069
I0814 19:15:31.423221   613 solver.cpp:334]     Train net output #0: loss = 0.000198876 (* 1 = 0.000198876 loss)
I0814 19:15:31.423243   613 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0814 19:15:33.051873   613 solver.cpp:312] Iteration 61800 (61.3996 iter/s, 1.62867s/100 iter), loss = 0.000304017
I0814 19:15:33.051897   613 solver.cpp:334]     Train net output #0: loss = 0.000303823 (* 1 = 0.000303823 loss)
I0814 19:15:33.051901   613 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0814 19:15:34.698359   613 solver.cpp:312] Iteration 61900 (60.7373 iter/s, 1.64644s/100 iter), loss = 0.00174319
I0814 19:15:34.698385   613 solver.cpp:334]     Train net output #0: loss = 0.001743 (* 1 = 0.001743 loss)
I0814 19:15:34.698390   613 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0814 19:15:36.291245   613 solver.cpp:509] Iteration 62000, Testing net (#0)
I0814 19:15:37.118389   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914119
I0814 19:15:37.118407   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:15:37.118412   613 solver.cpp:594]     Test net output #2: loss = 0.329698 (* 1 = 0.329698 loss)
I0814 19:15:37.118427   613 solver.cpp:264] [MultiGPU] Tests completed in 0.82716s
I0814 19:15:37.134057   613 solver.cpp:312] Iteration 62000 (41.0572 iter/s, 2.43563s/100 iter), loss = 0.000569987
I0814 19:15:37.134073   613 solver.cpp:334]     Train net output #0: loss = 0.000569794 (* 1 = 0.000569794 loss)
I0814 19:15:37.134075   613 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0814 19:15:38.821662   613 solver.cpp:312] Iteration 62100 (59.2575 iter/s, 1.68755s/100 iter), loss = 0.000310227
I0814 19:15:38.821719   613 solver.cpp:334]     Train net output #0: loss = 0.000310034 (* 1 = 0.000310034 loss)
I0814 19:15:38.821743   613 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0814 19:15:40.432026   613 solver.cpp:312] Iteration 62200 (62.0998 iter/s, 1.61031s/100 iter), loss = 0.000879724
I0814 19:15:40.432091   613 solver.cpp:334]     Train net output #0: loss = 0.00087953 (* 1 = 0.00087953 loss)
I0814 19:15:40.432109   613 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0814 19:15:42.095234   613 solver.cpp:312] Iteration 62300 (60.1266 iter/s, 1.66316s/100 iter), loss = 0.000756207
I0814 19:15:42.095297   613 solver.cpp:334]     Train net output #0: loss = 0.000756013 (* 1 = 0.000756013 loss)
I0814 19:15:42.095314   613 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0814 19:15:43.735851   613 solver.cpp:312] Iteration 62400 (60.9545 iter/s, 1.64057s/100 iter), loss = 0.000303245
I0814 19:15:43.735909   613 solver.cpp:334]     Train net output #0: loss = 0.000303052 (* 1 = 0.000303052 loss)
I0814 19:15:43.735929   613 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0814 19:15:45.360646   613 solver.cpp:312] Iteration 62500 (61.5483 iter/s, 1.62474s/100 iter), loss = 0.000848219
I0814 19:15:45.360692   613 solver.cpp:334]     Train net output #0: loss = 0.000848026 (* 1 = 0.000848026 loss)
I0814 19:15:45.360707   613 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0814 19:15:46.972729   613 solver.cpp:312] Iteration 62600 (62.0334 iter/s, 1.61204s/100 iter), loss = 0.00147055
I0814 19:15:46.972776   613 solver.cpp:334]     Train net output #0: loss = 0.00147035 (* 1 = 0.00147035 loss)
I0814 19:15:46.972790   613 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0814 19:15:48.579478   613 solver.cpp:312] Iteration 62700 (62.2394 iter/s, 1.6067s/100 iter), loss = 0.000820019
I0814 19:15:48.579538   613 solver.cpp:334]     Train net output #0: loss = 0.000819824 (* 1 = 0.000819824 loss)
I0814 19:15:48.579557   613 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0814 19:15:50.200582   613 solver.cpp:312] Iteration 62800 (61.6884 iter/s, 1.62105s/100 iter), loss = 0.00142233
I0814 19:15:50.200634   613 solver.cpp:334]     Train net output #0: loss = 0.00142213 (* 1 = 0.00142213 loss)
I0814 19:15:50.200647   613 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0814 19:15:51.812494   613 solver.cpp:312] Iteration 62900 (62.0401 iter/s, 1.61186s/100 iter), loss = 0.00137886
I0814 19:15:51.812556   613 solver.cpp:334]     Train net output #0: loss = 0.00137866 (* 1 = 0.00137866 loss)
I0814 19:15:51.812574   613 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0814 19:15:53.421255   613 solver.cpp:509] Iteration 63000, Testing net (#0)
I0814 19:15:54.238636   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912648
I0814 19:15:54.238656   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:15:54.238661   613 solver.cpp:594]     Test net output #2: loss = 0.340784 (* 1 = 0.340784 loss)
I0814 19:15:54.238674   613 solver.cpp:264] [MultiGPU] Tests completed in 0.817398s
I0814 19:15:54.254387   613 solver.cpp:312] Iteration 63000 (40.953 iter/s, 2.44183s/100 iter), loss = 0.00172038
I0814 19:15:54.254403   613 solver.cpp:334]     Train net output #0: loss = 0.00172019 (* 1 = 0.00172019 loss)
I0814 19:15:54.254407   613 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0814 19:15:55.850627   613 solver.cpp:312] Iteration 63100 (62.6494 iter/s, 1.59619s/100 iter), loss = 0.00104731
I0814 19:15:55.850656   613 solver.cpp:334]     Train net output #0: loss = 0.00104711 (* 1 = 0.00104711 loss)
I0814 19:15:55.850663   613 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0814 19:15:57.510680   613 solver.cpp:312] Iteration 63200 (60.2408 iter/s, 1.66s/100 iter), loss = 0.00151257
I0814 19:15:57.510705   613 solver.cpp:334]     Train net output #0: loss = 0.00151237 (* 1 = 0.00151237 loss)
I0814 19:15:57.510712   613 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0814 19:15:59.134155   613 solver.cpp:312] Iteration 63300 (61.5981 iter/s, 1.62343s/100 iter), loss = 0.00130969
I0814 19:15:59.134179   613 solver.cpp:334]     Train net output #0: loss = 0.00130949 (* 1 = 0.00130949 loss)
I0814 19:15:59.134186   613 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0814 19:16:00.771884   613 solver.cpp:312] Iteration 63400 (61.0621 iter/s, 1.63768s/100 iter), loss = 0.000394305
I0814 19:16:00.771908   613 solver.cpp:334]     Train net output #0: loss = 0.000394112 (* 1 = 0.000394112 loss)
I0814 19:16:00.771914   613 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0814 19:16:02.340396   613 solver.cpp:312] Iteration 63500 (63.7567 iter/s, 1.56846s/100 iter), loss = 0.000177362
I0814 19:16:02.340423   613 solver.cpp:334]     Train net output #0: loss = 0.000177169 (* 1 = 0.000177169 loss)
I0814 19:16:02.340430   613 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0814 19:16:03.988221   613 solver.cpp:312] Iteration 63600 (60.688 iter/s, 1.64777s/100 iter), loss = 0.00163312
I0814 19:16:03.988248   613 solver.cpp:334]     Train net output #0: loss = 0.00163293 (* 1 = 0.00163293 loss)
I0814 19:16:03.988255   613 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0814 19:16:05.639859   613 solver.cpp:312] Iteration 63700 (60.5478 iter/s, 1.65159s/100 iter), loss = 0.000635101
I0814 19:16:05.639886   613 solver.cpp:334]     Train net output #0: loss = 0.000634909 (* 1 = 0.000634909 loss)
I0814 19:16:05.639892   613 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0814 19:16:07.264003   613 solver.cpp:312] Iteration 63800 (61.5728 iter/s, 1.62409s/100 iter), loss = 0.000968047
I0814 19:16:07.264032   613 solver.cpp:334]     Train net output #0: loss = 0.000967856 (* 1 = 0.000967856 loss)
I0814 19:16:07.264039   613 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0814 19:16:08.872730   613 solver.cpp:312] Iteration 63900 (62.163 iter/s, 1.60868s/100 iter), loss = 0.00146501
I0814 19:16:08.872755   613 solver.cpp:334]     Train net output #0: loss = 0.00146482 (* 1 = 0.00146482 loss)
I0814 19:16:08.872761   613 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0814 19:16:10.465710   613 solver.cpp:312] Iteration 63999 (62.1496 iter/s, 1.59293s/99 iter), loss = 0.000846131
I0814 19:16:10.465735   613 solver.cpp:334]     Train net output #0: loss = 0.000845939 (* 1 = 0.000845939 loss)
I0814 19:16:10.465864   613 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 19:16:10.473784   613 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_64000.solverstate
I0814 19:16:10.482251   613 solver.cpp:486] Iteration 64000, loss = 0.00069849
I0814 19:16:10.482272   613 solver.cpp:509] Iteration 64000, Testing net (#0)
I0814 19:16:11.291815   613 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.908236
I0814 19:16:11.291836   613 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:16:11.291841   613 solver.cpp:594]     Test net output #2: loss = 0.358975 (* 1 = 0.358975 loss)
I0814 19:16:11.294932   556 parallel.cpp:71] Root Solver performance on device 0: 58.75 * 22 = 1292 img/sec (64000 itr in 1089 sec)
I0814 19:16:11.294945   556 parallel.cpp:76]      Solver performance on device 1: 58.75 * 22 = 1292 img/sec (64000 itr in 1089 sec)
I0814 19:16:11.294950   556 parallel.cpp:76]      Solver performance on device 2: 58.75 * 22 = 1292 img/sec (64000 itr in 1089 sec)
I0814 19:16:11.294951   556 parallel.cpp:79] Overall multi-GPU performance: 3877.4 img/sec
I0814 19:16:11.368559   556 caffe.cpp:247] Optimization Done in 18m 13s
I0814 19:16:12.246976 24692 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 19:16:12 2017
I0814 19:16:12.247100 24692 caffe.cpp:611] CuDNN version: 6021
I0814 19:16:12.247105 24692 caffe.cpp:612] CuBLAS version: 8000
I0814 19:16:12.247108 24692 caffe.cpp:613] CUDA version: 8000
I0814 19:16:12.247110 24692 caffe.cpp:614] CUDA driver version: 8000
I0814 19:16:12.496837 24692 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 19:16:12.497409 24692 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 19:16:12.497931 24692 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0814 19:16:12.498445 24692 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0814 19:16:12.498456 24692 caffe.cpp:208] Using GPUs 0, 1, 2
I0814 19:16:12.498781 24692 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0814 19:16:12.499105 24692 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0814 19:16:12.499429 24692 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0814 19:16:12.499464 24692 solver.cpp:42] Solver data type: FLOAT
I0814 19:16:12.499500 24692 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0814 19:16:12.506762 24692 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/train.prototxt
I0814 19:16:12.507329 24692 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0814 19:16:12.507345 24692 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0814 19:16:12.507385 24692 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:12.507647 24692 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0814 19:16:12.507796 24692 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:12.507800 24692 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:12.507805 24692 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:16:12.507835 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.507938 24692 net.cpp:184] Created Layer data (0)
I0814 19:16:12.507954 24692 net.cpp:530] data -> data
I0814 19:16:12.507982 24692 net.cpp:530] data -> label
I0814 19:16:12.508016 24692 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:12.508038 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.508898 24716 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:12.510085 24692 data_layer.cpp:185] [0] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:12.510177 24692 data_layer.cpp:209] [0] Output data size: 22, 3, 32, 32
I0814 19:16:12.510185 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.510208 24692 net.cpp:245] Setting up data
I0814 19:16:12.510217 24692 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0814 19:16:12.510224 24692 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0814 19:16:12.510232 24692 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:16:12.510236 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.510252 24692 net.cpp:184] Created Layer data/bias (1)
I0814 19:16:12.510257 24692 net.cpp:561] data/bias <- data
I0814 19:16:12.510264 24692 net.cpp:530] data/bias -> data/bias
I0814 19:16:12.512287 24692 net.cpp:245] Setting up data/bias
I0814 19:16:12.512300 24692 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0814 19:16:12.512310 24692 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:16:12.512315 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.512328 24692 net.cpp:184] Created Layer conv1a (2)
I0814 19:16:12.512331 24692 net.cpp:561] conv1a <- data/bias
I0814 19:16:12.512334 24692 net.cpp:530] conv1a -> conv1a
I0814 19:16:12.798638 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0814 19:16:12.798658 24692 net.cpp:245] Setting up conv1a
I0814 19:16:12.798665 24692 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0814 19:16:12.798672 24692 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:16:12.798676 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.798686 24692 net.cpp:184] Created Layer conv1a/bn (3)
I0814 19:16:12.798689 24692 net.cpp:561] conv1a/bn <- conv1a
I0814 19:16:12.798694 24692 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:16:12.799355 24692 net.cpp:245] Setting up conv1a/bn
I0814 19:16:12.799362 24692 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0814 19:16:12.799368 24692 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:16:12.799371 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.799376 24692 net.cpp:184] Created Layer conv1a/relu (4)
I0814 19:16:12.799378 24692 net.cpp:561] conv1a/relu <- conv1a
I0814 19:16:12.799381 24692 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:16:12.799393 24692 net.cpp:245] Setting up conv1a/relu
I0814 19:16:12.799396 24692 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0814 19:16:12.799398 24692 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:16:12.799401 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.799408 24692 net.cpp:184] Created Layer conv1b (5)
I0814 19:16:12.799410 24692 net.cpp:561] conv1b <- conv1a
I0814 19:16:12.799413 24692 net.cpp:530] conv1b -> conv1b
I0814 19:16:12.806608 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0814 19:16:12.806619 24692 net.cpp:245] Setting up conv1b
I0814 19:16:12.806624 24692 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0814 19:16:12.806630 24692 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:16:12.806633 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.806651 24692 net.cpp:184] Created Layer conv1b/bn (6)
I0814 19:16:12.806658 24692 net.cpp:561] conv1b/bn <- conv1b
I0814 19:16:12.806660 24692 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:16:12.807273 24692 net.cpp:245] Setting up conv1b/bn
I0814 19:16:12.807284 24692 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0814 19:16:12.807291 24692 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:16:12.807294 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807301 24692 net.cpp:184] Created Layer conv1b/relu (7)
I0814 19:16:12.807303 24692 net.cpp:561] conv1b/relu <- conv1b
I0814 19:16:12.807307 24692 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:16:12.807312 24692 net.cpp:245] Setting up conv1b/relu
I0814 19:16:12.807317 24692 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0814 19:16:12.807320 24692 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:16:12.807324 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807332 24692 net.cpp:184] Created Layer pool1 (8)
I0814 19:16:12.807337 24692 net.cpp:561] pool1 <- conv1b
I0814 19:16:12.807339 24692 net.cpp:530] pool1 -> pool1
I0814 19:16:12.807416 24692 net.cpp:245] Setting up pool1
I0814 19:16:12.807422 24692 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0814 19:16:12.807425 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:16:12.807430 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.807441 24692 net.cpp:184] Created Layer res2a_branch2a (9)
I0814 19:16:12.807445 24692 net.cpp:561] res2a_branch2a <- pool1
I0814 19:16:12.807447 24692 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:16:12.816977 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0814 19:16:12.816990 24692 net.cpp:245] Setting up res2a_branch2a
I0814 19:16:12.816994 24692 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0814 19:16:12.817004 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.817008 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817014 24692 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0814 19:16:12.817018 24692 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:16:12.817020 24692 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:16:12.817644 24692 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:16:12.817652 24692 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0814 19:16:12.817658 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.817662 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817665 24692 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0814 19:16:12.817668 24692 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:16:12.817672 24692 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:16:12.817675 24692 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:16:12.817678 24692 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0814 19:16:12.817680 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:16:12.817683 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.817694 24692 net.cpp:184] Created Layer res2a_branch2b (12)
I0814 19:16:12.817698 24692 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:16:12.817701 24692 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:16:12.824290 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0814 19:16:12.824301 24692 net.cpp:245] Setting up res2a_branch2b
I0814 19:16:12.824306 24692 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0814 19:16:12.824314 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.824318 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824322 24692 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0814 19:16:12.824326 24692 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:16:12.824328 24692 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:16:12.824935 24692 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:16:12.824942 24692 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0814 19:16:12.824949 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.824952 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824955 24692 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0814 19:16:12.824959 24692 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:16:12.824960 24692 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:16:12.824965 24692 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:16:12.824967 24692 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0814 19:16:12.824970 24692 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:16:12.824972 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.824976 24692 net.cpp:184] Created Layer pool2 (15)
I0814 19:16:12.824980 24692 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:16:12.824981 24692 net.cpp:530] pool2 -> pool2
I0814 19:16:12.825037 24692 net.cpp:245] Setting up pool2
I0814 19:16:12.825042 24692 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0814 19:16:12.825044 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:16:12.825047 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.825054 24692 net.cpp:184] Created Layer res3a_branch2a (16)
I0814 19:16:12.825057 24692 net.cpp:561] res3a_branch2a <- pool2
I0814 19:16:12.825059 24692 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:16:12.835561 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0814 19:16:12.835579 24692 net.cpp:245] Setting up res3a_branch2a
I0814 19:16:12.835585 24692 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0814 19:16:12.835592 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.835597 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.835605 24692 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0814 19:16:12.835609 24692 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:16:12.835613 24692 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:16:12.836254 24692 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:16:12.836263 24692 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0814 19:16:12.836272 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.836274 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.836278 24692 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0814 19:16:12.836282 24692 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:16:12.836283 24692 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:16:12.836287 24692 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:16:12.836300 24692 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0814 19:16:12.836304 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:16:12.836308 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.836320 24692 net.cpp:184] Created Layer res3a_branch2b (19)
I0814 19:16:12.836325 24692 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:16:12.836328 24692 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:16:12.841171 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0814 19:16:12.841181 24692 net.cpp:245] Setting up res3a_branch2b
I0814 19:16:12.841184 24692 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0814 19:16:12.841189 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.841192 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841197 24692 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0814 19:16:12.841200 24692 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:16:12.841203 24692 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:16:12.841784 24692 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:16:12.841790 24692 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0814 19:16:12.841796 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.841800 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841804 24692 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0814 19:16:12.841805 24692 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:16:12.841807 24692 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:16:12.841811 24692 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:16:12.841814 24692 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0814 19:16:12.841816 24692 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:16:12.841820 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841823 24692 net.cpp:184] Created Layer pool3 (22)
I0814 19:16:12.841825 24692 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:16:12.841828 24692 net.cpp:530] pool3 -> pool3
I0814 19:16:12.841886 24692 net.cpp:245] Setting up pool3
I0814 19:16:12.841892 24692 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0814 19:16:12.841893 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:16:12.841897 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.841904 24692 net.cpp:184] Created Layer res4a_branch2a (23)
I0814 19:16:12.841908 24692 net.cpp:561] res4a_branch2a <- pool3
I0814 19:16:12.841912 24692 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:16:12.860702 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0814 19:16:12.860720 24692 net.cpp:245] Setting up res4a_branch2a
I0814 19:16:12.860726 24692 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0814 19:16:12.860734 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.860740 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.860750 24692 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0814 19:16:12.860755 24692 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:16:12.860760 24692 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:16:12.861424 24692 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:16:12.861433 24692 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0814 19:16:12.861452 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.861457 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.861464 24692 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0814 19:16:12.861467 24692 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:16:12.861471 24692 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:16:12.861479 24692 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:16:12.861484 24692 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0814 19:16:12.861487 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:16:12.861491 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.861501 24692 net.cpp:184] Created Layer res4a_branch2b (26)
I0814 19:16:12.861505 24692 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:16:12.861510 24692 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:16:12.869359 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0814 19:16:12.869374 24692 net.cpp:245] Setting up res4a_branch2b
I0814 19:16:12.869379 24692 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0814 19:16:12.869386 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.869391 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.869400 24692 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0814 19:16:12.869403 24692 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:16:12.869408 24692 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:16:12.870019 24692 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:16:12.870028 24692 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0814 19:16:12.870039 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.870043 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870049 24692 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0814 19:16:12.870054 24692 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:16:12.870059 24692 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:16:12.870064 24692 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:16:12.870069 24692 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0814 19:16:12.870074 24692 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:16:12.870077 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870085 24692 net.cpp:184] Created Layer pool4 (29)
I0814 19:16:12.870088 24692 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:16:12.870093 24692 net.cpp:530] pool4 -> pool4
I0814 19:16:12.870153 24692 net.cpp:245] Setting up pool4
I0814 19:16:12.870159 24692 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0814 19:16:12.870164 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:16:12.870168 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.870177 24692 net.cpp:184] Created Layer res5a_branch2a (30)
I0814 19:16:12.870180 24692 net.cpp:561] res5a_branch2a <- pool4
I0814 19:16:12.870184 24692 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:16:12.911263 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0814 19:16:12.911281 24692 net.cpp:245] Setting up res5a_branch2a
I0814 19:16:12.911288 24692 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0814 19:16:12.911308 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.911312 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.911322 24692 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0814 19:16:12.911327 24692 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:16:12.911334 24692 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:16:12.912051 24692 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:16:12.912060 24692 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0814 19:16:12.912070 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.912073 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.912081 24692 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0814 19:16:12.912084 24692 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:16:12.912088 24692 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:16:12.912096 24692 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:16:12.912101 24692 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0814 19:16:12.912104 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:16:12.912109 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.912123 24692 net.cpp:184] Created Layer res5a_branch2b (33)
I0814 19:16:12.912127 24692 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:16:12.912137 24692 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:16:12.930754 24692 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0814 19:16:12.930773 24692 net.cpp:245] Setting up res5a_branch2b
I0814 19:16:12.930779 24692 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0814 19:16:12.930790 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.930796 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.930806 24692 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0814 19:16:12.930811 24692 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:16:12.930816 24692 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:16:12.931473 24692 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:16:12.931483 24692 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0814 19:16:12.931490 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.931495 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931500 24692 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0814 19:16:12.931504 24692 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:16:12.931509 24692 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:16:12.931515 24692 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:16:12.931520 24692 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0814 19:16:12.931524 24692 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:16:12.931529 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931535 24692 net.cpp:184] Created Layer pool5 (36)
I0814 19:16:12.931540 24692 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:16:12.931543 24692 net.cpp:530] pool5 -> pool5
I0814 19:16:12.931577 24692 net.cpp:245] Setting up pool5
I0814 19:16:12.931583 24692 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0814 19:16:12.931587 24692 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 19:16:12.931591 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931607 24692 net.cpp:184] Created Layer fc10 (37)
I0814 19:16:12.931612 24692 net.cpp:561] fc10 <- pool5
I0814 19:16:12.931617 24692 net.cpp:530] fc10 -> fc10
I0814 19:16:12.931886 24692 net.cpp:245] Setting up fc10
I0814 19:16:12.931893 24692 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0814 19:16:12.931901 24692 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:16:12.931905 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.931917 24692 net.cpp:184] Created Layer loss (38)
I0814 19:16:12.931921 24692 net.cpp:561] loss <- fc10
I0814 19:16:12.931926 24692 net.cpp:561] loss <- label
I0814 19:16:12.931931 24692 net.cpp:530] loss -> loss
I0814 19:16:12.932085 24692 net.cpp:245] Setting up loss
I0814 19:16:12.932092 24692 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0814 19:16:12.932096 24692 net.cpp:256]     with loss weight 1
I0814 19:16:12.932102 24692 net.cpp:323] loss needs backward computation.
I0814 19:16:12.932106 24692 net.cpp:323] fc10 needs backward computation.
I0814 19:16:12.932111 24692 net.cpp:323] pool5 needs backward computation.
I0814 19:16:12.932114 24692 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:16:12.932118 24692 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:16:12.932122 24692 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:16:12.932126 24692 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:16:12.932137 24692 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:16:12.932140 24692 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:16:12.932144 24692 net.cpp:323] pool4 needs backward computation.
I0814 19:16:12.932148 24692 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:16:12.932152 24692 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:16:12.932155 24692 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:16:12.932159 24692 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:16:12.932163 24692 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:16:12.932168 24692 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:16:12.932171 24692 net.cpp:323] pool3 needs backward computation.
I0814 19:16:12.932175 24692 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:16:12.932179 24692 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:16:12.932183 24692 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:16:12.932188 24692 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:16:12.932191 24692 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:16:12.932195 24692 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:16:12.932199 24692 net.cpp:323] pool2 needs backward computation.
I0814 19:16:12.932204 24692 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:16:12.932207 24692 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:16:12.932210 24692 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:16:12.932214 24692 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:16:12.932219 24692 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:16:12.932221 24692 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:16:12.932225 24692 net.cpp:323] pool1 needs backward computation.
I0814 19:16:12.932229 24692 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:16:12.932234 24692 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:16:12.932237 24692 net.cpp:323] conv1b needs backward computation.
I0814 19:16:12.932241 24692 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:16:12.932245 24692 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:16:12.932248 24692 net.cpp:323] conv1a needs backward computation.
I0814 19:16:12.932258 24692 net.cpp:325] data/bias does not need backward computation.
I0814 19:16:12.932263 24692 net.cpp:325] data does not need backward computation.
I0814 19:16:12.932267 24692 net.cpp:367] This network produces output loss
I0814 19:16:12.932296 24692 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0814 19:16:12.932299 24692 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0814 19:16:12.932303 24692 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0814 19:16:12.932307 24692 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0814 19:16:12.932310 24692 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0814 19:16:12.932313 24692 net.cpp:407] Network initialization done.
I0814 19:16:12.932672 24692 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:12.932719 24692 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:12.932844 24692 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 19:16:12.932938 24692 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:12.932943 24692 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:12.932947 24692 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:16:12.932951 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.932963 24692 net.cpp:184] Created Layer data (0)
I0814 19:16:12.932967 24692 net.cpp:530] data -> data
I0814 19:16:12.932972 24692 net.cpp:530] data -> label
I0814 19:16:12.932982 24692 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:12.932991 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.934274 24730 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:12.934339 24692 data_layer.cpp:185] (0) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:12.934404 24692 data_layer.cpp:209] (0) Output data size: 17, 3, 32, 32
I0814 19:16:12.934408 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:12.934422 24692 net.cpp:245] Setting up data
I0814 19:16:12.934428 24692 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0814 19:16:12.934434 24692 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0814 19:16:12.934438 24692 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 19:16:12.934443 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934454 24692 net.cpp:184] Created Layer label_data_1_split (1)
I0814 19:16:12.934459 24692 net.cpp:561] label_data_1_split <- label
I0814 19:16:12.934464 24692 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 19:16:12.934469 24692 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 19:16:12.934474 24692 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 19:16:12.934535 24692 net.cpp:245] Setting up label_data_1_split
I0814 19:16:12.934540 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934545 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934550 24692 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0814 19:16:12.934553 24692 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:16:12.934557 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934564 24692 net.cpp:184] Created Layer data/bias (2)
I0814 19:16:12.934568 24692 net.cpp:561] data/bias <- data
I0814 19:16:12.934572 24692 net.cpp:530] data/bias -> data/bias
I0814 19:16:12.934705 24692 net.cpp:245] Setting up data/bias
I0814 19:16:12.934712 24692 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0814 19:16:12.934720 24692 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:16:12.934723 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.934733 24692 net.cpp:184] Created Layer conv1a (3)
I0814 19:16:12.934737 24692 net.cpp:561] conv1a <- data/bias
I0814 19:16:12.934741 24692 net.cpp:530] conv1a -> conv1a
I0814 19:16:12.935092 24731 data_layer.cpp:97] (0) Parser threads: 1
I0814 19:16:12.935101 24731 data_layer.cpp:99] (0) Transformer threads: 1
I0814 19:16:12.937893 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0814 19:16:12.937904 24692 net.cpp:245] Setting up conv1a
I0814 19:16:12.937911 24692 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0814 19:16:12.937921 24692 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:16:12.937924 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.937937 24692 net.cpp:184] Created Layer conv1a/bn (4)
I0814 19:16:12.937940 24692 net.cpp:561] conv1a/bn <- conv1a
I0814 19:16:12.937944 24692 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:16:12.938659 24692 net.cpp:245] Setting up conv1a/bn
I0814 19:16:12.938668 24692 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0814 19:16:12.938678 24692 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:16:12.938683 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.938688 24692 net.cpp:184] Created Layer conv1a/relu (5)
I0814 19:16:12.938691 24692 net.cpp:561] conv1a/relu <- conv1a
I0814 19:16:12.938695 24692 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:16:12.938701 24692 net.cpp:245] Setting up conv1a/relu
I0814 19:16:12.938706 24692 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0814 19:16:12.938710 24692 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:16:12.938715 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.938724 24692 net.cpp:184] Created Layer conv1b (6)
I0814 19:16:12.938727 24692 net.cpp:561] conv1b <- conv1a
I0814 19:16:12.938731 24692 net.cpp:530] conv1b -> conv1b
I0814 19:16:12.941779 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0814 19:16:12.941790 24692 net.cpp:245] Setting up conv1b
I0814 19:16:12.941797 24692 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0814 19:16:12.941804 24692 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:16:12.941814 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.941823 24692 net.cpp:184] Created Layer conv1b/bn (7)
I0814 19:16:12.941826 24692 net.cpp:561] conv1b/bn <- conv1b
I0814 19:16:12.941830 24692 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:16:12.942487 24692 net.cpp:245] Setting up conv1b/bn
I0814 19:16:12.942494 24692 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0814 19:16:12.942504 24692 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:16:12.942508 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942513 24692 net.cpp:184] Created Layer conv1b/relu (8)
I0814 19:16:12.942517 24692 net.cpp:561] conv1b/relu <- conv1b
I0814 19:16:12.942522 24692 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:16:12.942528 24692 net.cpp:245] Setting up conv1b/relu
I0814 19:16:12.942533 24692 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0814 19:16:12.942536 24692 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:16:12.942540 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942546 24692 net.cpp:184] Created Layer pool1 (9)
I0814 19:16:12.942550 24692 net.cpp:561] pool1 <- conv1b
I0814 19:16:12.942554 24692 net.cpp:530] pool1 -> pool1
I0814 19:16:12.942621 24692 net.cpp:245] Setting up pool1
I0814 19:16:12.942627 24692 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0814 19:16:12.942632 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:16:12.942636 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.942646 24692 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 19:16:12.942649 24692 net.cpp:561] res2a_branch2a <- pool1
I0814 19:16:12.942653 24692 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:16:12.946233 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0814 19:16:12.946244 24692 net.cpp:245] Setting up res2a_branch2a
I0814 19:16:12.946250 24692 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0814 19:16:12.946259 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.946262 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946269 24692 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 19:16:12.946274 24692 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:16:12.946279 24692 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:16:12.946931 24692 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:16:12.946940 24692 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0814 19:16:12.946950 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.946954 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946959 24692 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 19:16:12.946964 24692 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:16:12.946969 24692 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:16:12.946976 24692 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:16:12.946980 24692 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0814 19:16:12.946985 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:16:12.946990 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.946997 24692 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 19:16:12.947001 24692 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:16:12.947005 24692 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:16:12.950170 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0814 19:16:12.950181 24692 net.cpp:245] Setting up res2a_branch2b
I0814 19:16:12.950187 24692 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0814 19:16:12.950196 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.950199 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950206 24692 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 19:16:12.950211 24692 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:16:12.950215 24692 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:16:12.950882 24692 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:16:12.950891 24692 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0814 19:16:12.950898 24692 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.950902 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950908 24692 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 19:16:12.950912 24692 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:16:12.950917 24692 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:16:12.950923 24692 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:16:12.950928 24692 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0814 19:16:12.950932 24692 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:16:12.950937 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.950943 24692 net.cpp:184] Created Layer pool2 (16)
I0814 19:16:12.950947 24692 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:16:12.950950 24692 net.cpp:530] pool2 -> pool2
I0814 19:16:12.951021 24692 net.cpp:245] Setting up pool2
I0814 19:16:12.951027 24692 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0814 19:16:12.951031 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:16:12.951036 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.951045 24692 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 19:16:12.951048 24692 net.cpp:561] res3a_branch2a <- pool2
I0814 19:16:12.951052 24692 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:16:12.956998 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0814 19:16:12.957010 24692 net.cpp:245] Setting up res3a_branch2a
I0814 19:16:12.957015 24692 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0814 19:16:12.957021 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.957026 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957033 24692 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 19:16:12.957037 24692 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:16:12.957041 24692 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:16:12.957693 24692 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:16:12.957701 24692 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0814 19:16:12.957712 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.957716 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957722 24692 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 19:16:12.957726 24692 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:16:12.957731 24692 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:16:12.957736 24692 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:16:12.957749 24692 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0814 19:16:12.957753 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:16:12.957758 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.957767 24692 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 19:16:12.957772 24692 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:16:12.957775 24692 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:16:12.960934 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0814 19:16:12.960944 24692 net.cpp:245] Setting up res3a_branch2b
I0814 19:16:12.960950 24692 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0814 19:16:12.960958 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.960961 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.960968 24692 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 19:16:12.960973 24692 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:16:12.960978 24692 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:16:12.961640 24692 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:16:12.961649 24692 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0814 19:16:12.961658 24692 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.961663 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961668 24692 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 19:16:12.961671 24692 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:16:12.961676 24692 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:16:12.961683 24692 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:16:12.961688 24692 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0814 19:16:12.961691 24692 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:16:12.961695 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961701 24692 net.cpp:184] Created Layer pool3 (23)
I0814 19:16:12.961705 24692 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:16:12.961709 24692 net.cpp:530] pool3 -> pool3
I0814 19:16:12.961778 24692 net.cpp:245] Setting up pool3
I0814 19:16:12.961784 24692 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0814 19:16:12.961789 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:16:12.961793 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.961802 24692 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 19:16:12.961805 24692 net.cpp:561] res4a_branch2a <- pool3
I0814 19:16:12.961809 24692 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:16:12.972277 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:12.972290 24692 net.cpp:245] Setting up res4a_branch2a
I0814 19:16:12.972295 24692 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0814 19:16:12.972303 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:12.972308 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.972316 24692 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 19:16:12.972321 24692 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:16:12.972324 24692 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:16:12.973012 24692 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:16:12.973021 24692 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0814 19:16:12.973037 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:16:12.973042 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.973047 24692 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 19:16:12.973052 24692 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:16:12.973055 24692 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:16:12.973062 24692 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:16:12.973067 24692 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0814 19:16:12.973070 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:16:12.973074 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.973088 24692 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 19:16:12.973091 24692 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:16:12.973095 24692 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:16:12.978668 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:12.978679 24692 net.cpp:245] Setting up res4a_branch2b
I0814 19:16:12.978691 24692 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0814 19:16:12.978698 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:12.978703 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.978711 24692 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 19:16:12.978715 24692 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:16:12.978719 24692 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:16:12.979394 24692 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:16:12.979403 24692 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0814 19:16:12.979413 24692 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:16:12.979416 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979423 24692 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 19:16:12.979426 24692 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:16:12.979432 24692 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:16:12.979439 24692 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:16:12.979444 24692 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0814 19:16:12.979447 24692 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:16:12.979451 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979459 24692 net.cpp:184] Created Layer pool4 (30)
I0814 19:16:12.979462 24692 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:16:12.979466 24692 net.cpp:530] pool4 -> pool4
I0814 19:16:12.979537 24692 net.cpp:245] Setting up pool4
I0814 19:16:12.979542 24692 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0814 19:16:12.979547 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:16:12.979552 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:12.979559 24692 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 19:16:12.979563 24692 net.cpp:561] res5a_branch2a <- pool4
I0814 19:16:12.979568 24692 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:16:13.010231 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0814 19:16:13.010248 24692 net.cpp:245] Setting up res5a_branch2a
I0814 19:16:13.010255 24692 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0814 19:16:13.010264 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:16:13.010278 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.010291 24692 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 19:16:13.010296 24692 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:16:13.010301 24692 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:16:13.011034 24692 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:16:13.011042 24692 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0814 19:16:13.011052 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:16:13.011057 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.011067 24692 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 19:16:13.011071 24692 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:16:13.011075 24692 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:16:13.011082 24692 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:16:13.011086 24692 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0814 19:16:13.011090 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:16:13.011096 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.011106 24692 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 19:16:13.011108 24692 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:16:13.011112 24692 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:16:13.027585 24692 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0814 19:16:13.027601 24692 net.cpp:245] Setting up res5a_branch2b
I0814 19:16:13.027607 24692 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0814 19:16:13.027618 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:16:13.027623 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.027633 24692 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 19:16:13.027638 24692 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:16:13.027643 24692 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:16:13.028348 24692 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:16:13.028357 24692 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0814 19:16:13.028367 24692 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:16:13.028372 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028378 24692 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 19:16:13.028381 24692 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:16:13.028386 24692 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:16:13.028393 24692 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:16:13.028398 24692 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0814 19:16:13.028401 24692 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:16:13.028406 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028416 24692 net.cpp:184] Created Layer pool5 (37)
I0814 19:16:13.028419 24692 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:16:13.028424 24692 net.cpp:530] pool5 -> pool5
I0814 19:16:13.028457 24692 net.cpp:245] Setting up pool5
I0814 19:16:13.028463 24692 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0814 19:16:13.028470 24692 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 19:16:13.028473 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028479 24692 net.cpp:184] Created Layer fc10 (38)
I0814 19:16:13.028492 24692 net.cpp:561] fc10 <- pool5
I0814 19:16:13.028497 24692 net.cpp:530] fc10 -> fc10
I0814 19:16:13.028776 24692 net.cpp:245] Setting up fc10
I0814 19:16:13.028784 24692 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0814 19:16:13.028791 24692 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0814 19:16:13.028795 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028801 24692 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0814 19:16:13.028805 24692 net.cpp:561] fc10_fc10_0_split <- fc10
I0814 19:16:13.028810 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0814 19:16:13.028815 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0814 19:16:13.028821 24692 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0814 19:16:13.028897 24692 net.cpp:245] Setting up fc10_fc10_0_split
I0814 19:16:13.028903 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028908 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028913 24692 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0814 19:16:13.028916 24692 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:16:13.028921 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.028985 24692 net.cpp:184] Created Layer loss (40)
I0814 19:16:13.028990 24692 net.cpp:561] loss <- fc10_fc10_0_split_0
I0814 19:16:13.028995 24692 net.cpp:561] loss <- label_data_1_split_0
I0814 19:16:13.028998 24692 net.cpp:530] loss -> loss
I0814 19:16:13.029150 24692 net.cpp:245] Setting up loss
I0814 19:16:13.029157 24692 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 19:16:13.029161 24692 net.cpp:256]     with loss weight 1
I0814 19:16:13.029167 24692 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 19:16:13.029171 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.029183 24692 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 19:16:13.029187 24692 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0814 19:16:13.029191 24692 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 19:16:13.029196 24692 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 19:16:13.029203 24692 net.cpp:245] Setting up accuracy/top1
I0814 19:16:13.029208 24692 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 19:16:13.029212 24692 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 19:16:13.029217 24692 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:16:13.029222 24692 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 19:16:13.029227 24692 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0814 19:16:13.029232 24692 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 19:16:13.029235 24692 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 19:16:13.029242 24692 net.cpp:245] Setting up accuracy/top5
I0814 19:16:13.029247 24692 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 19:16:13.029251 24692 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 19:16:13.029255 24692 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 19:16:13.029259 24692 net.cpp:323] loss needs backward computation.
I0814 19:16:13.029264 24692 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0814 19:16:13.029268 24692 net.cpp:323] fc10 needs backward computation.
I0814 19:16:13.029271 24692 net.cpp:323] pool5 needs backward computation.
I0814 19:16:13.029275 24692 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:16:13.029278 24692 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:16:13.029281 24692 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:16:13.029285 24692 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:16:13.029294 24692 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:16:13.029299 24692 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:16:13.029305 24692 net.cpp:323] pool4 needs backward computation.
I0814 19:16:13.029309 24692 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:16:13.029312 24692 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:16:13.029316 24692 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:16:13.029320 24692 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:16:13.029325 24692 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:16:13.029328 24692 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:16:13.029332 24692 net.cpp:323] pool3 needs backward computation.
I0814 19:16:13.029336 24692 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:16:13.029340 24692 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:16:13.029343 24692 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:16:13.029348 24692 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:16:13.029351 24692 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:16:13.029356 24692 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:16:13.029359 24692 net.cpp:323] pool2 needs backward computation.
I0814 19:16:13.029363 24692 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:16:13.029367 24692 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:16:13.029371 24692 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:16:13.029376 24692 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:16:13.029379 24692 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:16:13.029383 24692 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:16:13.029387 24692 net.cpp:323] pool1 needs backward computation.
I0814 19:16:13.029392 24692 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:16:13.029395 24692 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:16:13.029400 24692 net.cpp:323] conv1b needs backward computation.
I0814 19:16:13.029403 24692 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:16:13.029407 24692 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:16:13.029412 24692 net.cpp:323] conv1a needs backward computation.
I0814 19:16:13.029417 24692 net.cpp:325] data/bias does not need backward computation.
I0814 19:16:13.029420 24692 net.cpp:325] label_data_1_split does not need backward computation.
I0814 19:16:13.029425 24692 net.cpp:325] data does not need backward computation.
I0814 19:16:13.029429 24692 net.cpp:367] This network produces output accuracy/top1
I0814 19:16:13.029433 24692 net.cpp:367] This network produces output accuracy/top5
I0814 19:16:13.029436 24692 net.cpp:367] This network produces output loss
I0814 19:16:13.029465 24692 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0814 19:16:13.029469 24692 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0814 19:16:13.029472 24692 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0814 19:16:13.029475 24692 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 19:16:13.029479 24692 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 19:16:13.029484 24692 net.cpp:407] Network initialization done.
I0814 19:16:13.029532 24692 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.033550 24692 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-14_18-39-46/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 19:16:13.037422 24692 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 19:16:13.037443 24692 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 19:16:13.037475 24692 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 19:16:13.037500 24692 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.037751 24692 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 19:16:13.037757 24692 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 19:16:13.037770 24692 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.037917 24692 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 19:16:13.037923 24692 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 19:16:13.037926 24692 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.037943 24692 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038099 24692 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038103 24692 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038117 24692 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038261 24692 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.038266 24692 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 19:16:13.038270 24692 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.038311 24692 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038442 24692 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038447 24692 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038470 24692 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038590 24692 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.038595 24692 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 19:16:13.038599 24692 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.038712 24692 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.038836 24692 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.038841 24692 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.038898 24692 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039021 24692 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.039026 24692 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 19:16:13.039029 24692 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.039373 24692 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039506 24692 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.039511 24692 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.039661 24692 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.039786 24692 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.039791 24692 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 19:16:13.039794 24692 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 19:16:13.039808 24692 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 19:16:13.042537 24692 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 19:16:13.042558 24692 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 19:16:13.042592 24692 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 19:16:13.042604 24692 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.042862 24692 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 19:16:13.042876 24692 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 19:16:13.042889 24692 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043036 24692 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 19:16:13.043042 24692 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 19:16:13.043045 24692 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043063 24692 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043220 24692 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043226 24692 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.043241 24692 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043382 24692 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.043387 24692 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 19:16:13.043391 24692 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043431 24692 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043563 24692 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043568 24692 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.043591 24692 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043710 24692 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.043715 24692 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 19:16:13.043720 24692 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.043829 24692 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.043954 24692 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.043959 24692 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.044019 24692 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044145 24692 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.044152 24692 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 19:16:13.044154 24692 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 19:16:13.044512 24692 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044646 24692 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 19:16:13.044651 24692 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 19:16:13.044812 24692 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:16:13.044939 24692 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 19:16:13.044945 24692 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 19:16:13.044948 24692 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 19:16:13.044961 24692 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 19:16:13.045027 24692 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0814 19:16:13.045032 24692 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0814 19:16:13.045037 24692 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0814 19:16:13.045042 24692 parallel.cpp:59] Starting Optimization
I0814 19:16:13.045044 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:16:13.045074 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.045090 24692 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.045724 24732 device_alternate.hpp:116] NVML initialized on thread 140557102425856
I0814 19:16:13.058157 24732 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:16:13.058214 24733 device_alternate.hpp:116] NVML initialized on thread 140557094033152
I0814 19:16:13.058955 24733 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:16:13.058970 24734 device_alternate.hpp:116] NVML initialized on thread 140557085640448
I0814 19:16:13.059604 24734 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:16:13.063271 24733 solver.cpp:42] Solver data type: FLOAT
W0814 19:16:13.063776 24733 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:13.063886 24733 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.063892 24733 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.063927 24733 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:13.063940 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.067106 24734 solver.cpp:42] Solver data type: FLOAT
W0814 19:16:13.067440 24734 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0814 19:16:13.067500 24734 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.067504 24734 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.067523 24734 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0814 19:16:13.067529 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.067797 24735 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:13.068756 24736 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_train_lmdb
I0814 19:16:13.068892 24733 data_layer.cpp:185] [1] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:13.069808 24734 data_layer.cpp:185] [2] ReshapePrefetch 22, 3, 32, 32
I0814 19:16:13.069895 24733 data_layer.cpp:209] [1] Output data size: 22, 3, 32, 32
I0814 19:16:13.069905 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.069950 24734 data_layer.cpp:209] [2] Output data size: 22, 3, 32, 32
I0814 19:16:13.069960 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.474692 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 19:16:13.530175 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0814 19:16:13.531107 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 19:16:13.540727 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0814 19:16:13.544351 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 19:16:13.553397 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0814 19:16:13.554816 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 1 3  (limit 8.19G, req 0G)
I0814 19:16:13.571120 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0814 19:16:13.585873 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.18G, req 0G)
I0814 19:16:13.591416 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0814 19:16:13.592169 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0G)
I0814 19:16:13.598197 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0814 19:16:13.616382 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0G)
I0814 19:16:13.620589 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0814 19:16:13.627022 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0G)
I0814 19:16:13.630693 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0814 19:16:13.678791 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 19:16:13.681542 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0814 19:16:13.700254 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 19:16:13.702005 24733 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:13.702051 24733 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:13.702085 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0814 19:16:13.702141 24733 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.702145 24733 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.702160 24733 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:13.702167 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.702877 24777 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:13.702965 24733 data_layer.cpp:185] (1) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:13.703063 24733 data_layer.cpp:209] (1) Output data size: 17, 3, 32, 32
I0814 19:16:13.703068 24733 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0814 19:16:13.703743 24778 data_layer.cpp:97] (1) Parser threads: 1
I0814 19:16:13.703750 24778 data_layer.cpp:99] (1) Transformer threads: 1
I0814 19:16:13.704162 24734 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/test.prototxt
W0814 19:16:13.704216 24734 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0814 19:16:13.704305 24734 net.cpp:104] Using FLOAT as default forward math type
I0814 19:16:13.704310 24734 net.cpp:110] Using FLOAT as default backward math type
I0814 19:16:13.704326 24734 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0814 19:16:13.704334 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.705435 24779 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:16:13.705502 24734 data_layer.cpp:185] (2) ReshapePrefetch 17, 3, 32, 32
I0814 19:16:13.705593 24734 data_layer.cpp:209] (2) Output data size: 17, 3, 32, 32
I0814 19:16:13.705597 24734 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0814 19:16:13.706352 24780 data_layer.cpp:97] (2) Parser threads: 1
I0814 19:16:13.706358 24780 data_layer.cpp:99] (2) Transformer threads: 1
I0814 19:16:13.707751 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 19:16:13.709529 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0814 19:16:13.712790 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 19:16:13.714145 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0814 19:16:13.718060 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.720329 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.723400 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.725205 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0814 19:16:13.731783 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 19:16:13.733403 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0814 19:16:13.736258 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 19:16:13.738384 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0814 19:16:13.748807 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.750608 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.755908 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.757557 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0814 19:16:13.789119 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 19:16:13.791642 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0814 19:16:13.807274 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 19:16:13.808969 24733 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.809353 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0814 19:16:13.811841 24734 solver.cpp:56] Solver scaffolding done.
I0814 19:16:13.855973 24732 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0814 19:16:13.855973 24734 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0814 19:16:13.855973 24733 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0814 19:16:14.053553 24732 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:16:14.060941 24734 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.060961 24734 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.061065 24733 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.061075 24733 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.066520 24732 solver.cpp:438] Solving jacintonet11v2_train
I0814 19:16:14.066530 24732 solver.cpp:439] Learning Rate Policy: poly
I0814 19:16:14.073741 24733 solver.cpp:227] Starting Optimization on GPU 1
I0814 19:16:14.073743 24732 solver.cpp:227] Starting Optimization on GPU 0
I0814 19:16:14.073742 24734 solver.cpp:227] Starting Optimization on GPU 2
I0814 19:16:14.073904 24732 solver.cpp:509] Iteration 0, Testing net (#0)
I0814 19:16:14.074002 24781 device_alternate.hpp:116] NVML initialized on thread 140556330219264
I0814 19:16:14.074021 24781 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0814 19:16:14.074718 24782 device_alternate.hpp:116] NVML initialized on thread 140556321826560
I0814 19:16:14.074730 24782 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0814 19:16:14.074811 24783 device_alternate.hpp:116] NVML initialized on thread 140556313433856
I0814 19:16:14.074823 24783 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0814 19:16:14.084015 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0814 19:16:14.084233 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0814 19:16:14.089944 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0814 19:16:14.090318 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0814 19:16:14.091374 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0814 19:16:14.097708 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:14.099035 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0814 19:16:14.100181 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0814 19:16:14.104154 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:14.106902 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0814 19:16:14.108598 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0814 19:16:14.110101 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0814 19:16:14.114573 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0814 19:16:14.114815 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0814 19:16:14.116791 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0814 19:16:14.121697 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0814 19:16:14.122226 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0814 19:16:14.124912 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0814 19:16:14.128100 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0814 19:16:14.130647 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0814 19:16:14.131142 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0814 19:16:14.135939 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0814 19:16:14.137435 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0814 19:16:14.141229 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0814 19:16:14.141580 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0814 19:16:14.146869 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0814 19:16:14.147147 24733 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0814 19:16:14.150694 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0814 19:16:14.153164 24734 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0814 19:16:14.155694 24732 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0814 19:16:14.158076 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 1
I0814 19:16:14.158084 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0814 19:16:14.158088 24732 solver.cpp:594]     Test net output #2: loss = 0.045447 (* 1 = 0.045447 loss)
I0814 19:16:14.158092 24732 solver.cpp:254] [MultiGPU] Initial Test completed
I0814 19:16:14.158114 24732 blocking_queue.cpp:40] Data layer prefetch queue empty
I0814 19:16:14.167560 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0814 19:16:14.168050 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0814 19:16:14.169070 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0814 19:16:14.177225 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0814 19:16:14.177783 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0814 19:16:14.178200 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 4 3  (limit 7.79G, req 0G)
I0814 19:16:14.188315 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0814 19:16:14.188869 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0814 19:16:14.189529 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0814 19:16:14.197700 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0814 19:16:14.198231 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0814 19:16:14.198411 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0814 19:16:14.209532 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0814 19:16:14.209708 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0814 19:16:14.210427 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0814 19:16:14.217317 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0814 19:16:14.217862 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0814 19:16:14.218317 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 0  (limit 7.74G, req 0.01G)
I0814 19:16:14.233507 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0814 19:16:14.234000 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0814 19:16:14.241078 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0814 19:16:14.245455 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0814 19:16:14.247647 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0814 19:16:14.252099 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0814 19:16:14.259434 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0814 19:16:14.265215 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0814 19:16:14.266857 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0814 19:16:14.270272 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.69G, req 0.01G)
I0814 19:16:14.272994 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0814 19:16:14.278158 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0814 19:16:14.296975 24737 data_layer.cpp:97] [1] Parser threads: 1
I0814 19:16:14.296989 24737 data_layer.cpp:99] [1] Transformer threads: 1
I0814 19:16:14.311866 24738 data_layer.cpp:97] [2] Parser threads: 1
I0814 19:16:14.311882 24738 data_layer.cpp:99] [2] Transformer threads: 1
I0814 19:16:14.315632 24719 data_layer.cpp:97] [0] Parser threads: 1
I0814 19:16:14.315642 24719 data_layer.cpp:99] [0] Transformer threads: 1
I0814 19:16:14.325557 24732 solver.cpp:317] Iteration 0 (0.167431 s), loss = 0.000793919
I0814 19:16:14.325578 24732 solver.cpp:334]     Train net output #0: loss = 0.000793919 (* 1 = 0.000793919 loss)
I0814 19:16:14.325582 24732 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0814 19:16:14.351681 24732 solver.cpp:317] Iteration 1 (0.026113 s), loss = 0.00193649
I0814 19:16:14.351732 24732 solver.cpp:334]     Train net output #0: loss = 0.00193649 (* 1 = 0.00193649 loss)
I0814 19:16:14.362622 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0814 19:16:14.362893 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.97G, req 0.01G)
I0814 19:16:14.367728 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0814 19:16:14.372381 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 6 1 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.372856 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0814 19:16:14.375470 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.386443 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 1  (limit 6.42G, req 0.01G)
I0814 19:16:14.386715 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 1  (limit 6.33G, req 0.01G)
I0814 19:16:14.389232 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.393772 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.394269 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0814 19:16:14.396325 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.403630 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0814 19:16:14.404139 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.01G)
I0814 19:16:14.406261 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0814 19:16:14.408789 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.42G, req 0.01G)
I0814 19:16:14.409436 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.33G, req 0.01G)
I0814 19:16:14.411270 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0814 19:16:14.433231 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0814 19:16:14.433406 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.02G)
I0814 19:16:14.435832 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0814 19:16:14.441102 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0814 19:16:14.441431 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.02G)
I0814 19:16:14.443490 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0814 19:16:14.477843 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.478006 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.33G, req 0.03G)
I0814 19:16:14.479323 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.488833 24732 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.33G, req 0.03G)
I0814 19:16:14.489063 24734 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.489296 24733 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 5 5  (limit 6.42G, req 0.03G)
I0814 19:16:14.501296 24732 solver.cpp:317] Iteration 2 (0.149608 s), loss = 0.000792426
I0814 19:16:14.501310 24732 solver.cpp:334]     Train net output #0: loss = 0.000792426 (* 1 = 0.000792426 loss)
I0814 19:16:14.501329 24734 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:14.501329 24733 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:14.501562 24732 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0814 19:16:16.175485 24732 solver.cpp:312] Iteration 100 (58.5377 iter/s, 1.67413s/98 iter), loss = 0.00130582
I0814 19:16:16.175508 24732 solver.cpp:334]     Train net output #0: loss = 0.00130582 (* 1 = 0.00130582 loss)
I0814 19:16:16.175514 24732 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0814 19:16:17.840752 24732 solver.cpp:312] Iteration 200 (60.0522 iter/s, 1.66522s/100 iter), loss = 0.00184174
I0814 19:16:17.840804 24732 solver.cpp:334]     Train net output #0: loss = 0.00184174 (* 1 = 0.00184174 loss)
I0814 19:16:17.840818 24732 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0814 19:16:19.514755 24732 solver.cpp:312] Iteration 300 (59.7392 iter/s, 1.67394s/100 iter), loss = 0.51044
I0814 19:16:19.514816 24732 solver.cpp:334]     Train net output #0: loss = 0.51044 (* 1 = 0.51044 loss)
I0814 19:16:19.514834 24732 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0814 19:16:21.204285 24732 solver.cpp:312] Iteration 400 (59.1897 iter/s, 1.68948s/100 iter), loss = 0.42444
I0814 19:16:21.204354 24732 solver.cpp:334]     Train net output #0: loss = 0.42444 (* 1 = 0.42444 loss)
I0814 19:16:21.204375 24732 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0814 19:16:22.894191 24732 solver.cpp:312] Iteration 500 (59.1768 iter/s, 1.68985s/100 iter), loss = 0.817678
I0814 19:16:22.894218 24732 solver.cpp:334]     Train net output #0: loss = 0.817678 (* 1 = 0.817678 loss)
I0814 19:16:22.894222 24732 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0814 19:16:24.610862 24732 solver.cpp:312] Iteration 600 (58.2541 iter/s, 1.71662s/100 iter), loss = 0.690847
I0814 19:16:24.610885 24732 solver.cpp:334]     Train net output #0: loss = 0.690847 (* 1 = 0.690847 loss)
I0814 19:16:24.610891 24732 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0814 19:16:26.254678 24732 solver.cpp:312] Iteration 700 (60.836 iter/s, 1.64376s/100 iter), loss = 0.100845
I0814 19:16:26.254719 24732 solver.cpp:334]     Train net output #0: loss = 0.100845 (* 1 = 0.100845 loss)
I0814 19:16:26.254725 24732 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0814 19:16:27.127147 24716 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:16:27.913887 24732 solver.cpp:312] Iteration 800 (60.2715 iter/s, 1.65916s/100 iter), loss = 0.277027
I0814 19:16:27.913913 24732 solver.cpp:334]     Train net output #0: loss = 0.277027 (* 1 = 0.277027 loss)
I0814 19:16:27.913918 24732 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0814 19:16:29.575366 24732 solver.cpp:312] Iteration 900 (60.1892 iter/s, 1.66143s/100 iter), loss = 0.0669548
I0814 19:16:29.575422 24732 solver.cpp:334]     Train net output #0: loss = 0.0669546 (* 1 = 0.0669546 loss)
I0814 19:16:29.575438 24732 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0814 19:16:31.214596 24732 solver.cpp:363] Sparsity after update:
I0814 19:16:31.218852 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:16:31.218864 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:16:31.218875 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:16:31.218881 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:16:31.218885 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:16:31.218888 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:16:31.218891 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:16:31.218895 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:16:31.218899 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:16:31.218901 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:16:31.218905 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:16:31.218909 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:16:31.218912 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:16:31.218921 24732 solver.cpp:509] Iteration 1000, Testing net (#0)
I0814 19:16:32.046360 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.796766
I0814 19:16:32.046377 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992059
I0814 19:16:32.046382 24732 solver.cpp:594]     Test net output #2: loss = 0.69504 (* 1 = 0.69504 loss)
I0814 19:16:32.046398 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82745s
I0814 19:16:32.064290 24732 solver.cpp:312] Iteration 1000 (40.1792 iter/s, 2.48885s/100 iter), loss = 0.543985
I0814 19:16:32.064306 24732 solver.cpp:334]     Train net output #0: loss = 0.543984 (* 1 = 0.543984 loss)
I0814 19:16:32.064309 24732 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0814 19:16:33.744786 24732 solver.cpp:312] Iteration 1100 (59.5082 iter/s, 1.68044s/100 iter), loss = 0.352151
I0814 19:16:33.744832 24732 solver.cpp:334]     Train net output #0: loss = 0.352151 (* 1 = 0.352151 loss)
I0814 19:16:33.744844 24732 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0814 19:16:35.395673 24732 solver.cpp:312] Iteration 1200 (60.5753 iter/s, 1.65084s/100 iter), loss = 0.199608
I0814 19:16:35.395732 24732 solver.cpp:334]     Train net output #0: loss = 0.199608 (* 1 = 0.199608 loss)
I0814 19:16:35.395750 24732 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0814 19:16:37.045028 24732 solver.cpp:312] Iteration 1300 (60.6316 iter/s, 1.64931s/100 iter), loss = 0.12136
I0814 19:16:37.045258 24732 solver.cpp:334]     Train net output #0: loss = 0.12136 (* 1 = 0.12136 loss)
I0814 19:16:37.045372 24732 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0814 19:16:38.680660 24732 solver.cpp:312] Iteration 1400 (61.1404 iter/s, 1.63558s/100 iter), loss = 0.326903
I0814 19:16:38.680686 24732 solver.cpp:334]     Train net output #0: loss = 0.326903 (* 1 = 0.326903 loss)
I0814 19:16:38.680692 24732 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0814 19:16:40.356690 24732 solver.cpp:312] Iteration 1500 (59.6666 iter/s, 1.67598s/100 iter), loss = 0.155354
I0814 19:16:40.356762 24732 solver.cpp:334]     Train net output #0: loss = 0.155354 (* 1 = 0.155354 loss)
I0814 19:16:40.356792 24732 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0814 19:16:42.032835 24732 solver.cpp:312] Iteration 1600 (59.6626 iter/s, 1.67609s/100 iter), loss = 0.337506
I0814 19:16:42.032896 24732 solver.cpp:334]     Train net output #0: loss = 0.337505 (* 1 = 0.337505 loss)
I0814 19:16:42.032912 24732 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0814 19:16:43.701170 24732 solver.cpp:312] Iteration 1700 (59.9419 iter/s, 1.66828s/100 iter), loss = 0.361451
I0814 19:16:43.701283 24732 solver.cpp:334]     Train net output #0: loss = 0.361451 (* 1 = 0.361451 loss)
I0814 19:16:43.701297 24732 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0814 19:16:45.326378 24732 solver.cpp:312] Iteration 1800 (61.5325 iter/s, 1.62516s/100 iter), loss = 0.16691
I0814 19:16:45.326438 24732 solver.cpp:334]     Train net output #0: loss = 0.16691 (* 1 = 0.16691 loss)
I0814 19:16:45.326455 24732 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0814 19:16:47.012797 24732 solver.cpp:312] Iteration 1900 (59.299 iter/s, 1.68637s/100 iter), loss = 0.0448435
I0814 19:16:47.012861 24732 solver.cpp:334]     Train net output #0: loss = 0.0448431 (* 1 = 0.0448431 loss)
I0814 19:16:47.012881 24732 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0814 19:16:48.683086 24732 solver.cpp:363] Sparsity after update:
I0814 19:16:48.684618 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:16:48.684628 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:16:48.684635 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:16:48.684639 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:16:48.684648 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:16:48.684659 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:16:48.684662 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:16:48.684670 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:16:48.684674 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:16:48.684677 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:16:48.684685 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:16:48.684689 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:16:48.684697 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:16:48.684708 24732 solver.cpp:509] Iteration 2000, Testing net (#0)
I0814 19:16:49.494000 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.832648
I0814 19:16:49.494019 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.991765
I0814 19:16:49.494025 24732 solver.cpp:594]     Test net output #2: loss = 0.591745 (* 1 = 0.591745 loss)
I0814 19:16:49.494042 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.809306s
I0814 19:16:49.509876 24732 solver.cpp:312] Iteration 2000 (40.048 iter/s, 2.49701s/100 iter), loss = 0.0917527
I0814 19:16:49.509892 24732 solver.cpp:334]     Train net output #0: loss = 0.0917523 (* 1 = 0.0917523 loss)
I0814 19:16:49.509905 24732 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0814 19:16:51.199396 24732 solver.cpp:312] Iteration 2100 (59.1903 iter/s, 1.68947s/100 iter), loss = 0.0906303
I0814 19:16:51.199442 24732 solver.cpp:334]     Train net output #0: loss = 0.0906299 (* 1 = 0.0906299 loss)
I0814 19:16:51.199455 24732 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0814 19:16:52.881829 24732 solver.cpp:312] Iteration 2200 (59.4396 iter/s, 1.68238s/100 iter), loss = 0.0839446
I0814 19:16:52.881852 24732 solver.cpp:334]     Train net output #0: loss = 0.0839441 (* 1 = 0.0839441 loss)
I0814 19:16:52.881858 24732 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0814 19:16:54.530294 24732 solver.cpp:312] Iteration 2300 (60.6644 iter/s, 1.64841s/100 iter), loss = 0.0932575
I0814 19:16:54.530318 24732 solver.cpp:334]     Train net output #0: loss = 0.093257 (* 1 = 0.093257 loss)
I0814 19:16:54.530323 24732 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0814 19:16:56.170112 24732 solver.cpp:312] Iteration 2400 (60.9843 iter/s, 1.63977s/100 iter), loss = 0.0856774
I0814 19:16:56.170161 24732 solver.cpp:334]     Train net output #0: loss = 0.085677 (* 1 = 0.085677 loss)
I0814 19:16:56.170173 24732 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0814 19:16:57.848856 24732 solver.cpp:312] Iteration 2500 (59.5702 iter/s, 1.67869s/100 iter), loss = 0.0447884
I0814 19:16:57.848886 24732 solver.cpp:334]     Train net output #0: loss = 0.0447881 (* 1 = 0.0447881 loss)
I0814 19:16:57.848893 24732 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0814 19:16:59.528997 24732 solver.cpp:312] Iteration 2600 (59.5206 iter/s, 1.68009s/100 iter), loss = 0.0972106
I0814 19:16:59.529027 24732 solver.cpp:334]     Train net output #0: loss = 0.0972103 (* 1 = 0.0972103 loss)
I0814 19:16:59.529034 24732 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0814 19:17:01.180443 24732 solver.cpp:312] Iteration 2700 (60.5548 iter/s, 1.6514s/100 iter), loss = 0.0156879
I0814 19:17:01.180503 24732 solver.cpp:334]     Train net output #0: loss = 0.0156876 (* 1 = 0.0156876 loss)
I0814 19:17:01.180522 24732 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0814 19:17:02.815518 24732 solver.cpp:312] Iteration 2800 (61.1612 iter/s, 1.63502s/100 iter), loss = 0.0305617
I0814 19:17:02.815543 24732 solver.cpp:334]     Train net output #0: loss = 0.0305614 (* 1 = 0.0305614 loss)
I0814 19:17:02.815548 24732 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0814 19:17:04.486409 24732 solver.cpp:312] Iteration 2900 (59.8501 iter/s, 1.67084s/100 iter), loss = 0.117334
I0814 19:17:04.486433 24732 solver.cpp:334]     Train net output #0: loss = 0.117334 (* 1 = 0.117334 loss)
I0814 19:17:04.486439 24732 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0814 19:17:06.104096 24732 solver.cpp:363] Sparsity after update:
I0814 19:17:06.105787 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:17:06.105796 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:17:06.105804 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:17:06.105808 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:17:06.105819 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:17:06.105825 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:17:06.105829 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:17:06.105837 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:17:06.105841 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:17:06.105845 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:17:06.105852 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:17:06.105860 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:17:06.105865 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:17:06.105880 24732 solver.cpp:509] Iteration 3000, Testing net (#0)
I0814 19:17:06.920213 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.833531
I0814 19:17:06.920236 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989118
I0814 19:17:06.920243 24732 solver.cpp:594]     Test net output #2: loss = 0.619492 (* 1 = 0.619492 loss)
I0814 19:17:06.920320 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.814411s
I0814 19:17:06.936173 24732 solver.cpp:312] Iteration 3000 (40.8214 iter/s, 2.44969s/100 iter), loss = 0.0739115
I0814 19:17:06.936189 24732 solver.cpp:334]     Train net output #0: loss = 0.0739112 (* 1 = 0.0739112 loss)
I0814 19:17:06.936194 24732 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0814 19:17:08.613256 24732 solver.cpp:312] Iteration 3100 (59.6293 iter/s, 1.67703s/100 iter), loss = 0.0565981
I0814 19:17:08.613320 24732 solver.cpp:334]     Train net output #0: loss = 0.0565978 (* 1 = 0.0565978 loss)
I0814 19:17:08.613344 24732 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0814 19:17:10.242123 24732 solver.cpp:312] Iteration 3200 (61.3943 iter/s, 1.62882s/100 iter), loss = 0.130127
I0814 19:17:10.242148 24732 solver.cpp:334]     Train net output #0: loss = 0.130127 (* 1 = 0.130127 loss)
I0814 19:17:10.242154 24732 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0814 19:17:11.876368 24732 solver.cpp:312] Iteration 3300 (61.1923 iter/s, 1.63419s/100 iter), loss = 0.0155956
I0814 19:17:11.876392 24732 solver.cpp:334]     Train net output #0: loss = 0.0155953 (* 1 = 0.0155953 loss)
I0814 19:17:11.876397 24732 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0814 19:17:13.494652 24732 solver.cpp:312] Iteration 3400 (61.7957 iter/s, 1.61823s/100 iter), loss = 0.126574
I0814 19:17:13.494881 24732 solver.cpp:334]     Train net output #0: loss = 0.126574 (* 1 = 0.126574 loss)
I0814 19:17:13.494909 24732 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0814 19:17:15.199416 24732 solver.cpp:312] Iteration 3500 (58.6609 iter/s, 1.70471s/100 iter), loss = 0.322088
I0814 19:17:15.199522 24732 solver.cpp:334]     Train net output #0: loss = 0.322088 (* 1 = 0.322088 loss)
I0814 19:17:15.199543 24732 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0814 19:17:16.895972 24732 solver.cpp:312] Iteration 3600 (58.9448 iter/s, 1.6965s/100 iter), loss = 0.0153916
I0814 19:17:16.895997 24732 solver.cpp:334]     Train net output #0: loss = 0.0153914 (* 1 = 0.0153914 loss)
I0814 19:17:16.896000 24732 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0814 19:17:18.549640 24732 solver.cpp:312] Iteration 3700 (60.4735 iter/s, 1.65362s/100 iter), loss = 0.0263022
I0814 19:17:18.549664 24732 solver.cpp:334]     Train net output #0: loss = 0.026302 (* 1 = 0.026302 loss)
I0814 19:17:18.549670 24732 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0814 19:17:20.172900 24732 solver.cpp:312] Iteration 3800 (61.6065 iter/s, 1.62321s/100 iter), loss = 0.0770882
I0814 19:17:20.172935 24732 solver.cpp:334]     Train net output #0: loss = 0.077088 (* 1 = 0.077088 loss)
I0814 19:17:20.172942 24732 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0814 19:17:21.842576 24732 solver.cpp:312] Iteration 3900 (59.8937 iter/s, 1.66963s/100 iter), loss = 0.216337
I0814 19:17:21.842644 24732 solver.cpp:334]     Train net output #0: loss = 0.216337 (* 1 = 0.216337 loss)
I0814 19:17:21.842666 24732 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0814 19:17:23.500836 24732 solver.cpp:363] Sparsity after update:
I0814 19:17:23.502636 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:17:23.502648 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:17:23.502657 24732 net.cpp:2192] conv1b_param_0(0) 
I0814 19:17:23.502662 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:17:23.502667 24732 net.cpp:2192] res2a_branch2a_param_0(0) 
I0814 19:17:23.502671 24732 net.cpp:2192] res2a_branch2b_param_0(0) 
I0814 19:17:23.502676 24732 net.cpp:2192] res3a_branch2a_param_0(0) 
I0814 19:17:23.502679 24732 net.cpp:2192] res3a_branch2b_param_0(0) 
I0814 19:17:23.502684 24732 net.cpp:2192] res4a_branch2a_param_0(0) 
I0814 19:17:23.502687 24732 net.cpp:2192] res4a_branch2b_param_0(0) 
I0814 19:17:23.502691 24732 net.cpp:2192] res5a_branch2a_param_0(0) 
I0814 19:17:23.502696 24732 net.cpp:2192] res5a_branch2b_param_0(0) 
I0814 19:17:23.502701 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0814 19:17:23.502710 24732 solver.cpp:509] Iteration 4000, Testing net (#0)
I0814 19:17:24.315106 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.856472
I0814 19:17:24.315124 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:17:24.315129 24732 solver.cpp:594]     Test net output #2: loss = 0.52191 (* 1 = 0.52191 loss)
I0814 19:17:24.315145 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.812407s
I0814 19:17:24.332573 24783 solver.cpp:409] Finding and applying sparsity: 0.02
I0814 19:17:45.687419 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:17:45.689441 24732 solver.cpp:312] Iteration 4000 (4.19354 iter/s, 23.8462s/100 iter), loss = 0.112001
I0814 19:17:45.689462 24732 solver.cpp:334]     Train net output #0: loss = 0.112001 (* 1 = 0.112001 loss)
I0814 19:17:45.689471 24732 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0814 19:17:47.502506 24732 solver.cpp:312] Iteration 4100 (55.1569 iter/s, 1.81301s/100 iter), loss = 0.112267
I0814 19:17:47.502557 24732 solver.cpp:334]     Train net output #0: loss = 0.112267 (* 1 = 0.112267 loss)
I0814 19:17:47.502570 24732 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0814 19:17:49.201146 24732 solver.cpp:312] Iteration 4200 (58.8725 iter/s, 1.69859s/100 iter), loss = 0.0437128
I0814 19:17:49.201171 24732 solver.cpp:334]     Train net output #0: loss = 0.0437127 (* 1 = 0.0437127 loss)
I0814 19:17:49.201179 24732 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0814 19:17:50.885701 24732 solver.cpp:312] Iteration 4300 (59.3647 iter/s, 1.6845s/100 iter), loss = 0.207798
I0814 19:17:50.885725 24732 solver.cpp:334]     Train net output #0: loss = 0.207798 (* 1 = 0.207798 loss)
I0814 19:17:50.885731 24732 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0814 19:17:52.546205 24732 solver.cpp:312] Iteration 4400 (60.2245 iter/s, 1.66045s/100 iter), loss = 0.243687
I0814 19:17:52.546253 24732 solver.cpp:334]     Train net output #0: loss = 0.243687 (* 1 = 0.243687 loss)
I0814 19:17:52.546265 24732 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0814 19:17:54.173009 24732 solver.cpp:312] Iteration 4500 (61.4722 iter/s, 1.62675s/100 iter), loss = 0.0557635
I0814 19:17:54.173033 24732 solver.cpp:334]     Train net output #0: loss = 0.0557634 (* 1 = 0.0557634 loss)
I0814 19:17:54.173039 24732 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0814 19:17:55.835841 24732 solver.cpp:312] Iteration 4600 (60.1402 iter/s, 1.66278s/100 iter), loss = 0.0198239
I0814 19:17:55.835911 24732 solver.cpp:334]     Train net output #0: loss = 0.0198238 (* 1 = 0.0198238 loss)
I0814 19:17:55.835930 24732 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0814 19:17:57.504529 24732 solver.cpp:312] Iteration 4700 (59.9291 iter/s, 1.66864s/100 iter), loss = 0.0374973
I0814 19:17:57.504554 24732 solver.cpp:334]     Train net output #0: loss = 0.0374973 (* 1 = 0.0374973 loss)
I0814 19:17:57.504559 24732 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0814 19:17:59.191362 24732 solver.cpp:312] Iteration 4800 (59.2845 iter/s, 1.68678s/100 iter), loss = 0.15797
I0814 19:17:59.191383 24732 solver.cpp:334]     Train net output #0: loss = 0.15797 (* 1 = 0.15797 loss)
I0814 19:17:59.191390 24732 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0814 19:18:00.836530 24732 solver.cpp:312] Iteration 4900 (60.7859 iter/s, 1.64512s/100 iter), loss = 0.0388677
I0814 19:18:00.836557 24732 solver.cpp:334]     Train net output #0: loss = 0.0388676 (* 1 = 0.0388676 loss)
I0814 19:18:00.836563 24732 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0814 19:18:02.497040 24732 solver.cpp:363] Sparsity after update:
I0814 19:18:02.498653 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:18:02.498662 24732 net.cpp:2192] conv1a_param_0(0) 
I0814 19:18:02.498677 24732 net.cpp:2192] conv1b_param_0(0.0104) 
I0814 19:18:02.498684 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:18:02.498693 24732 net.cpp:2192] res2a_branch2a_param_0(0.0174) 
I0814 19:18:02.498703 24732 net.cpp:2192] res2a_branch2b_param_0(0.0139) 
I0814 19:18:02.498708 24732 net.cpp:2192] res3a_branch2a_param_0(0.0191) 
I0814 19:18:02.498715 24732 net.cpp:2192] res3a_branch2b_param_0(0.0174) 
I0814 19:18:02.498724 24732 net.cpp:2192] res4a_branch2a_param_0(0.02) 
I0814 19:18:02.498729 24732 net.cpp:2192] res4a_branch2b_param_0(0.0191) 
I0814 19:18:02.498738 24732 net.cpp:2192] res5a_branch2a_param_0(0.0199) 
I0814 19:18:02.498746 24732 net.cpp:2192] res5a_branch2b_param_0(0.0199) 
I0814 19:18:02.498750 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (46499/2.3599e+06) 0.0197
I0814 19:18:02.498775 24732 solver.cpp:509] Iteration 5000, Testing net (#0)
I0814 19:18:03.181918 24730 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:18:03.260169 24778 blocking_queue.cpp:40] Waiting for datum
I0814 19:18:03.323015 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.872942
I0814 19:18:03.323035 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:18:03.323040 24732 solver.cpp:594]     Test net output #2: loss = 0.472134 (* 1 = 0.472134 loss)
I0814 19:18:03.323060 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824256s
I0814 19:18:03.344606 24783 solver.cpp:409] Finding and applying sparsity: 0.04
I0814 19:18:24.106091 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:18:24.108299 24732 solver.cpp:312] Iteration 5000 (4.29717 iter/s, 23.2711s/100 iter), loss = 0.136508
I0814 19:18:24.108319 24732 solver.cpp:334]     Train net output #0: loss = 0.136508 (* 1 = 0.136508 loss)
I0814 19:18:24.108328 24732 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0814 19:18:26.040460 24732 solver.cpp:312] Iteration 5100 (51.7571 iter/s, 1.9321s/100 iter), loss = 0.108689
I0814 19:18:26.040518 24732 solver.cpp:334]     Train net output #0: loss = 0.108689 (* 1 = 0.108689 loss)
I0814 19:18:26.040537 24732 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0814 19:18:27.716502 24732 solver.cpp:312] Iteration 5200 (59.6662 iter/s, 1.67599s/100 iter), loss = 0.0221201
I0814 19:18:27.716560 24732 solver.cpp:334]     Train net output #0: loss = 0.0221201 (* 1 = 0.0221201 loss)
I0814 19:18:27.716579 24732 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0814 19:18:29.360836 24732 solver.cpp:312] Iteration 5300 (60.8169 iter/s, 1.64428s/100 iter), loss = 0.202996
I0814 19:18:29.360898 24732 solver.cpp:334]     Train net output #0: loss = 0.202996 (* 1 = 0.202996 loss)
I0814 19:18:29.360913 24732 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0814 19:18:31.000803 24732 solver.cpp:312] Iteration 5400 (60.9787 iter/s, 1.63992s/100 iter), loss = 0.029633
I0814 19:18:31.000861 24732 solver.cpp:334]     Train net output #0: loss = 0.0296329 (* 1 = 0.0296329 loss)
I0814 19:18:31.000880 24732 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0814 19:18:32.661025 24732 solver.cpp:312] Iteration 5500 (60.235 iter/s, 1.66016s/100 iter), loss = 0.0400521
I0814 19:18:32.661062 24732 solver.cpp:334]     Train net output #0: loss = 0.040052 (* 1 = 0.040052 loss)
I0814 19:18:32.661069 24732 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0814 19:18:34.336009 24732 solver.cpp:312] Iteration 5600 (59.7038 iter/s, 1.67494s/100 iter), loss = 0.0857176
I0814 19:18:34.336035 24732 solver.cpp:334]     Train net output #0: loss = 0.0857175 (* 1 = 0.0857175 loss)
I0814 19:18:34.336040 24732 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0814 19:18:36.019050 24732 solver.cpp:312] Iteration 5700 (59.4182 iter/s, 1.68299s/100 iter), loss = 0.00253385
I0814 19:18:36.019104 24732 solver.cpp:334]     Train net output #0: loss = 0.00253375 (* 1 = 0.00253375 loss)
I0814 19:18:36.019119 24732 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0814 19:18:37.638882 24732 solver.cpp:312] Iteration 5800 (61.7367 iter/s, 1.61978s/100 iter), loss = 0.0214135
I0814 19:18:37.638912 24732 solver.cpp:334]     Train net output #0: loss = 0.0214134 (* 1 = 0.0214134 loss)
I0814 19:18:37.638918 24732 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0814 19:18:39.277315 24732 solver.cpp:312] Iteration 5900 (61.0359 iter/s, 1.63838s/100 iter), loss = 0.0072344
I0814 19:18:39.277339 24732 solver.cpp:334]     Train net output #0: loss = 0.00723426 (* 1 = 0.00723426 loss)
I0814 19:18:39.277345 24732 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0814 19:18:40.928761 24732 solver.cpp:363] Sparsity after update:
I0814 19:18:40.930397 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:18:40.930405 24732 net.cpp:2192] conv1a_param_0(0.0129) 
I0814 19:18:40.930413 24732 net.cpp:2192] conv1b_param_0(0.0208) 
I0814 19:18:40.930414 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:18:40.930416 24732 net.cpp:2192] res2a_branch2a_param_0(0.0382) 
I0814 19:18:40.930418 24732 net.cpp:2192] res2a_branch2b_param_0(0.0347) 
I0814 19:18:40.930420 24732 net.cpp:2192] res3a_branch2a_param_0(0.0399) 
I0814 19:18:40.930423 24732 net.cpp:2192] res3a_branch2b_param_0(0.0382) 
I0814 19:18:40.930424 24732 net.cpp:2192] res4a_branch2a_param_0(0.0399) 
I0814 19:18:40.930426 24732 net.cpp:2192] res4a_branch2b_param_0(0.0399) 
I0814 19:18:40.930428 24732 net.cpp:2192] res5a_branch2a_param_0(0.0398) 
I0814 19:18:40.930429 24732 net.cpp:2192] res5a_branch2b_param_0(0.0399) 
I0814 19:18:40.930433 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (93649/2.3599e+06) 0.0397
I0814 19:18:40.930451 24732 solver.cpp:509] Iteration 6000, Testing net (#0)
I0814 19:18:41.745473 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861766
I0814 19:18:41.745489 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.989412
I0814 19:18:41.745494 24732 solver.cpp:594]     Test net output #2: loss = 0.640049 (* 1 = 0.640049 loss)
I0814 19:18:41.745510 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815031s
I0814 19:18:41.761589 24783 solver.cpp:409] Finding and applying sparsity: 0.06
I0814 19:19:02.545819 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:19:02.547879 24732 solver.cpp:312] Iteration 6000 (4.29739 iter/s, 23.2699s/100 iter), loss = 0.0283866
I0814 19:19:02.547904 24732 solver.cpp:334]     Train net output #0: loss = 0.0283864 (* 1 = 0.0283864 loss)
I0814 19:19:02.547912 24732 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0814 19:19:04.412271 24732 solver.cpp:312] Iteration 6100 (53.6385 iter/s, 1.86433s/100 iter), loss = 0.103321
I0814 19:19:04.412441 24732 solver.cpp:334]     Train net output #0: loss = 0.103321 (* 1 = 0.103321 loss)
I0814 19:19:04.412528 24732 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0814 19:19:06.074769 24732 solver.cpp:312] Iteration 6200 (60.1524 iter/s, 1.66244s/100 iter), loss = 0.0287645
I0814 19:19:06.074792 24732 solver.cpp:334]     Train net output #0: loss = 0.0287644 (* 1 = 0.0287644 loss)
I0814 19:19:06.074797 24732 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0814 19:19:07.768961 24732 solver.cpp:312] Iteration 6300 (59.027 iter/s, 1.69414s/100 iter), loss = 0.0572716
I0814 19:19:07.768986 24732 solver.cpp:334]     Train net output #0: loss = 0.0572715 (* 1 = 0.0572715 loss)
I0814 19:19:07.768992 24732 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0814 19:19:09.423302 24732 solver.cpp:312] Iteration 6400 (60.4489 iter/s, 1.65429s/100 iter), loss = 0.0334602
I0814 19:19:09.423326 24732 solver.cpp:334]     Train net output #0: loss = 0.03346 (* 1 = 0.03346 loss)
I0814 19:19:09.423332 24732 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0814 19:19:11.048087 24732 solver.cpp:312] Iteration 6500 (61.5485 iter/s, 1.62474s/100 iter), loss = 0.0116171
I0814 19:19:11.048140 24732 solver.cpp:334]     Train net output #0: loss = 0.011617 (* 1 = 0.011617 loss)
I0814 19:19:11.048153 24732 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0814 19:19:12.707631 24732 solver.cpp:312] Iteration 6600 (60.2595 iter/s, 1.65949s/100 iter), loss = 0.0662347
I0814 19:19:12.707659 24732 solver.cpp:334]     Train net output #0: loss = 0.0662346 (* 1 = 0.0662346 loss)
I0814 19:19:12.707664 24732 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0814 19:19:14.316431 24732 solver.cpp:312] Iteration 6700 (62.1601 iter/s, 1.60875s/100 iter), loss = 0.000953641
I0814 19:19:14.316483 24732 solver.cpp:334]     Train net output #0: loss = 0.00095356 (* 1 = 0.00095356 loss)
I0814 19:19:14.316500 24732 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0814 19:19:15.946666 24732 solver.cpp:312] Iteration 6800 (61.3428 iter/s, 1.63018s/100 iter), loss = 0.025519
I0814 19:19:15.946691 24732 solver.cpp:334]     Train net output #0: loss = 0.0255189 (* 1 = 0.0255189 loss)
I0814 19:19:15.946696 24732 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0814 19:19:17.612814 24732 solver.cpp:312] Iteration 6900 (60.0205 iter/s, 1.6661s/100 iter), loss = 0.0308791
I0814 19:19:17.612844 24732 solver.cpp:334]     Train net output #0: loss = 0.030879 (* 1 = 0.030879 loss)
I0814 19:19:17.612850 24732 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0814 19:19:19.261881 24732 solver.cpp:363] Sparsity after update:
I0814 19:19:19.263371 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:19:19.263380 24732 net.cpp:2192] conv1a_param_0(0.0262) 
I0814 19:19:19.263386 24732 net.cpp:2192] conv1b_param_0(0.0417) 
I0814 19:19:19.263388 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:19:19.263391 24732 net.cpp:2192] res2a_branch2a_param_0(0.059) 
I0814 19:19:19.263393 24732 net.cpp:2192] res2a_branch2b_param_0(0.0556) 
I0814 19:19:19.263396 24732 net.cpp:2192] res3a_branch2a_param_0(0.059) 
I0814 19:19:19.263397 24732 net.cpp:2192] res3a_branch2b_param_0(0.059) 
I0814 19:19:19.263399 24732 net.cpp:2192] res4a_branch2a_param_0(0.0599) 
I0814 19:19:19.263401 24732 net.cpp:2192] res4a_branch2b_param_0(0.059) 
I0814 19:19:19.263403 24732 net.cpp:2192] res5a_branch2a_param_0(0.0595) 
I0814 19:19:19.263406 24732 net.cpp:2192] res5a_branch2b_param_0(0.0598) 
I0814 19:19:19.263408 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (140133/2.3599e+06) 0.0594
I0814 19:19:19.263428 24732 solver.cpp:509] Iteration 7000, Testing net (#0)
I0814 19:19:20.082304 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861178
I0814 19:19:20.082320 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992353
I0814 19:19:20.082325 24732 solver.cpp:594]     Test net output #2: loss = 0.522509 (* 1 = 0.522509 loss)
I0814 19:19:20.082341 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818885s
I0814 19:19:20.098415 24783 solver.cpp:409] Finding and applying sparsity: 0.08
I0814 19:19:40.632807 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:19:40.634853 24732 solver.cpp:312] Iteration 7000 (4.34378 iter/s, 23.0214s/100 iter), loss = 0.00821189
I0814 19:19:40.634878 24732 solver.cpp:334]     Train net output #0: loss = 0.00821184 (* 1 = 0.00821184 loss)
I0814 19:19:40.634887 24732 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0814 19:19:42.486637 24732 solver.cpp:312] Iteration 7100 (54.0037 iter/s, 1.85172s/100 iter), loss = 0.041075
I0814 19:19:42.486663 24732 solver.cpp:334]     Train net output #0: loss = 0.0410749 (* 1 = 0.0410749 loss)
I0814 19:19:42.486668 24732 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0814 19:19:44.110129 24732 solver.cpp:312] Iteration 7200 (61.5976 iter/s, 1.62344s/100 iter), loss = 0.145245
I0814 19:19:44.110189 24732 solver.cpp:334]     Train net output #0: loss = 0.145245 (* 1 = 0.145245 loss)
I0814 19:19:44.110208 24732 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0814 19:19:45.813191 24732 solver.cpp:312] Iteration 7300 (58.7196 iter/s, 1.70301s/100 iter), loss = 0.0453097
I0814 19:19:45.813215 24732 solver.cpp:334]     Train net output #0: loss = 0.0453096 (* 1 = 0.0453096 loss)
I0814 19:19:45.813220 24732 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0814 19:19:47.454913 24732 solver.cpp:312] Iteration 7400 (60.9135 iter/s, 1.64167s/100 iter), loss = 0.00334753
I0814 19:19:47.454939 24732 solver.cpp:334]     Train net output #0: loss = 0.00334747 (* 1 = 0.00334747 loss)
I0814 19:19:47.454946 24732 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0814 19:19:49.123100 24732 solver.cpp:312] Iteration 7500 (59.9472 iter/s, 1.66814s/100 iter), loss = 0.00608127
I0814 19:19:49.123126 24732 solver.cpp:334]     Train net output #0: loss = 0.00608118 (* 1 = 0.00608118 loss)
I0814 19:19:49.123131 24732 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0814 19:19:50.721071 24732 solver.cpp:312] Iteration 7600 (62.5813 iter/s, 1.59792s/100 iter), loss = 0.0568019
I0814 19:19:50.721122 24732 solver.cpp:334]     Train net output #0: loss = 0.0568018 (* 1 = 0.0568018 loss)
I0814 19:19:50.721134 24732 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0814 19:19:52.371222 24732 solver.cpp:312] Iteration 7700 (60.6024 iter/s, 1.6501s/100 iter), loss = 0.00666856
I0814 19:19:52.371273 24732 solver.cpp:334]     Train net output #0: loss = 0.00666846 (* 1 = 0.00666846 loss)
I0814 19:19:52.371286 24732 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0814 19:19:54.013660 24732 solver.cpp:312] Iteration 7800 (60.887 iter/s, 1.64239s/100 iter), loss = 0.0197749
I0814 19:19:54.013730 24732 solver.cpp:334]     Train net output #0: loss = 0.0197748 (* 1 = 0.0197748 loss)
I0814 19:19:54.013748 24732 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0814 19:19:55.646589 24732 solver.cpp:312] Iteration 7900 (61.2416 iter/s, 1.63288s/100 iter), loss = 0.00503622
I0814 19:19:55.646615 24732 solver.cpp:334]     Train net output #0: loss = 0.00503612 (* 1 = 0.00503612 loss)
I0814 19:19:55.646620 24732 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0814 19:19:57.235558 24732 solver.cpp:363] Sparsity after update:
I0814 19:19:57.237262 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:19:57.237270 24732 net.cpp:2192] conv1a_param_0(0.0392) 
I0814 19:19:57.237278 24732 net.cpp:2192] conv1b_param_0(0.0521) 
I0814 19:19:57.237290 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:19:57.237295 24732 net.cpp:2192] res2a_branch2a_param_0(0.0799) 
I0814 19:19:57.237304 24732 net.cpp:2192] res2a_branch2b_param_0(0.0764) 
I0814 19:19:57.237308 24732 net.cpp:2192] res3a_branch2a_param_0(0.0799) 
I0814 19:19:57.237313 24732 net.cpp:2192] res3a_branch2b_param_0(0.0799) 
I0814 19:19:57.237320 24732 net.cpp:2192] res4a_branch2a_param_0(0.0799) 
I0814 19:19:57.237324 24732 net.cpp:2192] res4a_branch2b_param_0(0.0799) 
I0814 19:19:57.237332 24732 net.cpp:2192] res5a_branch2a_param_0(0.0792) 
I0814 19:19:57.237336 24732 net.cpp:2192] res5a_branch2b_param_0(0.0798) 
I0814 19:19:57.237344 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (187058/2.3599e+06) 0.0793
I0814 19:19:57.237368 24732 solver.cpp:509] Iteration 8000, Testing net (#0)
I0814 19:19:58.061476 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.867942
I0814 19:19:58.061494 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.992941
I0814 19:19:58.061499 24732 solver.cpp:594]     Test net output #2: loss = 0.578133 (* 1 = 0.578133 loss)
I0814 19:19:58.061517 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82412s
I0814 19:19:58.077247 24783 solver.cpp:409] Finding and applying sparsity: 0.1
I0814 19:20:18.529935 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:20:18.531989 24732 solver.cpp:312] Iteration 8000 (4.36972 iter/s, 22.8848s/100 iter), loss = 0.0149878
I0814 19:20:18.532011 24732 solver.cpp:334]     Train net output #0: loss = 0.0149877 (* 1 = 0.0149877 loss)
I0814 19:20:18.532019 24732 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0814 19:20:20.336511 24732 solver.cpp:312] Iteration 8100 (55.4181 iter/s, 1.80447s/100 iter), loss = 0.0200955
I0814 19:20:20.336560 24732 solver.cpp:334]     Train net output #0: loss = 0.0200954 (* 1 = 0.0200954 loss)
I0814 19:20:20.336571 24732 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0814 19:20:22.003707 24732 solver.cpp:312] Iteration 8200 (59.9828 iter/s, 1.66714s/100 iter), loss = 0.0121011
I0814 19:20:22.003768 24732 solver.cpp:334]     Train net output #0: loss = 0.012101 (* 1 = 0.012101 loss)
I0814 19:20:22.003790 24732 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0814 19:20:23.671622 24732 solver.cpp:312] Iteration 8300 (59.9571 iter/s, 1.66786s/100 iter), loss = 0.0666558
I0814 19:20:23.671677 24732 solver.cpp:334]     Train net output #0: loss = 0.0666556 (* 1 = 0.0666556 loss)
I0814 19:20:23.671691 24732 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0814 19:20:25.306351 24732 solver.cpp:312] Iteration 8400 (61.1741 iter/s, 1.63468s/100 iter), loss = 0.00567244
I0814 19:20:25.306375 24732 solver.cpp:334]     Train net output #0: loss = 0.00567231 (* 1 = 0.00567231 loss)
I0814 19:20:25.306380 24732 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0814 19:20:26.963155 24732 solver.cpp:312] Iteration 8500 (60.3591 iter/s, 1.65675s/100 iter), loss = 0.0407569
I0814 19:20:26.963201 24732 solver.cpp:334]     Train net output #0: loss = 0.0407568 (* 1 = 0.0407568 loss)
I0814 19:20:26.963213 24732 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0814 19:20:28.594146 24732 solver.cpp:312] Iteration 8600 (61.3143 iter/s, 1.63094s/100 iter), loss = 0.000875662
I0814 19:20:28.594172 24732 solver.cpp:334]     Train net output #0: loss = 0.000875571 (* 1 = 0.000875571 loss)
I0814 19:20:28.594177 24732 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0814 19:20:30.246922 24732 solver.cpp:312] Iteration 8700 (60.5061 iter/s, 1.65273s/100 iter), loss = 0.0311668
I0814 19:20:30.246973 24732 solver.cpp:334]     Train net output #0: loss = 0.0311667 (* 1 = 0.0311667 loss)
I0814 19:20:30.246985 24732 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0814 19:20:31.874056 24732 solver.cpp:312] Iteration 8800 (61.4597 iter/s, 1.62708s/100 iter), loss = 0.00413388
I0814 19:20:31.874080 24732 solver.cpp:334]     Train net output #0: loss = 0.00413377 (* 1 = 0.00413377 loss)
I0814 19:20:31.874085 24732 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0814 19:20:33.548640 24732 solver.cpp:312] Iteration 8900 (59.7183 iter/s, 1.67453s/100 iter), loss = 0.0278068
I0814 19:20:33.548698 24732 solver.cpp:334]     Train net output #0: loss = 0.0278067 (* 1 = 0.0278067 loss)
I0814 19:20:33.548715 24732 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0814 19:20:35.171685 24732 solver.cpp:363] Sparsity after update:
I0814 19:20:35.173333 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:20:35.173342 24732 net.cpp:2192] conv1a_param_0(0.0396) 
I0814 19:20:35.173352 24732 net.cpp:2192] conv1b_param_0(0.079) 
I0814 19:20:35.173357 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:20:35.173360 24732 net.cpp:2192] res2a_branch2a_param_0(0.0972) 
I0814 19:20:35.173365 24732 net.cpp:2192] res2a_branch2b_param_0(0.0972) 
I0814 19:20:35.173368 24732 net.cpp:2192] res3a_branch2a_param_0(0.099) 
I0814 19:20:35.173372 24732 net.cpp:2192] res3a_branch2b_param_0(0.0972) 
I0814 19:20:35.173377 24732 net.cpp:2192] res4a_branch2a_param_0(0.0998) 
I0814 19:20:35.173379 24732 net.cpp:2192] res4a_branch2b_param_0(0.099) 
I0814 19:20:35.173383 24732 net.cpp:2192] res5a_branch2a_param_0(0.0992) 
I0814 19:20:35.173388 24732 net.cpp:2192] res5a_branch2b_param_0(0.0998) 
I0814 19:20:35.173391 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (233683/2.3599e+06) 0.099
I0814 19:20:35.173424 24732 solver.cpp:509] Iteration 9000, Testing net (#0)
I0814 19:20:35.986474 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.858825
I0814 19:20:35.986490 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:20:35.986495 24732 solver.cpp:594]     Test net output #2: loss = 0.585828 (* 1 = 0.585828 loss)
I0814 19:20:35.986510 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813058s
I0814 19:20:36.002166 24783 solver.cpp:409] Finding and applying sparsity: 0.12
I0814 19:20:56.175587 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:20:56.177620 24732 solver.cpp:312] Iteration 9000 (4.41923 iter/s, 22.6284s/100 iter), loss = 0.0145252
I0814 19:20:56.177644 24732 solver.cpp:334]     Train net output #0: loss = 0.0145251 (* 1 = 0.0145251 loss)
I0814 19:20:56.177654 24732 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0814 19:20:57.743373 24716 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:20:57.964887 24732 solver.cpp:312] Iteration 9100 (55.9532 iter/s, 1.78721s/100 iter), loss = 0.0325974
I0814 19:20:57.964913 24732 solver.cpp:334]     Train net output #0: loss = 0.0325973 (* 1 = 0.0325973 loss)
I0814 19:20:57.964920 24732 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0814 19:20:59.581303 24732 solver.cpp:312] Iteration 9200 (61.8672 iter/s, 1.61637s/100 iter), loss = 0.0401114
I0814 19:20:59.581328 24732 solver.cpp:334]     Train net output #0: loss = 0.0401113 (* 1 = 0.0401113 loss)
I0814 19:20:59.581333 24732 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0814 19:21:01.196466 24732 solver.cpp:312] Iteration 9300 (61.9152 iter/s, 1.61511s/100 iter), loss = 0.00562272
I0814 19:21:01.196566 24732 solver.cpp:334]     Train net output #0: loss = 0.0056226 (* 1 = 0.0056226 loss)
I0814 19:21:01.196575 24732 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0814 19:21:02.843112 24732 solver.cpp:312] Iteration 9400 (60.7313 iter/s, 1.6466s/100 iter), loss = 0.0146899
I0814 19:21:02.843158 24732 solver.cpp:334]     Train net output #0: loss = 0.0146898 (* 1 = 0.0146898 loss)
I0814 19:21:02.843170 24732 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0814 19:21:04.458920 24732 solver.cpp:312] Iteration 9500 (61.8906 iter/s, 1.61575s/100 iter), loss = 0.0536371
I0814 19:21:04.458948 24732 solver.cpp:334]     Train net output #0: loss = 0.053637 (* 1 = 0.053637 loss)
I0814 19:21:04.458956 24732 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0814 19:21:06.126191 24732 solver.cpp:312] Iteration 9600 (59.9801 iter/s, 1.66722s/100 iter), loss = 0.0117439
I0814 19:21:06.126253 24732 solver.cpp:334]     Train net output #0: loss = 0.0117438 (* 1 = 0.0117438 loss)
I0814 19:21:06.126271 24732 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0814 19:21:07.790086 24732 solver.cpp:312] Iteration 9700 (60.1019 iter/s, 1.66384s/100 iter), loss = 0.0615666
I0814 19:21:07.790154 24732 solver.cpp:334]     Train net output #0: loss = 0.0615665 (* 1 = 0.0615665 loss)
I0814 19:21:07.790174 24732 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0814 19:21:09.434990 24732 solver.cpp:312] Iteration 9800 (60.7957 iter/s, 1.64485s/100 iter), loss = 0.00999332
I0814 19:21:09.435019 24732 solver.cpp:334]     Train net output #0: loss = 0.0099932 (* 1 = 0.0099932 loss)
I0814 19:21:09.435025 24732 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0814 19:21:11.088933 24732 solver.cpp:312] Iteration 9900 (60.4634 iter/s, 1.65389s/100 iter), loss = 0.00118539
I0814 19:21:11.088958 24732 solver.cpp:334]     Train net output #0: loss = 0.0011853 (* 1 = 0.0011853 loss)
I0814 19:21:11.088964 24732 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0814 19:21:12.676767 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0814 19:21:12.691593 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0814 19:21:12.695134 24732 solver.cpp:363] Sparsity after update:
I0814 19:21:12.697767 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:21:12.697777 24732 net.cpp:2192] conv1a_param_0(0.0525) 
I0814 19:21:12.697785 24732 net.cpp:2192] conv1b_param_0(0.0903) 
I0814 19:21:12.697796 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:21:12.697805 24732 net.cpp:2192] res2a_branch2a_param_0(0.118) 
I0814 19:21:12.697818 24732 net.cpp:2192] res2a_branch2b_param_0(0.118) 
I0814 19:21:12.697825 24732 net.cpp:2192] res3a_branch2a_param_0(0.12) 
I0814 19:21:12.697842 24732 net.cpp:2192] res3a_branch2b_param_0(0.118) 
I0814 19:21:12.697851 24732 net.cpp:2192] res4a_branch2a_param_0(0.12) 
I0814 19:21:12.697862 24732 net.cpp:2192] res4a_branch2b_param_0(0.12) 
I0814 19:21:12.697872 24732 net.cpp:2192] res5a_branch2a_param_0(0.119) 
I0814 19:21:12.697882 24732 net.cpp:2192] res5a_branch2b_param_0(0.12) 
I0814 19:21:12.697887 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (280283/2.3599e+06) 0.119
I0814 19:21:12.697904 24732 solver.cpp:509] Iteration 10000, Testing net (#0)
I0814 19:21:13.501556 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.861766
I0814 19:21:13.501576 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:21:13.501583 24732 solver.cpp:594]     Test net output #2: loss = 0.536421 (* 1 = 0.536421 loss)
I0814 19:21:13.501601 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.803669s
I0814 19:21:13.517264 24783 solver.cpp:409] Finding and applying sparsity: 0.14
I0814 19:21:33.793207 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:21:33.795333 24732 solver.cpp:312] Iteration 10000 (4.40417 iter/s, 22.7058s/100 iter), loss = 0.00449185
I0814 19:21:33.795351 24732 solver.cpp:334]     Train net output #0: loss = 0.00449177 (* 1 = 0.00449177 loss)
I0814 19:21:33.795356 24732 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0814 19:21:35.640600 24732 solver.cpp:312] Iteration 10100 (54.1945 iter/s, 1.84521s/100 iter), loss = 0.00606891
I0814 19:21:35.640656 24732 solver.cpp:334]     Train net output #0: loss = 0.00606883 (* 1 = 0.00606883 loss)
I0814 19:21:35.640671 24732 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0814 19:21:37.321328 24732 solver.cpp:312] Iteration 10200 (59.4998 iter/s, 1.68068s/100 iter), loss = 0.0389266
I0814 19:21:37.321400 24732 solver.cpp:334]     Train net output #0: loss = 0.0389265 (* 1 = 0.0389265 loss)
I0814 19:21:37.321422 24732 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0814 19:21:38.953063 24732 solver.cpp:312] Iteration 10300 (61.2864 iter/s, 1.63168s/100 iter), loss = 0.00270737
I0814 19:21:38.953127 24732 solver.cpp:334]     Train net output #0: loss = 0.00270732 (* 1 = 0.00270732 loss)
I0814 19:21:38.953147 24732 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0814 19:21:40.609830 24732 solver.cpp:312] Iteration 10400 (60.3605 iter/s, 1.65671s/100 iter), loss = 0.0197287
I0814 19:21:40.609854 24732 solver.cpp:334]     Train net output #0: loss = 0.0197287 (* 1 = 0.0197287 loss)
I0814 19:21:40.609860 24732 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0814 19:21:42.257455 24732 solver.cpp:312] Iteration 10500 (60.6953 iter/s, 1.64757s/100 iter), loss = 0.0205076
I0814 19:21:42.257510 24732 solver.cpp:334]     Train net output #0: loss = 0.0205075 (* 1 = 0.0205075 loss)
I0814 19:21:42.257524 24732 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0814 19:21:43.932659 24732 solver.cpp:312] Iteration 10600 (59.6961 iter/s, 1.67515s/100 iter), loss = 0.0260483
I0814 19:21:43.932705 24732 solver.cpp:334]     Train net output #0: loss = 0.0260483 (* 1 = 0.0260483 loss)
I0814 19:21:43.932713 24732 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0814 19:21:45.592733 24732 solver.cpp:312] Iteration 10700 (60.2402 iter/s, 1.66002s/100 iter), loss = 0.0125613
I0814 19:21:45.592763 24732 solver.cpp:334]     Train net output #0: loss = 0.0125612 (* 1 = 0.0125612 loss)
I0814 19:21:45.592772 24732 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0814 19:21:47.228042 24732 solver.cpp:312] Iteration 10800 (61.1524 iter/s, 1.63526s/100 iter), loss = 0.0351802
I0814 19:21:47.228106 24732 solver.cpp:334]     Train net output #0: loss = 0.0351801 (* 1 = 0.0351801 loss)
I0814 19:21:47.228124 24732 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0814 19:21:48.912852 24732 solver.cpp:312] Iteration 10900 (59.3558 iter/s, 1.68476s/100 iter), loss = 0.00205397
I0814 19:21:48.913084 24732 solver.cpp:334]     Train net output #0: loss = 0.00205388 (* 1 = 0.00205388 loss)
I0814 19:21:48.913091 24732 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0814 19:21:50.512024 24732 solver.cpp:363] Sparsity after update:
I0814 19:21:50.513788 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:21:50.513798 24732 net.cpp:2192] conv1a_param_0(0.0658) 
I0814 19:21:50.513808 24732 net.cpp:2192] conv1b_param_0(0.139) 
I0814 19:21:50.513813 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:21:50.513818 24732 net.cpp:2192] res2a_branch2a_param_0(0.139) 
I0814 19:21:50.513821 24732 net.cpp:2192] res2a_branch2b_param_0(0.139) 
I0814 19:21:50.513826 24732 net.cpp:2192] res3a_branch2a_param_0(0.139) 
I0814 19:21:50.513829 24732 net.cpp:2192] res3a_branch2b_param_0(0.139) 
I0814 19:21:50.513833 24732 net.cpp:2192] res4a_branch2a_param_0(0.14) 
I0814 19:21:50.513835 24732 net.cpp:2192] res4a_branch2b_param_0(0.139) 
I0814 19:21:50.513839 24732 net.cpp:2192] res5a_branch2a_param_0(0.138) 
I0814 19:21:50.513841 24732 net.cpp:2192] res5a_branch2b_param_0(0.14) 
I0814 19:21:50.513846 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (326330/2.3599e+06) 0.138
I0814 19:21:50.513871 24732 solver.cpp:509] Iteration 11000, Testing net (#0)
I0814 19:21:51.324314 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.884119
I0814 19:21:51.324334 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:21:51.324342 24732 solver.cpp:594]     Test net output #2: loss = 0.458849 (* 1 = 0.458849 loss)
I0814 19:21:51.324357 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810459s
I0814 19:21:51.341850 24783 solver.cpp:409] Finding and applying sparsity: 0.16
I0814 19:22:11.165117 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:22:11.167142 24732 solver.cpp:312] Iteration 11000 (4.49364 iter/s, 22.2537s/100 iter), loss = 0.0170473
I0814 19:22:11.167165 24732 solver.cpp:334]     Train net output #0: loss = 0.0170472 (* 1 = 0.0170472 loss)
I0814 19:22:11.167173 24732 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0814 19:22:13.029328 24732 solver.cpp:312] Iteration 11100 (53.702 iter/s, 1.86213s/100 iter), loss = 0.00276705
I0814 19:22:13.029353 24732 solver.cpp:334]     Train net output #0: loss = 0.00276692 (* 1 = 0.00276692 loss)
I0814 19:22:13.029361 24732 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0814 19:22:14.648807 24732 solver.cpp:312] Iteration 11200 (61.7503 iter/s, 1.61943s/100 iter), loss = 0.0562505
I0814 19:22:14.648833 24732 solver.cpp:334]     Train net output #0: loss = 0.0562503 (* 1 = 0.0562503 loss)
I0814 19:22:14.648840 24732 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0814 19:22:16.291115 24732 solver.cpp:312] Iteration 11300 (60.8919 iter/s, 1.64226s/100 iter), loss = 0.000931007
I0814 19:22:16.291167 24732 solver.cpp:334]     Train net output #0: loss = 0.000930859 (* 1 = 0.000930859 loss)
I0814 19:22:16.291179 24732 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0814 19:22:17.943562 24732 solver.cpp:312] Iteration 11400 (60.5182 iter/s, 1.6524s/100 iter), loss = 0.00305178
I0814 19:22:17.943584 24732 solver.cpp:334]     Train net output #0: loss = 0.00305163 (* 1 = 0.00305163 loss)
I0814 19:22:17.943590 24732 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0814 19:22:19.580310 24732 solver.cpp:312] Iteration 11500 (61.0987 iter/s, 1.6367s/100 iter), loss = 0.00348338
I0814 19:22:19.580376 24732 solver.cpp:334]     Train net output #0: loss = 0.00348322 (* 1 = 0.00348322 loss)
I0814 19:22:19.580396 24732 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0814 19:22:21.264436 24732 solver.cpp:312] Iteration 11600 (59.3799 iter/s, 1.68407s/100 iter), loss = 0.00606551
I0814 19:22:21.264461 24732 solver.cpp:334]     Train net output #0: loss = 0.00606534 (* 1 = 0.00606534 loss)
I0814 19:22:21.264467 24732 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0814 19:22:22.932440 24732 solver.cpp:312] Iteration 11700 (59.9538 iter/s, 1.66795s/100 iter), loss = 0.00193452
I0814 19:22:22.932466 24732 solver.cpp:334]     Train net output #0: loss = 0.00193434 (* 1 = 0.00193434 loss)
I0814 19:22:22.932471 24732 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0814 19:22:24.620512 24732 solver.cpp:312] Iteration 11800 (59.241 iter/s, 1.68802s/100 iter), loss = 0.000500849
I0814 19:22:24.620559 24732 solver.cpp:334]     Train net output #0: loss = 0.000500674 (* 1 = 0.000500674 loss)
I0814 19:22:24.620573 24732 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0814 19:22:26.287770 24732 solver.cpp:312] Iteration 11900 (59.9806 iter/s, 1.66721s/100 iter), loss = 0.173825
I0814 19:22:26.287799 24732 solver.cpp:334]     Train net output #0: loss = 0.173825 (* 1 = 0.173825 loss)
I0814 19:22:26.287806 24732 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0814 19:22:27.976153 24732 solver.cpp:363] Sparsity after update:
I0814 19:22:27.978051 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:22:27.978063 24732 net.cpp:2192] conv1a_param_0(0.0658) 
I0814 19:22:27.978130 24732 net.cpp:2192] conv1b_param_0(0.153) 
I0814 19:22:27.978138 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:22:27.978154 24732 net.cpp:2192] res2a_branch2a_param_0(0.16) 
I0814 19:22:27.978165 24732 net.cpp:2192] res2a_branch2b_param_0(0.16) 
I0814 19:22:27.978174 24732 net.cpp:2192] res3a_branch2a_param_0(0.16) 
I0814 19:22:27.978183 24732 net.cpp:2192] res3a_branch2b_param_0(0.16) 
I0814 19:22:27.978193 24732 net.cpp:2192] res4a_branch2a_param_0(0.16) 
I0814 19:22:27.978204 24732 net.cpp:2192] res4a_branch2b_param_0(0.16) 
I0814 19:22:27.978214 24732 net.cpp:2192] res5a_branch2a_param_0(0.158) 
I0814 19:22:27.978224 24732 net.cpp:2192] res5a_branch2b_param_0(0.16) 
I0814 19:22:27.978235 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (373383/2.3599e+06) 0.158
I0814 19:22:27.978265 24732 solver.cpp:509] Iteration 12000, Testing net (#0)
I0814 19:22:28.819255 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.894707
I0814 19:22:28.819274 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:22:28.819279 24732 solver.cpp:594]     Test net output #2: loss = 0.447308 (* 1 = 0.447308 loss)
I0814 19:22:28.819304 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.841012s
I0814 19:22:28.836529 24783 solver.cpp:409] Finding and applying sparsity: 0.18
I0814 19:22:48.833614 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:22:48.835667 24732 solver.cpp:312] Iteration 12000 (4.43512 iter/s, 22.5473s/100 iter), loss = 0.000283893
I0814 19:22:48.835690 24732 solver.cpp:334]     Train net output #0: loss = 0.000283734 (* 1 = 0.000283734 loss)
I0814 19:22:48.835701 24732 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0814 19:22:50.631300 24732 solver.cpp:312] Iteration 12100 (55.6924 iter/s, 1.79558s/100 iter), loss = 0.00120777
I0814 19:22:50.631325 24732 solver.cpp:334]     Train net output #0: loss = 0.00120761 (* 1 = 0.00120761 loss)
I0814 19:22:50.631331 24732 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0814 19:22:52.261951 24732 solver.cpp:312] Iteration 12200 (61.3272 iter/s, 1.6306s/100 iter), loss = 0.0508494
I0814 19:22:52.261976 24732 solver.cpp:334]     Train net output #0: loss = 0.0508492 (* 1 = 0.0508492 loss)
I0814 19:22:52.261982 24732 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0814 19:22:53.908377 24732 solver.cpp:312] Iteration 12300 (60.7395 iter/s, 1.64637s/100 iter), loss = 0.000822553
I0814 19:22:53.908408 24732 solver.cpp:334]     Train net output #0: loss = 0.000822384 (* 1 = 0.000822384 loss)
I0814 19:22:53.908416 24732 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0814 19:22:55.585548 24732 solver.cpp:312] Iteration 12400 (59.6261 iter/s, 1.67712s/100 iter), loss = 0.0011282
I0814 19:22:55.585572 24732 solver.cpp:334]     Train net output #0: loss = 0.00112804 (* 1 = 0.00112804 loss)
I0814 19:22:55.585577 24732 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0814 19:22:57.256640 24732 solver.cpp:312] Iteration 12500 (59.843 iter/s, 1.67104s/100 iter), loss = 0.00692147
I0814 19:22:57.256667 24732 solver.cpp:334]     Train net output #0: loss = 0.00692131 (* 1 = 0.00692131 loss)
I0814 19:22:57.256672 24732 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0814 19:22:58.964196 24732 solver.cpp:312] Iteration 12600 (58.5654 iter/s, 1.70749s/100 iter), loss = 0.00467734
I0814 19:22:58.964254 24732 solver.cpp:334]     Train net output #0: loss = 0.00467716 (* 1 = 0.00467716 loss)
I0814 19:22:58.964267 24732 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0814 19:23:00.586798 24732 solver.cpp:312] Iteration 12700 (61.6311 iter/s, 1.62256s/100 iter), loss = 0.00160508
I0814 19:23:00.586823 24732 solver.cpp:334]     Train net output #0: loss = 0.0016049 (* 1 = 0.0016049 loss)
I0814 19:23:00.586829 24732 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0814 19:23:02.248893 24732 solver.cpp:312] Iteration 12800 (60.167 iter/s, 1.66204s/100 iter), loss = 0.0407584
I0814 19:23:02.248947 24732 solver.cpp:334]     Train net output #0: loss = 0.0407582 (* 1 = 0.0407582 loss)
I0814 19:23:02.248961 24732 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0814 19:23:03.912156 24732 solver.cpp:312] Iteration 12900 (60.1246 iter/s, 1.66321s/100 iter), loss = 0.0214099
I0814 19:23:03.912184 24732 solver.cpp:334]     Train net output #0: loss = 0.0214097 (* 1 = 0.0214097 loss)
I0814 19:23:03.912191 24732 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0814 19:23:05.556000 24732 solver.cpp:363] Sparsity after update:
I0814 19:23:05.557713 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:23:05.557723 24732 net.cpp:2192] conv1a_param_0(0.0787) 
I0814 19:23:05.557729 24732 net.cpp:2192] conv1b_param_0(0.167) 
I0814 19:23:05.557730 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:23:05.557732 24732 net.cpp:2192] res2a_branch2a_param_0(0.177) 
I0814 19:23:05.557734 24732 net.cpp:2192] res2a_branch2b_param_0(0.174) 
I0814 19:23:05.557736 24732 net.cpp:2192] res3a_branch2a_param_0(0.179) 
I0814 19:23:05.557739 24732 net.cpp:2192] res3a_branch2b_param_0(0.177) 
I0814 19:23:05.557741 24732 net.cpp:2192] res4a_branch2a_param_0(0.18) 
I0814 19:23:05.557744 24732 net.cpp:2192] res4a_branch2b_param_0(0.179) 
I0814 19:23:05.557746 24732 net.cpp:2192] res5a_branch2a_param_0(0.176) 
I0814 19:23:05.557749 24732 net.cpp:2192] res5a_branch2b_param_0(0.179) 
I0814 19:23:05.557750 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (418411/2.3599e+06) 0.177
I0814 19:23:05.557770 24732 solver.cpp:509] Iteration 13000, Testing net (#0)
I0814 19:23:06.382977 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.897354
I0814 19:23:06.382997 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:23:06.383003 24732 solver.cpp:594]     Test net output #2: loss = 0.404941 (* 1 = 0.404941 loss)
I0814 19:23:06.383019 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825221s
I0814 19:23:06.398967 24783 solver.cpp:409] Finding and applying sparsity: 0.2
I0814 19:23:25.906666 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:23:25.908761 24732 solver.cpp:312] Iteration 13000 (4.54628 iter/s, 21.996s/100 iter), loss = 0.00205158
I0814 19:23:25.908779 24732 solver.cpp:334]     Train net output #0: loss = 0.00205138 (* 1 = 0.00205138 loss)
I0814 19:23:25.908784 24732 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0814 19:23:27.752334 24732 solver.cpp:312] Iteration 13100 (54.2442 iter/s, 1.84352s/100 iter), loss = 0.00191839
I0814 19:23:27.752398 24732 solver.cpp:334]     Train net output #0: loss = 0.0019182 (* 1 = 0.0019182 loss)
I0814 19:23:27.752418 24732 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0814 19:23:29.383154 24732 solver.cpp:312] Iteration 13200 (61.3208 iter/s, 1.63077s/100 iter), loss = 0.00631823
I0814 19:23:29.383306 24732 solver.cpp:334]     Train net output #0: loss = 0.00631801 (* 1 = 0.00631801 loss)
I0814 19:23:29.383329 24732 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0814 19:23:31.001551 24732 solver.cpp:312] Iteration 13300 (61.7915 iter/s, 1.61835s/100 iter), loss = 0.00179946
I0814 19:23:31.001574 24732 solver.cpp:334]     Train net output #0: loss = 0.00179925 (* 1 = 0.00179925 loss)
I0814 19:23:31.001577 24732 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0814 19:23:32.649865 24732 solver.cpp:312] Iteration 13400 (60.67 iter/s, 1.64826s/100 iter), loss = 0.000793511
I0814 19:23:32.649931 24732 solver.cpp:334]     Train net output #0: loss = 0.00079331 (* 1 = 0.00079331 loss)
I0814 19:23:32.649951 24732 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0814 19:23:34.293196 24732 solver.cpp:312] Iteration 13500 (60.8538 iter/s, 1.64328s/100 iter), loss = 0.0358798
I0814 19:23:34.293220 24732 solver.cpp:334]     Train net output #0: loss = 0.0358796 (* 1 = 0.0358796 loss)
I0814 19:23:34.293226 24732 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0814 19:23:35.947861 24732 solver.cpp:312] Iteration 13600 (60.4372 iter/s, 1.65461s/100 iter), loss = 0.00118653
I0814 19:23:35.947906 24732 solver.cpp:334]     Train net output #0: loss = 0.00118635 (* 1 = 0.00118635 loss)
I0814 19:23:35.947918 24732 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0814 19:23:36.506425 24716 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:23:37.589046 24732 solver.cpp:312] Iteration 13700 (60.9422 iter/s, 1.6409s/100 iter), loss = 0.020078
I0814 19:23:37.589074 24732 solver.cpp:334]     Train net output #0: loss = 0.0200778 (* 1 = 0.0200778 loss)
I0814 19:23:37.589082 24732 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0814 19:23:39.243427 24732 solver.cpp:312] Iteration 13800 (60.439 iter/s, 1.65456s/100 iter), loss = 0.00647985
I0814 19:23:39.243451 24732 solver.cpp:334]     Train net output #0: loss = 0.00647965 (* 1 = 0.00647965 loss)
I0814 19:23:39.243458 24732 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0814 19:23:40.875916 24732 solver.cpp:312] Iteration 13900 (61.2579 iter/s, 1.63244s/100 iter), loss = 0.00362782
I0814 19:23:40.875965 24732 solver.cpp:334]     Train net output #0: loss = 0.00362761 (* 1 = 0.00362761 loss)
I0814 19:23:40.875979 24732 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0814 19:23:42.506799 24732 solver.cpp:363] Sparsity after update:
I0814 19:23:42.508273 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:23:42.508283 24732 net.cpp:2192] conv1a_param_0(0.0917) 
I0814 19:23:42.508291 24732 net.cpp:2192] conv1b_param_0(0.194) 
I0814 19:23:42.508302 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:23:42.508308 24732 net.cpp:2192] res2a_branch2a_param_0(0.198) 
I0814 19:23:42.508316 24732 net.cpp:2192] res2a_branch2b_param_0(0.194) 
I0814 19:23:42.508321 24732 net.cpp:2192] res3a_branch2a_param_0(0.2) 
I0814 19:23:42.508324 24732 net.cpp:2192] res3a_branch2b_param_0(0.198) 
I0814 19:23:42.508333 24732 net.cpp:2192] res4a_branch2a_param_0(0.2) 
I0814 19:23:42.508337 24732 net.cpp:2192] res4a_branch2b_param_0(0.2) 
I0814 19:23:42.508340 24732 net.cpp:2192] res5a_branch2a_param_0(0.196) 
I0814 19:23:42.508348 24732 net.cpp:2192] res5a_branch2b_param_0(0.199) 
I0814 19:23:42.508368 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (465402/2.3599e+06) 0.197
I0814 19:23:42.508383 24732 solver.cpp:509] Iteration 14000, Testing net (#0)
I0814 19:23:43.317541 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902354
I0814 19:23:43.317560 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:23:43.317567 24732 solver.cpp:594]     Test net output #2: loss = 0.397323 (* 1 = 0.397323 loss)
I0814 19:23:43.317584 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.809173s
I0814 19:23:43.333268 24783 solver.cpp:409] Finding and applying sparsity: 0.22
I0814 19:24:02.541982 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:24:02.544035 24732 solver.cpp:312] Iteration 14000 (4.6152 iter/s, 21.6675s/100 iter), loss = 0.00258346
I0814 19:24:02.544055 24732 solver.cpp:334]     Train net output #0: loss = 0.00258325 (* 1 = 0.00258325 loss)
I0814 19:24:02.544064 24732 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0814 19:24:04.419647 24732 solver.cpp:312] Iteration 14100 (53.3176 iter/s, 1.87555s/100 iter), loss = 0.0185316
I0814 19:24:04.419674 24732 solver.cpp:334]     Train net output #0: loss = 0.0185314 (* 1 = 0.0185314 loss)
I0814 19:24:04.419679 24732 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0814 19:24:06.073278 24732 solver.cpp:312] Iteration 14200 (60.4749 iter/s, 1.65358s/100 iter), loss = 0.00103831
I0814 19:24:06.073298 24732 solver.cpp:334]     Train net output #0: loss = 0.00103809 (* 1 = 0.00103809 loss)
I0814 19:24:06.073302 24732 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0814 19:24:07.704717 24732 solver.cpp:312] Iteration 14300 (61.2975 iter/s, 1.63139s/100 iter), loss = 0.00158774
I0814 19:24:07.704741 24732 solver.cpp:334]     Train net output #0: loss = 0.00158754 (* 1 = 0.00158754 loss)
I0814 19:24:07.704747 24732 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0814 19:24:09.384285 24732 solver.cpp:312] Iteration 14400 (59.541 iter/s, 1.67951s/100 iter), loss = 0.00595956
I0814 19:24:09.384310 24732 solver.cpp:334]     Train net output #0: loss = 0.00595936 (* 1 = 0.00595936 loss)
I0814 19:24:09.384316 24732 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0814 19:24:11.037570 24732 solver.cpp:312] Iteration 14500 (60.4876 iter/s, 1.65323s/100 iter), loss = 0.0100729
I0814 19:24:11.037771 24732 solver.cpp:334]     Train net output #0: loss = 0.0100727 (* 1 = 0.0100727 loss)
I0814 19:24:11.037778 24732 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0814 19:24:12.697700 24732 solver.cpp:312] Iteration 14600 (60.2381 iter/s, 1.66008s/100 iter), loss = 0.0103488
I0814 19:24:12.697774 24732 solver.cpp:334]     Train net output #0: loss = 0.0103486 (* 1 = 0.0103486 loss)
I0814 19:24:12.697798 24732 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0814 19:24:14.341539 24732 solver.cpp:312] Iteration 14700 (60.8353 iter/s, 1.64378s/100 iter), loss = 0.00148125
I0814 19:24:14.341588 24732 solver.cpp:334]     Train net output #0: loss = 0.00148105 (* 1 = 0.00148105 loss)
I0814 19:24:14.341601 24732 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0814 19:24:16.006736 24732 solver.cpp:312] Iteration 14800 (60.0548 iter/s, 1.66515s/100 iter), loss = 0.00496316
I0814 19:24:16.006784 24732 solver.cpp:334]     Train net output #0: loss = 0.00496297 (* 1 = 0.00496297 loss)
I0814 19:24:16.006798 24732 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0814 19:24:17.674269 24732 solver.cpp:312] Iteration 14900 (59.9706 iter/s, 1.66748s/100 iter), loss = 0.00170011
I0814 19:24:17.674320 24732 solver.cpp:334]     Train net output #0: loss = 0.00169992 (* 1 = 0.00169992 loss)
I0814 19:24:17.674335 24732 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0814 19:24:19.316359 24732 solver.cpp:363] Sparsity after update:
I0814 19:24:19.317980 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:24:19.317988 24732 net.cpp:2192] conv1a_param_0(0.105) 
I0814 19:24:19.317994 24732 net.cpp:2192] conv1b_param_0(0.208) 
I0814 19:24:19.317996 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:24:19.317999 24732 net.cpp:2192] res2a_branch2a_param_0(0.219) 
I0814 19:24:19.318002 24732 net.cpp:2192] res2a_branch2b_param_0(0.215) 
I0814 19:24:19.318004 24732 net.cpp:2192] res3a_branch2a_param_0(0.219) 
I0814 19:24:19.318006 24732 net.cpp:2192] res3a_branch2b_param_0(0.219) 
I0814 19:24:19.318008 24732 net.cpp:2192] res4a_branch2a_param_0(0.22) 
I0814 19:24:19.318011 24732 net.cpp:2192] res4a_branch2b_param_0(0.219) 
I0814 19:24:19.318012 24732 net.cpp:2192] res5a_branch2a_param_0(0.213) 
I0814 19:24:19.318014 24732 net.cpp:2192] res5a_branch2b_param_0(0.219) 
I0814 19:24:19.318015 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (509010/2.3599e+06) 0.216
I0814 19:24:19.318035 24732 solver.cpp:509] Iteration 15000, Testing net (#0)
I0814 19:24:20.124647 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.893825
I0814 19:24:20.124665 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 19:24:20.124670 24732 solver.cpp:594]     Test net output #2: loss = 0.406789 (* 1 = 0.406789 loss)
I0814 19:24:20.124685 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.806622s
I0814 19:24:20.140568 24783 solver.cpp:409] Finding and applying sparsity: 0.24
I0814 19:24:39.458744 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:24:39.460844 24732 solver.cpp:312] Iteration 15000 (4.59011 iter/s, 21.786s/100 iter), loss = 0.00224286
I0814 19:24:39.460861 24732 solver.cpp:334]     Train net output #0: loss = 0.00224267 (* 1 = 0.00224267 loss)
I0814 19:24:39.460866 24732 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0814 19:24:41.355484 24732 solver.cpp:312] Iteration 15100 (52.7822 iter/s, 1.89458s/100 iter), loss = 0.000958177
I0814 19:24:41.355543 24732 solver.cpp:334]     Train net output #0: loss = 0.000957986 (* 1 = 0.000957986 loss)
I0814 19:24:41.355561 24732 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0814 19:24:43.014632 24732 solver.cpp:312] Iteration 15200 (60.2738 iter/s, 1.6591s/100 iter), loss = 0.000888344
I0814 19:24:43.014657 24732 solver.cpp:334]     Train net output #0: loss = 0.000888147 (* 1 = 0.000888147 loss)
I0814 19:24:43.014662 24732 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0814 19:24:44.655565 24732 solver.cpp:312] Iteration 15300 (60.9428 iter/s, 1.64088s/100 iter), loss = 0.00125217
I0814 19:24:44.655661 24732 solver.cpp:334]     Train net output #0: loss = 0.00125198 (* 1 = 0.00125198 loss)
I0814 19:24:44.655669 24732 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0814 19:24:46.305740 24732 solver.cpp:312] Iteration 15400 (60.6016 iter/s, 1.65012s/100 iter), loss = 0.0253144
I0814 19:24:46.305766 24732 solver.cpp:334]     Train net output #0: loss = 0.0253142 (* 1 = 0.0253142 loss)
I0814 19:24:46.305771 24732 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0814 19:24:47.958312 24732 solver.cpp:312] Iteration 15500 (60.5137 iter/s, 1.65252s/100 iter), loss = 0.000817344
I0814 19:24:47.958336 24732 solver.cpp:334]     Train net output #0: loss = 0.000817158 (* 1 = 0.000817158 loss)
I0814 19:24:47.958343 24732 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0814 19:24:49.621125 24732 solver.cpp:312] Iteration 15600 (60.1408 iter/s, 1.66277s/100 iter), loss = 0.00103404
I0814 19:24:49.621152 24732 solver.cpp:334]     Train net output #0: loss = 0.00103385 (* 1 = 0.00103385 loss)
I0814 19:24:49.621158 24732 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0814 19:24:51.248046 24732 solver.cpp:312] Iteration 15700 (61.4677 iter/s, 1.62687s/100 iter), loss = 0.00369776
I0814 19:24:51.248205 24732 solver.cpp:334]     Train net output #0: loss = 0.00369757 (* 1 = 0.00369757 loss)
I0814 19:24:51.248226 24732 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0814 19:24:52.894896 24732 solver.cpp:312] Iteration 15800 (60.7239 iter/s, 1.6468s/100 iter), loss = 0.00080073
I0814 19:24:52.894920 24732 solver.cpp:334]     Train net output #0: loss = 0.000800541 (* 1 = 0.000800541 loss)
I0814 19:24:52.894927 24732 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0814 19:24:54.565218 24732 solver.cpp:312] Iteration 15900 (59.8707 iter/s, 1.67027s/100 iter), loss = 0.00127364
I0814 19:24:54.565243 24732 solver.cpp:334]     Train net output #0: loss = 0.00127344 (* 1 = 0.00127344 loss)
I0814 19:24:54.565248 24732 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0814 19:24:56.224027 24732 solver.cpp:363] Sparsity after update:
I0814 19:24:56.225576 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:24:56.225586 24732 net.cpp:2192] conv1a_param_0(0.105) 
I0814 19:24:56.225594 24732 net.cpp:2192] conv1b_param_0(0.236) 
I0814 19:24:56.225608 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:24:56.225620 24732 net.cpp:2192] res2a_branch2a_param_0(0.24) 
I0814 19:24:56.225627 24732 net.cpp:2192] res2a_branch2b_param_0(0.236) 
I0814 19:24:56.225636 24732 net.cpp:2192] res3a_branch2a_param_0(0.24) 
I0814 19:24:56.225643 24732 net.cpp:2192] res3a_branch2b_param_0(0.24) 
I0814 19:24:56.225652 24732 net.cpp:2192] res4a_branch2a_param_0(0.24) 
I0814 19:24:56.225662 24732 net.cpp:2192] res4a_branch2b_param_0(0.24) 
I0814 19:24:56.225672 24732 net.cpp:2192] res5a_branch2a_param_0(0.235) 
I0814 19:24:56.225680 24732 net.cpp:2192] res5a_branch2b_param_0(0.239) 
I0814 19:24:56.225690 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (558485/2.3599e+06) 0.237
I0814 19:24:56.225728 24732 solver.cpp:509] Iteration 16000, Testing net (#0)
I0814 19:24:57.045595 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.90206
I0814 19:24:57.045614 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:24:57.045619 24732 solver.cpp:594]     Test net output #2: loss = 0.417604 (* 1 = 0.417604 loss)
I0814 19:24:57.045637 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819883s
I0814 19:24:57.061151 24783 solver.cpp:409] Finding and applying sparsity: 0.26
I0814 19:25:16.118401 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:25:16.120580 24732 solver.cpp:312] Iteration 16000 (4.63935 iter/s, 21.5548s/100 iter), loss = 0.00459923
I0814 19:25:16.120607 24732 solver.cpp:334]     Train net output #0: loss = 0.00459903 (* 1 = 0.00459903 loss)
I0814 19:25:16.120615 24732 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0814 19:25:17.963963 24732 solver.cpp:312] Iteration 16100 (54.2497 iter/s, 1.84333s/100 iter), loss = 0.00911604
I0814 19:25:17.964012 24732 solver.cpp:334]     Train net output #0: loss = 0.00911585 (* 1 = 0.00911585 loss)
I0814 19:25:17.964026 24732 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0814 19:25:19.584833 24732 solver.cpp:312] Iteration 16200 (61.6972 iter/s, 1.62082s/100 iter), loss = 0.000581402
I0814 19:25:19.584857 24732 solver.cpp:334]     Train net output #0: loss = 0.000581206 (* 1 = 0.000581206 loss)
I0814 19:25:19.584863 24732 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0814 19:25:21.231947 24732 solver.cpp:312] Iteration 16300 (60.7141 iter/s, 1.64706s/100 iter), loss = 0.00098809
I0814 19:25:21.232015 24732 solver.cpp:334]     Train net output #0: loss = 0.000987897 (* 1 = 0.000987897 loss)
I0814 19:25:21.232038 24732 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0814 19:25:22.837409 24732 solver.cpp:312] Iteration 16400 (62.2894 iter/s, 1.60541s/100 iter), loss = 0.00559085
I0814 19:25:22.837436 24732 solver.cpp:334]     Train net output #0: loss = 0.00559066 (* 1 = 0.00559066 loss)
I0814 19:25:22.837671 24732 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0814 19:25:24.469357 24732 solver.cpp:312] Iteration 16500 (61.2783 iter/s, 1.6319s/100 iter), loss = 0.00203228
I0814 19:25:24.469424 24732 solver.cpp:334]     Train net output #0: loss = 0.00203209 (* 1 = 0.00203209 loss)
I0814 19:25:24.469442 24732 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0814 19:25:26.101030 24732 solver.cpp:312] Iteration 16600 (61.2887 iter/s, 1.63162s/100 iter), loss = 0.00397477
I0814 19:25:26.101055 24732 solver.cpp:334]     Train net output #0: loss = 0.00397457 (* 1 = 0.00397457 loss)
I0814 19:25:26.101060 24732 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0814 19:25:27.772845 24732 solver.cpp:312] Iteration 16700 (59.8171 iter/s, 1.67176s/100 iter), loss = 0.00166215
I0814 19:25:27.772915 24732 solver.cpp:334]     Train net output #0: loss = 0.00166196 (* 1 = 0.00166196 loss)
I0814 19:25:27.772935 24732 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0814 19:25:29.423104 24732 solver.cpp:312] Iteration 16800 (60.5985 iter/s, 1.65021s/100 iter), loss = 0.00181501
I0814 19:25:29.423164 24732 solver.cpp:334]     Train net output #0: loss = 0.00181482 (* 1 = 0.00181482 loss)
I0814 19:25:29.423183 24732 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0814 19:25:31.098502 24732 solver.cpp:312] Iteration 16900 (59.6891 iter/s, 1.67535s/100 iter), loss = 0.00122949
I0814 19:25:31.098552 24732 solver.cpp:334]     Train net output #0: loss = 0.00122931 (* 1 = 0.00122931 loss)
I0814 19:25:31.098565 24732 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0814 19:25:32.723767 24732 solver.cpp:363] Sparsity after update:
I0814 19:25:32.725456 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:25:32.725466 24732 net.cpp:2192] conv1a_param_0(0.117) 
I0814 19:25:32.725474 24732 net.cpp:2192] conv1b_param_0(0.25) 
I0814 19:25:32.725486 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:25:32.725502 24732 net.cpp:2192] res2a_branch2a_param_0(0.257) 
I0814 19:25:32.725507 24732 net.cpp:2192] res2a_branch2b_param_0(0.257) 
I0814 19:25:32.725510 24732 net.cpp:2192] res3a_branch2a_param_0(0.259) 
I0814 19:25:32.725518 24732 net.cpp:2192] res3a_branch2b_param_0(0.257) 
I0814 19:25:32.725527 24732 net.cpp:2192] res4a_branch2a_param_0(0.26) 
I0814 19:25:32.725531 24732 net.cpp:2192] res4a_branch2b_param_0(0.259) 
I0814 19:25:32.725539 24732 net.cpp:2192] res5a_branch2a_param_0(0.252) 
I0814 19:25:32.725548 24732 net.cpp:2192] res5a_branch2b_param_0(0.259) 
I0814 19:25:32.725553 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (601508/2.3599e+06) 0.255
I0814 19:25:32.725574 24732 solver.cpp:509] Iteration 17000, Testing net (#0)
I0814 19:25:33.550528 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912648
I0814 19:25:33.550546 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:25:33.550554 24732 solver.cpp:594]     Test net output #2: loss = 0.352792 (* 1 = 0.352792 loss)
I0814 19:25:33.550571 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824969s
I0814 19:25:33.567263 24783 solver.cpp:409] Finding and applying sparsity: 0.28
I0814 19:25:51.833163 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:25:51.835222 24732 solver.cpp:312] Iteration 17000 (4.8225 iter/s, 20.7362s/100 iter), loss = 0.0027044
I0814 19:25:51.835240 24732 solver.cpp:334]     Train net output #0: loss = 0.00270421 (* 1 = 0.00270421 loss)
I0814 19:25:51.835247 24732 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0814 19:25:53.685092 24732 solver.cpp:312] Iteration 17100 (54.0595 iter/s, 1.84981s/100 iter), loss = 0.0032926
I0814 19:25:53.685117 24732 solver.cpp:334]     Train net output #0: loss = 0.00329241 (* 1 = 0.00329241 loss)
I0814 19:25:53.685120 24732 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0814 19:25:55.333523 24732 solver.cpp:312] Iteration 17200 (60.6656 iter/s, 1.64838s/100 iter), loss = 0.000255504
I0814 19:25:55.333554 24732 solver.cpp:334]     Train net output #0: loss = 0.000255316 (* 1 = 0.000255316 loss)
I0814 19:25:55.333560 24732 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0814 19:25:56.997534 24732 solver.cpp:312] Iteration 17300 (60.0977 iter/s, 1.66396s/100 iter), loss = 0.00171162
I0814 19:25:56.997562 24732 solver.cpp:334]     Train net output #0: loss = 0.00171144 (* 1 = 0.00171144 loss)
I0814 19:25:56.997570 24732 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0814 19:25:58.661736 24732 solver.cpp:312] Iteration 17400 (60.0907 iter/s, 1.66415s/100 iter), loss = 0.000400008
I0814 19:25:58.661782 24732 solver.cpp:334]     Train net output #0: loss = 0.00039982 (* 1 = 0.00039982 loss)
I0814 19:25:58.661793 24732 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0814 19:26:00.325840 24732 solver.cpp:312] Iteration 17500 (60.0942 iter/s, 1.66405s/100 iter), loss = 0.00337782
I0814 19:26:00.325891 24732 solver.cpp:334]     Train net output #0: loss = 0.00337763 (* 1 = 0.00337763 loss)
I0814 19:26:00.325904 24732 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0814 19:26:01.948827 24732 solver.cpp:312] Iteration 17600 (61.6167 iter/s, 1.62294s/100 iter), loss = 0.00152812
I0814 19:26:01.948876 24732 solver.cpp:334]     Train net output #0: loss = 0.00152793 (* 1 = 0.00152793 loss)
I0814 19:26:01.948891 24732 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0814 19:26:03.636660 24732 solver.cpp:312] Iteration 17700 (59.2495 iter/s, 1.68778s/100 iter), loss = 0.000561906
I0814 19:26:03.636689 24732 solver.cpp:334]     Train net output #0: loss = 0.000561716 (* 1 = 0.000561716 loss)
I0814 19:26:03.636695 24732 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0814 19:26:05.306705 24732 solver.cpp:312] Iteration 17800 (59.8805 iter/s, 1.66999s/100 iter), loss = 0.0235166
I0814 19:26:05.306771 24732 solver.cpp:334]     Train net output #0: loss = 0.0235164 (* 1 = 0.0235164 loss)
I0814 19:26:05.306798 24732 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0814 19:26:06.983981 24732 solver.cpp:312] Iteration 17900 (59.6222 iter/s, 1.67723s/100 iter), loss = 0.0037681
I0814 19:26:06.984045 24732 solver.cpp:334]     Train net output #0: loss = 0.0037679 (* 1 = 0.0037679 loss)
I0814 19:26:06.984063 24732 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0814 19:26:08.605217 24732 solver.cpp:363] Sparsity after update:
I0814 19:26:08.606905 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:26:08.606915 24732 net.cpp:2192] conv1a_param_0(0.131) 
I0814 19:26:08.606920 24732 net.cpp:2192] conv1b_param_0(0.278) 
I0814 19:26:08.606922 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:26:08.606925 24732 net.cpp:2192] res2a_branch2a_param_0(0.278) 
I0814 19:26:08.606926 24732 net.cpp:2192] res2a_branch2b_param_0(0.278) 
I0814 19:26:08.606928 24732 net.cpp:2192] res3a_branch2a_param_0(0.28) 
I0814 19:26:08.606930 24732 net.cpp:2192] res3a_branch2b_param_0(0.278) 
I0814 19:26:08.606935 24732 net.cpp:2192] res4a_branch2a_param_0(0.28) 
I0814 19:26:08.606937 24732 net.cpp:2192] res4a_branch2b_param_0(0.28) 
I0814 19:26:08.606940 24732 net.cpp:2192] res5a_branch2a_param_0(0.272) 
I0814 19:26:08.606943 24732 net.cpp:2192] res5a_branch2b_param_0(0.278) 
I0814 19:26:08.606948 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (648716/2.3599e+06) 0.275
I0814 19:26:08.606971 24732 solver.cpp:509] Iteration 18000, Testing net (#0)
I0814 19:26:09.089166 24730 data_reader.cpp:288] Starting prefetch of epoch 2
I0814 19:26:09.431809 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910001
I0814 19:26:09.431828 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:26:09.431833 24732 solver.cpp:594]     Test net output #2: loss = 0.362977 (* 1 = 0.362977 loss)
I0814 19:26:09.431846 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824849s
I0814 19:26:09.447471 24783 solver.cpp:409] Finding and applying sparsity: 0.3
I0814 19:26:27.494105 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:26:27.496201 24732 solver.cpp:312] Iteration 18000 (4.87528 iter/s, 20.5117s/100 iter), loss = 0.00116718
I0814 19:26:27.496222 24732 solver.cpp:334]     Train net output #0: loss = 0.00116699 (* 1 = 0.00116699 loss)
I0814 19:26:27.496232 24732 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0814 19:26:29.327950 24732 solver.cpp:312] Iteration 18100 (54.5944 iter/s, 1.83169s/100 iter), loss = 0.000726388
I0814 19:26:29.327975 24732 solver.cpp:334]     Train net output #0: loss = 0.000726198 (* 1 = 0.000726198 loss)
I0814 19:26:29.327980 24732 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0814 19:26:30.962028 24732 solver.cpp:312] Iteration 18200 (61.1985 iter/s, 1.63403s/100 iter), loss = 0.00112927
I0814 19:26:30.962054 24732 solver.cpp:334]     Train net output #0: loss = 0.00112908 (* 1 = 0.00112908 loss)
I0814 19:26:30.962059 24732 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0814 19:26:32.625844 24732 solver.cpp:312] Iteration 18300 (60.1047 iter/s, 1.66376s/100 iter), loss = 0.00370702
I0814 19:26:32.625869 24732 solver.cpp:334]     Train net output #0: loss = 0.00370683 (* 1 = 0.00370683 loss)
I0814 19:26:32.625874 24732 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0814 19:26:34.253134 24732 solver.cpp:312] Iteration 18400 (61.4538 iter/s, 1.62724s/100 iter), loss = 0.00169106
I0814 19:26:34.253199 24732 solver.cpp:334]     Train net output #0: loss = 0.00169087 (* 1 = 0.00169087 loss)
I0814 19:26:34.253219 24732 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0814 19:26:35.903472 24732 solver.cpp:312] Iteration 18500 (60.5956 iter/s, 1.65029s/100 iter), loss = 0.000595505
I0814 19:26:35.903519 24732 solver.cpp:334]     Train net output #0: loss = 0.000595316 (* 1 = 0.000595316 loss)
I0814 19:26:35.903532 24732 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0814 19:26:37.570554 24732 solver.cpp:312] Iteration 18600 (59.9869 iter/s, 1.66703s/100 iter), loss = 0.00122615
I0814 19:26:37.570581 24732 solver.cpp:334]     Train net output #0: loss = 0.00122597 (* 1 = 0.00122597 loss)
I0814 19:26:37.570587 24732 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0814 19:26:39.251938 24732 solver.cpp:312] Iteration 18700 (59.4766 iter/s, 1.68133s/100 iter), loss = 0.00290339
I0814 19:26:39.251960 24732 solver.cpp:334]     Train net output #0: loss = 0.00290321 (* 1 = 0.00290321 loss)
I0814 19:26:39.251965 24732 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0814 19:26:40.872900 24732 solver.cpp:312] Iteration 18800 (61.6937 iter/s, 1.62091s/100 iter), loss = 0.000255397
I0814 19:26:40.872963 24732 solver.cpp:334]     Train net output #0: loss = 0.000255219 (* 1 = 0.000255219 loss)
I0814 19:26:40.872982 24732 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0814 19:26:42.545444 24732 solver.cpp:312] Iteration 18900 (59.791 iter/s, 1.67249s/100 iter), loss = 0.00111211
I0814 19:26:42.545469 24732 solver.cpp:334]     Train net output #0: loss = 0.00111193 (* 1 = 0.00111193 loss)
I0814 19:26:42.545473 24732 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0814 19:26:44.194943 24732 solver.cpp:363] Sparsity after update:
I0814 19:26:44.196842 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:26:44.196866 24732 net.cpp:2192] conv1a_param_0(0.144) 
I0814 19:26:44.196876 24732 net.cpp:2192] conv1b_param_0(0.292) 
I0814 19:26:44.196878 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:26:44.196882 24732 net.cpp:2192] res2a_branch2a_param_0(0.299) 
I0814 19:26:44.196884 24732 net.cpp:2192] res2a_branch2b_param_0(0.299) 
I0814 19:26:44.196887 24732 net.cpp:2192] res3a_branch2a_param_0(0.299) 
I0814 19:26:44.196893 24732 net.cpp:2192] res3a_branch2b_param_0(0.299) 
I0814 19:26:44.196897 24732 net.cpp:2192] res4a_branch2a_param_0(0.299) 
I0814 19:26:44.196899 24732 net.cpp:2192] res4a_branch2b_param_0(0.299) 
I0814 19:26:44.196902 24732 net.cpp:2192] res5a_branch2a_param_0(0.291) 
I0814 19:26:44.196905 24732 net.cpp:2192] res5a_branch2b_param_0(0.298) 
I0814 19:26:44.196908 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (694260/2.3599e+06) 0.294
I0814 19:26:44.196944 24732 solver.cpp:509] Iteration 19000, Testing net (#0)
I0814 19:26:45.008393 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919707
I0814 19:26:45.008410 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:26:45.008416 24732 solver.cpp:594]     Test net output #2: loss = 0.352241 (* 1 = 0.352241 loss)
I0814 19:26:45.008435 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811463s
I0814 19:26:45.028421 24783 solver.cpp:409] Finding and applying sparsity: 0.32
I0814 19:27:01.880056 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:27:01.882175 24732 solver.cpp:312] Iteration 19000 (5.17165 iter/s, 19.3362s/100 iter), loss = 0.00388482
I0814 19:27:01.882195 24732 solver.cpp:334]     Train net output #0: loss = 0.00388464 (* 1 = 0.00388464 loss)
I0814 19:27:01.882205 24732 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0814 19:27:03.714684 24732 solver.cpp:312] Iteration 19100 (54.5718 iter/s, 1.83245s/100 iter), loss = 0.000352213
I0814 19:27:03.714736 24732 solver.cpp:334]     Train net output #0: loss = 0.000352037 (* 1 = 0.000352037 loss)
I0814 19:27:03.714752 24732 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0814 19:27:05.373013 24732 solver.cpp:312] Iteration 19200 (60.3036 iter/s, 1.65828s/100 iter), loss = 0.000106831
I0814 19:27:05.373162 24732 solver.cpp:334]     Train net output #0: loss = 0.000106656 (* 1 = 0.000106656 loss)
I0814 19:27:05.373186 24732 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0814 19:27:07.044878 24732 solver.cpp:312] Iteration 19300 (59.8153 iter/s, 1.67181s/100 iter), loss = 0.000979713
I0814 19:27:07.044903 24732 solver.cpp:334]     Train net output #0: loss = 0.000979536 (* 1 = 0.000979536 loss)
I0814 19:27:07.044909 24732 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0814 19:27:08.717198 24732 solver.cpp:312] Iteration 19400 (59.7991 iter/s, 1.67227s/100 iter), loss = 0.00395256
I0814 19:27:08.717226 24732 solver.cpp:334]     Train net output #0: loss = 0.00395239 (* 1 = 0.00395239 loss)
I0814 19:27:08.717233 24732 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0814 19:27:10.363628 24732 solver.cpp:312] Iteration 19500 (60.7394 iter/s, 1.64638s/100 iter), loss = 0.000343357
I0814 19:27:10.363653 24732 solver.cpp:334]     Train net output #0: loss = 0.000343181 (* 1 = 0.000343181 loss)
I0814 19:27:10.363659 24732 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0814 19:27:12.005643 24732 solver.cpp:312] Iteration 19600 (60.9027 iter/s, 1.64196s/100 iter), loss = 0.000878822
I0814 19:27:12.005671 24732 solver.cpp:334]     Train net output #0: loss = 0.000878649 (* 1 = 0.000878649 loss)
I0814 19:27:12.005676 24732 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0814 19:27:13.684731 24732 solver.cpp:312] Iteration 19700 (59.558 iter/s, 1.67903s/100 iter), loss = 0.000262267
I0814 19:27:13.684756 24732 solver.cpp:334]     Train net output #0: loss = 0.000262094 (* 1 = 0.000262094 loss)
I0814 19:27:13.684762 24732 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0814 19:27:15.308837 24732 solver.cpp:312] Iteration 19800 (61.5742 iter/s, 1.62406s/100 iter), loss = 0.000242115
I0814 19:27:15.308863 24732 solver.cpp:334]     Train net output #0: loss = 0.000241944 (* 1 = 0.000241944 loss)
I0814 19:27:15.308868 24732 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0814 19:27:16.936738 24732 solver.cpp:312] Iteration 19900 (61.4307 iter/s, 1.62785s/100 iter), loss = 0.00233469
I0814 19:27:16.936763 24732 solver.cpp:334]     Train net output #0: loss = 0.00233452 (* 1 = 0.00233452 loss)
I0814 19:27:16.936769 24732 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0814 19:27:18.577703 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0814 19:27:18.587214 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0814 19:27:18.592003 24732 solver.cpp:363] Sparsity after update:
I0814 19:27:18.594238 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:27:18.594249 24732 net.cpp:2192] conv1a_param_0(0.156) 
I0814 19:27:18.594267 24732 net.cpp:2192] conv1b_param_0(0.319) 
I0814 19:27:18.594277 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:27:18.594288 24732 net.cpp:2192] res2a_branch2a_param_0(0.319) 
I0814 19:27:18.594300 24732 net.cpp:2192] res2a_branch2b_param_0(0.319) 
I0814 19:27:18.594311 24732 net.cpp:2192] res3a_branch2a_param_0(0.319) 
I0814 19:27:18.595923 24732 net.cpp:2192] res3a_branch2b_param_0(0.319) 
I0814 19:27:18.595937 24732 net.cpp:2192] res4a_branch2a_param_0(0.319) 
I0814 19:27:18.595947 24732 net.cpp:2192] res4a_branch2b_param_0(0.319) 
I0814 19:27:18.595958 24732 net.cpp:2192] res5a_branch2a_param_0(0.309) 
I0814 19:27:18.595970 24732 net.cpp:2192] res5a_branch2b_param_0(0.316) 
I0814 19:27:18.595980 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (738132/2.3599e+06) 0.313
I0814 19:27:18.595999 24732 solver.cpp:509] Iteration 20000, Testing net (#0)
I0814 19:27:19.412324 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912354
I0814 19:27:19.412341 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:27:19.412346 24732 solver.cpp:594]     Test net output #2: loss = 0.368176 (* 1 = 0.368176 loss)
I0814 19:27:19.412361 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.816337s
I0814 19:27:19.428688 24783 solver.cpp:409] Finding and applying sparsity: 0.34
I0814 19:27:36.405364 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:27:36.407443 24732 solver.cpp:312] Iteration 20000 (5.13606 iter/s, 19.4702s/100 iter), loss = 0.000164382
I0814 19:27:36.407465 24732 solver.cpp:334]     Train net output #0: loss = 0.000164211 (* 1 = 0.000164211 loss)
I0814 19:27:36.407472 24732 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0814 19:27:38.236939 24732 solver.cpp:312] Iteration 20100 (54.6615 iter/s, 1.82944s/100 iter), loss = 0.00135074
I0814 19:27:38.236966 24732 solver.cpp:334]     Train net output #0: loss = 0.00135057 (* 1 = 0.00135057 loss)
I0814 19:27:38.236971 24732 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0814 19:27:39.888635 24732 solver.cpp:312] Iteration 20200 (60.5458 iter/s, 1.65164s/100 iter), loss = 0.00340738
I0814 19:27:39.888660 24732 solver.cpp:334]     Train net output #0: loss = 0.00340721 (* 1 = 0.00340721 loss)
I0814 19:27:39.888665 24732 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0814 19:27:41.526932 24732 solver.cpp:312] Iteration 20300 (61.0409 iter/s, 1.63825s/100 iter), loss = 0.000823781
I0814 19:27:41.526955 24732 solver.cpp:334]     Train net output #0: loss = 0.000823609 (* 1 = 0.000823609 loss)
I0814 19:27:41.526962 24732 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0814 19:27:43.187641 24732 solver.cpp:312] Iteration 20400 (60.2171 iter/s, 1.66066s/100 iter), loss = 0.000592722
I0814 19:27:43.187708 24732 solver.cpp:334]     Train net output #0: loss = 0.00059255 (* 1 = 0.00059255 loss)
I0814 19:27:43.187729 24732 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0814 19:27:44.843792 24732 solver.cpp:312] Iteration 20500 (60.3829 iter/s, 1.6561s/100 iter), loss = 0.000271238
I0814 19:27:44.843842 24732 solver.cpp:334]     Train net output #0: loss = 0.000271066 (* 1 = 0.000271066 loss)
I0814 19:27:44.843857 24732 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0814 19:27:46.524277 24732 solver.cpp:312] Iteration 20600 (59.5085 iter/s, 1.68043s/100 iter), loss = 0.00159072
I0814 19:27:46.524302 24732 solver.cpp:334]     Train net output #0: loss = 0.00159055 (* 1 = 0.00159055 loss)
I0814 19:27:46.524307 24732 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0814 19:27:48.216410 24732 solver.cpp:312] Iteration 20700 (59.0988 iter/s, 1.69208s/100 iter), loss = 0.00603862
I0814 19:27:48.216461 24732 solver.cpp:334]     Train net output #0: loss = 0.00603845 (* 1 = 0.00603845 loss)
I0814 19:27:48.216473 24732 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0814 19:27:49.831873 24732 solver.cpp:312] Iteration 20800 (61.9038 iter/s, 1.61541s/100 iter), loss = 0.000219169
I0814 19:27:49.831898 24732 solver.cpp:334]     Train net output #0: loss = 0.000218999 (* 1 = 0.000218999 loss)
I0814 19:27:49.831904 24732 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0814 19:27:51.499860 24732 solver.cpp:312] Iteration 20900 (59.9545 iter/s, 1.66793s/100 iter), loss = 0.000560707
I0814 19:27:51.499928 24732 solver.cpp:334]     Train net output #0: loss = 0.000560537 (* 1 = 0.000560537 loss)
I0814 19:27:51.499949 24732 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0814 19:27:53.156365 24732 solver.cpp:363] Sparsity after update:
I0814 19:27:53.157809 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:27:53.157819 24732 net.cpp:2192] conv1a_param_0(0.157) 
I0814 19:27:53.157826 24732 net.cpp:2192] conv1b_param_0(0.333) 
I0814 19:27:53.157837 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:27:53.157848 24732 net.cpp:2192] res2a_branch2a_param_0(0.337) 
I0814 19:27:53.157857 24732 net.cpp:2192] res2a_branch2b_param_0(0.333) 
I0814 19:27:53.157866 24732 net.cpp:2192] res3a_branch2a_param_0(0.339) 
I0814 19:27:53.157873 24732 net.cpp:2192] res3a_branch2b_param_0(0.337) 
I0814 19:27:53.157881 24732 net.cpp:2192] res4a_branch2a_param_0(0.339) 
I0814 19:27:53.157891 24732 net.cpp:2192] res4a_branch2b_param_0(0.339) 
I0814 19:27:53.157899 24732 net.cpp:2192] res5a_branch2a_param_0(0.328) 
I0814 19:27:53.157908 24732 net.cpp:2192] res5a_branch2b_param_0(0.336) 
I0814 19:27:53.157934 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (783046/2.3599e+06) 0.332
I0814 19:27:53.157958 24732 solver.cpp:509] Iteration 21000, Testing net (#0)
I0814 19:27:54.000222 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91353
I0814 19:27:54.000239 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:27:54.000246 24732 solver.cpp:594]     Test net output #2: loss = 0.346492 (* 1 = 0.346492 loss)
I0814 19:27:54.000262 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.842277s
I0814 19:27:54.016428 24783 solver.cpp:409] Finding and applying sparsity: 0.36
I0814 19:28:10.973839 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:28:10.975875 24732 solver.cpp:312] Iteration 21000 (5.13466 iter/s, 19.4755s/100 iter), loss = 0.00115068
I0814 19:28:10.975894 24732 solver.cpp:334]     Train net output #0: loss = 0.00115051 (* 1 = 0.00115051 loss)
I0814 19:28:10.975901 24732 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0814 19:28:12.817991 24732 solver.cpp:312] Iteration 21100 (54.2871 iter/s, 1.84206s/100 iter), loss = 0.000520022
I0814 19:28:12.818040 24732 solver.cpp:334]     Train net output #0: loss = 0.000519855 (* 1 = 0.000519855 loss)
I0814 19:28:12.818053 24732 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0814 19:28:14.491020 24732 solver.cpp:312] Iteration 21200 (59.7737 iter/s, 1.67298s/100 iter), loss = 0.00170081
I0814 19:28:14.491045 24732 solver.cpp:334]     Train net output #0: loss = 0.00170064 (* 1 = 0.00170064 loss)
I0814 19:28:14.491050 24732 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0814 19:28:16.169443 24732 solver.cpp:312] Iteration 21300 (59.5816 iter/s, 1.67837s/100 iter), loss = 0.000727657
I0814 19:28:16.169489 24732 solver.cpp:334]     Train net output #0: loss = 0.00072749 (* 1 = 0.00072749 loss)
I0814 19:28:16.169502 24732 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0814 19:28:17.843453 24732 solver.cpp:312] Iteration 21400 (59.7387 iter/s, 1.67396s/100 iter), loss = 0.000424811
I0814 19:28:17.843533 24732 solver.cpp:334]     Train net output #0: loss = 0.000424643 (* 1 = 0.000424643 loss)
I0814 19:28:17.843539 24732 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0814 19:28:19.513025 24732 solver.cpp:312] Iteration 21500 (59.8974 iter/s, 1.66952s/100 iter), loss = 0.000602788
I0814 19:28:19.513095 24732 solver.cpp:334]     Train net output #0: loss = 0.00060262 (* 1 = 0.00060262 loss)
I0814 19:28:19.513119 24732 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0814 19:28:21.190906 24732 solver.cpp:312] Iteration 21600 (59.6008 iter/s, 1.67783s/100 iter), loss = 0.00025335
I0814 19:28:21.190971 24732 solver.cpp:334]     Train net output #0: loss = 0.000253181 (* 1 = 0.000253181 loss)
I0814 19:28:21.190990 24732 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0814 19:28:22.844830 24732 solver.cpp:312] Iteration 21700 (60.4642 iter/s, 1.65387s/100 iter), loss = 0.00126061
I0814 19:28:22.844856 24732 solver.cpp:334]     Train net output #0: loss = 0.00126044 (* 1 = 0.00126044 loss)
I0814 19:28:22.844861 24732 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0814 19:28:24.467742 24732 solver.cpp:312] Iteration 21800 (61.6195 iter/s, 1.62286s/100 iter), loss = 0.00286068
I0814 19:28:24.467806 24732 solver.cpp:334]     Train net output #0: loss = 0.00286051 (* 1 = 0.00286051 loss)
I0814 19:28:24.467825 24732 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0814 19:28:26.116878 24732 solver.cpp:312] Iteration 21900 (60.6397 iter/s, 1.64908s/100 iter), loss = 0.000200331
I0814 19:28:26.116925 24732 solver.cpp:334]     Train net output #0: loss = 0.000200163 (* 1 = 0.000200163 loss)
I0814 19:28:26.116938 24732 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0814 19:28:27.753262 24732 solver.cpp:363] Sparsity after update:
I0814 19:28:27.754979 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:28:27.754988 24732 net.cpp:2192] conv1a_param_0(0.17) 
I0814 19:28:27.754997 24732 net.cpp:2192] conv1b_param_0(0.347) 
I0814 19:28:27.755013 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:28:27.755018 24732 net.cpp:2192] res2a_branch2a_param_0(0.358) 
I0814 19:28:27.755022 24732 net.cpp:2192] res2a_branch2b_param_0(0.354) 
I0814 19:28:27.755030 24732 net.cpp:2192] res3a_branch2a_param_0(0.359) 
I0814 19:28:27.755034 24732 net.cpp:2192] res3a_branch2b_param_0(0.358) 
I0814 19:28:27.755038 24732 net.cpp:2192] res4a_branch2a_param_0(0.359) 
I0814 19:28:27.755048 24732 net.cpp:2192] res4a_branch2b_param_0(0.359) 
I0814 19:28:27.755051 24732 net.cpp:2192] res5a_branch2a_param_0(0.349) 
I0814 19:28:27.755054 24732 net.cpp:2192] res5a_branch2b_param_0(0.357) 
I0814 19:28:27.755067 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (832165/2.3599e+06) 0.353
I0814 19:28:27.755084 24732 solver.cpp:509] Iteration 22000, Testing net (#0)
I0814 19:28:28.171067 24730 data_reader.cpp:288] Starting prefetch of epoch 3
I0814 19:28:28.571316 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910589
I0814 19:28:28.571336 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:28:28.571341 24732 solver.cpp:594]     Test net output #2: loss = 0.356396 (* 1 = 0.356396 loss)
I0814 19:28:28.571359 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.816246s
I0814 19:28:28.586876 24783 solver.cpp:409] Finding and applying sparsity: 0.38
I0814 19:28:44.376334 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:28:44.378371 24732 solver.cpp:312] Iteration 22000 (5.47615 iter/s, 18.261s/100 iter), loss = 0.000714687
I0814 19:28:44.378393 24732 solver.cpp:334]     Train net output #0: loss = 0.000714519 (* 1 = 0.000714519 loss)
I0814 19:28:44.378399 24732 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0814 19:28:46.252866 24732 solver.cpp:312] Iteration 22100 (53.3493 iter/s, 1.87444s/100 iter), loss = 0.00162986
I0814 19:28:46.252926 24732 solver.cpp:334]     Train net output #0: loss = 0.00162969 (* 1 = 0.00162969 loss)
I0814 19:28:46.252945 24732 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0814 19:28:47.878057 24732 solver.cpp:312] Iteration 22200 (61.5332 iter/s, 1.62514s/100 iter), loss = 0.000720889
I0814 19:28:47.878129 24732 solver.cpp:334]     Train net output #0: loss = 0.000720721 (* 1 = 0.000720721 loss)
I0814 19:28:47.878151 24732 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0814 19:28:49.577708 24732 solver.cpp:312] Iteration 22300 (58.8374 iter/s, 1.6996s/100 iter), loss = 0.000363916
I0814 19:28:49.577780 24732 solver.cpp:334]     Train net output #0: loss = 0.000363749 (* 1 = 0.000363749 loss)
I0814 19:28:49.577800 24732 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0814 19:28:51.218849 24732 solver.cpp:312] Iteration 22400 (60.9352 iter/s, 1.64109s/100 iter), loss = 0.00120533
I0814 19:28:51.218897 24732 solver.cpp:334]     Train net output #0: loss = 0.00120516 (* 1 = 0.00120516 loss)
I0814 19:28:51.218909 24732 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0814 19:28:52.895344 24732 solver.cpp:312] Iteration 22500 (59.6502 iter/s, 1.67644s/100 iter), loss = 0.000975279
I0814 19:28:52.895368 24732 solver.cpp:334]     Train net output #0: loss = 0.000975111 (* 1 = 0.000975111 loss)
I0814 19:28:52.895373 24732 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0814 19:28:54.539022 24732 solver.cpp:312] Iteration 22600 (60.8411 iter/s, 1.64363s/100 iter), loss = 8.19431e-05
I0814 19:28:54.539047 24732 solver.cpp:334]     Train net output #0: loss = 8.17747e-05 (* 1 = 8.17747e-05 loss)
I0814 19:28:54.539052 24732 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0814 19:28:56.221141 24732 solver.cpp:312] Iteration 22700 (59.4506 iter/s, 1.68207s/100 iter), loss = 0.00176383
I0814 19:28:56.221173 24732 solver.cpp:334]     Train net output #0: loss = 0.00176367 (* 1 = 0.00176367 loss)
I0814 19:28:56.221179 24732 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0814 19:28:57.896613 24732 solver.cpp:312] Iteration 22800 (59.6865 iter/s, 1.67542s/100 iter), loss = 0.00214933
I0814 19:28:57.896643 24732 solver.cpp:334]     Train net output #0: loss = 0.00214916 (* 1 = 0.00214916 loss)
I0814 19:28:57.896649 24732 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0814 19:28:59.572464 24732 solver.cpp:312] Iteration 22900 (59.6731 iter/s, 1.6758s/100 iter), loss = 0.000130738
I0814 19:28:59.572490 24732 solver.cpp:334]     Train net output #0: loss = 0.000130571 (* 1 = 0.000130571 loss)
I0814 19:28:59.572496 24732 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0814 19:29:01.191967 24732 solver.cpp:363] Sparsity after update:
I0814 19:29:01.194038 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:29:01.194073 24732 net.cpp:2192] conv1a_param_0(0.182) 
I0814 19:29:01.194094 24732 net.cpp:2192] conv1b_param_0(0.375) 
I0814 19:29:01.194111 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:29:01.194128 24732 net.cpp:2192] res2a_branch2a_param_0(0.378) 
I0814 19:29:01.194142 24732 net.cpp:2192] res2a_branch2b_param_0(0.375) 
I0814 19:29:01.194157 24732 net.cpp:2192] res3a_branch2a_param_0(0.378) 
I0814 19:29:01.194173 24732 net.cpp:2192] res3a_branch2b_param_0(0.378) 
I0814 19:29:01.194190 24732 net.cpp:2192] res4a_branch2a_param_0(0.379) 
I0814 19:29:01.194203 24732 net.cpp:2192] res4a_branch2b_param_0(0.378) 
I0814 19:29:01.194217 24732 net.cpp:2192] res5a_branch2a_param_0(0.364) 
I0814 19:29:01.194231 24732 net.cpp:2192] res5a_branch2b_param_0(0.373) 
I0814 19:29:01.194258 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (870749/2.3599e+06) 0.369
I0814 19:29:01.194281 24732 solver.cpp:509] Iteration 23000, Testing net (#0)
I0814 19:29:02.012845 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.914118
I0814 19:29:02.012862 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:29:02.012867 24732 solver.cpp:594]     Test net output #2: loss = 0.336545 (* 1 = 0.336545 loss)
I0814 19:29:02.012882 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818574s
I0814 19:29:02.028509 24783 solver.cpp:409] Finding and applying sparsity: 0.4
I0814 19:29:17.956312 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:29:17.958441 24732 solver.cpp:312] Iteration 23000 (5.43908 iter/s, 18.3855s/100 iter), loss = 0.000925433
I0814 19:29:17.958461 24732 solver.cpp:334]     Train net output #0: loss = 0.000925266 (* 1 = 0.000925266 loss)
I0814 19:29:17.958465 24732 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0814 19:29:19.830512 24732 solver.cpp:312] Iteration 23100 (53.4185 iter/s, 1.87201s/100 iter), loss = 0.0034249
I0814 19:29:19.830574 24732 solver.cpp:334]     Train net output #0: loss = 0.00342473 (* 1 = 0.00342473 loss)
I0814 19:29:19.830592 24732 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0814 19:29:21.521045 24732 solver.cpp:312] Iteration 23200 (59.1547 iter/s, 1.69048s/100 iter), loss = 0.00118038
I0814 19:29:21.521073 24732 solver.cpp:334]     Train net output #0: loss = 0.00118021 (* 1 = 0.00118021 loss)
I0814 19:29:21.521080 24732 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0814 19:29:23.172874 24732 solver.cpp:312] Iteration 23300 (60.5409 iter/s, 1.65178s/100 iter), loss = 0.00128308
I0814 19:29:23.172924 24732 solver.cpp:334]     Train net output #0: loss = 0.00128291 (* 1 = 0.00128291 loss)
I0814 19:29:23.172937 24732 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0814 19:29:24.797850 24732 solver.cpp:312] Iteration 23400 (61.5413 iter/s, 1.62492s/100 iter), loss = 0.000200392
I0814 19:29:24.797896 24732 solver.cpp:334]     Train net output #0: loss = 0.000200228 (* 1 = 0.000200228 loss)
I0814 19:29:24.797909 24732 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0814 19:29:26.433384 24732 solver.cpp:312] Iteration 23500 (61.144 iter/s, 1.63548s/100 iter), loss = 0.000291699
I0814 19:29:26.433409 24732 solver.cpp:334]     Train net output #0: loss = 0.000291535 (* 1 = 0.000291535 loss)
I0814 19:29:26.433414 24732 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0814 19:29:28.101857 24732 solver.cpp:312] Iteration 23600 (59.9369 iter/s, 1.66842s/100 iter), loss = 0.00163935
I0814 19:29:28.101919 24732 solver.cpp:334]     Train net output #0: loss = 0.00163919 (* 1 = 0.00163919 loss)
I0814 19:29:28.101938 24732 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0814 19:29:29.777272 24732 solver.cpp:312] Iteration 23700 (59.6886 iter/s, 1.67536s/100 iter), loss = 0.000572867
I0814 19:29:29.777297 24732 solver.cpp:334]     Train net output #0: loss = 0.000572702 (* 1 = 0.000572702 loss)
I0814 19:29:29.777302 24732 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0814 19:29:31.466960 24732 solver.cpp:312] Iteration 23800 (59.1843 iter/s, 1.68964s/100 iter), loss = 0.000369331
I0814 19:29:31.466985 24732 solver.cpp:334]     Train net output #0: loss = 0.000369167 (* 1 = 0.000369167 loss)
I0814 19:29:31.466989 24732 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0814 19:29:33.113544 24732 solver.cpp:312] Iteration 23900 (60.7337 iter/s, 1.64653s/100 iter), loss = 0.000809183
I0814 19:29:33.113636 24732 solver.cpp:334]     Train net output #0: loss = 0.000809019 (* 1 = 0.000809019 loss)
I0814 19:29:33.113646 24732 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0814 19:29:34.788504 24732 solver.cpp:363] Sparsity after update:
I0814 19:29:34.790416 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:29:34.790426 24732 net.cpp:2192] conv1a_param_0(0.195) 
I0814 19:29:34.790434 24732 net.cpp:2192] conv1b_param_0(0.389) 
I0814 19:29:34.790439 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:29:34.790444 24732 net.cpp:2192] res2a_branch2a_param_0(0.399) 
I0814 19:29:34.790448 24732 net.cpp:2192] res2a_branch2b_param_0(0.396) 
I0814 19:29:34.790452 24732 net.cpp:2192] res3a_branch2a_param_0(0.399) 
I0814 19:29:34.790455 24732 net.cpp:2192] res3a_branch2b_param_0(0.399) 
I0814 19:29:34.790459 24732 net.cpp:2192] res4a_branch2a_param_0(0.399) 
I0814 19:29:34.790462 24732 net.cpp:2192] res4a_branch2b_param_0(0.399) 
I0814 19:29:34.790467 24732 net.cpp:2192] res5a_branch2a_param_0(0.385) 
I0814 19:29:34.790470 24732 net.cpp:2192] res5a_branch2b_param_0(0.393) 
I0814 19:29:34.790496 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (919675/2.3599e+06) 0.39
I0814 19:29:34.790509 24732 solver.cpp:509] Iteration 24000, Testing net (#0)
I0814 19:29:35.615821 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91353
I0814 19:29:35.615839 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:29:35.615844 24732 solver.cpp:594]     Test net output #2: loss = 0.348943 (* 1 = 0.348943 loss)
I0814 19:29:35.615859 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825322s
I0814 19:29:35.631471 24783 solver.cpp:409] Finding and applying sparsity: 0.42
I0814 19:29:51.840077 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:29:51.842149 24732 solver.cpp:312] Iteration 24000 (5.33957 iter/s, 18.7281s/100 iter), loss = 0.00265554
I0814 19:29:51.842169 24732 solver.cpp:334]     Train net output #0: loss = 0.00265537 (* 1 = 0.00265537 loss)
I0814 19:29:51.842175 24732 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0814 19:29:53.710666 24732 solver.cpp:312] Iteration 24100 (53.5201 iter/s, 1.86846s/100 iter), loss = 0.00319621
I0814 19:29:53.710695 24732 solver.cpp:334]     Train net output #0: loss = 0.00319604 (* 1 = 0.00319604 loss)
I0814 19:29:53.710700 24732 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0814 19:29:55.384690 24732 solver.cpp:312] Iteration 24200 (59.7382 iter/s, 1.67397s/100 iter), loss = 0.000680116
I0814 19:29:55.384737 24732 solver.cpp:334]     Train net output #0: loss = 0.000679951 (* 1 = 0.000679951 loss)
I0814 19:29:55.384748 24732 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0814 19:29:57.017369 24732 solver.cpp:312] Iteration 24300 (61.2509 iter/s, 1.63263s/100 iter), loss = 0.000161865
I0814 19:29:57.017398 24732 solver.cpp:334]     Train net output #0: loss = 0.000161698 (* 1 = 0.000161698 loss)
I0814 19:29:57.017405 24732 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0814 19:29:58.674170 24732 solver.cpp:312] Iteration 24400 (60.3592 iter/s, 1.65675s/100 iter), loss = 0.000320495
I0814 19:29:58.674214 24732 solver.cpp:334]     Train net output #0: loss = 0.000320329 (* 1 = 0.000320329 loss)
I0814 19:29:58.674226 24732 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0814 19:30:00.320890 24732 solver.cpp:312] Iteration 24500 (60.7286 iter/s, 1.64667s/100 iter), loss = 0.000541651
I0814 19:30:00.320916 24732 solver.cpp:334]     Train net output #0: loss = 0.000541484 (* 1 = 0.000541484 loss)
I0814 19:30:00.320922 24732 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0814 19:30:01.961957 24732 solver.cpp:312] Iteration 24600 (60.9379 iter/s, 1.64101s/100 iter), loss = 0.00101073
I0814 19:30:01.961982 24732 solver.cpp:334]     Train net output #0: loss = 0.00101056 (* 1 = 0.00101056 loss)
I0814 19:30:01.961987 24732 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0814 19:30:03.595782 24732 solver.cpp:312] Iteration 24700 (61.208 iter/s, 1.63377s/100 iter), loss = 0.000269648
I0814 19:30:03.595808 24732 solver.cpp:334]     Train net output #0: loss = 0.000269482 (* 1 = 0.000269482 loss)
I0814 19:30:03.595813 24732 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0814 19:30:05.243026 24732 solver.cpp:312] Iteration 24800 (60.7094 iter/s, 1.64719s/100 iter), loss = 0.000247004
I0814 19:30:05.243093 24732 solver.cpp:334]     Train net output #0: loss = 0.000246839 (* 1 = 0.000246839 loss)
I0814 19:30:05.243113 24732 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0814 19:30:06.887406 24732 solver.cpp:312] Iteration 24900 (60.815 iter/s, 1.64433s/100 iter), loss = 0.000273741
I0814 19:30:06.887454 24732 solver.cpp:334]     Train net output #0: loss = 0.000273578 (* 1 = 0.000273578 loss)
I0814 19:30:06.887467 24732 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0814 19:30:08.501466 24732 solver.cpp:363] Sparsity after update:
I0814 19:30:08.503096 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:30:08.503105 24732 net.cpp:2192] conv1a_param_0(0.195) 
I0814 19:30:08.503110 24732 net.cpp:2192] conv1b_param_0(0.417) 
I0814 19:30:08.503113 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:30:08.503116 24732 net.cpp:2192] res2a_branch2a_param_0(0.417) 
I0814 19:30:08.503119 24732 net.cpp:2192] res2a_branch2b_param_0(0.417) 
I0814 19:30:08.503123 24732 net.cpp:2192] res3a_branch2a_param_0(0.418) 
I0814 19:30:08.503126 24732 net.cpp:2192] res3a_branch2b_param_0(0.417) 
I0814 19:30:08.503130 24732 net.cpp:2192] res4a_branch2a_param_0(0.419) 
I0814 19:30:08.503134 24732 net.cpp:2192] res4a_branch2b_param_0(0.418) 
I0814 19:30:08.503139 24732 net.cpp:2192] res5a_branch2a_param_0(0.402) 
I0814 19:30:08.503142 24732 net.cpp:2192] res5a_branch2b_param_0(0.413) 
I0814 19:30:08.503159 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (962243/2.3599e+06) 0.408
I0814 19:30:08.503171 24732 solver.cpp:509] Iteration 25000, Testing net (#0)
I0814 19:30:09.312572 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.913824
I0814 19:30:09.312589 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:30:09.312593 24732 solver.cpp:594]     Test net output #2: loss = 0.347035 (* 1 = 0.347035 loss)
I0814 19:30:09.312608 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.80941s
I0814 19:30:09.328233 24783 solver.cpp:409] Finding and applying sparsity: 0.44
I0814 19:30:25.837566 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:30:25.839690 24732 solver.cpp:312] Iteration 25000 (5.27655 iter/s, 18.9518s/100 iter), loss = 0.000333569
I0814 19:30:25.839709 24732 solver.cpp:334]     Train net output #0: loss = 0.000333405 (* 1 = 0.000333405 loss)
I0814 19:30:25.839715 24732 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0814 19:30:27.737409 24732 solver.cpp:312] Iteration 25100 (52.6965 iter/s, 1.89766s/100 iter), loss = 0.00061552
I0814 19:30:27.737467 24732 solver.cpp:334]     Train net output #0: loss = 0.000615355 (* 1 = 0.000615355 loss)
I0814 19:30:27.737483 24732 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0814 19:30:29.373842 24732 solver.cpp:312] Iteration 25200 (61.1105 iter/s, 1.63638s/100 iter), loss = 0.00137501
I0814 19:30:29.373865 24732 solver.cpp:334]     Train net output #0: loss = 0.00137484 (* 1 = 0.00137484 loss)
I0814 19:30:29.373872 24732 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0814 19:30:31.017971 24732 solver.cpp:312] Iteration 25300 (60.8243 iter/s, 1.64408s/100 iter), loss = 0.000502649
I0814 19:30:31.017992 24732 solver.cpp:334]     Train net output #0: loss = 0.000502485 (* 1 = 0.000502485 loss)
I0814 19:30:31.017997 24732 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0814 19:30:32.684404 24732 solver.cpp:312] Iteration 25400 (60.0102 iter/s, 1.66638s/100 iter), loss = 0.000292053
I0814 19:30:32.684428 24732 solver.cpp:334]     Train net output #0: loss = 0.000291889 (* 1 = 0.000291889 loss)
I0814 19:30:32.684433 24732 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0814 19:30:34.345160 24732 solver.cpp:312] Iteration 25500 (60.2155 iter/s, 1.6607s/100 iter), loss = 0.00109013
I0814 19:30:34.345185 24732 solver.cpp:334]     Train net output #0: loss = 0.00108996 (* 1 = 0.00108996 loss)
I0814 19:30:34.345191 24732 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0814 19:30:36.009021 24732 solver.cpp:312] Iteration 25600 (60.103 iter/s, 1.66381s/100 iter), loss = 0.000689709
I0814 19:30:36.009258 24732 solver.cpp:334]     Train net output #0: loss = 0.000689545 (* 1 = 0.000689545 loss)
I0814 19:30:36.009372 24732 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0814 19:30:37.646986 24732 solver.cpp:312] Iteration 25700 (61.0535 iter/s, 1.63791s/100 iter), loss = 0.000457109
I0814 19:30:37.647011 24732 solver.cpp:334]     Train net output #0: loss = 0.000456944 (* 1 = 0.000456944 loss)
I0814 19:30:37.647017 24732 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0814 19:30:39.271766 24732 solver.cpp:312] Iteration 25800 (61.5487 iter/s, 1.62473s/100 iter), loss = 0.00173828
I0814 19:30:39.271792 24732 solver.cpp:334]     Train net output #0: loss = 0.00173811 (* 1 = 0.00173811 loss)
I0814 19:30:39.271798 24732 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0814 19:30:40.929139 24732 solver.cpp:312] Iteration 25900 (60.3383 iter/s, 1.65732s/100 iter), loss = 0.000481751
I0814 19:30:40.929165 24732 solver.cpp:334]     Train net output #0: loss = 0.000481586 (* 1 = 0.000481586 loss)
I0814 19:30:40.929172 24732 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0814 19:30:42.516952 24732 solver.cpp:363] Sparsity after update:
I0814 19:30:42.518739 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:30:42.518750 24732 net.cpp:2192] conv1a_param_0(0.207) 
I0814 19:30:42.518759 24732 net.cpp:2192] conv1b_param_0(0.431) 
I0814 19:30:42.518762 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:30:42.518766 24732 net.cpp:2192] res2a_branch2a_param_0(0.438) 
I0814 19:30:42.518770 24732 net.cpp:2192] res2a_branch2b_param_0(0.438) 
I0814 19:30:42.518774 24732 net.cpp:2192] res3a_branch2a_param_0(0.439) 
I0814 19:30:42.518779 24732 net.cpp:2192] res3a_branch2b_param_0(0.438) 
I0814 19:30:42.518781 24732 net.cpp:2192] res4a_branch2a_param_0(0.439) 
I0814 19:30:42.518786 24732 net.cpp:2192] res4a_branch2b_param_0(0.439) 
I0814 19:30:42.518790 24732 net.cpp:2192] res5a_branch2a_param_0(0.42) 
I0814 19:30:42.518795 24732 net.cpp:2192] res5a_branch2b_param_0(0.432) 
I0814 19:30:42.518815 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.00632e+06/2.3599e+06) 0.426
I0814 19:30:42.518827 24732 solver.cpp:509] Iteration 26000, Testing net (#0)
I0814 19:30:43.330585 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.90853
I0814 19:30:43.330603 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:30:43.330608 24732 solver.cpp:594]     Test net output #2: loss = 0.357479 (* 1 = 0.357479 loss)
I0814 19:30:43.330622 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811768s
I0814 19:30:43.348109 24783 solver.cpp:409] Finding and applying sparsity: 0.46
I0814 19:31:00.378372 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:31:00.380445 24732 solver.cpp:312] Iteration 26000 (5.14118 iter/s, 19.4508s/100 iter), loss = 0.00160062
I0814 19:31:00.380470 24732 solver.cpp:334]     Train net output #0: loss = 0.00160046 (* 1 = 0.00160046 loss)
I0814 19:31:00.380480 24732 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0814 19:31:02.212952 24732 solver.cpp:312] Iteration 26100 (54.5718 iter/s, 1.83245s/100 iter), loss = 0.000865614
I0814 19:31:02.212975 24732 solver.cpp:334]     Train net output #0: loss = 0.000865449 (* 1 = 0.000865449 loss)
I0814 19:31:02.212980 24732 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0814 19:31:03.833384 24732 solver.cpp:312] Iteration 26200 (61.7139 iter/s, 1.62038s/100 iter), loss = 0.00111741
I0814 19:31:03.833408 24732 solver.cpp:334]     Train net output #0: loss = 0.00111724 (* 1 = 0.00111724 loss)
I0814 19:31:03.833415 24732 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0814 19:31:05.481046 24732 solver.cpp:312] Iteration 26300 (60.6939 iter/s, 1.64761s/100 iter), loss = 0.000301433
I0814 19:31:05.481072 24732 solver.cpp:334]     Train net output #0: loss = 0.000301269 (* 1 = 0.000301269 loss)
I0814 19:31:05.481078 24732 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0814 19:31:07.133733 24732 solver.cpp:312] Iteration 26400 (60.5094 iter/s, 1.65264s/100 iter), loss = 0.000989552
I0814 19:31:07.133792 24732 solver.cpp:334]     Train net output #0: loss = 0.000989388 (* 1 = 0.000989388 loss)
I0814 19:31:07.133810 24732 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0814 19:31:08.750846 24732 solver.cpp:312] Iteration 26500 (61.8405 iter/s, 1.61706s/100 iter), loss = 0.000129571
I0814 19:31:08.750871 24732 solver.cpp:334]     Train net output #0: loss = 0.000129408 (* 1 = 0.000129408 loss)
I0814 19:31:08.750876 24732 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0814 19:31:08.941117 24716 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:31:10.351873 24732 solver.cpp:312] Iteration 26600 (62.4618 iter/s, 1.60098s/100 iter), loss = 0.000611006
I0814 19:31:10.351897 24732 solver.cpp:334]     Train net output #0: loss = 0.000610843 (* 1 = 0.000610843 loss)
I0814 19:31:10.351903 24732 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0814 19:31:12.018054 24732 solver.cpp:312] Iteration 26700 (60.0194 iter/s, 1.66613s/100 iter), loss = 0.000370696
I0814 19:31:12.018105 24732 solver.cpp:334]     Train net output #0: loss = 0.000370532 (* 1 = 0.000370532 loss)
I0814 19:31:12.018117 24732 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0814 19:31:13.640735 24732 solver.cpp:312] Iteration 26800 (61.6284 iter/s, 1.62263s/100 iter), loss = 0.00152918
I0814 19:31:13.640760 24732 solver.cpp:334]     Train net output #0: loss = 0.00152902 (* 1 = 0.00152902 loss)
I0814 19:31:13.640765 24732 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0814 19:31:15.274369 24732 solver.cpp:312] Iteration 26900 (61.2151 iter/s, 1.63358s/100 iter), loss = 0.000436234
I0814 19:31:15.274417 24732 solver.cpp:334]     Train net output #0: loss = 0.00043607 (* 1 = 0.00043607 loss)
I0814 19:31:15.274430 24732 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0814 19:31:16.896174 24732 solver.cpp:363] Sparsity after update:
I0814 19:31:16.897851 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:31:16.897863 24732 net.cpp:2192] conv1a_param_0(0.218) 
I0814 19:31:16.897873 24732 net.cpp:2192] conv1b_param_0(0.458) 
I0814 19:31:16.897877 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:31:16.897881 24732 net.cpp:2192] res2a_branch2a_param_0(0.458) 
I0814 19:31:16.897884 24732 net.cpp:2192] res2a_branch2b_param_0(0.458) 
I0814 19:31:16.897887 24732 net.cpp:2192] res3a_branch2a_param_0(0.458) 
I0814 19:31:16.897891 24732 net.cpp:2192] res3a_branch2b_param_0(0.458) 
I0814 19:31:16.897893 24732 net.cpp:2192] res4a_branch2a_param_0(0.459) 
I0814 19:31:16.897897 24732 net.cpp:2192] res4a_branch2b_param_0(0.458) 
I0814 19:31:16.897900 24732 net.cpp:2192] res5a_branch2a_param_0(0.443) 
I0814 19:31:16.897919 24732 net.cpp:2192] res5a_branch2b_param_0(0.453) 
I0814 19:31:16.897922 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.05779e+06/2.3599e+06) 0.448
I0814 19:31:16.897933 24732 solver.cpp:509] Iteration 27000, Testing net (#0)
I0814 19:31:17.728698 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905883
I0814 19:31:17.728718 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994412
I0814 19:31:17.728723 24732 solver.cpp:594]     Test net output #2: loss = 0.370931 (* 1 = 0.370931 loss)
I0814 19:31:17.728739 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.830777s
I0814 19:31:17.744547 24783 solver.cpp:409] Finding and applying sparsity: 0.48
I0814 19:31:36.680570 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:31:36.682652 24732 solver.cpp:312] Iteration 27000 (4.67122 iter/s, 21.4077s/100 iter), loss = 0.000309829
I0814 19:31:36.682672 24732 solver.cpp:334]     Train net output #0: loss = 0.000309666 (* 1 = 0.000309666 loss)
I0814 19:31:36.682678 24732 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0814 19:31:38.506826 24732 solver.cpp:312] Iteration 27100 (54.821 iter/s, 1.82412s/100 iter), loss = 0.000403272
I0814 19:31:38.506850 24732 solver.cpp:334]     Train net output #0: loss = 0.000403109 (* 1 = 0.000403109 loss)
I0814 19:31:38.506855 24732 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0814 19:31:40.165752 24732 solver.cpp:312] Iteration 27200 (60.2819 iter/s, 1.65887s/100 iter), loss = 0.0023312
I0814 19:31:40.165782 24732 solver.cpp:334]     Train net output #0: loss = 0.00233103 (* 1 = 0.00233103 loss)
I0814 19:31:40.165789 24732 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0814 19:31:41.839046 24732 solver.cpp:312] Iteration 27300 (59.7642 iter/s, 1.67324s/100 iter), loss = 0.00058463
I0814 19:31:41.839076 24732 solver.cpp:334]     Train net output #0: loss = 0.000584469 (* 1 = 0.000584469 loss)
I0814 19:31:41.839082 24732 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0814 19:31:43.487133 24732 solver.cpp:312] Iteration 27400 (60.6782 iter/s, 1.64804s/100 iter), loss = 0.000583487
I0814 19:31:43.487157 24732 solver.cpp:334]     Train net output #0: loss = 0.000583326 (* 1 = 0.000583326 loss)
I0814 19:31:43.487161 24732 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0814 19:31:45.085449 24732 solver.cpp:312] Iteration 27500 (62.5677 iter/s, 1.59827s/100 iter), loss = 0.000790025
I0814 19:31:45.085695 24732 solver.cpp:334]     Train net output #0: loss = 0.000789866 (* 1 = 0.000789866 loss)
I0814 19:31:45.085710 24732 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0814 19:31:46.701463 24732 solver.cpp:312] Iteration 27600 (61.8827 iter/s, 1.61596s/100 iter), loss = 0.00206349
I0814 19:31:46.701491 24732 solver.cpp:334]     Train net output #0: loss = 0.00206333 (* 1 = 0.00206333 loss)
I0814 19:31:46.701498 24732 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0814 19:31:48.339429 24732 solver.cpp:312] Iteration 27700 (61.0532 iter/s, 1.63791s/100 iter), loss = 0.00050414
I0814 19:31:48.339453 24732 solver.cpp:334]     Train net output #0: loss = 0.00050398 (* 1 = 0.00050398 loss)
I0814 19:31:48.339459 24732 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0814 19:31:49.960377 24732 solver.cpp:312] Iteration 27800 (61.6942 iter/s, 1.6209s/100 iter), loss = 0.000481089
I0814 19:31:49.960403 24732 solver.cpp:334]     Train net output #0: loss = 0.000480929 (* 1 = 0.000480929 loss)
I0814 19:31:49.960408 24732 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0814 19:31:51.621775 24732 solver.cpp:312] Iteration 27900 (60.1921 iter/s, 1.66135s/100 iter), loss = 0.00072739
I0814 19:31:51.621798 24732 solver.cpp:334]     Train net output #0: loss = 0.00072723 (* 1 = 0.00072723 loss)
I0814 19:31:51.621803 24732 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0814 19:31:53.228600 24732 solver.cpp:363] Sparsity after update:
I0814 19:31:53.230195 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:31:53.230203 24732 net.cpp:2192] conv1a_param_0(0.233) 
I0814 19:31:53.230211 24732 net.cpp:2192] conv1b_param_0(0.472) 
I0814 19:31:53.230212 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:31:53.230216 24732 net.cpp:2192] res2a_branch2a_param_0(0.479) 
I0814 19:31:53.230217 24732 net.cpp:2192] res2a_branch2b_param_0(0.479) 
I0814 19:31:53.230219 24732 net.cpp:2192] res3a_branch2a_param_0(0.479) 
I0814 19:31:53.230221 24732 net.cpp:2192] res3a_branch2b_param_0(0.479) 
I0814 19:31:53.230223 24732 net.cpp:2192] res4a_branch2a_param_0(0.479) 
I0814 19:31:53.230224 24732 net.cpp:2192] res4a_branch2b_param_0(0.479) 
I0814 19:31:53.230226 24732 net.cpp:2192] res5a_branch2a_param_0(0.465) 
I0814 19:31:53.230228 24732 net.cpp:2192] res5a_branch2b_param_0(0.477) 
I0814 19:31:53.230252 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.11005e+06/2.3599e+06) 0.47
I0814 19:31:53.230260 24732 solver.cpp:509] Iteration 28000, Testing net (#0)
I0814 19:31:54.050652 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.904707
I0814 19:31:54.050669 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:31:54.050674 24732 solver.cpp:594]     Test net output #2: loss = 0.367862 (* 1 = 0.367862 loss)
I0814 19:31:54.050689 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.820402s
I0814 19:31:54.066413 24783 solver.cpp:409] Finding and applying sparsity: 0.5
I0814 19:32:13.692682 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:32:13.694844 24732 solver.cpp:312] Iteration 28000 (4.53053 iter/s, 22.0725s/100 iter), loss = 0.00112252
I0814 19:32:13.694865 24732 solver.cpp:334]     Train net output #0: loss = 0.00112236 (* 1 = 0.00112236 loss)
I0814 19:32:13.694923 24732 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0814 19:32:15.503646 24732 solver.cpp:312] Iteration 28100 (55.287 iter/s, 1.80874s/100 iter), loss = 0.000911977
I0814 19:32:15.503674 24732 solver.cpp:334]     Train net output #0: loss = 0.000911817 (* 1 = 0.000911817 loss)
I0814 19:32:15.503677 24732 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0814 19:32:17.159174 24732 solver.cpp:312] Iteration 28200 (60.4055 iter/s, 1.65548s/100 iter), loss = 0.000901886
I0814 19:32:17.159198 24732 solver.cpp:334]     Train net output #0: loss = 0.000901727 (* 1 = 0.000901727 loss)
I0814 19:32:17.159204 24732 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0814 19:32:18.843464 24732 solver.cpp:312] Iteration 28300 (59.3741 iter/s, 1.68424s/100 iter), loss = 0.000218925
I0814 19:32:18.843493 24732 solver.cpp:334]     Train net output #0: loss = 0.000218766 (* 1 = 0.000218766 loss)
I0814 19:32:18.843498 24732 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0814 19:32:20.500494 24732 solver.cpp:312] Iteration 28400 (60.3508 iter/s, 1.65698s/100 iter), loss = 0.000774538
I0814 19:32:20.500641 24732 solver.cpp:334]     Train net output #0: loss = 0.000774378 (* 1 = 0.000774378 loss)
I0814 19:32:20.500665 24732 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0814 19:32:22.126641 24732 solver.cpp:312] Iteration 28500 (61.497 iter/s, 1.62609s/100 iter), loss = 0.00396671
I0814 19:32:22.126708 24732 solver.cpp:334]     Train net output #0: loss = 0.00396655 (* 1 = 0.00396655 loss)
I0814 19:32:22.126729 24732 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0814 19:32:23.764106 24732 solver.cpp:312] Iteration 28600 (61.0719 iter/s, 1.63741s/100 iter), loss = 0.000655519
I0814 19:32:23.764158 24732 solver.cpp:334]     Train net output #0: loss = 0.00065536 (* 1 = 0.00065536 loss)
I0814 19:32:23.764170 24732 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0814 19:32:25.413324 24732 solver.cpp:312] Iteration 28700 (60.6366 iter/s, 1.64917s/100 iter), loss = 0.000288579
I0814 19:32:25.413349 24732 solver.cpp:334]     Train net output #0: loss = 0.00028842 (* 1 = 0.00028842 loss)
I0814 19:32:25.413354 24732 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0814 19:32:27.078706 24732 solver.cpp:312] Iteration 28800 (60.0482 iter/s, 1.66533s/100 iter), loss = 0.000924513
I0814 19:32:27.078732 24732 solver.cpp:334]     Train net output #0: loss = 0.000924354 (* 1 = 0.000924354 loss)
I0814 19:32:27.078739 24732 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0814 19:32:28.727010 24732 solver.cpp:312] Iteration 28900 (60.6703 iter/s, 1.64825s/100 iter), loss = 0.00735213
I0814 19:32:28.727058 24732 solver.cpp:334]     Train net output #0: loss = 0.00735198 (* 1 = 0.00735198 loss)
I0814 19:32:28.727071 24732 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0814 19:32:30.325834 24732 solver.cpp:363] Sparsity after update:
I0814 19:32:30.327410 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:32:30.327420 24732 net.cpp:2192] conv1a_param_0(0.233) 
I0814 19:32:30.327426 24732 net.cpp:2192] conv1b_param_0(0.5) 
I0814 19:32:30.327430 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:32:30.327440 24732 net.cpp:2192] res2a_branch2a_param_0(0.5) 
I0814 19:32:30.327450 24732 net.cpp:2192] res2a_branch2b_param_0(0.5) 
I0814 19:32:30.327455 24732 net.cpp:2192] res3a_branch2a_param_0(0.5) 
I0814 19:32:30.327462 24732 net.cpp:2192] res3a_branch2b_param_0(0.5) 
I0814 19:32:30.327466 24732 net.cpp:2192] res4a_branch2a_param_0(0.5) 
I0814 19:32:30.327474 24732 net.cpp:2192] res4a_branch2b_param_0(0.5) 
I0814 19:32:30.327478 24732 net.cpp:2192] res5a_branch2a_param_0(0.484) 
I0814 19:32:30.327486 24732 net.cpp:2192] res5a_branch2b_param_0(0.498) 
I0814 19:32:30.327491 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.1569e+06/2.3599e+06) 0.49
I0814 19:32:30.327512 24732 solver.cpp:509] Iteration 29000, Testing net (#0)
I0814 19:32:31.138837 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912942
I0814 19:32:31.138859 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:32:31.138864 24732 solver.cpp:594]     Test net output #2: loss = 0.354866 (* 1 = 0.354866 loss)
I0814 19:32:31.138883 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.811339s
I0814 19:32:31.156527 24783 solver.cpp:409] Finding and applying sparsity: 0.52
I0814 19:32:50.707296 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:32:50.709336 24732 solver.cpp:312] Iteration 29000 (4.54923 iter/s, 21.9817s/100 iter), loss = 0.00149718
I0814 19:32:50.709359 24732 solver.cpp:334]     Train net output #0: loss = 0.00149702 (* 1 = 0.00149702 loss)
I0814 19:32:50.709367 24732 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0814 19:32:52.548439 24732 solver.cpp:312] Iteration 29100 (54.376 iter/s, 1.83905s/100 iter), loss = 0.000561861
I0814 19:32:52.548487 24732 solver.cpp:334]     Train net output #0: loss = 0.000561702 (* 1 = 0.000561702 loss)
I0814 19:32:52.548499 24732 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0814 19:32:54.181155 24732 solver.cpp:312] Iteration 29200 (61.2496 iter/s, 1.63266s/100 iter), loss = 0.00187966
I0814 19:32:54.181202 24732 solver.cpp:334]     Train net output #0: loss = 0.00187951 (* 1 = 0.00187951 loss)
I0814 19:32:54.181216 24732 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0814 19:32:55.814488 24732 solver.cpp:312] Iteration 29300 (61.2264 iter/s, 1.63328s/100 iter), loss = 0.00044311
I0814 19:32:55.814553 24732 solver.cpp:334]     Train net output #0: loss = 0.000442951 (* 1 = 0.000442951 loss)
I0814 19:32:55.814571 24732 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0814 19:32:57.441095 24732 solver.cpp:312] Iteration 29400 (61.4796 iter/s, 1.62655s/100 iter), loss = 0.000546513
I0814 19:32:57.441123 24732 solver.cpp:334]     Train net output #0: loss = 0.000546354 (* 1 = 0.000546354 loss)
I0814 19:32:57.441128 24732 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0814 19:32:59.082372 24732 solver.cpp:312] Iteration 29500 (60.93 iter/s, 1.64123s/100 iter), loss = 0.00222468
I0814 19:32:59.082433 24732 solver.cpp:334]     Train net output #0: loss = 0.00222452 (* 1 = 0.00222452 loss)
I0814 19:32:59.082451 24732 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0814 19:33:00.734133 24732 solver.cpp:312] Iteration 29600 (60.5434 iter/s, 1.65171s/100 iter), loss = 0.000151832
I0814 19:33:00.734179 24732 solver.cpp:334]     Train net output #0: loss = 0.000151673 (* 1 = 0.000151673 loss)
I0814 19:33:00.734190 24732 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0814 19:33:02.346547 24732 solver.cpp:312] Iteration 29700 (62.0206 iter/s, 1.61237s/100 iter), loss = 0.00547447
I0814 19:33:02.346688 24732 solver.cpp:334]     Train net output #0: loss = 0.00547431 (* 1 = 0.00547431 loss)
I0814 19:33:02.346707 24732 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0814 19:33:04.012025 24732 solver.cpp:312] Iteration 29800 (60.0447 iter/s, 1.66543s/100 iter), loss = 0.0014514
I0814 19:33:04.012053 24732 solver.cpp:334]     Train net output #0: loss = 0.00145124 (* 1 = 0.00145124 loss)
I0814 19:33:04.012058 24732 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0814 19:33:05.701334 24732 solver.cpp:312] Iteration 29900 (59.1976 iter/s, 1.68926s/100 iter), loss = 0.000373155
I0814 19:33:05.701357 24732 solver.cpp:334]     Train net output #0: loss = 0.000372997 (* 1 = 0.000372997 loss)
I0814 19:33:05.701362 24732 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0814 19:33:07.339547 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0814 19:33:07.347620 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0814 19:33:07.351246 24732 solver.cpp:363] Sparsity after update:
I0814 19:33:07.352985 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:33:07.352996 24732 net.cpp:2192] conv1a_param_0(0.246) 
I0814 19:33:07.353005 24732 net.cpp:2192] conv1b_param_0(0.514) 
I0814 19:33:07.353018 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:33:07.353029 24732 net.cpp:2192] res2a_branch2a_param_0(0.517) 
I0814 19:33:07.353034 24732 net.cpp:2192] res2a_branch2b_param_0(0.514) 
I0814 19:33:07.353036 24732 net.cpp:2192] res3a_branch2a_param_0(0.519) 
I0814 19:33:07.353052 24732 net.cpp:2192] res3a_branch2b_param_0(0.517) 
I0814 19:33:07.353061 24732 net.cpp:2192] res4a_branch2a_param_0(0.52) 
I0814 19:33:07.353065 24732 net.cpp:2192] res4a_branch2b_param_0(0.519) 
I0814 19:33:07.353068 24732 net.cpp:2192] res5a_branch2a_param_0(0.501) 
I0814 19:33:07.353076 24732 net.cpp:2192] res5a_branch2b_param_0(0.517) 
I0814 19:33:07.353081 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.19849e+06/2.3599e+06) 0.508
I0814 19:33:07.353096 24732 solver.cpp:509] Iteration 30000, Testing net (#0)
I0814 19:33:08.172219 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917354
I0814 19:33:08.172236 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:33:08.172241 24732 solver.cpp:594]     Test net output #2: loss = 0.326121 (* 1 = 0.326121 loss)
I0814 19:33:08.172255 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819133s
I0814 19:33:08.187853 24783 solver.cpp:409] Finding and applying sparsity: 0.54
I0814 19:33:28.976816 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:33:28.978943 24732 solver.cpp:312] Iteration 30000 (4.29609 iter/s, 23.277s/100 iter), loss = 0.00156051
I0814 19:33:28.978962 24732 solver.cpp:334]     Train net output #0: loss = 0.00156035 (* 1 = 0.00156035 loss)
I0814 19:33:28.978971 24732 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0814 19:33:30.748152 24732 solver.cpp:312] Iteration 30100 (56.5242 iter/s, 1.76915s/100 iter), loss = 0.00143317
I0814 19:33:30.748175 24732 solver.cpp:334]     Train net output #0: loss = 0.00143301 (* 1 = 0.00143301 loss)
I0814 19:33:30.748181 24732 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0814 19:33:32.415329 24732 solver.cpp:312] Iteration 30200 (59.9835 iter/s, 1.66713s/100 iter), loss = 0.000640238
I0814 19:33:32.415359 24732 solver.cpp:334]     Train net output #0: loss = 0.00064008 (* 1 = 0.00064008 loss)
I0814 19:33:32.415364 24732 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0814 19:33:34.121999 24732 solver.cpp:312] Iteration 30300 (58.5956 iter/s, 1.70661s/100 iter), loss = 0.000781193
I0814 19:33:34.122025 24732 solver.cpp:334]     Train net output #0: loss = 0.000781035 (* 1 = 0.000781035 loss)
I0814 19:33:34.122030 24732 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0814 19:33:35.799001 24732 solver.cpp:312] Iteration 30400 (59.6321 iter/s, 1.67695s/100 iter), loss = 0.000635232
I0814 19:33:35.799026 24732 solver.cpp:334]     Train net output #0: loss = 0.000635074 (* 1 = 0.000635074 loss)
I0814 19:33:35.799031 24732 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0814 19:33:37.440696 24732 solver.cpp:312] Iteration 30500 (60.9146 iter/s, 1.64164s/100 iter), loss = 0.00117349
I0814 19:33:37.440847 24732 solver.cpp:334]     Train net output #0: loss = 0.00117333 (* 1 = 0.00117333 loss)
I0814 19:33:37.440873 24732 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0814 19:33:39.088780 24732 solver.cpp:312] Iteration 30600 (60.6784 iter/s, 1.64803s/100 iter), loss = 0.000742603
I0814 19:33:39.088804 24732 solver.cpp:334]     Train net output #0: loss = 0.000742445 (* 1 = 0.000742445 loss)
I0814 19:33:39.088809 24732 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0814 19:33:40.750020 24732 solver.cpp:312] Iteration 30700 (60.1978 iter/s, 1.66119s/100 iter), loss = 0.000245756
I0814 19:33:40.750072 24732 solver.cpp:334]     Train net output #0: loss = 0.000245597 (* 1 = 0.000245597 loss)
I0814 19:33:40.750085 24732 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0814 19:33:42.384970 24732 solver.cpp:312] Iteration 30800 (61.166 iter/s, 1.6349s/100 iter), loss = 0.000856125
I0814 19:33:42.385193 24732 solver.cpp:334]     Train net output #0: loss = 0.000855968 (* 1 = 0.000855968 loss)
I0814 19:33:42.385255 24732 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0814 19:33:44.028551 24732 solver.cpp:312] Iteration 30900 (60.8447 iter/s, 1.64353s/100 iter), loss = 0.00266532
I0814 19:33:44.028573 24732 solver.cpp:334]     Train net output #0: loss = 0.00266516 (* 1 = 0.00266516 loss)
I0814 19:33:44.028579 24732 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0814 19:33:45.620663 24732 solver.cpp:363] Sparsity after update:
I0814 19:33:45.622337 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:33:45.622345 24732 net.cpp:2192] conv1a_param_0(0.249) 
I0814 19:33:45.622354 24732 net.cpp:2192] conv1b_param_0(0.528) 
I0814 19:33:45.622359 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:33:45.622362 24732 net.cpp:2192] res2a_branch2a_param_0(0.538) 
I0814 19:33:45.622365 24732 net.cpp:2192] res2a_branch2b_param_0(0.534) 
I0814 19:33:45.622369 24732 net.cpp:2192] res3a_branch2a_param_0(0.54) 
I0814 19:33:45.622371 24732 net.cpp:2192] res3a_branch2b_param_0(0.538) 
I0814 19:33:45.622375 24732 net.cpp:2192] res4a_branch2a_param_0(0.54) 
I0814 19:33:45.622378 24732 net.cpp:2192] res4a_branch2b_param_0(0.54) 
I0814 19:33:45.622381 24732 net.cpp:2192] res5a_branch2a_param_0(0.521) 
I0814 19:33:45.622385 24732 net.cpp:2192] res5a_branch2b_param_0(0.537) 
I0814 19:33:45.622387 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.24669e+06/2.3599e+06) 0.528
I0814 19:33:45.622416 24732 solver.cpp:509] Iteration 31000, Testing net (#0)
I0814 19:33:45.915220 24730 data_reader.cpp:288] Starting prefetch of epoch 4
I0814 19:33:46.445405 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919119
I0814 19:33:46.445423 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:33:46.445430 24732 solver.cpp:594]     Test net output #2: loss = 0.318284 (* 1 = 0.318284 loss)
I0814 19:33:46.445447 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.823002s
I0814 19:33:46.463084 24783 solver.cpp:409] Finding and applying sparsity: 0.56
I0814 19:34:07.834796 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:34:07.836855 24732 solver.cpp:312] Iteration 31000 (4.20033 iter/s, 23.8077s/100 iter), loss = 0.000863788
I0814 19:34:07.836879 24732 solver.cpp:334]     Train net output #0: loss = 0.000863629 (* 1 = 0.000863629 loss)
I0814 19:34:07.836889 24732 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0814 19:34:09.664777 24732 solver.cpp:312] Iteration 31100 (54.7086 iter/s, 1.82786s/100 iter), loss = 0.00257866
I0814 19:34:09.664839 24732 solver.cpp:334]     Train net output #0: loss = 0.0025785 (* 1 = 0.0025785 loss)
I0814 19:34:09.664860 24732 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0814 19:34:11.274726 24732 solver.cpp:312] Iteration 31200 (62.1158 iter/s, 1.6099s/100 iter), loss = 0.00112861
I0814 19:34:11.274751 24732 solver.cpp:334]     Train net output #0: loss = 0.00112845 (* 1 = 0.00112845 loss)
I0814 19:34:11.274756 24732 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0814 19:34:12.925189 24732 solver.cpp:312] Iteration 31300 (60.5909 iter/s, 1.65041s/100 iter), loss = 0.00224881
I0814 19:34:12.925213 24732 solver.cpp:334]     Train net output #0: loss = 0.00224865 (* 1 = 0.00224865 loss)
I0814 19:34:12.925218 24732 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0814 19:34:14.576901 24732 solver.cpp:312] Iteration 31400 (60.5451 iter/s, 1.65166s/100 iter), loss = 0.00220639
I0814 19:34:14.576966 24732 solver.cpp:334]     Train net output #0: loss = 0.00220623 (* 1 = 0.00220623 loss)
I0814 19:34:14.576982 24732 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0814 19:34:16.256436 24732 solver.cpp:312] Iteration 31500 (59.5421 iter/s, 1.67948s/100 iter), loss = 0.00083995
I0814 19:34:16.256464 24732 solver.cpp:334]     Train net output #0: loss = 0.00083979 (* 1 = 0.00083979 loss)
I0814 19:34:16.256470 24732 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0814 19:34:17.887924 24732 solver.cpp:312] Iteration 31600 (61.2957 iter/s, 1.63144s/100 iter), loss = 0.00205442
I0814 19:34:17.887975 24732 solver.cpp:334]     Train net output #0: loss = 0.00205426 (* 1 = 0.00205426 loss)
I0814 19:34:17.887990 24732 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0814 19:34:19.519552 24732 solver.cpp:312] Iteration 31700 (61.2905 iter/s, 1.63157s/100 iter), loss = 0.000185766
I0814 19:34:19.519578 24732 solver.cpp:334]     Train net output #0: loss = 0.000185607 (* 1 = 0.000185607 loss)
I0814 19:34:19.519584 24732 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0814 19:34:21.195189 24732 solver.cpp:312] Iteration 31800 (59.6806 iter/s, 1.67559s/100 iter), loss = 0.000254502
I0814 19:34:21.195214 24732 solver.cpp:334]     Train net output #0: loss = 0.000254342 (* 1 = 0.000254342 loss)
I0814 19:34:21.195219 24732 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0814 19:34:22.830309 24732 solver.cpp:312] Iteration 31900 (61.1595 iter/s, 1.63507s/100 iter), loss = 0.000240022
I0814 19:34:22.830334 24732 solver.cpp:334]     Train net output #0: loss = 0.000239862 (* 1 = 0.000239862 loss)
I0814 19:34:22.830340 24732 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0814 19:34:24.446499 24732 solver.cpp:363] Sparsity after update:
I0814 19:34:24.448176 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:34:24.448185 24732 net.cpp:2192] conv1a_param_0(0.264) 
I0814 19:34:24.448191 24732 net.cpp:2192] conv1b_param_0(0.554) 
I0814 19:34:24.448194 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:34:24.448196 24732 net.cpp:2192] res2a_branch2a_param_0(0.559) 
I0814 19:34:24.448199 24732 net.cpp:2192] res2a_branch2b_param_0(0.554) 
I0814 19:34:24.448202 24732 net.cpp:2192] res3a_branch2a_param_0(0.559) 
I0814 19:34:24.448205 24732 net.cpp:2192] res3a_branch2b_param_0(0.559) 
I0814 19:34:24.448209 24732 net.cpp:2192] res4a_branch2a_param_0(0.56) 
I0814 19:34:24.448211 24732 net.cpp:2192] res4a_branch2b_param_0(0.559) 
I0814 19:34:24.448215 24732 net.cpp:2192] res5a_branch2a_param_0(0.54) 
I0814 19:34:24.448217 24732 net.cpp:2192] res5a_branch2b_param_0(0.557) 
I0814 19:34:24.448222 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.29177e+06/2.3599e+06) 0.547
I0814 19:34:24.448246 24732 solver.cpp:509] Iteration 32000, Testing net (#0)
I0814 19:34:25.271944 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.91206
I0814 19:34:25.271962 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:34:25.271970 24732 solver.cpp:594]     Test net output #2: loss = 0.350543 (* 1 = 0.350543 loss)
I0814 19:34:25.271987 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.823713s
I0814 19:34:25.287766 24783 solver.cpp:409] Finding and applying sparsity: 0.58
I0814 19:34:47.480098 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:34:47.482236 24732 solver.cpp:312] Iteration 32000 (4.05659 iter/s, 24.6512s/100 iter), loss = 0.00222368
I0814 19:34:47.482256 24732 solver.cpp:334]     Train net output #0: loss = 0.00222352 (* 1 = 0.00222352 loss)
I0814 19:34:47.482264 24732 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0814 19:34:49.316520 24732 solver.cpp:312] Iteration 32100 (54.519 iter/s, 1.83422s/100 iter), loss = 0.00046678
I0814 19:34:49.316545 24732 solver.cpp:334]     Train net output #0: loss = 0.000466619 (* 1 = 0.000466619 loss)
I0814 19:34:49.316550 24732 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0814 19:34:50.969218 24732 solver.cpp:312] Iteration 32200 (60.509 iter/s, 1.65265s/100 iter), loss = 0.00314207
I0814 19:34:50.969244 24732 solver.cpp:334]     Train net output #0: loss = 0.00314191 (* 1 = 0.00314191 loss)
I0814 19:34:50.969249 24732 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0814 19:34:52.608822 24732 solver.cpp:312] Iteration 32300 (60.9922 iter/s, 1.63955s/100 iter), loss = 0.00190914
I0814 19:34:52.608883 24732 solver.cpp:334]     Train net output #0: loss = 0.00190898 (* 1 = 0.00190898 loss)
I0814 19:34:52.608898 24732 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0814 19:34:54.272204 24732 solver.cpp:312] Iteration 32400 (60.1204 iter/s, 1.66333s/100 iter), loss = 0.000523257
I0814 19:34:54.272229 24732 solver.cpp:334]     Train net output #0: loss = 0.000523098 (* 1 = 0.000523098 loss)
I0814 19:34:54.272235 24732 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0814 19:34:55.918228 24732 solver.cpp:312] Iteration 32500 (60.7543 iter/s, 1.64597s/100 iter), loss = 0.00119819
I0814 19:34:55.918256 24732 solver.cpp:334]     Train net output #0: loss = 0.00119803 (* 1 = 0.00119803 loss)
I0814 19:34:55.918262 24732 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0814 19:34:57.557601 24732 solver.cpp:312] Iteration 32600 (61.0009 iter/s, 1.63932s/100 iter), loss = 0.00219254
I0814 19:34:57.557627 24732 solver.cpp:334]     Train net output #0: loss = 0.00219238 (* 1 = 0.00219238 loss)
I0814 19:34:57.557632 24732 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0814 19:34:59.177565 24732 solver.cpp:312] Iteration 32700 (61.7317 iter/s, 1.61991s/100 iter), loss = 0.000100954
I0814 19:34:59.177633 24732 solver.cpp:334]     Train net output #0: loss = 0.000100796 (* 1 = 0.000100796 loss)
I0814 19:34:59.177650 24732 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0814 19:35:00.829152 24732 solver.cpp:312] Iteration 32800 (60.5497 iter/s, 1.65154s/100 iter), loss = 0.000854233
I0814 19:35:00.829175 24732 solver.cpp:334]     Train net output #0: loss = 0.000854073 (* 1 = 0.000854073 loss)
I0814 19:35:00.829180 24732 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0814 19:35:02.529527 24732 solver.cpp:312] Iteration 32900 (58.8124 iter/s, 1.70032s/100 iter), loss = 0.000936709
I0814 19:35:02.529587 24732 solver.cpp:334]     Train net output #0: loss = 0.00093655 (* 1 = 0.00093655 loss)
I0814 19:35:02.529603 24732 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0814 19:35:04.150136 24732 solver.cpp:363] Sparsity after update:
I0814 19:35:04.151659 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:35:04.151669 24732 net.cpp:2192] conv1a_param_0(0.263) 
I0814 19:35:04.151677 24732 net.cpp:2192] conv1b_param_0(0.567) 
I0814 19:35:04.151690 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:35:04.151700 24732 net.cpp:2192] res2a_branch2a_param_0(0.58) 
I0814 19:35:04.151708 24732 net.cpp:2192] res2a_branch2b_param_0(0.574) 
I0814 19:35:04.151716 24732 net.cpp:2192] res3a_branch2a_param_0(0.58) 
I0814 19:35:04.151724 24732 net.cpp:2192] res3a_branch2b_param_0(0.58) 
I0814 19:35:04.151733 24732 net.cpp:2192] res4a_branch2a_param_0(0.58) 
I0814 19:35:04.151742 24732 net.cpp:2192] res4a_branch2b_param_0(0.58) 
I0814 19:35:04.151751 24732 net.cpp:2192] res5a_branch2a_param_0(0.56) 
I0814 19:35:04.151760 24732 net.cpp:2192] res5a_branch2b_param_0(0.577) 
I0814 19:35:04.151769 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.33914e+06/2.3599e+06) 0.567
I0814 19:35:04.151805 24732 solver.cpp:509] Iteration 33000, Testing net (#0)
I0814 19:35:04.964606 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909707
I0814 19:35:04.964623 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:35:04.964628 24732 solver.cpp:594]     Test net output #2: loss = 0.345967 (* 1 = 0.345967 loss)
I0814 19:35:04.964645 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.812813s
I0814 19:35:04.980237 24783 solver.cpp:409] Finding and applying sparsity: 0.6
I0814 19:35:28.398360 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:35:28.400420 24732 solver.cpp:312] Iteration 33000 (3.86545 iter/s, 25.8702s/100 iter), loss = 0.00117594
I0814 19:35:28.400439 24732 solver.cpp:334]     Train net output #0: loss = 0.00117578 (* 1 = 0.00117578 loss)
I0814 19:35:28.400445 24732 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0814 19:35:30.310865 24732 solver.cpp:312] Iteration 33100 (52.3455 iter/s, 1.91038s/100 iter), loss = 0.0109256
I0814 19:35:30.310935 24732 solver.cpp:334]     Train net output #0: loss = 0.0109255 (* 1 = 0.0109255 loss)
I0814 19:35:30.310955 24732 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0814 19:35:31.964501 24732 solver.cpp:312] Iteration 33200 (60.4746 iter/s, 1.65359s/100 iter), loss = 0.000171514
I0814 19:35:31.964548 24732 solver.cpp:334]     Train net output #0: loss = 0.000171357 (* 1 = 0.000171357 loss)
I0814 19:35:31.964560 24732 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0814 19:35:33.581265 24732 solver.cpp:312] Iteration 33300 (61.854 iter/s, 1.61671s/100 iter), loss = 0.00197934
I0814 19:35:33.581290 24732 solver.cpp:334]     Train net output #0: loss = 0.00197918 (* 1 = 0.00197918 loss)
I0814 19:35:33.581295 24732 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0814 19:35:35.269011 24732 solver.cpp:312] Iteration 33400 (59.2525 iter/s, 1.68769s/100 iter), loss = 0.000788773
I0814 19:35:35.269038 24732 solver.cpp:334]     Train net output #0: loss = 0.000788615 (* 1 = 0.000788615 loss)
I0814 19:35:35.269045 24732 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0814 19:35:36.948596 24732 solver.cpp:312] Iteration 33500 (59.5404 iter/s, 1.67953s/100 iter), loss = 0.00603715
I0814 19:35:36.948642 24732 solver.cpp:334]     Train net output #0: loss = 0.00603699 (* 1 = 0.00603699 loss)
I0814 19:35:36.948653 24732 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0814 19:35:38.611431 24732 solver.cpp:312] Iteration 33600 (60.1401 iter/s, 1.66278s/100 iter), loss = 0.000277507
I0814 19:35:38.611456 24732 solver.cpp:334]     Train net output #0: loss = 0.000277348 (* 1 = 0.000277348 loss)
I0814 19:35:38.611462 24732 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0814 19:35:40.274881 24732 solver.cpp:312] Iteration 33700 (60.1179 iter/s, 1.6634s/100 iter), loss = 0.000240681
I0814 19:35:40.274905 24732 solver.cpp:334]     Train net output #0: loss = 0.000240522 (* 1 = 0.000240522 loss)
I0814 19:35:40.274911 24732 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0814 19:35:41.931851 24732 solver.cpp:312] Iteration 33800 (60.353 iter/s, 1.65692s/100 iter), loss = 0.00152685
I0814 19:35:41.931872 24732 solver.cpp:334]     Train net output #0: loss = 0.00152669 (* 1 = 0.00152669 loss)
I0814 19:35:41.931876 24732 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0814 19:35:43.592573 24732 solver.cpp:312] Iteration 33900 (60.2167 iter/s, 1.66067s/100 iter), loss = 0.00136296
I0814 19:35:43.592603 24732 solver.cpp:334]     Train net output #0: loss = 0.0013628 (* 1 = 0.0013628 loss)
I0814 19:35:43.592610 24732 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0814 19:35:45.236728 24732 solver.cpp:363] Sparsity after update:
I0814 19:35:45.238250 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:35:45.238260 24732 net.cpp:2192] conv1a_param_0(0.277) 
I0814 19:35:45.238277 24732 net.cpp:2192] conv1b_param_0(0.592) 
I0814 19:35:45.238287 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:35:45.238297 24732 net.cpp:2192] res2a_branch2a_param_0(0.597) 
I0814 19:35:45.238306 24732 net.cpp:2192] res2a_branch2b_param_0(0.594) 
I0814 19:35:45.238313 24732 net.cpp:2192] res3a_branch2a_param_0(0.599) 
I0814 19:35:45.238322 24732 net.cpp:2192] res3a_branch2b_param_0(0.597) 
I0814 19:35:45.238330 24732 net.cpp:2192] res4a_branch2a_param_0(0.6) 
I0814 19:35:45.238339 24732 net.cpp:2192] res4a_branch2b_param_0(0.599) 
I0814 19:35:45.238348 24732 net.cpp:2192] res5a_branch2a_param_0(0.576) 
I0814 19:35:45.238358 24732 net.cpp:2192] res5a_branch2b_param_0(0.598) 
I0814 19:35:45.238368 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.38182e+06/2.3599e+06) 0.586
I0814 19:35:45.238400 24732 solver.cpp:509] Iteration 34000, Testing net (#0)
I0814 19:35:46.046381 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.904119
I0814 19:35:46.046397 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:35:46.046402 24732 solver.cpp:594]     Test net output #2: loss = 0.364571 (* 1 = 0.364571 loss)
I0814 19:35:46.046421 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.807992s
I0814 19:35:46.062113 24783 solver.cpp:409] Finding and applying sparsity: 0.62
I0814 19:36:10.650403 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:36:10.652444 24732 solver.cpp:312] Iteration 34000 (3.69561 iter/s, 27.0591s/100 iter), loss = 0.000525296
I0814 19:36:10.652468 24732 solver.cpp:334]     Train net output #0: loss = 0.000525138 (* 1 = 0.000525138 loss)
I0814 19:36:10.652477 24732 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0814 19:36:12.516901 24732 solver.cpp:312] Iteration 34100 (53.6366 iter/s, 1.8644s/100 iter), loss = 0.00103679
I0814 19:36:12.516927 24732 solver.cpp:334]     Train net output #0: loss = 0.00103663 (* 1 = 0.00103663 loss)
I0814 19:36:12.516930 24732 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0814 19:36:14.194895 24732 solver.cpp:312] Iteration 34200 (59.5969 iter/s, 1.67794s/100 iter), loss = 0.00256843
I0814 19:36:14.195042 24732 solver.cpp:334]     Train net output #0: loss = 0.00256827 (* 1 = 0.00256827 loss)
I0814 19:36:14.195063 24732 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0814 19:36:15.860190 24732 solver.cpp:312] Iteration 34300 (60.0513 iter/s, 1.66524s/100 iter), loss = 0.000546362
I0814 19:36:15.860245 24732 solver.cpp:334]     Train net output #0: loss = 0.000546202 (* 1 = 0.000546202 loss)
I0814 19:36:15.860262 24732 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0814 19:36:17.544739 24732 solver.cpp:312] Iteration 34400 (59.365 iter/s, 1.68449s/100 iter), loss = 0.000945464
I0814 19:36:17.544764 24732 solver.cpp:334]     Train net output #0: loss = 0.000945305 (* 1 = 0.000945305 loss)
I0814 19:36:17.544770 24732 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0814 19:36:19.200068 24732 solver.cpp:312] Iteration 34500 (60.4128 iter/s, 1.65528s/100 iter), loss = 0.000312246
I0814 19:36:19.200090 24732 solver.cpp:334]     Train net output #0: loss = 0.000312088 (* 1 = 0.000312088 loss)
I0814 19:36:19.200095 24732 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0814 19:36:20.797149 24732 solver.cpp:312] Iteration 34600 (62.6162 iter/s, 1.59703s/100 iter), loss = 0.00193179
I0814 19:36:20.797174 24732 solver.cpp:334]     Train net output #0: loss = 0.00193164 (* 1 = 0.00193164 loss)
I0814 19:36:20.797180 24732 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0814 19:36:22.453724 24732 solver.cpp:312] Iteration 34700 (60.3674 iter/s, 1.65652s/100 iter), loss = 0.00554511
I0814 19:36:22.453748 24732 solver.cpp:334]     Train net output #0: loss = 0.00554495 (* 1 = 0.00554495 loss)
I0814 19:36:22.453754 24732 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0814 19:36:24.140456 24732 solver.cpp:312] Iteration 34800 (59.288 iter/s, 1.68668s/100 iter), loss = 0.00187929
I0814 19:36:24.140506 24732 solver.cpp:334]     Train net output #0: loss = 0.00187914 (* 1 = 0.00187914 loss)
I0814 19:36:24.140519 24732 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0814 19:36:25.770324 24732 solver.cpp:312] Iteration 34900 (61.3566 iter/s, 1.62982s/100 iter), loss = 0.000575335
I0814 19:36:25.770494 24732 solver.cpp:334]     Train net output #0: loss = 0.000575177 (* 1 = 0.000575177 loss)
I0814 19:36:25.770584 24732 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0814 19:36:27.370738 24732 solver.cpp:363] Sparsity after update:
I0814 19:36:27.372261 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:36:27.372269 24732 net.cpp:2192] conv1a_param_0(0.289) 
I0814 19:36:27.372277 24732 net.cpp:2192] conv1b_param_0(0.605) 
I0814 19:36:27.372282 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:36:27.372292 24732 net.cpp:2192] res2a_branch2a_param_0(0.618) 
I0814 19:36:27.372301 24732 net.cpp:2192] res2a_branch2b_param_0(0.611) 
I0814 19:36:27.372310 24732 net.cpp:2192] res3a_branch2a_param_0(0.62) 
I0814 19:36:27.372319 24732 net.cpp:2192] res3a_branch2b_param_0(0.618) 
I0814 19:36:27.372328 24732 net.cpp:2192] res4a_branch2a_param_0(0.62) 
I0814 19:36:27.372335 24732 net.cpp:2192] res4a_branch2b_param_0(0.62) 
I0814 19:36:27.372344 24732 net.cpp:2192] res5a_branch2a_param_0(0.597) 
I0814 19:36:27.372354 24732 net.cpp:2192] res5a_branch2b_param_0(0.618) 
I0814 19:36:27.372362 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.43065e+06/2.3599e+06) 0.606
I0814 19:36:27.372397 24732 solver.cpp:509] Iteration 35000, Testing net (#0)
I0814 19:36:27.597641 24730 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:36:28.202903 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:36:28.202921 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996765
I0814 19:36:28.202929 24732 solver.cpp:594]     Test net output #2: loss = 0.366422 (* 1 = 0.366422 loss)
I0814 19:36:28.202945 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83052s
I0814 19:36:28.219471 24783 solver.cpp:409] Finding and applying sparsity: 0.64
I0814 19:36:54.335464 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:36:54.337527 24732 solver.cpp:312] Iteration 35000 (3.50061 iter/s, 28.5664s/100 iter), loss = 0.0012337
I0814 19:36:54.337551 24732 solver.cpp:334]     Train net output #0: loss = 0.00123354 (* 1 = 0.00123354 loss)
I0814 19:36:54.337560 24732 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0814 19:36:56.216511 24732 solver.cpp:312] Iteration 35100 (53.222 iter/s, 1.87892s/100 iter), loss = 0.00221305
I0814 19:36:56.216540 24732 solver.cpp:334]     Train net output #0: loss = 0.00221289 (* 1 = 0.00221289 loss)
I0814 19:36:56.216547 24732 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0814 19:36:57.861150 24732 solver.cpp:312] Iteration 35200 (60.8055 iter/s, 1.64459s/100 iter), loss = 0.00102358
I0814 19:36:57.861173 24732 solver.cpp:334]     Train net output #0: loss = 0.00102342 (* 1 = 0.00102342 loss)
I0814 19:36:57.861177 24732 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0814 19:36:59.494670 24732 solver.cpp:312] Iteration 35300 (61.2193 iter/s, 1.63347s/100 iter), loss = 0.00127552
I0814 19:36:59.494699 24732 solver.cpp:334]     Train net output #0: loss = 0.00127536 (* 1 = 0.00127536 loss)
I0814 19:36:59.494705 24732 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0814 19:37:01.170147 24732 solver.cpp:312] Iteration 35400 (59.6864 iter/s, 1.67542s/100 iter), loss = 0.000261689
I0814 19:37:01.170171 24732 solver.cpp:334]     Train net output #0: loss = 0.000261529 (* 1 = 0.000261529 loss)
I0814 19:37:01.170176 24732 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0814 19:37:02.793546 24732 solver.cpp:312] Iteration 35500 (61.6011 iter/s, 1.62335s/100 iter), loss = 0.00285857
I0814 19:37:02.793614 24732 solver.cpp:334]     Train net output #0: loss = 0.00285841 (* 1 = 0.00285841 loss)
I0814 19:37:02.793637 24732 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0814 19:37:04.465471 24732 solver.cpp:312] Iteration 35600 (59.8132 iter/s, 1.67187s/100 iter), loss = 0.00262108
I0814 19:37:04.465497 24732 solver.cpp:334]     Train net output #0: loss = 0.00262092 (* 1 = 0.00262092 loss)
I0814 19:37:04.465503 24732 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0814 19:37:06.116808 24732 solver.cpp:312] Iteration 35700 (60.5589 iter/s, 1.65129s/100 iter), loss = 0.00110272
I0814 19:37:06.116832 24732 solver.cpp:334]     Train net output #0: loss = 0.00110256 (* 1 = 0.00110256 loss)
I0814 19:37:06.116837 24732 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0814 19:37:07.748167 24732 solver.cpp:312] Iteration 35800 (61.3006 iter/s, 1.63131s/100 iter), loss = 0.00396452
I0814 19:37:07.748193 24732 solver.cpp:334]     Train net output #0: loss = 0.00396436 (* 1 = 0.00396436 loss)
I0814 19:37:07.748198 24732 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0814 19:37:09.403978 24732 solver.cpp:312] Iteration 35900 (60.3952 iter/s, 1.65576s/100 iter), loss = 0.000975836
I0814 19:37:09.404008 24732 solver.cpp:334]     Train net output #0: loss = 0.000975673 (* 1 = 0.000975673 loss)
I0814 19:37:09.404016 24732 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0814 19:37:11.095005 24732 solver.cpp:363] Sparsity after update:
I0814 19:37:11.096905 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:37:11.096916 24732 net.cpp:2192] conv1a_param_0(0.29) 
I0814 19:37:11.096926 24732 net.cpp:2192] conv1b_param_0(0.629) 
I0814 19:37:11.096931 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:37:11.096936 24732 net.cpp:2192] res2a_branch2a_param_0(0.639) 
I0814 19:37:11.096940 24732 net.cpp:2192] res2a_branch2b_param_0(0.627) 
I0814 19:37:11.096945 24732 net.cpp:2192] res3a_branch2a_param_0(0.639) 
I0814 19:37:11.096948 24732 net.cpp:2192] res3a_branch2b_param_0(0.638) 
I0814 19:37:11.096952 24732 net.cpp:2192] res4a_branch2a_param_0(0.64) 
I0814 19:37:11.096956 24732 net.cpp:2192] res4a_branch2b_param_0(0.639) 
I0814 19:37:11.096961 24732 net.cpp:2192] res5a_branch2a_param_0(0.611) 
I0814 19:37:11.096964 24732 net.cpp:2192] res5a_branch2b_param_0(0.638) 
I0814 19:37:11.096968 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.46989e+06/2.3599e+06) 0.623
I0814 19:37:11.096997 24732 solver.cpp:509] Iteration 36000, Testing net (#0)
I0814 19:37:11.915361 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909707
I0814 19:37:11.915385 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:37:11.915391 24732 solver.cpp:594]     Test net output #2: loss = 0.359379 (* 1 = 0.359379 loss)
I0814 19:37:11.915416 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.818391s
I0814 19:37:11.934762 24783 solver.cpp:409] Finding and applying sparsity: 0.66
I0814 19:37:39.371173 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:37:39.373263 24732 solver.cpp:312] Iteration 36000 (3.33684 iter/s, 29.9685s/100 iter), loss = 0.00183845
I0814 19:37:39.373287 24732 solver.cpp:334]     Train net output #0: loss = 0.00183828 (* 1 = 0.00183828 loss)
I0814 19:37:39.373296 24732 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0814 19:37:41.242388 24732 solver.cpp:312] Iteration 36100 (53.5026 iter/s, 1.86907s/100 iter), loss = 0.000688174
I0814 19:37:41.242441 24732 solver.cpp:334]     Train net output #0: loss = 0.00068801 (* 1 = 0.00068801 loss)
I0814 19:37:41.242456 24732 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0814 19:37:42.852712 24732 solver.cpp:312] Iteration 36200 (62.1012 iter/s, 1.61028s/100 iter), loss = 0.000879698
I0814 19:37:42.852761 24732 solver.cpp:334]     Train net output #0: loss = 0.000879534 (* 1 = 0.000879534 loss)
I0814 19:37:42.852774 24732 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0814 19:37:44.544373 24732 solver.cpp:312] Iteration 36300 (59.1154 iter/s, 1.69161s/100 iter), loss = 0.000450227
I0814 19:37:44.544422 24732 solver.cpp:334]     Train net output #0: loss = 0.000450061 (* 1 = 0.000450061 loss)
I0814 19:37:44.544436 24732 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0814 19:37:46.228101 24732 solver.cpp:312] Iteration 36400 (59.3939 iter/s, 1.68367s/100 iter), loss = 0.00354766
I0814 19:37:46.228124 24732 solver.cpp:334]     Train net output #0: loss = 0.00354749 (* 1 = 0.00354749 loss)
I0814 19:37:46.228134 24732 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0814 19:37:47.889587 24732 solver.cpp:312] Iteration 36500 (60.1889 iter/s, 1.66143s/100 iter), loss = 0.00163231
I0814 19:37:47.889612 24732 solver.cpp:334]     Train net output #0: loss = 0.00163215 (* 1 = 0.00163215 loss)
I0814 19:37:47.889617 24732 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0814 19:37:49.516026 24732 solver.cpp:312] Iteration 36600 (61.486 iter/s, 1.62639s/100 iter), loss = 0.00295144
I0814 19:37:49.516095 24732 solver.cpp:334]     Train net output #0: loss = 0.00295127 (* 1 = 0.00295127 loss)
I0814 19:37:49.516119 24732 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0814 19:37:51.159335 24732 solver.cpp:312] Iteration 36700 (60.8627 iter/s, 1.64304s/100 iter), loss = 0.00350237
I0814 19:37:51.159359 24732 solver.cpp:334]     Train net output #0: loss = 0.0035022 (* 1 = 0.0035022 loss)
I0814 19:37:51.159363 24732 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0814 19:37:52.784338 24732 solver.cpp:312] Iteration 36800 (61.5403 iter/s, 1.62495s/100 iter), loss = 0.00142127
I0814 19:37:52.784386 24732 solver.cpp:334]     Train net output #0: loss = 0.00142109 (* 1 = 0.00142109 loss)
I0814 19:37:52.784399 24732 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0814 19:37:54.423712 24732 solver.cpp:312] Iteration 36900 (61.0008 iter/s, 1.63932s/100 iter), loss = 0.000529255
I0814 19:37:54.423760 24732 solver.cpp:334]     Train net output #0: loss = 0.000529084 (* 1 = 0.000529084 loss)
I0814 19:37:54.423773 24732 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0814 19:37:56.043133 24732 solver.cpp:363] Sparsity after update:
I0814 19:37:56.045537 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:37:56.045550 24732 net.cpp:2192] conv1a_param_0(0.303) 
I0814 19:37:56.045557 24732 net.cpp:2192] conv1b_param_0(0.639) 
I0814 19:37:56.045563 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:37:56.045567 24732 net.cpp:2192] res2a_branch2a_param_0(0.66) 
I0814 19:37:56.045572 24732 net.cpp:2192] res2a_branch2b_param_0(0.643) 
I0814 19:37:56.045577 24732 net.cpp:2192] res3a_branch2a_param_0(0.66) 
I0814 19:37:56.045580 24732 net.cpp:2192] res3a_branch2b_param_0(0.657) 
I0814 19:37:56.045584 24732 net.cpp:2192] res4a_branch2a_param_0(0.66) 
I0814 19:37:56.045588 24732 net.cpp:2192] res4a_branch2b_param_0(0.66) 
I0814 19:37:56.045593 24732 net.cpp:2192] res5a_branch2a_param_0(0.635) 
I0814 19:37:56.045598 24732 net.cpp:2192] res5a_branch2b_param_0(0.659) 
I0814 19:37:56.045600 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.52254e+06/2.3599e+06) 0.645
I0814 19:37:56.045627 24732 solver.cpp:509] Iteration 37000, Testing net (#0)
I0814 19:37:56.858752 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.909413
I0814 19:37:56.858769 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:37:56.858774 24732 solver.cpp:594]     Test net output #2: loss = 0.373866 (* 1 = 0.373866 loss)
I0814 19:37:56.858790 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813135s
I0814 19:37:56.880726 24783 solver.cpp:409] Finding and applying sparsity: 0.68
I0814 19:38:25.348343 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:38:25.350414 24732 solver.cpp:312] Iteration 37000 (3.23354 iter/s, 30.9259s/100 iter), loss = 0.00039917
I0814 19:38:25.350431 24732 solver.cpp:334]     Train net output #0: loss = 0.000398996 (* 1 = 0.000398996 loss)
I0814 19:38:25.350436 24732 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0814 19:38:27.172596 24732 solver.cpp:312] Iteration 37100 (54.8811 iter/s, 1.82212s/100 iter), loss = 0.000275406
I0814 19:38:27.172621 24732 solver.cpp:334]     Train net output #0: loss = 0.000275228 (* 1 = 0.000275228 loss)
I0814 19:38:27.172626 24732 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0814 19:38:28.847478 24732 solver.cpp:312] Iteration 37200 (59.7075 iter/s, 1.67483s/100 iter), loss = 0.000402289
I0814 19:38:28.847509 24732 solver.cpp:334]     Train net output #0: loss = 0.000402112 (* 1 = 0.000402112 loss)
I0814 19:38:28.847515 24732 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0814 19:38:30.463680 24732 solver.cpp:312] Iteration 37300 (61.8755 iter/s, 1.61615s/100 iter), loss = 0.000458645
I0814 19:38:30.463703 24732 solver.cpp:334]     Train net output #0: loss = 0.000458469 (* 1 = 0.000458469 loss)
I0814 19:38:30.463709 24732 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0814 19:38:32.105969 24732 solver.cpp:312] Iteration 37400 (60.8924 iter/s, 1.64224s/100 iter), loss = 0.00144599
I0814 19:38:32.106019 24732 solver.cpp:334]     Train net output #0: loss = 0.00144582 (* 1 = 0.00144582 loss)
I0814 19:38:32.106031 24732 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0814 19:38:33.770913 24732 solver.cpp:312] Iteration 37500 (60.064 iter/s, 1.66489s/100 iter), loss = 0.00184216
I0814 19:38:33.770969 24732 solver.cpp:334]     Train net output #0: loss = 0.00184199 (* 1 = 0.00184199 loss)
I0814 19:38:33.770984 24732 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0814 19:38:35.419451 24732 solver.cpp:312] Iteration 37600 (60.6617 iter/s, 1.64849s/100 iter), loss = 0.00120978
I0814 19:38:35.419477 24732 solver.cpp:334]     Train net output #0: loss = 0.00120961 (* 1 = 0.00120961 loss)
I0814 19:38:35.419482 24732 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0814 19:38:37.041492 24732 solver.cpp:312] Iteration 37700 (61.6527 iter/s, 1.62199s/100 iter), loss = 0.00109412
I0814 19:38:37.041522 24732 solver.cpp:334]     Train net output #0: loss = 0.00109394 (* 1 = 0.00109394 loss)
I0814 19:38:37.041527 24732 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0814 19:38:38.680531 24732 solver.cpp:312] Iteration 37800 (61.0132 iter/s, 1.63899s/100 iter), loss = 0.002531
I0814 19:38:38.680558 24732 solver.cpp:334]     Train net output #0: loss = 0.00253082 (* 1 = 0.00253082 loss)
I0814 19:38:38.680565 24732 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0814 19:38:40.343515 24732 solver.cpp:312] Iteration 37900 (60.1347 iter/s, 1.66293s/100 iter), loss = 0.00168532
I0814 19:38:40.343749 24732 solver.cpp:334]     Train net output #0: loss = 0.00168514 (* 1 = 0.00168514 loss)
I0814 19:38:40.343755 24732 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0814 19:38:41.974885 24732 solver.cpp:363] Sparsity after update:
I0814 19:38:41.976487 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:38:41.976496 24732 net.cpp:2192] conv1a_param_0(0.315) 
I0814 19:38:41.976505 24732 net.cpp:2192] conv1b_param_0(0.65) 
I0814 19:38:41.976508 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:38:41.976511 24732 net.cpp:2192] res2a_branch2a_param_0(0.677) 
I0814 19:38:41.976516 24732 net.cpp:2192] res2a_branch2b_param_0(0.652) 
I0814 19:38:41.976519 24732 net.cpp:2192] res3a_branch2a_param_0(0.679) 
I0814 19:38:41.976523 24732 net.cpp:2192] res3a_branch2b_param_0(0.673) 
I0814 19:38:41.976528 24732 net.cpp:2192] res4a_branch2a_param_0(0.68) 
I0814 19:38:41.976532 24732 net.cpp:2192] res4a_branch2b_param_0(0.679) 
I0814 19:38:41.976536 24732 net.cpp:2192] res5a_branch2a_param_0(0.653) 
I0814 19:38:41.976541 24732 net.cpp:2192] res5a_branch2b_param_0(0.679) 
I0814 19:38:41.976544 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.56648e+06/2.3599e+06) 0.664
I0814 19:38:41.976565 24732 solver.cpp:509] Iteration 38000, Testing net (#0)
I0814 19:38:42.783870 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:38:42.783890 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:38:42.783896 24732 solver.cpp:594]     Test net output #2: loss = 0.392221 (* 1 = 0.392221 loss)
I0814 19:38:42.783912 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.807321s
I0814 19:38:42.799494 24783 solver.cpp:409] Finding and applying sparsity: 0.7
I0814 19:39:13.278408 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:39:13.280521 24732 solver.cpp:312] Iteration 38000 (3.03618 iter/s, 32.9361s/100 iter), loss = 0.00240681
I0814 19:39:13.280541 24732 solver.cpp:334]     Train net output #0: loss = 0.00240663 (* 1 = 0.00240663 loss)
I0814 19:39:13.280549 24732 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0814 19:39:15.175199 24732 solver.cpp:312] Iteration 38100 (52.7811 iter/s, 1.89462s/100 iter), loss = 0.00119895
I0814 19:39:15.175249 24732 solver.cpp:334]     Train net output #0: loss = 0.00119877 (* 1 = 0.00119877 loss)
I0814 19:39:15.175263 24732 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0814 19:39:16.820976 24732 solver.cpp:312] Iteration 38200 (60.7635 iter/s, 1.64573s/100 iter), loss = 0.00291007
I0814 19:39:16.821024 24732 solver.cpp:334]     Train net output #0: loss = 0.00290989 (* 1 = 0.00290989 loss)
I0814 19:39:16.821038 24732 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0814 19:39:18.445725 24732 solver.cpp:312] Iteration 38300 (61.55 iter/s, 1.62469s/100 iter), loss = 0.00115298
I0814 19:39:18.445801 24732 solver.cpp:334]     Train net output #0: loss = 0.00115279 (* 1 = 0.00115279 loss)
I0814 19:39:18.445816 24732 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0814 19:39:20.054780 24732 solver.cpp:312] Iteration 38400 (62.1502 iter/s, 1.609s/100 iter), loss = 0.000374273
I0814 19:39:20.054846 24732 solver.cpp:334]     Train net output #0: loss = 0.000374088 (* 1 = 0.000374088 loss)
I0814 19:39:20.054863 24732 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0814 19:39:21.668599 24732 solver.cpp:312] Iteration 38500 (61.9668 iter/s, 1.61377s/100 iter), loss = 0.00165111
I0814 19:39:21.668659 24732 solver.cpp:334]     Train net output #0: loss = 0.00165093 (* 1 = 0.00165093 loss)
I0814 19:39:21.668678 24732 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0814 19:39:23.308624 24732 solver.cpp:312] Iteration 38600 (60.9766 iter/s, 1.63997s/100 iter), loss = 0.000540637
I0814 19:39:23.308686 24732 solver.cpp:334]     Train net output #0: loss = 0.000540447 (* 1 = 0.000540447 loss)
I0814 19:39:23.308704 24732 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0814 19:39:24.972779 24732 solver.cpp:312] Iteration 38700 (60.0924 iter/s, 1.6641s/100 iter), loss = 0.000831069
I0814 19:39:24.972806 24732 solver.cpp:334]     Train net output #0: loss = 0.000830878 (* 1 = 0.000830878 loss)
I0814 19:39:24.972813 24732 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0814 19:39:26.632979 24732 solver.cpp:312] Iteration 38800 (60.2357 iter/s, 1.66015s/100 iter), loss = 0.000754916
I0814 19:39:26.633050 24732 solver.cpp:334]     Train net output #0: loss = 0.000754726 (* 1 = 0.000754726 loss)
I0814 19:39:26.633069 24732 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0814 19:39:28.262464 24732 solver.cpp:312] Iteration 38900 (61.371 iter/s, 1.62943s/100 iter), loss = 0.00249735
I0814 19:39:28.262488 24732 solver.cpp:334]     Train net output #0: loss = 0.00249716 (* 1 = 0.00249716 loss)
I0814 19:39:28.262495 24732 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0814 19:39:29.875116 24732 solver.cpp:363] Sparsity after update:
I0814 19:39:29.876773 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:39:29.876782 24732 net.cpp:2192] conv1a_param_0(0.333) 
I0814 19:39:29.876791 24732 net.cpp:2192] conv1b_param_0(0.67) 
I0814 19:39:29.876802 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:39:29.876808 24732 net.cpp:2192] res2a_branch2a_param_0(0.698) 
I0814 19:39:29.876817 24732 net.cpp:2192] res2a_branch2b_param_0(0.665) 
I0814 19:39:29.876821 24732 net.cpp:2192] res3a_branch2a_param_0(0.7) 
I0814 19:39:29.876826 24732 net.cpp:2192] res3a_branch2b_param_0(0.69) 
I0814 19:39:29.876833 24732 net.cpp:2192] res4a_branch2a_param_0(0.7) 
I0814 19:39:29.876837 24732 net.cpp:2192] res4a_branch2b_param_0(0.7) 
I0814 19:39:29.876847 24732 net.cpp:2192] res5a_branch2a_param_0(0.672) 
I0814 19:39:29.876850 24732 net.cpp:2192] res5a_branch2b_param_0(0.699) 
I0814 19:39:29.876858 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.6132e+06/2.3599e+06) 0.684
I0814 19:39:29.876883 24732 solver.cpp:509] Iteration 39000, Testing net (#0)
I0814 19:39:30.687156 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.900883
I0814 19:39:30.687175 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:39:30.687182 24732 solver.cpp:594]     Test net output #2: loss = 0.40881 (* 1 = 0.40881 loss)
I0814 19:39:30.687199 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810289s
I0814 19:39:30.702818 24783 solver.cpp:409] Finding and applying sparsity: 0.72
I0814 19:40:03.346062 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:40:03.348179 24732 solver.cpp:312] Iteration 39000 (2.85024 iter/s, 35.0848s/100 iter), loss = 0.000750362
I0814 19:40:03.348203 24732 solver.cpp:334]     Train net output #0: loss = 0.000750167 (* 1 = 0.000750167 loss)
I0814 19:40:03.348213 24732 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0814 19:40:05.158613 24732 solver.cpp:312] Iteration 39100 (55.2371 iter/s, 1.81038s/100 iter), loss = 0.000351459
I0814 19:40:05.158659 24732 solver.cpp:334]     Train net output #0: loss = 0.000351263 (* 1 = 0.000351263 loss)
I0814 19:40:05.158675 24732 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0814 19:40:06.801012 24732 solver.cpp:312] Iteration 39200 (60.8885 iter/s, 1.64235s/100 iter), loss = 0.000522055
I0814 19:40:06.801183 24732 solver.cpp:334]     Train net output #0: loss = 0.000521857 (* 1 = 0.000521857 loss)
I0814 19:40:06.801193 24732 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0814 19:40:08.492977 24732 solver.cpp:312] Iteration 39300 (59.1048 iter/s, 1.69191s/100 iter), loss = 0.00227447
I0814 19:40:08.493032 24732 solver.cpp:334]     Train net output #0: loss = 0.00227427 (* 1 = 0.00227427 loss)
I0814 19:40:08.493044 24732 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0814 19:40:09.985829 24716 data_reader.cpp:288] Starting prefetch of epoch 5
I0814 19:40:10.156713 24732 solver.cpp:312] Iteration 39400 (60.1077 iter/s, 1.66368s/100 iter), loss = 0.0034588
I0814 19:40:10.156771 24732 solver.cpp:334]     Train net output #0: loss = 0.0034586 (* 1 = 0.0034586 loss)
I0814 19:40:10.156790 24732 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0814 19:40:11.781260 24732 solver.cpp:312] Iteration 39500 (61.5574 iter/s, 1.6245s/100 iter), loss = 0.00317818
I0814 19:40:11.781323 24732 solver.cpp:334]     Train net output #0: loss = 0.00317798 (* 1 = 0.00317798 loss)
I0814 19:40:11.781352 24732 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0814 19:40:13.436818 24732 solver.cpp:312] Iteration 39600 (60.4045 iter/s, 1.65551s/100 iter), loss = 0.00258334
I0814 19:40:13.436862 24732 solver.cpp:334]     Train net output #0: loss = 0.00258315 (* 1 = 0.00258315 loss)
I0814 19:40:13.436875 24732 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0814 19:40:15.091969 24732 solver.cpp:312] Iteration 39700 (60.4194 iter/s, 1.6551s/100 iter), loss = 0.00438747
I0814 19:40:15.092017 24732 solver.cpp:334]     Train net output #0: loss = 0.00438727 (* 1 = 0.00438727 loss)
I0814 19:40:15.092030 24732 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0814 19:40:16.737113 24732 solver.cpp:312] Iteration 39800 (60.7868 iter/s, 1.64509s/100 iter), loss = 0.00173293
I0814 19:40:16.737169 24732 solver.cpp:334]     Train net output #0: loss = 0.00173273 (* 1 = 0.00173273 loss)
I0814 19:40:16.737184 24732 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0814 19:40:18.353957 24732 solver.cpp:312] Iteration 39900 (61.8508 iter/s, 1.61679s/100 iter), loss = 0.000918361
I0814 19:40:18.354087 24732 solver.cpp:334]     Train net output #0: loss = 0.000918167 (* 1 = 0.000918167 loss)
I0814 19:40:18.354104 24732 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0814 19:40:20.021356 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0814 19:40:20.029238 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0814 19:40:20.032728 24732 solver.cpp:363] Sparsity after update:
I0814 19:40:20.034801 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:40:20.034809 24732 net.cpp:2192] conv1a_param_0(0.333) 
I0814 19:40:20.034818 24732 net.cpp:2192] conv1b_param_0(0.679) 
I0814 19:40:20.034823 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:40:20.034827 24732 net.cpp:2192] res2a_branch2a_param_0(0.718) 
I0814 19:40:20.034832 24732 net.cpp:2192] res2a_branch2b_param_0(0.676) 
I0814 19:40:20.034847 24732 net.cpp:2192] res3a_branch2a_param_0(0.719) 
I0814 19:40:20.034852 24732 net.cpp:2192] res3a_branch2b_param_0(0.706) 
I0814 19:40:20.034857 24732 net.cpp:2192] res4a_branch2a_param_0(0.72) 
I0814 19:40:20.034859 24732 net.cpp:2192] res4a_branch2b_param_0(0.719) 
I0814 19:40:20.034863 24732 net.cpp:2192] res5a_branch2a_param_0(0.69) 
I0814 19:40:20.034868 24732 net.cpp:2192] res5a_branch2b_param_0(0.719) 
I0814 19:40:20.034873 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.65736e+06/2.3599e+06) 0.702
I0814 19:40:20.034883 24732 solver.cpp:509] Iteration 40000, Testing net (#0)
I0814 19:40:20.849427 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.902354
I0814 19:40:20.849447 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:40:20.849453 24732 solver.cpp:594]     Test net output #2: loss = 0.390551 (* 1 = 0.390551 loss)
I0814 19:40:20.849470 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.81456s
I0814 19:40:20.868772 24783 solver.cpp:409] Finding and applying sparsity: 0.74
I0814 19:40:54.905506 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:40:54.907546 24732 solver.cpp:312] Iteration 40000 (2.73578 iter/s, 36.5526s/100 iter), loss = 0.00185068
I0814 19:40:54.907568 24732 solver.cpp:334]     Train net output #0: loss = 0.00185049 (* 1 = 0.00185049 loss)
I0814 19:40:54.907582 24732 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0814 19:40:56.783093 24732 solver.cpp:312] Iteration 40100 (53.3195 iter/s, 1.87549s/100 iter), loss = 0.00275401
I0814 19:40:56.783118 24732 solver.cpp:334]     Train net output #0: loss = 0.00275382 (* 1 = 0.00275382 loss)
I0814 19:40:56.783123 24732 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0814 19:40:58.423709 24732 solver.cpp:312] Iteration 40200 (60.9546 iter/s, 1.64057s/100 iter), loss = 0.000428766
I0814 19:40:58.423756 24732 solver.cpp:334]     Train net output #0: loss = 0.000428577 (* 1 = 0.000428577 loss)
I0814 19:40:58.423768 24732 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0814 19:41:00.050674 24732 solver.cpp:312] Iteration 40300 (61.466 iter/s, 1.62692s/100 iter), loss = 0.00121535
I0814 19:41:00.050739 24732 solver.cpp:334]     Train net output #0: loss = 0.00121516 (* 1 = 0.00121516 loss)
I0814 19:41:00.050760 24732 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0814 19:41:01.741339 24732 solver.cpp:312] Iteration 40400 (59.1503 iter/s, 1.69061s/100 iter), loss = 0.00862113
I0814 19:41:01.741365 24732 solver.cpp:334]     Train net output #0: loss = 0.00862094 (* 1 = 0.00862094 loss)
I0814 19:41:01.741371 24732 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0814 19:41:03.391124 24732 solver.cpp:312] Iteration 40500 (60.6158 iter/s, 1.64974s/100 iter), loss = 0.00239062
I0814 19:41:03.391189 24732 solver.cpp:334]     Train net output #0: loss = 0.00239044 (* 1 = 0.00239044 loss)
I0814 19:41:03.391208 24732 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0814 19:41:05.072435 24732 solver.cpp:312] Iteration 40600 (59.4792 iter/s, 1.68126s/100 iter), loss = 0.000548397
I0814 19:41:05.072515 24732 solver.cpp:334]     Train net output #0: loss = 0.000548211 (* 1 = 0.000548211 loss)
I0814 19:41:05.072525 24732 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0814 19:41:06.764266 24732 solver.cpp:312] Iteration 40700 (59.1095 iter/s, 1.69177s/100 iter), loss = 0.000512979
I0814 19:41:06.764291 24732 solver.cpp:334]     Train net output #0: loss = 0.000512792 (* 1 = 0.000512792 loss)
I0814 19:41:06.764297 24732 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0814 19:41:08.403115 24732 solver.cpp:312] Iteration 40800 (61.0203 iter/s, 1.6388s/100 iter), loss = 0.000467377
I0814 19:41:08.403139 24732 solver.cpp:334]     Train net output #0: loss = 0.00046719 (* 1 = 0.00046719 loss)
I0814 19:41:08.403143 24732 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0814 19:41:10.004137 24732 solver.cpp:312] Iteration 40900 (62.4622 iter/s, 1.60097s/100 iter), loss = 0.00289349
I0814 19:41:10.004159 24732 solver.cpp:334]     Train net output #0: loss = 0.0028933 (* 1 = 0.0028933 loss)
I0814 19:41:10.004163 24732 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0814 19:41:11.657166 24732 solver.cpp:363] Sparsity after update:
I0814 19:41:11.658931 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:41:11.658941 24732 net.cpp:2192] conv1a_param_0(0.344) 
I0814 19:41:11.658951 24732 net.cpp:2192] conv1b_param_0(0.694) 
I0814 19:41:11.658956 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:41:11.658960 24732 net.cpp:2192] res2a_branch2a_param_0(0.737) 
I0814 19:41:11.658965 24732 net.cpp:2192] res2a_branch2b_param_0(0.684) 
I0814 19:41:11.658970 24732 net.cpp:2192] res3a_branch2a_param_0(0.739) 
I0814 19:41:11.658974 24732 net.cpp:2192] res3a_branch2b_param_0(0.719) 
I0814 19:41:11.658979 24732 net.cpp:2192] res4a_branch2a_param_0(0.74) 
I0814 19:41:11.658983 24732 net.cpp:2192] res4a_branch2b_param_0(0.739) 
I0814 19:41:11.658988 24732 net.cpp:2192] res5a_branch2a_param_0(0.706) 
I0814 19:41:11.658993 24732 net.cpp:2192] res5a_branch2b_param_0(0.739) 
I0814 19:41:11.658998 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.69971e+06/2.3599e+06) 0.72
I0814 19:41:11.659023 24732 solver.cpp:509] Iteration 41000, Testing net (#0)
I0814 19:41:12.497175 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907942
I0814 19:41:12.497195 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995588
I0814 19:41:12.497200 24732 solver.cpp:594]     Test net output #2: loss = 0.3791 (* 1 = 0.3791 loss)
I0814 19:41:12.497460 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.838168s
I0814 19:41:12.512814 24783 solver.cpp:409] Finding and applying sparsity: 0.76
I0814 19:41:48.404572 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:41:48.406741 24732 solver.cpp:312] Iteration 41000 (2.60406 iter/s, 38.4016s/100 iter), loss = 0.00119698
I0814 19:41:48.406761 24732 solver.cpp:334]     Train net output #0: loss = 0.0011968 (* 1 = 0.0011968 loss)
I0814 19:41:48.406770 24732 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0814 19:41:50.260859 24732 solver.cpp:312] Iteration 41100 (53.9357 iter/s, 1.85406s/100 iter), loss = 0.000816818
I0814 19:41:50.260906 24732 solver.cpp:334]     Train net output #0: loss = 0.00081663 (* 1 = 0.00081663 loss)
I0814 19:41:50.260921 24732 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0814 19:41:51.901219 24732 solver.cpp:312] Iteration 41200 (60.9641 iter/s, 1.64031s/100 iter), loss = 0.00551306
I0814 19:41:51.901266 24732 solver.cpp:334]     Train net output #0: loss = 0.00551287 (* 1 = 0.00551287 loss)
I0814 19:41:51.901279 24732 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0814 19:41:53.569437 24732 solver.cpp:312] Iteration 41300 (59.9461 iter/s, 1.66817s/100 iter), loss = 0.0023845
I0814 19:41:53.569461 24732 solver.cpp:334]     Train net output #0: loss = 0.00238431 (* 1 = 0.00238431 loss)
I0814 19:41:53.569466 24732 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0814 19:41:55.169376 24732 solver.cpp:312] Iteration 41400 (62.5043 iter/s, 1.59989s/100 iter), loss = 0.00184531
I0814 19:41:55.169438 24732 solver.cpp:334]     Train net output #0: loss = 0.00184512 (* 1 = 0.00184512 loss)
I0814 19:41:55.169457 24732 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0814 19:41:56.829900 24732 solver.cpp:312] Iteration 41500 (60.2239 iter/s, 1.66047s/100 iter), loss = 0.0043073
I0814 19:41:56.829926 24732 solver.cpp:334]     Train net output #0: loss = 0.00430711 (* 1 = 0.00430711 loss)
I0814 19:41:56.829931 24732 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0814 19:41:58.494933 24732 solver.cpp:312] Iteration 41600 (60.0607 iter/s, 1.66498s/100 iter), loss = 0.00144118
I0814 19:41:58.494992 24732 solver.cpp:334]     Train net output #0: loss = 0.00144099 (* 1 = 0.00144099 loss)
I0814 19:41:58.495010 24732 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0814 19:42:00.114933 24732 solver.cpp:312] Iteration 41700 (61.7304 iter/s, 1.61995s/100 iter), loss = 0.00301795
I0814 19:42:00.114956 24732 solver.cpp:334]     Train net output #0: loss = 0.00301777 (* 1 = 0.00301777 loss)
I0814 19:42:00.114960 24732 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0814 19:42:01.743558 24732 solver.cpp:312] Iteration 41800 (61.4035 iter/s, 1.62857s/100 iter), loss = 0.00376672
I0814 19:42:01.743584 24732 solver.cpp:334]     Train net output #0: loss = 0.00376654 (* 1 = 0.00376654 loss)
I0814 19:42:01.743590 24732 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0814 19:42:03.461050 24732 solver.cpp:312] Iteration 41900 (58.2262 iter/s, 1.71744s/100 iter), loss = 0.00418962
I0814 19:42:03.461122 24732 solver.cpp:334]     Train net output #0: loss = 0.00418944 (* 1 = 0.00418944 loss)
I0814 19:42:03.461145 24732 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0814 19:42:05.108731 24732 solver.cpp:363] Sparsity after update:
I0814 19:42:05.110352 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:42:05.110361 24732 net.cpp:2192] conv1a_param_0(0.358) 
I0814 19:42:05.110370 24732 net.cpp:2192] conv1b_param_0(0.702) 
I0814 19:42:05.110380 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:42:05.110386 24732 net.cpp:2192] res2a_branch2a_param_0(0.753) 
I0814 19:42:05.110390 24732 net.cpp:2192] res2a_branch2b_param_0(0.691) 
I0814 19:42:05.110399 24732 net.cpp:2192] res3a_branch2a_param_0(0.758) 
I0814 19:42:05.110404 24732 net.cpp:2192] res3a_branch2b_param_0(0.727) 
I0814 19:42:05.110407 24732 net.cpp:2192] res4a_branch2a_param_0(0.76) 
I0814 19:42:05.110415 24732 net.cpp:2192] res4a_branch2b_param_0(0.758) 
I0814 19:42:05.110420 24732 net.cpp:2192] res5a_branch2a_param_0(0.723) 
I0814 19:42:05.110424 24732 net.cpp:2192] res5a_branch2b_param_0(0.759) 
I0814 19:42:05.110427 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.74243e+06/2.3599e+06) 0.738
I0814 19:42:05.112087 24732 solver.cpp:509] Iteration 42000, Testing net (#0)
I0814 19:42:05.932862 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905001
I0814 19:42:05.932883 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:42:05.932888 24732 solver.cpp:594]     Test net output #2: loss = 0.390497 (* 1 = 0.390497 loss)
I0814 19:42:05.932902 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.820788s
I0814 19:42:05.948473 24783 solver.cpp:409] Finding and applying sparsity: 0.78
I0814 19:42:44.288733 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:42:44.290925 24732 solver.cpp:312] Iteration 42000 (2.44926 iter/s, 40.8287s/100 iter), loss = 0.00223816
I0814 19:42:44.290958 24732 solver.cpp:334]     Train net output #0: loss = 0.00223798 (* 1 = 0.00223798 loss)
I0814 19:42:44.290967 24732 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0814 19:42:46.185745 24732 solver.cpp:312] Iteration 42100 (52.777 iter/s, 1.89476s/100 iter), loss = 0.00208022
I0814 19:42:46.185768 24732 solver.cpp:334]     Train net output #0: loss = 0.00208003 (* 1 = 0.00208003 loss)
I0814 19:42:46.185773 24732 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0814 19:42:47.837906 24732 solver.cpp:312] Iteration 42200 (60.5287 iter/s, 1.65211s/100 iter), loss = 0.00034668
I0814 19:42:47.837955 24732 solver.cpp:334]     Train net output #0: loss = 0.000346499 (* 1 = 0.000346499 loss)
I0814 19:42:47.837967 24732 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0814 19:42:49.489023 24732 solver.cpp:312] Iteration 42300 (60.567 iter/s, 1.65106s/100 iter), loss = 0.00295807
I0814 19:42:49.489047 24732 solver.cpp:334]     Train net output #0: loss = 0.00295788 (* 1 = 0.00295788 loss)
I0814 19:42:49.489050 24732 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0814 19:42:51.112115 24732 solver.cpp:312] Iteration 42400 (61.6127 iter/s, 1.62304s/100 iter), loss = 0.00186282
I0814 19:42:51.112166 24732 solver.cpp:334]     Train net output #0: loss = 0.00186263 (* 1 = 0.00186263 loss)
I0814 19:42:51.112179 24732 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0814 19:42:52.764791 24732 solver.cpp:312] Iteration 42500 (60.5098 iter/s, 1.65262s/100 iter), loss = 0.00510715
I0814 19:42:52.764818 24732 solver.cpp:334]     Train net output #0: loss = 0.00510696 (* 1 = 0.00510696 loss)
I0814 19:42:52.764824 24732 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0814 19:42:54.427196 24732 solver.cpp:312] Iteration 42600 (60.1558 iter/s, 1.66235s/100 iter), loss = 0.00719461
I0814 19:42:54.427220 24732 solver.cpp:334]     Train net output #0: loss = 0.00719442 (* 1 = 0.00719442 loss)
I0814 19:42:54.427225 24732 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0814 19:42:56.089000 24732 solver.cpp:312] Iteration 42700 (60.1773 iter/s, 1.66175s/100 iter), loss = 0.00916129
I0814 19:42:56.089054 24732 solver.cpp:334]     Train net output #0: loss = 0.0091611 (* 1 = 0.0091611 loss)
I0814 19:42:56.089066 24732 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0814 19:42:57.744937 24732 solver.cpp:312] Iteration 42800 (60.3908 iter/s, 1.65588s/100 iter), loss = 0.000491053
I0814 19:42:57.745003 24732 solver.cpp:334]     Train net output #0: loss = 0.000490862 (* 1 = 0.000490862 loss)
I0814 19:42:57.745023 24732 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0814 19:42:59.398619 24732 solver.cpp:312] Iteration 42900 (60.4729 iter/s, 1.65363s/100 iter), loss = 0.0127771
I0814 19:42:59.398665 24732 solver.cpp:334]     Train net output #0: loss = 0.0127769 (* 1 = 0.0127769 loss)
I0814 19:42:59.398679 24732 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0814 19:43:01.019109 24732 solver.cpp:363] Sparsity after update:
I0814 19:43:01.020874 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:43:01.020884 24732 net.cpp:2192] conv1a_param_0(0.371) 
I0814 19:43:01.020891 24732 net.cpp:2192] conv1b_param_0(0.716) 
I0814 19:43:01.020895 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:43:01.020898 24732 net.cpp:2192] res2a_branch2a_param_0(0.772) 
I0814 19:43:01.020910 24732 net.cpp:2192] res2a_branch2b_param_0(0.696) 
I0814 19:43:01.020920 24732 net.cpp:2192] res3a_branch2a_param_0(0.776) 
I0814 19:43:01.020928 24732 net.cpp:2192] res3a_branch2b_param_0(0.735) 
I0814 19:43:01.020936 24732 net.cpp:2192] res4a_branch2a_param_0(0.78) 
I0814 19:43:01.020944 24732 net.cpp:2192] res4a_branch2b_param_0(0.778) 
I0814 19:43:01.020952 24732 net.cpp:2192] res5a_branch2a_param_0(0.746) 
I0814 19:43:01.020961 24732 net.cpp:2192] res5a_branch2b_param_0(0.779) 
I0814 19:43:01.020970 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.79219e+06/2.3599e+06) 0.759
I0814 19:43:01.020999 24732 solver.cpp:509] Iteration 43000, Testing net (#0)
I0814 19:43:01.834561 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.915589
I0814 19:43:01.834581 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:43:01.834586 24732 solver.cpp:594]     Test net output #2: loss = 0.368681 (* 1 = 0.368681 loss)
I0814 19:43:01.834602 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.813576s
I0814 19:43:01.850206 24783 solver.cpp:409] Finding and applying sparsity: 0.8
I0814 19:43:42.684126 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:43:42.686229 24732 solver.cpp:312] Iteration 43000 (2.31019 iter/s, 43.2864s/100 iter), loss = 0.000641299
I0814 19:43:42.686252 24732 solver.cpp:334]     Train net output #0: loss = 0.000641113 (* 1 = 0.000641113 loss)
I0814 19:43:42.686260 24732 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0814 19:43:44.578044 24732 solver.cpp:312] Iteration 43100 (52.861 iter/s, 1.89175s/100 iter), loss = 0.00202656
I0814 19:43:44.578069 24732 solver.cpp:334]     Train net output #0: loss = 0.00202638 (* 1 = 0.00202638 loss)
I0814 19:43:44.578075 24732 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0814 19:43:46.230785 24732 solver.cpp:312] Iteration 43200 (60.5074 iter/s, 1.65269s/100 iter), loss = 0.000817979
I0814 19:43:46.230841 24732 solver.cpp:334]     Train net output #0: loss = 0.000817792 (* 1 = 0.000817792 loss)
I0814 19:43:46.230856 24732 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0814 19:43:47.920801 24732 solver.cpp:312] Iteration 43300 (59.1729 iter/s, 1.68996s/100 iter), loss = 0.014249
I0814 19:43:47.920825 24732 solver.cpp:334]     Train net output #0: loss = 0.0142488 (* 1 = 0.0142488 loss)
I0814 19:43:47.920828 24732 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0814 19:43:49.535634 24732 solver.cpp:312] Iteration 43400 (61.9278 iter/s, 1.61478s/100 iter), loss = 0.00109175
I0814 19:43:49.535679 24732 solver.cpp:334]     Train net output #0: loss = 0.00109156 (* 1 = 0.00109156 loss)
I0814 19:43:49.535691 24732 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0814 19:43:51.194504 24732 solver.cpp:312] Iteration 43500 (60.2839 iter/s, 1.65882s/100 iter), loss = 0.000799006
I0814 19:43:51.194561 24732 solver.cpp:334]     Train net output #0: loss = 0.000798809 (* 1 = 0.000798809 loss)
I0814 19:43:51.194578 24732 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0814 19:43:52.875403 24732 solver.cpp:312] Iteration 43600 (59.4939 iter/s, 1.68085s/100 iter), loss = 0.00396464
I0814 19:43:52.875434 24732 solver.cpp:334]     Train net output #0: loss = 0.00396445 (* 1 = 0.00396445 loss)
I0814 19:43:52.875442 24732 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0814 19:43:54.567615 24732 solver.cpp:312] Iteration 43700 (59.0961 iter/s, 1.69216s/100 iter), loss = 0.0262592
I0814 19:43:54.567639 24732 solver.cpp:334]     Train net output #0: loss = 0.026259 (* 1 = 0.026259 loss)
I0814 19:43:54.567646 24732 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0814 19:43:56.186581 24732 solver.cpp:312] Iteration 43800 (61.7696 iter/s, 1.61892s/100 iter), loss = 0.00174655
I0814 19:43:56.186606 24732 solver.cpp:334]     Train net output #0: loss = 0.00174635 (* 1 = 0.00174635 loss)
I0814 19:43:56.186612 24732 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0814 19:43:57.823966 24732 solver.cpp:312] Iteration 43900 (61.0749 iter/s, 1.63733s/100 iter), loss = 0.00317649
I0814 19:43:57.824036 24732 solver.cpp:334]     Train net output #0: loss = 0.00317629 (* 1 = 0.00317629 loss)
I0814 19:43:57.824057 24732 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0814 19:43:58.425570 24716 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:43:59.486100 24732 solver.cpp:363] Sparsity after update:
I0814 19:43:59.487625 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:43:59.487633 24732 net.cpp:2192] conv1a_param_0(0.371) 
I0814 19:43:59.487642 24732 net.cpp:2192] conv1b_param_0(0.723) 
I0814 19:43:59.487646 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:43:59.487650 24732 net.cpp:2192] res2a_branch2a_param_0(0.788) 
I0814 19:43:59.487654 24732 net.cpp:2192] res2a_branch2b_param_0(0.701) 
I0814 19:43:59.487658 24732 net.cpp:2192] res3a_branch2a_param_0(0.792) 
I0814 19:43:59.487663 24732 net.cpp:2192] res3a_branch2b_param_0(0.741) 
I0814 19:43:59.487665 24732 net.cpp:2192] res4a_branch2a_param_0(0.799) 
I0814 19:43:59.487669 24732 net.cpp:2192] res4a_branch2b_param_0(0.796) 
I0814 19:43:59.487673 24732 net.cpp:2192] res5a_branch2a_param_0(0.764) 
I0814 19:43:59.487676 24732 net.cpp:2192] res5a_branch2b_param_0(0.799) 
I0814 19:43:59.487694 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.83555e+06/2.3599e+06) 0.778
I0814 19:43:59.487707 24732 solver.cpp:509] Iteration 44000, Testing net (#0)
I0814 19:44:00.291728 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910883
I0814 19:44:00.291748 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:44:00.291754 24732 solver.cpp:594]     Test net output #2: loss = 0.375525 (* 1 = 0.375525 loss)
I0814 19:44:00.291774 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.804037s
I0814 19:44:00.307719 24783 solver.cpp:409] Finding and applying sparsity: 0.82
I0814 19:44:43.451889 24783 net.cpp:2166] All zero weights of convolution layers are frozen
I0814 19:44:43.453974 24732 solver.cpp:312] Iteration 44000 (2.1916 iter/s, 45.6287s/100 iter), loss = 0.00289719
I0814 19:44:43.453994 24732 solver.cpp:334]     Train net output #0: loss = 0.00289699 (* 1 = 0.00289699 loss)
I0814 19:44:43.454002 24732 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0814 19:44:45.395800 24732 solver.cpp:312] Iteration 44100 (51.4995 iter/s, 1.94177s/100 iter), loss = 0.00316584
I0814 19:44:45.395823 24732 solver.cpp:334]     Train net output #0: loss = 0.00316564 (* 1 = 0.00316564 loss)
I0814 19:44:45.395828 24732 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0814 19:44:47.040112 24732 solver.cpp:312] Iteration 44200 (60.8177 iter/s, 1.64426s/100 iter), loss = 0.00374066
I0814 19:44:47.040176 24732 solver.cpp:334]     Train net output #0: loss = 0.00374048 (* 1 = 0.00374048 loss)
I0814 19:44:47.040194 24732 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0814 19:44:48.697098 24732 solver.cpp:312] Iteration 44300 (60.3524 iter/s, 1.65694s/100 iter), loss = 0.00155081
I0814 19:44:48.697149 24732 solver.cpp:334]     Train net output #0: loss = 0.00155064 (* 1 = 0.00155064 loss)
I0814 19:44:48.697165 24732 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0814 19:44:50.340046 24732 solver.cpp:312] Iteration 44400 (60.8681 iter/s, 1.6429s/100 iter), loss = 0.00750333
I0814 19:44:50.340072 24732 solver.cpp:334]     Train net output #0: loss = 0.00750316 (* 1 = 0.00750316 loss)
I0814 19:44:50.340078 24732 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0814 19:44:51.961629 24732 solver.cpp:312] Iteration 44500 (61.6701 iter/s, 1.62153s/100 iter), loss = 0.00136405
I0814 19:44:51.961659 24732 solver.cpp:334]     Train net output #0: loss = 0.00136388 (* 1 = 0.00136388 loss)
I0814 19:44:51.961666 24732 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0814 19:44:53.588526 24732 solver.cpp:312] Iteration 44600 (61.4685 iter/s, 1.62685s/100 iter), loss = 0.0017204
I0814 19:44:53.588551 24732 solver.cpp:334]     Train net output #0: loss = 0.00172023 (* 1 = 0.00172023 loss)
I0814 19:44:53.588557 24732 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0814 19:44:55.238241 24732 solver.cpp:312] Iteration 44700 (60.6185 iter/s, 1.64966s/100 iter), loss = 0.0013025
I0814 19:44:55.238266 24732 solver.cpp:334]     Train net output #0: loss = 0.00130233 (* 1 = 0.00130233 loss)
I0814 19:44:55.238272 24732 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0814 19:44:56.866319 24732 solver.cpp:312] Iteration 44800 (61.424 iter/s, 1.62803s/100 iter), loss = 0.00026684
I0814 19:44:56.866380 24732 solver.cpp:334]     Train net output #0: loss = 0.000266669 (* 1 = 0.000266669 loss)
I0814 19:44:56.866400 24732 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0814 19:44:58.496909 24732 solver.cpp:312] Iteration 44900 (61.3295 iter/s, 1.63054s/100 iter), loss = 0.0199795
I0814 19:44:58.496968 24732 solver.cpp:334]     Train net output #0: loss = 0.0199793 (* 1 = 0.0199793 loss)
I0814 19:44:58.496987 24732 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0814 19:45:00.118031 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:00.119702 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:00.119712 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:00.119719 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:00.119724 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:00.119729 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:00.119731 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:00.119735 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:00.119740 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:00.119742 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:00.119747 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:00.119750 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:00.119755 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:00.119758 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:00.119783 24732 solver.cpp:509] Iteration 45000, Testing net (#0)
I0814 19:45:00.942535 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.899413
I0814 19:45:00.942554 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:45:00.942561 24732 solver.cpp:594]     Test net output #2: loss = 0.429454 (* 1 = 0.429454 loss)
I0814 19:45:00.942579 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.822766s
I0814 19:45:00.958400 24732 solver.cpp:312] Iteration 45000 (40.6271 iter/s, 2.46141s/100 iter), loss = 0.00614304
I0814 19:45:00.958443 24732 solver.cpp:334]     Train net output #0: loss = 0.00614287 (* 1 = 0.00614287 loss)
I0814 19:45:00.958456 24732 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0814 19:45:02.578477 24732 solver.cpp:312] Iteration 45100 (61.7273 iter/s, 1.62003s/100 iter), loss = 0.00510173
I0814 19:45:02.578502 24732 solver.cpp:334]     Train net output #0: loss = 0.00510156 (* 1 = 0.00510156 loss)
I0814 19:45:02.578508 24732 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0814 19:45:04.249733 24732 solver.cpp:312] Iteration 45200 (59.8371 iter/s, 1.6712s/100 iter), loss = 0.00286821
I0814 19:45:04.249795 24732 solver.cpp:334]     Train net output #0: loss = 0.00286803 (* 1 = 0.00286803 loss)
I0814 19:45:04.249815 24732 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0814 19:45:05.877025 24732 solver.cpp:312] Iteration 45300 (61.4538 iter/s, 1.62724s/100 iter), loss = 0.000631504
I0814 19:45:05.877051 24732 solver.cpp:334]     Train net output #0: loss = 0.000631321 (* 1 = 0.000631321 loss)
I0814 19:45:05.877056 24732 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0814 19:45:07.492733 24732 solver.cpp:312] Iteration 45400 (61.8944 iter/s, 1.61565s/100 iter), loss = 0.0100706
I0814 19:45:07.493044 24732 solver.cpp:334]     Train net output #0: loss = 0.0100704 (* 1 = 0.0100704 loss)
I0814 19:45:07.493068 24732 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0814 19:45:09.102313 24732 solver.cpp:312] Iteration 45500 (62.13 iter/s, 1.60953s/100 iter), loss = 0.00429005
I0814 19:45:09.102337 24732 solver.cpp:334]     Train net output #0: loss = 0.00428986 (* 1 = 0.00428986 loss)
I0814 19:45:09.102344 24732 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0814 19:45:10.766691 24732 solver.cpp:312] Iteration 45600 (60.0844 iter/s, 1.66433s/100 iter), loss = 0.00164105
I0814 19:45:10.766716 24732 solver.cpp:334]     Train net output #0: loss = 0.00164086 (* 1 = 0.00164086 loss)
I0814 19:45:10.766721 24732 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0814 19:45:12.408849 24732 solver.cpp:312] Iteration 45700 (60.8974 iter/s, 1.64211s/100 iter), loss = 0.0057923
I0814 19:45:12.408874 24732 solver.cpp:334]     Train net output #0: loss = 0.00579212 (* 1 = 0.00579212 loss)
I0814 19:45:12.408880 24732 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0814 19:45:14.058796 24732 solver.cpp:312] Iteration 45800 (60.6099 iter/s, 1.6499s/100 iter), loss = 0.000685582
I0814 19:45:14.058871 24732 solver.cpp:334]     Train net output #0: loss = 0.000685394 (* 1 = 0.000685394 loss)
I0814 19:45:14.058878 24732 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0814 19:45:15.735945 24732 solver.cpp:312] Iteration 45900 (59.627 iter/s, 1.67709s/100 iter), loss = 0.00284487
I0814 19:45:15.735992 24732 solver.cpp:334]     Train net output #0: loss = 0.00284468 (* 1 = 0.00284468 loss)
I0814 19:45:15.736003 24732 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0814 19:45:17.380292 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:17.381755 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:17.381763 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:17.381780 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:17.381791 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:17.381800 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:17.381809 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:17.381817 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:17.381827 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:17.381836 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:17.381846 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:17.381855 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:17.381865 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:17.381875 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:17.381892 24732 solver.cpp:509] Iteration 46000, Testing net (#0)
I0814 19:45:18.189019 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906766
I0814 19:45:18.189038 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:45:18.189043 24732 solver.cpp:594]     Test net output #2: loss = 0.380803 (* 1 = 0.380803 loss)
I0814 19:45:18.189060 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.80714s
I0814 19:45:18.204962 24732 solver.cpp:312] Iteration 46000 (40.5031 iter/s, 2.46894s/100 iter), loss = 0.00646185
I0814 19:45:18.204977 24732 solver.cpp:334]     Train net output #0: loss = 0.00646166 (* 1 = 0.00646166 loss)
I0814 19:45:18.204980 24732 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0814 19:45:19.881498 24732 solver.cpp:312] Iteration 46100 (59.6487 iter/s, 1.67648s/100 iter), loss = 0.00628806
I0814 19:45:19.881561 24732 solver.cpp:334]     Train net output #0: loss = 0.00628787 (* 1 = 0.00628787 loss)
I0814 19:45:19.881580 24732 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0814 19:45:21.540662 24732 solver.cpp:312] Iteration 46200 (60.2733 iter/s, 1.65911s/100 iter), loss = 0.00107459
I0814 19:45:21.540729 24732 solver.cpp:334]     Train net output #0: loss = 0.0010744 (* 1 = 0.0010744 loss)
I0814 19:45:21.540750 24732 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0814 19:45:23.180186 24732 solver.cpp:312] Iteration 46300 (60.9954 iter/s, 1.63947s/100 iter), loss = 0.00255637
I0814 19:45:23.180209 24732 solver.cpp:334]     Train net output #0: loss = 0.00255618 (* 1 = 0.00255618 loss)
I0814 19:45:23.180215 24732 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0814 19:45:24.855367 24732 solver.cpp:312] Iteration 46400 (59.6968 iter/s, 1.67513s/100 iter), loss = 0.000989639
I0814 19:45:24.855393 24732 solver.cpp:334]     Train net output #0: loss = 0.000989447 (* 1 = 0.000989447 loss)
I0814 19:45:24.855399 24732 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0814 19:45:26.489683 24732 solver.cpp:312] Iteration 46500 (61.1897 iter/s, 1.63426s/100 iter), loss = 0.0096012
I0814 19:45:26.489713 24732 solver.cpp:334]     Train net output #0: loss = 0.00960101 (* 1 = 0.00960101 loss)
I0814 19:45:26.489720 24732 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0814 19:45:28.145165 24732 solver.cpp:312] Iteration 46600 (60.4073 iter/s, 1.65543s/100 iter), loss = 0.000646559
I0814 19:45:28.145229 24732 solver.cpp:334]     Train net output #0: loss = 0.000646367 (* 1 = 0.000646367 loss)
I0814 19:45:28.145267 24732 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0814 19:45:29.804136 24732 solver.cpp:312] Iteration 46700 (60.2804 iter/s, 1.65891s/100 iter), loss = 0.00427841
I0814 19:45:29.804159 24732 solver.cpp:334]     Train net output #0: loss = 0.00427821 (* 1 = 0.00427821 loss)
I0814 19:45:29.804165 24732 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0814 19:45:31.450904 24732 solver.cpp:312] Iteration 46800 (60.7269 iter/s, 1.64672s/100 iter), loss = 0.0141461
I0814 19:45:31.450966 24732 solver.cpp:334]     Train net output #0: loss = 0.0141459 (* 1 = 0.0141459 loss)
I0814 19:45:31.450985 24732 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0814 19:45:33.071557 24732 solver.cpp:312] Iteration 46900 (61.7055 iter/s, 1.6206s/100 iter), loss = 0.00131976
I0814 19:45:33.071581 24732 solver.cpp:334]     Train net output #0: loss = 0.00131956 (* 1 = 0.00131956 loss)
I0814 19:45:33.071588 24732 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0814 19:45:34.689520 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:34.691215 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:34.691224 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:34.691232 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:34.691243 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:34.691252 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:34.691262 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:34.691272 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:34.691279 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:34.691287 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:34.691295 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:34.691305 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:34.691315 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:34.691324 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:34.691342 24732 solver.cpp:509] Iteration 47000, Testing net (#0)
I0814 19:45:35.500025 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.899413
I0814 19:45:35.500043 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:45:35.500048 24732 solver.cpp:594]     Test net output #2: loss = 0.405145 (* 1 = 0.405145 loss)
I0814 19:45:35.500066 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.808695s
I0814 19:45:35.519748 24732 solver.cpp:312] Iteration 47000 (40.8477 iter/s, 2.44812s/100 iter), loss = 0.00161108
I0814 19:45:35.520150 24732 solver.cpp:334]     Train net output #0: loss = 0.00161088 (* 1 = 0.00161088 loss)
I0814 19:45:35.520161 24732 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0814 19:45:37.189707 24732 solver.cpp:312] Iteration 47100 (59.8836 iter/s, 1.66991s/100 iter), loss = 0.00546813
I0814 19:45:37.189774 24732 solver.cpp:334]     Train net output #0: loss = 0.00546794 (* 1 = 0.00546794 loss)
I0814 19:45:37.189792 24732 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0814 19:45:38.843749 24732 solver.cpp:312] Iteration 47200 (60.46 iter/s, 1.65399s/100 iter), loss = 0.000358653
I0814 19:45:38.843772 24732 solver.cpp:334]     Train net output #0: loss = 0.000358457 (* 1 = 0.000358457 loss)
I0814 19:45:38.843778 24732 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0814 19:45:40.493077 24732 solver.cpp:312] Iteration 47300 (60.6328 iter/s, 1.64927s/100 iter), loss = 0.00242342
I0814 19:45:40.493132 24732 solver.cpp:334]     Train net output #0: loss = 0.00242323 (* 1 = 0.00242323 loss)
I0814 19:45:40.493147 24732 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0814 19:45:42.146915 24732 solver.cpp:312] Iteration 47400 (60.4673 iter/s, 1.65379s/100 iter), loss = 0.00130676
I0814 19:45:42.146940 24732 solver.cpp:334]     Train net output #0: loss = 0.00130657 (* 1 = 0.00130657 loss)
I0814 19:45:42.146944 24732 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0814 19:45:43.780467 24732 solver.cpp:312] Iteration 47500 (61.2183 iter/s, 1.6335s/100 iter), loss = 0.00114526
I0814 19:45:43.780493 24732 solver.cpp:334]     Train net output #0: loss = 0.00114507 (* 1 = 0.00114507 loss)
I0814 19:45:43.780496 24732 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0814 19:45:45.448865 24732 solver.cpp:312] Iteration 47600 (59.9396 iter/s, 1.66835s/100 iter), loss = 0.000159153
I0814 19:45:45.448997 24732 solver.cpp:334]     Train net output #0: loss = 0.000158966 (* 1 = 0.000158966 loss)
I0814 19:45:45.449021 24732 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0814 19:45:47.134116 24732 solver.cpp:312] Iteration 47700 (59.3403 iter/s, 1.6852s/100 iter), loss = 0.00775853
I0814 19:45:47.134178 24732 solver.cpp:334]     Train net output #0: loss = 0.00775834 (* 1 = 0.00775834 loss)
I0814 19:45:47.134197 24732 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0814 19:45:48.783066 24732 solver.cpp:312] Iteration 47800 (60.6465 iter/s, 1.6489s/100 iter), loss = 0.00137352
I0814 19:45:48.783114 24732 solver.cpp:334]     Train net output #0: loss = 0.00137334 (* 1 = 0.00137334 loss)
I0814 19:45:48.783129 24732 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0814 19:45:50.400327 24732 solver.cpp:312] Iteration 47900 (61.8349 iter/s, 1.61721s/100 iter), loss = 0.000142569
I0814 19:45:50.400390 24732 solver.cpp:334]     Train net output #0: loss = 0.000142386 (* 1 = 0.000142386 loss)
I0814 19:45:50.400409 24732 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0814 19:45:52.045738 24732 solver.cpp:363] Sparsity after update:
I0814 19:45:52.047408 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:45:52.047420 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:45:52.047441 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:45:52.047453 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:45:52.047466 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:45:52.047477 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:45:52.047489 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:45:52.047500 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:45:52.047511 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:45:52.047519 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:45:52.047528 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:45:52.047539 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:45:52.047550 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:45:52.047575 24732 solver.cpp:509] Iteration 48000, Testing net (#0)
I0814 19:45:52.079618 24730 data_reader.cpp:288] Starting prefetch of epoch 6
I0814 19:45:52.886742 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905883
I0814 19:45:52.886762 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997353
I0814 19:45:52.886767 24732 solver.cpp:594]     Test net output #2: loss = 0.391059 (* 1 = 0.391059 loss)
I0814 19:45:52.886785 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83918s
I0814 19:45:52.905315 24732 solver.cpp:312] Iteration 48000 (39.9216 iter/s, 2.50491s/100 iter), loss = 0.00168554
I0814 19:45:52.905340 24732 solver.cpp:334]     Train net output #0: loss = 0.00168536 (* 1 = 0.00168536 loss)
I0814 19:45:52.905346 24732 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0814 19:45:54.521339 24732 solver.cpp:312] Iteration 48100 (61.8822 iter/s, 1.61597s/100 iter), loss = 0.00518548
I0814 19:45:54.521363 24732 solver.cpp:334]     Train net output #0: loss = 0.0051853 (* 1 = 0.0051853 loss)
I0814 19:45:54.521369 24732 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0814 19:45:56.203027 24732 solver.cpp:312] Iteration 48200 (59.4659 iter/s, 1.68164s/100 iter), loss = 0.00350444
I0814 19:45:56.203094 24732 solver.cpp:334]     Train net output #0: loss = 0.00350426 (* 1 = 0.00350426 loss)
I0814 19:45:56.203114 24732 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0814 19:45:57.866593 24732 solver.cpp:312] Iteration 48300 (60.1138 iter/s, 1.66351s/100 iter), loss = 0.00594211
I0814 19:45:57.866742 24732 solver.cpp:334]     Train net output #0: loss = 0.00594193 (* 1 = 0.00594193 loss)
I0814 19:45:57.866765 24732 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0814 19:45:59.510356 24732 solver.cpp:312] Iteration 48400 (60.8383 iter/s, 1.6437s/100 iter), loss = 0.00117012
I0814 19:45:59.510437 24732 solver.cpp:334]     Train net output #0: loss = 0.00116994 (* 1 = 0.00116994 loss)
I0814 19:45:59.510464 24732 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0814 19:46:01.116931 24732 solver.cpp:312] Iteration 48500 (62.2461 iter/s, 1.60653s/100 iter), loss = 0.000447479
I0814 19:46:01.116956 24732 solver.cpp:334]     Train net output #0: loss = 0.000447301 (* 1 = 0.000447301 loss)
I0814 19:46:01.116961 24732 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0814 19:46:02.765733 24732 solver.cpp:312] Iteration 48600 (60.6521 iter/s, 1.64875s/100 iter), loss = 0.00208838
I0814 19:46:02.765784 24732 solver.cpp:334]     Train net output #0: loss = 0.0020882 (* 1 = 0.0020882 loss)
I0814 19:46:02.765795 24732 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0814 19:46:04.425096 24732 solver.cpp:312] Iteration 48700 (60.266 iter/s, 1.65931s/100 iter), loss = 0.0002978
I0814 19:46:04.425145 24732 solver.cpp:334]     Train net output #0: loss = 0.000297624 (* 1 = 0.000297624 loss)
I0814 19:46:04.425158 24732 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0814 19:46:06.085487 24732 solver.cpp:312] Iteration 48800 (60.2287 iter/s, 1.66034s/100 iter), loss = 0.0012009
I0814 19:46:06.085512 24732 solver.cpp:334]     Train net output #0: loss = 0.00120072 (* 1 = 0.00120072 loss)
I0814 19:46:06.085517 24732 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0814 19:46:07.711839 24732 solver.cpp:312] Iteration 48900 (61.4893 iter/s, 1.6263s/100 iter), loss = 0.0009178
I0814 19:46:07.711869 24732 solver.cpp:334]     Train net output #0: loss = 0.000917628 (* 1 = 0.000917628 loss)
I0814 19:46:07.711874 24732 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0814 19:46:09.345917 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:09.347594 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:09.347604 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:09.347611 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:09.347615 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:09.347625 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:09.347641 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:09.347645 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:09.347650 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:09.347656 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:09.347661 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:09.347666 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:09.347673 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:09.347677 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:09.347692 24732 solver.cpp:509] Iteration 49000, Testing net (#0)
I0814 19:46:10.181808 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.910001
I0814 19:46:10.181826 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:46:10.181831 24732 solver.cpp:594]     Test net output #2: loss = 0.374734 (* 1 = 0.374734 loss)
I0814 19:46:10.181849 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.834128s
I0814 19:46:10.198374 24732 solver.cpp:312] Iteration 49000 (40.2178 iter/s, 2.48646s/100 iter), loss = 0.00557617
I0814 19:46:10.198390 24732 solver.cpp:334]     Train net output #0: loss = 0.005576 (* 1 = 0.005576 loss)
I0814 19:46:10.198393 24732 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0814 19:46:11.828636 24732 solver.cpp:312] Iteration 49100 (61.3419 iter/s, 1.63021s/100 iter), loss = 0.00342955
I0814 19:46:11.828699 24732 solver.cpp:334]     Train net output #0: loss = 0.00342938 (* 1 = 0.00342938 loss)
I0814 19:46:11.828717 24732 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0814 19:46:13.504168 24732 solver.cpp:312] Iteration 49200 (59.6845 iter/s, 1.67548s/100 iter), loss = 0.00228195
I0814 19:46:13.504240 24732 solver.cpp:334]     Train net output #0: loss = 0.00228178 (* 1 = 0.00228178 loss)
I0814 19:46:13.504276 24732 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0814 19:46:15.157135 24732 solver.cpp:312] Iteration 49300 (60.4993 iter/s, 1.65291s/100 iter), loss = 0.000719808
I0814 19:46:15.157182 24732 solver.cpp:334]     Train net output #0: loss = 0.000719639 (* 1 = 0.000719639 loss)
I0814 19:46:15.157196 24732 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0814 19:46:16.792868 24732 solver.cpp:312] Iteration 49400 (61.1366 iter/s, 1.63568s/100 iter), loss = 0.0010356
I0814 19:46:16.793000 24732 solver.cpp:334]     Train net output #0: loss = 0.00103543 (* 1 = 0.00103543 loss)
I0814 19:46:16.793028 24732 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0814 19:46:18.433589 24732 solver.cpp:312] Iteration 49500 (60.9508 iter/s, 1.64067s/100 iter), loss = 0.00156729
I0814 19:46:18.433621 24732 solver.cpp:334]     Train net output #0: loss = 0.00156712 (* 1 = 0.00156712 loss)
I0814 19:46:18.433627 24732 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0814 19:46:20.089946 24732 solver.cpp:312] Iteration 49600 (60.3753 iter/s, 1.65631s/100 iter), loss = 0.00166838
I0814 19:46:20.089994 24732 solver.cpp:334]     Train net output #0: loss = 0.00166821 (* 1 = 0.00166821 loss)
I0814 19:46:20.090008 24732 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0814 19:46:21.798602 24732 solver.cpp:312] Iteration 49700 (58.5273 iter/s, 1.7086s/100 iter), loss = 0.000140487
I0814 19:46:21.798691 24732 solver.cpp:334]     Train net output #0: loss = 0.000140318 (* 1 = 0.000140318 loss)
I0814 19:46:21.798701 24732 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0814 19:46:23.439321 24732 solver.cpp:312] Iteration 49800 (60.951 iter/s, 1.64066s/100 iter), loss = 0.000340135
I0814 19:46:23.439370 24732 solver.cpp:334]     Train net output #0: loss = 0.000339965 (* 1 = 0.000339965 loss)
I0814 19:46:23.439383 24732 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0814 19:46:25.055382 24732 solver.cpp:312] Iteration 49900 (61.8808 iter/s, 1.61601s/100 iter), loss = 0.000991804
I0814 19:46:25.055449 24732 solver.cpp:334]     Train net output #0: loss = 0.000991636 (* 1 = 0.000991636 loss)
I0814 19:46:25.055476 24732 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0814 19:46:26.712857 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0814 19:46:26.720752 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0814 19:46:26.724236 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:26.725920 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:26.725929 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:26.725937 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:26.725950 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:26.725955 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:26.725963 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:26.725968 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:26.725976 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:26.725981 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:26.725989 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:26.725994 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:26.726002 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:26.726006 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:26.726022 24732 solver.cpp:509] Iteration 50000, Testing net (#0)
I0814 19:46:27.536077 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905295
I0814 19:46:27.536098 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:46:27.536106 24732 solver.cpp:594]     Test net output #2: loss = 0.390708 (* 1 = 0.390708 loss)
I0814 19:46:27.536149 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810081s
I0814 19:46:27.554985 24732 solver.cpp:312] Iteration 50000 (40.0076 iter/s, 2.49952s/100 iter), loss = 0.000394828
I0814 19:46:27.555007 24732 solver.cpp:334]     Train net output #0: loss = 0.00039466 (* 1 = 0.00039466 loss)
I0814 19:46:27.555014 24732 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0814 19:46:29.165817 24732 solver.cpp:312] Iteration 50100 (62.0817 iter/s, 1.61078s/100 iter), loss = 0.0026114
I0814 19:46:29.165841 24732 solver.cpp:334]     Train net output #0: loss = 0.00261123 (* 1 = 0.00261123 loss)
I0814 19:46:29.165869 24732 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0814 19:46:30.823281 24732 solver.cpp:312] Iteration 50200 (60.335 iter/s, 1.65741s/100 iter), loss = 0.0212677
I0814 19:46:30.823305 24732 solver.cpp:334]     Train net output #0: loss = 0.0212675 (* 1 = 0.0212675 loss)
I0814 19:46:30.823310 24732 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0814 19:46:32.489959 24732 solver.cpp:312] Iteration 50300 (60.0015 iter/s, 1.66663s/100 iter), loss = 0.000550282
I0814 19:46:32.489984 24732 solver.cpp:334]     Train net output #0: loss = 0.000550118 (* 1 = 0.000550118 loss)
I0814 19:46:32.489989 24732 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0814 19:46:34.138135 24732 solver.cpp:312] Iteration 50400 (60.6751 iter/s, 1.64812s/100 iter), loss = 0.00320841
I0814 19:46:34.138160 24732 solver.cpp:334]     Train net output #0: loss = 0.00320825 (* 1 = 0.00320825 loss)
I0814 19:46:34.138166 24732 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0814 19:46:35.756839 24732 solver.cpp:312] Iteration 50500 (61.7797 iter/s, 1.61865s/100 iter), loss = 0.00501047
I0814 19:46:35.756907 24732 solver.cpp:334]     Train net output #0: loss = 0.00501031 (* 1 = 0.00501031 loss)
I0814 19:46:35.756928 24732 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0814 19:46:37.398265 24732 solver.cpp:312] Iteration 50600 (60.9246 iter/s, 1.64137s/100 iter), loss = 0.00362599
I0814 19:46:37.398334 24732 solver.cpp:334]     Train net output #0: loss = 0.00362583 (* 1 = 0.00362583 loss)
I0814 19:46:37.398353 24732 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0814 19:46:39.034327 24732 solver.cpp:312] Iteration 50700 (61.1243 iter/s, 1.63601s/100 iter), loss = 0.000554803
I0814 19:46:39.034375 24732 solver.cpp:334]     Train net output #0: loss = 0.000554639 (* 1 = 0.000554639 loss)
I0814 19:46:39.034389 24732 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0814 19:46:40.696765 24732 solver.cpp:312] Iteration 50800 (60.1546 iter/s, 1.66238s/100 iter), loss = 0.00195463
I0814 19:46:40.696812 24732 solver.cpp:334]     Train net output #0: loss = 0.00195447 (* 1 = 0.00195447 loss)
I0814 19:46:40.696826 24732 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0814 19:46:42.307680 24732 solver.cpp:312] Iteration 50900 (62.0786 iter/s, 1.61086s/100 iter), loss = 0.000837453
I0814 19:46:42.307705 24732 solver.cpp:334]     Train net output #0: loss = 0.00083729 (* 1 = 0.00083729 loss)
I0814 19:46:42.307710 24732 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0814 19:46:43.940557 24732 solver.cpp:363] Sparsity after update:
I0814 19:46:43.942268 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:46:43.942278 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:46:43.942286 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:46:43.942297 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:46:43.942307 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:46:43.942317 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:46:43.942324 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:46:43.942332 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:46:43.942340 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:46:43.942350 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:46:43.942359 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:46:43.942368 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:46:43.942378 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:46:43.942395 24732 solver.cpp:509] Iteration 51000, Testing net (#0)
I0814 19:46:44.759583 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907648
I0814 19:46:44.759603 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:46:44.759608 24732 solver.cpp:594]     Test net output #2: loss = 0.396453 (* 1 = 0.396453 loss)
I0814 19:46:44.759640 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.817216s
I0814 19:46:44.777479 24732 solver.cpp:312] Iteration 51000 (40.4903 iter/s, 2.46973s/100 iter), loss = 0.00299583
I0814 19:46:44.777498 24732 solver.cpp:334]     Train net output #0: loss = 0.00299567 (* 1 = 0.00299567 loss)
I0814 19:46:44.777503 24732 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0814 19:46:46.377229 24732 solver.cpp:312] Iteration 51100 (62.5119 iter/s, 1.5997s/100 iter), loss = 0.000207583
I0814 19:46:46.377254 24732 solver.cpp:334]     Train net output #0: loss = 0.000207419 (* 1 = 0.000207419 loss)
I0814 19:46:46.377260 24732 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0814 19:46:48.048111 24732 solver.cpp:312] Iteration 51200 (59.8505 iter/s, 1.67083s/100 iter), loss = 0.000346497
I0814 19:46:48.048197 24732 solver.cpp:334]     Train net output #0: loss = 0.000346334 (* 1 = 0.000346334 loss)
I0814 19:46:48.048204 24732 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0814 19:46:49.723917 24732 solver.cpp:312] Iteration 51300 (59.6747 iter/s, 1.67575s/100 iter), loss = 0.000380972
I0814 19:46:49.723976 24732 solver.cpp:334]     Train net output #0: loss = 0.000380807 (* 1 = 0.000380807 loss)
I0814 19:46:49.723994 24732 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0814 19:46:51.364089 24732 solver.cpp:312] Iteration 51400 (60.9712 iter/s, 1.64012s/100 iter), loss = 0.00129579
I0814 19:46:51.364114 24732 solver.cpp:334]     Train net output #0: loss = 0.00129562 (* 1 = 0.00129562 loss)
I0814 19:46:51.364120 24732 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0814 19:46:52.995292 24732 solver.cpp:312] Iteration 51500 (61.3064 iter/s, 1.63115s/100 iter), loss = 0.000211695
I0814 19:46:52.995316 24732 solver.cpp:334]     Train net output #0: loss = 0.00021153 (* 1 = 0.00021153 loss)
I0814 19:46:52.995322 24732 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0814 19:46:54.645207 24732 solver.cpp:312] Iteration 51600 (60.6111 iter/s, 1.64986s/100 iter), loss = 0.000746543
I0814 19:46:54.645232 24732 solver.cpp:334]     Train net output #0: loss = 0.000746378 (* 1 = 0.000746378 loss)
I0814 19:46:54.645237 24732 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0814 19:46:56.298024 24732 solver.cpp:312] Iteration 51700 (60.5047 iter/s, 1.65276s/100 iter), loss = 0.0202098
I0814 19:46:56.298173 24732 solver.cpp:334]     Train net output #0: loss = 0.0202096 (* 1 = 0.0202096 loss)
I0814 19:46:56.298193 24732 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0814 19:46:57.973520 24732 solver.cpp:312] Iteration 51800 (59.6857 iter/s, 1.67544s/100 iter), loss = 0.000722333
I0814 19:46:57.973549 24732 solver.cpp:334]     Train net output #0: loss = 0.00072217 (* 1 = 0.00072217 loss)
I0814 19:46:57.973556 24732 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0814 19:46:59.608155 24732 solver.cpp:312] Iteration 51900 (61.1777 iter/s, 1.63458s/100 iter), loss = 0.000391087
I0814 19:46:59.608177 24732 solver.cpp:334]     Train net output #0: loss = 0.000390924 (* 1 = 0.000390924 loss)
I0814 19:46:59.608183 24732 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0814 19:47:01.237748 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:01.239367 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:01.239377 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:01.239392 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:01.239398 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:01.239403 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:01.239410 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:01.239415 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:01.239418 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:01.239426 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:01.239430 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:01.239434 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:01.239441 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:01.239450 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:01.239461 24732 solver.cpp:509] Iteration 52000, Testing net (#0)
I0814 19:47:01.962494 24730 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:47:02.059015 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.901766
I0814 19:47:02.059034 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995
I0814 19:47:02.059039 24732 solver.cpp:594]     Test net output #2: loss = 0.415919 (* 1 = 0.415919 loss)
I0814 19:47:02.059056 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.819566s
I0814 19:47:02.076151 24732 solver.cpp:312] Iteration 52000 (40.5199 iter/s, 2.46792s/100 iter), loss = 0.000158553
I0814 19:47:02.076184 24732 solver.cpp:334]     Train net output #0: loss = 0.00015839 (* 1 = 0.00015839 loss)
I0814 19:47:02.076189 24732 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0814 19:47:03.753038 24732 solver.cpp:312] Iteration 52100 (59.6363 iter/s, 1.67683s/100 iter), loss = 0.000877061
I0814 19:47:03.753067 24732 solver.cpp:334]     Train net output #0: loss = 0.000876898 (* 1 = 0.000876898 loss)
I0814 19:47:03.753072 24732 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0814 19:47:05.421377 24732 solver.cpp:312] Iteration 52200 (59.9418 iter/s, 1.66829s/100 iter), loss = 0.00200717
I0814 19:47:05.421443 24732 solver.cpp:334]     Train net output #0: loss = 0.00200701 (* 1 = 0.00200701 loss)
I0814 19:47:05.421470 24732 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0814 19:47:07.104806 24732 solver.cpp:312] Iteration 52300 (59.4046 iter/s, 1.68337s/100 iter), loss = 0.00172062
I0814 19:47:07.104833 24732 solver.cpp:334]     Train net output #0: loss = 0.00172045 (* 1 = 0.00172045 loss)
I0814 19:47:07.104840 24732 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0814 19:47:08.771315 24732 solver.cpp:312] Iteration 52400 (60.0076 iter/s, 1.66646s/100 iter), loss = 0.000248581
I0814 19:47:08.771339 24732 solver.cpp:334]     Train net output #0: loss = 0.000248418 (* 1 = 0.000248418 loss)
I0814 19:47:08.771344 24732 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0814 19:47:10.403455 24732 solver.cpp:312] Iteration 52500 (61.2712 iter/s, 1.63209s/100 iter), loss = 0.000985062
I0814 19:47:10.403486 24732 solver.cpp:334]     Train net output #0: loss = 0.000984899 (* 1 = 0.000984899 loss)
I0814 19:47:10.403492 24732 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0814 19:47:12.045589 24732 solver.cpp:312] Iteration 52600 (60.8983 iter/s, 1.64208s/100 iter), loss = 0.000672462
I0814 19:47:12.045636 24732 solver.cpp:334]     Train net output #0: loss = 0.000672296 (* 1 = 0.000672296 loss)
I0814 19:47:12.045650 24732 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0814 19:47:13.696193 24732 solver.cpp:312] Iteration 52700 (60.5858 iter/s, 1.65055s/100 iter), loss = 0.000433707
I0814 19:47:13.696220 24732 solver.cpp:334]     Train net output #0: loss = 0.00043354 (* 1 = 0.00043354 loss)
I0814 19:47:13.696226 24732 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0814 19:47:15.352946 24732 solver.cpp:312] Iteration 52800 (60.3609 iter/s, 1.6567s/100 iter), loss = 0.00139823
I0814 19:47:15.353204 24732 solver.cpp:334]     Train net output #0: loss = 0.00139806 (* 1 = 0.00139806 loss)
I0814 19:47:15.353210 24732 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0814 19:47:17.011152 24732 solver.cpp:312] Iteration 52900 (60.3082 iter/s, 1.65815s/100 iter), loss = 0.000619081
I0814 19:47:17.011176 24732 solver.cpp:334]     Train net output #0: loss = 0.000618915 (* 1 = 0.000618915 loss)
I0814 19:47:17.011180 24732 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0814 19:47:18.617033 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:18.618651 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:18.618659 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:18.618664 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:18.618666 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:18.618670 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:18.618674 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:18.618676 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:18.618680 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:18.618690 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:18.618693 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:18.618697 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:18.618700 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:18.618703 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:18.618716 24732 solver.cpp:509] Iteration 53000, Testing net (#0)
I0814 19:47:19.434378 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911472
I0814 19:47:19.434399 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995294
I0814 19:47:19.434406 24732 solver.cpp:594]     Test net output #2: loss = 0.396804 (* 1 = 0.396804 loss)
I0814 19:47:19.434422 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815677s
I0814 19:47:19.450292 24732 solver.cpp:312] Iteration 53000 (40.9993 iter/s, 2.43907s/100 iter), loss = 0.000566136
I0814 19:47:19.450310 24732 solver.cpp:334]     Train net output #0: loss = 0.000565971 (* 1 = 0.000565971 loss)
I0814 19:47:19.450317 24732 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0814 19:47:21.083546 24732 solver.cpp:312] Iteration 53100 (61.2294 iter/s, 1.6332s/100 iter), loss = 0.00628255
I0814 19:47:21.083595 24732 solver.cpp:334]     Train net output #0: loss = 0.00628238 (* 1 = 0.00628238 loss)
I0814 19:47:21.083607 24732 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0814 19:47:22.775871 24732 solver.cpp:312] Iteration 53200 (59.0922 iter/s, 1.69227s/100 iter), loss = 0.000972055
I0814 19:47:22.775895 24732 solver.cpp:334]     Train net output #0: loss = 0.000971889 (* 1 = 0.000971889 loss)
I0814 19:47:22.775902 24732 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0814 19:47:24.406796 24732 solver.cpp:312] Iteration 53300 (61.3168 iter/s, 1.63087s/100 iter), loss = 0.000354552
I0814 19:47:24.406859 24732 solver.cpp:334]     Train net output #0: loss = 0.000354386 (* 1 = 0.000354386 loss)
I0814 19:47:24.406878 24732 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0814 19:47:26.058940 24732 solver.cpp:312] Iteration 53400 (60.5293 iter/s, 1.65209s/100 iter), loss = 0.000241542
I0814 19:47:26.058965 24732 solver.cpp:334]     Train net output #0: loss = 0.000241375 (* 1 = 0.000241375 loss)
I0814 19:47:26.058970 24732 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0814 19:47:27.696969 24732 solver.cpp:312] Iteration 53500 (61.051 iter/s, 1.63798s/100 iter), loss = 0.00137958
I0814 19:47:27.696992 24732 solver.cpp:334]     Train net output #0: loss = 0.00137942 (* 1 = 0.00137942 loss)
I0814 19:47:27.696996 24732 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0814 19:47:29.341034 24732 solver.cpp:312] Iteration 53600 (60.8268 iter/s, 1.64401s/100 iter), loss = 0.00105503
I0814 19:47:29.341081 24732 solver.cpp:334]     Train net output #0: loss = 0.00105487 (* 1 = 0.00105487 loss)
I0814 19:47:29.341094 24732 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0814 19:47:31.004618 24732 solver.cpp:312] Iteration 53700 (60.113 iter/s, 1.66353s/100 iter), loss = 0.000137793
I0814 19:47:31.004680 24732 solver.cpp:334]     Train net output #0: loss = 0.000137628 (* 1 = 0.000137628 loss)
I0814 19:47:31.004699 24732 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0814 19:47:32.705603 24732 solver.cpp:312] Iteration 53800 (58.7914 iter/s, 1.70093s/100 iter), loss = 0.00058894
I0814 19:47:32.705627 24732 solver.cpp:334]     Train net output #0: loss = 0.000588777 (* 1 = 0.000588777 loss)
I0814 19:47:32.705657 24732 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0814 19:47:34.350071 24732 solver.cpp:312] Iteration 53900 (60.8119 iter/s, 1.64442s/100 iter), loss = 0.00302605
I0814 19:47:34.350098 24732 solver.cpp:334]     Train net output #0: loss = 0.00302588 (* 1 = 0.00302588 loss)
I0814 19:47:34.350106 24732 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0814 19:47:35.992545 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:35.994345 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:35.994355 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:35.994364 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:35.994369 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:35.994374 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:35.994379 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:35.994382 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:35.994386 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:35.994390 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:35.994395 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:35.994400 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:35.994403 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:35.994408 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:35.994419 24732 solver.cpp:509] Iteration 54000, Testing net (#0)
I0814 19:47:36.809983 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.911766
I0814 19:47:36.810003 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:47:36.810009 24732 solver.cpp:594]     Test net output #2: loss = 0.380741 (* 1 = 0.380741 loss)
I0814 19:47:36.810027 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815579s
I0814 19:47:36.828711 24732 solver.cpp:312] Iteration 54000 (40.3459 iter/s, 2.47856s/100 iter), loss = 0.000415275
I0814 19:47:36.828727 24732 solver.cpp:334]     Train net output #0: loss = 0.000415111 (* 1 = 0.000415111 loss)
I0814 19:47:36.828732 24732 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0814 19:47:38.450276 24732 solver.cpp:312] Iteration 54100 (61.6708 iter/s, 1.62151s/100 iter), loss = 0.000484623
I0814 19:47:38.450332 24732 solver.cpp:334]     Train net output #0: loss = 0.000484459 (* 1 = 0.000484459 loss)
I0814 19:47:38.450346 24732 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0814 19:47:40.112210 24732 solver.cpp:312] Iteration 54200 (60.1729 iter/s, 1.66188s/100 iter), loss = 0.000882286
I0814 19:47:40.112232 24732 solver.cpp:334]     Train net output #0: loss = 0.000882122 (* 1 = 0.000882122 loss)
I0814 19:47:40.112238 24732 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0814 19:47:41.756743 24732 solver.cpp:312] Iteration 54300 (60.8094 iter/s, 1.64448s/100 iter), loss = 0.00134849
I0814 19:47:41.756773 24732 solver.cpp:334]     Train net output #0: loss = 0.00134833 (* 1 = 0.00134833 loss)
I0814 19:47:41.756780 24732 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0814 19:47:43.399646 24732 solver.cpp:312] Iteration 54400 (60.8699 iter/s, 1.64285s/100 iter), loss = 0.001735
I0814 19:47:43.399668 24732 solver.cpp:334]     Train net output #0: loss = 0.00173484 (* 1 = 0.00173484 loss)
I0814 19:47:43.399672 24732 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0814 19:47:45.031991 24732 solver.cpp:312] Iteration 54500 (61.2635 iter/s, 1.63229s/100 iter), loss = 0.00100654
I0814 19:47:45.032024 24732 solver.cpp:334]     Train net output #0: loss = 0.00100638 (* 1 = 0.00100638 loss)
I0814 19:47:45.032032 24732 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0814 19:47:46.696625 24732 solver.cpp:312] Iteration 54600 (60.0751 iter/s, 1.66458s/100 iter), loss = 0.000652223
I0814 19:47:46.696647 24732 solver.cpp:334]     Train net output #0: loss = 0.000652057 (* 1 = 0.000652057 loss)
I0814 19:47:46.696668 24732 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0814 19:47:48.337177 24732 solver.cpp:312] Iteration 54700 (60.9571 iter/s, 1.6405s/100 iter), loss = 0.00235922
I0814 19:47:48.337357 24732 solver.cpp:334]     Train net output #0: loss = 0.00235906 (* 1 = 0.00235906 loss)
I0814 19:47:48.337448 24732 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0814 19:47:49.984449 24732 solver.cpp:312] Iteration 54800 (60.7083 iter/s, 1.64722s/100 iter), loss = 0.00113437
I0814 19:47:49.984589 24732 solver.cpp:334]     Train net output #0: loss = 0.0011342 (* 1 = 0.0011342 loss)
I0814 19:47:49.984611 24732 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0814 19:47:51.659538 24732 solver.cpp:312] Iteration 54900 (59.7002 iter/s, 1.67504s/100 iter), loss = 0.000390641
I0814 19:47:51.659561 24732 solver.cpp:334]     Train net output #0: loss = 0.000390475 (* 1 = 0.000390475 loss)
I0814 19:47:51.659567 24732 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0814 19:47:53.313954 24732 solver.cpp:363] Sparsity after update:
I0814 19:47:53.315578 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:47:53.315587 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:47:53.315594 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:47:53.315598 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:47:53.315603 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:47:53.315608 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:47:53.315613 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:47:53.315615 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:47:53.315618 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:47:53.315623 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:47:53.315626 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:47:53.315630 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:47:53.315634 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:47:53.315644 24732 solver.cpp:509] Iteration 55000, Testing net (#0)
I0814 19:47:54.147377 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.919413
I0814 19:47:54.147397 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997059
I0814 19:47:54.147405 24732 solver.cpp:594]     Test net output #2: loss = 0.3577 (* 1 = 0.3577 loss)
I0814 19:47:54.147423 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.83175s
I0814 19:47:54.163492 24732 solver.cpp:312] Iteration 55000 (39.938 iter/s, 2.50388s/100 iter), loss = 0.000987634
I0814 19:47:54.163525 24732 solver.cpp:334]     Train net output #0: loss = 0.000987467 (* 1 = 0.000987467 loss)
I0814 19:47:54.163537 24732 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0814 19:47:55.813465 24732 solver.cpp:312] Iteration 55100 (60.6091 iter/s, 1.64992s/100 iter), loss = 0.00117576
I0814 19:47:55.813531 24732 solver.cpp:334]     Train net output #0: loss = 0.00117559 (* 1 = 0.00117559 loss)
I0814 19:47:55.813551 24732 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0814 19:47:57.434715 24732 solver.cpp:312] Iteration 55200 (61.6828 iter/s, 1.6212s/100 iter), loss = 0.00109533
I0814 19:47:57.434739 24732 solver.cpp:334]     Train net output #0: loss = 0.00109516 (* 1 = 0.00109516 loss)
I0814 19:47:57.434744 24732 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0814 19:47:59.115281 24732 solver.cpp:312] Iteration 55300 (59.5056 iter/s, 1.68051s/100 iter), loss = 0.000657295
I0814 19:47:59.115305 24732 solver.cpp:334]     Train net output #0: loss = 0.000657126 (* 1 = 0.000657126 loss)
I0814 19:47:59.115311 24732 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0814 19:48:00.773033 24732 solver.cpp:312] Iteration 55400 (60.3246 iter/s, 1.6577s/100 iter), loss = 0.00136124
I0814 19:48:00.773084 24732 solver.cpp:334]     Train net output #0: loss = 0.00136107 (* 1 = 0.00136107 loss)
I0814 19:48:00.773099 24732 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0814 19:48:02.405591 24732 solver.cpp:312] Iteration 55500 (61.2555 iter/s, 1.63251s/100 iter), loss = 0.0014403
I0814 19:48:02.405659 24732 solver.cpp:334]     Train net output #0: loss = 0.00144013 (* 1 = 0.00144013 loss)
I0814 19:48:02.405678 24732 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0814 19:48:04.053086 24732 solver.cpp:312] Iteration 55600 (60.7002 iter/s, 1.64744s/100 iter), loss = 0.000569539
I0814 19:48:04.053231 24732 solver.cpp:334]     Train net output #0: loss = 0.000569371 (* 1 = 0.000569371 loss)
I0814 19:48:04.053267 24732 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0814 19:48:05.686116 24732 solver.cpp:312] Iteration 55700 (61.2378 iter/s, 1.63298s/100 iter), loss = 0.000291419
I0814 19:48:05.686139 24732 solver.cpp:334]     Train net output #0: loss = 0.000291249 (* 1 = 0.000291249 loss)
I0814 19:48:05.686144 24732 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0814 19:48:07.356180 24732 solver.cpp:312] Iteration 55800 (59.8798 iter/s, 1.67001s/100 iter), loss = 0.0015388
I0814 19:48:07.356205 24732 solver.cpp:334]     Train net output #0: loss = 0.00153863 (* 1 = 0.00153863 loss)
I0814 19:48:07.356211 24732 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0814 19:48:09.024989 24732 solver.cpp:312] Iteration 55900 (59.9248 iter/s, 1.66876s/100 iter), loss = 0.000442932
I0814 19:48:09.025012 24732 solver.cpp:334]     Train net output #0: loss = 0.000442761 (* 1 = 0.000442761 loss)
I0814 19:48:09.025017 24732 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0814 19:48:10.624259 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:10.625927 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:10.625937 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:10.625944 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:10.625948 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:10.625952 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:10.625954 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:10.625965 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:10.625977 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:10.625984 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:10.625993 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:10.626000 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:10.626009 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:10.626019 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:10.626036 24732 solver.cpp:509] Iteration 56000, Testing net (#0)
I0814 19:48:11.441112 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.917942
I0814 19:48:11.441131 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997941
I0814 19:48:11.441136 24732 solver.cpp:594]     Test net output #2: loss = 0.342924 (* 1 = 0.342924 loss)
I0814 19:48:11.441150 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.815087s
I0814 19:48:11.457454 24732 solver.cpp:312] Iteration 56000 (41.1118 iter/s, 2.43239s/100 iter), loss = 0.00036006
I0814 19:48:11.457470 24732 solver.cpp:334]     Train net output #0: loss = 0.000359889 (* 1 = 0.000359889 loss)
I0814 19:48:11.457476 24732 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0814 19:48:12.395932 24716 data_reader.cpp:288] Starting prefetch of epoch 7
I0814 19:48:13.123493 24732 solver.cpp:312] Iteration 56100 (60.0246 iter/s, 1.66598s/100 iter), loss = 0.00147961
I0814 19:48:13.123517 24732 solver.cpp:334]     Train net output #0: loss = 0.00147944 (* 1 = 0.00147944 loss)
I0814 19:48:13.123523 24732 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0814 19:48:14.772747 24732 solver.cpp:312] Iteration 56200 (60.6355 iter/s, 1.6492s/100 iter), loss = 0.00152217
I0814 19:48:14.772776 24732 solver.cpp:334]     Train net output #0: loss = 0.001522 (* 1 = 0.001522 loss)
I0814 19:48:14.772783 24732 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0814 19:48:16.403604 24732 solver.cpp:312] Iteration 56300 (61.3193 iter/s, 1.63081s/100 iter), loss = 0.000724058
I0814 19:48:16.403657 24732 solver.cpp:334]     Train net output #0: loss = 0.000723888 (* 1 = 0.000723888 loss)
I0814 19:48:16.403671 24732 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0814 19:48:18.049775 24732 solver.cpp:312] Iteration 56400 (60.7489 iter/s, 1.64612s/100 iter), loss = 0.00254141
I0814 19:48:18.049938 24732 solver.cpp:334]     Train net output #0: loss = 0.00254124 (* 1 = 0.00254124 loss)
I0814 19:48:18.049976 24732 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0814 19:48:19.721666 24732 solver.cpp:312] Iteration 56500 (59.8145 iter/s, 1.67184s/100 iter), loss = 0.00216156
I0814 19:48:19.721688 24732 solver.cpp:334]     Train net output #0: loss = 0.00216139 (* 1 = 0.00216139 loss)
I0814 19:48:19.721693 24732 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0814 19:48:21.347795 24732 solver.cpp:312] Iteration 56600 (61.4977 iter/s, 1.62608s/100 iter), loss = 0.00169523
I0814 19:48:21.347893 24732 solver.cpp:334]     Train net output #0: loss = 0.00169506 (* 1 = 0.00169506 loss)
I0814 19:48:21.347909 24732 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0814 19:48:22.968783 24732 solver.cpp:312] Iteration 56700 (61.6927 iter/s, 1.62094s/100 iter), loss = 0.000410143
I0814 19:48:22.968835 24732 solver.cpp:334]     Train net output #0: loss = 0.000409973 (* 1 = 0.000409973 loss)
I0814 19:48:22.968849 24732 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0814 19:48:24.634289 24732 solver.cpp:312] Iteration 56800 (60.0438 iter/s, 1.66545s/100 iter), loss = 0.000460702
I0814 19:48:24.634315 24732 solver.cpp:334]     Train net output #0: loss = 0.000460532 (* 1 = 0.000460532 loss)
I0814 19:48:24.634320 24732 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0814 19:48:26.263267 24732 solver.cpp:312] Iteration 56900 (61.3902 iter/s, 1.62893s/100 iter), loss = 0.000387435
I0814 19:48:26.263290 24732 solver.cpp:334]     Train net output #0: loss = 0.000387265 (* 1 = 0.000387265 loss)
I0814 19:48:26.263296 24732 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0814 19:48:27.871809 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:27.873345 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:27.873354 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:27.873363 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:27.873366 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:27.873379 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:27.873389 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:27.873397 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:27.873402 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:27.873406 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:27.873414 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:27.873422 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:27.873427 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:27.873435 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:27.873452 24732 solver.cpp:509] Iteration 57000, Testing net (#0)
I0814 19:48:28.702396 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.913824
I0814 19:48:28.702414 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.997647
I0814 19:48:28.702419 24732 solver.cpp:594]     Test net output #2: loss = 0.356537 (* 1 = 0.356537 loss)
I0814 19:48:28.702433 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.828954s
I0814 19:48:28.718617 24732 solver.cpp:312] Iteration 57000 (40.7286 iter/s, 2.45528s/100 iter), loss = 0.000756286
I0814 19:48:28.718644 24732 solver.cpp:334]     Train net output #0: loss = 0.000756116 (* 1 = 0.000756116 loss)
I0814 19:48:28.718648 24732 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0814 19:48:30.344925 24732 solver.cpp:312] Iteration 57100 (61.4909 iter/s, 1.62626s/100 iter), loss = 0.00110375
I0814 19:48:30.344950 24732 solver.cpp:334]     Train net output #0: loss = 0.00110358 (* 1 = 0.00110358 loss)
I0814 19:48:30.344956 24732 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0814 19:48:31.963325 24732 solver.cpp:312] Iteration 57200 (61.7914 iter/s, 1.61835s/100 iter), loss = 0.00184722
I0814 19:48:31.963349 24732 solver.cpp:334]     Train net output #0: loss = 0.00184705 (* 1 = 0.00184705 loss)
I0814 19:48:31.963356 24732 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0814 19:48:33.585654 24732 solver.cpp:312] Iteration 57300 (61.6417 iter/s, 1.62228s/100 iter), loss = 0.00252326
I0814 19:48:33.585717 24732 solver.cpp:334]     Train net output #0: loss = 0.00252308 (* 1 = 0.00252308 loss)
I0814 19:48:33.585737 24732 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0814 19:48:35.215507 24732 solver.cpp:312] Iteration 57400 (61.3573 iter/s, 1.6298s/100 iter), loss = 0.000894318
I0814 19:48:35.215530 24732 solver.cpp:334]     Train net output #0: loss = 0.000894147 (* 1 = 0.000894147 loss)
I0814 19:48:35.215561 24732 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0814 19:48:36.879954 24732 solver.cpp:312] Iteration 57500 (60.082 iter/s, 1.66439s/100 iter), loss = 0.000937822
I0814 19:48:36.879987 24732 solver.cpp:334]     Train net output #0: loss = 0.000937651 (* 1 = 0.000937651 loss)
I0814 19:48:36.879994 24732 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0814 19:48:38.517138 24732 solver.cpp:312] Iteration 57600 (61.0824 iter/s, 1.63713s/100 iter), loss = 0.00163974
I0814 19:48:38.517163 24732 solver.cpp:334]     Train net output #0: loss = 0.00163956 (* 1 = 0.00163956 loss)
I0814 19:48:38.517168 24732 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0814 19:48:40.184813 24732 solver.cpp:312] Iteration 57700 (59.9656 iter/s, 1.66762s/100 iter), loss = 0.000532953
I0814 19:48:40.184839 24732 solver.cpp:334]     Train net output #0: loss = 0.00053278 (* 1 = 0.00053278 loss)
I0814 19:48:40.184844 24732 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0814 19:48:41.850992 24732 solver.cpp:312] Iteration 57800 (60.0194 iter/s, 1.66613s/100 iter), loss = 0.000564468
I0814 19:48:41.851233 24732 solver.cpp:334]     Train net output #0: loss = 0.000564295 (* 1 = 0.000564295 loss)
I0814 19:48:41.851238 24732 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0814 19:48:43.519412 24732 solver.cpp:312] Iteration 57900 (59.9389 iter/s, 1.66837s/100 iter), loss = 0.000786859
I0814 19:48:43.519474 24732 solver.cpp:334]     Train net output #0: loss = 0.000786685 (* 1 = 0.000786685 loss)
I0814 19:48:43.519495 24732 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0814 19:48:45.173496 24732 solver.cpp:363] Sparsity after update:
I0814 19:48:45.175148 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:48:45.175158 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:48:45.175165 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:48:45.175169 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:48:45.175179 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:48:45.175189 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:48:45.175197 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:48:45.175206 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:48:45.175215 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:48:45.175222 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:48:45.175231 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:48:45.175240 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:48:45.175251 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:48:45.175267 24732 solver.cpp:509] Iteration 58000, Testing net (#0)
I0814 19:48:45.995615 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907354
I0814 19:48:45.995632 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:48:45.995640 24732 solver.cpp:594]     Test net output #2: loss = 0.397232 (* 1 = 0.397232 loss)
I0814 19:48:45.995761 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.82036s
I0814 19:48:46.011548 24732 solver.cpp:312] Iteration 58000 (40.1274 iter/s, 2.49206s/100 iter), loss = 0.00265861
I0814 19:48:46.011564 24732 solver.cpp:334]     Train net output #0: loss = 0.00265843 (* 1 = 0.00265843 loss)
I0814 19:48:46.011569 24732 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0814 19:48:47.663524 24732 solver.cpp:312] Iteration 58100 (60.5356 iter/s, 1.65192s/100 iter), loss = 0.00976163
I0814 19:48:47.663555 24732 solver.cpp:334]     Train net output #0: loss = 0.00976146 (* 1 = 0.00976146 loss)
I0814 19:48:47.663563 24732 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0814 19:48:49.290735 24732 solver.cpp:312] Iteration 58200 (61.4569 iter/s, 1.62716s/100 iter), loss = 0.00060559
I0814 19:48:49.290802 24732 solver.cpp:334]     Train net output #0: loss = 0.000605417 (* 1 = 0.000605417 loss)
I0814 19:48:49.290822 24732 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0814 19:48:50.924865 24732 solver.cpp:312] Iteration 58300 (61.1966 iter/s, 1.63408s/100 iter), loss = 0.000936027
I0814 19:48:50.924893 24732 solver.cpp:334]     Train net output #0: loss = 0.000935853 (* 1 = 0.000935853 loss)
I0814 19:48:50.924901 24732 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0814 19:48:52.615259 24732 solver.cpp:312] Iteration 58400 (59.1596 iter/s, 1.69034s/100 iter), loss = 0.000307712
I0814 19:48:52.615329 24732 solver.cpp:334]     Train net output #0: loss = 0.000307538 (* 1 = 0.000307538 loss)
I0814 19:48:52.615334 24732 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0814 19:48:54.279206 24732 solver.cpp:312] Iteration 58500 (60.0999 iter/s, 1.6639s/100 iter), loss = 0.00497031
I0814 19:48:54.279266 24732 solver.cpp:334]     Train net output #0: loss = 0.00497014 (* 1 = 0.00497014 loss)
I0814 19:48:54.279284 24732 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0814 19:48:55.901024 24732 solver.cpp:312] Iteration 58600 (61.6611 iter/s, 1.62177s/100 iter), loss = 0.000380038
I0814 19:48:55.901075 24732 solver.cpp:334]     Train net output #0: loss = 0.000379865 (* 1 = 0.000379865 loss)
I0814 19:48:55.901089 24732 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0814 19:48:57.562435 24732 solver.cpp:312] Iteration 58700 (60.1918 iter/s, 1.66136s/100 iter), loss = 0.000670551
I0814 19:48:57.562466 24732 solver.cpp:334]     Train net output #0: loss = 0.000670378 (* 1 = 0.000670378 loss)
I0814 19:48:57.562472 24732 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0814 19:48:59.216187 24732 solver.cpp:312] Iteration 58800 (60.4704 iter/s, 1.6537s/100 iter), loss = 0.00192245
I0814 19:48:59.216215 24732 solver.cpp:334]     Train net output #0: loss = 0.00192227 (* 1 = 0.00192227 loss)
I0814 19:48:59.216222 24732 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0814 19:49:00.892971 24732 solver.cpp:312] Iteration 58900 (59.6398 iter/s, 1.67673s/100 iter), loss = 0.00160144
I0814 19:49:00.893020 24732 solver.cpp:334]     Train net output #0: loss = 0.00160127 (* 1 = 0.00160127 loss)
I0814 19:49:00.893035 24732 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0814 19:49:02.492782 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:02.494509 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:02.494518 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:02.494524 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:02.494525 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:02.494527 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:02.494529 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:02.494531 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:02.494534 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:02.494535 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:02.494539 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:02.494542 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:02.494546 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:02.494549 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:02.494560 24732 solver.cpp:509] Iteration 59000, Testing net (#0)
I0814 19:49:03.319313 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.907354
I0814 19:49:03.319331 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995882
I0814 19:49:03.319336 24732 solver.cpp:594]     Test net output #2: loss = 0.385154 (* 1 = 0.385154 loss)
I0814 19:49:03.319351 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.824763s
I0814 19:49:03.335222 24732 solver.cpp:312] Iteration 59000 (40.9471 iter/s, 2.44218s/100 iter), loss = 0.00155257
I0814 19:49:03.335238 24732 solver.cpp:334]     Train net output #0: loss = 0.0015524 (* 1 = 0.0015524 loss)
I0814 19:49:03.335243 24732 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0814 19:49:04.977433 24732 solver.cpp:312] Iteration 59100 (60.8955 iter/s, 1.64216s/100 iter), loss = 0.00065454
I0814 19:49:04.977675 24732 solver.cpp:334]     Train net output #0: loss = 0.000654368 (* 1 = 0.000654368 loss)
I0814 19:49:04.977679 24732 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0814 19:49:06.622251 24732 solver.cpp:312] Iteration 59200 (60.799 iter/s, 1.64476s/100 iter), loss = 0.00146386
I0814 19:49:06.622298 24732 solver.cpp:334]     Train net output #0: loss = 0.00146369 (* 1 = 0.00146369 loss)
I0814 19:49:06.622325 24732 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0814 19:49:08.287073 24732 solver.cpp:312] Iteration 59300 (60.0683 iter/s, 1.66477s/100 iter), loss = 0.000430182
I0814 19:49:08.287097 24732 solver.cpp:334]     Train net output #0: loss = 0.00043001 (* 1 = 0.00043001 loss)
I0814 19:49:08.287101 24732 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0814 19:49:09.972558 24732 solver.cpp:312] Iteration 59400 (59.332 iter/s, 1.68543s/100 iter), loss = 0.00262515
I0814 19:49:09.972618 24732 solver.cpp:334]     Train net output #0: loss = 0.00262498 (* 1 = 0.00262498 loss)
I0814 19:49:09.972635 24732 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0814 19:49:11.654566 24732 solver.cpp:312] Iteration 59500 (59.4547 iter/s, 1.68195s/100 iter), loss = 0.000638608
I0814 19:49:11.654589 24732 solver.cpp:334]     Train net output #0: loss = 0.000638436 (* 1 = 0.000638436 loss)
I0814 19:49:11.654594 24732 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0814 19:49:13.286159 24732 solver.cpp:312] Iteration 59600 (61.2918 iter/s, 1.63154s/100 iter), loss = 0.00144413
I0814 19:49:13.286206 24732 solver.cpp:334]     Train net output #0: loss = 0.00144396 (* 1 = 0.00144396 loss)
I0814 19:49:13.286221 24732 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0814 19:49:14.968873 24732 solver.cpp:312] Iteration 59700 (59.4297 iter/s, 1.68266s/100 iter), loss = 0.00114086
I0814 19:49:14.968899 24732 solver.cpp:334]     Train net output #0: loss = 0.00114069 (* 1 = 0.00114069 loss)
I0814 19:49:14.968905 24732 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0814 19:49:16.603477 24732 solver.cpp:312] Iteration 59800 (61.1789 iter/s, 1.63455s/100 iter), loss = 0.00365009
I0814 19:49:16.603505 24732 solver.cpp:334]     Train net output #0: loss = 0.00364992 (* 1 = 0.00364992 loss)
I0814 19:49:16.603513 24732 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0814 19:49:18.276494 24732 solver.cpp:312] Iteration 59900 (59.7741 iter/s, 1.67297s/100 iter), loss = 0.000228775
I0814 19:49:18.276571 24732 solver.cpp:334]     Train net output #0: loss = 0.000228603 (* 1 = 0.000228603 loss)
I0814 19:49:18.276592 24732 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0814 19:49:19.907140 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0814 19:49:19.915235 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0814 19:49:19.918814 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:19.920730 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:19.920742 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:19.920748 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:19.920753 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:19.920763 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:19.920773 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:19.920781 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:19.920789 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:19.920797 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:19.920805 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:19.920815 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:19.920825 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:19.920833 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:19.920851 24732 solver.cpp:509] Iteration 60000, Testing net (#0)
I0814 19:49:20.729743 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.905295
I0814 19:49:20.729759 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996471
I0814 19:49:20.729764 24732 solver.cpp:594]     Test net output #2: loss = 0.398874 (* 1 = 0.398874 loss)
I0814 19:49:20.729802 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.808922s
I0814 19:49:20.745714 24732 solver.cpp:312] Iteration 60000 (40.4998 iter/s, 2.46915s/100 iter), loss = 0.00259431
I0814 19:49:20.745730 24732 solver.cpp:334]     Train net output #0: loss = 0.00259414 (* 1 = 0.00259414 loss)
I0814 19:49:20.745736 24732 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0814 19:49:22.410923 24732 solver.cpp:312] Iteration 60100 (60.0545 iter/s, 1.66515s/100 iter), loss = 0.00137172
I0814 19:49:22.410950 24732 solver.cpp:334]     Train net output #0: loss = 0.00137154 (* 1 = 0.00137154 loss)
I0814 19:49:22.410956 24732 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0814 19:49:24.044682 24732 solver.cpp:312] Iteration 60200 (61.2104 iter/s, 1.63371s/100 iter), loss = 0.000442115
I0814 19:49:24.044766 24732 solver.cpp:334]     Train net output #0: loss = 0.000441942 (* 1 = 0.000441942 loss)
I0814 19:49:24.044775 24732 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0814 19:49:25.691916 24732 solver.cpp:312] Iteration 60300 (60.7098 iter/s, 1.64718s/100 iter), loss = 0.00176699
I0814 19:49:25.691943 24732 solver.cpp:334]     Train net output #0: loss = 0.00176682 (* 1 = 0.00176682 loss)
I0814 19:49:25.691949 24732 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0814 19:49:27.350850 24732 solver.cpp:312] Iteration 60400 (60.2817 iter/s, 1.65888s/100 iter), loss = 0.000311345
I0814 19:49:27.350877 24732 solver.cpp:334]     Train net output #0: loss = 0.000311172 (* 1 = 0.000311172 loss)
I0814 19:49:27.350883 24732 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0814 19:49:28.989075 24732 solver.cpp:312] Iteration 60500 (61.0436 iter/s, 1.63817s/100 iter), loss = 0.00100377
I0814 19:49:28.989100 24732 solver.cpp:334]     Train net output #0: loss = 0.0010036 (* 1 = 0.0010036 loss)
I0814 19:49:28.989105 24732 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0814 19:49:30.625948 24732 solver.cpp:312] Iteration 60600 (61.0939 iter/s, 1.63682s/100 iter), loss = 0.00261663
I0814 19:49:30.625973 24732 solver.cpp:334]     Train net output #0: loss = 0.00261646 (* 1 = 0.00261646 loss)
I0814 19:49:30.625979 24732 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0814 19:49:30.674412 24716 data_reader.cpp:288] Starting prefetch of epoch 8
I0814 19:49:32.297677 24732 solver.cpp:312] Iteration 60700 (59.8202 iter/s, 1.67168s/100 iter), loss = 0.00090154
I0814 19:49:32.297727 24732 solver.cpp:334]     Train net output #0: loss = 0.000901367 (* 1 = 0.000901367 loss)
I0814 19:49:32.297741 24732 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0814 19:49:33.992772 24732 solver.cpp:312] Iteration 60800 (58.9956 iter/s, 1.69504s/100 iter), loss = 0.00182894
I0814 19:49:33.992802 24732 solver.cpp:334]     Train net output #0: loss = 0.00182877 (* 1 = 0.00182877 loss)
I0814 19:49:33.992810 24732 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0814 19:49:35.642485 24732 solver.cpp:312] Iteration 60900 (60.6185 iter/s, 1.64966s/100 iter), loss = 0.00158305
I0814 19:49:35.642510 24732 solver.cpp:334]     Train net output #0: loss = 0.00158288 (* 1 = 0.00158288 loss)
I0814 19:49:35.642515 24732 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0814 19:49:37.268610 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:37.270051 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:37.270062 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:37.270069 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:37.270073 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:37.270078 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:37.270082 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:37.270087 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:37.270090 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:37.270094 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:37.270098 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:37.270102 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:37.270107 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:37.270109 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:37.270119 24732 solver.cpp:509] Iteration 61000, Testing net (#0)
I0814 19:49:38.096077 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906472
I0814 19:49:38.096094 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994706
I0814 19:49:38.096099 24732 solver.cpp:594]     Test net output #2: loss = 0.40549 (* 1 = 0.40549 loss)
I0814 19:49:38.096117 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.825969s
I0814 19:49:38.112051 24732 solver.cpp:312] Iteration 61000 (40.4941 iter/s, 2.46949s/100 iter), loss = 0.00127942
I0814 19:49:38.112083 24732 solver.cpp:334]     Train net output #0: loss = 0.00127924 (* 1 = 0.00127924 loss)
I0814 19:49:38.112089 24732 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0814 19:49:39.762760 24732 solver.cpp:312] Iteration 61100 (60.5819 iter/s, 1.65066s/100 iter), loss = 0.000589503
I0814 19:49:39.762781 24732 solver.cpp:334]     Train net output #0: loss = 0.000589332 (* 1 = 0.000589332 loss)
I0814 19:49:39.762787 24732 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0814 19:49:41.416812 24732 solver.cpp:312] Iteration 61200 (60.4596 iter/s, 1.654s/100 iter), loss = 0.00049441
I0814 19:49:41.416836 24732 solver.cpp:334]     Train net output #0: loss = 0.000494238 (* 1 = 0.000494238 loss)
I0814 19:49:41.416842 24732 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0814 19:49:43.094019 24732 solver.cpp:312] Iteration 61300 (59.6248 iter/s, 1.67715s/100 iter), loss = 0.000615211
I0814 19:49:43.094120 24732 solver.cpp:334]     Train net output #0: loss = 0.00061504 (* 1 = 0.00061504 loss)
I0814 19:49:43.094130 24732 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0814 19:49:44.740592 24732 solver.cpp:312] Iteration 61400 (60.7342 iter/s, 1.64652s/100 iter), loss = 0.00192736
I0814 19:49:44.740645 24732 solver.cpp:334]     Train net output #0: loss = 0.00192719 (* 1 = 0.00192719 loss)
I0814 19:49:44.740661 24732 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0814 19:49:46.397081 24732 solver.cpp:312] Iteration 61500 (60.3706 iter/s, 1.65644s/100 iter), loss = 0.00476211
I0814 19:49:46.397105 24732 solver.cpp:334]     Train net output #0: loss = 0.00476194 (* 1 = 0.00476194 loss)
I0814 19:49:46.397111 24732 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0814 19:49:48.028344 24732 solver.cpp:312] Iteration 61600 (61.3041 iter/s, 1.63121s/100 iter), loss = 0.00113627
I0814 19:49:48.028370 24732 solver.cpp:334]     Train net output #0: loss = 0.0011361 (* 1 = 0.0011361 loss)
I0814 19:49:48.028375 24732 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0814 19:49:49.669893 24732 solver.cpp:312] Iteration 61700 (60.9201 iter/s, 1.64149s/100 iter), loss = 0.000799573
I0814 19:49:49.669917 24732 solver.cpp:334]     Train net output #0: loss = 0.000799401 (* 1 = 0.000799401 loss)
I0814 19:49:49.669922 24732 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0814 19:49:51.330533 24732 solver.cpp:312] Iteration 61800 (60.2196 iter/s, 1.66059s/100 iter), loss = 0.000448428
I0814 19:49:51.330603 24732 solver.cpp:334]     Train net output #0: loss = 0.000448256 (* 1 = 0.000448256 loss)
I0814 19:49:51.330624 24732 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0814 19:49:52.993844 24732 solver.cpp:312] Iteration 61900 (60.1229 iter/s, 1.66326s/100 iter), loss = 0.000887192
I0814 19:49:52.993894 24732 solver.cpp:334]     Train net output #0: loss = 0.00088702 (* 1 = 0.00088702 loss)
I0814 19:49:52.993908 24732 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0814 19:49:54.636909 24732 solver.cpp:363] Sparsity after update:
I0814 19:49:54.638463 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:49:54.638471 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:49:54.638481 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:49:54.638485 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:49:54.638489 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:49:54.638492 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:49:54.638497 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:49:54.638500 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:49:54.638505 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:49:54.638509 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:49:54.638514 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:49:54.638516 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:49:54.638521 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:49:54.638532 24732 solver.cpp:509] Iteration 62000, Testing net (#0)
I0814 19:49:55.476330 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906178
I0814 19:49:55.476348 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.995294
I0814 19:49:55.476354 24732 solver.cpp:594]     Test net output #2: loss = 0.392083 (* 1 = 0.392083 loss)
I0814 19:49:55.476372 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.837811s
I0814 19:49:55.494695 24732 solver.cpp:312] Iteration 62000 (39.9876 iter/s, 2.50078s/100 iter), loss = 0.000428321
I0814 19:49:55.494711 24732 solver.cpp:334]     Train net output #0: loss = 0.000428148 (* 1 = 0.000428148 loss)
I0814 19:49:55.494716 24732 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0814 19:49:57.144325 24732 solver.cpp:312] Iteration 62100 (60.6217 iter/s, 1.64958s/100 iter), loss = 0.000326858
I0814 19:49:57.144372 24732 solver.cpp:334]     Train net output #0: loss = 0.000326687 (* 1 = 0.000326687 loss)
I0814 19:49:57.144385 24732 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0814 19:49:58.779141 24732 solver.cpp:312] Iteration 62200 (61.1709 iter/s, 1.63476s/100 iter), loss = 0.00236271
I0814 19:49:58.779189 24732 solver.cpp:334]     Train net output #0: loss = 0.00236254 (* 1 = 0.00236254 loss)
I0814 19:49:58.779204 24732 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0814 19:50:00.445168 24732 solver.cpp:312] Iteration 62300 (60.025 iter/s, 1.66597s/100 iter), loss = 0.00117782
I0814 19:50:00.445217 24732 solver.cpp:334]     Train net output #0: loss = 0.00117765 (* 1 = 0.00117765 loss)
I0814 19:50:00.445230 24732 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0814 19:50:02.125080 24732 solver.cpp:312] Iteration 62400 (59.5288 iter/s, 1.67986s/100 iter), loss = 0.000552193
I0814 19:50:02.125103 24732 solver.cpp:334]     Train net output #0: loss = 0.000552023 (* 1 = 0.000552023 loss)
I0814 19:50:02.125108 24732 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0814 19:50:03.789629 24732 solver.cpp:312] Iteration 62500 (60.0782 iter/s, 1.6645s/100 iter), loss = 0.000838799
I0814 19:50:03.789652 24732 solver.cpp:334]     Train net output #0: loss = 0.000838628 (* 1 = 0.000838628 loss)
I0814 19:50:03.789656 24732 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0814 19:50:05.468312 24732 solver.cpp:312] Iteration 62600 (59.5724 iter/s, 1.67863s/100 iter), loss = 0.0007496
I0814 19:50:05.468335 24732 solver.cpp:334]     Train net output #0: loss = 0.00074943 (* 1 = 0.00074943 loss)
I0814 19:50:05.468339 24732 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0814 19:50:07.097107 24732 solver.cpp:312] Iteration 62700 (61.397 iter/s, 1.62874s/100 iter), loss = 0.00127176
I0814 19:50:07.097134 24732 solver.cpp:334]     Train net output #0: loss = 0.00127159 (* 1 = 0.00127159 loss)
I0814 19:50:07.097141 24732 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0814 19:50:08.747892 24732 solver.cpp:312] Iteration 62800 (60.5891 iter/s, 1.65046s/100 iter), loss = 0.00196349
I0814 19:50:08.747949 24732 solver.cpp:334]     Train net output #0: loss = 0.00196332 (* 1 = 0.00196332 loss)
I0814 19:50:08.748013 24732 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0814 19:50:10.382938 24732 solver.cpp:312] Iteration 62900 (61.1621 iter/s, 1.635s/100 iter), loss = 0.000941342
I0814 19:50:10.382967 24732 solver.cpp:334]     Train net output #0: loss = 0.000941172 (* 1 = 0.000941172 loss)
I0814 19:50:10.382973 24732 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0814 19:50:12.064223 24732 solver.cpp:363] Sparsity after update:
I0814 19:50:12.065891 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:50:12.065901 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:50:12.065908 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:50:12.065920 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:50:12.065930 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:50:12.065940 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:50:12.065948 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:50:12.065956 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:50:12.065965 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:50:12.065974 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:50:12.065984 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:50:12.065992 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:50:12.066001 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:50:12.066020 24732 solver.cpp:509] Iteration 63000, Testing net (#0)
I0814 19:50:12.876919 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.906178
I0814 19:50:12.876938 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.996177
I0814 19:50:12.876943 24732 solver.cpp:594]     Test net output #2: loss = 0.391574 (* 1 = 0.391574 loss)
I0814 19:50:12.876960 24732 solver.cpp:264] [MultiGPU] Tests completed in 0.810913s
I0814 19:50:12.892778 24732 solver.cpp:312] Iteration 63000 (39.8443 iter/s, 2.50977s/100 iter), loss = 0.00362886
I0814 19:50:12.892794 24732 solver.cpp:334]     Train net output #0: loss = 0.0036287 (* 1 = 0.0036287 loss)
I0814 19:50:12.892801 24732 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0814 19:50:14.549154 24732 solver.cpp:312] Iteration 63100 (60.3747 iter/s, 1.65632s/100 iter), loss = 0.00053736
I0814 19:50:14.549235 24732 solver.cpp:334]     Train net output #0: loss = 0.000537191 (* 1 = 0.000537191 loss)
I0814 19:50:14.549243 24732 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0814 19:50:16.213310 24732 solver.cpp:312] Iteration 63200 (60.0925 iter/s, 1.6641s/100 iter), loss = 0.00127944
I0814 19:50:16.213340 24732 solver.cpp:334]     Train net output #0: loss = 0.00127927 (* 1 = 0.00127927 loss)
I0814 19:50:16.213574 24732 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0814 19:50:17.854145 24732 solver.cpp:312] Iteration 63300 (60.9465 iter/s, 1.64078s/100 iter), loss = 0.000680574
I0814 19:50:17.854173 24732 solver.cpp:334]     Train net output #0: loss = 0.000680404 (* 1 = 0.000680404 loss)
I0814 19:50:17.854180 24732 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0814 19:50:19.486371 24732 solver.cpp:312] Iteration 63400 (61.2679 iter/s, 1.63218s/100 iter), loss = 0.000565066
I0814 19:50:19.486420 24732 solver.cpp:334]     Train net output #0: loss = 0.000564896 (* 1 = 0.000564896 loss)
I0814 19:50:19.486434 24732 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0814 19:50:21.137411 24732 solver.cpp:312] Iteration 63500 (60.5699 iter/s, 1.65099s/100 iter), loss = 0.000795725
I0814 19:50:21.137467 24732 solver.cpp:334]     Train net output #0: loss = 0.000795555 (* 1 = 0.000795555 loss)
I0814 19:50:21.137483 24732 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0814 19:50:22.849174 24732 solver.cpp:312] Iteration 63600 (58.4211 iter/s, 1.71171s/100 iter), loss = 0.000704929
I0814 19:50:22.849200 24732 solver.cpp:334]     Train net output #0: loss = 0.000704759 (* 1 = 0.000704759 loss)
I0814 19:50:22.849223 24732 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0814 19:50:24.484115 24732 solver.cpp:312] Iteration 63700 (61.1662 iter/s, 1.63489s/100 iter), loss = 0.000765479
I0814 19:50:24.484150 24732 solver.cpp:334]     Train net output #0: loss = 0.000765308 (* 1 = 0.000765308 loss)
I0814 19:50:24.484158 24732 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0814 19:50:26.169351 24732 solver.cpp:312] Iteration 63800 (59.3407 iter/s, 1.68518s/100 iter), loss = 0.00073873
I0814 19:50:26.169436 24732 solver.cpp:334]     Train net output #0: loss = 0.000738558 (* 1 = 0.000738558 loss)
I0814 19:50:26.169443 24732 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0814 19:50:27.815717 24732 solver.cpp:312] Iteration 63900 (60.7418 iter/s, 1.64631s/100 iter), loss = 0.00074093
I0814 19:50:27.815740 24732 solver.cpp:334]     Train net output #0: loss = 0.000740759 (* 1 = 0.000740759 loss)
I0814 19:50:27.815745 24732 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0814 19:50:29.440295 24732 solver.cpp:312] Iteration 63999 (60.9408 iter/s, 1.62453s/99 iter), loss = 0.000398853
I0814 19:50:29.440321 24732 solver.cpp:334]     Train net output #0: loss = 0.000398682 (* 1 = 0.000398682 loss)
I0814 19:50:29.440327 24732 solver.cpp:363] Sparsity after update:
I0814 19:50:29.442262 24732 net.cpp:2183] Num Params(11), Sparsity (zero_weights/count): 
I0814 19:50:29.442270 24732 net.cpp:2192] conv1a_param_0(0.384) 
I0814 19:50:29.442276 24732 net.cpp:2192] conv1b_param_0(0.734) 
I0814 19:50:29.442278 24732 net.cpp:2192] fc10_param_0(0) 
I0814 19:50:29.442289 24732 net.cpp:2192] res2a_branch2a_param_0(0.803) 
I0814 19:50:29.442296 24732 net.cpp:2192] res2a_branch2b_param_0(0.705) 
I0814 19:50:29.442303 24732 net.cpp:2192] res3a_branch2a_param_0(0.807) 
I0814 19:50:29.442308 24732 net.cpp:2192] res3a_branch2b_param_0(0.747) 
I0814 19:50:29.442312 24732 net.cpp:2192] res4a_branch2a_param_0(0.819) 
I0814 19:50:29.442320 24732 net.cpp:2192] res4a_branch2b_param_0(0.813) 
I0814 19:50:29.442324 24732 net.cpp:2192] res5a_branch2a_param_0(0.787) 
I0814 19:50:29.442327 24732 net.cpp:2192] res5a_branch2b_param_0(0.819) 
I0814 19:50:29.442335 24732 net.cpp:2194] Total Sparsity (zero_weights/count) =  (1.88385e+06/2.3599e+06) 0.798
I0814 19:50:29.442389 24732 solver.cpp:639] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0814 19:50:29.450320 24732 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-14_18-39-46/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0814 19:50:29.458793 24732 solver.cpp:486] Iteration 64000, loss = 0.000476354
I0814 19:50:29.458817 24732 solver.cpp:509] Iteration 64000, Testing net (#0)
I0814 19:50:30.272869 24732 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.903236
I0814 19:50:30.272888 24732 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.994118
I0814 19:50:30.272894 24732 solver.cpp:594]     Test net output #2: loss = 0.417369 (* 1 = 0.417369 loss)
I0814 19:50:30.275840 24692 parallel.cpp:71] Root Solver performance on device 0: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275852 24692 parallel.cpp:76]      Solver performance on device 1: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275857 24692 parallel.cpp:76]      Solver performance on device 2: 31.14 * 22 = 685.2 img/sec (64000 itr in 2055 sec)
I0814 19:50:30.275858 24692 parallel.cpp:79] Overall multi-GPU performance: 2055.51 img/sec
I0814 19:50:30.348296 24692 caffe.cpp:247] Optimization Done in 34m 18s
I0814 19:50:31.172160 10755 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Aug 14 19:50:30 2017
I0814 19:50:31.173903 10755 caffe.cpp:611] CuDNN version: 6021
I0814 19:50:31.173908 10755 caffe.cpp:612] CuBLAS version: 8000
I0814 19:50:31.173910 10755 caffe.cpp:613] CUDA version: 8000
I0814 19:50:31.173913 10755 caffe.cpp:614] CUDA driver version: 8000
I0814 19:50:31.173928 10755 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0814 19:50:31.173935 10755 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0814 19:50:31.174499 10755 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0814 19:50:31.175046 10755 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0814 19:50:31.175052 10755 caffe.cpp:275] Use GPU with device ID 0
I0814 19:50:31.175374 10755 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0814 19:50:31.176645 10755 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0814 19:50:31.176750 10755 net.cpp:104] Using FLOAT as default forward math type
I0814 19:50:31.176755 10755 net.cpp:110] Using FLOAT as default backward math type
I0814 19:50:31.176759 10755 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0814 19:50:31.176761 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.176805 10755 net.cpp:184] Created Layer data (0)
I0814 19:50:31.176810 10755 net.cpp:530] data -> data
I0814 19:50:31.176818 10755 net.cpp:530] data -> label
I0814 19:50:31.176836 10755 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0814 19:50:31.177129 10755 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:31.183543 10778 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0814 19:50:31.184260 10755 data_layer.cpp:185] (0) ReshapePrefetch 50, 3, 32, 32
I0814 19:50:31.184300 10755 data_layer.cpp:209] (0) Output data size: 50, 3, 32, 32
I0814 19:50:31.184309 10755 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0814 19:50:31.184334 10755 net.cpp:245] Setting up data
I0814 19:50:31.184350 10755 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0814 19:50:31.184358 10755 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0814 19:50:31.184366 10755 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0814 19:50:31.184373 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.184386 10755 net.cpp:184] Created Layer label_data_1_split (1)
I0814 19:50:31.184392 10755 net.cpp:561] label_data_1_split <- label
I0814 19:50:31.184401 10755 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0814 19:50:31.184408 10755 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0814 19:50:31.184412 10755 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0814 19:50:31.184459 10755 net.cpp:245] Setting up label_data_1_split
I0814 19:50:31.184466 10755 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0814 19:50:31.184471 10755 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0814 19:50:31.184476 10755 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0814 19:50:31.184480 10755 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0814 19:50:31.184486 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.184497 10755 net.cpp:184] Created Layer data/bias (2)
I0814 19:50:31.184501 10755 net.cpp:561] data/bias <- data
I0814 19:50:31.184505 10755 net.cpp:530] data/bias -> data/bias
I0814 19:50:31.185703 10779 data_layer.cpp:97] (0) Parser threads: 1
I0814 19:50:31.185714 10779 data_layer.cpp:99] (0) Transformer threads: 1
I0814 19:50:31.186523 10755 net.cpp:245] Setting up data/bias
I0814 19:50:31.186539 10755 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0814 19:50:31.186553 10755 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0814 19:50:31.186558 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.186578 10755 net.cpp:184] Created Layer conv1a (3)
I0814 19:50:31.186583 10755 net.cpp:561] conv1a <- data/bias
I0814 19:50:31.186588 10755 net.cpp:530] conv1a -> conv1a
I0814 19:50:31.471671 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.15G, req 0G)
I0814 19:50:31.471690 10755 net.cpp:245] Setting up conv1a
I0814 19:50:31.471696 10755 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0814 19:50:31.471705 10755 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0814 19:50:31.471709 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.471720 10755 net.cpp:184] Created Layer conv1a/bn (4)
I0814 19:50:31.471724 10755 net.cpp:561] conv1a/bn <- conv1a
I0814 19:50:31.471727 10755 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0814 19:50:31.472170 10755 net.cpp:245] Setting up conv1a/bn
I0814 19:50:31.472177 10755 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0814 19:50:31.472184 10755 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0814 19:50:31.472187 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.472190 10755 net.cpp:184] Created Layer conv1a/relu (5)
I0814 19:50:31.472193 10755 net.cpp:561] conv1a/relu <- conv1a
I0814 19:50:31.472195 10755 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0814 19:50:31.472206 10755 net.cpp:245] Setting up conv1a/relu
I0814 19:50:31.472209 10755 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0814 19:50:31.472211 10755 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0814 19:50:31.472213 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.472221 10755 net.cpp:184] Created Layer conv1b (6)
I0814 19:50:31.472224 10755 net.cpp:561] conv1b <- conv1a
I0814 19:50:31.472228 10755 net.cpp:530] conv1b -> conv1b
I0814 19:50:31.476084 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.13G, req 0G)
I0814 19:50:31.476095 10755 net.cpp:245] Setting up conv1b
I0814 19:50:31.476099 10755 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0814 19:50:31.476104 10755 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0814 19:50:31.476106 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.476112 10755 net.cpp:184] Created Layer conv1b/bn (7)
I0814 19:50:31.476114 10755 net.cpp:561] conv1b/bn <- conv1b
I0814 19:50:31.476117 10755 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0814 19:50:31.476519 10755 net.cpp:245] Setting up conv1b/bn
I0814 19:50:31.476534 10755 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0814 19:50:31.476541 10755 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0814 19:50:31.476543 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.476547 10755 net.cpp:184] Created Layer conv1b/relu (8)
I0814 19:50:31.476549 10755 net.cpp:561] conv1b/relu <- conv1b
I0814 19:50:31.476552 10755 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0814 19:50:31.476557 10755 net.cpp:245] Setting up conv1b/relu
I0814 19:50:31.476558 10755 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0814 19:50:31.476562 10755 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0814 19:50:31.476563 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.476568 10755 net.cpp:184] Created Layer pool1 (9)
I0814 19:50:31.476570 10755 net.cpp:561] pool1 <- conv1b
I0814 19:50:31.476573 10755 net.cpp:530] pool1 -> pool1
I0814 19:50:31.476611 10755 net.cpp:245] Setting up pool1
I0814 19:50:31.476617 10755 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0814 19:50:31.476619 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0814 19:50:31.476621 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.476626 10755 net.cpp:184] Created Layer res2a_branch2a (10)
I0814 19:50:31.476629 10755 net.cpp:561] res2a_branch2a <- pool1
I0814 19:50:31.476631 10755 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0814 19:50:31.481863 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.11G, req 0G)
I0814 19:50:31.481873 10755 net.cpp:245] Setting up res2a_branch2a
I0814 19:50:31.481878 10755 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0814 19:50:31.481884 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:31.481885 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.481889 10755 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0814 19:50:31.481892 10755 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0814 19:50:31.481894 10755 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0814 19:50:31.482296 10755 net.cpp:245] Setting up res2a_branch2a/bn
I0814 19:50:31.482303 10755 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0814 19:50:31.482308 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0814 19:50:31.482312 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.482316 10755 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0814 19:50:31.482318 10755 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0814 19:50:31.482321 10755 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0814 19:50:31.482324 10755 net.cpp:245] Setting up res2a_branch2a/relu
I0814 19:50:31.482327 10755 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0814 19:50:31.482329 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0814 19:50:31.482332 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.482339 10755 net.cpp:184] Created Layer res2a_branch2b (13)
I0814 19:50:31.482342 10755 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0814 19:50:31.482344 10755 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0814 19:50:31.485611 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.09G, req 0G)
I0814 19:50:31.485621 10755 net.cpp:245] Setting up res2a_branch2b
I0814 19:50:31.485625 10755 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0814 19:50:31.485637 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:31.485641 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.485646 10755 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0814 19:50:31.485648 10755 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0814 19:50:31.485651 10755 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0814 19:50:31.486057 10755 net.cpp:245] Setting up res2a_branch2b/bn
I0814 19:50:31.486063 10755 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0814 19:50:31.486069 10755 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0814 19:50:31.486071 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.486074 10755 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0814 19:50:31.486076 10755 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0814 19:50:31.486078 10755 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0814 19:50:31.486081 10755 net.cpp:245] Setting up res2a_branch2b/relu
I0814 19:50:31.486084 10755 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0814 19:50:31.486086 10755 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0814 19:50:31.486088 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.486093 10755 net.cpp:184] Created Layer pool2 (16)
I0814 19:50:31.486094 10755 net.cpp:561] pool2 <- res2a_branch2b
I0814 19:50:31.486096 10755 net.cpp:530] pool2 -> pool2
I0814 19:50:31.486124 10755 net.cpp:245] Setting up pool2
I0814 19:50:31.486129 10755 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0814 19:50:31.486130 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0814 19:50:31.486133 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.486140 10755 net.cpp:184] Created Layer res3a_branch2a (17)
I0814 19:50:31.486141 10755 net.cpp:561] res3a_branch2a <- pool2
I0814 19:50:31.486143 10755 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0814 19:50:31.491309 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0G)
I0814 19:50:31.491319 10755 net.cpp:245] Setting up res3a_branch2a
I0814 19:50:31.491323 10755 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0814 19:50:31.491328 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:31.491329 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.491334 10755 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0814 19:50:31.491336 10755 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0814 19:50:31.491338 10755 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0814 19:50:31.491734 10755 net.cpp:245] Setting up res3a_branch2a/bn
I0814 19:50:31.491739 10755 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0814 19:50:31.491746 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0814 19:50:31.491750 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.491752 10755 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0814 19:50:31.491755 10755 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0814 19:50:31.491756 10755 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0814 19:50:31.491760 10755 net.cpp:245] Setting up res3a_branch2a/relu
I0814 19:50:31.491762 10755 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0814 19:50:31.491765 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0814 19:50:31.491766 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.491780 10755 net.cpp:184] Created Layer res3a_branch2b (20)
I0814 19:50:31.491782 10755 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0814 19:50:31.491785 10755 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0814 19:50:31.494822 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.07G, req 0G)
I0814 19:50:31.494832 10755 net.cpp:245] Setting up res3a_branch2b
I0814 19:50:31.494835 10755 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0814 19:50:31.494839 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:31.494841 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.494846 10755 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0814 19:50:31.494848 10755 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0814 19:50:31.494851 10755 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0814 19:50:31.495236 10755 net.cpp:245] Setting up res3a_branch2b/bn
I0814 19:50:31.495244 10755 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0814 19:50:31.495249 10755 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0814 19:50:31.495250 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.495254 10755 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0814 19:50:31.495255 10755 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0814 19:50:31.495259 10755 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0814 19:50:31.495261 10755 net.cpp:245] Setting up res3a_branch2b/relu
I0814 19:50:31.495263 10755 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0814 19:50:31.495265 10755 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0814 19:50:31.495267 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.495271 10755 net.cpp:184] Created Layer pool3 (23)
I0814 19:50:31.495273 10755 net.cpp:561] pool3 <- res3a_branch2b
I0814 19:50:31.495275 10755 net.cpp:530] pool3 -> pool3
I0814 19:50:31.495306 10755 net.cpp:245] Setting up pool3
I0814 19:50:31.495309 10755 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0814 19:50:31.495311 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0814 19:50:31.495314 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.495322 10755 net.cpp:184] Created Layer res4a_branch2a (24)
I0814 19:50:31.495326 10755 net.cpp:561] res4a_branch2a <- pool3
I0814 19:50:31.495327 10755 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0814 19:50:31.508143 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0G)
I0814 19:50:31.508162 10755 net.cpp:245] Setting up res4a_branch2a
I0814 19:50:31.508167 10755 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0814 19:50:31.508173 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:31.508177 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.508185 10755 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0814 19:50:31.508195 10755 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0814 19:50:31.508199 10755 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0814 19:50:31.508643 10755 net.cpp:245] Setting up res4a_branch2a/bn
I0814 19:50:31.508651 10755 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0814 19:50:31.508657 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0814 19:50:31.508661 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.508663 10755 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0814 19:50:31.508675 10755 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0814 19:50:31.508678 10755 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0814 19:50:31.508682 10755 net.cpp:245] Setting up res4a_branch2a/relu
I0814 19:50:31.508685 10755 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0814 19:50:31.508687 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0814 19:50:31.508690 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.508698 10755 net.cpp:184] Created Layer res4a_branch2b (27)
I0814 19:50:31.508700 10755 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0814 19:50:31.508703 10755 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0814 19:50:31.515209 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.03G, req 0G)
I0814 19:50:31.515221 10755 net.cpp:245] Setting up res4a_branch2b
I0814 19:50:31.515225 10755 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0814 19:50:31.515230 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:31.515233 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.515239 10755 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0814 19:50:31.515241 10755 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0814 19:50:31.515244 10755 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0814 19:50:31.515645 10755 net.cpp:245] Setting up res4a_branch2b/bn
I0814 19:50:31.515651 10755 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0814 19:50:31.515657 10755 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0814 19:50:31.515660 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.515662 10755 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0814 19:50:31.515664 10755 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0814 19:50:31.515666 10755 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0814 19:50:31.515671 10755 net.cpp:245] Setting up res4a_branch2b/relu
I0814 19:50:31.515672 10755 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0814 19:50:31.515674 10755 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0814 19:50:31.515676 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.515681 10755 net.cpp:184] Created Layer pool4 (30)
I0814 19:50:31.515683 10755 net.cpp:561] pool4 <- res4a_branch2b
I0814 19:50:31.515686 10755 net.cpp:530] pool4 -> pool4
I0814 19:50:31.515717 10755 net.cpp:245] Setting up pool4
I0814 19:50:31.515722 10755 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0814 19:50:31.515723 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0814 19:50:31.515727 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.515732 10755 net.cpp:184] Created Layer res5a_branch2a (31)
I0814 19:50:31.515733 10755 net.cpp:561] res5a_branch2a <- pool4
I0814 19:50:31.515735 10755 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0814 19:50:31.548516 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.01G, req 0G)
I0814 19:50:31.548532 10755 net.cpp:245] Setting up res5a_branch2a
I0814 19:50:31.548537 10755 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0814 19:50:31.548543 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0814 19:50:31.548547 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.548555 10755 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0814 19:50:31.548558 10755 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0814 19:50:31.548576 10755 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0814 19:50:31.549003 10755 net.cpp:245] Setting up res5a_branch2a/bn
I0814 19:50:31.549010 10755 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0814 19:50:31.549015 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0814 19:50:31.549018 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.549021 10755 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0814 19:50:31.549023 10755 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0814 19:50:31.549026 10755 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0814 19:50:31.549029 10755 net.cpp:245] Setting up res5a_branch2a/relu
I0814 19:50:31.549032 10755 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0814 19:50:31.549034 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0814 19:50:31.549036 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.549042 10755 net.cpp:184] Created Layer res5a_branch2b (34)
I0814 19:50:31.549044 10755 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0814 19:50:31.549047 10755 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0814 19:50:31.564648 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8G, req 0G)
I0814 19:50:31.564664 10755 net.cpp:245] Setting up res5a_branch2b
I0814 19:50:31.564669 10755 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0814 19:50:31.564680 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0814 19:50:31.564684 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.564692 10755 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0814 19:50:31.564697 10755 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0814 19:50:31.564699 10755 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0814 19:50:31.565145 10755 net.cpp:245] Setting up res5a_branch2b/bn
I0814 19:50:31.565153 10755 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0814 19:50:31.565160 10755 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0814 19:50:31.565162 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565165 10755 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0814 19:50:31.565170 10755 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0814 19:50:31.565171 10755 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0814 19:50:31.565176 10755 net.cpp:245] Setting up res5a_branch2b/relu
I0814 19:50:31.565179 10755 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0814 19:50:31.565181 10755 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0814 19:50:31.565184 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565188 10755 net.cpp:184] Created Layer pool5 (37)
I0814 19:50:31.565191 10755 net.cpp:561] pool5 <- res5a_branch2b
I0814 19:50:31.565193 10755 net.cpp:530] pool5 -> pool5
I0814 19:50:31.565210 10755 net.cpp:245] Setting up pool5
I0814 19:50:31.565214 10755 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0814 19:50:31.565217 10755 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0814 19:50:31.565219 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565225 10755 net.cpp:184] Created Layer fc10 (38)
I0814 19:50:31.565227 10755 net.cpp:561] fc10 <- pool5
I0814 19:50:31.565230 10755 net.cpp:530] fc10 -> fc10
I0814 19:50:31.565418 10755 net.cpp:245] Setting up fc10
I0814 19:50:31.565424 10755 net.cpp:252] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0814 19:50:31.565428 10755 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0814 19:50:31.565439 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565443 10755 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0814 19:50:31.565445 10755 net.cpp:561] fc10_fc10_0_split <- fc10
I0814 19:50:31.565448 10755 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0814 19:50:31.565451 10755 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0814 19:50:31.565454 10755 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0814 19:50:31.565485 10755 net.cpp:245] Setting up fc10_fc10_0_split
I0814 19:50:31.565490 10755 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0814 19:50:31.565492 10755 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0814 19:50:31.565495 10755 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0814 19:50:31.565497 10755 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0814 19:50:31.565500 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565510 10755 net.cpp:184] Created Layer loss (40)
I0814 19:50:31.565512 10755 net.cpp:561] loss <- fc10_fc10_0_split_0
I0814 19:50:31.565515 10755 net.cpp:561] loss <- label_data_1_split_0
I0814 19:50:31.565518 10755 net.cpp:530] loss -> loss
I0814 19:50:31.565624 10755 net.cpp:245] Setting up loss
I0814 19:50:31.565630 10755 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0814 19:50:31.565632 10755 net.cpp:256]     with loss weight 1
I0814 19:50:31.565636 10755 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0814 19:50:31.565639 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565647 10755 net.cpp:184] Created Layer accuracy/top1 (41)
I0814 19:50:31.565649 10755 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0814 19:50:31.565652 10755 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0814 19:50:31.565655 10755 net.cpp:530] accuracy/top1 -> accuracy/top1
I0814 19:50:31.565659 10755 net.cpp:245] Setting up accuracy/top1
I0814 19:50:31.565662 10755 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0814 19:50:31.565665 10755 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0814 19:50:31.565666 10755 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0814 19:50:31.565671 10755 net.cpp:184] Created Layer accuracy/top5 (42)
I0814 19:50:31.565675 10755 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0814 19:50:31.565676 10755 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0814 19:50:31.565678 10755 net.cpp:530] accuracy/top5 -> accuracy/top5
I0814 19:50:31.565683 10755 net.cpp:245] Setting up accuracy/top5
I0814 19:50:31.565686 10755 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0814 19:50:31.565688 10755 net.cpp:325] accuracy/top5 does not need backward computation.
I0814 19:50:31.565691 10755 net.cpp:325] accuracy/top1 does not need backward computation.
I0814 19:50:31.565693 10755 net.cpp:323] loss needs backward computation.
I0814 19:50:31.565696 10755 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0814 19:50:31.565698 10755 net.cpp:323] fc10 needs backward computation.
I0814 19:50:31.565701 10755 net.cpp:323] pool5 needs backward computation.
I0814 19:50:31.565702 10755 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0814 19:50:31.565704 10755 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0814 19:50:31.565706 10755 net.cpp:323] res5a_branch2b needs backward computation.
I0814 19:50:31.565708 10755 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0814 19:50:31.565711 10755 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0814 19:50:31.565712 10755 net.cpp:323] res5a_branch2a needs backward computation.
I0814 19:50:31.565714 10755 net.cpp:323] pool4 needs backward computation.
I0814 19:50:31.565716 10755 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0814 19:50:31.565724 10755 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0814 19:50:31.565726 10755 net.cpp:323] res4a_branch2b needs backward computation.
I0814 19:50:31.565728 10755 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0814 19:50:31.565731 10755 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0814 19:50:31.565733 10755 net.cpp:323] res4a_branch2a needs backward computation.
I0814 19:50:31.565735 10755 net.cpp:323] pool3 needs backward computation.
I0814 19:50:31.565737 10755 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0814 19:50:31.565739 10755 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0814 19:50:31.565742 10755 net.cpp:323] res3a_branch2b needs backward computation.
I0814 19:50:31.565743 10755 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0814 19:50:31.565745 10755 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0814 19:50:31.565747 10755 net.cpp:323] res3a_branch2a needs backward computation.
I0814 19:50:31.565749 10755 net.cpp:323] pool2 needs backward computation.
I0814 19:50:31.565752 10755 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0814 19:50:31.565754 10755 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0814 19:50:31.565757 10755 net.cpp:323] res2a_branch2b needs backward computation.
I0814 19:50:31.565758 10755 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0814 19:50:31.565760 10755 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0814 19:50:31.565762 10755 net.cpp:323] res2a_branch2a needs backward computation.
I0814 19:50:31.565764 10755 net.cpp:323] pool1 needs backward computation.
I0814 19:50:31.565767 10755 net.cpp:323] conv1b/relu needs backward computation.
I0814 19:50:31.565768 10755 net.cpp:323] conv1b/bn needs backward computation.
I0814 19:50:31.565770 10755 net.cpp:323] conv1b needs backward computation.
I0814 19:50:31.565773 10755 net.cpp:323] conv1a/relu needs backward computation.
I0814 19:50:31.565775 10755 net.cpp:323] conv1a/bn needs backward computation.
I0814 19:50:31.565778 10755 net.cpp:323] conv1a needs backward computation.
I0814 19:50:31.565779 10755 net.cpp:325] data/bias does not need backward computation.
I0814 19:50:31.565783 10755 net.cpp:325] label_data_1_split does not need backward computation.
I0814 19:50:31.565785 10755 net.cpp:325] data does not need backward computation.
I0814 19:50:31.565788 10755 net.cpp:367] This network produces output accuracy/top1
I0814 19:50:31.565789 10755 net.cpp:367] This network produces output accuracy/top5
I0814 19:50:31.565791 10755 net.cpp:367] This network produces output loss
I0814 19:50:31.565820 10755 net.cpp:389] Top memory (TEST) required for data: 275251200 diff: 8
I0814 19:50:31.565824 10755 net.cpp:392] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0814 19:50:31.565825 10755 net.cpp:395] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0814 19:50:31.565827 10755 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0814 19:50:31.565829 10755 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0814 19:50:31.565831 10755 net.cpp:407] Network initialization done.
I0814 19:50:31.570366 10755 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0814 19:50:31.570385 10755 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0814 19:50:31.570418 10755 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0814 19:50:31.570430 10755 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0814 19:50:31.570569 10755 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0814 19:50:31.570574 10755 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0814 19:50:31.570582 10755 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0814 19:50:31.570669 10755 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0814 19:50:31.570673 10755 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0814 19:50:31.570686 10755 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0814 19:50:31.570703 10755 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:50:31.570791 10755 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0814 19:50:31.570794 10755 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0814 19:50:31.570806 10755 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:50:31.570889 10755 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0814 19:50:31.570894 10755 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0814 19:50:31.570895 10755 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0814 19:50:31.570931 10755 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:50:31.571012 10755 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0814 19:50:31.571015 10755 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0814 19:50:31.571036 10755 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:50:31.571111 10755 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0814 19:50:31.571115 10755 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0814 19:50:31.571118 10755 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0814 19:50:31.571223 10755 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:50:31.571300 10755 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0814 19:50:31.571303 10755 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0814 19:50:31.571363 10755 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:50:31.571439 10755 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0814 19:50:31.571444 10755 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0814 19:50:31.571445 10755 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0814 19:50:31.571766 10755 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0814 19:50:31.571851 10755 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0814 19:50:31.571856 10755 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0814 19:50:31.572007 10755 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0814 19:50:31.572087 10755 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0814 19:50:31.572090 10755 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0814 19:50:31.572093 10755 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0814 19:50:31.572101 10755 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0814 19:50:31.572150 10755 caffe.cpp:290] Running for 200 iterations.
I0814 19:50:31.574908 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0G)
I0814 19:50:31.578533 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0G)
I0814 19:50:31.583724 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0G)
I0814 19:50:31.587438 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0814 19:50:31.592605 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0814 19:50:31.596015 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0814 19:50:31.603981 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.85G, req 0G)
I0814 19:50:31.608724 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0814 19:50:31.618803 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.79G, req 0G)
I0814 19:50:31.623905 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.78G, req 0G)
I0814 19:50:31.626350 10755 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0814 19:50:31.626363 10755 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0814 19:50:31.626368 10755 caffe.cpp:313] Batch 0, loss = 0.100625
I0814 19:50:31.631131 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 7.04G, req 0G)
I0814 19:50:31.635664 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 1  (limit 6.3G, req 0G)
I0814 19:50:31.644870 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0G)
I0814 19:50:31.649826 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0G)
I0814 19:50:31.656549 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0G)
I0814 19:50:31.659636 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0G)
I0814 19:50:31.673465 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0G)
I0814 19:50:31.678851 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0G)
I0814 19:50:31.699581 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 6.3G, req 0.05G)
I0814 19:50:31.705207 10755 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0.05G)
I0814 19:50:31.706328 10755 caffe.cpp:313] Batch 1, accuracy/top1 = 0.9
I0814 19:50:31.706337 10755 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0814 19:50:31.706339 10755 caffe.cpp:313] Batch 1, loss = 0.315879
I0814 19:50:31.714907 10755 caffe.cpp:313] Batch 2, accuracy/top1 = 0.92
I0814 19:50:31.714922 10755 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0814 19:50:31.714926 10755 caffe.cpp:313] Batch 2, loss = 0.347668
I0814 19:50:31.723526 10755 caffe.cpp:313] Batch 3, accuracy/top1 = 0.96
I0814 19:50:31.723536 10755 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0814 19:50:31.723539 10755 caffe.cpp:313] Batch 3, loss = 0.29209
I0814 19:50:31.732050 10755 caffe.cpp:313] Batch 4, accuracy/top1 = 0.8
I0814 19:50:31.732059 10755 caffe.cpp:313] Batch 4, accuracy/top5 = 0.98
I0814 19:50:31.732061 10755 caffe.cpp:313] Batch 4, loss = 0.610224
I0814 19:50:31.740620 10755 caffe.cpp:313] Batch 5, accuracy/top1 = 0.94
I0814 19:50:31.740628 10755 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0814 19:50:31.740631 10755 caffe.cpp:313] Batch 5, loss = 0.171393
I0814 19:50:31.749140 10755 caffe.cpp:313] Batch 6, accuracy/top1 = 0.9
I0814 19:50:31.749158 10755 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0814 19:50:31.749161 10755 caffe.cpp:313] Batch 6, loss = 0.259339
I0814 19:50:31.757704 10755 caffe.cpp:313] Batch 7, accuracy/top1 = 0.9
I0814 19:50:31.757711 10755 caffe.cpp:313] Batch 7, accuracy/top5 = 0.98
I0814 19:50:31.757714 10755 caffe.cpp:313] Batch 7, loss = 0.49531
I0814 19:50:31.766191 10755 caffe.cpp:313] Batch 8, accuracy/top1 = 0.96
I0814 19:50:31.766201 10755 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0814 19:50:31.766202 10755 caffe.cpp:313] Batch 8, loss = 0.132838
I0814 19:50:31.775075 10755 caffe.cpp:313] Batch 9, accuracy/top1 = 0.98
I0814 19:50:31.775111 10755 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0814 19:50:31.775115 10755 caffe.cpp:313] Batch 9, loss = 0.0664658
I0814 19:50:31.783957 10755 caffe.cpp:313] Batch 10, accuracy/top1 = 0.96
I0814 19:50:31.783982 10755 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0814 19:50:31.783985 10755 caffe.cpp:313] Batch 10, loss = 0.14868
I0814 19:50:31.792687 10755 caffe.cpp:313] Batch 11, accuracy/top1 = 0.96
I0814 19:50:31.792721 10755 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0814 19:50:31.792723 10755 caffe.cpp:313] Batch 11, loss = 0.102325
I0814 19:50:31.801517 10755 caffe.cpp:313] Batch 12, accuracy/top1 = 1
I0814 19:50:31.801533 10755 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0814 19:50:31.801535 10755 caffe.cpp:313] Batch 12, loss = 0.0311558
I0814 19:50:31.810169 10755 caffe.cpp:313] Batch 13, accuracy/top1 = 0.94
I0814 19:50:31.810189 10755 caffe.cpp:313] Batch 13, accuracy/top5 = 0.98
I0814 19:50:31.810191 10755 caffe.cpp:313] Batch 13, loss = 0.426188
I0814 19:50:31.818698 10755 caffe.cpp:313] Batch 14, accuracy/top1 = 0.88
I0814 19:50:31.818707 10755 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0814 19:50:31.818711 10755 caffe.cpp:313] Batch 14, loss = 0.493968
I0814 19:50:31.827244 10755 caffe.cpp:313] Batch 15, accuracy/top1 = 0.88
I0814 19:50:31.827252 10755 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0814 19:50:31.827255 10755 caffe.cpp:313] Batch 15, loss = 0.590294
I0814 19:50:31.835736 10755 caffe.cpp:313] Batch 16, accuracy/top1 = 0.96
I0814 19:50:31.835744 10755 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0814 19:50:31.835747 10755 caffe.cpp:313] Batch 16, loss = 0.441693
I0814 19:50:31.844266 10755 caffe.cpp:313] Batch 17, accuracy/top1 = 0.92
I0814 19:50:31.844277 10755 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0814 19:50:31.844280 10755 caffe.cpp:313] Batch 17, loss = 0.465488
I0814 19:50:31.852733 10755 caffe.cpp:313] Batch 18, accuracy/top1 = 0.9
I0814 19:50:31.852741 10755 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0814 19:50:31.852744 10755 caffe.cpp:313] Batch 18, loss = 0.329198
I0814 19:50:31.861243 10755 caffe.cpp:313] Batch 19, accuracy/top1 = 0.9
I0814 19:50:31.861250 10755 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0814 19:50:31.861253 10755 caffe.cpp:313] Batch 19, loss = 0.281484
I0814 19:50:31.869745 10755 caffe.cpp:313] Batch 20, accuracy/top1 = 0.92
I0814 19:50:31.869758 10755 caffe.cpp:313] Batch 20, accuracy/top5 = 0.98
I0814 19:50:31.869761 10755 caffe.cpp:313] Batch 20, loss = 0.296368
I0814 19:50:31.878298 10755 caffe.cpp:313] Batch 21, accuracy/top1 = 0.9
I0814 19:50:31.878306 10755 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0814 19:50:31.878309 10755 caffe.cpp:313] Batch 21, loss = 0.334493
I0814 19:50:31.886770 10755 caffe.cpp:313] Batch 22, accuracy/top1 = 0.86
I0814 19:50:31.886777 10755 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0814 19:50:31.886780 10755 caffe.cpp:313] Batch 22, loss = 0.639861
I0814 19:50:31.895265 10755 caffe.cpp:313] Batch 23, accuracy/top1 = 0.86
I0814 19:50:31.895273 10755 caffe.cpp:313] Batch 23, accuracy/top5 = 0.98
I0814 19:50:31.895275 10755 caffe.cpp:313] Batch 23, loss = 0.519464
I0814 19:50:31.903831 10755 caffe.cpp:313] Batch 24, accuracy/top1 = 0.84
I0814 19:50:31.903847 10755 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0814 19:50:31.903851 10755 caffe.cpp:313] Batch 24, loss = 0.37739
I0814 19:50:31.912356 10755 caffe.cpp:313] Batch 25, accuracy/top1 = 0.92
I0814 19:50:31.912364 10755 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0814 19:50:31.912367 10755 caffe.cpp:313] Batch 25, loss = 0.171298
I0814 19:50:31.920842 10755 caffe.cpp:313] Batch 26, accuracy/top1 = 0.9
I0814 19:50:31.920850 10755 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0814 19:50:31.920852 10755 caffe.cpp:313] Batch 26, loss = 0.51853
I0814 19:50:31.929392 10755 caffe.cpp:313] Batch 27, accuracy/top1 = 0.92
I0814 19:50:31.929405 10755 caffe.cpp:313] Batch 27, accuracy/top5 = 0.98
I0814 19:50:31.929409 10755 caffe.cpp:313] Batch 27, loss = 0.429186
I0814 19:50:31.937901 10755 caffe.cpp:313] Batch 28, accuracy/top1 = 0.96
I0814 19:50:31.937909 10755 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0814 19:50:31.937911 10755 caffe.cpp:313] Batch 28, loss = 0.206304
I0814 19:50:31.946398 10755 caffe.cpp:313] Batch 29, accuracy/top1 = 0.88
I0814 19:50:31.946404 10755 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0814 19:50:31.946408 10755 caffe.cpp:313] Batch 29, loss = 0.472898
I0814 19:50:31.954881 10755 caffe.cpp:313] Batch 30, accuracy/top1 = 0.94
I0814 19:50:31.954898 10755 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0814 19:50:31.954901 10755 caffe.cpp:313] Batch 30, loss = 0.335067
I0814 19:50:31.963436 10755 caffe.cpp:313] Batch 31, accuracy/top1 = 0.92
I0814 19:50:31.963459 10755 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0814 19:50:31.963461 10755 caffe.cpp:313] Batch 31, loss = 0.371
I0814 19:50:31.971958 10755 caffe.cpp:313] Batch 32, accuracy/top1 = 0.88
I0814 19:50:31.971966 10755 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0814 19:50:31.971969 10755 caffe.cpp:313] Batch 32, loss = 0.534583
I0814 19:50:31.980427 10755 caffe.cpp:313] Batch 33, accuracy/top1 = 0.96
I0814 19:50:31.980434 10755 caffe.cpp:313] Batch 33, accuracy/top5 = 0.98
I0814 19:50:31.980437 10755 caffe.cpp:313] Batch 33, loss = 0.268204
I0814 19:50:31.988929 10755 caffe.cpp:313] Batch 34, accuracy/top1 = 0.9
I0814 19:50:31.988939 10755 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0814 19:50:31.988941 10755 caffe.cpp:313] Batch 34, loss = 0.521848
I0814 19:50:31.997422 10755 caffe.cpp:313] Batch 35, accuracy/top1 = 0.86
I0814 19:50:31.997433 10755 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0814 19:50:31.997436 10755 caffe.cpp:313] Batch 35, loss = 0.313666
I0814 19:50:32.005920 10755 caffe.cpp:313] Batch 36, accuracy/top1 = 0.88
I0814 19:50:32.005928 10755 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0814 19:50:32.005931 10755 caffe.cpp:313] Batch 36, loss = 0.365687
I0814 19:50:32.014385 10755 caffe.cpp:313] Batch 37, accuracy/top1 = 0.86
I0814 19:50:32.014394 10755 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0814 19:50:32.014395 10755 caffe.cpp:313] Batch 37, loss = 0.600546
I0814 19:50:32.022966 10755 caffe.cpp:313] Batch 38, accuracy/top1 = 0.86
I0814 19:50:32.022981 10755 caffe.cpp:313] Batch 38, accuracy/top5 = 0.98
I0814 19:50:32.022984 10755 caffe.cpp:313] Batch 38, loss = 0.696199
I0814 19:50:32.031478 10755 caffe.cpp:313] Batch 39, accuracy/top1 = 0.84
I0814 19:50:32.031487 10755 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0814 19:50:32.031491 10755 caffe.cpp:313] Batch 39, loss = 0.410507
I0814 19:50:32.040020 10755 caffe.cpp:313] Batch 40, accuracy/top1 = 0.9
I0814 19:50:32.040030 10755 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0814 19:50:32.040031 10755 caffe.cpp:313] Batch 40, loss = 0.512227
I0814 19:50:32.048492 10755 caffe.cpp:313] Batch 41, accuracy/top1 = 0.94
I0814 19:50:32.048499 10755 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0814 19:50:32.048502 10755 caffe.cpp:313] Batch 41, loss = 0.183967
I0814 19:50:32.057034 10755 caffe.cpp:313] Batch 42, accuracy/top1 = 0.9
I0814 19:50:32.057044 10755 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0814 19:50:32.057046 10755 caffe.cpp:313] Batch 42, loss = 0.306774
I0814 19:50:32.065493 10755 caffe.cpp:313] Batch 43, accuracy/top1 = 0.82
I0814 19:50:32.065501 10755 caffe.cpp:313] Batch 43, accuracy/top5 = 0.98
I0814 19:50:32.065503 10755 caffe.cpp:313] Batch 43, loss = 0.692476
I0814 19:50:32.073999 10755 caffe.cpp:313] Batch 44, accuracy/top1 = 0.86
I0814 19:50:32.074007 10755 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0814 19:50:32.074010 10755 caffe.cpp:313] Batch 44, loss = 0.888008
I0814 19:50:32.082481 10755 caffe.cpp:313] Batch 45, accuracy/top1 = 0.88
I0814 19:50:32.082490 10755 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0814 19:50:32.082494 10755 caffe.cpp:313] Batch 45, loss = 0.519683
I0814 19:50:32.090948 10755 caffe.cpp:313] Batch 46, accuracy/top1 = 0.92
I0814 19:50:32.090956 10755 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0814 19:50:32.090958 10755 caffe.cpp:313] Batch 46, loss = 0.24832
I0814 19:50:32.099405 10755 caffe.cpp:313] Batch 47, accuracy/top1 = 0.82
I0814 19:50:32.099412 10755 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0814 19:50:32.099414 10755 caffe.cpp:313] Batch 47, loss = 0.573334
I0814 19:50:32.107887 10755 caffe.cpp:313] Batch 48, accuracy/top1 = 0.92
I0814 19:50:32.107893 10755 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0814 19:50:32.107897 10755 caffe.cpp:313] Batch 48, loss = 0.698296
I0814 19:50:32.116487 10755 caffe.cpp:313] Batch 49, accuracy/top1 = 0.9
I0814 19:50:32.116504 10755 caffe.cpp:313] Batch 49, accuracy/top5 = 0.98
I0814 19:50:32.116520 10755 caffe.cpp:313] Batch 49, loss = 0.331494
I0814 19:50:32.125110 10755 caffe.cpp:313] Batch 50, accuracy/top1 = 0.84
I0814 19:50:32.125130 10755 caffe.cpp:313] Batch 50, accuracy/top5 = 0.96
I0814 19:50:32.125134 10755 caffe.cpp:313] Batch 50, loss = 0.909261
I0814 19:50:32.133756 10755 caffe.cpp:313] Batch 51, accuracy/top1 = 0.88
I0814 19:50:32.133779 10755 caffe.cpp:313] Batch 51, accuracy/top5 = 1
I0814 19:50:32.133782 10755 caffe.cpp:313] Batch 51, loss = 0.548951
I0814 19:50:32.142400 10755 caffe.cpp:313] Batch 52, accuracy/top1 = 0.98
I0814 19:50:32.142423 10755 caffe.cpp:313] Batch 52, accuracy/top5 = 1
I0814 19:50:32.142427 10755 caffe.cpp:313] Batch 52, loss = 0.0446025
I0814 19:50:32.151000 10755 caffe.cpp:313] Batch 53, accuracy/top1 = 0.96
I0814 19:50:32.151013 10755 caffe.cpp:313] Batch 53, accuracy/top5 = 1
I0814 19:50:32.151016 10755 caffe.cpp:313] Batch 53, loss = 0.266108
I0814 19:50:32.159545 10755 caffe.cpp:313] Batch 54, accuracy/top1 = 0.92
I0814 19:50:32.159554 10755 caffe.cpp:313] Batch 54, accuracy/top5 = 1
I0814 19:50:32.159557 10755 caffe.cpp:313] Batch 54, loss = 0.465162
I0814 19:50:32.168004 10755 caffe.cpp:313] Batch 55, accuracy/top1 = 0.94
I0814 19:50:32.168012 10755 caffe.cpp:313] Batch 55, accuracy/top5 = 1
I0814 19:50:32.168016 10755 caffe.cpp:313] Batch 55, loss = 0.220884
I0814 19:50:32.176582 10755 caffe.cpp:313] Batch 56, accuracy/top1 = 0.86
I0814 19:50:32.176599 10755 caffe.cpp:313] Batch 56, accuracy/top5 = 0.98
I0814 19:50:32.176602 10755 caffe.cpp:313] Batch 56, loss = 0.909096
I0814 19:50:32.185106 10755 caffe.cpp:313] Batch 57, accuracy/top1 = 0.94
I0814 19:50:32.185114 10755 caffe.cpp:313] Batch 57, accuracy/top5 = 1
I0814 19:50:32.185118 10755 caffe.cpp:313] Batch 57, loss = 0.302571
I0814 19:50:32.193619 10755 caffe.cpp:313] Batch 58, accuracy/top1 = 0.92
I0814 19:50:32.193627 10755 caffe.cpp:313] Batch 58, accuracy/top5 = 1
I0814 19:50:32.193631 10755 caffe.cpp:313] Batch 58, loss = 0.33654
I0814 19:50:32.202111 10755 caffe.cpp:313] Batch 59, accuracy/top1 = 0.92
I0814 19:50:32.202122 10755 caffe.cpp:313] Batch 59, accuracy/top5 = 1
I0814 19:50:32.202126 10755 caffe.cpp:313] Batch 59, loss = 0.259225
I0814 19:50:32.210710 10755 caffe.cpp:313] Batch 60, accuracy/top1 = 0.84
I0814 19:50:32.210726 10755 caffe.cpp:313] Batch 60, accuracy/top5 = 1
I0814 19:50:32.210727 10755 caffe.cpp:313] Batch 60, loss = 0.8207
I0814 19:50:32.219215 10755 caffe.cpp:313] Batch 61, accuracy/top1 = 0.88
I0814 19:50:32.219223 10755 caffe.cpp:313] Batch 61, accuracy/top5 = 0.98
I0814 19:50:32.219226 10755 caffe.cpp:313] Batch 61, loss = 0.597165
I0814 19:50:32.227684 10755 caffe.cpp:313] Batch 62, accuracy/top1 = 0.98
I0814 19:50:32.227691 10755 caffe.cpp:313] Batch 62, accuracy/top5 = 1
I0814 19:50:32.227694 10755 caffe.cpp:313] Batch 62, loss = 0.089375
I0814 19:50:32.236208 10755 caffe.cpp:313] Batch 63, accuracy/top1 = 0.94
I0814 19:50:32.236222 10755 caffe.cpp:313] Batch 63, accuracy/top5 = 1
I0814 19:50:32.236225 10755 caffe.cpp:313] Batch 63, loss = 0.235466
I0814 19:50:32.244771 10755 caffe.cpp:313] Batch 64, accuracy/top1 = 0.9
I0814 19:50:32.244779 10755 caffe.cpp:313] Batch 64, accuracy/top5 = 1
I0814 19:50:32.244781 10755 caffe.cpp:313] Batch 64, loss = 0.468478
I0814 19:50:32.253245 10755 caffe.cpp:313] Batch 65, accuracy/top1 = 0.94
I0814 19:50:32.253253 10755 caffe.cpp:313] Batch 65, accuracy/top5 = 1
I0814 19:50:32.253255 10755 caffe.cpp:313] Batch 65, loss = 0.238921
I0814 19:50:32.261737 10755 caffe.cpp:313] Batch 66, accuracy/top1 = 0.92
I0814 19:50:32.261744 10755 caffe.cpp:313] Batch 66, accuracy/top5 = 1
I0814 19:50:32.261747 10755 caffe.cpp:313] Batch 66, loss = 0.33557
I0814 19:50:32.270267 10755 caffe.cpp:313] Batch 67, accuracy/top1 = 0.94
I0814 19:50:32.270284 10755 caffe.cpp:313] Batch 67, accuracy/top5 = 1
I0814 19:50:32.270287 10755 caffe.cpp:313] Batch 67, loss = 0.416252
I0814 19:50:32.278838 10755 caffe.cpp:313] Batch 68, accuracy/top1 = 0.9
I0814 19:50:32.278846 10755 caffe.cpp:313] Batch 68, accuracy/top5 = 1
I0814 19:50:32.278858 10755 caffe.cpp:313] Batch 68, loss = 0.45165
I0814 19:50:32.287317 10755 caffe.cpp:313] Batch 69, accuracy/top1 = 0.92
I0814 19:50:32.287325 10755 caffe.cpp:313] Batch 69, accuracy/top5 = 1
I0814 19:50:32.287328 10755 caffe.cpp:313] Batch 69, loss = 0.262525
I0814 19:50:32.295822 10755 caffe.cpp:313] Batch 70, accuracy/top1 = 0.96
I0814 19:50:32.295833 10755 caffe.cpp:313] Batch 70, accuracy/top5 = 1
I0814 19:50:32.295836 10755 caffe.cpp:313] Batch 70, loss = 0.318256
I0814 19:50:32.304316 10755 caffe.cpp:313] Batch 71, accuracy/top1 = 0.92
I0814 19:50:32.304327 10755 caffe.cpp:313] Batch 71, accuracy/top5 = 1
I0814 19:50:32.304328 10755 caffe.cpp:313] Batch 71, loss = 0.403984
I0814 19:50:32.312822 10755 caffe.cpp:313] Batch 72, accuracy/top1 = 0.82
I0814 19:50:32.312829 10755 caffe.cpp:313] Batch 72, accuracy/top5 = 0.98
I0814 19:50:32.312832 10755 caffe.cpp:313] Batch 72, loss = 1.11955
I0814 19:50:32.321266 10755 caffe.cpp:313] Batch 73, accuracy/top1 = 0.94
I0814 19:50:32.321274 10755 caffe.cpp:313] Batch 73, accuracy/top5 = 1
I0814 19:50:32.321276 10755 caffe.cpp:313] Batch 73, loss = 0.245141
I0814 19:50:32.329818 10755 caffe.cpp:313] Batch 74, accuracy/top1 = 0.96
I0814 19:50:32.329833 10755 caffe.cpp:313] Batch 74, accuracy/top5 = 1
I0814 19:50:32.329835 10755 caffe.cpp:313] Batch 74, loss = 0.0716145
I0814 19:50:32.338346 10755 caffe.cpp:313] Batch 75, accuracy/top1 = 0.84
I0814 19:50:32.338354 10755 caffe.cpp:313] Batch 75, accuracy/top5 = 1
I0814 19:50:32.338357 10755 caffe.cpp:313] Batch 75, loss = 0.523388
I0814 19:50:32.346801 10755 caffe.cpp:313] Batch 76, accuracy/top1 = 0.92
I0814 19:50:32.346809 10755 caffe.cpp:313] Batch 76, accuracy/top5 = 1
I0814 19:50:32.346812 10755 caffe.cpp:313] Batch 76, loss = 0.359712
I0814 19:50:32.355304 10755 caffe.cpp:313] Batch 77, accuracy/top1 = 0.92
I0814 19:50:32.355314 10755 caffe.cpp:313] Batch 77, accuracy/top5 = 1
I0814 19:50:32.355319 10755 caffe.cpp:313] Batch 77, loss = 0.483487
I0814 19:50:32.363849 10755 caffe.cpp:313] Batch 78, accuracy/top1 = 0.94
I0814 19:50:32.363867 10755 caffe.cpp:313] Batch 78, accuracy/top5 = 1
I0814 19:50:32.363869 10755 caffe.cpp:313] Batch 78, loss = 0.288579
I0814 19:50:32.372367 10755 caffe.cpp:313] Batch 79, accuracy/top1 = 0.92
I0814 19:50:32.372375 10755 caffe.cpp:313] Batch 79, accuracy/top5 = 0.98
I0814 19:50:32.372378 10755 caffe.cpp:313] Batch 79, loss = 0.641949
I0814 19:50:32.380815 10755 caffe.cpp:313] Batch 80, accuracy/top1 = 0.96
I0814 19:50:32.380822 10755 caffe.cpp:313] Batch 80, accuracy/top5 = 1
I0814 19:50:32.380825 10755 caffe.cpp:313] Batch 80, loss = 0.271266
I0814 19:50:32.389369 10755 caffe.cpp:313] Batch 81, accuracy/top1 = 0.82
I0814 19:50:32.389382 10755 caffe.cpp:313] Batch 81, accuracy/top5 = 1
I0814 19:50:32.389385 10755 caffe.cpp:313] Batch 81, loss = 0.604369
I0814 19:50:32.397882 10755 caffe.cpp:313] Batch 82, accuracy/top1 = 0.9
I0814 19:50:32.397891 10755 caffe.cpp:313] Batch 82, accuracy/top5 = 0.98
I0814 19:50:32.397893 10755 caffe.cpp:313] Batch 82, loss = 0.476336
I0814 19:50:32.406368 10755 caffe.cpp:313] Batch 83, accuracy/top1 = 0.96
I0814 19:50:32.406374 10755 caffe.cpp:313] Batch 83, accuracy/top5 = 1
I0814 19:50:32.406378 10755 caffe.cpp:313] Batch 83, loss = 0.18133
I0814 19:50:32.414808 10755 caffe.cpp:313] Batch 84, accuracy/top1 = 0.94
I0814 19:50:32.414814 10755 caffe.cpp:313] Batch 84, accuracy/top5 = 0.98
I0814 19:50:32.414818 10755 caffe.cpp:313] Batch 84, loss = 0.258922
I0814 19:50:32.423410 10755 caffe.cpp:313] Batch 85, accuracy/top1 = 0.9
I0814 19:50:32.423435 10755 caffe.cpp:313] Batch 85, accuracy/top5 = 1
I0814 19:50:32.423439 10755 caffe.cpp:313] Batch 85, loss = 0.234108
I0814 19:50:32.431918 10755 caffe.cpp:313] Batch 86, accuracy/top1 = 0.96
I0814 19:50:32.431927 10755 caffe.cpp:313] Batch 86, accuracy/top5 = 1
I0814 19:50:32.431929 10755 caffe.cpp:313] Batch 86, loss = 0.179991
I0814 19:50:32.440445 10755 caffe.cpp:313] Batch 87, accuracy/top1 = 0.98
I0814 19:50:32.440452 10755 caffe.cpp:313] Batch 87, accuracy/top5 = 1
I0814 19:50:32.440465 10755 caffe.cpp:313] Batch 87, loss = 0.0717596
I0814 19:50:32.448952 10755 caffe.cpp:313] Batch 88, accuracy/top1 = 0.94
I0814 19:50:32.448961 10755 caffe.cpp:313] Batch 88, accuracy/top5 = 1
I0814 19:50:32.448964 10755 caffe.cpp:313] Batch 88, loss = 0.352118
I0814 19:50:32.457476 10755 caffe.cpp:313] Batch 89, accuracy/top1 = 0.96
I0814 19:50:32.457489 10755 caffe.cpp:313] Batch 89, accuracy/top5 = 1
I0814 19:50:32.457491 10755 caffe.cpp:313] Batch 89, loss = 0.17045
I0814 19:50:32.465956 10755 caffe.cpp:313] Batch 90, accuracy/top1 = 0.88
I0814 19:50:32.465965 10755 caffe.cpp:313] Batch 90, accuracy/top5 = 1
I0814 19:50:32.465966 10755 caffe.cpp:313] Batch 90, loss = 0.746801
I0814 19:50:32.474447 10755 caffe.cpp:313] Batch 91, accuracy/top1 = 0.86
I0814 19:50:32.474453 10755 caffe.cpp:313] Batch 91, accuracy/top5 = 1
I0814 19:50:32.474457 10755 caffe.cpp:313] Batch 91, loss = 0.389802
I0814 19:50:32.482949 10755 caffe.cpp:313] Batch 92, accuracy/top1 = 0.88
I0814 19:50:32.482964 10755 caffe.cpp:313] Batch 92, accuracy/top5 = 1
I0814 19:50:32.482966 10755 caffe.cpp:313] Batch 92, loss = 0.641958
I0814 19:50:32.491518 10755 caffe.cpp:313] Batch 93, accuracy/top1 = 0.96
I0814 19:50:32.491528 10755 caffe.cpp:313] Batch 93, accuracy/top5 = 1
I0814 19:50:32.491530 10755 caffe.cpp:313] Batch 93, loss = 0.0938224
I0814 19:50:32.499980 10755 caffe.cpp:313] Batch 94, accuracy/top1 = 0.92
I0814 19:50:32.499987 10755 caffe.cpp:313] Batch 94, accuracy/top5 = 1
I0814 19:50:32.499990 10755 caffe.cpp:313] Batch 94, loss = 0.258961
I0814 19:50:32.508452 10755 caffe.cpp:313] Batch 95, accuracy/top1 = 0.86
I0814 19:50:32.508460 10755 caffe.cpp:313] Batch 95, accuracy/top5 = 0.98
I0814 19:50:32.508462 10755 caffe.cpp:313] Batch 95, loss = 1.0524
I0814 19:50:32.516986 10755 caffe.cpp:313] Batch 96, accuracy/top1 = 0.98
I0814 19:50:32.517004 10755 caffe.cpp:313] Batch 96, accuracy/top5 = 1
I0814 19:50:32.517006 10755 caffe.cpp:313] Batch 96, loss = 0.128782
I0814 19:50:32.525490 10755 caffe.cpp:313] Batch 97, accuracy/top1 = 0.94
I0814 19:50:32.525498 10755 caffe.cpp:313] Batch 97, accuracy/top5 = 1
I0814 19:50:32.525501 10755 caffe.cpp:313] Batch 97, loss = 0.17278
I0814 19:50:32.533964 10755 caffe.cpp:313] Batch 98, accuracy/top1 = 0.88
I0814 19:50:32.533972 10755 caffe.cpp:313] Batch 98, accuracy/top5 = 1
I0814 19:50:32.533974 10755 caffe.cpp:313] Batch 98, loss = 0.444864
I0814 19:50:32.542498 10755 caffe.cpp:313] Batch 99, accuracy/top1 = 0.86
I0814 19:50:32.542511 10755 caffe.cpp:313] Batch 99, accuracy/top5 = 0.98
I0814 19:50:32.542513 10755 caffe.cpp:313] Batch 99, loss = 0.617713
I0814 19:50:32.550985 10755 caffe.cpp:313] Batch 100, accuracy/top1 = 0.92
I0814 19:50:32.550994 10755 caffe.cpp:313] Batch 100, accuracy/top5 = 1
I0814 19:50:32.550997 10755 caffe.cpp:313] Batch 100, loss = 0.238268
I0814 19:50:32.559487 10755 caffe.cpp:313] Batch 101, accuracy/top1 = 0.92
I0814 19:50:32.559495 10755 caffe.cpp:313] Batch 101, accuracy/top5 = 1
I0814 19:50:32.559497 10755 caffe.cpp:313] Batch 101, loss = 0.310587
I0814 19:50:32.567921 10755 caffe.cpp:313] Batch 102, accuracy/top1 = 0.94
I0814 19:50:32.567929 10755 caffe.cpp:313] Batch 102, accuracy/top5 = 1
I0814 19:50:32.567932 10755 caffe.cpp:313] Batch 102, loss = 0.190743
I0814 19:50:32.576522 10755 caffe.cpp:313] Batch 103, accuracy/top1 = 0.86
I0814 19:50:32.576537 10755 caffe.cpp:313] Batch 103, accuracy/top5 = 1
I0814 19:50:32.576540 10755 caffe.cpp:313] Batch 103, loss = 0.494796
I0814 19:50:32.585023 10755 caffe.cpp:313] Batch 104, accuracy/top1 = 0.86
I0814 19:50:32.585032 10755 caffe.cpp:313] Batch 104, accuracy/top5 = 1
I0814 19:50:32.585034 10755 caffe.cpp:313] Batch 104, loss = 0.53487
I0814 19:50:32.593521 10755 caffe.cpp:313] Batch 105, accuracy/top1 = 0.96
I0814 19:50:32.593529 10755 caffe.cpp:313] Batch 105, accuracy/top5 = 1
I0814 19:50:32.593533 10755 caffe.cpp:313] Batch 105, loss = 0.102968
I0814 19:50:32.602035 10755 caffe.cpp:313] Batch 106, accuracy/top1 = 0.94
I0814 19:50:32.602046 10755 caffe.cpp:313] Batch 106, accuracy/top5 = 0.98
I0814 19:50:32.602062 10755 caffe.cpp:313] Batch 106, loss = 0.180287
I0814 19:50:32.610625 10755 caffe.cpp:313] Batch 107, accuracy/top1 = 0.88
I0814 19:50:32.610641 10755 caffe.cpp:313] Batch 107, accuracy/top5 = 1
I0814 19:50:32.610644 10755 caffe.cpp:313] Batch 107, loss = 0.616268
I0814 19:50:32.619113 10755 caffe.cpp:313] Batch 108, accuracy/top1 = 0.9
I0814 19:50:32.619122 10755 caffe.cpp:313] Batch 108, accuracy/top5 = 1
I0814 19:50:32.619123 10755 caffe.cpp:313] Batch 108, loss = 0.248375
I0814 19:50:32.627588 10755 caffe.cpp:313] Batch 109, accuracy/top1 = 0.88
I0814 19:50:32.627595 10755 caffe.cpp:313] Batch 109, accuracy/top5 = 1
I0814 19:50:32.627598 10755 caffe.cpp:313] Batch 109, loss = 0.334915
I0814 19:50:32.636096 10755 caffe.cpp:313] Batch 110, accuracy/top1 = 0.88
I0814 19:50:32.636111 10755 caffe.cpp:313] Batch 110, accuracy/top5 = 1
I0814 19:50:32.636114 10755 caffe.cpp:313] Batch 110, loss = 0.657376
I0814 19:50:32.644655 10755 caffe.cpp:313] Batch 111, accuracy/top1 = 0.9
I0814 19:50:32.644665 10755 caffe.cpp:313] Batch 111, accuracy/top5 = 1
I0814 19:50:32.644666 10755 caffe.cpp:313] Batch 111, loss = 0.400542
I0814 19:50:32.653134 10755 caffe.cpp:313] Batch 112, accuracy/top1 = 0.86
I0814 19:50:32.653142 10755 caffe.cpp:313] Batch 112, accuracy/top5 = 0.98
I0814 19:50:32.653146 10755 caffe.cpp:313] Batch 112, loss = 0.535607
I0814 19:50:32.661640 10755 caffe.cpp:313] Batch 113, accuracy/top1 = 0.92
I0814 19:50:32.661648 10755 caffe.cpp:313] Batch 113, accuracy/top5 = 1
I0814 19:50:32.661650 10755 caffe.cpp:313] Batch 113, loss = 0.158365
I0814 19:50:32.670176 10755 caffe.cpp:313] Batch 114, accuracy/top1 = 0.94
I0814 19:50:32.670193 10755 caffe.cpp:313] Batch 114, accuracy/top5 = 1
I0814 19:50:32.670197 10755 caffe.cpp:313] Batch 114, loss = 0.188412
I0814 19:50:32.678717 10755 caffe.cpp:313] Batch 115, accuracy/top1 = 0.94
I0814 19:50:32.678725 10755 caffe.cpp:313] Batch 115, accuracy/top5 = 1
I0814 19:50:32.678728 10755 caffe.cpp:313] Batch 115, loss = 0.129272
I0814 19:50:32.687175 10755 caffe.cpp:313] Batch 116, accuracy/top1 = 0.86
I0814 19:50:32.687182 10755 caffe.cpp:313] Batch 116, accuracy/top5 = 1
I0814 19:50:32.687185 10755 caffe.cpp:313] Batch 116, loss = 0.65865
I0814 19:50:32.695708 10755 caffe.cpp:313] Batch 117, accuracy/top1 = 0.86
I0814 19:50:32.695720 10755 caffe.cpp:313] Batch 117, accuracy/top5 = 1
I0814 19:50:32.695724 10755 caffe.cpp:313] Batch 117, loss = 0.971622
I0814 19:50:32.704218 10755 caffe.cpp:313] Batch 118, accuracy/top1 = 0.84
I0814 19:50:32.704227 10755 caffe.cpp:313] Batch 118, accuracy/top5 = 1
I0814 19:50:32.704231 10755 caffe.cpp:313] Batch 118, loss = 0.544698
I0814 19:50:32.712707 10755 caffe.cpp:313] Batch 119, accuracy/top1 = 0.96
I0814 19:50:32.712714 10755 caffe.cpp:313] Batch 119, accuracy/top5 = 1
I0814 19:50:32.712716 10755 caffe.cpp:313] Batch 119, loss = 0.111922
I0814 19:50:32.721179 10755 caffe.cpp:313] Batch 120, accuracy/top1 = 0.88
I0814 19:50:32.721186 10755 caffe.cpp:313] Batch 120, accuracy/top5 = 1
I0814 19:50:32.721189 10755 caffe.cpp:313] Batch 120, loss = 0.48357
I0814 19:50:32.729734 10755 caffe.cpp:313] Batch 121, accuracy/top1 = 0.92
I0814 19:50:32.729748 10755 caffe.cpp:313] Batch 121, accuracy/top5 = 1
I0814 19:50:32.729751 10755 caffe.cpp:313] Batch 121, loss = 0.332828
I0814 19:50:32.738261 10755 caffe.cpp:313] Batch 122, accuracy/top1 = 0.9
I0814 19:50:32.738270 10755 caffe.cpp:313] Batch 122, accuracy/top5 = 1
I0814 19:50:32.738272 10755 caffe.cpp:313] Batch 122, loss = 0.420839
I0814 19:50:32.746728 10755 caffe.cpp:313] Batch 123, accuracy/top1 = 0.9
I0814 19:50:32.746737 10755 caffe.cpp:313] Batch 123, accuracy/top5 = 0.98
I0814 19:50:32.746738 10755 caffe.cpp:313] Batch 123, loss = 0.491513
I0814 19:50:32.755234 10755 caffe.cpp:313] Batch 124, accuracy/top1 = 0.94
I0814 19:50:32.755245 10755 caffe.cpp:313] Batch 124, accuracy/top5 = 1
I0814 19:50:32.755249 10755 caffe.cpp:313] Batch 124, loss = 0.156552
I0814 19:50:32.763794 10755 caffe.cpp:313] Batch 125, accuracy/top1 = 0.96
I0814 19:50:32.763810 10755 caffe.cpp:313] Batch 125, accuracy/top5 = 1
I0814 19:50:32.763823 10755 caffe.cpp:313] Batch 125, loss = 0.380199
I0814 19:50:32.772442 10755 caffe.cpp:313] Batch 126, accuracy/top1 = 0.96
I0814 19:50:32.772472 10755 caffe.cpp:313] Batch 126, accuracy/top5 = 1
I0814 19:50:32.772475 10755 caffe.cpp:313] Batch 126, loss = 0.285239
I0814 19:50:32.781354 10755 caffe.cpp:313] Batch 127, accuracy/top1 = 0.9
I0814 19:50:32.781374 10755 caffe.cpp:313] Batch 127, accuracy/top5 = 1
I0814 19:50:32.781378 10755 caffe.cpp:313] Batch 127, loss = 0.494171
I0814 19:50:32.790156 10755 caffe.cpp:313] Batch 128, accuracy/top1 = 0.88
I0814 19:50:32.790179 10755 caffe.cpp:313] Batch 128, accuracy/top5 = 1
I0814 19:50:32.790182 10755 caffe.cpp:313] Batch 128, loss = 0.29217
I0814 19:50:32.798750 10755 caffe.cpp:313] Batch 129, accuracy/top1 = 0.94
I0814 19:50:32.798766 10755 caffe.cpp:313] Batch 129, accuracy/top5 = 1
I0814 19:50:32.798769 10755 caffe.cpp:313] Batch 129, loss = 0.168878
I0814 19:50:32.807312 10755 caffe.cpp:313] Batch 130, accuracy/top1 = 0.88
I0814 19:50:32.807322 10755 caffe.cpp:313] Batch 130, accuracy/top5 = 1
I0814 19:50:32.807325 10755 caffe.cpp:313] Batch 130, loss = 0.564289
I0814 19:50:32.815814 10755 caffe.cpp:313] Batch 131, accuracy/top1 = 0.84
I0814 19:50:32.815822 10755 caffe.cpp:313] Batch 131, accuracy/top5 = 1
I0814 19:50:32.815826 10755 caffe.cpp:313] Batch 131, loss = 0.284893
I0814 19:50:32.824369 10755 caffe.cpp:313] Batch 132, accuracy/top1 = 0.96
I0814 19:50:32.824388 10755 caffe.cpp:313] Batch 132, accuracy/top5 = 1
I0814 19:50:32.824393 10755 caffe.cpp:313] Batch 132, loss = 0.285295
I0814 19:50:32.832876 10755 caffe.cpp:313] Batch 133, accuracy/top1 = 0.96
I0814 19:50:32.832885 10755 caffe.cpp:313] Batch 133, accuracy/top5 = 1
I0814 19:50:32.832888 10755 caffe.cpp:313] Batch 133, loss = 0.15588
I0814 19:50:32.841394 10755 caffe.cpp:313] Batch 134, accuracy/top1 = 0.94
I0814 19:50:32.841403 10755 caffe.cpp:313] Batch 134, accuracy/top5 = 1
I0814 19:50:32.841406 10755 caffe.cpp:313] Batch 134, loss = 0.314242
I0814 19:50:32.849922 10755 caffe.cpp:313] Batch 135, accuracy/top1 = 0.92
I0814 19:50:32.849937 10755 caffe.cpp:313] Batch 135, accuracy/top5 = 0.98
I0814 19:50:32.849941 10755 caffe.cpp:313] Batch 135, loss = 0.568448
I0814 19:50:32.858474 10755 caffe.cpp:313] Batch 136, accuracy/top1 = 0.98
I0814 19:50:32.858484 10755 caffe.cpp:313] Batch 136, accuracy/top5 = 1
I0814 19:50:32.858489 10755 caffe.cpp:313] Batch 136, loss = 0.0449102
I0814 19:50:32.866955 10755 caffe.cpp:313] Batch 137, accuracy/top1 = 0.9
I0814 19:50:32.866964 10755 caffe.cpp:313] Batch 137, accuracy/top5 = 0.98
I0814 19:50:32.866967 10755 caffe.cpp:313] Batch 137, loss = 0.511689
I0814 19:50:32.875427 10755 caffe.cpp:313] Batch 138, accuracy/top1 = 0.9
I0814 19:50:32.875435 10755 caffe.cpp:313] Batch 138, accuracy/top5 = 1
I0814 19:50:32.875439 10755 caffe.cpp:313] Batch 138, loss = 0.217307
I0814 19:50:32.883988 10755 caffe.cpp:313] Batch 139, accuracy/top1 = 0.86
I0814 19:50:32.884016 10755 caffe.cpp:313] Batch 139, accuracy/top5 = 0.98
I0814 19:50:32.884021 10755 caffe.cpp:313] Batch 139, loss = 0.521206
I0814 19:50:32.892557 10755 caffe.cpp:313] Batch 140, accuracy/top1 = 0.92
I0814 19:50:32.892565 10755 caffe.cpp:313] Batch 140, accuracy/top5 = 1
I0814 19:50:32.892570 10755 caffe.cpp:313] Batch 140, loss = 0.369981
I0814 19:50:32.901029 10755 caffe.cpp:313] Batch 141, accuracy/top1 = 0.92
I0814 19:50:32.901037 10755 caffe.cpp:313] Batch 141, accuracy/top5 = 1
I0814 19:50:32.901041 10755 caffe.cpp:313] Batch 141, loss = 0.480835
I0814 19:50:32.909574 10755 caffe.cpp:313] Batch 142, accuracy/top1 = 0.96
I0814 19:50:32.909584 10755 caffe.cpp:313] Batch 142, accuracy/top5 = 1
I0814 19:50:32.909588 10755 caffe.cpp:313] Batch 142, loss = 0.11987
I0814 19:50:32.918094 10755 caffe.cpp:313] Batch 143, accuracy/top1 = 0.92
I0814 19:50:32.918107 10755 caffe.cpp:313] Batch 143, accuracy/top5 = 1
I0814 19:50:32.918110 10755 caffe.cpp:313] Batch 143, loss = 0.361423
I0814 19:50:32.926604 10755 caffe.cpp:313] Batch 144, accuracy/top1 = 0.9
I0814 19:50:32.926612 10755 caffe.cpp:313] Batch 144, accuracy/top5 = 1
I0814 19:50:32.926625 10755 caffe.cpp:313] Batch 144, loss = 0.435919
I0814 19:50:32.935071 10755 caffe.cpp:313] Batch 145, accuracy/top1 = 0.94
I0814 19:50:32.935079 10755 caffe.cpp:313] Batch 145, accuracy/top5 = 1
I0814 19:50:32.935083 10755 caffe.cpp:313] Batch 145, loss = 0.289341
I0814 19:50:32.943620 10755 caffe.cpp:313] Batch 146, accuracy/top1 = 0.94
I0814 19:50:32.943636 10755 caffe.cpp:313] Batch 146, accuracy/top5 = 1
I0814 19:50:32.943640 10755 caffe.cpp:313] Batch 146, loss = 0.37779
I0814 19:50:32.952137 10755 caffe.cpp:313] Batch 147, accuracy/top1 = 0.9
I0814 19:50:32.952147 10755 caffe.cpp:313] Batch 147, accuracy/top5 = 1
I0814 19:50:32.952152 10755 caffe.cpp:313] Batch 147, loss = 0.330188
I0814 19:50:32.960664 10755 caffe.cpp:313] Batch 148, accuracy/top1 = 0.9
I0814 19:50:32.960672 10755 caffe.cpp:313] Batch 148, accuracy/top5 = 1
I0814 19:50:32.960676 10755 caffe.cpp:313] Batch 148, loss = 0.396103
I0814 19:50:32.969115 10755 caffe.cpp:313] Batch 149, accuracy/top1 = 0.9
I0814 19:50:32.969122 10755 caffe.cpp:313] Batch 149, accuracy/top5 = 1
I0814 19:50:32.969126 10755 caffe.cpp:313] Batch 149, loss = 0.562689
I0814 19:50:32.977692 10755 caffe.cpp:313] Batch 150, accuracy/top1 = 0.88
I0814 19:50:32.977711 10755 caffe.cpp:313] Batch 150, accuracy/top5 = 0.98
I0814 19:50:32.977715 10755 caffe.cpp:313] Batch 150, loss = 0.45187
I0814 19:50:32.986202 10755 caffe.cpp:313] Batch 151, accuracy/top1 = 0.9
I0814 19:50:32.986209 10755 caffe.cpp:313] Batch 151, accuracy/top5 = 0.98
I0814 19:50:32.986213 10755 caffe.cpp:313] Batch 151, loss = 0.473166
I0814 19:50:32.994729 10755 caffe.cpp:313] Batch 152, accuracy/top1 = 0.78
I0814 19:50:32.994737 10755 caffe.cpp:313] Batch 152, accuracy/top5 = 1
I0814 19:50:32.994740 10755 caffe.cpp:313] Batch 152, loss = 0.616565
I0814 19:50:33.003226 10755 caffe.cpp:313] Batch 153, accuracy/top1 = 0.92
I0814 19:50:33.003239 10755 caffe.cpp:313] Batch 153, accuracy/top5 = 0.98
I0814 19:50:33.003243 10755 caffe.cpp:313] Batch 153, loss = 0.908251
I0814 19:50:33.011775 10755 caffe.cpp:313] Batch 154, accuracy/top1 = 0.94
I0814 19:50:33.011783 10755 caffe.cpp:313] Batch 154, accuracy/top5 = 1
I0814 19:50:33.011787 10755 caffe.cpp:313] Batch 154, loss = 0.346939
I0814 19:50:33.020248 10755 caffe.cpp:313] Batch 155, accuracy/top1 = 0.86
I0814 19:50:33.020256 10755 caffe.cpp:313] Batch 155, accuracy/top5 = 1
I0814 19:50:33.020261 10755 caffe.cpp:313] Batch 155, loss = 0.500325
I0814 19:50:33.028751 10755 caffe.cpp:313] Batch 156, accuracy/top1 = 0.94
I0814 19:50:33.028759 10755 caffe.cpp:313] Batch 156, accuracy/top5 = 1
I0814 19:50:33.028764 10755 caffe.cpp:313] Batch 156, loss = 0.349172
I0814 19:50:33.037273 10755 caffe.cpp:313] Batch 157, accuracy/top1 = 0.92
I0814 19:50:33.037282 10755 caffe.cpp:313] Batch 157, accuracy/top5 = 0.98
I0814 19:50:33.037287 10755 caffe.cpp:313] Batch 157, loss = 0.374471
I0814 19:50:33.045770 10755 caffe.cpp:313] Batch 158, accuracy/top1 = 0.9
I0814 19:50:33.045778 10755 caffe.cpp:313] Batch 158, accuracy/top5 = 1
I0814 19:50:33.045783 10755 caffe.cpp:313] Batch 158, loss = 0.244353
I0814 19:50:33.054241 10755 caffe.cpp:313] Batch 159, accuracy/top1 = 0.96
I0814 19:50:33.054250 10755 caffe.cpp:313] Batch 159, accuracy/top5 = 1
I0814 19:50:33.054253 10755 caffe.cpp:313] Batch 159, loss = 0.228276
I0814 19:50:33.062759 10755 caffe.cpp:313] Batch 160, accuracy/top1 = 0.9
I0814 19:50:33.062768 10755 caffe.cpp:313] Batch 160, accuracy/top5 = 1
I0814 19:50:33.062772 10755 caffe.cpp:313] Batch 160, loss = 0.345793
I0814 19:50:33.071219 10755 caffe.cpp:313] Batch 161, accuracy/top1 = 0.92
I0814 19:50:33.071228 10755 caffe.cpp:313] Batch 161, accuracy/top5 = 1
I0814 19:50:33.071233 10755 caffe.cpp:313] Batch 161, loss = 0.260705
I0814 19:50:33.079715 10755 caffe.cpp:313] Batch 162, accuracy/top1 = 0.92
I0814 19:50:33.079722 10755 caffe.cpp:313] Batch 162, accuracy/top5 = 1
I0814 19:50:33.079726 10755 caffe.cpp:313] Batch 162, loss = 0.311747
I0814 19:50:33.088215 10755 caffe.cpp:313] Batch 163, accuracy/top1 = 0.9
I0814 19:50:33.088232 10755 caffe.cpp:313] Batch 163, accuracy/top5 = 1
I0814 19:50:33.088235 10755 caffe.cpp:313] Batch 163, loss = 0.313155
I0814 19:50:33.096724 10755 caffe.cpp:313] Batch 164, accuracy/top1 = 0.9
I0814 19:50:33.096732 10755 caffe.cpp:313] Batch 164, accuracy/top5 = 1
I0814 19:50:33.096736 10755 caffe.cpp:313] Batch 164, loss = 0.372354
I0814 19:50:33.105237 10755 caffe.cpp:313] Batch 165, accuracy/top1 = 0.94
I0814 19:50:33.105249 10755 caffe.cpp:313] Batch 165, accuracy/top5 = 1
I0814 19:50:33.105252 10755 caffe.cpp:313] Batch 165, loss = 0.208863
I0814 19:50:33.113818 10755 caffe.cpp:313] Batch 166, accuracy/top1 = 0.9
I0814 19:50:33.113842 10755 caffe.cpp:313] Batch 166, accuracy/top5 = 1
I0814 19:50:33.113847 10755 caffe.cpp:313] Batch 166, loss = 0.316849
I0814 19:50:33.122393 10755 caffe.cpp:313] Batch 167, accuracy/top1 = 0.94
I0814 19:50:33.122403 10755 caffe.cpp:313] Batch 167, accuracy/top5 = 0.98
I0814 19:50:33.122407 10755 caffe.cpp:313] Batch 167, loss = 0.337274
I0814 19:50:33.130931 10755 caffe.cpp:313] Batch 168, accuracy/top1 = 0.92
I0814 19:50:33.130940 10755 caffe.cpp:313] Batch 168, accuracy/top5 = 0.98
I0814 19:50:33.130944 10755 caffe.cpp:313] Batch 168, loss = 0.442809
I0814 19:50:33.139436 10755 caffe.cpp:313] Batch 169, accuracy/top1 = 0.86
I0814 19:50:33.139444 10755 caffe.cpp:313] Batch 169, accuracy/top5 = 1
I0814 19:50:33.139448 10755 caffe.cpp:313] Batch 169, loss = 0.536299
I0814 19:50:33.147909 10755 caffe.cpp:313] Batch 170, accuracy/top1 = 0.88
I0814 19:50:33.147917 10755 caffe.cpp:313] Batch 170, accuracy/top5 = 1
I0814 19:50:33.147922 10755 caffe.cpp:313] Batch 170, loss = 0.398131
I0814 19:50:33.156442 10755 caffe.cpp:313] Batch 171, accuracy/top1 = 0.88
I0814 19:50:33.156452 10755 caffe.cpp:313] Batch 171, accuracy/top5 = 0.98
I0814 19:50:33.156456 10755 caffe.cpp:313] Batch 171, loss = 0.597865
I0814 19:50:33.164904 10755 caffe.cpp:313] Batch 172, accuracy/top1 = 0.96
I0814 19:50:33.164912 10755 caffe.cpp:313] Batch 172, accuracy/top5 = 1
I0814 19:50:33.164916 10755 caffe.cpp:313] Batch 172, loss = 0.126567
I0814 19:50:33.173426 10755 caffe.cpp:313] Batch 173, accuracy/top1 = 0.98
I0814 19:50:33.173434 10755 caffe.cpp:313] Batch 173, accuracy/top5 = 1
I0814 19:50:33.173439 10755 caffe.cpp:313] Batch 173, loss = 0.133908
I0814 19:50:33.181910 10755 caffe.cpp:313] Batch 174, accuracy/top1 = 0.86
I0814 19:50:33.181916 10755 caffe.cpp:313] Batch 174, accuracy/top5 = 1
I0814 19:50:33.181921 10755 caffe.cpp:313] Batch 174, loss = 0.690073
I0814 19:50:33.190446 10755 caffe.cpp:313] Batch 175, accuracy/top1 = 0.86
I0814 19:50:33.190455 10755 caffe.cpp:313] Batch 175, accuracy/top5 = 1
I0814 19:50:33.190459 10755 caffe.cpp:313] Batch 175, loss = 0.444352
I0814 19:50:33.198917 10755 caffe.cpp:313] Batch 176, accuracy/top1 = 0.84
I0814 19:50:33.198926 10755 caffe.cpp:313] Batch 176, accuracy/top5 = 1
I0814 19:50:33.198930 10755 caffe.cpp:313] Batch 176, loss = 0.544615
I0814 19:50:33.207475 10755 caffe.cpp:313] Batch 177, accuracy/top1 = 0.92
I0814 19:50:33.207504 10755 caffe.cpp:313] Batch 177, accuracy/top5 = 1
I0814 19:50:33.207509 10755 caffe.cpp:313] Batch 177, loss = 0.152009
I0814 19:50:33.216127 10755 caffe.cpp:313] Batch 178, accuracy/top1 = 0.88
I0814 19:50:33.216156 10755 caffe.cpp:313] Batch 178, accuracy/top5 = 0.98
I0814 19:50:33.216159 10755 caffe.cpp:313] Batch 178, loss = 0.40595
I0814 19:50:33.224776 10755 caffe.cpp:313] Batch 179, accuracy/top1 = 0.88
I0814 19:50:33.224797 10755 caffe.cpp:313] Batch 179, accuracy/top5 = 1
I0814 19:50:33.224800 10755 caffe.cpp:313] Batch 179, loss = 0.462325
I0814 19:50:33.233296 10755 caffe.cpp:313] Batch 180, accuracy/top1 = 0.94
I0814 19:50:33.233305 10755 caffe.cpp:313] Batch 180, accuracy/top5 = 1
I0814 19:50:33.233309 10755 caffe.cpp:313] Batch 180, loss = 0.234718
I0814 19:50:33.241787 10755 caffe.cpp:313] Batch 181, accuracy/top1 = 0.92
I0814 19:50:33.241796 10755 caffe.cpp:313] Batch 181, accuracy/top5 = 1
I0814 19:50:33.241799 10755 caffe.cpp:313] Batch 181, loss = 0.256871
I0814 19:50:33.250322 10755 caffe.cpp:313] Batch 182, accuracy/top1 = 0.92
I0814 19:50:33.250340 10755 caffe.cpp:313] Batch 182, accuracy/top5 = 1
I0814 19:50:33.250345 10755 caffe.cpp:313] Batch 182, loss = 0.296282
I0814 19:50:33.258802 10755 caffe.cpp:313] Batch 183, accuracy/top1 = 0.94
I0814 19:50:33.258811 10755 caffe.cpp:313] Batch 183, accuracy/top5 = 1
I0814 19:50:33.258816 10755 caffe.cpp:313] Batch 183, loss = 0.267656
I0814 19:50:33.267345 10755 caffe.cpp:313] Batch 184, accuracy/top1 = 0.86
I0814 19:50:33.267364 10755 caffe.cpp:313] Batch 184, accuracy/top5 = 1
I0814 19:50:33.267366 10755 caffe.cpp:313] Batch 184, loss = 0.574574
I0814 19:50:33.275954 10755 caffe.cpp:313] Batch 185, accuracy/top1 = 0.92
I0814 19:50:33.275966 10755 caffe.cpp:313] Batch 185, accuracy/top5 = 0.98
I0814 19:50:33.275970 10755 caffe.cpp:313] Batch 185, loss = 0.719346
I0814 19:50:33.284566 10755 caffe.cpp:313] Batch 186, accuracy/top1 = 0.92
I0814 19:50:33.284585 10755 caffe.cpp:313] Batch 186, accuracy/top5 = 1
I0814 19:50:33.284588 10755 caffe.cpp:313] Batch 186, loss = 0.209358
I0814 19:50:33.293164 10755 caffe.cpp:313] Batch 187, accuracy/top1 = 0.9
I0814 19:50:33.293179 10755 caffe.cpp:313] Batch 187, accuracy/top5 = 0.98
I0814 19:50:33.293181 10755 caffe.cpp:313] Batch 187, loss = 0.656761
I0814 19:50:33.301700 10755 caffe.cpp:313] Batch 188, accuracy/top1 = 0.94
I0814 19:50:33.301713 10755 caffe.cpp:313] Batch 188, accuracy/top5 = 1
I0814 19:50:33.301717 10755 caffe.cpp:313] Batch 188, loss = 0.277387
I0814 19:50:33.310313 10755 caffe.cpp:313] Batch 189, accuracy/top1 = 0.9
I0814 19:50:33.310333 10755 caffe.cpp:313] Batch 189, accuracy/top5 = 1
I0814 19:50:33.310335 10755 caffe.cpp:313] Batch 189, loss = 0.222446
I0814 19:50:33.318912 10755 caffe.cpp:313] Batch 190, accuracy/top1 = 0.88
I0814 19:50:33.318928 10755 caffe.cpp:313] Batch 190, accuracy/top5 = 1
I0814 19:50:33.318931 10755 caffe.cpp:313] Batch 190, loss = 0.568371
I0814 19:50:33.327468 10755 caffe.cpp:313] Batch 191, accuracy/top1 = 0.9
I0814 19:50:33.327478 10755 caffe.cpp:313] Batch 191, accuracy/top5 = 1
I0814 19:50:33.327481 10755 caffe.cpp:313] Batch 191, loss = 0.400969
I0814 19:50:33.335973 10755 caffe.cpp:313] Batch 192, accuracy/top1 = 0.88
I0814 19:50:33.335986 10755 caffe.cpp:313] Batch 192, accuracy/top5 = 0.98
I0814 19:50:33.335989 10755 caffe.cpp:313] Batch 192, loss = 0.551783
I0814 19:50:33.344646 10755 caffe.cpp:313] Batch 193, accuracy/top1 = 0.96
I0814 19:50:33.344665 10755 caffe.cpp:313] Batch 193, accuracy/top5 = 1
I0814 19:50:33.344667 10755 caffe.cpp:313] Batch 193, loss = 0.0642611
I0814 19:50:33.353185 10755 caffe.cpp:313] Batch 194, accuracy/top1 = 0.88
I0814 19:50:33.353197 10755 caffe.cpp:313] Batch 194, accuracy/top5 = 0.98
I0814 19:50:33.353200 10755 caffe.cpp:313] Batch 194, loss = 0.770513
I0814 19:50:33.361752 10755 caffe.cpp:313] Batch 195, accuracy/top1 = 0.88
I0814 19:50:33.361765 10755 caffe.cpp:313] Batch 195, accuracy/top5 = 0.98
I0814 19:50:33.361768 10755 caffe.cpp:313] Batch 195, loss = 0.294953
I0814 19:50:33.370307 10755 caffe.cpp:313] Batch 196, accuracy/top1 = 0.9
I0814 19:50:33.370324 10755 caffe.cpp:313] Batch 196, accuracy/top5 = 1
I0814 19:50:33.370327 10755 caffe.cpp:313] Batch 196, loss = 0.456237
I0814 19:50:33.370844 10778 data_reader.cpp:288] Starting prefetch of epoch 1
I0814 19:50:33.378948 10755 caffe.cpp:313] Batch 197, accuracy/top1 = 0.94
I0814 19:50:33.378964 10755 caffe.cpp:313] Batch 197, accuracy/top5 = 1
I0814 19:50:33.378968 10755 caffe.cpp:313] Batch 197, loss = 0.251772
I0814 19:50:33.387473 10755 caffe.cpp:313] Batch 198, accuracy/top1 = 0.98
I0814 19:50:33.387485 10755 caffe.cpp:313] Batch 198, accuracy/top5 = 1
I0814 19:50:33.387487 10755 caffe.cpp:313] Batch 198, loss = 0.152817
I0814 19:50:33.396008 10755 caffe.cpp:313] Batch 199, accuracy/top1 = 0.92
I0814 19:50:33.396019 10755 caffe.cpp:313] Batch 199, accuracy/top5 = 1
I0814 19:50:33.396023 10755 caffe.cpp:313] Batch 199, loss = 0.424113
I0814 19:50:33.396024 10755 caffe.cpp:318] Loss: 0.386379
I0814 19:50:33.396031 10755 caffe.cpp:330] accuracy/top1 = 0.9094
I0814 19:50:33.396045 10755 caffe.cpp:330] accuracy/top5 = 0.9961
I0814 19:50:33.396050 10755 caffe.cpp:330] loss = 0.386379 (* 1 = 0.386379 loss)
I0817 10:56:45.067646 13090 caffe.cpp:608] This is NVCaffe 0.16.3 started at Thu Aug 17 10:56:44 2017
I0817 10:56:45.067780 13090 caffe.cpp:611] CuDNN version: 6021
I0817 10:56:45.067785 13090 caffe.cpp:612] CuBLAS version: 8000
I0817 10:56:45.067787 13090 caffe.cpp:613] CUDA version: 8000
I0817 10:56:45.067790 13090 caffe.cpp:614] CUDA driver version: 8000
I0817 10:56:45.067798 13090 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0817 10:56:45.067802 13090 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0817 10:56:45.068411 13090 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0817 10:56:45.069005 13090 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0817 10:56:45.069010 13090 caffe.cpp:275] Use GPU with device ID 0
I0817 10:56:45.069382 13090 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0817 10:56:45.070638 13090 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
quantize: true
I0817 10:56:45.070775 13090 net.cpp:104] Using FLOAT as default forward math type
I0817 10:56:45.070780 13090 net.cpp:110] Using FLOAT as default backward math type
I0817 10:56:45.070783 13090 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0817 10:56:45.070787 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.070837 13090 net.cpp:184] Created Layer data (0)
I0817 10:56:45.070843 13090 net.cpp:530] data -> data
I0817 10:56:45.070853 13090 net.cpp:530] data -> label
I0817 10:56:45.070870 13090 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0817 10:56:45.071169 13090 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0817 10:56:45.077896 13129 db_lmdb.cpp:24] Opened lmdb ./data/cifar10_test_lmdb
I0817 10:56:45.078531 13090 data_layer.cpp:185] (0) ReshapePrefetch 50, 3, 32, 32
I0817 10:56:45.078567 13090 data_layer.cpp:209] (0) Output data size: 50, 3, 32, 32
I0817 10:56:45.078573 13090 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0817 10:56:45.078595 13090 net.cpp:245] Setting up data
I0817 10:56:45.078606 13090 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0817 10:56:45.078626 13090 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0817 10:56:45.078635 13090 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0817 10:56:45.078649 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.078662 13090 net.cpp:184] Created Layer label_data_1_split (1)
I0817 10:56:45.078667 13090 net.cpp:561] label_data_1_split <- label
I0817 10:56:45.078677 13090 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0817 10:56:45.078683 13090 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0817 10:56:45.078696 13090 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0817 10:56:45.078728 13090 net.cpp:245] Setting up label_data_1_split
I0817 10:56:45.078733 13090 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:56:45.078744 13090 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:56:45.078749 13090 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0817 10:56:45.078753 13090 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0817 10:56:45.078757 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.078768 13090 net.cpp:184] Created Layer data/bias (2)
I0817 10:56:45.078771 13090 net.cpp:561] data/bias <- data
I0817 10:56:45.078776 13090 net.cpp:530] data/bias -> data/bias
I0817 10:56:45.079766 13130 data_layer.cpp:97] (0) Parser threads: 1
I0817 10:56:45.079774 13130 data_layer.cpp:99] (0) Transformer threads: 1
I0817 10:56:45.080616 13090 net.cpp:245] Setting up data/bias
I0817 10:56:45.080626 13090 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0817 10:56:45.080638 13090 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0817 10:56:45.080643 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.080658 13090 net.cpp:184] Created Layer conv1a (3)
I0817 10:56:45.080662 13090 net.cpp:561] conv1a <- data/bias
I0817 10:56:45.080667 13090 net.cpp:530] conv1a -> conv1a
I0817 10:56:45.366888 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.15G, req 0G)
I0817 10:56:45.366907 13090 net.cpp:245] Setting up conv1a
I0817 10:56:45.366915 13090 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0817 10:56:45.366926 13090 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0817 10:56:45.366932 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.366946 13090 net.cpp:184] Created Layer conv1a/bn (4)
I0817 10:56:45.366951 13090 net.cpp:561] conv1a/bn <- conv1a
I0817 10:56:45.366956 13090 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0817 10:56:45.367395 13090 net.cpp:245] Setting up conv1a/bn
I0817 10:56:45.367403 13090 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0817 10:56:45.367413 13090 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0817 10:56:45.367418 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.367429 13090 net.cpp:184] Created Layer conv1a/relu (5)
I0817 10:56:45.367432 13090 net.cpp:561] conv1a/relu <- conv1a
I0817 10:56:45.367436 13090 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0817 10:56:45.367449 13090 net.cpp:245] Setting up conv1a/relu
I0817 10:56:45.367455 13090 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0817 10:56:45.367458 13090 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0817 10:56:45.367462 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.367475 13090 net.cpp:184] Created Layer conv1b (6)
I0817 10:56:45.367480 13090 net.cpp:561] conv1b <- conv1a
I0817 10:56:45.367483 13090 net.cpp:530] conv1b -> conv1b
I0817 10:56:45.371485 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.13G, req 0G)
I0817 10:56:45.371496 13090 net.cpp:245] Setting up conv1b
I0817 10:56:45.371502 13090 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0817 10:56:45.371511 13090 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0817 10:56:45.371515 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.371523 13090 net.cpp:184] Created Layer conv1b/bn (7)
I0817 10:56:45.371527 13090 net.cpp:561] conv1b/bn <- conv1b
I0817 10:56:45.371531 13090 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0817 10:56:45.371942 13090 net.cpp:245] Setting up conv1b/bn
I0817 10:56:45.371949 13090 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0817 10:56:45.371958 13090 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0817 10:56:45.371963 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.371968 13090 net.cpp:184] Created Layer conv1b/relu (8)
I0817 10:56:45.371971 13090 net.cpp:561] conv1b/relu <- conv1b
I0817 10:56:45.371975 13090 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0817 10:56:45.371984 13090 net.cpp:245] Setting up conv1b/relu
I0817 10:56:45.371989 13090 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0817 10:56:45.371994 13090 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0817 10:56:45.371997 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.372005 13090 net.cpp:184] Created Layer pool1 (9)
I0817 10:56:45.372009 13090 net.cpp:561] pool1 <- conv1b
I0817 10:56:45.372012 13090 net.cpp:530] pool1 -> pool1
I0817 10:56:45.372053 13090 net.cpp:245] Setting up pool1
I0817 10:56:45.372059 13090 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0817 10:56:45.372063 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0817 10:56:45.372068 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.372076 13090 net.cpp:184] Created Layer res2a_branch2a (10)
I0817 10:56:45.372081 13090 net.cpp:561] res2a_branch2a <- pool1
I0817 10:56:45.372083 13090 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0817 10:56:45.377739 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.11G, req 0G)
I0817 10:56:45.377753 13090 net.cpp:245] Setting up res2a_branch2a
I0817 10:56:45.377758 13090 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0817 10:56:45.377768 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0817 10:56:45.377773 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.377779 13090 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0817 10:56:45.377784 13090 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0817 10:56:45.377789 13090 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0817 10:56:45.378199 13090 net.cpp:245] Setting up res2a_branch2a/bn
I0817 10:56:45.378207 13090 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0817 10:56:45.378216 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0817 10:56:45.378221 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.378226 13090 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0817 10:56:45.378229 13090 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0817 10:56:45.378233 13090 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0817 10:56:45.378239 13090 net.cpp:245] Setting up res2a_branch2a/relu
I0817 10:56:45.378244 13090 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0817 10:56:45.378248 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0817 10:56:45.378253 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.378264 13090 net.cpp:184] Created Layer res2a_branch2b (13)
I0817 10:56:45.378268 13090 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0817 10:56:45.378273 13090 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0817 10:56:45.381749 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.09G, req 0G)
I0817 10:56:45.381762 13090 net.cpp:245] Setting up res2a_branch2b
I0817 10:56:45.381767 13090 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0817 10:56:45.381783 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0817 10:56:45.381788 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.381796 13090 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0817 10:56:45.381800 13090 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0817 10:56:45.381804 13090 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0817 10:56:45.382216 13090 net.cpp:245] Setting up res2a_branch2b/bn
I0817 10:56:45.382225 13090 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0817 10:56:45.382233 13090 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0817 10:56:45.382237 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.382242 13090 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0817 10:56:45.382246 13090 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0817 10:56:45.382251 13090 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0817 10:56:45.382257 13090 net.cpp:245] Setting up res2a_branch2b/relu
I0817 10:56:45.382261 13090 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0817 10:56:45.382266 13090 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0817 10:56:45.382269 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.382277 13090 net.cpp:184] Created Layer pool2 (16)
I0817 10:56:45.382280 13090 net.cpp:561] pool2 <- res2a_branch2b
I0817 10:56:45.382284 13090 net.cpp:530] pool2 -> pool2
I0817 10:56:45.382316 13090 net.cpp:245] Setting up pool2
I0817 10:56:45.382323 13090 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0817 10:56:45.382326 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0817 10:56:45.382331 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.382340 13090 net.cpp:184] Created Layer res3a_branch2a (17)
I0817 10:56:45.382344 13090 net.cpp:561] res3a_branch2a <- pool2
I0817 10:56:45.382349 13090 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0817 10:56:45.387804 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0G)
I0817 10:56:45.387814 13090 net.cpp:245] Setting up res3a_branch2a
I0817 10:56:45.387820 13090 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0817 10:56:45.387827 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0817 10:56:45.387832 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.387838 13090 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0817 10:56:45.387842 13090 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0817 10:56:45.387846 13090 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0817 10:56:45.388249 13090 net.cpp:245] Setting up res3a_branch2a/bn
I0817 10:56:45.388257 13090 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0817 10:56:45.388267 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0817 10:56:45.388272 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.388276 13090 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0817 10:56:45.388280 13090 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0817 10:56:45.388284 13090 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0817 10:56:45.388290 13090 net.cpp:245] Setting up res3a_branch2a/relu
I0817 10:56:45.388296 13090 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0817 10:56:45.388301 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0817 10:56:45.388304 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.388320 13090 net.cpp:184] Created Layer res3a_branch2b (20)
I0817 10:56:45.388324 13090 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0817 10:56:45.388329 13090 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0817 10:56:45.391377 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.07G, req 0G)
I0817 10:56:45.391386 13090 net.cpp:245] Setting up res3a_branch2b
I0817 10:56:45.391392 13090 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0817 10:56:45.391399 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0817 10:56:45.391404 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.391412 13090 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0817 10:56:45.391417 13090 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0817 10:56:45.391419 13090 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0817 10:56:45.391804 13090 net.cpp:245] Setting up res3a_branch2b/bn
I0817 10:56:45.391811 13090 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0817 10:56:45.391820 13090 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0817 10:56:45.391824 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.391829 13090 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0817 10:56:45.391834 13090 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0817 10:56:45.391837 13090 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0817 10:56:45.391844 13090 net.cpp:245] Setting up res3a_branch2b/relu
I0817 10:56:45.391849 13090 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0817 10:56:45.391852 13090 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0817 10:56:45.391856 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.391862 13090 net.cpp:184] Created Layer pool3 (23)
I0817 10:56:45.391866 13090 net.cpp:561] pool3 <- res3a_branch2b
I0817 10:56:45.391870 13090 net.cpp:530] pool3 -> pool3
I0817 10:56:45.391901 13090 net.cpp:245] Setting up pool3
I0817 10:56:45.391907 13090 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0817 10:56:45.391911 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0817 10:56:45.391916 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.391927 13090 net.cpp:184] Created Layer res4a_branch2a (24)
I0817 10:56:45.391929 13090 net.cpp:561] res4a_branch2a <- pool3
I0817 10:56:45.391933 13090 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0817 10:56:45.405740 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0G)
I0817 10:56:45.405758 13090 net.cpp:245] Setting up res4a_branch2a
I0817 10:56:45.405764 13090 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0817 10:56:45.405773 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0817 10:56:45.405779 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.405791 13090 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0817 10:56:45.405796 13090 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0817 10:56:45.405800 13090 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0817 10:56:45.406229 13090 net.cpp:245] Setting up res4a_branch2a/bn
I0817 10:56:45.406236 13090 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0817 10:56:45.406244 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0817 10:56:45.406250 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.406255 13090 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0817 10:56:45.406267 13090 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0817 10:56:45.406271 13090 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0817 10:56:45.406280 13090 net.cpp:245] Setting up res4a_branch2a/relu
I0817 10:56:45.406285 13090 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0817 10:56:45.406287 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0817 10:56:45.406292 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.406302 13090 net.cpp:184] Created Layer res4a_branch2b (27)
I0817 10:56:45.406306 13090 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0817 10:56:45.406309 13090 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0817 10:56:45.413095 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.03G, req 0G)
I0817 10:56:45.413106 13090 net.cpp:245] Setting up res4a_branch2b
I0817 10:56:45.413113 13090 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0817 10:56:45.413120 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0817 10:56:45.413125 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.413132 13090 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0817 10:56:45.413136 13090 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0817 10:56:45.413141 13090 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0817 10:56:45.413549 13090 net.cpp:245] Setting up res4a_branch2b/bn
I0817 10:56:45.413558 13090 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0817 10:56:45.413566 13090 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0817 10:56:45.413570 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.413575 13090 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0817 10:56:45.413580 13090 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0817 10:56:45.413583 13090 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0817 10:56:45.413590 13090 net.cpp:245] Setting up res4a_branch2b/relu
I0817 10:56:45.413594 13090 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0817 10:56:45.413599 13090 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0817 10:56:45.413604 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.413611 13090 net.cpp:184] Created Layer pool4 (30)
I0817 10:56:45.413615 13090 net.cpp:561] pool4 <- res4a_branch2b
I0817 10:56:45.413619 13090 net.cpp:530] pool4 -> pool4
I0817 10:56:45.413653 13090 net.cpp:245] Setting up pool4
I0817 10:56:45.413660 13090 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0817 10:56:45.413663 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0817 10:56:45.413667 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.413676 13090 net.cpp:184] Created Layer res5a_branch2a (31)
I0817 10:56:45.413681 13090 net.cpp:561] res5a_branch2a <- pool4
I0817 10:56:45.413684 13090 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0817 10:56:45.447432 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 6  (limit 8.01G, req 0.01G)
I0817 10:56:45.447449 13090 net.cpp:245] Setting up res5a_branch2a
I0817 10:56:45.447456 13090 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0817 10:56:45.447463 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0817 10:56:45.447468 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.447479 13090 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0817 10:56:45.447484 13090 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0817 10:56:45.447497 13090 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0817 10:56:45.447932 13090 net.cpp:245] Setting up res5a_branch2a/bn
I0817 10:56:45.447940 13090 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0817 10:56:45.447949 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0817 10:56:45.447953 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.447958 13090 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0817 10:56:45.447962 13090 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0817 10:56:45.447966 13090 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0817 10:56:45.447973 13090 net.cpp:245] Setting up res5a_branch2a/relu
I0817 10:56:45.447978 13090 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0817 10:56:45.447983 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0817 10:56:45.447988 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.447996 13090 net.cpp:184] Created Layer res5a_branch2b (34)
I0817 10:56:45.447999 13090 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0817 10:56:45.448004 13090 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0817 10:56:45.463727 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8G, req 0.01G)
I0817 10:56:45.463744 13090 net.cpp:245] Setting up res5a_branch2b
I0817 10:56:45.463752 13090 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0817 10:56:45.463763 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0817 10:56:45.463769 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.463779 13090 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0817 10:56:45.463784 13090 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0817 10:56:45.463788 13090 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0817 10:56:45.464244 13090 net.cpp:245] Setting up res5a_branch2b/bn
I0817 10:56:45.464253 13090 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0817 10:56:45.464262 13090 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0817 10:56:45.464267 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464272 13090 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0817 10:56:45.464277 13090 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0817 10:56:45.464280 13090 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0817 10:56:45.464287 13090 net.cpp:245] Setting up res5a_branch2b/relu
I0817 10:56:45.464293 13090 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0817 10:56:45.464296 13090 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0817 10:56:45.464300 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464308 13090 net.cpp:184] Created Layer pool5 (37)
I0817 10:56:45.464311 13090 net.cpp:561] pool5 <- res5a_branch2b
I0817 10:56:45.464315 13090 net.cpp:530] pool5 -> pool5
I0817 10:56:45.464335 13090 net.cpp:245] Setting up pool5
I0817 10:56:45.464340 13090 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0817 10:56:45.464345 13090 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0817 10:56:45.464349 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464357 13090 net.cpp:184] Created Layer fc10 (38)
I0817 10:56:45.464361 13090 net.cpp:561] fc10 <- pool5
I0817 10:56:45.464365 13090 net.cpp:530] fc10 -> fc10
I0817 10:56:45.464557 13090 net.cpp:245] Setting up fc10
I0817 10:56:45.464565 13090 net.cpp:252] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0817 10:56:45.464570 13090 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0817 10:56:45.464583 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464589 13090 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0817 10:56:45.464593 13090 net.cpp:561] fc10_fc10_0_split <- fc10
I0817 10:56:45.464597 13090 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0817 10:56:45.464603 13090 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0817 10:56:45.464608 13090 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0817 10:56:45.464648 13090 net.cpp:245] Setting up fc10_fc10_0_split
I0817 10:56:45.464653 13090 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0817 10:56:45.464658 13090 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0817 10:56:45.464663 13090 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0817 10:56:45.464668 13090 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0817 10:56:45.464671 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464682 13090 net.cpp:184] Created Layer loss (40)
I0817 10:56:45.464686 13090 net.cpp:561] loss <- fc10_fc10_0_split_0
I0817 10:56:45.464690 13090 net.cpp:561] loss <- label_data_1_split_0
I0817 10:56:45.464695 13090 net.cpp:530] loss -> loss
I0817 10:56:45.464807 13090 net.cpp:245] Setting up loss
I0817 10:56:45.464814 13090 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0817 10:56:45.464818 13090 net.cpp:256]     with loss weight 1
I0817 10:56:45.464824 13090 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0817 10:56:45.464828 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464839 13090 net.cpp:184] Created Layer accuracy/top1 (41)
I0817 10:56:45.464843 13090 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0817 10:56:45.464848 13090 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0817 10:56:45.464851 13090 net.cpp:530] accuracy/top1 -> accuracy/top1
I0817 10:56:45.464859 13090 net.cpp:245] Setting up accuracy/top1
I0817 10:56:45.464864 13090 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0817 10:56:45.464869 13090 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0817 10:56:45.464872 13090 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0817 10:56:45.464882 13090 net.cpp:184] Created Layer accuracy/top5 (42)
I0817 10:56:45.464886 13090 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0817 10:56:45.464890 13090 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0817 10:56:45.464895 13090 net.cpp:530] accuracy/top5 -> accuracy/top5
I0817 10:56:45.464900 13090 net.cpp:245] Setting up accuracy/top5
I0817 10:56:45.464905 13090 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0817 10:56:45.464910 13090 net.cpp:325] accuracy/top5 does not need backward computation.
I0817 10:56:45.464913 13090 net.cpp:325] accuracy/top1 does not need backward computation.
I0817 10:56:45.464917 13090 net.cpp:323] loss needs backward computation.
I0817 10:56:45.464922 13090 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0817 10:56:45.464926 13090 net.cpp:323] fc10 needs backward computation.
I0817 10:56:45.464929 13090 net.cpp:323] pool5 needs backward computation.
I0817 10:56:45.464933 13090 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0817 10:56:45.464936 13090 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0817 10:56:45.464941 13090 net.cpp:323] res5a_branch2b needs backward computation.
I0817 10:56:45.464943 13090 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0817 10:56:45.464947 13090 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0817 10:56:45.464951 13090 net.cpp:323] res5a_branch2a needs backward computation.
I0817 10:56:45.464954 13090 net.cpp:323] pool4 needs backward computation.
I0817 10:56:45.464957 13090 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0817 10:56:45.464967 13090 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0817 10:56:45.464969 13090 net.cpp:323] res4a_branch2b needs backward computation.
I0817 10:56:45.464973 13090 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0817 10:56:45.464977 13090 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0817 10:56:45.464980 13090 net.cpp:323] res4a_branch2a needs backward computation.
I0817 10:56:45.464984 13090 net.cpp:323] pool3 needs backward computation.
I0817 10:56:45.464988 13090 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0817 10:56:45.464992 13090 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0817 10:56:45.464995 13090 net.cpp:323] res3a_branch2b needs backward computation.
I0817 10:56:45.464998 13090 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0817 10:56:45.465003 13090 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0817 10:56:45.465005 13090 net.cpp:323] res3a_branch2a needs backward computation.
I0817 10:56:45.465009 13090 net.cpp:323] pool2 needs backward computation.
I0817 10:56:45.465013 13090 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0817 10:56:45.465016 13090 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0817 10:56:45.465019 13090 net.cpp:323] res2a_branch2b needs backward computation.
I0817 10:56:45.465023 13090 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0817 10:56:45.465026 13090 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0817 10:56:45.465030 13090 net.cpp:323] res2a_branch2a needs backward computation.
I0817 10:56:45.465034 13090 net.cpp:323] pool1 needs backward computation.
I0817 10:56:45.465037 13090 net.cpp:323] conv1b/relu needs backward computation.
I0817 10:56:45.465041 13090 net.cpp:323] conv1b/bn needs backward computation.
I0817 10:56:45.465045 13090 net.cpp:323] conv1b needs backward computation.
I0817 10:56:45.465049 13090 net.cpp:323] conv1a/relu needs backward computation.
I0817 10:56:45.465054 13090 net.cpp:323] conv1a/bn needs backward computation.
I0817 10:56:45.465056 13090 net.cpp:323] conv1a needs backward computation.
I0817 10:56:45.465060 13090 net.cpp:325] data/bias does not need backward computation.
I0817 10:56:45.465065 13090 net.cpp:325] label_data_1_split does not need backward computation.
I0817 10:56:45.465070 13090 net.cpp:325] data does not need backward computation.
I0817 10:56:45.465073 13090 net.cpp:367] This network produces output accuracy/top1
I0817 10:56:45.465077 13090 net.cpp:367] This network produces output accuracy/top5
I0817 10:56:45.465080 13090 net.cpp:367] This network produces output loss
I0817 10:56:45.465113 13090 net.cpp:389] Top memory (TEST) required for data: 275251200 diff: 8
I0817 10:56:45.465116 13090 net.cpp:392] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0817 10:56:45.465121 13090 net.cpp:395] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0817 10:56:45.465123 13090 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0817 10:56:45.465126 13090 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0817 10:56:45.465129 13090 net.cpp:407] Network initialization done.
I0817 10:56:45.468950 13090 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0817 10:56:45.468968 13090 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0817 10:56:45.469003 13090 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0817 10:56:45.469018 13090 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469162 13090 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0817 10:56:45.469168 13090 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0817 10:56:45.469182 13090 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469279 13090 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0817 10:56:45.469285 13090 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0817 10:56:45.469296 13090 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0817 10:56:45.469316 13090 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469408 13090 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0817 10:56:45.469413 13090 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0817 10:56:45.469427 13090 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469519 13090 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0817 10:56:45.469524 13090 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0817 10:56:45.469527 13090 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0817 10:56:45.469566 13090 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469650 13090 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0817 10:56:45.469656 13090 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0817 10:56:45.469681 13090 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469758 13090 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0817 10:56:45.469764 13090 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0817 10:56:45.469768 13090 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0817 10:56:45.469874 13090 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:56:45.469954 13090 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0817 10:56:45.469960 13090 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0817 10:56:45.470019 13090 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:56:45.470104 13090 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0817 10:56:45.470109 13090 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0817 10:56:45.470113 13090 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0817 10:56:45.470438 13090 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0817 10:56:45.470527 13090 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0817 10:56:45.470533 13090 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0817 10:56:45.470691 13090 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0817 10:56:45.470773 13090 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0817 10:56:45.470779 13090 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0817 10:56:45.470782 13090 net.cpp:1095] Copying source layer fc10 Type:InnerProduct #blobs=2
I0817 10:56:45.470794 13090 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0817 10:56:45.470859 13090 caffe.cpp:290] Running for 200 iterations.
I0817 10:56:45.473675 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0817 10:56:45.477483 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0817 10:56:45.483161 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0817 10:56:45.487105 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0817 10:56:45.492678 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0.01G)
I0817 10:56:45.495896 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0817 10:56:45.504179 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.85G, req 0.01G)
I0817 10:56:45.509001 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0.01G)
I0817 10:56:45.519584 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.79G, req 0.01G)
I0817 10:56:45.524695 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.78G, req 0.01G)
I0817 10:56:45.546489 13090 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0817 10:56:45.546514 13090 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0817 10:56:45.546519 13090 caffe.cpp:313] Batch 0, loss = 0.100625
I0817 10:56:45.546524 13090 net.cpp:1620] Adding quantization params at infer/iter index: 1
I0817 10:56:45.567019 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 7.02G, req 0.01G)
I0817 10:56:45.571694 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 1  (limit 6.28G, req 0.01G)
I0817 10:56:45.581414 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.586927 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.594473 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.598047 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.613203 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.619249 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 6.28G, req 0.01G)
I0817 10:56:45.642210 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 6.28G, req 0.05G)
I0817 10:56:45.648738 13090 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 6  (limit 6.28G, req 0.05G)
I0817 10:56:45.668464 13090 caffe.cpp:313] Batch 1, accuracy/top1 = 0.88
I0817 10:56:45.668474 13090 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0817 10:56:45.668478 13090 caffe.cpp:313] Batch 1, loss = 0.412283
I0817 10:56:45.697438 13090 caffe.cpp:313] Batch 2, accuracy/top1 = 0.92
I0817 10:56:45.697460 13090 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0817 10:56:45.697463 13090 caffe.cpp:313] Batch 2, loss = 0.386679
I0817 10:56:45.725821 13090 caffe.cpp:313] Batch 3, accuracy/top1 = 0.94
I0817 10:56:45.725841 13090 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0817 10:56:45.725844 13090 caffe.cpp:313] Batch 3, loss = 0.344895
I0817 10:56:45.754405 13090 caffe.cpp:313] Batch 4, accuracy/top1 = 0.82
I0817 10:56:45.754420 13090 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0817 10:56:45.754423 13090 caffe.cpp:313] Batch 4, loss = 0.751796
I0817 10:56:45.782744 13090 caffe.cpp:313] Batch 5, accuracy/top1 = 0.92
I0817 10:56:45.782763 13090 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0817 10:56:45.782765 13090 caffe.cpp:313] Batch 5, loss = 0.276413
I0817 10:56:45.811089 13090 caffe.cpp:313] Batch 6, accuracy/top1 = 0.92
I0817 10:56:45.811107 13090 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0817 10:56:45.811110 13090 caffe.cpp:313] Batch 6, loss = 0.260274
I0817 10:56:45.839519 13090 caffe.cpp:313] Batch 7, accuracy/top1 = 0.88
I0817 10:56:45.839539 13090 caffe.cpp:313] Batch 7, accuracy/top5 = 0.98
I0817 10:56:45.839541 13090 caffe.cpp:313] Batch 7, loss = 0.563354
I0817 10:56:45.868070 13090 caffe.cpp:313] Batch 8, accuracy/top1 = 0.92
I0817 10:56:45.868089 13090 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0817 10:56:45.868093 13090 caffe.cpp:313] Batch 8, loss = 0.160036
I0817 10:56:45.896348 13090 caffe.cpp:313] Batch 9, accuracy/top1 = 0.96
I0817 10:56:45.896370 13090 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0817 10:56:45.896373 13090 caffe.cpp:313] Batch 9, loss = 0.0834524
I0817 10:56:45.924547 13090 caffe.cpp:313] Batch 10, accuracy/top1 = 0.96
I0817 10:56:45.924584 13090 caffe.cpp:313] Batch 10, accuracy/top5 = 0.98
I0817 10:56:45.924588 13090 caffe.cpp:313] Batch 10, loss = 0.170259
I0817 10:56:45.952872 13090 caffe.cpp:313] Batch 11, accuracy/top1 = 0.98
I0817 10:56:45.952894 13090 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0817 10:56:45.952898 13090 caffe.cpp:313] Batch 11, loss = 0.0723114
I0817 10:56:45.980962 13090 caffe.cpp:313] Batch 12, accuracy/top1 = 0.98
I0817 10:56:45.980983 13090 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0817 10:56:45.980986 13090 caffe.cpp:313] Batch 12, loss = 0.0340025
I0817 10:56:46.009078 13090 caffe.cpp:313] Batch 13, accuracy/top1 = 0.9
I0817 10:56:46.009101 13090 caffe.cpp:313] Batch 13, accuracy/top5 = 0.98
I0817 10:56:46.009104 13090 caffe.cpp:313] Batch 13, loss = 0.426724
I0817 10:56:46.037271 13090 caffe.cpp:313] Batch 14, accuracy/top1 = 0.86
I0817 10:56:46.037293 13090 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0817 10:56:46.037297 13090 caffe.cpp:313] Batch 14, loss = 0.487667
I0817 10:56:46.065332 13090 caffe.cpp:313] Batch 15, accuracy/top1 = 0.88
I0817 10:56:46.065352 13090 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0817 10:56:46.065356 13090 caffe.cpp:313] Batch 15, loss = 0.641143
I0817 10:56:46.093346 13090 caffe.cpp:313] Batch 16, accuracy/top1 = 0.96
I0817 10:56:46.093358 13090 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0817 10:56:46.093361 13090 caffe.cpp:313] Batch 16, loss = 0.490478
I0817 10:56:46.121441 13090 caffe.cpp:313] Batch 17, accuracy/top1 = 0.9
I0817 10:56:46.121462 13090 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0817 10:56:46.121465 13090 caffe.cpp:313] Batch 17, loss = 0.53443
I0817 10:56:46.149545 13090 caffe.cpp:313] Batch 18, accuracy/top1 = 0.88
I0817 10:56:46.149566 13090 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0817 10:56:46.149569 13090 caffe.cpp:313] Batch 18, loss = 0.35055
I0817 10:56:46.177464 13090 caffe.cpp:313] Batch 19, accuracy/top1 = 0.9
I0817 10:56:46.177484 13090 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0817 10:56:46.177487 13090 caffe.cpp:313] Batch 19, loss = 0.314866
I0817 10:56:46.205447 13090 caffe.cpp:313] Batch 20, accuracy/top1 = 0.92
I0817 10:56:46.205468 13090 caffe.cpp:313] Batch 20, accuracy/top5 = 0.98
I0817 10:56:46.205471 13090 caffe.cpp:313] Batch 20, loss = 0.318336
I0817 10:56:46.233562 13090 caffe.cpp:313] Batch 21, accuracy/top1 = 0.86
I0817 10:56:46.233582 13090 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0817 10:56:46.233585 13090 caffe.cpp:313] Batch 21, loss = 0.451315
I0817 10:56:46.261495 13090 caffe.cpp:313] Batch 22, accuracy/top1 = 0.86
I0817 10:56:46.261517 13090 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0817 10:56:46.261519 13090 caffe.cpp:313] Batch 22, loss = 0.753798
I0817 10:56:46.289470 13090 caffe.cpp:313] Batch 23, accuracy/top1 = 0.86
I0817 10:56:46.289491 13090 caffe.cpp:313] Batch 23, accuracy/top5 = 0.98
I0817 10:56:46.289494 13090 caffe.cpp:313] Batch 23, loss = 0.570205
I0817 10:56:46.317553 13090 caffe.cpp:313] Batch 24, accuracy/top1 = 0.9
I0817 10:56:46.317574 13090 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0817 10:56:46.317575 13090 caffe.cpp:313] Batch 24, loss = 0.445365
I0817 10:56:46.345564 13090 caffe.cpp:313] Batch 25, accuracy/top1 = 0.94
I0817 10:56:46.345587 13090 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0817 10:56:46.345589 13090 caffe.cpp:313] Batch 25, loss = 0.180972
I0817 10:56:46.373667 13090 caffe.cpp:313] Batch 26, accuracy/top1 = 0.88
I0817 10:56:46.373685 13090 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0817 10:56:46.373687 13090 caffe.cpp:313] Batch 26, loss = 0.612638
I0817 10:56:46.401722 13090 caffe.cpp:313] Batch 27, accuracy/top1 = 0.9
I0817 10:56:46.401741 13090 caffe.cpp:313] Batch 27, accuracy/top5 = 0.98
I0817 10:56:46.401746 13090 caffe.cpp:313] Batch 27, loss = 0.429215
I0817 10:56:46.429602 13090 caffe.cpp:313] Batch 28, accuracy/top1 = 0.94
I0817 10:56:46.429615 13090 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0817 10:56:46.429617 13090 caffe.cpp:313] Batch 28, loss = 0.274033
I0817 10:56:46.457589 13090 caffe.cpp:313] Batch 29, accuracy/top1 = 0.9
I0817 10:56:46.457609 13090 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0817 10:56:46.457629 13090 caffe.cpp:313] Batch 29, loss = 0.457347
I0817 10:56:46.485654 13090 caffe.cpp:313] Batch 30, accuracy/top1 = 0.92
I0817 10:56:46.485674 13090 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0817 10:56:46.485677 13090 caffe.cpp:313] Batch 30, loss = 0.35528
I0817 10:56:46.513643 13090 caffe.cpp:313] Batch 31, accuracy/top1 = 0.92
I0817 10:56:46.513664 13090 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0817 10:56:46.513666 13090 caffe.cpp:313] Batch 31, loss = 0.416111
I0817 10:56:46.541709 13090 caffe.cpp:313] Batch 32, accuracy/top1 = 0.9
I0817 10:56:46.541729 13090 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0817 10:56:46.541733 13090 caffe.cpp:313] Batch 32, loss = 0.570185
I0817 10:56:46.569754 13090 caffe.cpp:313] Batch 33, accuracy/top1 = 0.96
I0817 10:56:46.569775 13090 caffe.cpp:313] Batch 33, accuracy/top5 = 0.98
I0817 10:56:46.569778 13090 caffe.cpp:313] Batch 33, loss = 0.273328
I0817 10:56:46.597755 13090 caffe.cpp:313] Batch 34, accuracy/top1 = 0.9
I0817 10:56:46.597775 13090 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0817 10:56:46.597779 13090 caffe.cpp:313] Batch 34, loss = 0.58576
I0817 10:56:46.625823 13090 caffe.cpp:313] Batch 35, accuracy/top1 = 0.88
I0817 10:56:46.625844 13090 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0817 10:56:46.625846 13090 caffe.cpp:313] Batch 35, loss = 0.308609
I0817 10:56:46.653944 13090 caffe.cpp:313] Batch 36, accuracy/top1 = 0.9
I0817 10:56:46.653964 13090 caffe.cpp:313] Batch 36, accuracy/top5 = 0.98
I0817 10:56:46.653967 13090 caffe.cpp:313] Batch 36, loss = 0.371238
I0817 10:56:46.681963 13090 caffe.cpp:313] Batch 37, accuracy/top1 = 0.88
I0817 10:56:46.681984 13090 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0817 10:56:46.681988 13090 caffe.cpp:313] Batch 37, loss = 0.558127
I0817 10:56:46.710068 13090 caffe.cpp:313] Batch 38, accuracy/top1 = 0.88
I0817 10:56:46.710088 13090 caffe.cpp:313] Batch 38, accuracy/top5 = 0.98
I0817 10:56:46.710090 13090 caffe.cpp:313] Batch 38, loss = 0.751057
I0817 10:56:46.738195 13090 caffe.cpp:313] Batch 39, accuracy/top1 = 0.86
I0817 10:56:46.738237 13090 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0817 10:56:46.738241 13090 caffe.cpp:313] Batch 39, loss = 0.419143
I0817 10:56:46.766537 13090 caffe.cpp:313] Batch 40, accuracy/top1 = 0.9
I0817 10:56:46.766553 13090 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0817 10:56:46.766556 13090 caffe.cpp:313] Batch 40, loss = 0.612968
I0817 10:56:46.794600 13090 caffe.cpp:313] Batch 41, accuracy/top1 = 0.94
I0817 10:56:46.794620 13090 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0817 10:56:46.794621 13090 caffe.cpp:313] Batch 41, loss = 0.198216
I0817 10:56:46.822597 13090 caffe.cpp:313] Batch 42, accuracy/top1 = 0.88
I0817 10:56:46.822618 13090 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0817 10:56:46.822619 13090 caffe.cpp:313] Batch 42, loss = 0.348649
I0817 10:56:46.850666 13090 caffe.cpp:313] Batch 43, accuracy/top1 = 0.82
I0817 10:56:46.850687 13090 caffe.cpp:313] Batch 43, accuracy/top5 = 0.98
I0817 10:56:46.850689 13090 caffe.cpp:313] Batch 43, loss = 0.756153
I0817 10:56:46.878615 13090 caffe.cpp:313] Batch 44, accuracy/top1 = 0.86
I0817 10:56:46.878635 13090 caffe.cpp:313] Batch 44, accuracy/top5 = 0.98
I0817 10:56:46.878638 13090 caffe.cpp:313] Batch 44, loss = 0.941077
I0817 10:56:46.906617 13090 caffe.cpp:313] Batch 45, accuracy/top1 = 0.88
I0817 10:56:46.906638 13090 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0817 10:56:46.906641 13090 caffe.cpp:313] Batch 45, loss = 0.585894
I0817 10:56:46.934695 13090 caffe.cpp:313] Batch 46, accuracy/top1 = 0.94
I0817 10:56:46.934717 13090 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0817 10:56:46.934721 13090 caffe.cpp:313] Batch 46, loss = 0.300705
I0817 10:56:46.962775 13090 caffe.cpp:313] Batch 47, accuracy/top1 = 0.84
I0817 10:56:46.962797 13090 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0817 10:56:46.962800 13090 caffe.cpp:313] Batch 47, loss = 0.593381
I0817 10:56:46.990873 13090 caffe.cpp:313] Batch 48, accuracy/top1 = 0.94
I0817 10:56:46.990895 13090 caffe.cpp:313] Batch 48, accuracy/top5 = 0.98
I0817 10:56:46.990912 13090 caffe.cpp:313] Batch 48, loss = 0.77136
I0817 10:56:47.019043 13090 caffe.cpp:313] Batch 49, accuracy/top1 = 0.92
I0817 10:56:47.019064 13090 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0817 10:56:47.019068 13090 caffe.cpp:313] Batch 49, loss = 0.354651
I0817 10:56:47.047137 13090 caffe.cpp:313] Batch 50, accuracy/top1 = 0.82
I0817 10:56:47.047159 13090 caffe.cpp:313] Batch 50, accuracy/top5 = 0.94
I0817 10:56:47.047163 13090 caffe.cpp:313] Batch 50, loss = 0.97158
I0817 10:56:47.075266 13090 caffe.cpp:313] Batch 51, accuracy/top1 = 0.88
I0817 10:56:47.075287 13090 caffe.cpp:313] Batch 51, accuracy/top5 = 1
I0817 10:56:47.075290 13090 caffe.cpp:313] Batch 51, loss = 0.601748
I0817 10:56:47.103387 13090 caffe.cpp:313] Batch 52, accuracy/top1 = 0.98
I0817 10:56:47.103407 13090 caffe.cpp:313] Batch 52, accuracy/top5 = 1
I0817 10:56:47.103410 13090 caffe.cpp:313] Batch 52, loss = 0.048612
I0817 10:56:47.131357 13090 caffe.cpp:313] Batch 53, accuracy/top1 = 0.94
I0817 10:56:47.131373 13090 caffe.cpp:313] Batch 53, accuracy/top5 = 1
I0817 10:56:47.131376 13090 caffe.cpp:313] Batch 53, loss = 0.272445
I0817 10:56:47.159360 13090 caffe.cpp:313] Batch 54, accuracy/top1 = 0.92
I0817 10:56:47.159379 13090 caffe.cpp:313] Batch 54, accuracy/top5 = 1
I0817 10:56:47.159381 13090 caffe.cpp:313] Batch 54, loss = 0.500511
I0817 10:56:47.187392 13090 caffe.cpp:313] Batch 55, accuracy/top1 = 0.92
I0817 10:56:47.187413 13090 caffe.cpp:313] Batch 55, accuracy/top5 = 1
I0817 10:56:47.187417 13090 caffe.cpp:313] Batch 55, loss = 0.267661
I0817 10:56:47.215382 13090 caffe.cpp:313] Batch 56, accuracy/top1 = 0.84
I0817 10:56:47.215402 13090 caffe.cpp:313] Batch 56, accuracy/top5 = 0.98
I0817 10:56:47.215405 13090 caffe.cpp:313] Batch 56, loss = 0.953781
I0817 10:56:47.243366 13090 caffe.cpp:313] Batch 57, accuracy/top1 = 0.94
I0817 10:56:47.243387 13090 caffe.cpp:313] Batch 57, accuracy/top5 = 1
I0817 10:56:47.243391 13090 caffe.cpp:313] Batch 57, loss = 0.301149
I0817 10:56:47.271325 13090 caffe.cpp:313] Batch 58, accuracy/top1 = 0.92
I0817 10:56:47.271347 13090 caffe.cpp:313] Batch 58, accuracy/top5 = 1
I0817 10:56:47.271349 13090 caffe.cpp:313] Batch 58, loss = 0.353996
I0817 10:56:47.299202 13090 caffe.cpp:313] Batch 59, accuracy/top1 = 0.92
I0817 10:56:47.299226 13090 caffe.cpp:313] Batch 59, accuracy/top5 = 1
I0817 10:56:47.299227 13090 caffe.cpp:313] Batch 59, loss = 0.272568
I0817 10:56:47.327255 13090 caffe.cpp:313] Batch 60, accuracy/top1 = 0.86
I0817 10:56:47.327276 13090 caffe.cpp:313] Batch 60, accuracy/top5 = 1
I0817 10:56:47.327280 13090 caffe.cpp:313] Batch 60, loss = 0.902624
I0817 10:56:47.355185 13090 caffe.cpp:313] Batch 61, accuracy/top1 = 0.88
I0817 10:56:47.355206 13090 caffe.cpp:313] Batch 61, accuracy/top5 = 0.98
I0817 10:56:47.355209 13090 caffe.cpp:313] Batch 61, loss = 0.601743
I0817 10:56:47.383126 13090 caffe.cpp:313] Batch 62, accuracy/top1 = 0.98
I0817 10:56:47.383143 13090 caffe.cpp:313] Batch 62, accuracy/top5 = 1
I0817 10:56:47.383147 13090 caffe.cpp:313] Batch 62, loss = 0.111855
I0817 10:56:47.411003 13090 caffe.cpp:313] Batch 63, accuracy/top1 = 0.92
I0817 10:56:47.411025 13090 caffe.cpp:313] Batch 63, accuracy/top5 = 1
I0817 10:56:47.411027 13090 caffe.cpp:313] Batch 63, loss = 0.289785
I0817 10:56:47.439003 13090 caffe.cpp:313] Batch 64, accuracy/top1 = 0.92
I0817 10:56:47.439016 13090 caffe.cpp:313] Batch 64, accuracy/top5 = 0.98
I0817 10:56:47.439019 13090 caffe.cpp:313] Batch 64, loss = 0.527357
I0817 10:56:47.466892 13090 caffe.cpp:313] Batch 65, accuracy/top1 = 0.94
I0817 10:56:47.466913 13090 caffe.cpp:313] Batch 65, accuracy/top5 = 1
I0817 10:56:47.466915 13090 caffe.cpp:313] Batch 65, loss = 0.259236
I0817 10:56:47.494870 13090 caffe.cpp:313] Batch 66, accuracy/top1 = 0.92
I0817 10:56:47.494891 13090 caffe.cpp:313] Batch 66, accuracy/top5 = 1
I0817 10:56:47.494894 13090 caffe.cpp:313] Batch 66, loss = 0.315278
I0817 10:56:47.522812 13090 caffe.cpp:313] Batch 67, accuracy/top1 = 0.94
I0817 10:56:47.522832 13090 caffe.cpp:313] Batch 67, accuracy/top5 = 1
I0817 10:56:47.522853 13090 caffe.cpp:313] Batch 67, loss = 0.327737
I0817 10:56:47.550739 13090 caffe.cpp:313] Batch 68, accuracy/top1 = 0.9
I0817 10:56:47.550758 13090 caffe.cpp:313] Batch 68, accuracy/top5 = 1
I0817 10:56:47.550761 13090 caffe.cpp:313] Batch 68, loss = 0.462679
I0817 10:56:47.578706 13090 caffe.cpp:313] Batch 69, accuracy/top1 = 0.9
I0817 10:56:47.578723 13090 caffe.cpp:313] Batch 69, accuracy/top5 = 1
I0817 10:56:47.578725 13090 caffe.cpp:313] Batch 69, loss = 0.311315
I0817 10:56:47.606722 13090 caffe.cpp:313] Batch 70, accuracy/top1 = 0.96
I0817 10:56:47.606744 13090 caffe.cpp:313] Batch 70, accuracy/top5 = 1
I0817 10:56:47.606746 13090 caffe.cpp:313] Batch 70, loss = 0.335412
I0817 10:56:47.634680 13090 caffe.cpp:313] Batch 71, accuracy/top1 = 0.92
I0817 10:56:47.634701 13090 caffe.cpp:313] Batch 71, accuracy/top5 = 1
I0817 10:56:47.634703 13090 caffe.cpp:313] Batch 71, loss = 0.423739
I0817 10:56:47.662612 13090 caffe.cpp:313] Batch 72, accuracy/top1 = 0.82
I0817 10:56:47.662636 13090 caffe.cpp:313] Batch 72, accuracy/top5 = 0.94
I0817 10:56:47.662637 13090 caffe.cpp:313] Batch 72, loss = 1.24783
I0817 10:56:47.690613 13090 caffe.cpp:313] Batch 73, accuracy/top1 = 0.92
I0817 10:56:47.690631 13090 caffe.cpp:313] Batch 73, accuracy/top5 = 1
I0817 10:56:47.690634 13090 caffe.cpp:313] Batch 73, loss = 0.236768
I0817 10:56:47.718571 13090 caffe.cpp:313] Batch 74, accuracy/top1 = 0.96
I0817 10:56:47.718586 13090 caffe.cpp:313] Batch 74, accuracy/top5 = 1
I0817 10:56:47.718590 13090 caffe.cpp:313] Batch 74, loss = 0.0755851
I0817 10:56:47.746791 13090 caffe.cpp:313] Batch 75, accuracy/top1 = 0.84
I0817 10:56:47.746832 13090 caffe.cpp:313] Batch 75, accuracy/top5 = 1
I0817 10:56:47.746835 13090 caffe.cpp:313] Batch 75, loss = 0.56859
I0817 10:56:47.775002 13090 caffe.cpp:313] Batch 76, accuracy/top1 = 0.94
I0817 10:56:47.775018 13090 caffe.cpp:313] Batch 76, accuracy/top5 = 1
I0817 10:56:47.775022 13090 caffe.cpp:313] Batch 76, loss = 0.370066
I0817 10:56:47.803052 13090 caffe.cpp:313] Batch 77, accuracy/top1 = 0.92
I0817 10:56:47.803072 13090 caffe.cpp:313] Batch 77, accuracy/top5 = 1
I0817 10:56:47.803076 13090 caffe.cpp:313] Batch 77, loss = 0.52417
I0817 10:56:47.830916 13090 caffe.cpp:313] Batch 78, accuracy/top1 = 0.92
I0817 10:56:47.830937 13090 caffe.cpp:313] Batch 78, accuracy/top5 = 1
I0817 10:56:47.830940 13090 caffe.cpp:313] Batch 78, loss = 0.324342
I0817 10:56:47.858944 13090 caffe.cpp:313] Batch 79, accuracy/top1 = 0.92
I0817 10:56:47.858964 13090 caffe.cpp:313] Batch 79, accuracy/top5 = 0.98
I0817 10:56:47.858968 13090 caffe.cpp:313] Batch 79, loss = 0.68135
I0817 10:56:47.886961 13090 caffe.cpp:313] Batch 80, accuracy/top1 = 0.92
I0817 10:56:47.886979 13090 caffe.cpp:313] Batch 80, accuracy/top5 = 1
I0817 10:56:47.886982 13090 caffe.cpp:313] Batch 80, loss = 0.297256
I0817 10:56:47.914939 13090 caffe.cpp:313] Batch 81, accuracy/top1 = 0.8
I0817 10:56:47.914958 13090 caffe.cpp:313] Batch 81, accuracy/top5 = 1
I0817 10:56:47.914961 13090 caffe.cpp:313] Batch 81, loss = 0.617473
I0817 10:56:47.943102 13090 caffe.cpp:313] Batch 82, accuracy/top1 = 0.9
I0817 10:56:47.943122 13090 caffe.cpp:313] Batch 82, accuracy/top5 = 0.98
I0817 10:56:47.943125 13090 caffe.cpp:313] Batch 82, loss = 0.483736
I0817 10:56:47.971243 13090 caffe.cpp:313] Batch 83, accuracy/top1 = 0.96
I0817 10:56:47.971272 13090 caffe.cpp:313] Batch 83, accuracy/top5 = 1
I0817 10:56:47.971276 13090 caffe.cpp:313] Batch 83, loss = 0.227727
I0817 10:56:47.999357 13090 caffe.cpp:313] Batch 84, accuracy/top1 = 0.94
I0817 10:56:47.999379 13090 caffe.cpp:313] Batch 84, accuracy/top5 = 0.98
I0817 10:56:47.999382 13090 caffe.cpp:313] Batch 84, loss = 0.255229
I0817 10:56:48.027302 13090 caffe.cpp:313] Batch 85, accuracy/top1 = 0.92
I0817 10:56:48.027321 13090 caffe.cpp:313] Batch 85, accuracy/top5 = 1
I0817 10:56:48.027324 13090 caffe.cpp:313] Batch 85, loss = 0.208643
I0817 10:56:48.055212 13090 caffe.cpp:313] Batch 86, accuracy/top1 = 0.94
I0817 10:56:48.055224 13090 caffe.cpp:313] Batch 86, accuracy/top5 = 1
I0817 10:56:48.055227 13090 caffe.cpp:313] Batch 86, loss = 0.209156
I0817 10:56:48.083104 13090 caffe.cpp:313] Batch 87, accuracy/top1 = 0.94
I0817 10:56:48.083124 13090 caffe.cpp:313] Batch 87, accuracy/top5 = 1
I0817 10:56:48.083127 13090 caffe.cpp:313] Batch 87, loss = 0.112809
I0817 10:56:48.111078 13090 caffe.cpp:313] Batch 88, accuracy/top1 = 0.94
I0817 10:56:48.111098 13090 caffe.cpp:313] Batch 88, accuracy/top5 = 1
I0817 10:56:48.111100 13090 caffe.cpp:313] Batch 88, loss = 0.439367
I0817 10:56:48.139091 13090 caffe.cpp:313] Batch 89, accuracy/top1 = 0.9
I0817 10:56:48.139111 13090 caffe.cpp:313] Batch 89, accuracy/top5 = 1
I0817 10:56:48.139113 13090 caffe.cpp:313] Batch 89, loss = 0.226918
I0817 10:56:48.167071 13090 caffe.cpp:313] Batch 90, accuracy/top1 = 0.86
I0817 10:56:48.167093 13090 caffe.cpp:313] Batch 90, accuracy/top5 = 1
I0817 10:56:48.167095 13090 caffe.cpp:313] Batch 90, loss = 0.851893
I0817 10:56:48.195026 13090 caffe.cpp:313] Batch 91, accuracy/top1 = 0.86
I0817 10:56:48.195047 13090 caffe.cpp:313] Batch 91, accuracy/top5 = 1
I0817 10:56:48.195050 13090 caffe.cpp:313] Batch 91, loss = 0.427049
I0817 10:56:48.222980 13090 caffe.cpp:313] Batch 92, accuracy/top1 = 0.88
I0817 10:56:48.223001 13090 caffe.cpp:313] Batch 92, accuracy/top5 = 1
I0817 10:56:48.223004 13090 caffe.cpp:313] Batch 92, loss = 0.672547
I0817 10:56:48.250943 13090 caffe.cpp:313] Batch 93, accuracy/top1 = 0.96
I0817 10:56:48.250965 13090 caffe.cpp:313] Batch 93, accuracy/top5 = 1
I0817 10:56:48.250968 13090 caffe.cpp:313] Batch 93, loss = 0.0968448
I0817 10:56:48.278880 13090 caffe.cpp:313] Batch 94, accuracy/top1 = 0.9
I0817 10:56:48.278909 13090 caffe.cpp:313] Batch 94, accuracy/top5 = 1
I0817 10:56:48.278913 13090 caffe.cpp:313] Batch 94, loss = 0.295178
I0817 10:56:48.306938 13090 caffe.cpp:313] Batch 95, accuracy/top1 = 0.86
I0817 10:56:48.306957 13090 caffe.cpp:313] Batch 95, accuracy/top5 = 0.96
I0817 10:56:48.306959 13090 caffe.cpp:313] Batch 95, loss = 1.17511
I0817 10:56:48.336005 13090 caffe.cpp:313] Batch 96, accuracy/top1 = 0.96
I0817 10:56:48.336068 13090 caffe.cpp:313] Batch 96, accuracy/top5 = 1
I0817 10:56:48.336081 13090 caffe.cpp:313] Batch 96, loss = 0.124347
I0817 10:56:48.366399 13090 caffe.cpp:313] Batch 97, accuracy/top1 = 0.9
I0817 10:56:48.366422 13090 caffe.cpp:313] Batch 97, accuracy/top5 = 1
I0817 10:56:48.366425 13090 caffe.cpp:313] Batch 97, loss = 0.1822
I0817 10:56:48.394536 13090 caffe.cpp:313] Batch 98, accuracy/top1 = 0.9
I0817 10:56:48.394563 13090 caffe.cpp:313] Batch 98, accuracy/top5 = 1
I0817 10:56:48.394567 13090 caffe.cpp:313] Batch 98, loss = 0.446353
I0817 10:56:48.423959 13090 caffe.cpp:313] Batch 99, accuracy/top1 = 0.86
I0817 10:56:48.423979 13090 caffe.cpp:313] Batch 99, accuracy/top5 = 0.98
I0817 10:56:48.423984 13090 caffe.cpp:313] Batch 99, loss = 0.643017
I0817 10:56:48.451954 13090 caffe.cpp:313] Batch 100, accuracy/top1 = 0.94
I0817 10:56:48.451977 13090 caffe.cpp:313] Batch 100, accuracy/top5 = 1
I0817 10:56:48.451980 13090 caffe.cpp:313] Batch 100, loss = 0.214564
I0817 10:56:48.479842 13090 caffe.cpp:313] Batch 101, accuracy/top1 = 0.92
I0817 10:56:48.479854 13090 caffe.cpp:313] Batch 101, accuracy/top5 = 1
I0817 10:56:48.479858 13090 caffe.cpp:313] Batch 101, loss = 0.360145
I0817 10:56:48.507773 13090 caffe.cpp:313] Batch 102, accuracy/top1 = 0.94
I0817 10:56:48.507793 13090 caffe.cpp:313] Batch 102, accuracy/top5 = 1
I0817 10:56:48.507797 13090 caffe.cpp:313] Batch 102, loss = 0.207367
I0817 10:56:48.535782 13090 caffe.cpp:313] Batch 103, accuracy/top1 = 0.84
I0817 10:56:48.535804 13090 caffe.cpp:313] Batch 103, accuracy/top5 = 1
I0817 10:56:48.535807 13090 caffe.cpp:313] Batch 103, loss = 0.523954
I0817 10:56:48.563827 13090 caffe.cpp:313] Batch 104, accuracy/top1 = 0.84
I0817 10:56:48.563845 13090 caffe.cpp:313] Batch 104, accuracy/top5 = 0.98
I0817 10:56:48.563849 13090 caffe.cpp:313] Batch 104, loss = 0.583018
I0817 10:56:48.591778 13090 caffe.cpp:313] Batch 105, accuracy/top1 = 0.96
I0817 10:56:48.591799 13090 caffe.cpp:313] Batch 105, accuracy/top5 = 1
I0817 10:56:48.591804 13090 caffe.cpp:313] Batch 105, loss = 0.106811
I0817 10:56:48.619797 13090 caffe.cpp:313] Batch 106, accuracy/top1 = 0.96
I0817 10:56:48.619818 13090 caffe.cpp:313] Batch 106, accuracy/top5 = 0.98
I0817 10:56:48.619822 13090 caffe.cpp:313] Batch 106, loss = 0.155111
I0817 10:56:48.647714 13090 caffe.cpp:313] Batch 107, accuracy/top1 = 0.86
I0817 10:56:48.647734 13090 caffe.cpp:313] Batch 107, accuracy/top5 = 1
I0817 10:56:48.647738 13090 caffe.cpp:313] Batch 107, loss = 0.677264
I0817 10:56:48.675691 13090 caffe.cpp:313] Batch 108, accuracy/top1 = 0.88
I0817 10:56:48.675712 13090 caffe.cpp:313] Batch 108, accuracy/top5 = 1
I0817 10:56:48.675716 13090 caffe.cpp:313] Batch 108, loss = 0.260989
I0817 10:56:48.703611 13090 caffe.cpp:313] Batch 109, accuracy/top1 = 0.9
I0817 10:56:48.703634 13090 caffe.cpp:313] Batch 109, accuracy/top5 = 1
I0817 10:56:48.703636 13090 caffe.cpp:313] Batch 109, loss = 0.28432
I0817 10:56:48.731533 13090 caffe.cpp:313] Batch 110, accuracy/top1 = 0.86
I0817 10:56:48.731552 13090 caffe.cpp:313] Batch 110, accuracy/top5 = 1
I0817 10:56:48.731556 13090 caffe.cpp:313] Batch 110, loss = 0.659759
I0817 10:56:48.759889 13090 caffe.cpp:313] Batch 111, accuracy/top1 = 0.9
I0817 10:56:48.759909 13090 caffe.cpp:313] Batch 111, accuracy/top5 = 1
I0817 10:56:48.759913 13090 caffe.cpp:313] Batch 111, loss = 0.376121
I0817 10:56:48.787961 13090 caffe.cpp:313] Batch 112, accuracy/top1 = 0.86
I0817 10:56:48.787978 13090 caffe.cpp:313] Batch 112, accuracy/top5 = 1
I0817 10:56:48.787982 13090 caffe.cpp:313] Batch 112, loss = 0.594581
I0817 10:56:48.815804 13090 caffe.cpp:313] Batch 113, accuracy/top1 = 0.94
I0817 10:56:48.815819 13090 caffe.cpp:313] Batch 113, accuracy/top5 = 1
I0817 10:56:48.815824 13090 caffe.cpp:313] Batch 113, loss = 0.125192
I0817 10:56:48.843750 13090 caffe.cpp:313] Batch 114, accuracy/top1 = 0.94
I0817 10:56:48.843770 13090 caffe.cpp:313] Batch 114, accuracy/top5 = 1
I0817 10:56:48.843775 13090 caffe.cpp:313] Batch 114, loss = 0.191867
I0817 10:56:48.871749 13090 caffe.cpp:313] Batch 115, accuracy/top1 = 0.96
I0817 10:56:48.871769 13090 caffe.cpp:313] Batch 115, accuracy/top5 = 1
I0817 10:56:48.871773 13090 caffe.cpp:313] Batch 115, loss = 0.113377
I0817 10:56:48.899615 13090 caffe.cpp:313] Batch 116, accuracy/top1 = 0.84
I0817 10:56:48.899634 13090 caffe.cpp:313] Batch 116, accuracy/top5 = 1
I0817 10:56:48.899638 13090 caffe.cpp:313] Batch 116, loss = 0.761634
I0817 10:56:48.927556 13090 caffe.cpp:313] Batch 117, accuracy/top1 = 0.86
I0817 10:56:48.927577 13090 caffe.cpp:313] Batch 117, accuracy/top5 = 1
I0817 10:56:48.927580 13090 caffe.cpp:313] Batch 117, loss = 1.06775
I0817 10:56:48.955615 13090 caffe.cpp:313] Batch 118, accuracy/top1 = 0.82
I0817 10:56:48.955636 13090 caffe.cpp:313] Batch 118, accuracy/top5 = 1
I0817 10:56:48.955641 13090 caffe.cpp:313] Batch 118, loss = 0.606036
I0817 10:56:48.983613 13090 caffe.cpp:313] Batch 119, accuracy/top1 = 0.94
I0817 10:56:48.983633 13090 caffe.cpp:313] Batch 119, accuracy/top5 = 1
I0817 10:56:48.983636 13090 caffe.cpp:313] Batch 119, loss = 0.113589
I0817 10:56:49.011571 13090 caffe.cpp:313] Batch 120, accuracy/top1 = 0.9
I0817 10:56:49.011592 13090 caffe.cpp:313] Batch 120, accuracy/top5 = 1
I0817 10:56:49.011595 13090 caffe.cpp:313] Batch 120, loss = 0.444157
I0817 10:56:49.039649 13090 caffe.cpp:313] Batch 121, accuracy/top1 = 0.92
I0817 10:56:49.039670 13090 caffe.cpp:313] Batch 121, accuracy/top5 = 1
I0817 10:56:49.039674 13090 caffe.cpp:313] Batch 121, loss = 0.340397
I0817 10:56:49.067664 13090 caffe.cpp:313] Batch 122, accuracy/top1 = 0.9
I0817 10:56:49.067683 13090 caffe.cpp:313] Batch 122, accuracy/top5 = 1
I0817 10:56:49.067687 13090 caffe.cpp:313] Batch 122, loss = 0.497816
I0817 10:56:49.095613 13090 caffe.cpp:313] Batch 123, accuracy/top1 = 0.88
I0817 10:56:49.095625 13090 caffe.cpp:313] Batch 123, accuracy/top5 = 0.98
I0817 10:56:49.095629 13090 caffe.cpp:313] Batch 123, loss = 0.507548
I0817 10:56:49.123605 13090 caffe.cpp:313] Batch 124, accuracy/top1 = 0.92
I0817 10:56:49.123627 13090 caffe.cpp:313] Batch 124, accuracy/top5 = 1
I0817 10:56:49.123651 13090 caffe.cpp:313] Batch 124, loss = 0.17225
I0817 10:56:49.151485 13090 caffe.cpp:313] Batch 125, accuracy/top1 = 0.96
I0817 10:56:49.151506 13090 caffe.cpp:313] Batch 125, accuracy/top5 = 1
I0817 10:56:49.151510 13090 caffe.cpp:313] Batch 125, loss = 0.415378
I0817 10:56:49.179486 13090 caffe.cpp:313] Batch 126, accuracy/top1 = 0.96
I0817 10:56:49.179507 13090 caffe.cpp:313] Batch 126, accuracy/top5 = 1
I0817 10:56:49.179510 13090 caffe.cpp:313] Batch 126, loss = 0.332565
I0817 10:56:49.207521 13090 caffe.cpp:313] Batch 127, accuracy/top1 = 0.9
I0817 10:56:49.207542 13090 caffe.cpp:313] Batch 127, accuracy/top5 = 1
I0817 10:56:49.207546 13090 caffe.cpp:313] Batch 127, loss = 0.510373
I0817 10:56:49.235504 13090 caffe.cpp:313] Batch 128, accuracy/top1 = 0.86
I0817 10:56:49.235525 13090 caffe.cpp:313] Batch 128, accuracy/top5 = 1
I0817 10:56:49.235528 13090 caffe.cpp:313] Batch 128, loss = 0.342778
I0817 10:56:49.263391 13090 caffe.cpp:313] Batch 129, accuracy/top1 = 0.92
I0817 10:56:49.263412 13090 caffe.cpp:313] Batch 129, accuracy/top5 = 1
I0817 10:56:49.263417 13090 caffe.cpp:313] Batch 129, loss = 0.156237
I0817 10:56:49.291359 13090 caffe.cpp:313] Batch 130, accuracy/top1 = 0.86
I0817 10:56:49.291381 13090 caffe.cpp:313] Batch 130, accuracy/top5 = 1
I0817 10:56:49.291384 13090 caffe.cpp:313] Batch 130, loss = 0.715816
I0817 10:56:49.319448 13090 caffe.cpp:313] Batch 131, accuracy/top1 = 0.86
I0817 10:56:49.319469 13090 caffe.cpp:313] Batch 131, accuracy/top5 = 1
I0817 10:56:49.319473 13090 caffe.cpp:313] Batch 131, loss = 0.322335
I0817 10:56:49.347393 13090 caffe.cpp:313] Batch 132, accuracy/top1 = 0.96
I0817 10:56:49.347414 13090 caffe.cpp:313] Batch 132, accuracy/top5 = 1
I0817 10:56:49.347419 13090 caffe.cpp:313] Batch 132, loss = 0.298582
I0817 10:56:49.375409 13090 caffe.cpp:313] Batch 133, accuracy/top1 = 0.96
I0817 10:56:49.375428 13090 caffe.cpp:313] Batch 133, accuracy/top5 = 1
I0817 10:56:49.375432 13090 caffe.cpp:313] Batch 133, loss = 0.171794
I0817 10:56:49.403429 13090 caffe.cpp:313] Batch 134, accuracy/top1 = 0.9
I0817 10:56:49.403440 13090 caffe.cpp:313] Batch 134, accuracy/top5 = 1
I0817 10:56:49.403445 13090 caffe.cpp:313] Batch 134, loss = 0.397686
I0817 10:56:49.431478 13090 caffe.cpp:313] Batch 135, accuracy/top1 = 0.9
I0817 10:56:49.431499 13090 caffe.cpp:313] Batch 135, accuracy/top5 = 0.98
I0817 10:56:49.431502 13090 caffe.cpp:313] Batch 135, loss = 0.625398
I0817 10:56:49.459455 13090 caffe.cpp:313] Batch 136, accuracy/top1 = 0.98
I0817 10:56:49.459476 13090 caffe.cpp:313] Batch 136, accuracy/top5 = 1
I0817 10:56:49.459481 13090 caffe.cpp:313] Batch 136, loss = 0.0651484
I0817 10:56:49.487406 13090 caffe.cpp:313] Batch 137, accuracy/top1 = 0.88
I0817 10:56:49.487426 13090 caffe.cpp:313] Batch 137, accuracy/top5 = 0.98
I0817 10:56:49.487429 13090 caffe.cpp:313] Batch 137, loss = 0.578115
I0817 10:56:49.515359 13090 caffe.cpp:313] Batch 138, accuracy/top1 = 0.9
I0817 10:56:49.515381 13090 caffe.cpp:313] Batch 138, accuracy/top5 = 0.98
I0817 10:56:49.515384 13090 caffe.cpp:313] Batch 138, loss = 0.291582
I0817 10:56:49.543350 13090 caffe.cpp:313] Batch 139, accuracy/top1 = 0.88
I0817 10:56:49.543372 13090 caffe.cpp:313] Batch 139, accuracy/top5 = 0.98
I0817 10:56:49.543375 13090 caffe.cpp:313] Batch 139, loss = 0.611778
I0817 10:56:49.571316 13090 caffe.cpp:313] Batch 140, accuracy/top1 = 0.92
I0817 10:56:49.571336 13090 caffe.cpp:313] Batch 140, accuracy/top5 = 1
I0817 10:56:49.571341 13090 caffe.cpp:313] Batch 140, loss = 0.463441
I0817 10:56:49.599359 13090 caffe.cpp:313] Batch 141, accuracy/top1 = 0.94
I0817 10:56:49.599381 13090 caffe.cpp:313] Batch 141, accuracy/top5 = 1
I0817 10:56:49.599385 13090 caffe.cpp:313] Batch 141, loss = 0.471566
I0817 10:56:49.627274 13090 caffe.cpp:313] Batch 142, accuracy/top1 = 0.96
I0817 10:56:49.627301 13090 caffe.cpp:313] Batch 142, accuracy/top5 = 1
I0817 10:56:49.627305 13090 caffe.cpp:313] Batch 142, loss = 0.113668
I0817 10:56:49.655320 13090 caffe.cpp:313] Batch 143, accuracy/top1 = 0.92
I0817 10:56:49.655340 13090 caffe.cpp:313] Batch 143, accuracy/top5 = 1
I0817 10:56:49.655362 13090 caffe.cpp:313] Batch 143, loss = 0.376748
I0817 10:56:49.683254 13090 caffe.cpp:313] Batch 144, accuracy/top1 = 0.9
I0817 10:56:49.683267 13090 caffe.cpp:313] Batch 144, accuracy/top5 = 0.98
I0817 10:56:49.683271 13090 caffe.cpp:313] Batch 144, loss = 0.474375
I0817 10:56:49.711206 13090 caffe.cpp:313] Batch 145, accuracy/top1 = 0.94
I0817 10:56:49.711223 13090 caffe.cpp:313] Batch 145, accuracy/top5 = 1
I0817 10:56:49.711227 13090 caffe.cpp:313] Batch 145, loss = 0.291983
I0817 10:56:49.739215 13090 caffe.cpp:313] Batch 146, accuracy/top1 = 0.94
I0817 10:56:49.739236 13090 caffe.cpp:313] Batch 146, accuracy/top5 = 1
I0817 10:56:49.739240 13090 caffe.cpp:313] Batch 146, loss = 0.356782
I0817 10:56:49.767498 13090 caffe.cpp:313] Batch 147, accuracy/top1 = 0.86
I0817 10:56:49.767515 13090 caffe.cpp:313] Batch 147, accuracy/top5 = 1
I0817 10:56:49.767519 13090 caffe.cpp:313] Batch 147, loss = 0.363241
I0817 10:56:49.795508 13090 caffe.cpp:313] Batch 148, accuracy/top1 = 0.9
I0817 10:56:49.795531 13090 caffe.cpp:313] Batch 148, accuracy/top5 = 1
I0817 10:56:49.795534 13090 caffe.cpp:313] Batch 148, loss = 0.390203
I0817 10:56:49.823542 13090 caffe.cpp:313] Batch 149, accuracy/top1 = 0.9
I0817 10:56:49.823565 13090 caffe.cpp:313] Batch 149, accuracy/top5 = 1
I0817 10:56:49.823568 13090 caffe.cpp:313] Batch 149, loss = 0.618897
I0817 10:56:49.851642 13090 caffe.cpp:313] Batch 150, accuracy/top1 = 0.88
I0817 10:56:49.851663 13090 caffe.cpp:313] Batch 150, accuracy/top5 = 0.98
I0817 10:56:49.851667 13090 caffe.cpp:313] Batch 150, loss = 0.548176
I0817 10:56:49.879678 13090 caffe.cpp:313] Batch 151, accuracy/top1 = 0.92
I0817 10:56:49.879698 13090 caffe.cpp:313] Batch 151, accuracy/top5 = 0.98
I0817 10:56:49.879703 13090 caffe.cpp:313] Batch 151, loss = 0.513219
I0817 10:56:49.907701 13090 caffe.cpp:313] Batch 152, accuracy/top1 = 0.74
I0817 10:56:49.907722 13090 caffe.cpp:313] Batch 152, accuracy/top5 = 1
I0817 10:56:49.907727 13090 caffe.cpp:313] Batch 152, loss = 0.632307
I0817 10:56:49.935806 13090 caffe.cpp:313] Batch 153, accuracy/top1 = 0.9
I0817 10:56:49.935829 13090 caffe.cpp:313] Batch 153, accuracy/top5 = 0.98
I0817 10:56:49.935833 13090 caffe.cpp:313] Batch 153, loss = 0.988571
I0817 10:56:49.963932 13090 caffe.cpp:313] Batch 154, accuracy/top1 = 0.94
I0817 10:56:49.963950 13090 caffe.cpp:313] Batch 154, accuracy/top5 = 1
I0817 10:56:49.963955 13090 caffe.cpp:313] Batch 154, loss = 0.374184
I0817 10:56:49.992188 13090 caffe.cpp:313] Batch 155, accuracy/top1 = 0.88
I0817 10:56:49.992209 13090 caffe.cpp:313] Batch 155, accuracy/top5 = 1
I0817 10:56:49.992213 13090 caffe.cpp:313] Batch 155, loss = 0.530075
I0817 10:56:50.020259 13090 caffe.cpp:313] Batch 156, accuracy/top1 = 0.94
I0817 10:56:50.020277 13090 caffe.cpp:313] Batch 156, accuracy/top5 = 1
I0817 10:56:50.020282 13090 caffe.cpp:313] Batch 156, loss = 0.394105
I0817 10:56:50.048427 13090 caffe.cpp:313] Batch 157, accuracy/top1 = 0.92
I0817 10:56:50.048447 13090 caffe.cpp:313] Batch 157, accuracy/top5 = 0.98
I0817 10:56:50.048451 13090 caffe.cpp:313] Batch 157, loss = 0.433785
I0817 10:56:50.076634 13090 caffe.cpp:313] Batch 158, accuracy/top1 = 0.9
I0817 10:56:50.076653 13090 caffe.cpp:313] Batch 158, accuracy/top5 = 1
I0817 10:56:50.076658 13090 caffe.cpp:313] Batch 158, loss = 0.228094
I0817 10:56:50.104830 13090 caffe.cpp:313] Batch 159, accuracy/top1 = 0.96
I0817 10:56:50.104851 13090 caffe.cpp:313] Batch 159, accuracy/top5 = 1
I0817 10:56:50.104854 13090 caffe.cpp:313] Batch 159, loss = 0.302686
I0817 10:56:50.133009 13090 caffe.cpp:313] Batch 160, accuracy/top1 = 0.9
I0817 10:56:50.133030 13090 caffe.cpp:313] Batch 160, accuracy/top5 = 1
I0817 10:56:50.133034 13090 caffe.cpp:313] Batch 160, loss = 0.353987
I0817 10:56:50.161020 13090 caffe.cpp:313] Batch 161, accuracy/top1 = 0.9
I0817 10:56:50.161041 13090 caffe.cpp:313] Batch 161, accuracy/top5 = 1
I0817 10:56:50.161044 13090 caffe.cpp:313] Batch 161, loss = 0.31375
I0817 10:56:50.189250 13090 caffe.cpp:313] Batch 162, accuracy/top1 = 0.92
I0817 10:56:50.189285 13090 caffe.cpp:313] Batch 162, accuracy/top5 = 1
I0817 10:56:50.189291 13090 caffe.cpp:313] Batch 162, loss = 0.325773
I0817 10:56:50.217336 13090 caffe.cpp:313] Batch 163, accuracy/top1 = 0.9
I0817 10:56:50.217356 13090 caffe.cpp:313] Batch 163, accuracy/top5 = 1
I0817 10:56:50.217361 13090 caffe.cpp:313] Batch 163, loss = 0.353562
I0817 10:56:50.245388 13090 caffe.cpp:313] Batch 164, accuracy/top1 = 0.9
I0817 10:56:50.245410 13090 caffe.cpp:313] Batch 164, accuracy/top5 = 1
I0817 10:56:50.245414 13090 caffe.cpp:313] Batch 164, loss = 0.426529
I0817 10:56:50.273499 13090 caffe.cpp:313] Batch 165, accuracy/top1 = 0.94
I0817 10:56:50.273520 13090 caffe.cpp:313] Batch 165, accuracy/top5 = 1
I0817 10:56:50.273524 13090 caffe.cpp:313] Batch 165, loss = 0.230849
I0817 10:56:50.301566 13090 caffe.cpp:313] Batch 166, accuracy/top1 = 0.92
I0817 10:56:50.301587 13090 caffe.cpp:313] Batch 166, accuracy/top5 = 1
I0817 10:56:50.301591 13090 caffe.cpp:313] Batch 166, loss = 0.37399
I0817 10:56:50.329592 13090 caffe.cpp:313] Batch 167, accuracy/top1 = 0.92
I0817 10:56:50.329619 13090 caffe.cpp:313] Batch 167, accuracy/top5 = 0.98
I0817 10:56:50.329623 13090 caffe.cpp:313] Batch 167, loss = 0.30702
I0817 10:56:50.357775 13090 caffe.cpp:313] Batch 168, accuracy/top1 = 0.92
I0817 10:56:50.357795 13090 caffe.cpp:313] Batch 168, accuracy/top5 = 0.98
I0817 10:56:50.357798 13090 caffe.cpp:313] Batch 168, loss = 0.477426
I0817 10:56:50.385905 13090 caffe.cpp:313] Batch 169, accuracy/top1 = 0.88
I0817 10:56:50.385922 13090 caffe.cpp:313] Batch 169, accuracy/top5 = 1
I0817 10:56:50.385926 13090 caffe.cpp:313] Batch 169, loss = 0.560137
I0817 10:56:50.413898 13090 caffe.cpp:313] Batch 170, accuracy/top1 = 0.88
I0817 10:56:50.413915 13090 caffe.cpp:313] Batch 170, accuracy/top5 = 0.98
I0817 10:56:50.413919 13090 caffe.cpp:313] Batch 170, loss = 0.446241
I0817 10:56:50.442003 13090 caffe.cpp:313] Batch 171, accuracy/top1 = 0.9
I0817 10:56:50.442024 13090 caffe.cpp:313] Batch 171, accuracy/top5 = 0.96
I0817 10:56:50.442028 13090 caffe.cpp:313] Batch 171, loss = 0.61658
I0817 10:56:50.470069 13090 caffe.cpp:313] Batch 172, accuracy/top1 = 0.94
I0817 10:56:50.470089 13090 caffe.cpp:313] Batch 172, accuracy/top5 = 1
I0817 10:56:50.470093 13090 caffe.cpp:313] Batch 172, loss = 0.145413
I0817 10:56:50.498057 13090 caffe.cpp:313] Batch 173, accuracy/top1 = 0.98
I0817 10:56:50.498077 13090 caffe.cpp:313] Batch 173, accuracy/top5 = 1
I0817 10:56:50.498081 13090 caffe.cpp:313] Batch 173, loss = 0.142151
I0817 10:56:50.526067 13090 caffe.cpp:313] Batch 174, accuracy/top1 = 0.86
I0817 10:56:50.526087 13090 caffe.cpp:313] Batch 174, accuracy/top5 = 1
I0817 10:56:50.526091 13090 caffe.cpp:313] Batch 174, loss = 0.748648
I0817 10:56:50.554103 13090 caffe.cpp:313] Batch 175, accuracy/top1 = 0.9
I0817 10:56:50.554124 13090 caffe.cpp:313] Batch 175, accuracy/top5 = 1
I0817 10:56:50.554128 13090 caffe.cpp:313] Batch 175, loss = 0.437237
I0817 10:56:50.582057 13090 caffe.cpp:313] Batch 176, accuracy/top1 = 0.82
I0817 10:56:50.582079 13090 caffe.cpp:313] Batch 176, accuracy/top5 = 1
I0817 10:56:50.582083 13090 caffe.cpp:313] Batch 176, loss = 0.668445
I0817 10:56:50.610107 13090 caffe.cpp:313] Batch 177, accuracy/top1 = 0.94
I0817 10:56:50.610127 13090 caffe.cpp:313] Batch 177, accuracy/top5 = 1
I0817 10:56:50.610131 13090 caffe.cpp:313] Batch 177, loss = 0.173378
I0817 10:56:50.638101 13090 caffe.cpp:313] Batch 178, accuracy/top1 = 0.88
I0817 10:56:50.638123 13090 caffe.cpp:313] Batch 178, accuracy/top5 = 0.98
I0817 10:56:50.638126 13090 caffe.cpp:313] Batch 178, loss = 0.453007
I0817 10:56:50.666117 13090 caffe.cpp:313] Batch 179, accuracy/top1 = 0.88
I0817 10:56:50.666137 13090 caffe.cpp:313] Batch 179, accuracy/top5 = 1
I0817 10:56:50.666141 13090 caffe.cpp:313] Batch 179, loss = 0.548267
I0817 10:56:50.694144 13090 caffe.cpp:313] Batch 180, accuracy/top1 = 0.94
I0817 10:56:50.694161 13090 caffe.cpp:313] Batch 180, accuracy/top5 = 1
I0817 10:56:50.694164 13090 caffe.cpp:313] Batch 180, loss = 0.239104
I0817 10:56:50.721979 13090 caffe.cpp:313] Batch 181, accuracy/top1 = 0.92
I0817 10:56:50.722008 13090 caffe.cpp:313] Batch 181, accuracy/top5 = 1
I0817 10:56:50.722012 13090 caffe.cpp:313] Batch 181, loss = 0.283075
I0817 10:56:50.750167 13090 caffe.cpp:313] Batch 182, accuracy/top1 = 0.92
I0817 10:56:50.750186 13090 caffe.cpp:313] Batch 182, accuracy/top5 = 1
I0817 10:56:50.750190 13090 caffe.cpp:313] Batch 182, loss = 0.307971
I0817 10:56:50.778177 13090 caffe.cpp:313] Batch 183, accuracy/top1 = 0.94
I0817 10:56:50.778197 13090 caffe.cpp:313] Batch 183, accuracy/top5 = 1
I0817 10:56:50.778200 13090 caffe.cpp:313] Batch 183, loss = 0.29129
I0817 10:56:50.806195 13090 caffe.cpp:313] Batch 184, accuracy/top1 = 0.86
I0817 10:56:50.806216 13090 caffe.cpp:313] Batch 184, accuracy/top5 = 1
I0817 10:56:50.806219 13090 caffe.cpp:313] Batch 184, loss = 0.613376
I0817 10:56:50.834154 13090 caffe.cpp:313] Batch 185, accuracy/top1 = 0.92
I0817 10:56:50.834175 13090 caffe.cpp:313] Batch 185, accuracy/top5 = 1
I0817 10:56:50.834179 13090 caffe.cpp:313] Batch 185, loss = 0.786444
I0817 10:56:50.862097 13090 caffe.cpp:313] Batch 186, accuracy/top1 = 0.94
I0817 10:56:50.862118 13090 caffe.cpp:313] Batch 186, accuracy/top5 = 1
I0817 10:56:50.862123 13090 caffe.cpp:313] Batch 186, loss = 0.260609
I0817 10:56:50.890159 13090 caffe.cpp:313] Batch 187, accuracy/top1 = 0.9
I0817 10:56:50.890180 13090 caffe.cpp:313] Batch 187, accuracy/top5 = 1
I0817 10:56:50.890184 13090 caffe.cpp:313] Batch 187, loss = 0.67075
I0817 10:56:50.918161 13090 caffe.cpp:313] Batch 188, accuracy/top1 = 0.94
I0817 10:56:50.918182 13090 caffe.cpp:313] Batch 188, accuracy/top5 = 1
I0817 10:56:50.918186 13090 caffe.cpp:313] Batch 188, loss = 0.251532
I0817 10:56:50.946209 13090 caffe.cpp:313] Batch 189, accuracy/top1 = 0.92
I0817 10:56:50.946238 13090 caffe.cpp:313] Batch 189, accuracy/top5 = 1
I0817 10:56:50.946241 13090 caffe.cpp:313] Batch 189, loss = 0.229256
I0817 10:56:50.974320 13090 caffe.cpp:313] Batch 190, accuracy/top1 = 0.88
I0817 10:56:50.974339 13090 caffe.cpp:313] Batch 190, accuracy/top5 = 1
I0817 10:56:50.974344 13090 caffe.cpp:313] Batch 190, loss = 0.650465
I0817 10:56:51.002349 13090 caffe.cpp:313] Batch 191, accuracy/top1 = 0.9
I0817 10:56:51.002367 13090 caffe.cpp:313] Batch 191, accuracy/top5 = 1
I0817 10:56:51.002372 13090 caffe.cpp:313] Batch 191, loss = 0.340139
I0817 10:56:51.030367 13090 caffe.cpp:313] Batch 192, accuracy/top1 = 0.9
I0817 10:56:51.030385 13090 caffe.cpp:313] Batch 192, accuracy/top5 = 0.96
I0817 10:56:51.030390 13090 caffe.cpp:313] Batch 192, loss = 0.57218
I0817 10:56:51.058459 13090 caffe.cpp:313] Batch 193, accuracy/top1 = 0.96
I0817 10:56:51.058477 13090 caffe.cpp:313] Batch 193, accuracy/top5 = 1
I0817 10:56:51.058481 13090 caffe.cpp:313] Batch 193, loss = 0.077374
I0817 10:56:51.086484 13090 caffe.cpp:313] Batch 194, accuracy/top1 = 0.86
I0817 10:56:51.086505 13090 caffe.cpp:313] Batch 194, accuracy/top5 = 0.98
I0817 10:56:51.086509 13090 caffe.cpp:313] Batch 194, loss = 0.816084
I0817 10:56:51.114512 13090 caffe.cpp:313] Batch 195, accuracy/top1 = 0.9
I0817 10:56:51.114534 13090 caffe.cpp:313] Batch 195, accuracy/top5 = 1
I0817 10:56:51.114538 13090 caffe.cpp:313] Batch 195, loss = 0.283724
I0817 10:56:51.142526 13090 caffe.cpp:313] Batch 196, accuracy/top1 = 0.86
I0817 10:56:51.142549 13090 caffe.cpp:313] Batch 196, accuracy/top5 = 1
I0817 10:56:51.142552 13090 caffe.cpp:313] Batch 196, loss = 0.582662
I0817 10:56:51.142976 13129 data_reader.cpp:288] Starting prefetch of epoch 1
I0817 10:56:51.170543 13090 caffe.cpp:313] Batch 197, accuracy/top1 = 0.92
I0817 10:56:51.170562 13090 caffe.cpp:313] Batch 197, accuracy/top5 = 1
I0817 10:56:51.170565 13090 caffe.cpp:313] Batch 197, loss = 0.293263
I0817 10:56:51.198424 13090 caffe.cpp:313] Batch 198, accuracy/top1 = 0.98
I0817 10:56:51.198446 13090 caffe.cpp:313] Batch 198, accuracy/top5 = 1
I0817 10:56:51.198448 13090 caffe.cpp:313] Batch 198, loss = 0.162477
I0817 10:56:51.226527 13090 caffe.cpp:313] Batch 199, accuracy/top1 = 0.9
I0817 10:56:51.226548 13090 caffe.cpp:313] Batch 199, accuracy/top5 = 1
I0817 10:56:51.226565 13090 caffe.cpp:313] Batch 199, loss = 0.465003
I0817 10:56:51.226568 13090 caffe.cpp:318] Loss: 0.418941
I0817 10:56:51.226577 13090 caffe.cpp:330] accuracy/top1 = 0.9065
I0817 10:56:51.226580 13090 caffe.cpp:330] accuracy/top5 = 0.9952
I0817 10:56:51.226584 13090 caffe.cpp:330] loss = 0.418941 (* 1 = 0.418941 loss)
