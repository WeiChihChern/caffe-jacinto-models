Logging output to training/eval-log-2017-10-03_06-23-55.txt
I1003 06:23:56.482982 18825 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:23:56.483685 18825 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:23:56.484282 18825 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:23:56.484854 18825 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:23:56.486508 18825 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/initial/deploy.prototxt
I1003 06:23:56.486526 18825 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W1003 06:23:56.486531 18825 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I1003 06:23:56.486802 18825 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I1003 06:23:56.486955 18825 net.cpp:104] Using FLOAT as default forward math type
I1003 06:23:56.486960 18825 net.cpp:110] Using FLOAT as default backward math type
I1003 06:23:56.486963 18825 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I1003 06:23:56.486968 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.486976 18825 net.cpp:184] Created Layer input (0)
I1003 06:23:56.486982 18825 net.cpp:530] input -> data
I1003 06:23:56.487689 18825 net.cpp:245] Setting up input
I1003 06:23:56.487699 18825 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I1003 06:23:56.487707 18825 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1003 06:23:56.487717 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.487727 18825 net.cpp:184] Created Layer data/bias (1)
I1003 06:23:56.487731 18825 net.cpp:561] data/bias <- data
I1003 06:23:56.487735 18825 net.cpp:530] data/bias -> data/bias
I1003 06:23:56.492353 18825 net.cpp:245] Setting up data/bias
I1003 06:23:56.492372 18825 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I1003 06:23:56.492383 18825 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1003 06:23:56.492388 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.492411 18825 net.cpp:184] Created Layer conv1a (2)
I1003 06:23:56.492419 18825 net.cpp:561] conv1a <- data/bias
I1003 06:23:56.492424 18825 net.cpp:530] conv1a -> conv1a
I1003 06:23:56.856953 18825 net.cpp:245] Setting up conv1a
I1003 06:23:56.856976 18825 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I1003 06:23:56.856987 18825 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1003 06:23:56.856992 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.857002 18825 net.cpp:184] Created Layer conv1a/bn (3)
I1003 06:23:56.857005 18825 net.cpp:561] conv1a/bn <- conv1a
I1003 06:23:56.857009 18825 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1003 06:23:56.857875 18825 net.cpp:245] Setting up conv1a/bn
I1003 06:23:56.857885 18825 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I1003 06:23:56.857892 18825 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1003 06:23:56.857895 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.857899 18825 net.cpp:184] Created Layer conv1a/relu (4)
I1003 06:23:56.857902 18825 net.cpp:561] conv1a/relu <- conv1a
I1003 06:23:56.857904 18825 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1003 06:23:56.857913 18825 net.cpp:245] Setting up conv1a/relu
I1003 06:23:56.857916 18825 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I1003 06:23:56.857918 18825 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1003 06:23:56.857920 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.857933 18825 net.cpp:184] Created Layer conv1b (5)
I1003 06:23:56.857934 18825 net.cpp:561] conv1b <- conv1a
I1003 06:23:56.857937 18825 net.cpp:530] conv1b -> conv1b
I1003 06:23:56.859570 18825 net.cpp:245] Setting up conv1b
I1003 06:23:56.859580 18825 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I1003 06:23:56.859586 18825 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1003 06:23:56.859587 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.859592 18825 net.cpp:184] Created Layer conv1b/bn (6)
I1003 06:23:56.859596 18825 net.cpp:561] conv1b/bn <- conv1b
I1003 06:23:56.859597 18825 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1003 06:23:56.860442 18825 net.cpp:245] Setting up conv1b/bn
I1003 06:23:56.860451 18825 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I1003 06:23:56.860457 18825 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1003 06:23:56.860460 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.860465 18825 net.cpp:184] Created Layer conv1b/relu (7)
I1003 06:23:56.860466 18825 net.cpp:561] conv1b/relu <- conv1b
I1003 06:23:56.860468 18825 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1003 06:23:56.860472 18825 net.cpp:245] Setting up conv1b/relu
I1003 06:23:56.860474 18825 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I1003 06:23:56.860477 18825 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1003 06:23:56.860491 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.860498 18825 net.cpp:184] Created Layer pool1 (8)
I1003 06:23:56.860503 18825 net.cpp:561] pool1 <- conv1b
I1003 06:23:56.860507 18825 net.cpp:530] pool1 -> pool1
I1003 06:23:56.860553 18825 net.cpp:245] Setting up pool1
I1003 06:23:56.860564 18825 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I1003 06:23:56.860569 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1003 06:23:56.860572 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.860580 18825 net.cpp:184] Created Layer res2a_branch2a (9)
I1003 06:23:56.860584 18825 net.cpp:561] res2a_branch2a <- pool1
I1003 06:23:56.860587 18825 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1003 06:23:56.861863 18825 net.cpp:245] Setting up res2a_branch2a
I1003 06:23:56.861872 18825 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I1003 06:23:56.861878 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1003 06:23:56.861881 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.861886 18825 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I1003 06:23:56.861888 18825 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1003 06:23:56.861891 18825 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1003 06:23:56.862355 18825 net.cpp:245] Setting up res2a_branch2a/bn
I1003 06:23:56.862363 18825 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I1003 06:23:56.862368 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1003 06:23:56.862371 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.862375 18825 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I1003 06:23:56.862376 18825 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1003 06:23:56.862378 18825 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1003 06:23:56.862382 18825 net.cpp:245] Setting up res2a_branch2a/relu
I1003 06:23:56.862385 18825 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I1003 06:23:56.862386 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1003 06:23:56.862388 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.862397 18825 net.cpp:184] Created Layer res2a_branch2b (12)
I1003 06:23:56.862401 18825 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1003 06:23:56.862404 18825 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1003 06:23:56.863477 18825 net.cpp:245] Setting up res2a_branch2b
I1003 06:23:56.863487 18825 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I1003 06:23:56.863492 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1003 06:23:56.863494 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.863500 18825 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I1003 06:23:56.863502 18825 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1003 06:23:56.863505 18825 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1003 06:23:56.864334 18825 net.cpp:245] Setting up res2a_branch2b/bn
I1003 06:23:56.864344 18825 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I1003 06:23:56.864349 18825 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1003 06:23:56.864352 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.864356 18825 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I1003 06:23:56.864358 18825 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1003 06:23:56.864361 18825 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1003 06:23:56.864373 18825 net.cpp:245] Setting up res2a_branch2b/relu
I1003 06:23:56.864377 18825 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I1003 06:23:56.864378 18825 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1003 06:23:56.864380 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.864384 18825 net.cpp:184] Created Layer pool2 (15)
I1003 06:23:56.864387 18825 net.cpp:561] pool2 <- res2a_branch2b
I1003 06:23:56.864390 18825 net.cpp:530] pool2 -> pool2
I1003 06:23:56.864431 18825 net.cpp:245] Setting up pool2
I1003 06:23:56.864439 18825 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I1003 06:23:56.864444 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1003 06:23:56.864447 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.864455 18825 net.cpp:184] Created Layer res3a_branch2a (16)
I1003 06:23:56.864459 18825 net.cpp:561] res3a_branch2a <- pool2
I1003 06:23:56.864464 18825 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1003 06:23:56.867158 18825 net.cpp:245] Setting up res3a_branch2a
I1003 06:23:56.867167 18825 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I1003 06:23:56.867172 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1003 06:23:56.867175 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.867179 18825 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I1003 06:23:56.867182 18825 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1003 06:23:56.867184 18825 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1003 06:23:56.867642 18825 net.cpp:245] Setting up res3a_branch2a/bn
I1003 06:23:56.867650 18825 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I1003 06:23:56.867657 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1003 06:23:56.867660 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.867662 18825 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I1003 06:23:56.867666 18825 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1003 06:23:56.867667 18825 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1003 06:23:56.867671 18825 net.cpp:245] Setting up res3a_branch2a/relu
I1003 06:23:56.867673 18825 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I1003 06:23:56.867676 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1003 06:23:56.867677 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.867682 18825 net.cpp:184] Created Layer res3a_branch2b (19)
I1003 06:23:56.867684 18825 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1003 06:23:56.867687 18825 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1003 06:23:56.868690 18825 net.cpp:245] Setting up res3a_branch2b
I1003 06:23:56.868700 18825 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I1003 06:23:56.868703 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1003 06:23:56.868705 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.868710 18825 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I1003 06:23:56.868711 18825 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1003 06:23:56.868716 18825 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1003 06:23:56.869210 18825 net.cpp:245] Setting up res3a_branch2b/bn
I1003 06:23:56.869221 18825 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I1003 06:23:56.869230 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1003 06:23:56.869235 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.869249 18825 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I1003 06:23:56.869253 18825 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1003 06:23:56.869262 18825 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1003 06:23:56.869268 18825 net.cpp:245] Setting up res3a_branch2b/relu
I1003 06:23:56.869274 18825 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I1003 06:23:56.869278 18825 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1003 06:23:56.869283 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.869292 18825 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1003 06:23:56.869297 18825 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1003 06:23:56.869300 18825 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:23:56.869304 18825 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:23:56.869331 18825 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1003 06:23:56.869338 18825 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:23:56.869343 18825 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:23:56.869345 18825 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1003 06:23:56.869350 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.869356 18825 net.cpp:184] Created Layer pool3 (23)
I1003 06:23:56.869360 18825 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:23:56.869364 18825 net.cpp:530] pool3 -> pool3
I1003 06:23:56.869395 18825 net.cpp:245] Setting up pool3
I1003 06:23:56.869401 18825 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I1003 06:23:56.869405 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1003 06:23:56.869410 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.869421 18825 net.cpp:184] Created Layer res4a_branch2a (24)
I1003 06:23:56.869424 18825 net.cpp:561] res4a_branch2a <- pool3
I1003 06:23:56.869428 18825 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1003 06:23:56.875396 18825 net.cpp:245] Setting up res4a_branch2a
I1003 06:23:56.875404 18825 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I1003 06:23:56.875411 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1003 06:23:56.875416 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.875422 18825 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I1003 06:23:56.875427 18825 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1003 06:23:56.875430 18825 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1003 06:23:56.875815 18825 net.cpp:245] Setting up res4a_branch2a/bn
I1003 06:23:56.875823 18825 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I1003 06:23:56.875831 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1003 06:23:56.875835 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.875840 18825 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I1003 06:23:56.875845 18825 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1003 06:23:56.875849 18825 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1003 06:23:56.875855 18825 net.cpp:245] Setting up res4a_branch2a/relu
I1003 06:23:56.875861 18825 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I1003 06:23:56.875866 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1003 06:23:56.875876 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.875887 18825 net.cpp:184] Created Layer res4a_branch2b (27)
I1003 06:23:56.875891 18825 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1003 06:23:56.875895 18825 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1003 06:23:56.878969 18825 net.cpp:245] Setting up res4a_branch2b
I1003 06:23:56.878978 18825 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I1003 06:23:56.878994 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1003 06:23:56.878999 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.879006 18825 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I1003 06:23:56.879010 18825 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1003 06:23:56.879014 18825 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1003 06:23:56.879395 18825 net.cpp:245] Setting up res4a_branch2b/bn
I1003 06:23:56.879401 18825 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I1003 06:23:56.879410 18825 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1003 06:23:56.879415 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.879420 18825 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I1003 06:23:56.879423 18825 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1003 06:23:56.879427 18825 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1003 06:23:56.879432 18825 net.cpp:245] Setting up res4a_branch2b/relu
I1003 06:23:56.879438 18825 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I1003 06:23:56.879442 18825 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1003 06:23:56.879447 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.879453 18825 net.cpp:184] Created Layer pool4 (30)
I1003 06:23:56.879457 18825 net.cpp:561] pool4 <- res4a_branch2b
I1003 06:23:56.879462 18825 net.cpp:530] pool4 -> pool4
I1003 06:23:56.879493 18825 net.cpp:245] Setting up pool4
I1003 06:23:56.879499 18825 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I1003 06:23:56.879503 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1003 06:23:56.879508 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.879516 18825 net.cpp:184] Created Layer res5a_branch2a (31)
I1003 06:23:56.879520 18825 net.cpp:561] res5a_branch2a <- pool4
I1003 06:23:56.879524 18825 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1003 06:23:56.904845 18825 net.cpp:245] Setting up res5a_branch2a
I1003 06:23:56.904866 18825 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I1003 06:23:56.904875 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1003 06:23:56.904881 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.904892 18825 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I1003 06:23:56.904897 18825 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1003 06:23:56.904903 18825 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1003 06:23:56.905305 18825 net.cpp:245] Setting up res5a_branch2a/bn
I1003 06:23:56.905313 18825 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I1003 06:23:56.905321 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1003 06:23:56.905326 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.905331 18825 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I1003 06:23:56.905335 18825 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1003 06:23:56.905349 18825 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1003 06:23:56.905355 18825 net.cpp:245] Setting up res5a_branch2a/relu
I1003 06:23:56.905361 18825 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I1003 06:23:56.905364 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1003 06:23:56.905369 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.905382 18825 net.cpp:184] Created Layer res5a_branch2b (34)
I1003 06:23:56.905386 18825 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1003 06:23:56.905390 18825 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1003 06:23:56.917834 18825 net.cpp:245] Setting up res5a_branch2b
I1003 06:23:56.917855 18825 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I1003 06:23:56.917868 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1003 06:23:56.917874 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.917884 18825 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I1003 06:23:56.917889 18825 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1003 06:23:56.917894 18825 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1003 06:23:56.918292 18825 net.cpp:245] Setting up res5a_branch2b/bn
I1003 06:23:56.918300 18825 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I1003 06:23:56.918309 18825 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1003 06:23:56.918314 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.918319 18825 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I1003 06:23:56.918323 18825 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1003 06:23:56.918328 18825 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1003 06:23:56.918334 18825 net.cpp:245] Setting up res5a_branch2b/relu
I1003 06:23:56.918339 18825 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I1003 06:23:56.918344 18825 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1003 06:23:56.918349 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.918359 18825 net.cpp:184] Created Layer out5a (37)
I1003 06:23:56.918364 18825 net.cpp:561] out5a <- res5a_branch2b
I1003 06:23:56.918367 18825 net.cpp:530] out5a -> out5a
I1003 06:23:56.922154 18825 net.cpp:245] Setting up out5a
I1003 06:23:56.922170 18825 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I1003 06:23:56.922178 18825 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1003 06:23:56.922183 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.922193 18825 net.cpp:184] Created Layer out5a/bn (38)
I1003 06:23:56.922197 18825 net.cpp:561] out5a/bn <- out5a
I1003 06:23:56.922201 18825 net.cpp:513] out5a/bn -> out5a (in-place)
I1003 06:23:56.922646 18825 net.cpp:245] Setting up out5a/bn
I1003 06:23:56.922655 18825 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I1003 06:23:56.922664 18825 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1003 06:23:56.922669 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.922673 18825 net.cpp:184] Created Layer out5a/relu (39)
I1003 06:23:56.922677 18825 net.cpp:561] out5a/relu <- out5a
I1003 06:23:56.922683 18825 net.cpp:513] out5a/relu -> out5a (in-place)
I1003 06:23:56.922689 18825 net.cpp:245] Setting up out5a/relu
I1003 06:23:56.922694 18825 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I1003 06:23:56.922698 18825 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1003 06:23:56.922703 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.922725 18825 net.cpp:184] Created Layer out5a_up2 (40)
I1003 06:23:56.922729 18825 net.cpp:561] out5a_up2 <- out5a
I1003 06:23:56.922734 18825 net.cpp:530] out5a_up2 -> out5a_up2
I1003 06:23:56.922870 18825 net.cpp:245] Setting up out5a_up2
I1003 06:23:56.922876 18825 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I1003 06:23:56.922881 18825 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1003 06:23:56.922886 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.922896 18825 net.cpp:184] Created Layer out3a (41)
I1003 06:23:56.922900 18825 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:23:56.922905 18825 net.cpp:530] out3a -> out3a
I1003 06:23:56.923816 18825 net.cpp:245] Setting up out3a
I1003 06:23:56.923825 18825 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I1003 06:23:56.923831 18825 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1003 06:23:56.923835 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.923843 18825 net.cpp:184] Created Layer out3a/bn (42)
I1003 06:23:56.923847 18825 net.cpp:561] out3a/bn <- out3a
I1003 06:23:56.923851 18825 net.cpp:513] out3a/bn -> out3a (in-place)
I1003 06:23:56.924355 18825 net.cpp:245] Setting up out3a/bn
I1003 06:23:56.924365 18825 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I1003 06:23:56.924373 18825 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1003 06:23:56.924378 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.924383 18825 net.cpp:184] Created Layer out3a/relu (43)
I1003 06:23:56.924386 18825 net.cpp:561] out3a/relu <- out3a
I1003 06:23:56.924391 18825 net.cpp:513] out3a/relu -> out3a (in-place)
I1003 06:23:56.924397 18825 net.cpp:245] Setting up out3a/relu
I1003 06:23:56.924403 18825 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I1003 06:23:56.924407 18825 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1003 06:23:56.924412 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.924424 18825 net.cpp:184] Created Layer out3_out5_combined (44)
I1003 06:23:56.924427 18825 net.cpp:561] out3_out5_combined <- out5a_up2
I1003 06:23:56.924432 18825 net.cpp:561] out3_out5_combined <- out3a
I1003 06:23:56.924437 18825 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1003 06:23:56.924458 18825 net.cpp:245] Setting up out3_out5_combined
I1003 06:23:56.924463 18825 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I1003 06:23:56.924466 18825 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1003 06:23:56.924471 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.924486 18825 net.cpp:184] Created Layer ctx_conv1 (45)
I1003 06:23:56.924490 18825 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1003 06:23:56.924495 18825 net.cpp:530] ctx_conv1 -> ctx_conv1
I1003 06:23:56.925402 18825 net.cpp:245] Setting up ctx_conv1
I1003 06:23:56.925410 18825 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I1003 06:23:56.925417 18825 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1003 06:23:56.925421 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.925428 18825 net.cpp:184] Created Layer ctx_conv1/bn (46)
I1003 06:23:56.925432 18825 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1003 06:23:56.925436 18825 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1003 06:23:56.925853 18825 net.cpp:245] Setting up ctx_conv1/bn
I1003 06:23:56.925860 18825 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I1003 06:23:56.925869 18825 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1003 06:23:56.925880 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.925886 18825 net.cpp:184] Created Layer ctx_conv1/relu (47)
I1003 06:23:56.925891 18825 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1003 06:23:56.925895 18825 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1003 06:23:56.925901 18825 net.cpp:245] Setting up ctx_conv1/relu
I1003 06:23:56.925906 18825 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I1003 06:23:56.925910 18825 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1003 06:23:56.925915 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.925930 18825 net.cpp:184] Created Layer ctx_conv2 (48)
I1003 06:23:56.925933 18825 net.cpp:561] ctx_conv2 <- ctx_conv1
I1003 06:23:56.925937 18825 net.cpp:530] ctx_conv2 -> ctx_conv2
I1003 06:23:56.926838 18825 net.cpp:245] Setting up ctx_conv2
I1003 06:23:56.926846 18825 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I1003 06:23:56.926852 18825 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1003 06:23:56.926856 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.926863 18825 net.cpp:184] Created Layer ctx_conv2/bn (49)
I1003 06:23:56.926867 18825 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1003 06:23:56.926870 18825 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1003 06:23:56.927278 18825 net.cpp:245] Setting up ctx_conv2/bn
I1003 06:23:56.927285 18825 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I1003 06:23:56.927294 18825 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1003 06:23:56.927297 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.927302 18825 net.cpp:184] Created Layer ctx_conv2/relu (50)
I1003 06:23:56.927306 18825 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1003 06:23:56.927310 18825 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1003 06:23:56.927316 18825 net.cpp:245] Setting up ctx_conv2/relu
I1003 06:23:56.927321 18825 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I1003 06:23:56.927326 18825 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1003 06:23:56.927330 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.927343 18825 net.cpp:184] Created Layer ctx_conv3 (51)
I1003 06:23:56.927347 18825 net.cpp:561] ctx_conv3 <- ctx_conv2
I1003 06:23:56.927351 18825 net.cpp:530] ctx_conv3 -> ctx_conv3
I1003 06:23:56.928249 18825 net.cpp:245] Setting up ctx_conv3
I1003 06:23:56.928257 18825 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I1003 06:23:56.928263 18825 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1003 06:23:56.928267 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.928274 18825 net.cpp:184] Created Layer ctx_conv3/bn (52)
I1003 06:23:56.928279 18825 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1003 06:23:56.928283 18825 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1003 06:23:56.928691 18825 net.cpp:245] Setting up ctx_conv3/bn
I1003 06:23:56.928699 18825 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I1003 06:23:56.928707 18825 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1003 06:23:56.928711 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.928716 18825 net.cpp:184] Created Layer ctx_conv3/relu (53)
I1003 06:23:56.928720 18825 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1003 06:23:56.928725 18825 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1003 06:23:56.928730 18825 net.cpp:245] Setting up ctx_conv3/relu
I1003 06:23:56.928735 18825 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I1003 06:23:56.928745 18825 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1003 06:23:56.928750 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.928758 18825 net.cpp:184] Created Layer ctx_conv4 (54)
I1003 06:23:56.928761 18825 net.cpp:561] ctx_conv4 <- ctx_conv3
I1003 06:23:56.928766 18825 net.cpp:530] ctx_conv4 -> ctx_conv4
I1003 06:23:56.929666 18825 net.cpp:245] Setting up ctx_conv4
I1003 06:23:56.929673 18825 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I1003 06:23:56.929680 18825 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1003 06:23:56.929685 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.929690 18825 net.cpp:184] Created Layer ctx_conv4/bn (55)
I1003 06:23:56.929695 18825 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1003 06:23:56.929699 18825 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1003 06:23:56.930116 18825 net.cpp:245] Setting up ctx_conv4/bn
I1003 06:23:56.930124 18825 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I1003 06:23:56.930132 18825 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1003 06:23:56.930136 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930141 18825 net.cpp:184] Created Layer ctx_conv4/relu (56)
I1003 06:23:56.930145 18825 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1003 06:23:56.930148 18825 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1003 06:23:56.930155 18825 net.cpp:245] Setting up ctx_conv4/relu
I1003 06:23:56.930160 18825 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I1003 06:23:56.930164 18825 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1003 06:23:56.930168 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930184 18825 net.cpp:184] Created Layer ctx_final (57)
I1003 06:23:56.930188 18825 net.cpp:561] ctx_final <- ctx_conv4
I1003 06:23:56.930192 18825 net.cpp:530] ctx_final -> ctx_final
I1003 06:23:56.930464 18825 net.cpp:245] Setting up ctx_final
I1003 06:23:56.930472 18825 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I1003 06:23:56.930479 18825 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1003 06:23:56.930482 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930487 18825 net.cpp:184] Created Layer ctx_final/relu (58)
I1003 06:23:56.930492 18825 net.cpp:561] ctx_final/relu <- ctx_final
I1003 06:23:56.930495 18825 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1003 06:23:56.930501 18825 net.cpp:245] Setting up ctx_final/relu
I1003 06:23:56.930507 18825 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I1003 06:23:56.930511 18825 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1003 06:23:56.930516 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930522 18825 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I1003 06:23:56.930526 18825 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1003 06:23:56.930531 18825 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1003 06:23:56.930655 18825 net.cpp:245] Setting up out_deconv_final_up2
I1003 06:23:56.930660 18825 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I1003 06:23:56.930666 18825 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1003 06:23:56.930670 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930676 18825 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I1003 06:23:56.930681 18825 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1003 06:23:56.930691 18825 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1003 06:23:56.930809 18825 net.cpp:245] Setting up out_deconv_final_up4
I1003 06:23:56.930815 18825 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I1003 06:23:56.930819 18825 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1003 06:23:56.930824 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930830 18825 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I1003 06:23:56.930835 18825 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1003 06:23:56.930838 18825 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1003 06:23:56.930960 18825 net.cpp:245] Setting up out_deconv_final_up8
I1003 06:23:56.930966 18825 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I1003 06:23:56.930971 18825 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I1003 06:23:56.930975 18825 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:23:56.930987 18825 net.cpp:184] Created Layer argMaxOut (62)
I1003 06:23:56.930990 18825 net.cpp:561] argMaxOut <- out_deconv_final_up8
I1003 06:23:56.930994 18825 net.cpp:530] argMaxOut -> argMaxOut
I1003 06:23:56.931011 18825 net.cpp:245] Setting up argMaxOut
I1003 06:23:56.931017 18825 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I1003 06:23:56.931021 18825 net.cpp:325] argMaxOut does not need backward computation.
I1003 06:23:56.931025 18825 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I1003 06:23:56.931030 18825 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I1003 06:23:56.931032 18825 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I1003 06:23:56.931036 18825 net.cpp:325] ctx_final/relu does not need backward computation.
I1003 06:23:56.931041 18825 net.cpp:325] ctx_final does not need backward computation.
I1003 06:23:56.931051 18825 net.cpp:325] ctx_conv4/relu does not need backward computation.
I1003 06:23:56.931053 18825 net.cpp:325] ctx_conv4/bn does not need backward computation.
I1003 06:23:56.931056 18825 net.cpp:325] ctx_conv4 does not need backward computation.
I1003 06:23:56.931059 18825 net.cpp:325] ctx_conv3/relu does not need backward computation.
I1003 06:23:56.931063 18825 net.cpp:325] ctx_conv3/bn does not need backward computation.
I1003 06:23:56.931066 18825 net.cpp:325] ctx_conv3 does not need backward computation.
I1003 06:23:56.931069 18825 net.cpp:325] ctx_conv2/relu does not need backward computation.
I1003 06:23:56.931072 18825 net.cpp:325] ctx_conv2/bn does not need backward computation.
I1003 06:23:56.931076 18825 net.cpp:325] ctx_conv2 does not need backward computation.
I1003 06:23:56.931078 18825 net.cpp:325] ctx_conv1/relu does not need backward computation.
I1003 06:23:56.931082 18825 net.cpp:325] ctx_conv1/bn does not need backward computation.
I1003 06:23:56.931085 18825 net.cpp:325] ctx_conv1 does not need backward computation.
I1003 06:23:56.931089 18825 net.cpp:325] out3_out5_combined does not need backward computation.
I1003 06:23:56.931093 18825 net.cpp:325] out3a/relu does not need backward computation.
I1003 06:23:56.931097 18825 net.cpp:325] out3a/bn does not need backward computation.
I1003 06:23:56.931102 18825 net.cpp:325] out3a does not need backward computation.
I1003 06:23:56.931105 18825 net.cpp:325] out5a_up2 does not need backward computation.
I1003 06:23:56.931108 18825 net.cpp:325] out5a/relu does not need backward computation.
I1003 06:23:56.931113 18825 net.cpp:325] out5a/bn does not need backward computation.
I1003 06:23:56.931116 18825 net.cpp:325] out5a does not need backward computation.
I1003 06:23:56.931119 18825 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I1003 06:23:56.931123 18825 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I1003 06:23:56.931131 18825 net.cpp:325] res5a_branch2b does not need backward computation.
I1003 06:23:56.931135 18825 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I1003 06:23:56.931139 18825 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I1003 06:23:56.931143 18825 net.cpp:325] res5a_branch2a does not need backward computation.
I1003 06:23:56.931149 18825 net.cpp:325] pool4 does not need backward computation.
I1003 06:23:56.931152 18825 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I1003 06:23:56.931157 18825 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I1003 06:23:56.931161 18825 net.cpp:325] res4a_branch2b does not need backward computation.
I1003 06:23:56.931165 18825 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I1003 06:23:56.931169 18825 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I1003 06:23:56.931172 18825 net.cpp:325] res4a_branch2a does not need backward computation.
I1003 06:23:56.931176 18825 net.cpp:325] pool3 does not need backward computation.
I1003 06:23:56.931180 18825 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I1003 06:23:56.931185 18825 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I1003 06:23:56.931188 18825 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I1003 06:23:56.931192 18825 net.cpp:325] res3a_branch2b does not need backward computation.
I1003 06:23:56.931196 18825 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I1003 06:23:56.931200 18825 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I1003 06:23:56.931205 18825 net.cpp:325] res3a_branch2a does not need backward computation.
I1003 06:23:56.931208 18825 net.cpp:325] pool2 does not need backward computation.
I1003 06:23:56.931213 18825 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I1003 06:23:56.931217 18825 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I1003 06:23:56.931221 18825 net.cpp:325] res2a_branch2b does not need backward computation.
I1003 06:23:56.931226 18825 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I1003 06:23:56.931229 18825 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I1003 06:23:56.931232 18825 net.cpp:325] res2a_branch2a does not need backward computation.
I1003 06:23:56.931236 18825 net.cpp:325] pool1 does not need backward computation.
I1003 06:23:56.931241 18825 net.cpp:325] conv1b/relu does not need backward computation.
I1003 06:23:56.931244 18825 net.cpp:325] conv1b/bn does not need backward computation.
I1003 06:23:56.931248 18825 net.cpp:325] conv1b does not need backward computation.
I1003 06:23:56.931252 18825 net.cpp:325] conv1a/relu does not need backward computation.
I1003 06:23:56.931257 18825 net.cpp:325] conv1a/bn does not need backward computation.
I1003 06:23:56.931259 18825 net.cpp:325] conv1a does not need backward computation.
I1003 06:23:56.931263 18825 net.cpp:325] data/bias does not need backward computation.
I1003 06:23:56.931268 18825 net.cpp:325] input does not need backward computation.
I1003 06:23:56.931272 18825 net.cpp:367] This network produces output argMaxOut
I1003 06:23:56.931308 18825 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I1003 06:23:56.931313 18825 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I1003 06:23:56.931315 18825 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I1003 06:23:56.931319 18825 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1003 06:23:56.931322 18825 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I1003 06:23:56.931325 18825 net.cpp:407] Network initialization done.
I1003 06:23:56.936642 18825 net.cpp:1078] Ignoring source layer data
I1003 06:23:56.936661 18825 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1003 06:23:56.936694 18825 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1003 06:23:56.936717 18825 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.936863 18825 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1003 06:23:56.936869 18825 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1003 06:23:56.936882 18825 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1003 06:23:56.936975 18825 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1003 06:23:56.936980 18825 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1003 06:23:56.936983 18825 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1003 06:23:56.937000 18825 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937095 18825 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1003 06:23:56.937100 18825 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1003 06:23:56.937115 18825 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937206 18825 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1003 06:23:56.937211 18825 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1003 06:23:56.937213 18825 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1003 06:23:56.937252 18825 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937342 18825 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1003 06:23:56.937347 18825 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1003 06:23:56.937371 18825 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937450 18825 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1003 06:23:56.937455 18825 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1003 06:23:56.937459 18825 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1003 06:23:56.937463 18825 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1003 06:23:56.937572 18825 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937654 18825 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1003 06:23:56.937659 18825 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1003 06:23:56.937718 18825 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:23:56.937799 18825 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1003 06:23:56.937804 18825 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1003 06:23:56.937808 18825 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1003 06:23:56.938133 18825 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.938216 18825 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1003 06:23:56.938221 18825 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1003 06:23:56.938410 18825 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:23:56.938488 18825 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1003 06:23:56.938493 18825 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1003 06:23:56.938547 18825 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.938640 18825 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1003 06:23:56.938645 18825 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1003 06:23:56.938653 18825 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1003 06:23:56.938676 18825 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1003 06:23:56.938774 18825 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1003 06:23:56.938779 18825 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1003 06:23:56.938783 18825 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1003 06:23:56.938804 18825 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1003 06:23:56.938894 18825 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1003 06:23:56.938899 18825 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1003 06:23:56.938920 18825 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1003 06:23:56.939010 18825 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1003 06:23:56.939015 18825 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1003 06:23:56.939035 18825 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1003 06:23:56.939126 18825 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1003 06:23:56.939131 18825 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1003 06:23:56.939153 18825 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1003 06:23:56.939241 18825 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1003 06:23:56.939246 18825 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1003 06:23:56.939260 18825 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1003 06:23:56.939263 18825 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1003 06:23:56.939271 18825 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1003 06:23:56.939280 18825 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1003 06:23:56.939290 18825 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/initial/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/initial/cityscapes5_jsegnet21v2_iter_120000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I1003 06:23:57.805707 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:57.937034 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:57.955791 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:57.999742 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.019670 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.026664 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.036356 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.039933 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.054152 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.060806 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:23:58.087754 18825 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.97455676307, mean_iou=0.825654093029, iou=[ 0.95761541  0.96552323  0.77736137  0.55684367  0.87092677]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.974087220335, mean_iou=0.835573666559, iou=[ 0.95694168  0.96616225  0.77317111  0.60001612  0.88157717]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.972279479265, mean_iou=0.831190916937, iou=[ 0.95383573  0.96290285  0.75695414  0.5989778   0.88328406]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.968052986999, mean_iou=0.834122352618, iou=[ 0.94755897  0.95598472  0.7779777   0.61160269  0.87748767]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.968415051219, mean_iou=0.832107341958, iou=[ 0.94849567  0.95594567  0.77530833  0.60325509  0.87753194]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.968699133756, mean_iou=0.831438460416, iou=[ 0.94871221  0.95732821  0.76329873  0.60415327  0.88369987]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.970359993272, mean_iou=0.833659281884, iou=[ 0.95158456  0.95964065  0.76359984  0.60063193  0.89283943]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.970072399923, mean_iou=0.836577431454, iou=[ 0.95109814  0.96016518  0.77522614  0.60331524  0.89308245]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.970829195916, mean_iou=0.836909934376, iou=[ 0.95236795  0.96181186  0.77214726  0.60211117  0.89611143]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.970090030393, mean_iou=0.834795998338, iou=[ 0.95095104  0.96003117  0.76996983  0.59613745  0.8968905 ]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.970688072964, mean_iou=0.83419376851, iou=[ 0.9519074   0.96142499  0.76600342  0.59530888  0.89632416]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.966411284777, mean_iou=0.829865895596, iou=[ 0.94531399  0.94781238  0.76353566  0.59883879  0.89382866]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.956796906605, mean_iou=0.820687013607, iou=[ 0.93037939  0.91851504  0.76018755  0.59997672  0.89437636]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.958541529725, mean_iou=0.823647375804, iou=[ 0.9330663   0.92254256  0.75990567  0.60525262  0.89746974]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.959750649819, mean_iou=0.827369767594, iou=[ 0.93499136  0.92570648  0.76819407  0.61111176  0.89684516]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.960953154021, mean_iou=0.829570079657, iou=[ 0.93684065  0.92900417  0.76911842  0.61402358  0.89886357]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.96223073922, mean_iou=0.831926170326, iou=[ 0.93875248  0.93172754  0.76830677  0.61649376  0.9043503 ]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.963611155642, mean_iou=0.832859802092, iou=[ 0.9409029   0.93473389  0.76774481  0.61393505  0.90698236]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.960983104341, mean_iou=0.831626139192, iou=[ 0.93662951  0.92718749  0.7718439   0.61488053  0.90758926]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.96181507313, mean_iou=0.833717946324, iou=[ 0.93801851  0.92912061  0.77197472  0.62122001  0.90825587]
-------------------------------------------------------------
Final: pixel_accuracy=0.96181507313, mean_iou=0.833717946324, iou=[ 0.93801851  0.92912061  0.77197472  0.62122001  0.90825587]
-------------------------------------------------------------
initial eval.
I1003 06:26:33.754779 22155 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:26:33.757889 22155 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:26:33.759183 22155 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:26:33.760464 22155 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:26:33.764433 22155 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/deploy.prototxt
I1003 06:26:33.764466 22155 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W1003 06:26:33.764472 22155 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I1003 06:26:33.765143 22155 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I1003 06:26:33.765509 22155 net.cpp:104] Using FLOAT as default forward math type
I1003 06:26:33.765519 22155 net.cpp:110] Using FLOAT as default backward math type
I1003 06:26:33.765527 22155 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I1003 06:26:33.765533 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:33.765555 22155 net.cpp:184] Created Layer input (0)
I1003 06:26:33.765563 22155 net.cpp:530] input -> data
I1003 06:26:33.766734 22155 net.cpp:245] Setting up input
I1003 06:26:33.766753 22155 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I1003 06:26:33.766760 22155 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1003 06:26:33.766780 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:33.766806 22155 net.cpp:184] Created Layer data/bias (1)
I1003 06:26:33.766819 22155 net.cpp:561] data/bias <- data
I1003 06:26:33.766827 22155 net.cpp:530] data/bias -> data/bias
I1003 06:26:33.773591 22155 net.cpp:245] Setting up data/bias
I1003 06:26:33.773618 22155 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I1003 06:26:33.773632 22155 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1003 06:26:33.773638 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:33.773668 22155 net.cpp:184] Created Layer conv1a (2)
I1003 06:26:33.773674 22155 net.cpp:561] conv1a <- data/bias
I1003 06:26:33.773681 22155 net.cpp:530] conv1a -> conv1a
I1003 06:26:34.157795 22155 net.cpp:245] Setting up conv1a
I1003 06:26:34.157819 22155 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I1003 06:26:34.157829 22155 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1003 06:26:34.157835 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.157843 22155 net.cpp:184] Created Layer conv1a/bn (3)
I1003 06:26:34.157846 22155 net.cpp:561] conv1a/bn <- conv1a
I1003 06:26:34.157850 22155 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1003 06:26:34.158726 22155 net.cpp:245] Setting up conv1a/bn
I1003 06:26:34.158736 22155 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I1003 06:26:34.158743 22155 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1003 06:26:34.158746 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.158751 22155 net.cpp:184] Created Layer conv1a/relu (4)
I1003 06:26:34.158752 22155 net.cpp:561] conv1a/relu <- conv1a
I1003 06:26:34.158754 22155 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1003 06:26:34.158764 22155 net.cpp:245] Setting up conv1a/relu
I1003 06:26:34.158767 22155 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I1003 06:26:34.158769 22155 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1003 06:26:34.158772 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.158787 22155 net.cpp:184] Created Layer conv1b (5)
I1003 06:26:34.158790 22155 net.cpp:561] conv1b <- conv1a
I1003 06:26:34.158792 22155 net.cpp:530] conv1b -> conv1b
I1003 06:26:34.160398 22155 net.cpp:245] Setting up conv1b
I1003 06:26:34.160408 22155 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I1003 06:26:34.160413 22155 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1003 06:26:34.160416 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.160420 22155 net.cpp:184] Created Layer conv1b/bn (6)
I1003 06:26:34.160423 22155 net.cpp:561] conv1b/bn <- conv1b
I1003 06:26:34.160425 22155 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1003 06:26:34.161272 22155 net.cpp:245] Setting up conv1b/bn
I1003 06:26:34.161280 22155 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I1003 06:26:34.161286 22155 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1003 06:26:34.161289 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.161293 22155 net.cpp:184] Created Layer conv1b/relu (7)
I1003 06:26:34.161294 22155 net.cpp:561] conv1b/relu <- conv1b
I1003 06:26:34.161298 22155 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1003 06:26:34.161300 22155 net.cpp:245] Setting up conv1b/relu
I1003 06:26:34.161303 22155 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I1003 06:26:34.161305 22155 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1003 06:26:34.161319 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.161322 22155 net.cpp:184] Created Layer pool1 (8)
I1003 06:26:34.161325 22155 net.cpp:561] pool1 <- conv1b
I1003 06:26:34.161329 22155 net.cpp:530] pool1 -> pool1
I1003 06:26:34.161373 22155 net.cpp:245] Setting up pool1
I1003 06:26:34.161381 22155 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I1003 06:26:34.161386 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1003 06:26:34.161389 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.161396 22155 net.cpp:184] Created Layer res2a_branch2a (9)
I1003 06:26:34.161401 22155 net.cpp:561] res2a_branch2a <- pool1
I1003 06:26:34.161404 22155 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1003 06:26:34.162673 22155 net.cpp:245] Setting up res2a_branch2a
I1003 06:26:34.162683 22155 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I1003 06:26:34.162688 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1003 06:26:34.162690 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.162696 22155 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I1003 06:26:34.162698 22155 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1003 06:26:34.162701 22155 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1003 06:26:34.163170 22155 net.cpp:245] Setting up res2a_branch2a/bn
I1003 06:26:34.163178 22155 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I1003 06:26:34.163184 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1003 06:26:34.163187 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.163189 22155 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I1003 06:26:34.163192 22155 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1003 06:26:34.163194 22155 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1003 06:26:34.163198 22155 net.cpp:245] Setting up res2a_branch2a/relu
I1003 06:26:34.163200 22155 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I1003 06:26:34.163203 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1003 06:26:34.163204 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.163218 22155 net.cpp:184] Created Layer res2a_branch2b (12)
I1003 06:26:34.163221 22155 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1003 06:26:34.163223 22155 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1003 06:26:34.164296 22155 net.cpp:245] Setting up res2a_branch2b
I1003 06:26:34.164305 22155 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I1003 06:26:34.164310 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1003 06:26:34.164314 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.164321 22155 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I1003 06:26:34.164324 22155 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1003 06:26:34.164326 22155 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1003 06:26:34.165151 22155 net.cpp:245] Setting up res2a_branch2b/bn
I1003 06:26:34.165160 22155 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I1003 06:26:34.165166 22155 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1003 06:26:34.165169 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.165176 22155 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I1003 06:26:34.165179 22155 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1003 06:26:34.165181 22155 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1003 06:26:34.165192 22155 net.cpp:245] Setting up res2a_branch2b/relu
I1003 06:26:34.165196 22155 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I1003 06:26:34.165199 22155 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1003 06:26:34.165200 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.165205 22155 net.cpp:184] Created Layer pool2 (15)
I1003 06:26:34.165207 22155 net.cpp:561] pool2 <- res2a_branch2b
I1003 06:26:34.165210 22155 net.cpp:530] pool2 -> pool2
I1003 06:26:34.165256 22155 net.cpp:245] Setting up pool2
I1003 06:26:34.165264 22155 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I1003 06:26:34.165268 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1003 06:26:34.165272 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.165287 22155 net.cpp:184] Created Layer res3a_branch2a (16)
I1003 06:26:34.165290 22155 net.cpp:561] res3a_branch2a <- pool2
I1003 06:26:34.165294 22155 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1003 06:26:34.167986 22155 net.cpp:245] Setting up res3a_branch2a
I1003 06:26:34.167996 22155 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I1003 06:26:34.168000 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1003 06:26:34.168004 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.168007 22155 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I1003 06:26:34.168010 22155 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1003 06:26:34.168012 22155 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1003 06:26:34.168483 22155 net.cpp:245] Setting up res3a_branch2a/bn
I1003 06:26:34.168490 22155 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I1003 06:26:34.168498 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1003 06:26:34.168500 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.168503 22155 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I1003 06:26:34.168505 22155 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1003 06:26:34.168509 22155 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1003 06:26:34.168511 22155 net.cpp:245] Setting up res3a_branch2a/relu
I1003 06:26:34.168514 22155 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I1003 06:26:34.168516 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1003 06:26:34.168519 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.168524 22155 net.cpp:184] Created Layer res3a_branch2b (19)
I1003 06:26:34.168525 22155 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1003 06:26:34.168527 22155 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1003 06:26:34.169523 22155 net.cpp:245] Setting up res3a_branch2b
I1003 06:26:34.169533 22155 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I1003 06:26:34.169536 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1003 06:26:34.169539 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.169543 22155 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I1003 06:26:34.169545 22155 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1003 06:26:34.169548 22155 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1003 06:26:34.169950 22155 net.cpp:245] Setting up res3a_branch2b/bn
I1003 06:26:34.169960 22155 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I1003 06:26:34.169968 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1003 06:26:34.169972 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.169987 22155 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I1003 06:26:34.169991 22155 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1003 06:26:34.169996 22155 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1003 06:26:34.170003 22155 net.cpp:245] Setting up res3a_branch2b/relu
I1003 06:26:34.170008 22155 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I1003 06:26:34.170012 22155 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1003 06:26:34.170017 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.170025 22155 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1003 06:26:34.170037 22155 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1003 06:26:34.170040 22155 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:26:34.170047 22155 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:26:34.170073 22155 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1003 06:26:34.170078 22155 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:26:34.170083 22155 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:26:34.170086 22155 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1003 06:26:34.170090 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.170096 22155 net.cpp:184] Created Layer pool3 (23)
I1003 06:26:34.170101 22155 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:26:34.170105 22155 net.cpp:530] pool3 -> pool3
I1003 06:26:34.170136 22155 net.cpp:245] Setting up pool3
I1003 06:26:34.170141 22155 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I1003 06:26:34.170146 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1003 06:26:34.170150 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.170161 22155 net.cpp:184] Created Layer res4a_branch2a (24)
I1003 06:26:34.170166 22155 net.cpp:561] res4a_branch2a <- pool3
I1003 06:26:34.170169 22155 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1003 06:26:34.176139 22155 net.cpp:245] Setting up res4a_branch2a
I1003 06:26:34.176148 22155 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I1003 06:26:34.176154 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1003 06:26:34.176159 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.176167 22155 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I1003 06:26:34.176172 22155 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1003 06:26:34.176179 22155 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1003 06:26:34.176566 22155 net.cpp:245] Setting up res4a_branch2a/bn
I1003 06:26:34.176573 22155 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I1003 06:26:34.176582 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1003 06:26:34.176586 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.176591 22155 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I1003 06:26:34.176595 22155 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1003 06:26:34.176599 22155 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1003 06:26:34.176605 22155 net.cpp:245] Setting up res4a_branch2a/relu
I1003 06:26:34.176611 22155 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I1003 06:26:34.176616 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1003 06:26:34.176626 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.176637 22155 net.cpp:184] Created Layer res4a_branch2b (27)
I1003 06:26:34.176641 22155 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1003 06:26:34.176645 22155 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1003 06:26:34.179716 22155 net.cpp:245] Setting up res4a_branch2b
I1003 06:26:34.179724 22155 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I1003 06:26:34.179731 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1003 06:26:34.179736 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.179744 22155 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I1003 06:26:34.179747 22155 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1003 06:26:34.179752 22155 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1003 06:26:34.180127 22155 net.cpp:245] Setting up res4a_branch2b/bn
I1003 06:26:34.180135 22155 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I1003 06:26:34.180143 22155 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1003 06:26:34.180147 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.180152 22155 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I1003 06:26:34.180156 22155 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1003 06:26:34.180160 22155 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1003 06:26:34.180166 22155 net.cpp:245] Setting up res4a_branch2b/relu
I1003 06:26:34.180172 22155 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I1003 06:26:34.180179 22155 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1003 06:26:34.180183 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.180188 22155 net.cpp:184] Created Layer pool4 (30)
I1003 06:26:34.180192 22155 net.cpp:561] pool4 <- res4a_branch2b
I1003 06:26:34.180197 22155 net.cpp:530] pool4 -> pool4
I1003 06:26:34.180229 22155 net.cpp:245] Setting up pool4
I1003 06:26:34.180234 22155 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I1003 06:26:34.180239 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1003 06:26:34.180243 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.180253 22155 net.cpp:184] Created Layer res5a_branch2a (31)
I1003 06:26:34.180256 22155 net.cpp:561] res5a_branch2a <- pool4
I1003 06:26:34.180260 22155 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1003 06:26:34.205432 22155 net.cpp:245] Setting up res5a_branch2a
I1003 06:26:34.205454 22155 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I1003 06:26:34.205464 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1003 06:26:34.205469 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.205480 22155 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I1003 06:26:34.205485 22155 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1003 06:26:34.205492 22155 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1003 06:26:34.205899 22155 net.cpp:245] Setting up res5a_branch2a/bn
I1003 06:26:34.205907 22155 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I1003 06:26:34.205915 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1003 06:26:34.205920 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.205926 22155 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I1003 06:26:34.205929 22155 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1003 06:26:34.205943 22155 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1003 06:26:34.205951 22155 net.cpp:245] Setting up res5a_branch2a/relu
I1003 06:26:34.205956 22155 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I1003 06:26:34.205960 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1003 06:26:34.205965 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.205981 22155 net.cpp:184] Created Layer res5a_branch2b (34)
I1003 06:26:34.205984 22155 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1003 06:26:34.205988 22155 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1003 06:26:34.218431 22155 net.cpp:245] Setting up res5a_branch2b
I1003 06:26:34.218453 22155 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I1003 06:26:34.218468 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1003 06:26:34.218474 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.218484 22155 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I1003 06:26:34.218490 22155 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1003 06:26:34.218495 22155 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1003 06:26:34.218897 22155 net.cpp:245] Setting up res5a_branch2b/bn
I1003 06:26:34.218904 22155 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I1003 06:26:34.218914 22155 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1003 06:26:34.218917 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.218924 22155 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I1003 06:26:34.218928 22155 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1003 06:26:34.218933 22155 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1003 06:26:34.218940 22155 net.cpp:245] Setting up res5a_branch2b/relu
I1003 06:26:34.218945 22155 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I1003 06:26:34.218950 22155 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1003 06:26:34.218955 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.218966 22155 net.cpp:184] Created Layer out5a (37)
I1003 06:26:34.218969 22155 net.cpp:561] out5a <- res5a_branch2b
I1003 06:26:34.218973 22155 net.cpp:530] out5a -> out5a
I1003 06:26:34.222745 22155 net.cpp:245] Setting up out5a
I1003 06:26:34.222762 22155 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I1003 06:26:34.222769 22155 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1003 06:26:34.222774 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.222782 22155 net.cpp:184] Created Layer out5a/bn (38)
I1003 06:26:34.222787 22155 net.cpp:561] out5a/bn <- out5a
I1003 06:26:34.222791 22155 net.cpp:513] out5a/bn -> out5a (in-place)
I1003 06:26:34.223218 22155 net.cpp:245] Setting up out5a/bn
I1003 06:26:34.223227 22155 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I1003 06:26:34.223234 22155 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1003 06:26:34.223239 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.223244 22155 net.cpp:184] Created Layer out5a/relu (39)
I1003 06:26:34.223248 22155 net.cpp:561] out5a/relu <- out5a
I1003 06:26:34.223253 22155 net.cpp:513] out5a/relu -> out5a (in-place)
I1003 06:26:34.223258 22155 net.cpp:245] Setting up out5a/relu
I1003 06:26:34.223263 22155 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I1003 06:26:34.223268 22155 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1003 06:26:34.223271 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.223300 22155 net.cpp:184] Created Layer out5a_up2 (40)
I1003 06:26:34.223304 22155 net.cpp:561] out5a_up2 <- out5a
I1003 06:26:34.223309 22155 net.cpp:530] out5a_up2 -> out5a_up2
I1003 06:26:34.223446 22155 net.cpp:245] Setting up out5a_up2
I1003 06:26:34.223453 22155 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I1003 06:26:34.223459 22155 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1003 06:26:34.223464 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.223472 22155 net.cpp:184] Created Layer out3a (41)
I1003 06:26:34.223476 22155 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:26:34.223480 22155 net.cpp:530] out3a -> out3a
I1003 06:26:34.224392 22155 net.cpp:245] Setting up out3a
I1003 06:26:34.224400 22155 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I1003 06:26:34.224407 22155 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1003 06:26:34.224412 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.224419 22155 net.cpp:184] Created Layer out3a/bn (42)
I1003 06:26:34.224423 22155 net.cpp:561] out3a/bn <- out3a
I1003 06:26:34.224427 22155 net.cpp:513] out3a/bn -> out3a (in-place)
I1003 06:26:34.224822 22155 net.cpp:245] Setting up out3a/bn
I1003 06:26:34.224829 22155 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I1003 06:26:34.224838 22155 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1003 06:26:34.224843 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.224848 22155 net.cpp:184] Created Layer out3a/relu (43)
I1003 06:26:34.224851 22155 net.cpp:561] out3a/relu <- out3a
I1003 06:26:34.224855 22155 net.cpp:513] out3a/relu -> out3a (in-place)
I1003 06:26:34.224861 22155 net.cpp:245] Setting up out3a/relu
I1003 06:26:34.224867 22155 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I1003 06:26:34.224871 22155 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1003 06:26:34.224876 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.224887 22155 net.cpp:184] Created Layer out3_out5_combined (44)
I1003 06:26:34.224891 22155 net.cpp:561] out3_out5_combined <- out5a_up2
I1003 06:26:34.224895 22155 net.cpp:561] out3_out5_combined <- out3a
I1003 06:26:34.224900 22155 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1003 06:26:34.224918 22155 net.cpp:245] Setting up out3_out5_combined
I1003 06:26:34.224923 22155 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I1003 06:26:34.224927 22155 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1003 06:26:34.224932 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.224942 22155 net.cpp:184] Created Layer ctx_conv1 (45)
I1003 06:26:34.224947 22155 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1003 06:26:34.224951 22155 net.cpp:530] ctx_conv1 -> ctx_conv1
I1003 06:26:34.225848 22155 net.cpp:245] Setting up ctx_conv1
I1003 06:26:34.225857 22155 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I1003 06:26:34.225863 22155 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1003 06:26:34.225867 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.225874 22155 net.cpp:184] Created Layer ctx_conv1/bn (46)
I1003 06:26:34.225878 22155 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1003 06:26:34.225883 22155 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1003 06:26:34.226271 22155 net.cpp:245] Setting up ctx_conv1/bn
I1003 06:26:34.226279 22155 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I1003 06:26:34.226287 22155 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1003 06:26:34.226300 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.226305 22155 net.cpp:184] Created Layer ctx_conv1/relu (47)
I1003 06:26:34.226310 22155 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1003 06:26:34.226313 22155 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1003 06:26:34.226320 22155 net.cpp:245] Setting up ctx_conv1/relu
I1003 06:26:34.226325 22155 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I1003 06:26:34.226330 22155 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1003 06:26:34.226336 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.226346 22155 net.cpp:184] Created Layer ctx_conv2 (48)
I1003 06:26:34.226349 22155 net.cpp:561] ctx_conv2 <- ctx_conv1
I1003 06:26:34.226352 22155 net.cpp:530] ctx_conv2 -> ctx_conv2
I1003 06:26:34.227293 22155 net.cpp:245] Setting up ctx_conv2
I1003 06:26:34.227300 22155 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I1003 06:26:34.227306 22155 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1003 06:26:34.227311 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.227318 22155 net.cpp:184] Created Layer ctx_conv2/bn (49)
I1003 06:26:34.227321 22155 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1003 06:26:34.227325 22155 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1003 06:26:34.227727 22155 net.cpp:245] Setting up ctx_conv2/bn
I1003 06:26:34.227735 22155 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I1003 06:26:34.227742 22155 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1003 06:26:34.227747 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.227752 22155 net.cpp:184] Created Layer ctx_conv2/relu (50)
I1003 06:26:34.227756 22155 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1003 06:26:34.227761 22155 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1003 06:26:34.227767 22155 net.cpp:245] Setting up ctx_conv2/relu
I1003 06:26:34.227772 22155 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I1003 06:26:34.227777 22155 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1003 06:26:34.227782 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.227797 22155 net.cpp:184] Created Layer ctx_conv3 (51)
I1003 06:26:34.227802 22155 net.cpp:561] ctx_conv3 <- ctx_conv2
I1003 06:26:34.227805 22155 net.cpp:530] ctx_conv3 -> ctx_conv3
I1003 06:26:34.228705 22155 net.cpp:245] Setting up ctx_conv3
I1003 06:26:34.228713 22155 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I1003 06:26:34.228719 22155 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1003 06:26:34.228724 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.228732 22155 net.cpp:184] Created Layer ctx_conv3/bn (52)
I1003 06:26:34.228735 22155 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1003 06:26:34.228739 22155 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1003 06:26:34.229140 22155 net.cpp:245] Setting up ctx_conv3/bn
I1003 06:26:34.229147 22155 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I1003 06:26:34.229156 22155 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1003 06:26:34.229159 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.229164 22155 net.cpp:184] Created Layer ctx_conv3/relu (53)
I1003 06:26:34.229168 22155 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1003 06:26:34.229172 22155 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1003 06:26:34.229177 22155 net.cpp:245] Setting up ctx_conv3/relu
I1003 06:26:34.229183 22155 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I1003 06:26:34.229192 22155 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1003 06:26:34.229198 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.229207 22155 net.cpp:184] Created Layer ctx_conv4 (54)
I1003 06:26:34.229210 22155 net.cpp:561] ctx_conv4 <- ctx_conv3
I1003 06:26:34.229214 22155 net.cpp:530] ctx_conv4 -> ctx_conv4
I1003 06:26:34.230114 22155 net.cpp:245] Setting up ctx_conv4
I1003 06:26:34.230123 22155 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I1003 06:26:34.230129 22155 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1003 06:26:34.230134 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.230139 22155 net.cpp:184] Created Layer ctx_conv4/bn (55)
I1003 06:26:34.230144 22155 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1003 06:26:34.230147 22155 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1003 06:26:34.230556 22155 net.cpp:245] Setting up ctx_conv4/bn
I1003 06:26:34.230564 22155 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I1003 06:26:34.230572 22155 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1003 06:26:34.230577 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.230581 22155 net.cpp:184] Created Layer ctx_conv4/relu (56)
I1003 06:26:34.230587 22155 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1003 06:26:34.230590 22155 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1003 06:26:34.230597 22155 net.cpp:245] Setting up ctx_conv4/relu
I1003 06:26:34.230602 22155 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I1003 06:26:34.230607 22155 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1003 06:26:34.230610 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.230628 22155 net.cpp:184] Created Layer ctx_final (57)
I1003 06:26:34.230631 22155 net.cpp:561] ctx_final <- ctx_conv4
I1003 06:26:34.230635 22155 net.cpp:530] ctx_final -> ctx_final
I1003 06:26:34.230902 22155 net.cpp:245] Setting up ctx_final
I1003 06:26:34.230911 22155 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I1003 06:26:34.230916 22155 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1003 06:26:34.230921 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.230926 22155 net.cpp:184] Created Layer ctx_final/relu (58)
I1003 06:26:34.230931 22155 net.cpp:561] ctx_final/relu <- ctx_final
I1003 06:26:34.230934 22155 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1003 06:26:34.230942 22155 net.cpp:245] Setting up ctx_final/relu
I1003 06:26:34.230947 22155 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I1003 06:26:34.230950 22155 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1003 06:26:34.230955 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.230962 22155 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I1003 06:26:34.230967 22155 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1003 06:26:34.230970 22155 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1003 06:26:34.231094 22155 net.cpp:245] Setting up out_deconv_final_up2
I1003 06:26:34.231101 22155 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I1003 06:26:34.231106 22155 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1003 06:26:34.231111 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.231117 22155 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I1003 06:26:34.231122 22155 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1003 06:26:34.231130 22155 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1003 06:26:34.231248 22155 net.cpp:245] Setting up out_deconv_final_up4
I1003 06:26:34.231254 22155 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I1003 06:26:34.231259 22155 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1003 06:26:34.231263 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.231271 22155 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I1003 06:26:34.231274 22155 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1003 06:26:34.231278 22155 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1003 06:26:34.231396 22155 net.cpp:245] Setting up out_deconv_final_up8
I1003 06:26:34.231403 22155 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I1003 06:26:34.231408 22155 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I1003 06:26:34.231412 22155 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:26:34.231423 22155 net.cpp:184] Created Layer argMaxOut (62)
I1003 06:26:34.231427 22155 net.cpp:561] argMaxOut <- out_deconv_final_up8
I1003 06:26:34.231431 22155 net.cpp:530] argMaxOut -> argMaxOut
I1003 06:26:34.231448 22155 net.cpp:245] Setting up argMaxOut
I1003 06:26:34.231454 22155 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I1003 06:26:34.231458 22155 net.cpp:325] argMaxOut does not need backward computation.
I1003 06:26:34.231462 22155 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I1003 06:26:34.231467 22155 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I1003 06:26:34.231470 22155 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I1003 06:26:34.231474 22155 net.cpp:325] ctx_final/relu does not need backward computation.
I1003 06:26:34.231478 22155 net.cpp:325] ctx_final does not need backward computation.
I1003 06:26:34.231489 22155 net.cpp:325] ctx_conv4/relu does not need backward computation.
I1003 06:26:34.231493 22155 net.cpp:325] ctx_conv4/bn does not need backward computation.
I1003 06:26:34.231497 22155 net.cpp:325] ctx_conv4 does not need backward computation.
I1003 06:26:34.231500 22155 net.cpp:325] ctx_conv3/relu does not need backward computation.
I1003 06:26:34.231504 22155 net.cpp:325] ctx_conv3/bn does not need backward computation.
I1003 06:26:34.231508 22155 net.cpp:325] ctx_conv3 does not need backward computation.
I1003 06:26:34.231513 22155 net.cpp:325] ctx_conv2/relu does not need backward computation.
I1003 06:26:34.231515 22155 net.cpp:325] ctx_conv2/bn does not need backward computation.
I1003 06:26:34.231519 22155 net.cpp:325] ctx_conv2 does not need backward computation.
I1003 06:26:34.231523 22155 net.cpp:325] ctx_conv1/relu does not need backward computation.
I1003 06:26:34.231526 22155 net.cpp:325] ctx_conv1/bn does not need backward computation.
I1003 06:26:34.231530 22155 net.cpp:325] ctx_conv1 does not need backward computation.
I1003 06:26:34.231534 22155 net.cpp:325] out3_out5_combined does not need backward computation.
I1003 06:26:34.231539 22155 net.cpp:325] out3a/relu does not need backward computation.
I1003 06:26:34.231542 22155 net.cpp:325] out3a/bn does not need backward computation.
I1003 06:26:34.231546 22155 net.cpp:325] out3a does not need backward computation.
I1003 06:26:34.231550 22155 net.cpp:325] out5a_up2 does not need backward computation.
I1003 06:26:34.231554 22155 net.cpp:325] out5a/relu does not need backward computation.
I1003 06:26:34.231559 22155 net.cpp:325] out5a/bn does not need backward computation.
I1003 06:26:34.231562 22155 net.cpp:325] out5a does not need backward computation.
I1003 06:26:34.231566 22155 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I1003 06:26:34.231570 22155 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I1003 06:26:34.231578 22155 net.cpp:325] res5a_branch2b does not need backward computation.
I1003 06:26:34.231583 22155 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I1003 06:26:34.231588 22155 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I1003 06:26:34.231592 22155 net.cpp:325] res5a_branch2a does not need backward computation.
I1003 06:26:34.231597 22155 net.cpp:325] pool4 does not need backward computation.
I1003 06:26:34.231602 22155 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I1003 06:26:34.231606 22155 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I1003 06:26:34.231609 22155 net.cpp:325] res4a_branch2b does not need backward computation.
I1003 06:26:34.231613 22155 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I1003 06:26:34.231617 22155 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I1003 06:26:34.231621 22155 net.cpp:325] res4a_branch2a does not need backward computation.
I1003 06:26:34.231626 22155 net.cpp:325] pool3 does not need backward computation.
I1003 06:26:34.231631 22155 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I1003 06:26:34.231636 22155 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I1003 06:26:34.231640 22155 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I1003 06:26:34.231644 22155 net.cpp:325] res3a_branch2b does not need backward computation.
I1003 06:26:34.231648 22155 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I1003 06:26:34.231653 22155 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I1003 06:26:34.231657 22155 net.cpp:325] res3a_branch2a does not need backward computation.
I1003 06:26:34.231662 22155 net.cpp:325] pool2 does not need backward computation.
I1003 06:26:34.231665 22155 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I1003 06:26:34.231669 22155 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I1003 06:26:34.231674 22155 net.cpp:325] res2a_branch2b does not need backward computation.
I1003 06:26:34.231678 22155 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I1003 06:26:34.231683 22155 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I1003 06:26:34.231688 22155 net.cpp:325] res2a_branch2a does not need backward computation.
I1003 06:26:34.231691 22155 net.cpp:325] pool1 does not need backward computation.
I1003 06:26:34.231695 22155 net.cpp:325] conv1b/relu does not need backward computation.
I1003 06:26:34.231699 22155 net.cpp:325] conv1b/bn does not need backward computation.
I1003 06:26:34.231703 22155 net.cpp:325] conv1b does not need backward computation.
I1003 06:26:34.231708 22155 net.cpp:325] conv1a/relu does not need backward computation.
I1003 06:26:34.231711 22155 net.cpp:325] conv1a/bn does not need backward computation.
I1003 06:26:34.231715 22155 net.cpp:325] conv1a does not need backward computation.
I1003 06:26:34.231720 22155 net.cpp:325] data/bias does not need backward computation.
I1003 06:26:34.231724 22155 net.cpp:325] input does not need backward computation.
I1003 06:26:34.231729 22155 net.cpp:367] This network produces output argMaxOut
I1003 06:26:34.231765 22155 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I1003 06:26:34.231770 22155 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I1003 06:26:34.231772 22155 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I1003 06:26:34.231776 22155 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1003 06:26:34.231779 22155 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I1003 06:26:34.231782 22155 net.cpp:407] Network initialization done.
I1003 06:26:34.236857 22155 net.cpp:1078] Ignoring source layer data
I1003 06:26:34.236877 22155 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1003 06:26:34.236912 22155 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1003 06:26:34.236937 22155 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237087 22155 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1003 06:26:34.237092 22155 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1003 06:26:34.237105 22155 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237197 22155 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1003 06:26:34.237203 22155 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1003 06:26:34.237206 22155 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1003 06:26:34.237224 22155 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237319 22155 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1003 06:26:34.237324 22155 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1003 06:26:34.237339 22155 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237428 22155 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1003 06:26:34.237433 22155 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1003 06:26:34.237437 22155 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1003 06:26:34.237475 22155 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237562 22155 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1003 06:26:34.237568 22155 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1003 06:26:34.237591 22155 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237670 22155 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1003 06:26:34.237675 22155 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1003 06:26:34.237679 22155 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1003 06:26:34.237682 22155 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1003 06:26:34.237789 22155 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.237871 22155 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1003 06:26:34.237876 22155 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1003 06:26:34.237936 22155 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:26:34.238015 22155 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1003 06:26:34.238020 22155 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1003 06:26:34.238023 22155 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1003 06:26:34.238363 22155 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.238446 22155 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1003 06:26:34.238451 22155 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1003 06:26:34.238620 22155 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:26:34.238699 22155 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1003 06:26:34.238704 22155 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1003 06:26:34.238760 22155 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.238853 22155 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1003 06:26:34.238858 22155 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1003 06:26:34.238867 22155 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1003 06:26:34.238888 22155 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1003 06:26:34.238983 22155 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1003 06:26:34.238988 22155 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1003 06:26:34.238992 22155 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1003 06:26:34.239013 22155 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1003 06:26:34.239102 22155 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1003 06:26:34.239107 22155 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1003 06:26:34.239130 22155 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1003 06:26:34.239219 22155 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1003 06:26:34.239224 22155 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1003 06:26:34.239244 22155 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1003 06:26:34.239336 22155 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1003 06:26:34.239341 22155 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1003 06:26:34.239358 22155 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1003 06:26:34.239445 22155 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1003 06:26:34.239450 22155 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1003 06:26:34.239462 22155 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1003 06:26:34.239466 22155 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1003 06:26:34.239475 22155 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1003 06:26:34.239482 22155 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1003 06:26:34.239491 22155 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I1003 06:26:35.057281 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.188575 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.207404 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.249671 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.261862 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.267822 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.280262 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.286556 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.301479 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.308622 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:26:35.335647 22155 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.974895376059, mean_iou=0.830725874432, iou=[ 0.95835131  0.96192297  0.76209772  0.57632984  0.89492754]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.974940431396, mean_iou=0.842585442508, iou=[ 0.95851373  0.9654392   0.77501327  0.61850047  0.89546054]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.973365629312, mean_iou=0.838990334377, iou=[ 0.9558037   0.96321194  0.76492054  0.6177781   0.89323739]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.969792194239, mean_iou=0.842326522189, iou=[ 0.9507911   0.95792195  0.78721321  0.63172349  0.88398287]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.969829148462, mean_iou=0.8407784266, iou=[ 0.95095913  0.95686799  0.78552621  0.62466419  0.88587461]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.970399632244, mean_iou=0.839846558827, iou=[ 0.95164908  0.95867018  0.77524078  0.6197995   0.89387326]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.971826848895, mean_iou=0.841308593818, iou=[ 0.95410677  0.96075466  0.77492692  0.6153653   0.90138933]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.97147181353, mean_iou=0.843970618086, iou=[ 0.9534511   0.96125984  0.78457955  0.61906035  0.90150225]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.972165665566, mean_iou=0.843972935061, iou=[ 0.9546116   0.96292011  0.78016491  0.6181776   0.90399045]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.971554478501, mean_iou=0.841843411668, iou=[ 0.95345149  0.96149347  0.77825049  0.61172892  0.90429269]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.97188773219, mean_iou=0.840700260474, iou=[ 0.95400066  0.96212386  0.77387782  0.60944114  0.90405783]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.96731822367, mean_iou=0.836409640531, iou=[ 0.94692036  0.94779734  0.77115099  0.61516093  0.90101858]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.958405435286, mean_iou=0.827825409707, iou=[ 0.93302602  0.92037946  0.76762549  0.61545232  0.90264376]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.960045412699, mean_iou=0.830461133707, iou=[ 0.9355459   0.92424638  0.76665229  0.6204114   0.9054497 ]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.961181643347, mean_iou=0.833986862354, iou=[ 0.93737181  0.92718015  0.77477301  0.62557802  0.90503132]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.96228167395, mean_iou=0.836059045034, iou=[ 0.93905244  0.93027608  0.77454606  0.62946785  0.9069528 ]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.96350338897, mean_iou=0.838305055609, iou=[ 0.94087553  0.93296163  0.77425953  0.63198063  0.91144796]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.964833869587, mean_iou=0.839492197605, iou=[ 0.94295531  0.93594266  0.77350557  0.63168469  0.91337275]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.96160498692, mean_iou=0.837559893079, iou=[ 0.93768933  0.92676659  0.77828509  0.63134555  0.9137129 ]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.962218842053, mean_iou=0.839370371168, iou=[ 0.9387087   0.92810479  0.77845766  0.637117    0.91446371]
-------------------------------------------------------------
Final: pixel_accuracy=0.962218842053, mean_iou=0.839370371168, iou=[ 0.9387087   0.92810479  0.77845766  0.637117    0.91446371]
-------------------------------------------------------------
l1reg eval.
I1003 06:29:09.947482 25442 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:29:09.948179 25442 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:29:09.948753 25442 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:29:09.949311 25442 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:29:09.950961 25442 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/deploy.prototxt
I1003 06:29:09.950975 25442 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W1003 06:29:09.950978 25442 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I1003 06:29:09.951264 25442 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I1003 06:29:09.951403 25442 net.cpp:104] Using FLOAT as default forward math type
I1003 06:29:09.951408 25442 net.cpp:110] Using FLOAT as default backward math type
I1003 06:29:09.951412 25442 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I1003 06:29:09.951416 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:09.951426 25442 net.cpp:184] Created Layer input (0)
I1003 06:29:09.951428 25442 net.cpp:530] input -> data
I1003 06:29:09.952105 25442 net.cpp:245] Setting up input
I1003 06:29:09.952118 25442 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I1003 06:29:09.952122 25442 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1003 06:29:09.952131 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:09.952137 25442 net.cpp:184] Created Layer data/bias (1)
I1003 06:29:09.952141 25442 net.cpp:561] data/bias <- data
I1003 06:29:09.952143 25442 net.cpp:530] data/bias -> data/bias
I1003 06:29:09.956514 25442 net.cpp:245] Setting up data/bias
I1003 06:29:09.956533 25442 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I1003 06:29:09.956542 25442 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1003 06:29:09.956547 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:09.956565 25442 net.cpp:184] Created Layer conv1a (2)
I1003 06:29:09.956570 25442 net.cpp:561] conv1a <- data/bias
I1003 06:29:09.956574 25442 net.cpp:530] conv1a -> conv1a
I1003 06:29:10.328299 25442 net.cpp:245] Setting up conv1a
I1003 06:29:10.328323 25442 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I1003 06:29:10.328333 25442 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1003 06:29:10.328337 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.328346 25442 net.cpp:184] Created Layer conv1a/bn (3)
I1003 06:29:10.328349 25442 net.cpp:561] conv1a/bn <- conv1a
I1003 06:29:10.328353 25442 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1003 06:29:10.329169 25442 net.cpp:245] Setting up conv1a/bn
I1003 06:29:10.329179 25442 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I1003 06:29:10.329185 25442 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1003 06:29:10.329188 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.329192 25442 net.cpp:184] Created Layer conv1a/relu (4)
I1003 06:29:10.329195 25442 net.cpp:561] conv1a/relu <- conv1a
I1003 06:29:10.329197 25442 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1003 06:29:10.329206 25442 net.cpp:245] Setting up conv1a/relu
I1003 06:29:10.329210 25442 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I1003 06:29:10.329211 25442 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1003 06:29:10.329215 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.329226 25442 net.cpp:184] Created Layer conv1b (5)
I1003 06:29:10.329228 25442 net.cpp:561] conv1b <- conv1a
I1003 06:29:10.329231 25442 net.cpp:530] conv1b -> conv1b
I1003 06:29:10.330714 25442 net.cpp:245] Setting up conv1b
I1003 06:29:10.330724 25442 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I1003 06:29:10.330729 25442 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1003 06:29:10.330731 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.330740 25442 net.cpp:184] Created Layer conv1b/bn (6)
I1003 06:29:10.330742 25442 net.cpp:561] conv1b/bn <- conv1b
I1003 06:29:10.330745 25442 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1003 06:29:10.331532 25442 net.cpp:245] Setting up conv1b/bn
I1003 06:29:10.331540 25442 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I1003 06:29:10.331547 25442 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1003 06:29:10.331548 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.331552 25442 net.cpp:184] Created Layer conv1b/relu (7)
I1003 06:29:10.331553 25442 net.cpp:561] conv1b/relu <- conv1b
I1003 06:29:10.331557 25442 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1003 06:29:10.331559 25442 net.cpp:245] Setting up conv1b/relu
I1003 06:29:10.331562 25442 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I1003 06:29:10.331564 25442 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1003 06:29:10.331578 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.331583 25442 net.cpp:184] Created Layer pool1 (8)
I1003 06:29:10.331585 25442 net.cpp:561] pool1 <- conv1b
I1003 06:29:10.331588 25442 net.cpp:530] pool1 -> pool1
I1003 06:29:10.331624 25442 net.cpp:245] Setting up pool1
I1003 06:29:10.331629 25442 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I1003 06:29:10.331630 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1003 06:29:10.331632 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.331637 25442 net.cpp:184] Created Layer res2a_branch2a (9)
I1003 06:29:10.331640 25442 net.cpp:561] res2a_branch2a <- pool1
I1003 06:29:10.331642 25442 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1003 06:29:10.332764 25442 net.cpp:245] Setting up res2a_branch2a
I1003 06:29:10.332773 25442 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I1003 06:29:10.332779 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1003 06:29:10.332782 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.332787 25442 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I1003 06:29:10.332789 25442 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1003 06:29:10.332792 25442 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1003 06:29:10.333189 25442 net.cpp:245] Setting up res2a_branch2a/bn
I1003 06:29:10.333195 25442 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I1003 06:29:10.333200 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1003 06:29:10.333202 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.333205 25442 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I1003 06:29:10.333207 25442 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1003 06:29:10.333209 25442 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1003 06:29:10.333214 25442 net.cpp:245] Setting up res2a_branch2a/relu
I1003 06:29:10.333215 25442 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I1003 06:29:10.333217 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1003 06:29:10.333220 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.333228 25442 net.cpp:184] Created Layer res2a_branch2b (12)
I1003 06:29:10.333231 25442 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1003 06:29:10.333233 25442 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1003 06:29:10.334169 25442 net.cpp:245] Setting up res2a_branch2b
I1003 06:29:10.334178 25442 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I1003 06:29:10.334183 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1003 06:29:10.334187 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.334192 25442 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I1003 06:29:10.334194 25442 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1003 06:29:10.334197 25442 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1003 06:29:10.334955 25442 net.cpp:245] Setting up res2a_branch2b/bn
I1003 06:29:10.334964 25442 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I1003 06:29:10.334969 25442 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1003 06:29:10.334972 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.334975 25442 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I1003 06:29:10.334978 25442 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1003 06:29:10.334980 25442 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1003 06:29:10.334992 25442 net.cpp:245] Setting up res2a_branch2b/relu
I1003 06:29:10.334995 25442 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I1003 06:29:10.334997 25442 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1003 06:29:10.335000 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.335002 25442 net.cpp:184] Created Layer pool2 (15)
I1003 06:29:10.335005 25442 net.cpp:561] pool2 <- res2a_branch2b
I1003 06:29:10.335009 25442 net.cpp:530] pool2 -> pool2
I1003 06:29:10.335041 25442 net.cpp:245] Setting up pool2
I1003 06:29:10.335045 25442 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I1003 06:29:10.335047 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1003 06:29:10.335049 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.335055 25442 net.cpp:184] Created Layer res3a_branch2a (16)
I1003 06:29:10.335058 25442 net.cpp:561] res3a_branch2a <- pool2
I1003 06:29:10.335060 25442 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1003 06:29:10.337589 25442 net.cpp:245] Setting up res3a_branch2a
I1003 06:29:10.337599 25442 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I1003 06:29:10.337604 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1003 06:29:10.337606 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.337610 25442 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I1003 06:29:10.337612 25442 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1003 06:29:10.337615 25442 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1003 06:29:10.338004 25442 net.cpp:245] Setting up res3a_branch2a/bn
I1003 06:29:10.338011 25442 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I1003 06:29:10.338018 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1003 06:29:10.338021 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.338023 25442 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I1003 06:29:10.338026 25442 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1003 06:29:10.338027 25442 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1003 06:29:10.338032 25442 net.cpp:245] Setting up res3a_branch2a/relu
I1003 06:29:10.338033 25442 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I1003 06:29:10.338035 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1003 06:29:10.338038 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.338042 25442 net.cpp:184] Created Layer res3a_branch2b (19)
I1003 06:29:10.338045 25442 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1003 06:29:10.338047 25442 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1003 06:29:10.338938 25442 net.cpp:245] Setting up res3a_branch2b
I1003 06:29:10.338945 25442 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I1003 06:29:10.338949 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1003 06:29:10.338951 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.338955 25442 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I1003 06:29:10.338958 25442 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1003 06:29:10.338959 25442 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1003 06:29:10.339336 25442 net.cpp:245] Setting up res3a_branch2b/bn
I1003 06:29:10.339342 25442 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I1003 06:29:10.339347 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1003 06:29:10.339350 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.339360 25442 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I1003 06:29:10.339362 25442 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1003 06:29:10.339365 25442 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1003 06:29:10.339368 25442 net.cpp:245] Setting up res3a_branch2b/relu
I1003 06:29:10.339370 25442 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I1003 06:29:10.339372 25442 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1003 06:29:10.339375 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.339380 25442 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1003 06:29:10.339382 25442 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1003 06:29:10.339385 25442 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:29:10.339388 25442 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:29:10.339411 25442 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1003 06:29:10.339413 25442 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:29:10.339416 25442 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:29:10.339418 25442 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1003 06:29:10.339421 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.339423 25442 net.cpp:184] Created Layer pool3 (23)
I1003 06:29:10.339426 25442 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:29:10.339428 25442 net.cpp:530] pool3 -> pool3
I1003 06:29:10.339454 25442 net.cpp:245] Setting up pool3
I1003 06:29:10.339458 25442 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I1003 06:29:10.339462 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1003 06:29:10.339463 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.339469 25442 net.cpp:184] Created Layer res4a_branch2a (24)
I1003 06:29:10.339471 25442 net.cpp:561] res4a_branch2a <- pool3
I1003 06:29:10.339474 25442 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1003 06:29:10.345538 25442 net.cpp:245] Setting up res4a_branch2a
I1003 06:29:10.345549 25442 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I1003 06:29:10.345554 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1003 06:29:10.345557 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.345561 25442 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I1003 06:29:10.345564 25442 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1003 06:29:10.345566 25442 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1003 06:29:10.345955 25442 net.cpp:245] Setting up res4a_branch2a/bn
I1003 06:29:10.345962 25442 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I1003 06:29:10.345968 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1003 06:29:10.345969 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.345973 25442 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I1003 06:29:10.345974 25442 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1003 06:29:10.345976 25442 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1003 06:29:10.345980 25442 net.cpp:245] Setting up res4a_branch2a/relu
I1003 06:29:10.345983 25442 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I1003 06:29:10.345985 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1003 06:29:10.345995 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.346002 25442 net.cpp:184] Created Layer res4a_branch2b (27)
I1003 06:29:10.346006 25442 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1003 06:29:10.346009 25442 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1003 06:29:10.349084 25442 net.cpp:245] Setting up res4a_branch2b
I1003 06:29:10.349092 25442 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I1003 06:29:10.349095 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1003 06:29:10.349098 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.349102 25442 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I1003 06:29:10.349105 25442 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1003 06:29:10.349108 25442 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1003 06:29:10.349483 25442 net.cpp:245] Setting up res4a_branch2b/bn
I1003 06:29:10.349488 25442 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I1003 06:29:10.349493 25442 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1003 06:29:10.349496 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.349498 25442 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I1003 06:29:10.349500 25442 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1003 06:29:10.349503 25442 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1003 06:29:10.349506 25442 net.cpp:245] Setting up res4a_branch2b/relu
I1003 06:29:10.349509 25442 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I1003 06:29:10.349511 25442 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1003 06:29:10.349514 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.349516 25442 net.cpp:184] Created Layer pool4 (30)
I1003 06:29:10.349519 25442 net.cpp:561] pool4 <- res4a_branch2b
I1003 06:29:10.349521 25442 net.cpp:530] pool4 -> pool4
I1003 06:29:10.349550 25442 net.cpp:245] Setting up pool4
I1003 06:29:10.349555 25442 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I1003 06:29:10.349556 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1003 06:29:10.349558 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.349565 25442 net.cpp:184] Created Layer res5a_branch2a (31)
I1003 06:29:10.349566 25442 net.cpp:561] res5a_branch2a <- pool4
I1003 06:29:10.349570 25442 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1003 06:29:10.374807 25442 net.cpp:245] Setting up res5a_branch2a
I1003 06:29:10.374830 25442 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I1003 06:29:10.374836 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1003 06:29:10.374840 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.374848 25442 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I1003 06:29:10.374853 25442 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1003 06:29:10.374856 25442 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1003 06:29:10.375259 25442 net.cpp:245] Setting up res5a_branch2a/bn
I1003 06:29:10.375267 25442 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I1003 06:29:10.375272 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1003 06:29:10.375274 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.375278 25442 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I1003 06:29:10.375280 25442 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1003 06:29:10.375295 25442 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1003 06:29:10.375300 25442 net.cpp:245] Setting up res5a_branch2a/relu
I1003 06:29:10.375303 25442 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I1003 06:29:10.375305 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1003 06:29:10.375308 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.375319 25442 net.cpp:184] Created Layer res5a_branch2b (34)
I1003 06:29:10.375321 25442 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1003 06:29:10.375324 25442 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1003 06:29:10.387707 25442 net.cpp:245] Setting up res5a_branch2b
I1003 06:29:10.387717 25442 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I1003 06:29:10.387725 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1003 06:29:10.387728 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.387732 25442 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I1003 06:29:10.387735 25442 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1003 06:29:10.387737 25442 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1003 06:29:10.388120 25442 net.cpp:245] Setting up res5a_branch2b/bn
I1003 06:29:10.388128 25442 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I1003 06:29:10.388133 25442 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1003 06:29:10.388134 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.388137 25442 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I1003 06:29:10.388139 25442 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1003 06:29:10.388142 25442 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1003 06:29:10.388145 25442 net.cpp:245] Setting up res5a_branch2b/relu
I1003 06:29:10.388149 25442 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I1003 06:29:10.388150 25442 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1003 06:29:10.388152 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.388159 25442 net.cpp:184] Created Layer out5a (37)
I1003 06:29:10.388160 25442 net.cpp:561] out5a <- res5a_branch2b
I1003 06:29:10.388164 25442 net.cpp:530] out5a -> out5a
I1003 06:29:10.391870 25442 net.cpp:245] Setting up out5a
I1003 06:29:10.391891 25442 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I1003 06:29:10.391897 25442 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1003 06:29:10.391901 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.391907 25442 net.cpp:184] Created Layer out5a/bn (38)
I1003 06:29:10.391911 25442 net.cpp:561] out5a/bn <- out5a
I1003 06:29:10.391914 25442 net.cpp:513] out5a/bn -> out5a (in-place)
I1003 06:29:10.392361 25442 net.cpp:245] Setting up out5a/bn
I1003 06:29:10.392369 25442 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I1003 06:29:10.392374 25442 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1003 06:29:10.392376 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.392379 25442 net.cpp:184] Created Layer out5a/relu (39)
I1003 06:29:10.392381 25442 net.cpp:561] out5a/relu <- out5a
I1003 06:29:10.392385 25442 net.cpp:513] out5a/relu -> out5a (in-place)
I1003 06:29:10.392388 25442 net.cpp:245] Setting up out5a/relu
I1003 06:29:10.392390 25442 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I1003 06:29:10.392392 25442 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1003 06:29:10.392395 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.392422 25442 net.cpp:184] Created Layer out5a_up2 (40)
I1003 06:29:10.392426 25442 net.cpp:561] out5a_up2 <- out5a
I1003 06:29:10.392429 25442 net.cpp:530] out5a_up2 -> out5a_up2
I1003 06:29:10.392570 25442 net.cpp:245] Setting up out5a_up2
I1003 06:29:10.392575 25442 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I1003 06:29:10.392578 25442 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1003 06:29:10.392581 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.392586 25442 net.cpp:184] Created Layer out3a (41)
I1003 06:29:10.392590 25442 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:29:10.392592 25442 net.cpp:530] out3a -> out3a
I1003 06:29:10.393510 25442 net.cpp:245] Setting up out3a
I1003 06:29:10.393517 25442 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I1003 06:29:10.393522 25442 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1003 06:29:10.393524 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.393528 25442 net.cpp:184] Created Layer out3a/bn (42)
I1003 06:29:10.393532 25442 net.cpp:561] out3a/bn <- out3a
I1003 06:29:10.393533 25442 net.cpp:513] out3a/bn -> out3a (in-place)
I1003 06:29:10.393942 25442 net.cpp:245] Setting up out3a/bn
I1003 06:29:10.393949 25442 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I1003 06:29:10.393955 25442 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1003 06:29:10.393959 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.393965 25442 net.cpp:184] Created Layer out3a/relu (43)
I1003 06:29:10.393970 25442 net.cpp:561] out3a/relu <- out3a
I1003 06:29:10.393972 25442 net.cpp:513] out3a/relu -> out3a (in-place)
I1003 06:29:10.393977 25442 net.cpp:245] Setting up out3a/relu
I1003 06:29:10.393981 25442 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I1003 06:29:10.393985 25442 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1003 06:29:10.393988 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.393997 25442 net.cpp:184] Created Layer out3_out5_combined (44)
I1003 06:29:10.394001 25442 net.cpp:561] out3_out5_combined <- out5a_up2
I1003 06:29:10.394003 25442 net.cpp:561] out3_out5_combined <- out3a
I1003 06:29:10.394007 25442 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1003 06:29:10.394026 25442 net.cpp:245] Setting up out3_out5_combined
I1003 06:29:10.394031 25442 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I1003 06:29:10.394033 25442 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1003 06:29:10.394037 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.394047 25442 net.cpp:184] Created Layer ctx_conv1 (45)
I1003 06:29:10.394049 25442 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1003 06:29:10.394052 25442 net.cpp:530] ctx_conv1 -> ctx_conv1
I1003 06:29:10.394961 25442 net.cpp:245] Setting up ctx_conv1
I1003 06:29:10.394968 25442 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I1003 06:29:10.394973 25442 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1003 06:29:10.394978 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.394984 25442 net.cpp:184] Created Layer ctx_conv1/bn (46)
I1003 06:29:10.394987 25442 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1003 06:29:10.394990 25442 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1003 06:29:10.395401 25442 net.cpp:245] Setting up ctx_conv1/bn
I1003 06:29:10.395408 25442 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I1003 06:29:10.395414 25442 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1003 06:29:10.395427 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.395431 25442 net.cpp:184] Created Layer ctx_conv1/relu (47)
I1003 06:29:10.395436 25442 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1003 06:29:10.395439 25442 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1003 06:29:10.395444 25442 net.cpp:245] Setting up ctx_conv1/relu
I1003 06:29:10.395448 25442 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I1003 06:29:10.395452 25442 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1003 06:29:10.395454 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.395464 25442 net.cpp:184] Created Layer ctx_conv2 (48)
I1003 06:29:10.395467 25442 net.cpp:561] ctx_conv2 <- ctx_conv1
I1003 06:29:10.395470 25442 net.cpp:530] ctx_conv2 -> ctx_conv2
I1003 06:29:10.396415 25442 net.cpp:245] Setting up ctx_conv2
I1003 06:29:10.396430 25442 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I1003 06:29:10.396436 25442 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1003 06:29:10.396441 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.396451 25442 net.cpp:184] Created Layer ctx_conv2/bn (49)
I1003 06:29:10.396456 25442 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1003 06:29:10.396459 25442 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1003 06:29:10.397140 25442 net.cpp:245] Setting up ctx_conv2/bn
I1003 06:29:10.397177 25442 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I1003 06:29:10.397194 25442 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1003 06:29:10.397202 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.397212 25442 net.cpp:184] Created Layer ctx_conv2/relu (50)
I1003 06:29:10.397219 25442 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1003 06:29:10.397225 25442 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1003 06:29:10.397233 25442 net.cpp:245] Setting up ctx_conv2/relu
I1003 06:29:10.397238 25442 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I1003 06:29:10.397243 25442 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1003 06:29:10.397248 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.397270 25442 net.cpp:184] Created Layer ctx_conv3 (51)
I1003 06:29:10.397274 25442 net.cpp:561] ctx_conv3 <- ctx_conv2
I1003 06:29:10.397277 25442 net.cpp:530] ctx_conv3 -> ctx_conv3
I1003 06:29:10.398241 25442 net.cpp:245] Setting up ctx_conv3
I1003 06:29:10.398252 25442 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I1003 06:29:10.398257 25442 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1003 06:29:10.398262 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.398270 25442 net.cpp:184] Created Layer ctx_conv3/bn (52)
I1003 06:29:10.398275 25442 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1003 06:29:10.398278 25442 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1003 06:29:10.398697 25442 net.cpp:245] Setting up ctx_conv3/bn
I1003 06:29:10.398705 25442 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I1003 06:29:10.398715 25442 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1003 06:29:10.398720 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.398725 25442 net.cpp:184] Created Layer ctx_conv3/relu (53)
I1003 06:29:10.398730 25442 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1003 06:29:10.398733 25442 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1003 06:29:10.398738 25442 net.cpp:245] Setting up ctx_conv3/relu
I1003 06:29:10.398743 25442 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I1003 06:29:10.398759 25442 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1003 06:29:10.398766 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.398774 25442 net.cpp:184] Created Layer ctx_conv4 (54)
I1003 06:29:10.398778 25442 net.cpp:561] ctx_conv4 <- ctx_conv3
I1003 06:29:10.398782 25442 net.cpp:530] ctx_conv4 -> ctx_conv4
I1003 06:29:10.399719 25442 net.cpp:245] Setting up ctx_conv4
I1003 06:29:10.399731 25442 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I1003 06:29:10.399739 25442 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1003 06:29:10.399744 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.399755 25442 net.cpp:184] Created Layer ctx_conv4/bn (55)
I1003 06:29:10.399758 25442 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1003 06:29:10.399763 25442 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1003 06:29:10.400226 25442 net.cpp:245] Setting up ctx_conv4/bn
I1003 06:29:10.400234 25442 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I1003 06:29:10.400244 25442 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1003 06:29:10.400249 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400254 25442 net.cpp:184] Created Layer ctx_conv4/relu (56)
I1003 06:29:10.400257 25442 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1003 06:29:10.400262 25442 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1003 06:29:10.400269 25442 net.cpp:245] Setting up ctx_conv4/relu
I1003 06:29:10.400274 25442 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I1003 06:29:10.400277 25442 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1003 06:29:10.400281 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400293 25442 net.cpp:184] Created Layer ctx_final (57)
I1003 06:29:10.400297 25442 net.cpp:561] ctx_final <- ctx_conv4
I1003 06:29:10.400300 25442 net.cpp:530] ctx_final -> ctx_final
I1003 06:29:10.400588 25442 net.cpp:245] Setting up ctx_final
I1003 06:29:10.400595 25442 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I1003 06:29:10.400602 25442 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1003 06:29:10.400606 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400611 25442 net.cpp:184] Created Layer ctx_final/relu (58)
I1003 06:29:10.400615 25442 net.cpp:561] ctx_final/relu <- ctx_final
I1003 06:29:10.400619 25442 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1003 06:29:10.400625 25442 net.cpp:245] Setting up ctx_final/relu
I1003 06:29:10.400630 25442 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I1003 06:29:10.400635 25442 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1003 06:29:10.400638 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400646 25442 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I1003 06:29:10.400650 25442 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1003 06:29:10.400655 25442 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1003 06:29:10.400777 25442 net.cpp:245] Setting up out_deconv_final_up2
I1003 06:29:10.400784 25442 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I1003 06:29:10.400789 25442 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1003 06:29:10.400792 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400799 25442 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I1003 06:29:10.400802 25442 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1003 06:29:10.400815 25442 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1003 06:29:10.400934 25442 net.cpp:245] Setting up out_deconv_final_up4
I1003 06:29:10.400940 25442 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I1003 06:29:10.400944 25442 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1003 06:29:10.400949 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.400955 25442 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I1003 06:29:10.400959 25442 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1003 06:29:10.400964 25442 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1003 06:29:10.401078 25442 net.cpp:245] Setting up out_deconv_final_up8
I1003 06:29:10.401084 25442 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I1003 06:29:10.401089 25442 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I1003 06:29:10.401094 25442 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:29:10.401110 25442 net.cpp:184] Created Layer argMaxOut (62)
I1003 06:29:10.401114 25442 net.cpp:561] argMaxOut <- out_deconv_final_up8
I1003 06:29:10.401118 25442 net.cpp:530] argMaxOut -> argMaxOut
I1003 06:29:10.401137 25442 net.cpp:245] Setting up argMaxOut
I1003 06:29:10.401142 25442 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I1003 06:29:10.401146 25442 net.cpp:325] argMaxOut does not need backward computation.
I1003 06:29:10.401149 25442 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I1003 06:29:10.401154 25442 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I1003 06:29:10.401156 25442 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I1003 06:29:10.401160 25442 net.cpp:325] ctx_final/relu does not need backward computation.
I1003 06:29:10.401163 25442 net.cpp:325] ctx_final does not need backward computation.
I1003 06:29:10.401170 25442 net.cpp:325] ctx_conv4/relu does not need backward computation.
I1003 06:29:10.401175 25442 net.cpp:325] ctx_conv4/bn does not need backward computation.
I1003 06:29:10.401178 25442 net.cpp:325] ctx_conv4 does not need backward computation.
I1003 06:29:10.401181 25442 net.cpp:325] ctx_conv3/relu does not need backward computation.
I1003 06:29:10.401185 25442 net.cpp:325] ctx_conv3/bn does not need backward computation.
I1003 06:29:10.401188 25442 net.cpp:325] ctx_conv3 does not need backward computation.
I1003 06:29:10.401192 25442 net.cpp:325] ctx_conv2/relu does not need backward computation.
I1003 06:29:10.401196 25442 net.cpp:325] ctx_conv2/bn does not need backward computation.
I1003 06:29:10.401199 25442 net.cpp:325] ctx_conv2 does not need backward computation.
I1003 06:29:10.401203 25442 net.cpp:325] ctx_conv1/relu does not need backward computation.
I1003 06:29:10.401207 25442 net.cpp:325] ctx_conv1/bn does not need backward computation.
I1003 06:29:10.401211 25442 net.cpp:325] ctx_conv1 does not need backward computation.
I1003 06:29:10.401214 25442 net.cpp:325] out3_out5_combined does not need backward computation.
I1003 06:29:10.401218 25442 net.cpp:325] out3a/relu does not need backward computation.
I1003 06:29:10.401222 25442 net.cpp:325] out3a/bn does not need backward computation.
I1003 06:29:10.401226 25442 net.cpp:325] out3a does not need backward computation.
I1003 06:29:10.401229 25442 net.cpp:325] out5a_up2 does not need backward computation.
I1003 06:29:10.401233 25442 net.cpp:325] out5a/relu does not need backward computation.
I1003 06:29:10.401237 25442 net.cpp:325] out5a/bn does not need backward computation.
I1003 06:29:10.401240 25442 net.cpp:325] out5a does not need backward computation.
I1003 06:29:10.401244 25442 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I1003 06:29:10.401247 25442 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I1003 06:29:10.401257 25442 net.cpp:325] res5a_branch2b does not need backward computation.
I1003 06:29:10.401262 25442 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I1003 06:29:10.401265 25442 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I1003 06:29:10.401268 25442 net.cpp:325] res5a_branch2a does not need backward computation.
I1003 06:29:10.401273 25442 net.cpp:325] pool4 does not need backward computation.
I1003 06:29:10.401276 25442 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I1003 06:29:10.401280 25442 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I1003 06:29:10.401283 25442 net.cpp:325] res4a_branch2b does not need backward computation.
I1003 06:29:10.401288 25442 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I1003 06:29:10.401291 25442 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I1003 06:29:10.401295 25442 net.cpp:325] res4a_branch2a does not need backward computation.
I1003 06:29:10.401299 25442 net.cpp:325] pool3 does not need backward computation.
I1003 06:29:10.401304 25442 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I1003 06:29:10.401309 25442 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I1003 06:29:10.401311 25442 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I1003 06:29:10.401315 25442 net.cpp:325] res3a_branch2b does not need backward computation.
I1003 06:29:10.401319 25442 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I1003 06:29:10.401324 25442 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I1003 06:29:10.401326 25442 net.cpp:325] res3a_branch2a does not need backward computation.
I1003 06:29:10.401331 25442 net.cpp:325] pool2 does not need backward computation.
I1003 06:29:10.401335 25442 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I1003 06:29:10.401340 25442 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I1003 06:29:10.401345 25442 net.cpp:325] res2a_branch2b does not need backward computation.
I1003 06:29:10.401347 25442 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I1003 06:29:10.401351 25442 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I1003 06:29:10.401355 25442 net.cpp:325] res2a_branch2a does not need backward computation.
I1003 06:29:10.401360 25442 net.cpp:325] pool1 does not need backward computation.
I1003 06:29:10.401362 25442 net.cpp:325] conv1b/relu does not need backward computation.
I1003 06:29:10.401367 25442 net.cpp:325] conv1b/bn does not need backward computation.
I1003 06:29:10.401371 25442 net.cpp:325] conv1b does not need backward computation.
I1003 06:29:10.401376 25442 net.cpp:325] conv1a/relu does not need backward computation.
I1003 06:29:10.401381 25442 net.cpp:325] conv1a/bn does not need backward computation.
I1003 06:29:10.401384 25442 net.cpp:325] conv1a does not need backward computation.
I1003 06:29:10.401388 25442 net.cpp:325] data/bias does not need backward computation.
I1003 06:29:10.401392 25442 net.cpp:325] input does not need backward computation.
I1003 06:29:10.401396 25442 net.cpp:367] This network produces output argMaxOut
I1003 06:29:10.401444 25442 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I1003 06:29:10.401448 25442 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I1003 06:29:10.401451 25442 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I1003 06:29:10.401454 25442 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1003 06:29:10.401458 25442 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I1003 06:29:10.401460 25442 net.cpp:407] Network initialization done.
I1003 06:29:10.406527 25442 net.cpp:1078] Ignoring source layer data
I1003 06:29:10.406544 25442 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1003 06:29:10.406587 25442 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1003 06:29:10.406612 25442 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.406766 25442 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1003 06:29:10.406771 25442 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1003 06:29:10.406783 25442 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1003 06:29:10.406880 25442 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1003 06:29:10.406885 25442 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1003 06:29:10.406888 25442 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1003 06:29:10.406908 25442 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407006 25442 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1003 06:29:10.407011 25442 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1003 06:29:10.407028 25442 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407119 25442 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1003 06:29:10.407125 25442 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1003 06:29:10.407129 25442 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1003 06:29:10.407184 25442 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407275 25442 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1003 06:29:10.407280 25442 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1003 06:29:10.407306 25442 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407387 25442 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1003 06:29:10.407392 25442 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1003 06:29:10.407395 25442 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1003 06:29:10.407398 25442 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1003 06:29:10.407526 25442 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407610 25442 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1003 06:29:10.407615 25442 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1003 06:29:10.407691 25442 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:29:10.407773 25442 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1003 06:29:10.407778 25442 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1003 06:29:10.407781 25442 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1003 06:29:10.408223 25442 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.408310 25442 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1003 06:29:10.408315 25442 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1003 06:29:10.408510 25442 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:29:10.408591 25442 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1003 06:29:10.408596 25442 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1003 06:29:10.408654 25442 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.408748 25442 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1003 06:29:10.408754 25442 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1003 06:29:10.408762 25442 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1003 06:29:10.408785 25442 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1003 06:29:10.408884 25442 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1003 06:29:10.408888 25442 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1003 06:29:10.408891 25442 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1003 06:29:10.408915 25442 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1003 06:29:10.409003 25442 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1003 06:29:10.409008 25442 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1003 06:29:10.409032 25442 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1003 06:29:10.409123 25442 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1003 06:29:10.409128 25442 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1003 06:29:10.409152 25442 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1003 06:29:10.409243 25442 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1003 06:29:10.409248 25442 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1003 06:29:10.409272 25442 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1003 06:29:10.409361 25442 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1003 06:29:10.409366 25442 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1003 06:29:10.409379 25442 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1003 06:29:10.409384 25442 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1003 06:29:10.409392 25442 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1003 06:29:10.409401 25442 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1003 06:29:10.409410 25442 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I1003 06:29:11.219082 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.344117 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.361312 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.400264 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.413020 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.419920 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.431831 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.436482 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.452652 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.460629 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
I1003 06:29:11.487098 25442 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.89G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.973396283452, mean_iou=0.817398199964, iou=[ 0.95513862  0.959275    0.73945451  0.53148659  0.90163628]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.973585125888, mean_iou=0.83196544188, iou=[ 0.95589329  0.9638328   0.75919098  0.58430251  0.89660763]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.971288637565, mean_iou=0.829245351987, iou=[ 0.95202758  0.95990761  0.75064106  0.59186949  0.89178102]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.967864035033, mean_iou=0.835291766731, iou=[ 0.94709043  0.95450533  0.77221883  0.61390123  0.88874301]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.968027182639, mean_iou=0.833795268782, iou=[ 0.94770348  0.95392943  0.76969732  0.61133507  0.88631104]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.968485557138, mean_iou=0.831087721463, iou=[ 0.94826556  0.95552463  0.75756462  0.59948498  0.89459881]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.969832839474, mean_iou=0.832059929494, iou=[ 0.95060584  0.95753802  0.75662354  0.59471089  0.90082136]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.969411959571, mean_iou=0.83491609743, iou=[ 0.94987628  0.9580552   0.76714937  0.59896513  0.9005345 ]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.970150409569, mean_iou=0.834465143951, iou=[ 0.95110474  0.95998818  0.76341826  0.5957999   0.90201464]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.969225841726, mean_iou=0.831858592919, iou=[ 0.94952142  0.95792185  0.76169693  0.59031701  0.89983576]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.969739089345, mean_iou=0.830950318566, iou=[ 0.9503598   0.95893864  0.75644192  0.58881585  0.90019538]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.964841855377, mean_iou=0.825932122752, iou=[ 0.94289253  0.94398104  0.7520109   0.59548637  0.89528978]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.956168238469, mean_iou=0.816998156589, iou=[ 0.92957069  0.9172696   0.74724919  0.59463931  0.896262  ]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.957902648713, mean_iou=0.820044677999, iou=[ 0.93223566  0.92137175  0.74768435  0.60061168  0.89831994]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.958961919491, mean_iou=0.823598866072, iou=[ 0.93395147  0.92405548  0.75746034  0.60485055  0.89767649]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.960142098296, mean_iou=0.826022466679, iou=[ 0.93576641  0.92724662  0.75809331  0.60917863  0.89982737]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.961443519812, mean_iou=0.828578284213, iou=[ 0.93770819  0.92998492  0.75834121  0.61192931  0.9049278 ]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.962788148941, mean_iou=0.82973453113, iou=[ 0.93985249  0.93293353  0.75777321  0.61163627  0.90647717]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.95986328696, mean_iou=0.828378523906, iou=[ 0.93510381  0.92448974  0.76374348  0.61151629  0.9070393 ]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.960473059137, mean_iou=0.830152773545, iou=[ 0.93611064  0.9258019   0.76407736  0.61692592  0.90784806]
-------------------------------------------------------------
Final: pixel_accuracy=0.960473059137, mean_iou=0.830152773545, iou=[ 0.93611064  0.9258019   0.76407736  0.61692592  0.90784806]
-------------------------------------------------------------
sparse eval.
I1003 06:31:46.117113 28747 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:31:46.117805 28747 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:31:46.118376 28747 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1003 06:31:46.118930 28747 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I1003 06:31:46.120589 28747 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/test_quantize/deploy.prototxt
I1003 06:31:46.120601 28747 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W1003 06:31:46.120604 28747 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I1003 06:31:46.120872 28747 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
quantize: true
I1003 06:31:46.120990 28747 net.cpp:104] Using FLOAT as default forward math type
I1003 06:31:46.120993 28747 net.cpp:110] Using FLOAT as default backward math type
I1003 06:31:46.120995 28747 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I1003 06:31:46.120998 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.121008 28747 net.cpp:184] Created Layer input (0)
I1003 06:31:46.121011 28747 net.cpp:530] input -> data
I1003 06:31:46.121642 28747 net.cpp:245] Setting up input
I1003 06:31:46.121652 28747 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I1003 06:31:46.121664 28747 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1003 06:31:46.121667 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.121675 28747 net.cpp:184] Created Layer data/bias (1)
I1003 06:31:46.121677 28747 net.cpp:561] data/bias <- data
I1003 06:31:46.121680 28747 net.cpp:530] data/bias -> data/bias
I1003 06:31:46.126031 28747 net.cpp:245] Setting up data/bias
I1003 06:31:46.126050 28747 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I1003 06:31:46.126058 28747 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1003 06:31:46.126063 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.126082 28747 net.cpp:184] Created Layer conv1a (2)
I1003 06:31:46.126086 28747 net.cpp:561] conv1a <- data/bias
I1003 06:31:46.126090 28747 net.cpp:530] conv1a -> conv1a
I1003 06:31:46.502243 28747 net.cpp:245] Setting up conv1a
I1003 06:31:46.502266 28747 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I1003 06:31:46.502277 28747 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1003 06:31:46.502281 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.502291 28747 net.cpp:184] Created Layer conv1a/bn (3)
I1003 06:31:46.502295 28747 net.cpp:561] conv1a/bn <- conv1a
I1003 06:31:46.502297 28747 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1003 06:31:46.503114 28747 net.cpp:245] Setting up conv1a/bn
I1003 06:31:46.503125 28747 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I1003 06:31:46.503134 28747 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1003 06:31:46.503136 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.503141 28747 net.cpp:184] Created Layer conv1a/relu (4)
I1003 06:31:46.503144 28747 net.cpp:561] conv1a/relu <- conv1a
I1003 06:31:46.503146 28747 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1003 06:31:46.503157 28747 net.cpp:245] Setting up conv1a/relu
I1003 06:31:46.503161 28747 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I1003 06:31:46.503163 28747 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1003 06:31:46.503165 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.503176 28747 net.cpp:184] Created Layer conv1b (5)
I1003 06:31:46.503180 28747 net.cpp:561] conv1b <- conv1a
I1003 06:31:46.503181 28747 net.cpp:530] conv1b -> conv1b
I1003 06:31:46.504711 28747 net.cpp:245] Setting up conv1b
I1003 06:31:46.504724 28747 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I1003 06:31:46.504730 28747 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1003 06:31:46.504734 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.504740 28747 net.cpp:184] Created Layer conv1b/bn (6)
I1003 06:31:46.504743 28747 net.cpp:561] conv1b/bn <- conv1b
I1003 06:31:46.504745 28747 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1003 06:31:46.505663 28747 net.cpp:245] Setting up conv1b/bn
I1003 06:31:46.505692 28747 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I1003 06:31:46.505703 28747 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1003 06:31:46.505707 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.505713 28747 net.cpp:184] Created Layer conv1b/relu (7)
I1003 06:31:46.505717 28747 net.cpp:561] conv1b/relu <- conv1b
I1003 06:31:46.505720 28747 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1003 06:31:46.505725 28747 net.cpp:245] Setting up conv1b/relu
I1003 06:31:46.505728 28747 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I1003 06:31:46.505729 28747 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1003 06:31:46.505748 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.505753 28747 net.cpp:184] Created Layer pool1 (8)
I1003 06:31:46.505755 28747 net.cpp:561] pool1 <- conv1b
I1003 06:31:46.505758 28747 net.cpp:530] pool1 -> pool1
I1003 06:31:46.505798 28747 net.cpp:245] Setting up pool1
I1003 06:31:46.505802 28747 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I1003 06:31:46.505805 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1003 06:31:46.505807 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.505815 28747 net.cpp:184] Created Layer res2a_branch2a (9)
I1003 06:31:46.505817 28747 net.cpp:561] res2a_branch2a <- pool1
I1003 06:31:46.505820 28747 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1003 06:31:46.507030 28747 net.cpp:245] Setting up res2a_branch2a
I1003 06:31:46.507038 28747 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I1003 06:31:46.507045 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1003 06:31:46.507047 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.507055 28747 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I1003 06:31:46.507057 28747 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1003 06:31:46.507060 28747 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1003 06:31:46.507477 28747 net.cpp:245] Setting up res2a_branch2a/bn
I1003 06:31:46.507483 28747 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I1003 06:31:46.507488 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1003 06:31:46.507491 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.507494 28747 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I1003 06:31:46.507496 28747 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1003 06:31:46.507498 28747 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1003 06:31:46.507503 28747 net.cpp:245] Setting up res2a_branch2a/relu
I1003 06:31:46.507504 28747 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I1003 06:31:46.507506 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1003 06:31:46.507509 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.507517 28747 net.cpp:184] Created Layer res2a_branch2b (12)
I1003 06:31:46.507519 28747 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1003 06:31:46.507521 28747 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1003 06:31:46.508554 28747 net.cpp:245] Setting up res2a_branch2b
I1003 06:31:46.508566 28747 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I1003 06:31:46.508571 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1003 06:31:46.508574 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.508581 28747 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I1003 06:31:46.508584 28747 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1003 06:31:46.508587 28747 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1003 06:31:46.509398 28747 net.cpp:245] Setting up res2a_branch2b/bn
I1003 06:31:46.509412 28747 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I1003 06:31:46.509419 28747 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1003 06:31:46.509423 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.509428 28747 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I1003 06:31:46.509430 28747 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1003 06:31:46.509434 28747 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1003 06:31:46.509449 28747 net.cpp:245] Setting up res2a_branch2b/relu
I1003 06:31:46.509451 28747 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I1003 06:31:46.509454 28747 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1003 06:31:46.509457 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.509461 28747 net.cpp:184] Created Layer pool2 (15)
I1003 06:31:46.509464 28747 net.cpp:561] pool2 <- res2a_branch2b
I1003 06:31:46.509466 28747 net.cpp:530] pool2 -> pool2
I1003 06:31:46.509503 28747 net.cpp:245] Setting up pool2
I1003 06:31:46.509508 28747 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I1003 06:31:46.509510 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1003 06:31:46.509512 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.509523 28747 net.cpp:184] Created Layer res3a_branch2a (16)
I1003 06:31:46.509526 28747 net.cpp:561] res3a_branch2a <- pool2
I1003 06:31:46.509529 28747 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1003 06:31:46.512406 28747 net.cpp:245] Setting up res3a_branch2a
I1003 06:31:46.512428 28747 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I1003 06:31:46.512435 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1003 06:31:46.512439 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.512446 28747 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I1003 06:31:46.512449 28747 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1003 06:31:46.512454 28747 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1003 06:31:46.512872 28747 net.cpp:245] Setting up res3a_branch2a/bn
I1003 06:31:46.512881 28747 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I1003 06:31:46.512888 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1003 06:31:46.512892 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.512895 28747 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I1003 06:31:46.512897 28747 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1003 06:31:46.512899 28747 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1003 06:31:46.512903 28747 net.cpp:245] Setting up res3a_branch2a/relu
I1003 06:31:46.512907 28747 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I1003 06:31:46.512909 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1003 06:31:46.512912 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.512922 28747 net.cpp:184] Created Layer res3a_branch2b (19)
I1003 06:31:46.512924 28747 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1003 06:31:46.512926 28747 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1003 06:31:46.513968 28747 net.cpp:245] Setting up res3a_branch2b
I1003 06:31:46.513988 28747 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I1003 06:31:46.513994 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1003 06:31:46.513998 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.514008 28747 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I1003 06:31:46.514010 28747 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1003 06:31:46.514014 28747 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1003 06:31:46.514462 28747 net.cpp:245] Setting up res3a_branch2b/bn
I1003 06:31:46.514472 28747 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I1003 06:31:46.514478 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1003 06:31:46.514482 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.514494 28747 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I1003 06:31:46.514497 28747 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1003 06:31:46.514500 28747 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1003 06:31:46.514505 28747 net.cpp:245] Setting up res3a_branch2b/relu
I1003 06:31:46.514508 28747 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I1003 06:31:46.514509 28747 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1003 06:31:46.514513 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.514518 28747 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1003 06:31:46.514523 28747 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1003 06:31:46.514524 28747 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:31:46.514528 28747 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:31:46.514554 28747 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1003 06:31:46.514557 28747 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:31:46.514560 28747 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I1003 06:31:46.514562 28747 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1003 06:31:46.514564 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.514569 28747 net.cpp:184] Created Layer pool3 (23)
I1003 06:31:46.514571 28747 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1003 06:31:46.514574 28747 net.cpp:530] pool3 -> pool3
I1003 06:31:46.514602 28747 net.cpp:245] Setting up pool3
I1003 06:31:46.514606 28747 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I1003 06:31:46.514608 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1003 06:31:46.514611 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.514619 28747 net.cpp:184] Created Layer res4a_branch2a (24)
I1003 06:31:46.514621 28747 net.cpp:561] res4a_branch2a <- pool3
I1003 06:31:46.514624 28747 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1003 06:31:46.520709 28747 net.cpp:245] Setting up res4a_branch2a
I1003 06:31:46.520721 28747 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I1003 06:31:46.520726 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1003 06:31:46.520730 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.520735 28747 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I1003 06:31:46.520738 28747 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1003 06:31:46.520741 28747 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1003 06:31:46.521137 28747 net.cpp:245] Setting up res4a_branch2a/bn
I1003 06:31:46.521143 28747 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I1003 06:31:46.521149 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1003 06:31:46.521152 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.521154 28747 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I1003 06:31:46.521157 28747 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1003 06:31:46.521159 28747 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1003 06:31:46.521162 28747 net.cpp:245] Setting up res4a_branch2a/relu
I1003 06:31:46.521165 28747 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I1003 06:31:46.521176 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1003 06:31:46.521178 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.521188 28747 net.cpp:184] Created Layer res4a_branch2b (27)
I1003 06:31:46.521190 28747 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1003 06:31:46.521194 28747 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1003 06:31:46.524288 28747 net.cpp:245] Setting up res4a_branch2b
I1003 06:31:46.524296 28747 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I1003 06:31:46.524300 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1003 06:31:46.524302 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.524307 28747 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I1003 06:31:46.524308 28747 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1003 06:31:46.524312 28747 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1003 06:31:46.524694 28747 net.cpp:245] Setting up res4a_branch2b/bn
I1003 06:31:46.524700 28747 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I1003 06:31:46.524705 28747 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1003 06:31:46.524708 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.524710 28747 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I1003 06:31:46.524713 28747 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1003 06:31:46.524715 28747 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1003 06:31:46.524718 28747 net.cpp:245] Setting up res4a_branch2b/relu
I1003 06:31:46.524720 28747 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I1003 06:31:46.524722 28747 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1003 06:31:46.524724 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.524729 28747 net.cpp:184] Created Layer pool4 (30)
I1003 06:31:46.524730 28747 net.cpp:561] pool4 <- res4a_branch2b
I1003 06:31:46.524732 28747 net.cpp:530] pool4 -> pool4
I1003 06:31:46.524763 28747 net.cpp:245] Setting up pool4
I1003 06:31:46.524767 28747 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I1003 06:31:46.524770 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1003 06:31:46.524771 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.524777 28747 net.cpp:184] Created Layer res5a_branch2a (31)
I1003 06:31:46.524780 28747 net.cpp:561] res5a_branch2a <- pool4
I1003 06:31:46.524782 28747 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1003 06:31:46.550137 28747 net.cpp:245] Setting up res5a_branch2a
I1003 06:31:46.550158 28747 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I1003 06:31:46.550165 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1003 06:31:46.550169 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.550179 28747 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I1003 06:31:46.550181 28747 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1003 06:31:46.550184 28747 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1003 06:31:46.550601 28747 net.cpp:245] Setting up res5a_branch2a/bn
I1003 06:31:46.550609 28747 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I1003 06:31:46.550616 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1003 06:31:46.550618 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.550621 28747 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I1003 06:31:46.550623 28747 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1003 06:31:46.550637 28747 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1003 06:31:46.550642 28747 net.cpp:245] Setting up res5a_branch2a/relu
I1003 06:31:46.550643 28747 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I1003 06:31:46.550645 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1003 06:31:46.550648 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.550658 28747 net.cpp:184] Created Layer res5a_branch2b (34)
I1003 06:31:46.550662 28747 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1003 06:31:46.550663 28747 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1003 06:31:46.563076 28747 net.cpp:245] Setting up res5a_branch2b
I1003 06:31:46.563087 28747 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I1003 06:31:46.563094 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1003 06:31:46.563097 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.563102 28747 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I1003 06:31:46.563104 28747 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1003 06:31:46.563107 28747 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1003 06:31:46.563504 28747 net.cpp:245] Setting up res5a_branch2b/bn
I1003 06:31:46.563513 28747 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I1003 06:31:46.563519 28747 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1003 06:31:46.563520 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.563524 28747 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I1003 06:31:46.563525 28747 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1003 06:31:46.563529 28747 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1003 06:31:46.563531 28747 net.cpp:245] Setting up res5a_branch2b/relu
I1003 06:31:46.563534 28747 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I1003 06:31:46.563536 28747 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1003 06:31:46.563539 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.563545 28747 net.cpp:184] Created Layer out5a (37)
I1003 06:31:46.563547 28747 net.cpp:561] out5a <- res5a_branch2b
I1003 06:31:46.563550 28747 net.cpp:530] out5a -> out5a
I1003 06:31:46.567560 28747 net.cpp:245] Setting up out5a
I1003 06:31:46.567584 28747 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I1003 06:31:46.567589 28747 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1003 06:31:46.567595 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.567601 28747 net.cpp:184] Created Layer out5a/bn (38)
I1003 06:31:46.567605 28747 net.cpp:561] out5a/bn <- out5a
I1003 06:31:46.567607 28747 net.cpp:513] out5a/bn -> out5a (in-place)
I1003 06:31:46.568051 28747 net.cpp:245] Setting up out5a/bn
I1003 06:31:46.568058 28747 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I1003 06:31:46.568064 28747 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1003 06:31:46.568068 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.568070 28747 net.cpp:184] Created Layer out5a/relu (39)
I1003 06:31:46.568073 28747 net.cpp:561] out5a/relu <- out5a
I1003 06:31:46.568074 28747 net.cpp:513] out5a/relu -> out5a (in-place)
I1003 06:31:46.568078 28747 net.cpp:245] Setting up out5a/relu
I1003 06:31:46.568081 28747 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I1003 06:31:46.568084 28747 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1003 06:31:46.568085 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.568109 28747 net.cpp:184] Created Layer out5a_up2 (40)
I1003 06:31:46.568112 28747 net.cpp:561] out5a_up2 <- out5a
I1003 06:31:46.568115 28747 net.cpp:530] out5a_up2 -> out5a_up2
I1003 06:31:46.568264 28747 net.cpp:245] Setting up out5a_up2
I1003 06:31:46.568269 28747 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I1003 06:31:46.568272 28747 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1003 06:31:46.568275 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.568281 28747 net.cpp:184] Created Layer out3a (41)
I1003 06:31:46.568284 28747 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1003 06:31:46.568287 28747 net.cpp:530] out3a -> out3a
I1003 06:31:46.569211 28747 net.cpp:245] Setting up out3a
I1003 06:31:46.569217 28747 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I1003 06:31:46.569221 28747 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1003 06:31:46.569224 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.569228 28747 net.cpp:184] Created Layer out3a/bn (42)
I1003 06:31:46.569231 28747 net.cpp:561] out3a/bn <- out3a
I1003 06:31:46.569233 28747 net.cpp:513] out3a/bn -> out3a (in-place)
I1003 06:31:46.569648 28747 net.cpp:245] Setting up out3a/bn
I1003 06:31:46.569655 28747 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I1003 06:31:46.569661 28747 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1003 06:31:46.569664 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.569666 28747 net.cpp:184] Created Layer out3a/relu (43)
I1003 06:31:46.569669 28747 net.cpp:561] out3a/relu <- out3a
I1003 06:31:46.569670 28747 net.cpp:513] out3a/relu -> out3a (in-place)
I1003 06:31:46.569674 28747 net.cpp:245] Setting up out3a/relu
I1003 06:31:46.569676 28747 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I1003 06:31:46.569679 28747 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1003 06:31:46.569680 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.569686 28747 net.cpp:184] Created Layer out3_out5_combined (44)
I1003 06:31:46.569689 28747 net.cpp:561] out3_out5_combined <- out5a_up2
I1003 06:31:46.569691 28747 net.cpp:561] out3_out5_combined <- out3a
I1003 06:31:46.569694 28747 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1003 06:31:46.569710 28747 net.cpp:245] Setting up out3_out5_combined
I1003 06:31:46.569713 28747 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I1003 06:31:46.569715 28747 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1003 06:31:46.569717 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.569725 28747 net.cpp:184] Created Layer ctx_conv1 (45)
I1003 06:31:46.569728 28747 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1003 06:31:46.569730 28747 net.cpp:530] ctx_conv1 -> ctx_conv1
I1003 06:31:46.570719 28747 net.cpp:245] Setting up ctx_conv1
I1003 06:31:46.570725 28747 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I1003 06:31:46.570730 28747 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1003 06:31:46.570732 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.570736 28747 net.cpp:184] Created Layer ctx_conv1/bn (46)
I1003 06:31:46.570739 28747 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1003 06:31:46.570741 28747 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1003 06:31:46.571158 28747 net.cpp:245] Setting up ctx_conv1/bn
I1003 06:31:46.571166 28747 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I1003 06:31:46.571171 28747 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1003 06:31:46.571179 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.571182 28747 net.cpp:184] Created Layer ctx_conv1/relu (47)
I1003 06:31:46.571184 28747 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1003 06:31:46.571187 28747 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1003 06:31:46.571190 28747 net.cpp:245] Setting up ctx_conv1/relu
I1003 06:31:46.571192 28747 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I1003 06:31:46.571195 28747 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1003 06:31:46.571197 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.571209 28747 net.cpp:184] Created Layer ctx_conv2 (48)
I1003 06:31:46.571213 28747 net.cpp:561] ctx_conv2 <- ctx_conv1
I1003 06:31:46.571214 28747 net.cpp:530] ctx_conv2 -> ctx_conv2
I1003 06:31:46.572114 28747 net.cpp:245] Setting up ctx_conv2
I1003 06:31:46.572121 28747 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I1003 06:31:46.572124 28747 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1003 06:31:46.572127 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.572131 28747 net.cpp:184] Created Layer ctx_conv2/bn (49)
I1003 06:31:46.572134 28747 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1003 06:31:46.572135 28747 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1003 06:31:46.572563 28747 net.cpp:245] Setting up ctx_conv2/bn
I1003 06:31:46.572571 28747 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I1003 06:31:46.572577 28747 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1003 06:31:46.572579 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.572582 28747 net.cpp:184] Created Layer ctx_conv2/relu (50)
I1003 06:31:46.572584 28747 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1003 06:31:46.572587 28747 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1003 06:31:46.572590 28747 net.cpp:245] Setting up ctx_conv2/relu
I1003 06:31:46.572592 28747 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I1003 06:31:46.572594 28747 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1003 06:31:46.572597 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.572605 28747 net.cpp:184] Created Layer ctx_conv3 (51)
I1003 06:31:46.572608 28747 net.cpp:561] ctx_conv3 <- ctx_conv2
I1003 06:31:46.572610 28747 net.cpp:530] ctx_conv3 -> ctx_conv3
I1003 06:31:46.573508 28747 net.cpp:245] Setting up ctx_conv3
I1003 06:31:46.573514 28747 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I1003 06:31:46.573518 28747 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1003 06:31:46.573520 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.573524 28747 net.cpp:184] Created Layer ctx_conv3/bn (52)
I1003 06:31:46.573526 28747 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1003 06:31:46.573529 28747 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1003 06:31:46.573937 28747 net.cpp:245] Setting up ctx_conv3/bn
I1003 06:31:46.573943 28747 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I1003 06:31:46.573949 28747 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1003 06:31:46.573951 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.573954 28747 net.cpp:184] Created Layer ctx_conv3/relu (53)
I1003 06:31:46.573956 28747 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1003 06:31:46.573958 28747 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1003 06:31:46.573961 28747 net.cpp:245] Setting up ctx_conv3/relu
I1003 06:31:46.573969 28747 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I1003 06:31:46.573972 28747 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1003 06:31:46.573974 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.573979 28747 net.cpp:184] Created Layer ctx_conv4 (54)
I1003 06:31:46.573982 28747 net.cpp:561] ctx_conv4 <- ctx_conv3
I1003 06:31:46.573983 28747 net.cpp:530] ctx_conv4 -> ctx_conv4
I1003 06:31:46.574878 28747 net.cpp:245] Setting up ctx_conv4
I1003 06:31:46.574884 28747 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I1003 06:31:46.574888 28747 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1003 06:31:46.574890 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.574894 28747 net.cpp:184] Created Layer ctx_conv4/bn (55)
I1003 06:31:46.574897 28747 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1003 06:31:46.574898 28747 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1003 06:31:46.575314 28747 net.cpp:245] Setting up ctx_conv4/bn
I1003 06:31:46.575321 28747 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I1003 06:31:46.575326 28747 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1003 06:31:46.575328 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575331 28747 net.cpp:184] Created Layer ctx_conv4/relu (56)
I1003 06:31:46.575333 28747 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1003 06:31:46.575335 28747 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1003 06:31:46.575338 28747 net.cpp:245] Setting up ctx_conv4/relu
I1003 06:31:46.575340 28747 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I1003 06:31:46.575342 28747 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1003 06:31:46.575345 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575359 28747 net.cpp:184] Created Layer ctx_final (57)
I1003 06:31:46.575362 28747 net.cpp:561] ctx_final <- ctx_conv4
I1003 06:31:46.575364 28747 net.cpp:530] ctx_final -> ctx_final
I1003 06:31:46.575633 28747 net.cpp:245] Setting up ctx_final
I1003 06:31:46.575639 28747 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I1003 06:31:46.575644 28747 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1003 06:31:46.575645 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575649 28747 net.cpp:184] Created Layer ctx_final/relu (58)
I1003 06:31:46.575650 28747 net.cpp:561] ctx_final/relu <- ctx_final
I1003 06:31:46.575654 28747 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1003 06:31:46.575656 28747 net.cpp:245] Setting up ctx_final/relu
I1003 06:31:46.575659 28747 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I1003 06:31:46.575660 28747 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1003 06:31:46.575662 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575666 28747 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I1003 06:31:46.575669 28747 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1003 06:31:46.575670 28747 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1003 06:31:46.575790 28747 net.cpp:245] Setting up out_deconv_final_up2
I1003 06:31:46.575795 28747 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I1003 06:31:46.575798 28747 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1003 06:31:46.575800 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575804 28747 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I1003 06:31:46.575806 28747 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1003 06:31:46.575814 28747 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1003 06:31:46.575930 28747 net.cpp:245] Setting up out_deconv_final_up4
I1003 06:31:46.575935 28747 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I1003 06:31:46.575938 28747 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1003 06:31:46.575940 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.575944 28747 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I1003 06:31:46.575947 28747 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1003 06:31:46.575948 28747 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1003 06:31:46.576071 28747 net.cpp:245] Setting up out_deconv_final_up8
I1003 06:31:46.576076 28747 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I1003 06:31:46.576079 28747 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I1003 06:31:46.576081 28747 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1003 06:31:46.576089 28747 net.cpp:184] Created Layer argMaxOut (62)
I1003 06:31:46.576092 28747 net.cpp:561] argMaxOut <- out_deconv_final_up8
I1003 06:31:46.576094 28747 net.cpp:530] argMaxOut -> argMaxOut
I1003 06:31:46.576108 28747 net.cpp:245] Setting up argMaxOut
I1003 06:31:46.576112 28747 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I1003 06:31:46.576114 28747 net.cpp:325] argMaxOut does not need backward computation.
I1003 06:31:46.576117 28747 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I1003 06:31:46.576118 28747 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I1003 06:31:46.576120 28747 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I1003 06:31:46.576122 28747 net.cpp:325] ctx_final/relu does not need backward computation.
I1003 06:31:46.576124 28747 net.cpp:325] ctx_final does not need backward computation.
I1003 06:31:46.576130 28747 net.cpp:325] ctx_conv4/relu does not need backward computation.
I1003 06:31:46.576133 28747 net.cpp:325] ctx_conv4/bn does not need backward computation.
I1003 06:31:46.576134 28747 net.cpp:325] ctx_conv4 does not need backward computation.
I1003 06:31:46.576136 28747 net.cpp:325] ctx_conv3/relu does not need backward computation.
I1003 06:31:46.576138 28747 net.cpp:325] ctx_conv3/bn does not need backward computation.
I1003 06:31:46.576139 28747 net.cpp:325] ctx_conv3 does not need backward computation.
I1003 06:31:46.576141 28747 net.cpp:325] ctx_conv2/relu does not need backward computation.
I1003 06:31:46.576143 28747 net.cpp:325] ctx_conv2/bn does not need backward computation.
I1003 06:31:46.576144 28747 net.cpp:325] ctx_conv2 does not need backward computation.
I1003 06:31:46.576146 28747 net.cpp:325] ctx_conv1/relu does not need backward computation.
I1003 06:31:46.576148 28747 net.cpp:325] ctx_conv1/bn does not need backward computation.
I1003 06:31:46.576150 28747 net.cpp:325] ctx_conv1 does not need backward computation.
I1003 06:31:46.576153 28747 net.cpp:325] out3_out5_combined does not need backward computation.
I1003 06:31:46.576154 28747 net.cpp:325] out3a/relu does not need backward computation.
I1003 06:31:46.576156 28747 net.cpp:325] out3a/bn does not need backward computation.
I1003 06:31:46.576159 28747 net.cpp:325] out3a does not need backward computation.
I1003 06:31:46.576159 28747 net.cpp:325] out5a_up2 does not need backward computation.
I1003 06:31:46.576162 28747 net.cpp:325] out5a/relu does not need backward computation.
I1003 06:31:46.576164 28747 net.cpp:325] out5a/bn does not need backward computation.
I1003 06:31:46.576165 28747 net.cpp:325] out5a does not need backward computation.
I1003 06:31:46.576167 28747 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I1003 06:31:46.576169 28747 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I1003 06:31:46.576179 28747 net.cpp:325] res5a_branch2b does not need backward computation.
I1003 06:31:46.576180 28747 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I1003 06:31:46.576182 28747 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I1003 06:31:46.576184 28747 net.cpp:325] res5a_branch2a does not need backward computation.
I1003 06:31:46.576187 28747 net.cpp:325] pool4 does not need backward computation.
I1003 06:31:46.576189 28747 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I1003 06:31:46.576191 28747 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I1003 06:31:46.576192 28747 net.cpp:325] res4a_branch2b does not need backward computation.
I1003 06:31:46.576195 28747 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I1003 06:31:46.576196 28747 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I1003 06:31:46.576200 28747 net.cpp:325] res4a_branch2a does not need backward computation.
I1003 06:31:46.576201 28747 net.cpp:325] pool3 does not need backward computation.
I1003 06:31:46.576203 28747 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I1003 06:31:46.576205 28747 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I1003 06:31:46.576207 28747 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I1003 06:31:46.576210 28747 net.cpp:325] res3a_branch2b does not need backward computation.
I1003 06:31:46.576211 28747 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I1003 06:31:46.576213 28747 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I1003 06:31:46.576215 28747 net.cpp:325] res3a_branch2a does not need backward computation.
I1003 06:31:46.576217 28747 net.cpp:325] pool2 does not need backward computation.
I1003 06:31:46.576220 28747 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I1003 06:31:46.576221 28747 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I1003 06:31:46.576223 28747 net.cpp:325] res2a_branch2b does not need backward computation.
I1003 06:31:46.576225 28747 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I1003 06:31:46.576227 28747 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I1003 06:31:46.576230 28747 net.cpp:325] res2a_branch2a does not need backward computation.
I1003 06:31:46.576231 28747 net.cpp:325] pool1 does not need backward computation.
I1003 06:31:46.576233 28747 net.cpp:325] conv1b/relu does not need backward computation.
I1003 06:31:46.576236 28747 net.cpp:325] conv1b/bn does not need backward computation.
I1003 06:31:46.576237 28747 net.cpp:325] conv1b does not need backward computation.
I1003 06:31:46.576239 28747 net.cpp:325] conv1a/relu does not need backward computation.
I1003 06:31:46.576241 28747 net.cpp:325] conv1a/bn does not need backward computation.
I1003 06:31:46.576243 28747 net.cpp:325] conv1a does not need backward computation.
I1003 06:31:46.576246 28747 net.cpp:325] data/bias does not need backward computation.
I1003 06:31:46.576247 28747 net.cpp:325] input does not need backward computation.
I1003 06:31:46.576249 28747 net.cpp:367] This network produces output argMaxOut
I1003 06:31:46.576283 28747 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I1003 06:31:46.576287 28747 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I1003 06:31:46.576289 28747 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I1003 06:31:46.576292 28747 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1003 06:31:46.576293 28747 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I1003 06:31:46.576295 28747 net.cpp:407] Network initialization done.
I1003 06:31:46.581187 28747 net.cpp:1078] Ignoring source layer data
I1003 06:31:46.581208 28747 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1003 06:31:46.581259 28747 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1003 06:31:46.581275 28747 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.581440 28747 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1003 06:31:46.581446 28747 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1003 06:31:46.581455 28747 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1003 06:31:46.581573 28747 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1003 06:31:46.581580 28747 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1003 06:31:46.581584 28747 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1003 06:31:46.581600 28747 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.581723 28747 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1003 06:31:46.581729 28747 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1003 06:31:46.581745 28747 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:31:46.581861 28747 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1003 06:31:46.581868 28747 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1003 06:31:46.581873 28747 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1003 06:31:46.581913 28747 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.582026 28747 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1003 06:31:46.582031 28747 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1003 06:31:46.582051 28747 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:31:46.582156 28747 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1003 06:31:46.582162 28747 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1003 06:31:46.582166 28747 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1003 06:31:46.582170 28747 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1003 06:31:46.582283 28747 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.582381 28747 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1003 06:31:46.582386 28747 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1003 06:31:46.582445 28747 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:31:46.582550 28747 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1003 06:31:46.582554 28747 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1003 06:31:46.582556 28747 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1003 06:31:46.582888 28747 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.582983 28747 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1003 06:31:46.582988 28747 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1003 06:31:46.583160 28747 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1003 06:31:46.583247 28747 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1003 06:31:46.583253 28747 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1003 06:31:46.583307 28747 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.583448 28747 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1003 06:31:46.583454 28747 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1003 06:31:46.583461 28747 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1003 06:31:46.583482 28747 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1003 06:31:46.583598 28747 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1003 06:31:46.583606 28747 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1003 06:31:46.583609 28747 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1003 06:31:46.583632 28747 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1003 06:31:46.583748 28747 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1003 06:31:46.583755 28747 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1003 06:31:46.583778 28747 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1003 06:31:46.583895 28747 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1003 06:31:46.583900 28747 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1003 06:31:46.583920 28747 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1003 06:31:46.584033 28747 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1003 06:31:46.584040 28747 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1003 06:31:46.584059 28747 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1003 06:31:46.584170 28747 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1003 06:31:46.584180 28747 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1003 06:31:46.584192 28747 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1003 06:31:46.584195 28747 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1003 06:31:46.584199 28747 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1003 06:31:46.584204 28747 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1003 06:31:46.584209 28747 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/test_quantize/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='./training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
I1003 06:31:47.132370 28747 net.cpp:1597] Adding quantization params at infer/iter index: 1
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I1003 06:31:47.502672 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.634364 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.654146 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.699836 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.714582 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.721870 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.734598 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.740983 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.758625 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.766510 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.87G, req 0.01G)	t: 0
I1003 06:31:47.794579 28747 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.87G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.972776062082, mean_iou=0.815782650192, iou=[ 0.95394704  0.95808159  0.73618616  0.52947617  0.9012223 ]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.973333044226, mean_iou=0.831163332093, iou=[ 0.95540199  0.96337208  0.75745681  0.58318995  0.89639583]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.970983135423, mean_iou=0.827808914091, iou=[ 0.95149548  0.95944725  0.74852632  0.58862452  0.890951  ]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.96785743497, mean_iou=0.834357164121, iou=[ 0.94708816  0.95473901  0.77073297  0.61088704  0.88833865]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.967904151544, mean_iou=0.832580886301, iou=[ 0.94750398  0.95390522  0.76827163  0.60767696  0.88554665]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.96833657436, mean_iou=0.829829238739, iou=[ 0.94804043  0.95547797  0.75605262  0.59609571  0.89347946]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.96966422115, mean_iou=0.83079275275, iou=[ 0.95036669  0.95739636  0.7550836   0.59135911  0.89975801]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.969195485195, mean_iou=0.833452540325, iou=[ 0.9495467   0.95782786  0.76568336  0.59477278  0.899432  ]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.969970240752, mean_iou=0.833124789781, iou=[ 0.95085401  0.95983295  0.76198612  0.59212815  0.90082271]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.96911903826, mean_iou=0.830735472467, iou=[ 0.94937953  0.95780091  0.76034437  0.58652778  0.89962477]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.969642800329, mean_iou=0.829900882143, iou=[ 0.95022688  0.95882396  0.75537517  0.58509839  0.89998001]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.96484873434, mean_iou=0.825098092074, iou=[ 0.94292634  0.94408808  0.75137396  0.59184247  0.89525961]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.956002105643, mean_iou=0.816089913298, iou=[ 0.92929492  0.91695124  0.74673349  0.59122797  0.89624194]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.957732737486, mean_iou=0.819063132874, iou=[ 0.93196474  0.92106041  0.74700882  0.59711544  0.89816626]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.958843562885, mean_iou=0.822727448309, iou=[ 0.93375971  0.92388137  0.75681719  0.60166305  0.89751592]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.9600193593, mean_iou=0.825161525869, iou=[ 0.93557151  0.92706144  0.75743297  0.6061218   0.89961992]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.961337773956, mean_iou=0.827758771029, iou=[ 0.93753984  0.9298369   0.75777518  0.60890785  0.90473409]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.962703824904, mean_iou=0.829013990084, iou=[ 0.93970455  0.93282813  0.75717794  0.60892467  0.90643467]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.959985873602, mean_iou=0.827853410855, iou=[ 0.9352854   0.92498252  0.76315638  0.6088581   0.90698467]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.960616500917, mean_iou=0.82967180905, iou=[ 0.93633176  0.92634658  0.76357447  0.61436403  0.9077422 ]
-------------------------------------------------------------
Final: pixel_accuracy=0.960616500917, mean_iou=0.82967180905, iou=[ 0.93633176  0.92634658  0.76357447  0.61436403  0.9077422 ]
-------------------------------------------------------------
sparse eval.
