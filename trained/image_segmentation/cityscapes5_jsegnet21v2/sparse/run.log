I1002 20:39:25.146265 31268 caffe.cpp:807] This is NVCaffe 0.16.4 started at Mon Oct  2 20:39:23 2017
I1002 20:39:25.146656 31268 caffe.cpp:810] CuDNN version: 7002
I1002 20:39:25.146661 31268 caffe.cpp:811] CuBLAS version: 8000
I1002 20:39:25.146663 31268 caffe.cpp:812] CUDA version: 8000
I1002 20:39:25.146667 31268 caffe.cpp:813] CUDA driver version: 8000
I1002 20:39:25.845526 31268 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1002 20:39:25.846076 31268 gpu_memory.cpp:161] Total memory: 8506769408, Free: 5760811008, dev_info[0]: total=8506769408 free=5760811008
I1002 20:39:25.846559 31268 gpu_memory.cpp:161] Total memory: 8508145664, Free: 5760811008, dev_info[1]: total=8508145664 free=5859508224
I1002 20:39:25.847043 31268 gpu_memory.cpp:161] Total memory: 8508145664, Free: 5760811008, dev_info[2]: total=8508145664 free=5859508224
I1002 20:39:25.847051 31268 caffe.cpp:214] Using GPUs 0, 1, 2
I1002 20:39:25.847378 31268 caffe.cpp:219] GPU 0: GeForce GTX 1080
I1002 20:39:25.847710 31268 caffe.cpp:219] GPU 1: GeForce GTX 1080
I1002 20:39:25.848213 31268 caffe.cpp:219] GPU 2: GeForce GTX 1080
I1002 20:39:25.848994 31268 solver.cpp:43] Solver data type: FLOAT
I1002 20:39:25.849063 31268 solver.cpp:46] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 60000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 30000
stepvalue: 45000
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 1000
sparsity_start_factor: 0.6
I1002 20:39:25.868530 31268 solver.cpp:78] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt
I1002 20:39:25.891201 31268 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I1002 20:39:25.891223 31268 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W1002 20:39:25.891278 31268 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I1002 20:39:25.892197 31268 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: true
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I1002 20:39:25.892419 31268 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:25.892426 31268 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:25.892431 31268 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I1002 20:39:25.892436 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:25.898900 31268 net.cpp:184] Created Layer data (0)
I1002 20:39:25.898916 31268 net.cpp:530] data -> data
I1002 20:39:25.898934 31268 net.cpp:530] data -> label
I1002 20:39:25.922089 31268 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:25.922124 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:25.993788 31342 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I1002 20:39:26.085970 31268 data_layer.cpp:187] [0] ReshapePrefetch 6, 3, 640, 640
I1002 20:39:26.086163 31268 data_layer.cpp:211] [0] Output data size: 6, 3, 640, 640
I1002 20:39:26.086186 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:26.086272 31268 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:26.086292 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:26.117909 31343 data_layer.cpp:101] [0] Parser threads: 1
I1002 20:39:26.117947 31343 data_layer.cpp:103] [0] Transformer threads: 1
I1002 20:39:26.155885 31344 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I1002 20:39:26.170016 31343 blocking_queue.cpp:40] Waiting for datum
I1002 20:39:26.237557 31268 data_layer.cpp:187] [0] ReshapePrefetch 6, 1, 640, 640
I1002 20:39:26.261099 31268 data_layer.cpp:211] [0] Output data size: 6, 1, 640, 640
I1002 20:39:26.261308 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:26.263867 31268 net.cpp:245] Setting up data
I1002 20:39:26.263886 31268 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I1002 20:39:26.263895 31268 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I1002 20:39:26.263902 31268 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1002 20:39:26.263907 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:26.264375 31268 net.cpp:184] Created Layer data/bias (1)
I1002 20:39:26.264401 31268 net.cpp:561] data/bias <- data
I1002 20:39:26.264426 31268 net.cpp:530] data/bias -> data/bias
I1002 20:39:26.264816 31346 data_layer.cpp:101] [0] Parser threads: 1
I1002 20:39:26.264829 31346 data_layer.cpp:103] [0] Transformer threads: 1
I1002 20:39:26.271458 31268 net.cpp:245] Setting up data/bias
I1002 20:39:26.271510 31268 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I1002 20:39:26.271545 31268 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1002 20:39:26.271559 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:26.271857 31268 net.cpp:184] Created Layer conv1a (2)
I1002 20:39:26.271867 31268 net.cpp:561] conv1a <- data/bias
I1002 20:39:26.271873 31268 net.cpp:530] conv1a -> conv1a
I1002 20:39:28.540146 31268 net.cpp:245] Setting up conv1a
I1002 20:39:28.540184 31268 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I1002 20:39:28.540200 31268 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1002 20:39:28.540207 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.540231 31268 net.cpp:184] Created Layer conv1a/bn (3)
I1002 20:39:28.540236 31268 net.cpp:561] conv1a/bn <- conv1a
I1002 20:39:28.540242 31268 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1002 20:39:28.565076 31268 net.cpp:245] Setting up conv1a/bn
I1002 20:39:28.565104 31268 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I1002 20:39:28.565119 31268 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1002 20:39:28.565125 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.565137 31268 net.cpp:184] Created Layer conv1a/relu (4)
I1002 20:39:28.565142 31268 net.cpp:561] conv1a/relu <- conv1a
I1002 20:39:28.565146 31268 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1002 20:39:28.565160 31268 net.cpp:245] Setting up conv1a/relu
I1002 20:39:28.565165 31268 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I1002 20:39:28.565168 31268 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1002 20:39:28.565171 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.565186 31268 net.cpp:184] Created Layer conv1b (5)
I1002 20:39:28.565191 31268 net.cpp:561] conv1b <- conv1a
I1002 20:39:28.565193 31268 net.cpp:530] conv1b -> conv1b
I1002 20:39:28.567095 31268 net.cpp:245] Setting up conv1b
I1002 20:39:28.567134 31268 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I1002 20:39:28.567158 31268 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1002 20:39:28.567168 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.567188 31268 net.cpp:184] Created Layer conv1b/bn (6)
I1002 20:39:28.567195 31268 net.cpp:561] conv1b/bn <- conv1b
I1002 20:39:28.567204 31268 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1002 20:39:28.568435 31268 net.cpp:245] Setting up conv1b/bn
I1002 20:39:28.568454 31268 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I1002 20:39:28.568465 31268 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1002 20:39:28.568471 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.568480 31268 net.cpp:184] Created Layer conv1b/relu (7)
I1002 20:39:28.568485 31268 net.cpp:561] conv1b/relu <- conv1b
I1002 20:39:28.568488 31268 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1002 20:39:28.568495 31268 net.cpp:245] Setting up conv1b/relu
I1002 20:39:28.568500 31268 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I1002 20:39:28.568502 31268 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1002 20:39:28.568506 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.568514 31268 net.cpp:184] Created Layer pool1 (8)
I1002 20:39:28.568517 31268 net.cpp:561] pool1 <- conv1b
I1002 20:39:28.568521 31268 net.cpp:530] pool1 -> pool1
I1002 20:39:28.581456 31268 net.cpp:245] Setting up pool1
I1002 20:39:28.581483 31268 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I1002 20:39:28.581488 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1002 20:39:28.581507 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.581523 31268 net.cpp:184] Created Layer res2a_branch2a (9)
I1002 20:39:28.581531 31268 net.cpp:561] res2a_branch2a <- pool1
I1002 20:39:28.581537 31268 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1002 20:39:28.583914 31268 net.cpp:245] Setting up res2a_branch2a
I1002 20:39:28.583933 31268 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I1002 20:39:28.583941 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:28.583945 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.583952 31268 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I1002 20:39:28.583956 31268 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1002 20:39:28.583959 31268 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1002 20:39:28.584659 31268 net.cpp:245] Setting up res2a_branch2a/bn
I1002 20:39:28.584669 31268 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I1002 20:39:28.584676 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1002 20:39:28.584679 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.584683 31268 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I1002 20:39:28.584686 31268 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1002 20:39:28.584688 31268 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1002 20:39:28.584692 31268 net.cpp:245] Setting up res2a_branch2a/relu
I1002 20:39:28.584694 31268 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I1002 20:39:28.584697 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1002 20:39:28.584699 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.584705 31268 net.cpp:184] Created Layer res2a_branch2b (12)
I1002 20:39:28.584707 31268 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1002 20:39:28.584710 31268 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1002 20:39:28.586277 31268 net.cpp:245] Setting up res2a_branch2b
I1002 20:39:28.586292 31268 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I1002 20:39:28.586297 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:28.586300 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.586308 31268 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I1002 20:39:28.586311 31268 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1002 20:39:28.586315 31268 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1002 20:39:28.586966 31268 net.cpp:245] Setting up res2a_branch2b/bn
I1002 20:39:28.586974 31268 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I1002 20:39:28.586980 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1002 20:39:28.586983 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.586987 31268 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I1002 20:39:28.586990 31268 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1002 20:39:28.586992 31268 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1002 20:39:28.586997 31268 net.cpp:245] Setting up res2a_branch2b/relu
I1002 20:39:28.586998 31268 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I1002 20:39:28.587000 31268 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1002 20:39:28.587002 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.587007 31268 net.cpp:184] Created Layer pool2 (15)
I1002 20:39:28.587020 31268 net.cpp:561] pool2 <- res2a_branch2b
I1002 20:39:28.587023 31268 net.cpp:530] pool2 -> pool2
I1002 20:39:28.587083 31268 net.cpp:245] Setting up pool2
I1002 20:39:28.587087 31268 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I1002 20:39:28.587090 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1002 20:39:28.587092 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.587102 31268 net.cpp:184] Created Layer res3a_branch2a (16)
I1002 20:39:28.587105 31268 net.cpp:561] res3a_branch2a <- pool2
I1002 20:39:28.587107 31268 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1002 20:39:28.588950 31268 net.cpp:245] Setting up res3a_branch2a
I1002 20:39:28.588960 31268 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I1002 20:39:28.588965 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:28.588968 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.588973 31268 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I1002 20:39:28.588977 31268 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1002 20:39:28.588979 31268 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1002 20:39:28.589637 31268 net.cpp:245] Setting up res3a_branch2a/bn
I1002 20:39:28.589646 31268 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I1002 20:39:28.589654 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1002 20:39:28.589658 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.589663 31268 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I1002 20:39:28.589664 31268 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1002 20:39:28.589666 31268 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1002 20:39:28.589670 31268 net.cpp:245] Setting up res3a_branch2a/relu
I1002 20:39:28.589673 31268 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I1002 20:39:28.589674 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1002 20:39:28.589678 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.589689 31268 net.cpp:184] Created Layer res3a_branch2b (19)
I1002 20:39:28.589690 31268 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1002 20:39:28.589692 31268 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1002 20:39:28.590780 31268 net.cpp:245] Setting up res3a_branch2b
I1002 20:39:28.590788 31268 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I1002 20:39:28.590792 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:28.590795 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.590801 31268 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I1002 20:39:28.590803 31268 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1002 20:39:28.590806 31268 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1002 20:39:28.591405 31268 net.cpp:245] Setting up res3a_branch2b/bn
I1002 20:39:28.591413 31268 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I1002 20:39:28.591418 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1002 20:39:28.591420 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.591424 31268 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I1002 20:39:28.591428 31268 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1002 20:39:28.591430 31268 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1002 20:39:28.591434 31268 net.cpp:245] Setting up res3a_branch2b/relu
I1002 20:39:28.591436 31268 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I1002 20:39:28.591449 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1002 20:39:28.591451 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.591457 31268 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1002 20:39:28.591459 31268 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1002 20:39:28.591461 31268 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1002 20:39:28.591464 31268 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1002 20:39:28.591511 31268 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1002 20:39:28.591516 31268 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I1002 20:39:28.591518 31268 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I1002 20:39:28.591521 31268 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1002 20:39:28.591522 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.591526 31268 net.cpp:184] Created Layer pool3 (23)
I1002 20:39:28.591527 31268 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1002 20:39:28.591531 31268 net.cpp:530] pool3 -> pool3
I1002 20:39:28.591594 31268 net.cpp:245] Setting up pool3
I1002 20:39:28.591603 31268 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I1002 20:39:28.591608 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1002 20:39:28.591610 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.591619 31268 net.cpp:184] Created Layer res4a_branch2a (24)
I1002 20:39:28.591621 31268 net.cpp:561] res4a_branch2a <- pool3
I1002 20:39:28.591624 31268 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1002 20:39:28.602463 31268 net.cpp:245] Setting up res4a_branch2a
I1002 20:39:28.602493 31268 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I1002 20:39:28.602505 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:28.602511 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.602524 31268 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I1002 20:39:28.602530 31268 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1002 20:39:28.602535 31268 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1002 20:39:28.604100 31268 net.cpp:245] Setting up res4a_branch2a/bn
I1002 20:39:28.604112 31268 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I1002 20:39:28.604123 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1002 20:39:28.604128 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.604135 31268 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I1002 20:39:28.604138 31268 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1002 20:39:28.604142 31268 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1002 20:39:28.604149 31268 net.cpp:245] Setting up res4a_branch2a/relu
I1002 20:39:28.604153 31268 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I1002 20:39:28.604157 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1002 20:39:28.604161 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.604173 31268 net.cpp:184] Created Layer res4a_branch2b (27)
I1002 20:39:28.604183 31268 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1002 20:39:28.604187 31268 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1002 20:39:28.607733 31268 net.cpp:245] Setting up res4a_branch2b
I1002 20:39:28.607769 31268 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I1002 20:39:28.607777 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:28.607782 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.607795 31268 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I1002 20:39:28.607800 31268 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1002 20:39:28.607805 31268 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1002 20:39:28.608613 31268 net.cpp:245] Setting up res4a_branch2b/bn
I1002 20:39:28.608625 31268 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I1002 20:39:28.608635 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1002 20:39:28.608641 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.608649 31268 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I1002 20:39:28.608652 31268 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1002 20:39:28.608656 31268 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1002 20:39:28.608664 31268 net.cpp:245] Setting up res4a_branch2b/relu
I1002 20:39:28.608667 31268 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I1002 20:39:28.608671 31268 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1002 20:39:28.608675 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.608680 31268 net.cpp:184] Created Layer pool4 (30)
I1002 20:39:28.608685 31268 net.cpp:561] pool4 <- res4a_branch2b
I1002 20:39:28.608687 31268 net.cpp:530] pool4 -> pool4
I1002 20:39:28.608788 31268 net.cpp:245] Setting up pool4
I1002 20:39:28.608796 31268 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I1002 20:39:28.608800 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1002 20:39:28.608804 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.608815 31268 net.cpp:184] Created Layer res5a_branch2a (31)
I1002 20:39:28.608819 31268 net.cpp:561] res5a_branch2a <- pool4
I1002 20:39:28.608822 31268 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1002 20:39:28.641155 31268 net.cpp:245] Setting up res5a_branch2a
I1002 20:39:28.641178 31268 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I1002 20:39:28.641187 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:28.641193 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.641204 31268 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I1002 20:39:28.641221 31268 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1002 20:39:28.641232 31268 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1002 20:39:28.643468 31268 net.cpp:245] Setting up res5a_branch2a/bn
I1002 20:39:28.643479 31268 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I1002 20:39:28.643489 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1002 20:39:28.643496 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.643502 31268 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I1002 20:39:28.643507 31268 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1002 20:39:28.643510 31268 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1002 20:39:28.643517 31268 net.cpp:245] Setting up res5a_branch2a/relu
I1002 20:39:28.643522 31268 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I1002 20:39:28.643524 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1002 20:39:28.643528 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.643553 31268 net.cpp:184] Created Layer res5a_branch2b (34)
I1002 20:39:28.643555 31268 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1002 20:39:28.643559 31268 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1002 20:39:28.659333 31268 net.cpp:245] Setting up res5a_branch2b
I1002 20:39:28.659374 31268 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I1002 20:39:28.659395 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:28.659401 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.659415 31268 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I1002 20:39:28.659435 31268 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1002 20:39:28.659447 31268 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1002 20:39:28.660214 31268 net.cpp:245] Setting up res5a_branch2b/bn
I1002 20:39:28.660225 31268 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I1002 20:39:28.660235 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1002 20:39:28.660240 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.660246 31268 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I1002 20:39:28.660250 31268 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1002 20:39:28.660254 31268 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1002 20:39:28.660260 31268 net.cpp:245] Setting up res5a_branch2b/relu
I1002 20:39:28.660265 31268 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I1002 20:39:28.660269 31268 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1002 20:39:28.660272 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.660282 31268 net.cpp:184] Created Layer out5a (37)
I1002 20:39:28.660287 31268 net.cpp:561] out5a <- res5a_branch2b
I1002 20:39:28.660292 31268 net.cpp:530] out5a -> out5a
I1002 20:39:28.665068 31268 net.cpp:245] Setting up out5a
I1002 20:39:28.665092 31268 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I1002 20:39:28.665102 31268 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1002 20:39:28.665107 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.665130 31268 net.cpp:184] Created Layer out5a/bn (38)
I1002 20:39:28.665143 31268 net.cpp:561] out5a/bn <- out5a
I1002 20:39:28.665153 31268 net.cpp:513] out5a/bn -> out5a (in-place)
I1002 20:39:28.669205 31268 net.cpp:245] Setting up out5a/bn
I1002 20:39:28.669246 31268 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I1002 20:39:28.669266 31268 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1002 20:39:28.669278 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.669294 31268 net.cpp:184] Created Layer out5a/relu (39)
I1002 20:39:28.669306 31268 net.cpp:561] out5a/relu <- out5a
I1002 20:39:28.669317 31268 net.cpp:513] out5a/relu -> out5a (in-place)
I1002 20:39:28.669332 31268 net.cpp:245] Setting up out5a/relu
I1002 20:39:28.669343 31268 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I1002 20:39:28.669354 31268 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1002 20:39:28.669364 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.669893 31268 net.cpp:184] Created Layer out5a_up2 (40)
I1002 20:39:28.669914 31268 net.cpp:561] out5a_up2 <- out5a
I1002 20:39:28.669929 31268 net.cpp:530] out5a_up2 -> out5a_up2
I1002 20:39:28.670377 31268 net.cpp:245] Setting up out5a_up2
I1002 20:39:28.670397 31268 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I1002 20:39:28.670418 31268 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1002 20:39:28.670439 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.670456 31268 net.cpp:184] Created Layer out3a (41)
I1002 20:39:28.670467 31268 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1002 20:39:28.670480 31268 net.cpp:530] out3a -> out3a
I1002 20:39:28.672099 31268 net.cpp:245] Setting up out3a
I1002 20:39:28.672125 31268 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I1002 20:39:28.672140 31268 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1002 20:39:28.672152 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.672168 31268 net.cpp:184] Created Layer out3a/bn (42)
I1002 20:39:28.672194 31268 net.cpp:561] out3a/bn <- out3a
I1002 20:39:28.672206 31268 net.cpp:513] out3a/bn -> out3a (in-place)
I1002 20:39:28.674243 31268 net.cpp:245] Setting up out3a/bn
I1002 20:39:28.674270 31268 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I1002 20:39:28.674289 31268 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1002 20:39:28.674300 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.674314 31268 net.cpp:184] Created Layer out3a/relu (43)
I1002 20:39:28.674324 31268 net.cpp:561] out3a/relu <- out3a
I1002 20:39:28.674335 31268 net.cpp:513] out3a/relu -> out3a (in-place)
I1002 20:39:28.674347 31268 net.cpp:245] Setting up out3a/relu
I1002 20:39:28.674358 31268 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I1002 20:39:28.674367 31268 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1002 20:39:28.674377 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.680630 31268 net.cpp:184] Created Layer out3_out5_combined (44)
I1002 20:39:28.680650 31268 net.cpp:561] out3_out5_combined <- out5a_up2
I1002 20:39:28.680657 31268 net.cpp:561] out3_out5_combined <- out3a
I1002 20:39:28.680662 31268 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1002 20:39:28.680770 31268 net.cpp:245] Setting up out3_out5_combined
I1002 20:39:28.680783 31268 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I1002 20:39:28.680788 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1002 20:39:28.680793 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.680804 31268 net.cpp:184] Created Layer ctx_conv1 (45)
I1002 20:39:28.680809 31268 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1002 20:39:28.680812 31268 net.cpp:530] ctx_conv1 -> ctx_conv1
I1002 20:39:28.682163 31268 net.cpp:245] Setting up ctx_conv1
I1002 20:39:28.682178 31268 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I1002 20:39:28.682186 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1002 20:39:28.682191 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.682201 31268 net.cpp:184] Created Layer ctx_conv1/bn (46)
I1002 20:39:28.682206 31268 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1002 20:39:28.682210 31268 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1002 20:39:28.682874 31268 net.cpp:245] Setting up ctx_conv1/bn
I1002 20:39:28.682883 31268 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I1002 20:39:28.682891 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1002 20:39:28.682898 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.682905 31268 net.cpp:184] Created Layer ctx_conv1/relu (47)
I1002 20:39:28.682910 31268 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1002 20:39:28.682914 31268 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1002 20:39:28.682920 31268 net.cpp:245] Setting up ctx_conv1/relu
I1002 20:39:28.682925 31268 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I1002 20:39:28.682941 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1002 20:39:28.682945 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.682956 31268 net.cpp:184] Created Layer ctx_conv2 (48)
I1002 20:39:28.682960 31268 net.cpp:561] ctx_conv2 <- ctx_conv1
I1002 20:39:28.682965 31268 net.cpp:530] ctx_conv2 -> ctx_conv2
I1002 20:39:28.684073 31268 net.cpp:245] Setting up ctx_conv2
I1002 20:39:28.684082 31268 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I1002 20:39:28.684089 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1002 20:39:28.684094 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.684103 31268 net.cpp:184] Created Layer ctx_conv2/bn (49)
I1002 20:39:28.684108 31268 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1002 20:39:28.684111 31268 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1002 20:39:28.684757 31268 net.cpp:245] Setting up ctx_conv2/bn
I1002 20:39:28.684767 31268 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I1002 20:39:28.684775 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1002 20:39:28.684780 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.684784 31268 net.cpp:184] Created Layer ctx_conv2/relu (50)
I1002 20:39:28.684789 31268 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1002 20:39:28.684793 31268 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1002 20:39:28.684799 31268 net.cpp:245] Setting up ctx_conv2/relu
I1002 20:39:28.684805 31268 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I1002 20:39:28.684809 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1002 20:39:28.684813 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.684823 31268 net.cpp:184] Created Layer ctx_conv3 (51)
I1002 20:39:28.684825 31268 net.cpp:561] ctx_conv3 <- ctx_conv2
I1002 20:39:28.684829 31268 net.cpp:530] ctx_conv3 -> ctx_conv3
I1002 20:39:28.685907 31268 net.cpp:245] Setting up ctx_conv3
I1002 20:39:28.685915 31268 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I1002 20:39:28.685921 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1002 20:39:28.685926 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.685933 31268 net.cpp:184] Created Layer ctx_conv3/bn (52)
I1002 20:39:28.685937 31268 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1002 20:39:28.685941 31268 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1002 20:39:28.686564 31268 net.cpp:245] Setting up ctx_conv3/bn
I1002 20:39:28.686573 31268 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I1002 20:39:28.686581 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1002 20:39:28.686586 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.686594 31268 net.cpp:184] Created Layer ctx_conv3/relu (53)
I1002 20:39:28.686599 31268 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1002 20:39:28.686604 31268 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1002 20:39:28.686619 31268 net.cpp:245] Setting up ctx_conv3/relu
I1002 20:39:28.686625 31268 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I1002 20:39:28.686628 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1002 20:39:28.686638 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.686648 31268 net.cpp:184] Created Layer ctx_conv4 (54)
I1002 20:39:28.686652 31268 net.cpp:561] ctx_conv4 <- ctx_conv3
I1002 20:39:28.686656 31268 net.cpp:530] ctx_conv4 -> ctx_conv4
I1002 20:39:28.687752 31268 net.cpp:245] Setting up ctx_conv4
I1002 20:39:28.687760 31268 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I1002 20:39:28.687767 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1002 20:39:28.687772 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.687783 31268 net.cpp:184] Created Layer ctx_conv4/bn (55)
I1002 20:39:28.687786 31268 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1002 20:39:28.687790 31268 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1002 20:39:28.688477 31268 net.cpp:245] Setting up ctx_conv4/bn
I1002 20:39:28.688486 31268 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I1002 20:39:28.688496 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1002 20:39:28.688510 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.688522 31268 net.cpp:184] Created Layer ctx_conv4/relu (56)
I1002 20:39:28.688534 31268 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1002 20:39:28.688544 31268 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1002 20:39:28.688555 31268 net.cpp:245] Setting up ctx_conv4/relu
I1002 20:39:28.688568 31268 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I1002 20:39:28.688578 31268 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1002 20:39:28.688590 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.688604 31268 net.cpp:184] Created Layer ctx_final (57)
I1002 20:39:28.688616 31268 net.cpp:561] ctx_final <- ctx_conv4
I1002 20:39:28.688626 31268 net.cpp:530] ctx_final -> ctx_final
I1002 20:39:28.690217 31268 net.cpp:245] Setting up ctx_final
I1002 20:39:28.690240 31268 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I1002 20:39:28.690256 31268 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1002 20:39:28.690268 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.690279 31268 net.cpp:184] Created Layer ctx_final/relu (58)
I1002 20:39:28.690290 31268 net.cpp:561] ctx_final/relu <- ctx_final
I1002 20:39:28.690301 31268 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1002 20:39:28.690315 31268 net.cpp:245] Setting up ctx_final/relu
I1002 20:39:28.690330 31268 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I1002 20:39:28.690346 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1002 20:39:28.690362 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.690385 31268 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I1002 20:39:28.690402 31268 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1002 20:39:28.690419 31268 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1002 20:39:28.691056 31268 net.cpp:245] Setting up out_deconv_final_up2
I1002 20:39:28.691088 31268 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I1002 20:39:28.691109 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1002 20:39:28.691126 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.691148 31268 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I1002 20:39:28.691170 31268 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1002 20:39:28.691187 31268 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1002 20:39:28.691576 31268 net.cpp:245] Setting up out_deconv_final_up4
I1002 20:39:28.691592 31268 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I1002 20:39:28.691604 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1002 20:39:28.691622 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.691651 31268 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I1002 20:39:28.691663 31268 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1002 20:39:28.691673 31268 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1002 20:39:28.692044 31268 net.cpp:245] Setting up out_deconv_final_up8
I1002 20:39:28.692060 31268 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I1002 20:39:28.692073 31268 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I1002 20:39:28.692085 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.696213 31268 net.cpp:184] Created Layer loss (62)
I1002 20:39:28.696228 31268 net.cpp:561] loss <- out_deconv_final_up8
I1002 20:39:28.696233 31268 net.cpp:561] loss <- label
I1002 20:39:28.696238 31268 net.cpp:530] loss -> loss
I1002 20:39:28.698292 31268 net.cpp:245] Setting up loss
I1002 20:39:28.698307 31268 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I1002 20:39:28.698312 31268 net.cpp:256]     with loss weight 1
I1002 20:39:28.698334 31268 net.cpp:323] loss needs backward computation.
I1002 20:39:28.698340 31268 net.cpp:323] out_deconv_final_up8 needs backward computation.
I1002 20:39:28.698345 31268 net.cpp:323] out_deconv_final_up4 needs backward computation.
I1002 20:39:28.698348 31268 net.cpp:323] out_deconv_final_up2 needs backward computation.
I1002 20:39:28.698354 31268 net.cpp:323] ctx_final/relu needs backward computation.
I1002 20:39:28.698356 31268 net.cpp:323] ctx_final needs backward computation.
I1002 20:39:28.698360 31268 net.cpp:323] ctx_conv4/relu needs backward computation.
I1002 20:39:28.698364 31268 net.cpp:323] ctx_conv4/bn needs backward computation.
I1002 20:39:28.698369 31268 net.cpp:323] ctx_conv4 needs backward computation.
I1002 20:39:28.698372 31268 net.cpp:323] ctx_conv3/relu needs backward computation.
I1002 20:39:28.698376 31268 net.cpp:323] ctx_conv3/bn needs backward computation.
I1002 20:39:28.698379 31268 net.cpp:323] ctx_conv3 needs backward computation.
I1002 20:39:28.698384 31268 net.cpp:323] ctx_conv2/relu needs backward computation.
I1002 20:39:28.698386 31268 net.cpp:323] ctx_conv2/bn needs backward computation.
I1002 20:39:28.698390 31268 net.cpp:323] ctx_conv2 needs backward computation.
I1002 20:39:28.698395 31268 net.cpp:323] ctx_conv1/relu needs backward computation.
I1002 20:39:28.698398 31268 net.cpp:323] ctx_conv1/bn needs backward computation.
I1002 20:39:28.698401 31268 net.cpp:323] ctx_conv1 needs backward computation.
I1002 20:39:28.698405 31268 net.cpp:323] out3_out5_combined needs backward computation.
I1002 20:39:28.698410 31268 net.cpp:323] out3a/relu needs backward computation.
I1002 20:39:28.698413 31268 net.cpp:323] out3a/bn needs backward computation.
I1002 20:39:28.698418 31268 net.cpp:323] out3a needs backward computation.
I1002 20:39:28.698422 31268 net.cpp:323] out5a_up2 needs backward computation.
I1002 20:39:28.698426 31268 net.cpp:323] out5a/relu needs backward computation.
I1002 20:39:28.698429 31268 net.cpp:323] out5a/bn needs backward computation.
I1002 20:39:28.698431 31268 net.cpp:323] out5a needs backward computation.
I1002 20:39:28.698436 31268 net.cpp:323] res5a_branch2b/relu needs backward computation.
I1002 20:39:28.698439 31268 net.cpp:323] res5a_branch2b/bn needs backward computation.
I1002 20:39:28.698442 31268 net.cpp:323] res5a_branch2b needs backward computation.
I1002 20:39:28.698446 31268 net.cpp:323] res5a_branch2a/relu needs backward computation.
I1002 20:39:28.698449 31268 net.cpp:323] res5a_branch2a/bn needs backward computation.
I1002 20:39:28.698452 31268 net.cpp:323] res5a_branch2a needs backward computation.
I1002 20:39:28.698456 31268 net.cpp:323] pool4 needs backward computation.
I1002 20:39:28.698460 31268 net.cpp:323] res4a_branch2b/relu needs backward computation.
I1002 20:39:28.698463 31268 net.cpp:323] res4a_branch2b/bn needs backward computation.
I1002 20:39:28.698469 31268 net.cpp:323] res4a_branch2b needs backward computation.
I1002 20:39:28.698488 31268 net.cpp:323] res4a_branch2a/relu needs backward computation.
I1002 20:39:28.698493 31268 net.cpp:323] res4a_branch2a/bn needs backward computation.
I1002 20:39:28.698495 31268 net.cpp:323] res4a_branch2a needs backward computation.
I1002 20:39:28.698498 31268 net.cpp:323] pool3 needs backward computation.
I1002 20:39:28.698503 31268 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I1002 20:39:28.698505 31268 net.cpp:323] res3a_branch2b/relu needs backward computation.
I1002 20:39:28.698508 31268 net.cpp:323] res3a_branch2b/bn needs backward computation.
I1002 20:39:28.698511 31268 net.cpp:323] res3a_branch2b needs backward computation.
I1002 20:39:28.698514 31268 net.cpp:323] res3a_branch2a/relu needs backward computation.
I1002 20:39:28.698518 31268 net.cpp:323] res3a_branch2a/bn needs backward computation.
I1002 20:39:28.698520 31268 net.cpp:323] res3a_branch2a needs backward computation.
I1002 20:39:28.698523 31268 net.cpp:323] pool2 needs backward computation.
I1002 20:39:28.698526 31268 net.cpp:323] res2a_branch2b/relu needs backward computation.
I1002 20:39:28.698529 31268 net.cpp:323] res2a_branch2b/bn needs backward computation.
I1002 20:39:28.698532 31268 net.cpp:323] res2a_branch2b needs backward computation.
I1002 20:39:28.698535 31268 net.cpp:323] res2a_branch2a/relu needs backward computation.
I1002 20:39:28.698539 31268 net.cpp:323] res2a_branch2a/bn needs backward computation.
I1002 20:39:28.698541 31268 net.cpp:323] res2a_branch2a needs backward computation.
I1002 20:39:28.698544 31268 net.cpp:323] pool1 needs backward computation.
I1002 20:39:28.698547 31268 net.cpp:323] conv1b/relu needs backward computation.
I1002 20:39:28.698551 31268 net.cpp:323] conv1b/bn needs backward computation.
I1002 20:39:28.698554 31268 net.cpp:323] conv1b needs backward computation.
I1002 20:39:28.698557 31268 net.cpp:323] conv1a/relu needs backward computation.
I1002 20:39:28.698559 31268 net.cpp:323] conv1a/bn needs backward computation.
I1002 20:39:28.698561 31268 net.cpp:323] conv1a needs backward computation.
I1002 20:39:28.698565 31268 net.cpp:325] data/bias does not need backward computation.
I1002 20:39:28.698567 31268 net.cpp:325] data does not need backward computation.
I1002 20:39:28.698570 31268 net.cpp:367] This network produces output loss
I1002 20:39:28.698654 31268 net.cpp:389] Top memory (TRAIN) required for data: 1435238408 diff: 1435238408
I1002 20:39:28.698660 31268 net.cpp:392] Bottom memory (TRAIN) required for data: 1435238400 diff: 1435238400
I1002 20:39:28.698664 31268 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 772915200 diff: 772915200
I1002 20:39:28.698668 31268 net.cpp:398] Parameters memory (TRAIN) required for data: 10817840 diff: 10817840
I1002 20:39:28.698673 31268 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I1002 20:39:28.698676 31268 net.cpp:407] Network initialization done.
I1002 20:39:28.707764 31268 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W1002 20:39:28.707962 31268 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I1002 20:39:28.708328 31268 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I1002 20:39:28.709167 31268 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:28.709189 31268 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:28.709211 31268 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I1002 20:39:28.709223 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:28.709239 31268 net.cpp:184] Created Layer data (0)
I1002 20:39:28.709250 31268 net.cpp:530] data -> data
I1002 20:39:28.709264 31268 net.cpp:530] data -> label
I1002 20:39:28.709311 31268 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:28.709329 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:28.762914 31405 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I1002 20:39:28.846384 31268 data_layer.cpp:187] (0) ReshapePrefetch 2, 3, 640, 640
I1002 20:39:28.846469 31268 data_layer.cpp:211] (0) Output data size: 2, 3, 640, 640
I1002 20:39:28.846478 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:28.846520 31268 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:28.846530 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:28.861270 31417 data_layer.cpp:101] (0) Parser threads: 1
I1002 20:39:28.861300 31417 data_layer.cpp:103] (0) Transformer threads: 1
I1002 20:39:28.905915 31418 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I1002 20:39:29.037117 31268 data_layer.cpp:187] (0) ReshapePrefetch 2, 1, 640, 640
I1002 20:39:29.037253 31268 data_layer.cpp:211] (0) Output data size: 2, 1, 640, 640
I1002 20:39:29.037286 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:29.037345 31268 net.cpp:245] Setting up data
I1002 20:39:29.037355 31268 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I1002 20:39:29.037361 31268 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I1002 20:39:29.037367 31268 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I1002 20:39:29.037374 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.037386 31268 net.cpp:184] Created Layer label_data_1_split (1)
I1002 20:39:29.037390 31268 net.cpp:561] label_data_1_split <- label
I1002 20:39:29.037396 31268 net.cpp:530] label_data_1_split -> label_data_1_split_0
I1002 20:39:29.037405 31268 net.cpp:530] label_data_1_split -> label_data_1_split_1
I1002 20:39:29.037408 31268 net.cpp:530] label_data_1_split -> label_data_1_split_2
I1002 20:39:29.037516 31268 net.cpp:245] Setting up label_data_1_split
I1002 20:39:29.037523 31268 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I1002 20:39:29.037526 31268 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I1002 20:39:29.037530 31268 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I1002 20:39:29.037534 31268 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I1002 20:39:29.037539 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.037547 31268 net.cpp:184] Created Layer data/bias (2)
I1002 20:39:29.037550 31268 net.cpp:561] data/bias <- data
I1002 20:39:29.037554 31268 net.cpp:530] data/bias -> data/bias
I1002 20:39:29.038766 31419 data_layer.cpp:101] (0) Parser threads: 1
I1002 20:39:29.038780 31419 data_layer.cpp:103] (0) Transformer threads: 1
I1002 20:39:29.042080 31268 net.cpp:245] Setting up data/bias
I1002 20:39:29.042101 31268 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I1002 20:39:29.042114 31268 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I1002 20:39:29.042120 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.042136 31268 net.cpp:184] Created Layer conv1a (3)
I1002 20:39:29.042140 31268 net.cpp:561] conv1a <- data/bias
I1002 20:39:29.042146 31268 net.cpp:530] conv1a -> conv1a
I1002 20:39:29.043402 31268 net.cpp:245] Setting up conv1a
I1002 20:39:29.043414 31268 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I1002 20:39:29.043422 31268 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1002 20:39:29.043442 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.043457 31268 net.cpp:184] Created Layer conv1a/bn (4)
I1002 20:39:29.043467 31268 net.cpp:561] conv1a/bn <- conv1a
I1002 20:39:29.043476 31268 net.cpp:513] conv1a/bn -> conv1a (in-place)
I1002 20:39:29.046594 31268 net.cpp:245] Setting up conv1a/bn
I1002 20:39:29.046617 31268 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I1002 20:39:29.046636 31268 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I1002 20:39:29.046646 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.046658 31268 net.cpp:184] Created Layer conv1a/relu (5)
I1002 20:39:29.046667 31268 net.cpp:561] conv1a/relu <- conv1a
I1002 20:39:29.046676 31268 net.cpp:513] conv1a/relu -> conv1a (in-place)
I1002 20:39:29.046689 31268 net.cpp:245] Setting up conv1a/relu
I1002 20:39:29.046699 31268 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I1002 20:39:29.046707 31268 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I1002 20:39:29.046716 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.046736 31268 net.cpp:184] Created Layer conv1b (6)
I1002 20:39:29.046756 31268 net.cpp:561] conv1b <- conv1a
I1002 20:39:29.046766 31268 net.cpp:530] conv1b -> conv1b
I1002 20:39:29.047569 31268 net.cpp:245] Setting up conv1b
I1002 20:39:29.047605 31268 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I1002 20:39:29.047621 31268 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1002 20:39:29.047632 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.047644 31268 net.cpp:184] Created Layer conv1b/bn (7)
I1002 20:39:29.047653 31268 net.cpp:561] conv1b/bn <- conv1b
I1002 20:39:29.047664 31268 net.cpp:513] conv1b/bn -> conv1b (in-place)
I1002 20:39:29.049721 31268 net.cpp:245] Setting up conv1b/bn
I1002 20:39:29.049746 31268 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I1002 20:39:29.049763 31268 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I1002 20:39:29.049774 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.049789 31268 net.cpp:184] Created Layer conv1b/relu (8)
I1002 20:39:29.049799 31268 net.cpp:561] conv1b/relu <- conv1b
I1002 20:39:29.049809 31268 net.cpp:513] conv1b/relu -> conv1b (in-place)
I1002 20:39:29.049821 31268 net.cpp:245] Setting up conv1b/relu
I1002 20:39:29.049831 31268 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I1002 20:39:29.049840 31268 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I1002 20:39:29.049849 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.049861 31268 net.cpp:184] Created Layer pool1 (9)
I1002 20:39:29.049870 31268 net.cpp:561] pool1 <- conv1b
I1002 20:39:29.049880 31268 net.cpp:530] pool1 -> pool1
I1002 20:39:29.049986 31268 net.cpp:245] Setting up pool1
I1002 20:39:29.050000 31268 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I1002 20:39:29.050010 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I1002 20:39:29.050019 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.050035 31268 net.cpp:184] Created Layer res2a_branch2a (10)
I1002 20:39:29.050045 31268 net.cpp:561] res2a_branch2a <- pool1
I1002 20:39:29.050053 31268 net.cpp:530] res2a_branch2a -> res2a_branch2a
I1002 20:39:29.051200 31268 net.cpp:245] Setting up res2a_branch2a
I1002 20:39:29.051219 31268 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I1002 20:39:29.051235 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:29.051245 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.051259 31268 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I1002 20:39:29.051267 31268 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I1002 20:39:29.051276 31268 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1002 20:39:29.053083 31268 net.cpp:245] Setting up res2a_branch2a/bn
I1002 20:39:29.053105 31268 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I1002 20:39:29.053122 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1002 20:39:29.053133 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.053143 31268 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I1002 20:39:29.053153 31268 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I1002 20:39:29.053161 31268 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1002 20:39:29.053174 31268 net.cpp:245] Setting up res2a_branch2a/relu
I1002 20:39:29.053184 31268 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I1002 20:39:29.053192 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I1002 20:39:29.053207 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.053231 31268 net.cpp:184] Created Layer res2a_branch2b (13)
I1002 20:39:29.053241 31268 net.cpp:561] res2a_branch2b <- res2a_branch2a
I1002 20:39:29.053249 31268 net.cpp:530] res2a_branch2b -> res2a_branch2b
I1002 20:39:29.054059 31268 net.cpp:245] Setting up res2a_branch2b
I1002 20:39:29.054069 31268 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I1002 20:39:29.054074 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:29.054077 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.054085 31268 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I1002 20:39:29.054088 31268 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I1002 20:39:29.054092 31268 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1002 20:39:29.054944 31268 net.cpp:245] Setting up res2a_branch2b/bn
I1002 20:39:29.054953 31268 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I1002 20:39:29.054962 31268 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1002 20:39:29.054966 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.054971 31268 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I1002 20:39:29.054975 31268 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I1002 20:39:29.054981 31268 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1002 20:39:29.054987 31268 net.cpp:245] Setting up res2a_branch2b/relu
I1002 20:39:29.054992 31268 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I1002 20:39:29.054996 31268 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I1002 20:39:29.055001 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.055007 31268 net.cpp:184] Created Layer pool2 (16)
I1002 20:39:29.055011 31268 net.cpp:561] pool2 <- res2a_branch2b
I1002 20:39:29.055016 31268 net.cpp:530] pool2 -> pool2
I1002 20:39:29.055104 31268 net.cpp:245] Setting up pool2
I1002 20:39:29.055111 31268 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I1002 20:39:29.055115 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I1002 20:39:29.055131 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.055141 31268 net.cpp:184] Created Layer res3a_branch2a (17)
I1002 20:39:29.055146 31268 net.cpp:561] res3a_branch2a <- pool2
I1002 20:39:29.055150 31268 net.cpp:530] res3a_branch2a -> res3a_branch2a
I1002 20:39:29.057641 31268 net.cpp:245] Setting up res3a_branch2a
I1002 20:39:29.057687 31268 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I1002 20:39:29.057698 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:29.057714 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.057724 31268 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I1002 20:39:29.057729 31268 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I1002 20:39:29.057734 31268 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1002 20:39:29.058580 31268 net.cpp:245] Setting up res3a_branch2a/bn
I1002 20:39:29.058589 31268 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I1002 20:39:29.058601 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1002 20:39:29.058605 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.058610 31268 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I1002 20:39:29.058614 31268 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I1002 20:39:29.058616 31268 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1002 20:39:29.058621 31268 net.cpp:245] Setting up res3a_branch2a/relu
I1002 20:39:29.058635 31268 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I1002 20:39:29.058639 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I1002 20:39:29.058642 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.058651 31268 net.cpp:184] Created Layer res3a_branch2b (20)
I1002 20:39:29.058655 31268 net.cpp:561] res3a_branch2b <- res3a_branch2a
I1002 20:39:29.058660 31268 net.cpp:530] res3a_branch2b -> res3a_branch2b
I1002 20:39:29.060148 31268 net.cpp:245] Setting up res3a_branch2b
I1002 20:39:29.060158 31268 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I1002 20:39:29.060164 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:29.060168 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.060178 31268 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I1002 20:39:29.060183 31268 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I1002 20:39:29.060186 31268 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1002 20:39:29.061022 31268 net.cpp:245] Setting up res3a_branch2b/bn
I1002 20:39:29.061031 31268 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I1002 20:39:29.061039 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1002 20:39:29.061043 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.061048 31268 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I1002 20:39:29.061053 31268 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I1002 20:39:29.061056 31268 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1002 20:39:29.061061 31268 net.cpp:245] Setting up res3a_branch2b/relu
I1002 20:39:29.061066 31268 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I1002 20:39:29.061070 31268 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1002 20:39:29.061074 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.061080 31268 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I1002 20:39:29.061084 31268 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1002 20:39:29.061087 31268 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1002 20:39:29.061092 31268 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1002 20:39:29.061151 31268 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1002 20:39:29.061156 31268 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I1002 20:39:29.061161 31268 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I1002 20:39:29.061166 31268 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I1002 20:39:29.061169 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.061174 31268 net.cpp:184] Created Layer pool3 (24)
I1002 20:39:29.061178 31268 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1002 20:39:29.061182 31268 net.cpp:530] pool3 -> pool3
I1002 20:39:29.061267 31268 net.cpp:245] Setting up pool3
I1002 20:39:29.061275 31268 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I1002 20:39:29.061280 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I1002 20:39:29.061282 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.061291 31268 net.cpp:184] Created Layer res4a_branch2a (25)
I1002 20:39:29.061295 31268 net.cpp:561] res4a_branch2a <- pool3
I1002 20:39:29.061300 31268 net.cpp:530] res4a_branch2a -> res4a_branch2a
I1002 20:39:29.070307 31268 net.cpp:245] Setting up res4a_branch2a
I1002 20:39:29.070377 31268 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I1002 20:39:29.070395 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:29.070408 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.070425 31268 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I1002 20:39:29.070436 31268 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I1002 20:39:29.070446 31268 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1002 20:39:29.107681 31268 net.cpp:245] Setting up res4a_branch2a/bn
I1002 20:39:29.107760 31268 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I1002 20:39:29.107792 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1002 20:39:29.107808 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.107823 31268 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I1002 20:39:29.107836 31268 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I1002 20:39:29.107847 31268 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1002 20:39:29.107863 31268 net.cpp:245] Setting up res4a_branch2a/relu
I1002 20:39:29.107875 31268 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I1002 20:39:29.107887 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I1002 20:39:29.107897 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.107918 31268 net.cpp:184] Created Layer res4a_branch2b (28)
I1002 20:39:29.107928 31268 net.cpp:561] res4a_branch2b <- res4a_branch2a
I1002 20:39:29.107941 31268 net.cpp:530] res4a_branch2b -> res4a_branch2b
I1002 20:39:29.112726 31268 net.cpp:245] Setting up res4a_branch2b
I1002 20:39:29.112752 31268 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I1002 20:39:29.112761 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:29.112767 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.112788 31268 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I1002 20:39:29.112794 31268 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I1002 20:39:29.112800 31268 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1002 20:39:29.114454 31268 net.cpp:245] Setting up res4a_branch2b/bn
I1002 20:39:29.114464 31268 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I1002 20:39:29.114472 31268 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1002 20:39:29.114477 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.114488 31268 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I1002 20:39:29.114493 31268 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I1002 20:39:29.114504 31268 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1002 20:39:29.114511 31268 net.cpp:245] Setting up res4a_branch2b/relu
I1002 20:39:29.114517 31268 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I1002 20:39:29.114522 31268 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I1002 20:39:29.114526 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.114534 31268 net.cpp:184] Created Layer pool4 (31)
I1002 20:39:29.114539 31268 net.cpp:561] pool4 <- res4a_branch2b
I1002 20:39:29.114544 31268 net.cpp:530] pool4 -> pool4
I1002 20:39:29.114634 31268 net.cpp:245] Setting up pool4
I1002 20:39:29.114641 31268 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I1002 20:39:29.114645 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I1002 20:39:29.114660 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.114677 31268 net.cpp:184] Created Layer res5a_branch2a (32)
I1002 20:39:29.114681 31268 net.cpp:561] res5a_branch2a <- pool4
I1002 20:39:29.114693 31268 net.cpp:530] res5a_branch2a -> res5a_branch2a
I1002 20:39:29.145103 31268 net.cpp:245] Setting up res5a_branch2a
I1002 20:39:29.145130 31268 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I1002 20:39:29.145139 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1002 20:39:29.145144 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.145154 31268 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I1002 20:39:29.145161 31268 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I1002 20:39:29.145166 31268 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1002 20:39:29.147564 31268 net.cpp:245] Setting up res5a_branch2a/bn
I1002 20:39:29.147575 31268 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I1002 20:39:29.147585 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1002 20:39:29.147591 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.147596 31268 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I1002 20:39:29.147600 31268 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I1002 20:39:29.147604 31268 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1002 20:39:29.147610 31268 net.cpp:245] Setting up res5a_branch2a/relu
I1002 20:39:29.147614 31268 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I1002 20:39:29.147619 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I1002 20:39:29.147622 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.147630 31268 net.cpp:184] Created Layer res5a_branch2b (35)
I1002 20:39:29.147634 31268 net.cpp:561] res5a_branch2b <- res5a_branch2a
I1002 20:39:29.147637 31268 net.cpp:530] res5a_branch2b -> res5a_branch2b
I1002 20:39:29.162360 31268 net.cpp:245] Setting up res5a_branch2b
I1002 20:39:29.162394 31268 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I1002 20:39:29.162408 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1002 20:39:29.162415 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.162427 31268 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I1002 20:39:29.162432 31268 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I1002 20:39:29.162436 31268 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1002 20:39:29.163393 31268 net.cpp:245] Setting up res5a_branch2b/bn
I1002 20:39:29.163403 31268 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I1002 20:39:29.163414 31268 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1002 20:39:29.163420 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.163426 31268 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I1002 20:39:29.163431 31268 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I1002 20:39:29.163436 31268 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1002 20:39:29.163444 31268 net.cpp:245] Setting up res5a_branch2b/relu
I1002 20:39:29.163450 31268 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I1002 20:39:29.163455 31268 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I1002 20:39:29.163460 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.163475 31268 net.cpp:184] Created Layer out5a (38)
I1002 20:39:29.163480 31268 net.cpp:561] out5a <- res5a_branch2b
I1002 20:39:29.163486 31268 net.cpp:530] out5a -> out5a
I1002 20:39:29.168035 31268 net.cpp:245] Setting up out5a
I1002 20:39:29.168051 31268 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I1002 20:39:29.168059 31268 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I1002 20:39:29.168064 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.168072 31268 net.cpp:184] Created Layer out5a/bn (39)
I1002 20:39:29.168081 31268 net.cpp:561] out5a/bn <- out5a
I1002 20:39:29.168088 31268 net.cpp:513] out5a/bn -> out5a (in-place)
I1002 20:39:29.170074 31268 net.cpp:245] Setting up out5a/bn
I1002 20:39:29.170087 31268 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I1002 20:39:29.170097 31268 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I1002 20:39:29.170104 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.170109 31268 net.cpp:184] Created Layer out5a/relu (40)
I1002 20:39:29.170114 31268 net.cpp:561] out5a/relu <- out5a
I1002 20:39:29.170117 31268 net.cpp:513] out5a/relu -> out5a (in-place)
I1002 20:39:29.170125 31268 net.cpp:245] Setting up out5a/relu
I1002 20:39:29.170131 31268 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I1002 20:39:29.170135 31268 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I1002 20:39:29.170140 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.170148 31268 net.cpp:184] Created Layer out5a_up2 (41)
I1002 20:39:29.170151 31268 net.cpp:561] out5a_up2 <- out5a
I1002 20:39:29.170156 31268 net.cpp:530] out5a_up2 -> out5a_up2
I1002 20:39:29.170482 31268 net.cpp:245] Setting up out5a_up2
I1002 20:39:29.170488 31268 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I1002 20:39:29.170495 31268 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I1002 20:39:29.170500 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.170508 31268 net.cpp:184] Created Layer out3a (42)
I1002 20:39:29.170513 31268 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1002 20:39:29.170517 31268 net.cpp:530] out3a -> out3a
I1002 20:39:29.171787 31268 net.cpp:245] Setting up out3a
I1002 20:39:29.171795 31268 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I1002 20:39:29.171802 31268 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I1002 20:39:29.171808 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.171814 31268 net.cpp:184] Created Layer out3a/bn (43)
I1002 20:39:29.171821 31268 net.cpp:561] out3a/bn <- out3a
I1002 20:39:29.171825 31268 net.cpp:513] out3a/bn -> out3a (in-place)
I1002 20:39:29.173343 31268 net.cpp:245] Setting up out3a/bn
I1002 20:39:29.173353 31268 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I1002 20:39:29.173363 31268 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I1002 20:39:29.173369 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.173374 31268 net.cpp:184] Created Layer out3a/relu (44)
I1002 20:39:29.173377 31268 net.cpp:561] out3a/relu <- out3a
I1002 20:39:29.173382 31268 net.cpp:513] out3a/relu -> out3a (in-place)
I1002 20:39:29.173388 31268 net.cpp:245] Setting up out3a/relu
I1002 20:39:29.173391 31268 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I1002 20:39:29.173395 31268 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1002 20:39:29.173399 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.173404 31268 net.cpp:184] Created Layer out3_out5_combined (45)
I1002 20:39:29.173408 31268 net.cpp:561] out3_out5_combined <- out5a_up2
I1002 20:39:29.173413 31268 net.cpp:561] out3_out5_combined <- out3a
I1002 20:39:29.173429 31268 net.cpp:530] out3_out5_combined -> out3_out5_combined
I1002 20:39:29.173477 31268 net.cpp:245] Setting up out3_out5_combined
I1002 20:39:29.173486 31268 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I1002 20:39:29.173491 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I1002 20:39:29.173494 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.173503 31268 net.cpp:184] Created Layer ctx_conv1 (46)
I1002 20:39:29.173507 31268 net.cpp:561] ctx_conv1 <- out3_out5_combined
I1002 20:39:29.173511 31268 net.cpp:530] ctx_conv1 -> ctx_conv1
I1002 20:39:29.175181 31268 net.cpp:245] Setting up ctx_conv1
I1002 20:39:29.175195 31268 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I1002 20:39:29.175202 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1002 20:39:29.175209 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.175217 31268 net.cpp:184] Created Layer ctx_conv1/bn (47)
I1002 20:39:29.175221 31268 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I1002 20:39:29.175225 31268 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I1002 20:39:29.176203 31268 net.cpp:245] Setting up ctx_conv1/bn
I1002 20:39:29.176213 31268 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I1002 20:39:29.176221 31268 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1002 20:39:29.176228 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.176234 31268 net.cpp:184] Created Layer ctx_conv1/relu (48)
I1002 20:39:29.176236 31268 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I1002 20:39:29.176240 31268 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I1002 20:39:29.176246 31268 net.cpp:245] Setting up ctx_conv1/relu
I1002 20:39:29.176251 31268 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I1002 20:39:29.176254 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I1002 20:39:29.176259 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.176272 31268 net.cpp:184] Created Layer ctx_conv2 (49)
I1002 20:39:29.176276 31268 net.cpp:561] ctx_conv2 <- ctx_conv1
I1002 20:39:29.176280 31268 net.cpp:530] ctx_conv2 -> ctx_conv2
I1002 20:39:29.177496 31268 net.cpp:245] Setting up ctx_conv2
I1002 20:39:29.177505 31268 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I1002 20:39:29.177512 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1002 20:39:29.177516 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.177523 31268 net.cpp:184] Created Layer ctx_conv2/bn (50)
I1002 20:39:29.177527 31268 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I1002 20:39:29.177531 31268 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I1002 20:39:29.178215 31268 net.cpp:245] Setting up ctx_conv2/bn
I1002 20:39:29.178223 31268 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I1002 20:39:29.178232 31268 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1002 20:39:29.178237 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.178244 31268 net.cpp:184] Created Layer ctx_conv2/relu (51)
I1002 20:39:29.178249 31268 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I1002 20:39:29.178253 31268 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I1002 20:39:29.178261 31268 net.cpp:245] Setting up ctx_conv2/relu
I1002 20:39:29.178267 31268 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I1002 20:39:29.178270 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I1002 20:39:29.178275 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.178288 31268 net.cpp:184] Created Layer ctx_conv3 (52)
I1002 20:39:29.178303 31268 net.cpp:561] ctx_conv3 <- ctx_conv2
I1002 20:39:29.178306 31268 net.cpp:530] ctx_conv3 -> ctx_conv3
I1002 20:39:29.179428 31268 net.cpp:245] Setting up ctx_conv3
I1002 20:39:29.179436 31268 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I1002 20:39:29.179443 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1002 20:39:29.179447 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.179455 31268 net.cpp:184] Created Layer ctx_conv3/bn (53)
I1002 20:39:29.179458 31268 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I1002 20:39:29.179463 31268 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I1002 20:39:29.180133 31268 net.cpp:245] Setting up ctx_conv3/bn
I1002 20:39:29.180141 31268 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I1002 20:39:29.180150 31268 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1002 20:39:29.180155 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.180160 31268 net.cpp:184] Created Layer ctx_conv3/relu (54)
I1002 20:39:29.180163 31268 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I1002 20:39:29.180167 31268 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I1002 20:39:29.180173 31268 net.cpp:245] Setting up ctx_conv3/relu
I1002 20:39:29.180187 31268 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I1002 20:39:29.180192 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I1002 20:39:29.180197 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.180205 31268 net.cpp:184] Created Layer ctx_conv4 (55)
I1002 20:39:29.180209 31268 net.cpp:561] ctx_conv4 <- ctx_conv3
I1002 20:39:29.180213 31268 net.cpp:530] ctx_conv4 -> ctx_conv4
I1002 20:39:29.181335 31268 net.cpp:245] Setting up ctx_conv4
I1002 20:39:29.181344 31268 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I1002 20:39:29.181350 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1002 20:39:29.181355 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.181361 31268 net.cpp:184] Created Layer ctx_conv4/bn (56)
I1002 20:39:29.181365 31268 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I1002 20:39:29.181368 31268 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I1002 20:39:29.182052 31268 net.cpp:245] Setting up ctx_conv4/bn
I1002 20:39:29.182060 31268 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I1002 20:39:29.182070 31268 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1002 20:39:29.182075 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.182078 31268 net.cpp:184] Created Layer ctx_conv4/relu (57)
I1002 20:39:29.182082 31268 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I1002 20:39:29.182086 31268 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I1002 20:39:29.182093 31268 net.cpp:245] Setting up ctx_conv4/relu
I1002 20:39:29.182097 31268 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I1002 20:39:29.182101 31268 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I1002 20:39:29.182106 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.182117 31268 net.cpp:184] Created Layer ctx_final (58)
I1002 20:39:29.182121 31268 net.cpp:561] ctx_final <- ctx_conv4
I1002 20:39:29.182124 31268 net.cpp:530] ctx_final -> ctx_final
I1002 20:39:29.182684 31268 net.cpp:245] Setting up ctx_final
I1002 20:39:29.182693 31268 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I1002 20:39:29.182700 31268 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I1002 20:39:29.182704 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.182719 31268 net.cpp:184] Created Layer ctx_final/relu (59)
I1002 20:39:29.182724 31268 net.cpp:561] ctx_final/relu <- ctx_final
I1002 20:39:29.182729 31268 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I1002 20:39:29.182735 31268 net.cpp:245] Setting up ctx_final/relu
I1002 20:39:29.182740 31268 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I1002 20:39:29.182744 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1002 20:39:29.182749 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.182757 31268 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I1002 20:39:29.182760 31268 net.cpp:561] out_deconv_final_up2 <- ctx_final
I1002 20:39:29.182765 31268 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I1002 20:39:29.183058 31268 net.cpp:245] Setting up out_deconv_final_up2
I1002 20:39:29.183065 31268 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I1002 20:39:29.183071 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1002 20:39:29.183075 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.183084 31268 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I1002 20:39:29.183086 31268 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I1002 20:39:29.183091 31268 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I1002 20:39:29.183377 31268 net.cpp:245] Setting up out_deconv_final_up4
I1002 20:39:29.183383 31268 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I1002 20:39:29.183389 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1002 20:39:29.183393 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.183400 31268 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I1002 20:39:29.183403 31268 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I1002 20:39:29.183408 31268 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I1002 20:39:29.183712 31268 net.cpp:245] Setting up out_deconv_final_up8
I1002 20:39:29.183735 31268 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I1002 20:39:29.183748 31268 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I1002 20:39:29.183760 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.183769 31268 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I1002 20:39:29.183774 31268 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I1002 20:39:29.183785 31268 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I1002 20:39:29.183790 31268 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I1002 20:39:29.183802 31268 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I1002 20:39:29.183881 31268 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I1002 20:39:29.183888 31268 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I1002 20:39:29.183899 31268 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I1002 20:39:29.183903 31268 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I1002 20:39:29.183908 31268 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I1002 20:39:29.183912 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.183929 31268 net.cpp:184] Created Layer loss (64)
I1002 20:39:29.183933 31268 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I1002 20:39:29.183938 31268 net.cpp:561] loss <- label_data_1_split_0
I1002 20:39:29.183943 31268 net.cpp:530] loss -> loss
I1002 20:39:29.184844 31268 net.cpp:245] Setting up loss
I1002 20:39:29.184854 31268 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I1002 20:39:29.184857 31268 net.cpp:256]     with loss weight 1
I1002 20:39:29.184867 31268 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I1002 20:39:29.184872 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.184881 31268 net.cpp:184] Created Layer accuracy/top1 (65)
I1002 20:39:29.184885 31268 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I1002 20:39:29.184891 31268 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I1002 20:39:29.184896 31268 net.cpp:530] accuracy/top1 -> accuracy/top1
I1002 20:39:29.184904 31268 net.cpp:245] Setting up accuracy/top1
I1002 20:39:29.184909 31268 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I1002 20:39:29.184913 31268 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I1002 20:39:29.184918 31268 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1002 20:39:29.184923 31268 net.cpp:184] Created Layer accuracy/top5 (66)
I1002 20:39:29.184927 31268 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I1002 20:39:29.184932 31268 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I1002 20:39:29.184937 31268 net.cpp:530] accuracy/top5 -> accuracy/top5
I1002 20:39:29.184945 31268 net.cpp:245] Setting up accuracy/top5
I1002 20:39:29.184949 31268 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I1002 20:39:29.184954 31268 net.cpp:325] accuracy/top5 does not need backward computation.
I1002 20:39:29.184958 31268 net.cpp:325] accuracy/top1 does not need backward computation.
I1002 20:39:29.184963 31268 net.cpp:323] loss needs backward computation.
I1002 20:39:29.184967 31268 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I1002 20:39:29.184973 31268 net.cpp:323] out_deconv_final_up8 needs backward computation.
I1002 20:39:29.184976 31268 net.cpp:323] out_deconv_final_up4 needs backward computation.
I1002 20:39:29.184980 31268 net.cpp:323] out_deconv_final_up2 needs backward computation.
I1002 20:39:29.184985 31268 net.cpp:323] ctx_final/relu needs backward computation.
I1002 20:39:29.184989 31268 net.cpp:323] ctx_final needs backward computation.
I1002 20:39:29.184993 31268 net.cpp:323] ctx_conv4/relu needs backward computation.
I1002 20:39:29.184998 31268 net.cpp:323] ctx_conv4/bn needs backward computation.
I1002 20:39:29.185001 31268 net.cpp:323] ctx_conv4 needs backward computation.
I1002 20:39:29.185005 31268 net.cpp:323] ctx_conv3/relu needs backward computation.
I1002 20:39:29.185009 31268 net.cpp:323] ctx_conv3/bn needs backward computation.
I1002 20:39:29.185014 31268 net.cpp:323] ctx_conv3 needs backward computation.
I1002 20:39:29.185017 31268 net.cpp:323] ctx_conv2/relu needs backward computation.
I1002 20:39:29.185021 31268 net.cpp:323] ctx_conv2/bn needs backward computation.
I1002 20:39:29.185025 31268 net.cpp:323] ctx_conv2 needs backward computation.
I1002 20:39:29.185029 31268 net.cpp:323] ctx_conv1/relu needs backward computation.
I1002 20:39:29.185034 31268 net.cpp:323] ctx_conv1/bn needs backward computation.
I1002 20:39:29.185036 31268 net.cpp:323] ctx_conv1 needs backward computation.
I1002 20:39:29.185040 31268 net.cpp:323] out3_out5_combined needs backward computation.
I1002 20:39:29.185045 31268 net.cpp:323] out3a/relu needs backward computation.
I1002 20:39:29.185050 31268 net.cpp:323] out3a/bn needs backward computation.
I1002 20:39:29.185053 31268 net.cpp:323] out3a needs backward computation.
I1002 20:39:29.185058 31268 net.cpp:323] out5a_up2 needs backward computation.
I1002 20:39:29.185068 31268 net.cpp:323] out5a/relu needs backward computation.
I1002 20:39:29.185073 31268 net.cpp:323] out5a/bn needs backward computation.
I1002 20:39:29.185077 31268 net.cpp:323] out5a needs backward computation.
I1002 20:39:29.185082 31268 net.cpp:323] res5a_branch2b/relu needs backward computation.
I1002 20:39:29.185086 31268 net.cpp:323] res5a_branch2b/bn needs backward computation.
I1002 20:39:29.185091 31268 net.cpp:323] res5a_branch2b needs backward computation.
I1002 20:39:29.185094 31268 net.cpp:323] res5a_branch2a/relu needs backward computation.
I1002 20:39:29.185098 31268 net.cpp:323] res5a_branch2a/bn needs backward computation.
I1002 20:39:29.185102 31268 net.cpp:323] res5a_branch2a needs backward computation.
I1002 20:39:29.185107 31268 net.cpp:323] pool4 needs backward computation.
I1002 20:39:29.185111 31268 net.cpp:323] res4a_branch2b/relu needs backward computation.
I1002 20:39:29.185115 31268 net.cpp:323] res4a_branch2b/bn needs backward computation.
I1002 20:39:29.185119 31268 net.cpp:323] res4a_branch2b needs backward computation.
I1002 20:39:29.185123 31268 net.cpp:323] res4a_branch2a/relu needs backward computation.
I1002 20:39:29.185127 31268 net.cpp:323] res4a_branch2a/bn needs backward computation.
I1002 20:39:29.185132 31268 net.cpp:323] res4a_branch2a needs backward computation.
I1002 20:39:29.185137 31268 net.cpp:323] pool3 needs backward computation.
I1002 20:39:29.185142 31268 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I1002 20:39:29.185145 31268 net.cpp:323] res3a_branch2b/relu needs backward computation.
I1002 20:39:29.185149 31268 net.cpp:323] res3a_branch2b/bn needs backward computation.
I1002 20:39:29.185153 31268 net.cpp:323] res3a_branch2b needs backward computation.
I1002 20:39:29.185158 31268 net.cpp:323] res3a_branch2a/relu needs backward computation.
I1002 20:39:29.185163 31268 net.cpp:323] res3a_branch2a/bn needs backward computation.
I1002 20:39:29.185166 31268 net.cpp:323] res3a_branch2a needs backward computation.
I1002 20:39:29.185171 31268 net.cpp:323] pool2 needs backward computation.
I1002 20:39:29.185175 31268 net.cpp:323] res2a_branch2b/relu needs backward computation.
I1002 20:39:29.185179 31268 net.cpp:323] res2a_branch2b/bn needs backward computation.
I1002 20:39:29.185184 31268 net.cpp:323] res2a_branch2b needs backward computation.
I1002 20:39:29.185187 31268 net.cpp:323] res2a_branch2a/relu needs backward computation.
I1002 20:39:29.185191 31268 net.cpp:323] res2a_branch2a/bn needs backward computation.
I1002 20:39:29.185196 31268 net.cpp:323] res2a_branch2a needs backward computation.
I1002 20:39:29.185200 31268 net.cpp:323] pool1 needs backward computation.
I1002 20:39:29.185204 31268 net.cpp:323] conv1b/relu needs backward computation.
I1002 20:39:29.185209 31268 net.cpp:323] conv1b/bn needs backward computation.
I1002 20:39:29.185214 31268 net.cpp:323] conv1b needs backward computation.
I1002 20:39:29.185217 31268 net.cpp:323] conv1a/relu needs backward computation.
I1002 20:39:29.185221 31268 net.cpp:323] conv1a/bn needs backward computation.
I1002 20:39:29.185225 31268 net.cpp:323] conv1a needs backward computation.
I1002 20:39:29.185230 31268 net.cpp:325] data/bias does not need backward computation.
I1002 20:39:29.185235 31268 net.cpp:325] label_data_1_split does not need backward computation.
I1002 20:39:29.185240 31268 net.cpp:325] data does not need backward computation.
I1002 20:39:29.185243 31268 net.cpp:367] This network produces output accuracy/top1
I1002 20:39:29.185247 31268 net.cpp:367] This network produces output accuracy/top5
I1002 20:39:29.185252 31268 net.cpp:367] This network produces output loss
I1002 20:39:29.185299 31268 net.cpp:389] Top memory (TEST) required for data: 566886424 diff: 566886424
I1002 20:39:29.185303 31268 net.cpp:392] Bottom memory (TEST) required for data: 566886400 diff: 566886400
I1002 20:39:29.185307 31268 net.cpp:395] Shared (in-place) memory (TEST) by data: 257638400 diff: 257638400
I1002 20:39:29.185310 31268 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1002 20:39:29.185318 31268 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I1002 20:39:29.185323 31268 net.cpp:407] Network initialization done.
I1002 20:39:29.185400 31268 solver.cpp:57] Solver scaffolding done.
I1002 20:39:29.194872 31268 caffe.cpp:143] Finetuning from training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I1002 20:39:29.446904 31268 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I1002 20:39:29.446924 31268 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1002 20:39:29.446955 31268 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1002 20:39:29.446986 31268 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.447351 31268 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1002 20:39:29.447360 31268 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1002 20:39:29.447376 31268 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.447589 31268 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1002 20:39:29.447597 31268 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1002 20:39:29.447600 31268 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.447625 31268 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.447844 31268 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.447851 31268 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.447872 31268 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.448076 31268 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.448082 31268 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1002 20:39:29.448086 31268 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.448141 31268 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.448338 31268 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.448346 31268 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.448384 31268 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.448552 31268 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.448559 31268 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1002 20:39:29.448562 31268 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1002 20:39:29.448566 31268 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.448720 31268 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.448876 31268 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.448884 31268 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.448984 31268 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.449137 31268 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.449144 31268 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1002 20:39:29.449148 31268 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.449872 31268 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.450095 31268 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.450103 31268 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.450397 31268 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.450623 31268 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.450645 31268 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1002 20:39:29.450736 31268 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.450986 31268 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1002 20:39:29.451005 31268 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1002 20:39:29.451025 31268 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1002 20:39:29.451062 31268 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.451287 31268 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1002 20:39:29.451304 31268 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1002 20:39:29.451316 31268 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1002 20:39:29.451351 31268 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1002 20:39:29.451572 31268 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1002 20:39:29.451589 31268 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1002 20:39:29.451629 31268 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1002 20:39:29.451858 31268 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1002 20:39:29.451875 31268 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1002 20:39:29.451920 31268 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1002 20:39:29.452142 31268 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1002 20:39:29.452158 31268 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1002 20:39:29.452203 31268 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1002 20:39:29.452427 31268 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1002 20:39:29.452445 31268 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1002 20:39:29.452467 31268 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1002 20:39:29.452478 31268 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1002 20:39:29.452497 31268 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1002 20:39:29.452513 31268 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1002 20:39:29.452529 31268 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I1002 20:39:29.460031 31268 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I1002 20:39:29.460086 31268 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I1002 20:39:29.460125 31268 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I1002 20:39:29.460150 31268 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.460521 31268 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1002 20:39:29.460541 31268 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I1002 20:39:29.460562 31268 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.460798 31268 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1002 20:39:29.460814 31268 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I1002 20:39:29.460824 31268 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.460855 31268 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.461087 31268 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.461105 31268 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.461130 31268 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.461349 31268 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.461371 31268 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I1002 20:39:29.461381 31268 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.461436 31268 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.461644 31268 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.461665 31268 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.461704 31268 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.461885 31268 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.461900 31268 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1002 20:39:29.461913 31268 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I1002 20:39:29.461923 31268 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.462115 31268 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.462316 31268 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.462333 31268 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.462421 31268 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.462558 31268 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.462563 31268 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I1002 20:39:29.462566 31268 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1002 20:39:29.463114 31268 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.463246 31268 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1002 20:39:29.463253 31268 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1002 20:39:29.463534 31268 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1002 20:39:29.463662 31268 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1002 20:39:29.463668 31268 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I1002 20:39:29.463735 31268 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.463902 31268 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I1002 20:39:29.463908 31268 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1002 20:39:29.463917 31268 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I1002 20:39:29.463946 31268 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1002 20:39:29.464103 31268 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I1002 20:39:29.464109 31268 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1002 20:39:29.464112 31268 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1002 20:39:29.464146 31268 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1002 20:39:29.464313 31268 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1002 20:39:29.464318 31268 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1002 20:39:29.464347 31268 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1002 20:39:29.464504 31268 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1002 20:39:29.464509 31268 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1002 20:39:29.464540 31268 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1002 20:39:29.464694 31268 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1002 20:39:29.464700 31268 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1002 20:39:29.464731 31268 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1002 20:39:29.464896 31268 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1002 20:39:29.464901 31268 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I1002 20:39:29.464916 31268 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1002 20:39:29.464921 31268 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1002 20:39:29.464929 31268 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1002 20:39:29.464938 31268 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1002 20:39:29.464948 31268 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I1002 20:39:29.465055 31268 parallel.cpp:106] [0 - 0] P2pSync adding callback
I1002 20:39:29.465061 31268 parallel.cpp:106] [1 - 1] P2pSync adding callback
I1002 20:39:29.465065 31268 parallel.cpp:106] [2 - 2] P2pSync adding callback
I1002 20:39:29.465070 31268 parallel.cpp:59] Starting Optimization
I1002 20:39:29.465073 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1002 20:39:29.465103 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:29.465121 31268 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:29.465855 31421 device_alternate.hpp:116] NVML initialized on thread 135633349797632
I1002 20:39:29.517819 31421 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I1002 20:39:29.517850 31422 device_alternate.hpp:116] NVML initialized on thread 135633341404928
I1002 20:39:29.518636 31422 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I1002 20:39:29.518653 31423 device_alternate.hpp:116] NVML initialized on thread 135633333012224
I1002 20:39:29.521287 31423 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I1002 20:39:29.524334 31422 solver.cpp:43] Solver data type: FLOAT
W1002 20:39:29.525279 31422 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I1002 20:39:29.525462 31422 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:29.525470 31422 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:29.525506 31422 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:29.525516 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:29.528940 31423 solver.cpp:43] Solver data type: FLOAT
W1002 20:39:29.529541 31423 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I1002 20:39:29.529697 31423 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:29.529705 31423 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:29.529748 31423 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:29.529759 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:29.530220 31424 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I1002 20:39:29.554054 31425 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I1002 20:39:29.640635 31422 data_layer.cpp:187] [1] ReshapePrefetch 6, 3, 640, 640
I1002 20:39:29.640743 31422 data_layer.cpp:211] [1] Output data size: 6, 3, 640, 640
I1002 20:39:29.640750 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:29.666513 31422 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:29.666539 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:29.680848 31423 data_layer.cpp:187] [2] ReshapePrefetch 6, 3, 640, 640
I1002 20:39:29.681087 31423 data_layer.cpp:211] [2] Output data size: 6, 3, 640, 640
I1002 20:39:29.681116 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:29.681885 31423 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I1002 20:39:29.681900 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:29.682241 31429 data_layer.cpp:101] [1] Parser threads: 1
I1002 20:39:29.682265 31429 data_layer.cpp:103] [1] Transformer threads: 1
I1002 20:39:29.684353 31430 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I1002 20:39:29.685217 31431 data_layer.cpp:101] [2] Parser threads: 1
I1002 20:39:29.685233 31431 data_layer.cpp:103] [2] Transformer threads: 1
I1002 20:39:29.693368 31422 data_layer.cpp:187] [1] ReshapePrefetch 6, 1, 640, 640
I1002 20:39:29.703274 31432 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I1002 20:39:29.706501 31422 data_layer.cpp:211] [1] Output data size: 6, 1, 640, 640
I1002 20:39:29.706535 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:29.707566 31423 data_layer.cpp:187] [2] ReshapePrefetch 6, 1, 640, 640
I1002 20:39:29.707680 31423 data_layer.cpp:211] [2] Output data size: 6, 1, 640, 640
I1002 20:39:29.707697 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:29.795120 31433 data_layer.cpp:101] [1] Parser threads: 1
I1002 20:39:29.795482 31433 data_layer.cpp:103] [1] Transformer threads: 1
I1002 20:39:29.819715 31434 data_layer.cpp:101] [2] Parser threads: 1
I1002 20:39:29.819743 31434 data_layer.cpp:103] [2] Transformer threads: 1
I1002 20:39:34.007488 31423 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W1002 20:39:34.007719 31423 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I1002 20:39:34.007885 31423 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:34.007892 31423 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:34.007922 31423 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:34.007930 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:34.008786 31536 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I1002 20:39:34.012992 31423 data_layer.cpp:187] (2) ReshapePrefetch 2, 3, 640, 640
I1002 20:39:34.013294 31423 data_layer.cpp:211] (2) Output data size: 2, 3, 640, 640
I1002 20:39:34.013403 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:34.015105 31423 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:34.015215 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:34.024544 31537 data_layer.cpp:101] (2) Parser threads: 1
I1002 20:39:34.024760 31537 data_layer.cpp:103] (2) Transformer threads: 1
I1002 20:39:34.068455 31538 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I1002 20:39:34.069598 31423 data_layer.cpp:187] (2) ReshapePrefetch 2, 1, 640, 640
I1002 20:39:34.069710 31423 data_layer.cpp:211] (2) Output data size: 2, 1, 640, 640
I1002 20:39:34.069716 31423 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I1002 20:39:34.071197 31422 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W1002 20:39:34.071307 31422 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I1002 20:39:34.071478 31422 net.cpp:104] Using FLOAT as default forward math type
I1002 20:39:34.071498 31422 net.cpp:110] Using FLOAT as default backward math type
I1002 20:39:34.071542 31422 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:34.071559 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:34.112403 31539 data_layer.cpp:101] (2) Parser threads: 1
I1002 20:39:34.112437 31539 data_layer.cpp:103] (2) Transformer threads: 1
I1002 20:39:34.192697 31540 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I1002 20:39:34.272080 31422 data_layer.cpp:187] (1) ReshapePrefetch 2, 3, 640, 640
I1002 20:39:34.272239 31422 data_layer.cpp:211] (1) Output data size: 2, 3, 640, 640
I1002 20:39:34.272248 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:34.326313 31422 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I1002 20:39:34.326344 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:34.394381 31541 data_layer.cpp:101] (1) Parser threads: 1
I1002 20:39:34.394868 31541 data_layer.cpp:103] (1) Transformer threads: 1
I1002 20:39:34.395318 31542 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I1002 20:39:34.405692 31422 data_layer.cpp:187] (1) ReshapePrefetch 2, 1, 640, 640
I1002 20:39:34.405781 31422 data_layer.cpp:211] (1) Output data size: 2, 1, 640, 640
I1002 20:39:34.405788 31422 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I1002 20:39:34.416015 31543 data_layer.cpp:101] (1) Parser threads: 1
I1002 20:39:34.416085 31543 data_layer.cpp:103] (1) Transformer threads: 1
I1002 20:39:34.514487 31423 solver.cpp:57] Solver scaffolding done.
I1002 20:39:34.559327 31422 solver.cpp:57] Solver scaffolding done.
I1002 20:39:34.598659 31422 parallel.cpp:161] [1 - 1] P2pSync adding callback
I1002 20:39:34.598659 31423 parallel.cpp:161] [2 - 2] P2pSync adding callback
I1002 20:39:34.598659 31421 parallel.cpp:161] [0 - 0] P2pSync adding callback
I1002 20:39:35.190225 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 20:39:35.215821 31421 solver.cpp:494] Solving jsegnet21v2_train
I1002 20:39:35.215850 31421 solver.cpp:495] Learning Rate Policy: multistep
I1002 20:39:35.226992 31423 solver.cpp:494] Solving jsegnet21v2_train
I1002 20:39:35.227016 31423 solver.cpp:495] Learning Rate Policy: multistep
I1002 20:39:35.229712 31422 solver.cpp:494] Solving jsegnet21v2_train
I1002 20:39:35.229723 31422 solver.cpp:495] Learning Rate Policy: multistep
I1002 20:39:35.242041 31422 net.cpp:1412] [1] Reserving 10800128 bytes of shared learnable space
I1002 20:39:35.245363 31423 net.cpp:1412] [2] Reserving 10800128 bytes of shared learnable space
I1002 20:39:35.246407 31421 net.cpp:1412] [0] Reserving 10800128 bytes of shared learnable space
I1002 20:39:35.256633 31421 solver.cpp:228] Starting Optimization on GPU 0
I1002 20:39:35.256633 31423 solver.cpp:228] Starting Optimization on GPU 2
I1002 20:39:35.256930 31421 solver.cpp:567] Iteration 0, Testing net (#0)
I1002 20:39:35.257025 31573 device_alternate.hpp:116] NVML initialized on thread 127633173145344
I1002 20:39:35.257140 31573 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I1002 20:39:35.256633 31422 solver.cpp:228] Starting Optimization on GPU 1
I1002 20:39:35.260202 31572 device_alternate.hpp:116] NVML initialized on thread 127633181538048
I1002 20:39:35.264197 31572 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I1002 20:39:35.265702 31574 device_alternate.hpp:116] NVML initialized on thread 127633164752640
I1002 20:39:35.265730 31574 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I1002 20:39:35.809000 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.873999
I1002 20:39:35.809027 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 20:39:35.809036 31421 solver.cpp:659]     Test net output #2: loss = 0.499414 (* 1 = 0.499414 loss)
I1002 20:39:35.809041 31421 solver.cpp:255] [MultiGPU] Initial Test completed
I1002 20:39:36.755302 31421 solver.cpp:319] Iteration 0 (0.946112 s), loss = 0.0710956
I1002 20:39:36.755336 31421 solver.cpp:336]     Train net output #0: loss = 0.0710956 (* 1 = 0.0710956 loss)
I1002 20:39:36.755342 31421 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I1002 20:39:37.148241 31421 solver.cpp:319] Iteration 1 (0.392783 s), loss = 0.0594584
I1002 20:39:37.148288 31421 solver.cpp:336]     Train net output #0: loss = 0.0594584 (* 1 = 0.0594584 loss)
I1002 20:39:37.312466 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.73G 3/1 1 0 3 	(avail 0.02G, req 0G)	t: 0 3.04 2.71
I1002 20:39:37.324694 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.81G 3/1 1 0 3 	(avail 0.01G, req 0G)	t: 0 3.21 2.88
I1002 20:39:37.333498 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.81G 3/1 1 0 3 	(avail 0.01G, req 0G)	t: 0 3.45 3.01
I1002 20:39:37.458467 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.73G 32/4 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.6 1.27
I1002 20:39:37.469532 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.81G 32/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.64 1.31
I1002 20:39:37.473783 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.81G 32/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.7 1.34
I1002 20:39:37.707696 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.81G 32/1 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.71 1.56
I1002 20:39:37.715904 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.81G 32/1 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.73 1.61
I1002 20:39:37.752449 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.73G 32/1 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.73 1.5
I1002 20:39:37.818733 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.81G 64/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.27 0.65
I1002 20:39:37.822177 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.81G 64/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.27 0.71
I1002 20:39:37.835873 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.73G 64/4 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.27 0.64
I1002 20:39:38.005239 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.81G 64/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.45 0.92
I1002 20:39:38.013523 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.73G 64/1 6 4 5 	(avail 0.02G, req 0.07G)	t: 0 0.45 0.92
I1002 20:39:38.016139 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.81G 64/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.48 0.97
I1002 20:39:38.065590 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.81G 128/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.13 0.29
I1002 20:39:38.074641 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.73G 128/4 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.13 0.27
I1002 20:39:38.081337 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.81G 128/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.14 0.3
I1002 20:39:38.232183 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.81G 128/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.47 0.53
I1002 20:39:38.248867 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.73G 128/1 6 4 5 	(avail 0.02G, req 0.07G)	t: 0 0.45 0.54
I1002 20:39:38.249001 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.81G 128/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.51 0.56
I1002 20:39:38.286377 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.81G 256/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.1 0.21
I1002 20:39:38.286748 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.73G 256/4 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.1 0.19
I1002 20:39:38.291095 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.81G 256/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.1 0.19
I1002 20:39:38.405284 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'out3a' with space 1.73G 128/2 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.21 0.4
I1002 20:39:38.416321 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'out3a' with space 1.81G 128/2 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.22 0.41
I1002 20:39:38.417562 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'out3a' with space 1.81G 128/2 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.23 0.43
I1002 20:39:38.512809 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.73G 64/1 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.3 0.6
I1002 20:39:38.515419 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.81G 64/1 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.28 0.63
I1002 20:39:38.522218 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.81G 64/1 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.3 0.66
I1002 20:39:38.569267 31421 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 1.73G 64/1 6 1 3 	(avail 0.02G, req 0.07G)	t: 0 0.1 0.35
I1002 20:39:38.578742 31422 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 1.81G 64/1 6 1 5 	(avail 0.01G, req 0.07G)	t: 0 0.1 0.34
I1002 20:39:38.590301 31423 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 1.81G 64/1 6 1 5 	(avail 0.01G, req 0.07G)	t: 0 0.11 0.35
I1002 20:39:38.895213 31421 solver.cpp:319] Iteration 2 (1.74692 s), loss = 0.052284
I1002 20:39:38.895282 31421 solver.cpp:336]     Train net output #0: loss = 0.052284 (* 1 = 0.052284 loss)
I1002 20:39:38.895977 31423 cudnn_conv_layer.cpp:474] [2] Layer 'conv1a' reallocating workspace 1.81G to 0.14G
I1002 20:39:38.898314 31421 cudnn_conv_layer.cpp:474] [0] Layer 'conv1a' reallocating workspace 1.73G to 0.14G
I1002 20:39:38.899442 31422 cudnn_conv_layer.cpp:474] [1] Layer 'conv1a' reallocating workspace 1.81G to 0.14G
I1002 20:39:40.137596 31423 blocking_queue.cpp:40] Data layer prefetch queue empty
I1002 20:40:41.272300 31421 solver.cpp:314] Iteration 100 (1.57114 iter/s, 62.3753s/98 iter), loss = 0.063928
I1002 20:40:41.272404 31421 solver.cpp:336]     Train net output #0: loss = 0.063928 (* 1 = 0.063928 loss)
I1002 20:40:41.272413 31421 sgd_solver.cpp:136] Iteration 100, lr = 0.01, m = 0.9
I1002 20:41:20.754662 31430 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 20:41:42.066395 31421 solver.cpp:314] Iteration 200 (1.64497 iter/s, 60.7915s/100 iter), loss = 0.142457
I1002 20:41:42.066463 31421 solver.cpp:336]     Train net output #0: loss = 0.142457 (* 1 = 0.142457 loss)
I1002 20:41:42.066473 31421 sgd_solver.cpp:136] Iteration 200, lr = 0.01, m = 0.9
I1002 20:42:37.985242 31421 solver.cpp:314] Iteration 300 (1.78836 iter/s, 55.9172s/100 iter), loss = 0.145339
I1002 20:42:37.989224 31421 solver.cpp:336]     Train net output #0: loss = 0.145339 (* 1 = 0.145339 loss)
I1002 20:42:37.989244 31421 sgd_solver.cpp:136] Iteration 300, lr = 0.01, m = 0.9
I1002 20:42:53.539528 31342 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 20:43:32.312193 31421 solver.cpp:314] Iteration 400 (1.84076 iter/s, 54.3254s/100 iter), loss = 0.115784
I1002 20:43:32.312268 31421 solver.cpp:336]     Train net output #0: loss = 0.115784 (* 1 = 0.115784 loss)
I1002 20:43:32.312280 31421 sgd_solver.cpp:136] Iteration 400, lr = 0.01, m = 0.9
I1002 20:44:25.436292 31421 solver.cpp:314] Iteration 500 (1.88244 iter/s, 53.1226s/100 iter), loss = 0.103379
I1002 20:44:25.436380 31421 solver.cpp:336]     Train net output #0: loss = 0.103379 (* 1 = 0.103379 loss)
I1002 20:44:25.436393 31421 sgd_solver.cpp:136] Iteration 500, lr = 0.01, m = 0.9
I1002 20:45:18.315623 31421 solver.cpp:314] Iteration 600 (1.8912 iter/s, 52.8764s/100 iter), loss = 0.0751301
I1002 20:45:18.320238 31421 solver.cpp:336]     Train net output #0: loss = 0.07513 (* 1 = 0.07513 loss)
I1002 20:45:18.320262 31421 sgd_solver.cpp:136] Iteration 600, lr = 0.01, m = 0.9
I1002 20:45:49.106904 31344 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 20:46:11.281213 31421 solver.cpp:314] Iteration 700 (1.88808 iter/s, 52.9638s/100 iter), loss = 0.097927
I1002 20:46:11.281275 31421 solver.cpp:336]     Train net output #0: loss = 0.097927 (* 1 = 0.097927 loss)
I1002 20:46:11.281289 31421 sgd_solver.cpp:136] Iteration 700, lr = 0.01, m = 0.9
I1002 20:47:03.789427 31421 solver.cpp:314] Iteration 800 (1.9046 iter/s, 52.5046s/100 iter), loss = 0.2027
I1002 20:47:03.789593 31421 solver.cpp:336]     Train net output #0: loss = 0.2027 (* 1 = 0.2027 loss)
I1002 20:47:03.789613 31421 sgd_solver.cpp:136] Iteration 800, lr = 0.01, m = 0.9
I1002 20:47:57.209877 31421 solver.cpp:314] Iteration 900 (1.87199 iter/s, 53.4189s/100 iter), loss = 0.0807888
I1002 20:47:57.212236 31421 solver.cpp:336]     Train net output #0: loss = 0.0807888 (* 1 = 0.0807888 loss)
I1002 20:47:57.212255 31421 sgd_solver.cpp:136] Iteration 900, lr = 0.01, m = 0.9
I1002 20:48:44.344024 31432 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 20:48:49.839756 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.6 sparsity_achieved=0 iter=1000
I1002 21:02:07.988430 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 21:02:08.026155 31421 solver.cpp:352] Sparsity after update:
I1002 21:02:08.028842 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 21:02:08.028888 31421 net.cpp:2312] conv1a_param_0(0.245) 
I1002 21:02:08.028908 31421 net.cpp:2312] conv1b_param_0(0.583) 
I1002 21:02:08.028930 31421 net.cpp:2312] ctx_conv1_param_0(0.599) 
I1002 21:02:08.028940 31421 net.cpp:2312] ctx_conv2_param_0(0.599) 
I1002 21:02:08.028951 31421 net.cpp:2312] ctx_conv3_param_0(0.599) 
I1002 21:02:08.028961 31421 net.cpp:2312] ctx_conv4_param_0(0.599) 
I1002 21:02:08.028971 31421 net.cpp:2312] ctx_final_param_0(0.29) 
I1002 21:02:08.028981 31421 net.cpp:2312] out3a_param_0(0.599) 
I1002 21:02:08.028990 31421 net.cpp:2312] out5a_param_0(0.6) 
I1002 21:02:08.029000 31421 net.cpp:2312] res2a_branch2a_param_0(0.597) 
I1002 21:02:08.029009 31421 net.cpp:2312] res2a_branch2b_param_0(0.569) 
I1002 21:02:08.029019 31421 net.cpp:2312] res3a_branch2a_param_0(0.599) 
I1002 21:02:08.029029 31421 net.cpp:2312] res3a_branch2b_param_0(0.597) 
I1002 21:02:08.029039 31421 net.cpp:2312] res4a_branch2a_param_0(0.6) 
I1002 21:02:08.029049 31421 net.cpp:2312] res4a_branch2b_param_0(0.599) 
I1002 21:02:08.029060 31421 net.cpp:2312] res5a_branch2a_param_0(0.599) 
I1002 21:02:08.029070 31421 net.cpp:2312] res5a_branch2b_param_0(0.6) 
I1002 21:02:08.029080 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.60974e+06/2.69117e+06) 0.598
I1002 21:02:08.685737 31421 solver.cpp:314] Iteration 1000 (0.117446 iter/s, 851.452s/100 iter), loss = 0.0890882
I1002 21:02:08.685770 31421 solver.cpp:336]     Train net output #0: loss = 0.0890881 (* 1 = 0.0890881 loss)
I1002 21:02:08.685776 31421 sgd_solver.cpp:136] Iteration 1000, lr = 0.01, m = 0.9
I1002 21:03:03.469180 31421 solver.cpp:314] Iteration 1100 (1.82542 iter/s, 54.7819s/100 iter), loss = 0.0626409
I1002 21:03:03.469290 31421 solver.cpp:336]     Train net output #0: loss = 0.0626409 (* 1 = 0.0626409 loss)
I1002 21:03:03.469300 31421 sgd_solver.cpp:136] Iteration 1100, lr = 0.01, m = 0.9
I1002 21:03:33.198217 31424 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 21:03:33.198217 31342 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 21:03:58.025604 31421 solver.cpp:314] Iteration 1200 (1.83302 iter/s, 54.5549s/100 iter), loss = 0.0712862
I1002 21:03:58.028165 31421 solver.cpp:336]     Train net output #0: loss = 0.0712861 (* 1 = 0.0712861 loss)
I1002 21:03:58.028179 31421 sgd_solver.cpp:136] Iteration 1200, lr = 0.01, m = 0.9
I1002 21:04:46.730937 31429 blocking_queue.cpp:40] Waiting for datum
I1002 21:04:52.758972 31421 solver.cpp:314] Iteration 1300 (1.82709 iter/s, 54.7318s/100 iter), loss = 0.0791404
I1002 21:04:52.759037 31421 solver.cpp:336]     Train net output #0: loss = 0.0791403 (* 1 = 0.0791403 loss)
I1002 21:04:52.759045 31421 sgd_solver.cpp:136] Iteration 1300, lr = 0.01, m = 0.9
I1002 21:05:49.072126 31421 solver.cpp:314] Iteration 1400 (1.77583 iter/s, 56.3116s/100 iter), loss = 0.100286
I1002 21:05:49.072199 31421 solver.cpp:336]     Train net output #0: loss = 0.100286 (* 1 = 0.100286 loss)
I1002 21:05:49.072209 31421 sgd_solver.cpp:136] Iteration 1400, lr = 0.01, m = 0.9
I1002 21:06:37.848804 31425 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 21:06:45.451174 31421 solver.cpp:314] Iteration 1500 (1.77376 iter/s, 56.3774s/100 iter), loss = 0.0974504
I1002 21:06:45.451210 31421 solver.cpp:336]     Train net output #0: loss = 0.0974503 (* 1 = 0.0974503 loss)
I1002 21:06:45.451216 31421 sgd_solver.cpp:136] Iteration 1500, lr = 0.01, m = 0.9
I1002 21:07:38.149060 31421 solver.cpp:314] Iteration 1600 (1.89766 iter/s, 52.6964s/100 iter), loss = 0.068044
I1002 21:07:38.156235 31421 solver.cpp:336]     Train net output #0: loss = 0.0680439 (* 1 = 0.0680439 loss)
I1002 21:07:38.156256 31421 sgd_solver.cpp:136] Iteration 1600, lr = 0.01, m = 0.9
I1002 21:08:30.127408 31421 solver.cpp:314] Iteration 1700 (1.92393 iter/s, 51.9769s/100 iter), loss = 0.09177
I1002 21:08:30.127470 31421 solver.cpp:336]     Train net output #0: loss = 0.0917699 (* 1 = 0.0917699 loss)
I1002 21:08:30.127478 31421 sgd_solver.cpp:136] Iteration 1700, lr = 0.01, m = 0.9
I1002 21:09:24.055439 31421 solver.cpp:314] Iteration 1800 (1.85438 iter/s, 53.9265s/100 iter), loss = 0.0592322
I1002 21:09:24.055640 31421 solver.cpp:336]     Train net output #0: loss = 0.0592321 (* 1 = 0.0592321 loss)
I1002 21:09:24.055676 31421 sgd_solver.cpp:136] Iteration 1800, lr = 0.01, m = 0.9
I1002 21:09:32.114604 31430 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 21:10:16.931982 31421 solver.cpp:314] Iteration 1900 (1.89125 iter/s, 52.8751s/100 iter), loss = 0.0500359
I1002 21:10:16.932327 31421 solver.cpp:336]     Train net output #0: loss = 0.0500358 (* 1 = 0.0500358 loss)
I1002 21:10:16.932337 31421 sgd_solver.cpp:136] Iteration 1900, lr = 0.01, m = 0.9
I1002 21:11:09.725817 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.61 sparsity_achieved=0.598157 iter=2000
I1002 21:11:10.079022 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.079023 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.142076 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.146473 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.172902 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.190946 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.192487 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.205693 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.208308 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.217649 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.222846 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.244474 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.245908 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.277895 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.281034 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.292338 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.300200 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.302217 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.381901 31423 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.398999 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.433099 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:10.504215 31422 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I1002 21:11:28.280285 31538 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 21:19:05.385181 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 21:19:05.390626 31421 solver.cpp:352] Sparsity after update:
I1002 21:19:05.393062 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 21:19:05.393201 31421 net.cpp:2312] conv1a_param_0(0.237) 
I1002 21:19:05.393271 31421 net.cpp:2312] conv1b_param_0(0.586) 
I1002 21:19:05.393323 31421 net.cpp:2312] ctx_conv1_param_0(0.609) 
I1002 21:19:05.393335 31421 net.cpp:2312] ctx_conv2_param_0(0.609) 
I1002 21:19:05.393347 31421 net.cpp:2312] ctx_conv3_param_0(0.609) 
I1002 21:19:05.393357 31421 net.cpp:2312] ctx_conv4_param_0(0.609) 
I1002 21:19:05.393366 31421 net.cpp:2312] ctx_final_param_0(0.226) 
I1002 21:19:05.393375 31421 net.cpp:2312] out3a_param_0(0.609) 
I1002 21:19:05.393386 31421 net.cpp:2312] out5a_param_0(0.61) 
I1002 21:19:05.393398 31421 net.cpp:2312] res2a_branch2a_param_0(0.608) 
I1002 21:19:05.393409 31421 net.cpp:2312] res2a_branch2b_param_0(0.583) 
I1002 21:19:05.393421 31421 net.cpp:2312] res3a_branch2a_param_0(0.609) 
I1002 21:19:05.393534 31421 net.cpp:2312] res3a_branch2b_param_0(0.608) 
I1002 21:19:05.393548 31421 net.cpp:2312] res4a_branch2a_param_0(0.609) 
I1002 21:19:05.393560 31421 net.cpp:2312] res4a_branch2b_param_0(0.609) 
I1002 21:19:05.393573 31421 net.cpp:2312] res5a_branch2a_param_0(0.609) 
I1002 21:19:05.393584 31421 net.cpp:2312] res5a_branch2b_param_0(0.609) 
I1002 21:19:05.393596 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.63604e+06/2.69117e+06) 0.608
I1002 21:19:05.393622 31421 solver.cpp:567] Iteration 2000, Testing net (#0)
I1002 21:19:05.598947 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.653990 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.673444 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.685591 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.705273 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.721501 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.732069 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.739078 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.822991 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.841473 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:05.890893 31421 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.6G, req 0G)	t: 0
I1002 21:19:29.872684 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.951985
I1002 21:19:29.872740 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 21:19:29.872758 31421 solver.cpp:659]     Test net output #2: loss = 0.15229 (* 1 = 0.15229 loss)
I1002 21:19:29.872802 31421 solver.cpp:265] [MultiGPU] Tests completed in 24.4785s
I1002 21:19:30.305235 31421 solver.cpp:314] Iteration 2000 (0.180715 iter/s, 553.358s/100 iter), loss = 0.0631281
I1002 21:19:30.305266 31421 solver.cpp:336]     Train net output #0: loss = 0.063128 (* 1 = 0.063128 loss)
I1002 21:19:30.305274 31421 sgd_solver.cpp:136] Iteration 2000, lr = 0.01, m = 0.9
I1002 21:20:26.372206 31421 solver.cpp:314] Iteration 2100 (1.78363 iter/s, 56.0654s/100 iter), loss = 0.054616
I1002 21:20:26.378233 31421 solver.cpp:336]     Train net output #0: loss = 0.054616 (* 1 = 0.054616 loss)
I1002 21:20:26.378248 31421 sgd_solver.cpp:136] Iteration 2100, lr = 0.01, m = 0.9
I1002 21:20:52.103015 31425 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 21:21:22.316742 31421 solver.cpp:314] Iteration 2200 (1.78753 iter/s, 55.943s/100 iter), loss = 0.113002
I1002 21:21:22.316897 31421 solver.cpp:336]     Train net output #0: loss = 0.113002 (* 1 = 0.113002 loss)
I1002 21:21:22.316906 31421 sgd_solver.cpp:136] Iteration 2200, lr = 0.01, m = 0.9
I1002 21:22:19.539046 31421 solver.cpp:314] Iteration 2300 (1.74762 iter/s, 57.2207s/100 iter), loss = 0.0601642
I1002 21:22:19.539170 31421 solver.cpp:336]     Train net output #0: loss = 0.0601642 (* 1 = 0.0601642 loss)
I1002 21:22:19.539213 31421 sgd_solver.cpp:136] Iteration 2300, lr = 0.01, m = 0.9
I1002 21:23:16.175799 31421 solver.cpp:314] Iteration 2400 (1.76569 iter/s, 56.6352s/100 iter), loss = 0.0622001
I1002 21:23:16.182986 31421 solver.cpp:336]     Train net output #0: loss = 0.0622 (* 1 = 0.0622 loss)
I1002 21:23:16.183007 31421 sgd_solver.cpp:136] Iteration 2400, lr = 0.01, m = 0.9
I1002 21:23:59.974289 31425 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 21:24:13.100210 31421 solver.cpp:314] Iteration 2500 (1.75677 iter/s, 56.9228s/100 iter), loss = 0.0800639
I1002 21:24:13.100251 31421 solver.cpp:336]     Train net output #0: loss = 0.0800639 (* 1 = 0.0800639 loss)
I1002 21:24:13.100260 31421 sgd_solver.cpp:136] Iteration 2500, lr = 0.01, m = 0.9
I1002 21:24:25.389211 31431 blocking_queue.cpp:40] Waiting for datum
I1002 21:25:05.430486 31421 solver.cpp:314] Iteration 2600 (1.91099 iter/s, 52.3288s/100 iter), loss = 0.0690568
I1002 21:25:05.430577 31421 solver.cpp:336]     Train net output #0: loss = 0.0690568 (* 1 = 0.0690568 loss)
I1002 21:25:05.430583 31421 sgd_solver.cpp:136] Iteration 2600, lr = 0.01, m = 0.9
I1002 21:25:58.461700 31421 solver.cpp:314] Iteration 2700 (1.88574 iter/s, 53.0297s/100 iter), loss = 0.066033
I1002 21:25:58.461851 31421 solver.cpp:336]     Train net output #0: loss = 0.066033 (* 1 = 0.066033 loss)
I1002 21:25:58.461866 31421 sgd_solver.cpp:136] Iteration 2700, lr = 0.01, m = 0.9
I1002 21:26:50.551774 31421 solver.cpp:314] Iteration 2800 (1.91981 iter/s, 52.0886s/100 iter), loss = 0.0525053
I1002 21:26:50.551841 31421 solver.cpp:336]     Train net output #0: loss = 0.0525053 (* 1 = 0.0525053 loss)
I1002 21:26:50.551851 31421 sgd_solver.cpp:136] Iteration 2800, lr = 0.01, m = 0.9
I1002 21:26:54.394309 31342 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 21:27:36.374814 31421 solver.cpp:314] Iteration 2900 (2.18237 iter/s, 45.8217s/100 iter), loss = 0.0734701
I1002 21:27:36.374884 31421 solver.cpp:336]     Train net output #0: loss = 0.0734701 (* 1 = 0.0734701 loss)
I1002 21:27:36.374892 31421 sgd_solver.cpp:136] Iteration 2900, lr = 0.01, m = 0.9
I1002 21:28:28.786095 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.62 sparsity_achieved=0.607929 iter=3000
I1002 21:36:58.106876 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 21:36:58.135318 31421 solver.cpp:352] Sparsity after update:
I1002 21:36:58.160696 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 21:36:58.160717 31421 net.cpp:2312] conv1a_param_0(0.255) 
I1002 21:36:58.160732 31421 net.cpp:2312] conv1b_param_0(0.599) 
I1002 21:36:58.160735 31421 net.cpp:2312] ctx_conv1_param_0(0.62) 
I1002 21:36:58.160738 31421 net.cpp:2312] ctx_conv2_param_0(0.62) 
I1002 21:36:58.160742 31421 net.cpp:2312] ctx_conv3_param_0(0.62) 
I1002 21:36:58.160745 31421 net.cpp:2312] ctx_conv4_param_0(0.62) 
I1002 21:36:58.160748 31421 net.cpp:2312] ctx_final_param_0(0.266) 
I1002 21:36:58.160751 31421 net.cpp:2312] out3a_param_0(0.62) 
I1002 21:36:58.160754 31421 net.cpp:2312] out5a_param_0(0.62) 
I1002 21:36:58.160758 31421 net.cpp:2312] res2a_branch2a_param_0(0.618) 
I1002 21:36:58.160760 31421 net.cpp:2312] res2a_branch2b_param_0(0.596) 
I1002 21:36:58.160763 31421 net.cpp:2312] res3a_branch2a_param_0(0.62) 
I1002 21:36:58.160766 31421 net.cpp:2312] res3a_branch2b_param_0(0.618) 
I1002 21:36:58.160769 31421 net.cpp:2312] res4a_branch2a_param_0(0.62) 
I1002 21:36:58.160773 31421 net.cpp:2312] res4a_branch2b_param_0(0.62) 
I1002 21:36:58.160778 31421 net.cpp:2312] res5a_branch2a_param_0(0.619) 
I1002 21:36:58.160787 31421 net.cpp:2312] res5a_branch2b_param_0(0.62) 
I1002 21:36:58.160792 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.66367e+06/2.69117e+06) 0.618
I1002 21:36:58.764206 31421 solver.cpp:314] Iteration 3000 (0.177818 iter/s, 562.374s/100 iter), loss = 0.0827947
I1002 21:36:58.764240 31421 solver.cpp:336]     Train net output #0: loss = 0.0827946 (* 1 = 0.0827946 loss)
I1002 21:36:58.764246 31421 sgd_solver.cpp:136] Iteration 3000, lr = 0.01, m = 0.9
I1002 21:37:52.704159 31421 solver.cpp:314] Iteration 3100 (1.85397 iter/s, 53.9384s/100 iter), loss = 0.0501584
I1002 21:37:52.704282 31421 solver.cpp:336]     Train net output #0: loss = 0.0501584 (* 1 = 0.0501584 loss)
I1002 21:37:52.704290 31421 sgd_solver.cpp:136] Iteration 3100, lr = 0.01, m = 0.9
I1002 21:38:13.879997 31344 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 21:38:49.352212 31421 solver.cpp:314] Iteration 3200 (1.76534 iter/s, 56.6465s/100 iter), loss = 0.163056
I1002 21:38:49.356201 31421 solver.cpp:336]     Train net output #0: loss = 0.163056 (* 1 = 0.163056 loss)
I1002 21:38:49.356211 31421 sgd_solver.cpp:136] Iteration 3200, lr = 0.01, m = 0.9
I1002 21:39:47.548203 31421 solver.cpp:314] Iteration 3300 (1.71838 iter/s, 58.1944s/100 iter), loss = 0.0659468
I1002 21:39:47.548710 31421 solver.cpp:336]     Train net output #0: loss = 0.0659467 (* 1 = 0.0659467 loss)
I1002 21:39:47.548718 31421 sgd_solver.cpp:136] Iteration 3300, lr = 0.01, m = 0.9
I1002 21:40:46.644215 31421 solver.cpp:314] Iteration 3400 (1.69221 iter/s, 59.0943s/100 iter), loss = 0.0435672
I1002 21:40:46.644294 31421 solver.cpp:336]     Train net output #0: loss = 0.0435671 (* 1 = 0.0435671 loss)
I1002 21:40:46.644306 31421 sgd_solver.cpp:136] Iteration 3400, lr = 0.01, m = 0.9
I1002 21:41:26.332623 31344 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 21:41:43.481782 31421 solver.cpp:314] Iteration 3500 (1.75945 iter/s, 56.836s/100 iter), loss = 0.0837295
I1002 21:41:43.481809 31421 solver.cpp:336]     Train net output #0: loss = 0.0837294 (* 1 = 0.0837294 loss)
I1002 21:41:43.481814 31421 sgd_solver.cpp:136] Iteration 3500, lr = 0.01, m = 0.9
I1002 21:42:36.560688 31421 solver.cpp:314] Iteration 3600 (1.88404 iter/s, 53.0774s/100 iter), loss = 0.0715162
I1002 21:42:36.560758 31421 solver.cpp:336]     Train net output #0: loss = 0.0715162 (* 1 = 0.0715162 loss)
I1002 21:42:36.560765 31421 sgd_solver.cpp:136] Iteration 3600, lr = 0.01, m = 0.9
I1002 21:43:29.116205 31421 solver.cpp:314] Iteration 3700 (1.9028 iter/s, 52.554s/100 iter), loss = 0.0734255
I1002 21:43:29.116278 31421 solver.cpp:336]     Train net output #0: loss = 0.0734254 (* 1 = 0.0734254 loss)
I1002 21:43:29.116289 31421 sgd_solver.cpp:136] Iteration 3700, lr = 0.01, m = 0.9
I1002 21:44:21.112502 31344 data_reader.cpp:305] Starting prefetch of epoch 4
I1002 21:44:21.930565 31421 solver.cpp:314] Iteration 3800 (1.89348 iter/s, 52.8129s/100 iter), loss = 0.0850747
I1002 21:44:21.930606 31421 solver.cpp:336]     Train net output #0: loss = 0.0850746 (* 1 = 0.0850746 loss)
I1002 21:44:21.930613 31421 sgd_solver.cpp:136] Iteration 3800, lr = 0.01, m = 0.9
I1002 21:45:14.954794 31421 solver.cpp:314] Iteration 3900 (1.88598 iter/s, 53.0227s/100 iter), loss = 0.0595461
I1002 21:45:14.954879 31421 solver.cpp:336]     Train net output #0: loss = 0.0595461 (* 1 = 0.0595461 loss)
I1002 21:45:14.954888 31421 sgd_solver.cpp:136] Iteration 3900, lr = 0.01, m = 0.9
I1002 21:45:49.138749 31425 data_reader.cpp:305] Starting prefetch of epoch 4
I1002 21:46:08.079977 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.63 sparsity_achieved=0.618196 iter=4000
I1002 21:54:13.308444 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 21:54:13.319351 31421 solver.cpp:352] Sparsity after update:
I1002 21:54:13.333891 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 21:54:13.333910 31421 net.cpp:2312] conv1a_param_0(0.256) 
I1002 21:54:13.333931 31421 net.cpp:2312] conv1b_param_0(0.611) 
I1002 21:54:13.333935 31421 net.cpp:2312] ctx_conv1_param_0(0.628) 
I1002 21:54:13.333938 31421 net.cpp:2312] ctx_conv2_param_0(0.628) 
I1002 21:54:13.333961 31421 net.cpp:2312] ctx_conv3_param_0(0.628) 
I1002 21:54:13.333972 31421 net.cpp:2312] ctx_conv4_param_0(0.628) 
I1002 21:54:13.333982 31421 net.cpp:2312] ctx_final_param_0(0.268) 
I1002 21:54:13.333992 31421 net.cpp:2312] out3a_param_0(0.628) 
I1002 21:54:13.334003 31421 net.cpp:2312] out5a_param_0(0.63) 
I1002 21:54:13.334013 31421 net.cpp:2312] res2a_branch2a_param_0(0.628) 
I1002 21:54:13.334024 31421 net.cpp:2312] res2a_branch2b_param_0(0.603) 
I1002 21:54:13.334034 31421 net.cpp:2312] res3a_branch2a_param_0(0.628) 
I1002 21:54:13.334045 31421 net.cpp:2312] res3a_branch2b_param_0(0.628) 
I1002 21:54:13.334055 31421 net.cpp:2312] res4a_branch2a_param_0(0.629) 
I1002 21:54:13.334065 31421 net.cpp:2312] res4a_branch2b_param_0(0.628) 
I1002 21:54:13.334075 31421 net.cpp:2312] res5a_branch2a_param_0(0.629) 
I1002 21:54:13.334086 31421 net.cpp:2312] res5a_branch2b_param_0(0.629) 
I1002 21:54:13.334096 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.69022e+06/2.69117e+06) 0.628
I1002 21:54:13.334117 31421 solver.cpp:567] Iteration 4000, Testing net (#0)
I1002 21:54:20.324275 31418 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 21:54:36.242149 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.955205
I1002 21:54:36.242175 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 21:54:36.242183 31421 solver.cpp:659]     Test net output #2: loss = 0.124898 (* 1 = 0.124898 loss)
I1002 21:54:36.242218 31421 solver.cpp:265] [MultiGPU] Tests completed in 22.9075s
I1002 21:54:36.675225 31421 solver.cpp:314] Iteration 4000 (0.178029 iter/s, 561.705s/100 iter), loss = 0.0581345
I1002 21:54:36.675266 31421 solver.cpp:336]     Train net output #0: loss = 0.0581345 (* 1 = 0.0581345 loss)
I1002 21:54:36.675272 31421 sgd_solver.cpp:136] Iteration 4000, lr = 0.01, m = 0.9
I1002 21:55:33.164207 31421 solver.cpp:314] Iteration 4100 (1.77031 iter/s, 56.4874s/100 iter), loss = 0.0918057
I1002 21:55:33.165581 31421 solver.cpp:336]     Train net output #0: loss = 0.0918056 (* 1 = 0.0918056 loss)
I1002 21:55:33.165591 31421 sgd_solver.cpp:136] Iteration 4100, lr = 0.01, m = 0.9
I1002 21:56:17.512442 31343 blocking_queue.cpp:40] Waiting for datum
I1002 21:56:28.737115 31421 solver.cpp:314] Iteration 4200 (1.79949 iter/s, 55.5713s/100 iter), loss = 0.0648489
I1002 21:56:28.737156 31421 solver.cpp:336]     Train net output #0: loss = 0.0648489 (* 1 = 0.0648489 loss)
I1002 21:56:28.737164 31421 sgd_solver.cpp:136] Iteration 4200, lr = 0.01, m = 0.9
I1002 21:57:21.570055 31344 data_reader.cpp:305] Starting prefetch of epoch 5
I1002 21:57:24.723057 31421 solver.cpp:314] Iteration 4300 (1.78621 iter/s, 55.9843s/100 iter), loss = 0.0742311
I1002 21:57:24.723155 31421 solver.cpp:336]     Train net output #0: loss = 0.074231 (* 1 = 0.074231 loss)
I1002 21:57:24.723178 31421 sgd_solver.cpp:136] Iteration 4300, lr = 0.01, m = 0.9
I1002 21:58:19.738070 31421 solver.cpp:314] Iteration 4400 (1.81774 iter/s, 55.0135s/100 iter), loss = 0.0583495
I1002 21:58:19.738173 31421 solver.cpp:336]     Train net output #0: loss = 0.0583495 (* 1 = 0.0583495 loss)
I1002 21:58:19.738180 31421 sgd_solver.cpp:136] Iteration 4400, lr = 0.01, m = 0.9
I1002 21:58:53.837651 31342 data_reader.cpp:305] Starting prefetch of epoch 4
I1002 21:59:15.667407 31421 solver.cpp:314] Iteration 4500 (1.78802 iter/s, 55.9278s/100 iter), loss = 0.0498926
I1002 21:59:15.667445 31421 solver.cpp:336]     Train net output #0: loss = 0.0498926 (* 1 = 0.0498926 loss)
I1002 21:59:15.667453 31421 sgd_solver.cpp:136] Iteration 4500, lr = 0.01, m = 0.9
I1002 22:00:10.079540 31421 solver.cpp:314] Iteration 4600 (1.83788 iter/s, 54.4106s/100 iter), loss = 0.0990071
I1002 22:00:10.089859 31421 solver.cpp:336]     Train net output #0: loss = 0.0990071 (* 1 = 0.0990071 loss)
I1002 22:00:10.089876 31421 sgd_solver.cpp:136] Iteration 4600, lr = 0.01, m = 0.9
I1002 22:01:02.889549 31421 solver.cpp:314] Iteration 4700 (1.89363 iter/s, 52.8085s/100 iter), loss = 0.0739878
I1002 22:01:02.892205 31421 solver.cpp:336]     Train net output #0: loss = 0.0739878 (* 1 = 0.0739878 loss)
I1002 22:01:02.892213 31421 sgd_solver.cpp:136] Iteration 4700, lr = 0.01, m = 0.9
I1002 22:01:51.146083 31342 data_reader.cpp:305] Starting prefetch of epoch 5
I1002 22:01:56.561241 31421 solver.cpp:314] Iteration 4800 (1.86323 iter/s, 53.6702s/100 iter), loss = 0.0719998
I1002 22:01:56.561270 31421 solver.cpp:336]     Train net output #0: loss = 0.0719997 (* 1 = 0.0719997 loss)
I1002 22:01:56.561275 31421 sgd_solver.cpp:136] Iteration 4800, lr = 0.01, m = 0.9
I1002 22:02:50.208050 31421 solver.cpp:314] Iteration 4900 (1.8641 iter/s, 53.6453s/100 iter), loss = 0.0771861
I1002 22:02:50.208169 31421 solver.cpp:336]     Train net output #0: loss = 0.077186 (* 1 = 0.077186 loss)
I1002 22:02:50.208189 31421 sgd_solver.cpp:136] Iteration 4900, lr = 0.01, m = 0.9
I1002 22:03:43.025372 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.64 sparsity_achieved=0.628062 iter=5000
I1002 22:11:35.281280 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 22:11:35.290263 31421 solver.cpp:352] Sparsity after update:
I1002 22:11:35.292348 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 22:11:35.292361 31421 net.cpp:2312] conv1a_param_0(0.264) 
I1002 22:11:35.292376 31421 net.cpp:2312] conv1b_param_0(0.622) 
I1002 22:11:35.292381 31421 net.cpp:2312] ctx_conv1_param_0(0.639) 
I1002 22:11:35.292383 31421 net.cpp:2312] ctx_conv2_param_0(0.639) 
I1002 22:11:35.292400 31421 net.cpp:2312] ctx_conv3_param_0(0.639) 
I1002 22:11:35.292407 31421 net.cpp:2312] ctx_conv4_param_0(0.639) 
I1002 22:11:35.292409 31421 net.cpp:2312] ctx_final_param_0(0.314) 
I1002 22:11:35.292412 31421 net.cpp:2312] out3a_param_0(0.639) 
I1002 22:11:35.292417 31421 net.cpp:2312] out5a_param_0(0.64) 
I1002 22:11:35.292419 31421 net.cpp:2312] res2a_branch2a_param_0(0.639) 
I1002 22:11:35.292423 31421 net.cpp:2312] res2a_branch2b_param_0(0.615) 
I1002 22:11:35.292426 31421 net.cpp:2312] res3a_branch2a_param_0(0.639) 
I1002 22:11:35.292429 31421 net.cpp:2312] res3a_branch2b_param_0(0.639) 
I1002 22:11:35.292435 31421 net.cpp:2312] res4a_branch2a_param_0(0.64) 
I1002 22:11:35.292441 31421 net.cpp:2312] res4a_branch2b_param_0(0.639) 
I1002 22:11:35.292445 31421 net.cpp:2312] res5a_branch2a_param_0(0.639) 
I1002 22:11:35.292449 31421 net.cpp:2312] res5a_branch2b_param_0(0.64) 
I1002 22:11:35.292454 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.71784e+06/2.69117e+06) 0.638
I1002 22:11:35.691705 31421 solver.cpp:314] Iteration 5000 (0.190306 iter/s, 525.469s/100 iter), loss = 0.076515
I1002 22:11:35.691773 31421 solver.cpp:336]     Train net output #0: loss = 0.076515 (* 1 = 0.076515 loss)
I1002 22:11:35.691781 31421 sgd_solver.cpp:136] Iteration 5000, lr = 0.01, m = 0.9
I1002 22:12:29.746966 31421 solver.cpp:314] Iteration 5100 (1.85001 iter/s, 54.0537s/100 iter), loss = 0.104215
I1002 22:12:29.747030 31421 solver.cpp:336]     Train net output #0: loss = 0.104215 (* 1 = 0.104215 loss)
I1002 22:12:29.747036 31421 sgd_solver.cpp:136] Iteration 5100, lr = 0.01, m = 0.9
I1002 22:12:41.224153 31430 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 22:13:24.104092 31421 solver.cpp:314] Iteration 5200 (1.83974 iter/s, 54.3556s/100 iter), loss = 0.0735567
I1002 22:13:24.108261 31421 solver.cpp:336]     Train net output #0: loss = 0.0735567 (* 1 = 0.0735567 loss)
I1002 22:13:24.108281 31421 sgd_solver.cpp:136] Iteration 5200, lr = 0.01, m = 0.9
I1002 22:14:19.436652 31421 solver.cpp:314] Iteration 5300 (1.8073 iter/s, 55.331s/100 iter), loss = 0.0668591
I1002 22:14:19.464221 31421 solver.cpp:336]     Train net output #0: loss = 0.0668591 (* 1 = 0.0668591 loss)
I1002 22:14:19.464243 31421 sgd_solver.cpp:136] Iteration 5300, lr = 0.01, m = 0.9
I1002 22:15:14.248553 31421 solver.cpp:314] Iteration 5400 (1.82447 iter/s, 54.8104s/100 iter), loss = 0.0506066
I1002 22:15:14.249166 31421 solver.cpp:336]     Train net output #0: loss = 0.0506065 (* 1 = 0.0506065 loss)
I1002 22:15:14.249177 31421 sgd_solver.cpp:136] Iteration 5400, lr = 0.01, m = 0.9
I1002 22:15:36.427991 31343 blocking_queue.cpp:40] Waiting for datum
I1002 22:15:43.554590 31432 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 22:16:09.513842 31421 solver.cpp:314] Iteration 5500 (1.80951 iter/s, 55.2637s/100 iter), loss = 0.0525693
I1002 22:16:09.524848 31421 solver.cpp:336]     Train net output #0: loss = 0.0525693 (* 1 = 0.0525693 loss)
I1002 22:16:09.524866 31421 sgd_solver.cpp:136] Iteration 5500, lr = 0.01, m = 0.9
I1002 22:17:03.134551 31421 solver.cpp:314] Iteration 5600 (1.865 iter/s, 53.6192s/100 iter), loss = 0.0375206
I1002 22:17:03.134691 31421 solver.cpp:336]     Train net output #0: loss = 0.0375206 (* 1 = 0.0375206 loss)
I1002 22:17:03.134706 31421 sgd_solver.cpp:136] Iteration 5600, lr = 0.01, m = 0.9
I1002 22:17:56.332209 31421 solver.cpp:314] Iteration 5700 (1.87984 iter/s, 53.1962s/100 iter), loss = 0.109287
I1002 22:17:56.362239 31421 solver.cpp:336]     Train net output #0: loss = 0.109287 (* 1 = 0.109287 loss)
I1002 22:17:56.362265 31421 sgd_solver.cpp:136] Iteration 5700, lr = 0.01, m = 0.9
I1002 22:18:40.131615 31344 data_reader.cpp:305] Starting prefetch of epoch 6
I1002 22:18:50.175907 31421 solver.cpp:314] Iteration 5800 (1.85728 iter/s, 53.8422s/100 iter), loss = 0.0621788
I1002 22:18:50.175943 31421 solver.cpp:336]     Train net output #0: loss = 0.0621788 (* 1 = 0.0621788 loss)
I1002 22:18:50.175951 31421 sgd_solver.cpp:136] Iteration 5800, lr = 0.01, m = 0.9
I1002 22:19:42.424967 31421 solver.cpp:314] Iteration 5900 (1.91396 iter/s, 52.2476s/100 iter), loss = 0.0835332
I1002 22:19:42.425523 31421 solver.cpp:336]     Train net output #0: loss = 0.0835333 (* 1 = 0.0835333 loss)
I1002 22:19:42.425546 31421 sgd_solver.cpp:136] Iteration 5900, lr = 0.01, m = 0.9
I1002 22:20:08.113845 31424 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 22:20:35.637006 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.65 sparsity_achieved=0.638326 iter=6000
I1002 22:28:31.492586 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 22:28:31.548503 31421 solver.cpp:352] Sparsity after update:
I1002 22:28:31.566536 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 22:28:31.566556 31421 net.cpp:2312] conv1a_param_0(0.265) 
I1002 22:28:31.566567 31421 net.cpp:2312] conv1b_param_0(0.623) 
I1002 22:28:31.566570 31421 net.cpp:2312] ctx_conv1_param_0(0.649) 
I1002 22:28:31.566571 31421 net.cpp:2312] ctx_conv2_param_0(0.649) 
I1002 22:28:31.566573 31421 net.cpp:2312] ctx_conv3_param_0(0.649) 
I1002 22:28:31.566576 31421 net.cpp:2312] ctx_conv4_param_0(0.649) 
I1002 22:28:31.566577 31421 net.cpp:2312] ctx_final_param_0(0.243) 
I1002 22:28:31.566578 31421 net.cpp:2312] out3a_param_0(0.649) 
I1002 22:28:31.566581 31421 net.cpp:2312] out5a_param_0(0.65) 
I1002 22:28:31.566582 31421 net.cpp:2312] res2a_branch2a_param_0(0.649) 
I1002 22:28:31.566584 31421 net.cpp:2312] res2a_branch2b_param_0(0.622) 
I1002 22:28:31.566586 31421 net.cpp:2312] res3a_branch2a_param_0(0.649) 
I1002 22:28:31.566588 31421 net.cpp:2312] res3a_branch2b_param_0(0.649) 
I1002 22:28:31.566591 31421 net.cpp:2312] res4a_branch2a_param_0(0.649) 
I1002 22:28:31.566592 31421 net.cpp:2312] res4a_branch2b_param_0(0.649) 
I1002 22:28:31.566594 31421 net.cpp:2312] res5a_branch2a_param_0(0.649) 
I1002 22:28:31.566596 31421 net.cpp:2312] res5a_branch2b_param_0(0.649) 
I1002 22:28:31.566598 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.74404e+06/2.69117e+06) 0.648
I1002 22:28:31.566610 31421 solver.cpp:567] Iteration 6000, Testing net (#0)
I1002 22:28:54.193167 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.950309
I1002 22:28:54.193194 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 22:28:54.193202 31421 solver.cpp:659]     Test net output #2: loss = 0.157737 (* 1 = 0.157737 loss)
I1002 22:28:54.193230 31421 solver.cpp:265] [MultiGPU] Tests completed in 22.626s
I1002 22:28:54.639590 31421 solver.cpp:314] Iteration 6000 (0.181094 iter/s, 552.199s/100 iter), loss = 0.0829692
I1002 22:28:54.639622 31421 solver.cpp:336]     Train net output #0: loss = 0.0829692 (* 1 = 0.0829692 loss)
I1002 22:28:54.639629 31421 sgd_solver.cpp:136] Iteration 6000, lr = 0.01, m = 0.9
I1002 22:29:50.197613 31421 solver.cpp:314] Iteration 6100 (1.79997 iter/s, 55.5565s/100 iter), loss = 0.0796773
I1002 22:29:50.197690 31421 solver.cpp:336]     Train net output #0: loss = 0.0796773 (* 1 = 0.0796773 loss)
I1002 22:29:50.197700 31421 sgd_solver.cpp:136] Iteration 6100, lr = 0.01, m = 0.9
I1002 22:29:57.061931 31432 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 22:30:45.287845 31421 solver.cpp:314] Iteration 6200 (1.81525 iter/s, 55.0887s/100 iter), loss = 0.0704965
I1002 22:30:45.292204 31421 solver.cpp:336]     Train net output #0: loss = 0.0704965 (* 1 = 0.0704965 loss)
I1002 22:30:45.292213 31421 sgd_solver.cpp:136] Iteration 6200, lr = 0.01, m = 0.9
I1002 22:31:40.339467 31421 solver.cpp:314] Iteration 6300 (1.81653 iter/s, 55.0501s/100 iter), loss = 0.063791
I1002 22:31:40.339529 31421 solver.cpp:336]     Train net output #0: loss = 0.063791 (* 1 = 0.063791 loss)
I1002 22:31:40.339534 31421 sgd_solver.cpp:136] Iteration 6300, lr = 0.01, m = 0.9
I1002 22:32:34.998817 31421 solver.cpp:314] Iteration 6400 (1.82956 iter/s, 54.6578s/100 iter), loss = 0.0822476
I1002 22:32:35.004294 31421 solver.cpp:336]     Train net output #0: loss = 0.0822476 (* 1 = 0.0822476 loss)
I1002 22:32:35.004307 31421 sgd_solver.cpp:136] Iteration 6400, lr = 0.01, m = 0.9
I1002 22:32:58.460258 31432 data_reader.cpp:305] Starting prefetch of epoch 4
I1002 22:33:30.413100 31421 solver.cpp:314] Iteration 6500 (1.80464 iter/s, 55.4127s/100 iter), loss = 0.0826356
I1002 22:33:30.413352 31421 solver.cpp:336]     Train net output #0: loss = 0.0826357 (* 1 = 0.0826357 loss)
I1002 22:33:30.413362 31421 sgd_solver.cpp:136] Iteration 6500, lr = 0.01, m = 0.9
I1002 22:34:23.810734 31421 solver.cpp:314] Iteration 6600 (1.8728 iter/s, 53.3961s/100 iter), loss = 0.0704095
I1002 22:34:23.810967 31421 solver.cpp:336]     Train net output #0: loss = 0.0704095 (* 1 = 0.0704095 loss)
I1002 22:34:23.811081 31421 sgd_solver.cpp:136] Iteration 6600, lr = 0.01, m = 0.9
I1002 22:34:28.249627 31425 data_reader.cpp:305] Starting prefetch of epoch 5
I1002 22:35:16.453693 31421 solver.cpp:314] Iteration 6700 (1.89964 iter/s, 52.6415s/100 iter), loss = 0.100991
I1002 22:35:16.453752 31421 solver.cpp:336]     Train net output #0: loss = 0.100991 (* 1 = 0.100991 loss)
I1002 22:35:16.453758 31421 sgd_solver.cpp:136] Iteration 6700, lr = 0.01, m = 0.9
I1002 22:36:09.856550 31421 solver.cpp:314] Iteration 6800 (1.87261 iter/s, 53.4013s/100 iter), loss = 0.0436154
I1002 22:36:09.856907 31421 solver.cpp:336]     Train net output #0: loss = 0.0436154 (* 1 = 0.0436154 loss)
I1002 22:36:09.856915 31421 sgd_solver.cpp:136] Iteration 6800, lr = 0.01, m = 0.9
I1002 22:37:03.540989 31421 solver.cpp:314] Iteration 6900 (1.86279 iter/s, 53.6829s/100 iter), loss = 0.0851793
I1002 22:37:03.541066 31421 solver.cpp:336]     Train net output #0: loss = 0.0851793 (* 1 = 0.0851793 loss)
I1002 22:37:03.541074 31421 sgd_solver.cpp:136] Iteration 6900, lr = 0.01, m = 0.9
I1002 22:37:24.243082 31342 data_reader.cpp:305] Starting prefetch of epoch 6
I1002 22:37:54.758474 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.66 sparsity_achieved=0.648061 iter=7000
I1002 22:46:06.273273 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 22:46:06.321528 31421 solver.cpp:352] Sparsity after update:
I1002 22:46:06.337590 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 22:46:06.337630 31421 net.cpp:2312] conv1a_param_0(0.265) 
I1002 22:46:06.337647 31421 net.cpp:2312] conv1b_param_0(0.632) 
I1002 22:46:06.337651 31421 net.cpp:2312] ctx_conv1_param_0(0.66) 
I1002 22:46:06.337657 31421 net.cpp:2312] ctx_conv2_param_0(0.66) 
I1002 22:46:06.337662 31421 net.cpp:2312] ctx_conv3_param_0(0.66) 
I1002 22:46:06.337666 31421 net.cpp:2312] ctx_conv4_param_0(0.66) 
I1002 22:46:06.337668 31421 net.cpp:2312] ctx_final_param_0(0.28) 
I1002 22:46:06.337672 31421 net.cpp:2312] out3a_param_0(0.66) 
I1002 22:46:06.337676 31421 net.cpp:2312] out5a_param_0(0.66) 
I1002 22:46:06.337678 31421 net.cpp:2312] res2a_branch2a_param_0(0.66) 
I1002 22:46:06.337682 31421 net.cpp:2312] res2a_branch2b_param_0(0.632) 
I1002 22:46:06.337685 31421 net.cpp:2312] res3a_branch2a_param_0(0.66) 
I1002 22:46:06.337688 31421 net.cpp:2312] res3a_branch2b_param_0(0.659) 
I1002 22:46:06.337692 31421 net.cpp:2312] res4a_branch2a_param_0(0.66) 
I1002 22:46:06.337694 31421 net.cpp:2312] res4a_branch2b_param_0(0.66) 
I1002 22:46:06.337698 31421 net.cpp:2312] res5a_branch2a_param_0(0.659) 
I1002 22:46:06.337702 31421 net.cpp:2312] res5a_branch2b_param_0(0.66) 
I1002 22:46:06.337704 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.77184e+06/2.69117e+06) 0.658
I1002 22:46:06.940902 31421 solver.cpp:314] Iteration 7000 (0.184032 iter/s, 543.385s/100 iter), loss = 0.105434
I1002 22:46:06.940943 31421 solver.cpp:336]     Train net output #0: loss = 0.105434 (* 1 = 0.105434 loss)
I1002 22:46:06.940950 31421 sgd_solver.cpp:136] Iteration 7000, lr = 0.01, m = 0.9
I1002 22:47:02.075922 31421 solver.cpp:314] Iteration 7100 (1.81378 iter/s, 55.1335s/100 iter), loss = 0.081607
I1002 22:47:02.075991 31421 solver.cpp:336]     Train net output #0: loss = 0.081607 (* 1 = 0.081607 loss)
I1002 22:47:02.076001 31421 sgd_solver.cpp:136] Iteration 7100, lr = 0.01, m = 0.9
I1002 22:47:35.565510 31429 blocking_queue.cpp:40] Waiting for datum
I1002 22:47:57.170150 31421 solver.cpp:314] Iteration 7200 (1.81512 iter/s, 55.0927s/100 iter), loss = 0.0824463
I1002 22:47:57.170195 31421 solver.cpp:336]     Train net output #0: loss = 0.0824463 (* 1 = 0.0824463 loss)
I1002 22:47:57.170203 31421 sgd_solver.cpp:136] Iteration 7200, lr = 0.01, m = 0.9
I1002 22:48:36.471185 31432 data_reader.cpp:305] Starting prefetch of epoch 5
I1002 22:48:52.936164 31421 solver.cpp:314] Iteration 7300 (1.79326 iter/s, 55.7644s/100 iter), loss = 0.0570447
I1002 22:48:52.936199 31421 solver.cpp:336]     Train net output #0: loss = 0.0570448 (* 1 = 0.0570448 loss)
I1002 22:48:52.936203 31421 sgd_solver.cpp:136] Iteration 7300, lr = 0.01, m = 0.9
I1002 22:49:47.259858 31421 solver.cpp:314] Iteration 7400 (1.84087 iter/s, 54.3222s/100 iter), loss = 0.116907
I1002 22:49:47.259933 31421 solver.cpp:336]     Train net output #0: loss = 0.116907 (* 1 = 0.116907 loss)
I1002 22:49:47.259941 31421 sgd_solver.cpp:136] Iteration 7400, lr = 0.01, m = 0.9
I1002 22:50:42.018970 31421 solver.cpp:314] Iteration 7500 (1.82623 iter/s, 54.7576s/100 iter), loss = 0.0866264
I1002 22:50:42.019031 31421 solver.cpp:336]     Train net output #0: loss = 0.0866265 (* 1 = 0.0866265 loss)
I1002 22:50:42.019037 31421 sgd_solver.cpp:136] Iteration 7500, lr = 0.01, m = 0.9
I1002 22:51:34.519269 31432 data_reader.cpp:305] Starting prefetch of epoch 6
I1002 22:51:34.816213 31421 solver.cpp:314] Iteration 7600 (1.89409 iter/s, 52.7957s/100 iter), loss = 0.0562593
I1002 22:51:34.816246 31421 solver.cpp:336]     Train net output #0: loss = 0.0562594 (* 1 = 0.0562594 loss)
I1002 22:51:34.816254 31421 sgd_solver.cpp:136] Iteration 7600, lr = 0.01, m = 0.9
I1002 22:52:27.480620 31421 solver.cpp:314] Iteration 7700 (1.89887 iter/s, 52.6629s/100 iter), loss = 0.0622251
I1002 22:52:27.480775 31421 solver.cpp:336]     Train net output #0: loss = 0.0622251 (* 1 = 0.0622251 loss)
I1002 22:52:27.480789 31421 sgd_solver.cpp:136] Iteration 7700, lr = 0.01, m = 0.9
I1002 22:52:58.951033 31342 data_reader.cpp:305] Starting prefetch of epoch 7
I1002 22:53:13.968878 31421 solver.cpp:314] Iteration 7800 (2.15114 iter/s, 46.4869s/100 iter), loss = 0.157506
I1002 22:53:13.969009 31421 solver.cpp:336]     Train net output #0: loss = 0.157506 (* 1 = 0.157506 loss)
I1002 22:53:13.969030 31421 sgd_solver.cpp:136] Iteration 7800, lr = 0.01, m = 0.9
I1002 22:54:06.105166 31421 solver.cpp:314] Iteration 7900 (1.9181 iter/s, 52.1348s/100 iter), loss = 0.054693
I1002 22:54:06.124323 31421 solver.cpp:336]     Train net output #0: loss = 0.054693 (* 1 = 0.054693 loss)
I1002 22:54:06.124366 31421 sgd_solver.cpp:136] Iteration 7900, lr = 0.01, m = 0.9
I1002 22:54:58.572841 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.67 sparsity_achieved=0.658391 iter=8000
I1002 22:55:07.559804 31536 data_reader.cpp:305] Starting prefetch of epoch 1
I1002 23:03:55.776988 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 23:03:55.799451 31421 solver.cpp:352] Sparsity after update:
I1002 23:03:55.806310 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 23:03:55.806325 31421 net.cpp:2312] conv1a_param_0(0.264) 
I1002 23:03:55.806341 31421 net.cpp:2312] conv1b_param_0(0.642) 
I1002 23:03:55.806345 31421 net.cpp:2312] ctx_conv1_param_0(0.668) 
I1002 23:03:55.806349 31421 net.cpp:2312] ctx_conv2_param_0(0.668) 
I1002 23:03:55.806352 31421 net.cpp:2312] ctx_conv3_param_0(0.668) 
I1002 23:03:55.806358 31421 net.cpp:2312] ctx_conv4_param_0(0.668) 
I1002 23:03:55.806361 31421 net.cpp:2312] ctx_final_param_0(0.329) 
I1002 23:03:55.806365 31421 net.cpp:2312] out3a_param_0(0.668) 
I1002 23:03:55.806367 31421 net.cpp:2312] out5a_param_0(0.67) 
I1002 23:03:55.806370 31421 net.cpp:2312] res2a_branch2a_param_0(0.667) 
I1002 23:03:55.806373 31421 net.cpp:2312] res2a_branch2b_param_0(0.638) 
I1002 23:03:55.806376 31421 net.cpp:2312] res3a_branch2a_param_0(0.668) 
I1002 23:03:55.806380 31421 net.cpp:2312] res3a_branch2b_param_0(0.666) 
I1002 23:03:55.806382 31421 net.cpp:2312] res4a_branch2a_param_0(0.669) 
I1002 23:03:55.806385 31421 net.cpp:2312] res4a_branch2b_param_0(0.668) 
I1002 23:03:55.806388 31421 net.cpp:2312] res5a_branch2a_param_0(0.669) 
I1002 23:03:55.806392 31421 net.cpp:2312] res5a_branch2b_param_0(0.669) 
I1002 23:03:55.806396 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.79774e+06/2.69117e+06) 0.668
I1002 23:03:55.806411 31421 solver.cpp:567] Iteration 8000, Testing net (#0)
I1002 23:04:20.162662 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.944362
I1002 23:04:20.162688 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 23:04:20.162693 31421 solver.cpp:659]     Test net output #2: loss = 0.149443 (* 1 = 0.149443 loss)
I1002 23:04:20.162721 31421 solver.cpp:265] [MultiGPU] Tests completed in 24.3556s
I1002 23:04:20.639809 31421 solver.cpp:314] Iteration 8000 (0.162729 iter/s, 614.517s/100 iter), loss = 0.0918992
I1002 23:04:20.639845 31421 solver.cpp:336]     Train net output #0: loss = 0.0918992 (* 1 = 0.0918992 loss)
I1002 23:04:20.639853 31421 sgd_solver.cpp:136] Iteration 8000, lr = 0.01, m = 0.9
I1002 23:05:12.903388 31430 data_reader.cpp:305] Starting prefetch of epoch 4
I1002 23:05:15.298460 31421 solver.cpp:314] Iteration 8100 (1.82959 iter/s, 54.6571s/100 iter), loss = 0.0543542
I1002 23:05:15.298494 31421 solver.cpp:336]     Train net output #0: loss = 0.0543541 (* 1 = 0.0543541 loss)
I1002 23:05:15.298501 31421 sgd_solver.cpp:136] Iteration 8100, lr = 0.01, m = 0.9
I1002 23:06:13.026325 31421 solver.cpp:314] Iteration 8200 (1.73231 iter/s, 57.7262s/100 iter), loss = 0.0554342
I1002 23:06:13.026445 31421 solver.cpp:336]     Train net output #0: loss = 0.0554342 (* 1 = 0.0554342 loss)
I1002 23:06:13.026455 31421 sgd_solver.cpp:136] Iteration 8200, lr = 0.01, m = 0.9
I1002 23:06:47.558408 31424 data_reader.cpp:305] Starting prefetch of epoch 3
I1002 23:07:09.917476 31421 solver.cpp:314] Iteration 8300 (1.75779 iter/s, 56.8896s/100 iter), loss = 0.103131
I1002 23:07:09.917500 31421 solver.cpp:336]     Train net output #0: loss = 0.103131 (* 1 = 0.103131 loss)
I1002 23:07:09.917505 31421 sgd_solver.cpp:136] Iteration 8300, lr = 0.01, m = 0.9
I1002 23:08:09.746249 31421 solver.cpp:314] Iteration 8400 (1.67148 iter/s, 59.8271s/100 iter), loss = 0.0636602
I1002 23:08:09.746359 31421 solver.cpp:336]     Train net output #0: loss = 0.0636602 (* 1 = 0.0636602 loss)
I1002 23:08:09.746367 31421 sgd_solver.cpp:136] Iteration 8400, lr = 0.01, m = 0.9
I1002 23:08:13.211315 31431 blocking_queue.cpp:40] Waiting for datum
I1002 23:09:06.836804 31421 solver.cpp:314] Iteration 8500 (1.75165 iter/s, 57.0889s/100 iter), loss = 0.0600011
I1002 23:09:06.836897 31421 solver.cpp:336]     Train net output #0: loss = 0.0600011 (* 1 = 0.0600011 loss)
I1002 23:09:06.836915 31421 sgd_solver.cpp:136] Iteration 8500, lr = 0.01, m = 0.9
I1002 23:09:55.589399 31425 data_reader.cpp:305] Starting prefetch of epoch 6
I1002 23:10:00.419018 31421 solver.cpp:314] Iteration 8600 (1.86634 iter/s, 53.5807s/100 iter), loss = 0.0787878
I1002 23:10:00.419334 31421 solver.cpp:336]     Train net output #0: loss = 0.0787878 (* 1 = 0.0787878 loss)
I1002 23:10:00.419453 31421 sgd_solver.cpp:136] Iteration 8600, lr = 0.01, m = 0.9
I1002 23:10:52.808208 31421 solver.cpp:314] Iteration 8700 (1.90884 iter/s, 52.3877s/100 iter), loss = 0.0637411
I1002 23:10:52.813205 31421 solver.cpp:336]     Train net output #0: loss = 0.0637411 (* 1 = 0.0637411 loss)
I1002 23:10:52.813220 31421 sgd_solver.cpp:136] Iteration 8700, lr = 0.01, m = 0.9
I1002 23:11:45.818226 31421 solver.cpp:314] Iteration 8800 (1.88649 iter/s, 53.0085s/100 iter), loss = 0.0718462
I1002 23:11:45.818292 31421 solver.cpp:336]     Train net output #0: loss = 0.0718462 (* 1 = 0.0718462 loss)
I1002 23:11:45.818300 31421 sgd_solver.cpp:136] Iteration 8800, lr = 0.01, m = 0.9
I1002 23:12:39.654475 31421 solver.cpp:314] Iteration 8900 (1.85754 iter/s, 53.8347s/100 iter), loss = 0.0604027
I1002 23:12:39.654572 31421 solver.cpp:336]     Train net output #0: loss = 0.0604027 (* 1 = 0.0604027 loss)
I1002 23:12:39.654580 31421 sgd_solver.cpp:136] Iteration 8900, lr = 0.01, m = 0.9
I1002 23:12:51.189319 31430 data_reader.cpp:305] Starting prefetch of epoch 5
I1002 23:13:33.049829 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.68 sparsity_achieved=0.668016 iter=9000
I1002 23:22:06.212973 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 23:22:06.236022 31421 solver.cpp:352] Sparsity after update:
I1002 23:22:06.254812 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 23:22:06.254835 31421 net.cpp:2312] conv1a_param_0(0.266) 
I1002 23:22:06.254858 31421 net.cpp:2312] conv1b_param_0(0.644) 
I1002 23:22:06.254861 31421 net.cpp:2312] ctx_conv1_param_0(0.679) 
I1002 23:22:06.254864 31421 net.cpp:2312] ctx_conv2_param_0(0.679) 
I1002 23:22:06.254868 31421 net.cpp:2312] ctx_conv3_param_0(0.679) 
I1002 23:22:06.254871 31421 net.cpp:2312] ctx_conv4_param_0(0.679) 
I1002 23:22:06.254889 31421 net.cpp:2312] ctx_final_param_0(0.212) 
I1002 23:22:06.254899 31421 net.cpp:2312] out3a_param_0(0.679) 
I1002 23:22:06.254909 31421 net.cpp:2312] out5a_param_0(0.68) 
I1002 23:22:06.254920 31421 net.cpp:2312] res2a_branch2a_param_0(0.677) 
I1002 23:22:06.254928 31421 net.cpp:2312] res2a_branch2b_param_0(0.643) 
I1002 23:22:06.254937 31421 net.cpp:2312] res3a_branch2a_param_0(0.679) 
I1002 23:22:06.254946 31421 net.cpp:2312] res3a_branch2b_param_0(0.676) 
I1002 23:22:06.254956 31421 net.cpp:2312] res4a_branch2a_param_0(0.68) 
I1002 23:22:06.254966 31421 net.cpp:2312] res4a_branch2b_param_0(0.679) 
I1002 23:22:06.254976 31421 net.cpp:2312] res5a_branch2a_param_0(0.679) 
I1002 23:22:06.254986 31421 net.cpp:2312] res5a_branch2b_param_0(0.68) 
I1002 23:22:06.255003 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.82447e+06/2.69117e+06) 0.678
I1002 23:22:06.881072 31421 solver.cpp:314] Iteration 9000 (0.176301 iter/s, 567.211s/100 iter), loss = 0.0826693
I1002 23:22:06.881134 31421 solver.cpp:336]     Train net output #0: loss = 0.0826693 (* 1 = 0.0826693 loss)
I1002 23:22:06.881150 31421 sgd_solver.cpp:136] Iteration 9000, lr = 0.01, m = 0.9
I1002 23:22:54.688225 31425 data_reader.cpp:305] Starting prefetch of epoch 7
I1002 23:23:00.716217 31421 solver.cpp:314] Iteration 9100 (1.85758 iter/s, 53.8336s/100 iter), loss = 0.0848457
I1002 23:23:00.716305 31421 solver.cpp:336]     Train net output #0: loss = 0.0848458 (* 1 = 0.0848458 loss)
I1002 23:23:00.716320 31421 sgd_solver.cpp:136] Iteration 9100, lr = 0.01, m = 0.9
I1002 23:23:55.885771 31421 solver.cpp:314] Iteration 9200 (1.81264 iter/s, 55.168s/100 iter), loss = 0.0570116
I1002 23:23:55.885953 31421 solver.cpp:336]     Train net output #0: loss = 0.0570116 (* 1 = 0.0570116 loss)
I1002 23:23:55.885968 31421 sgd_solver.cpp:136] Iteration 9200, lr = 0.01, m = 0.9
I1002 23:24:50.864033 31421 solver.cpp:314] Iteration 9300 (1.81895 iter/s, 54.9767s/100 iter), loss = 0.0850698
I1002 23:24:50.868218 31421 solver.cpp:336]     Train net output #0: loss = 0.0850698 (* 1 = 0.0850698 loss)
I1002 23:24:50.868229 31421 sgd_solver.cpp:136] Iteration 9300, lr = 0.01, m = 0.9
I1002 23:25:47.807679 31421 solver.cpp:314] Iteration 9400 (1.75617 iter/s, 56.942s/100 iter), loss = 0.0788165
I1002 23:25:47.807760 31421 solver.cpp:336]     Train net output #0: loss = 0.0788165 (* 1 = 0.0788165 loss)
I1002 23:25:47.807767 31421 sgd_solver.cpp:136] Iteration 9400, lr = 0.01, m = 0.9
I1002 23:25:58.017742 31425 data_reader.cpp:305] Starting prefetch of epoch 8
I1002 23:26:43.781464 31421 solver.cpp:314] Iteration 9500 (1.7866 iter/s, 55.9722s/100 iter), loss = 0.0676432
I1002 23:26:43.781522 31421 solver.cpp:336]     Train net output #0: loss = 0.0676432 (* 1 = 0.0676432 loss)
I1002 23:26:43.781528 31421 sgd_solver.cpp:136] Iteration 9500, lr = 0.01, m = 0.9
I1002 23:27:37.324128 31421 solver.cpp:314] Iteration 9600 (1.86772 iter/s, 53.5411s/100 iter), loss = 0.0715804
I1002 23:27:37.324215 31421 solver.cpp:336]     Train net output #0: loss = 0.0715804 (* 1 = 0.0715804 loss)
I1002 23:27:37.324223 31421 sgd_solver.cpp:136] Iteration 9600, lr = 0.01, m = 0.9
I1002 23:28:30.240953 31421 solver.cpp:314] Iteration 9700 (1.88981 iter/s, 52.9153s/100 iter), loss = 0.0576395
I1002 23:28:30.241114 31421 solver.cpp:336]     Train net output #0: loss = 0.0576395 (* 1 = 0.0576395 loss)
I1002 23:28:30.241124 31421 sgd_solver.cpp:136] Iteration 9700, lr = 0.01, m = 0.9
I1002 23:28:55.518189 31344 data_reader.cpp:305] Starting prefetch of epoch 7
I1002 23:29:22.032228 31421 solver.cpp:314] Iteration 9800 (1.93088 iter/s, 51.7898s/100 iter), loss = 0.0576571
I1002 23:29:22.033208 31421 solver.cpp:336]     Train net output #0: loss = 0.0576571 (* 1 = 0.0576571 loss)
I1002 23:29:22.033315 31421 sgd_solver.cpp:136] Iteration 9800, lr = 0.01, m = 0.9
I1002 23:30:15.551134 31421 solver.cpp:314] Iteration 9900 (1.86855 iter/s, 53.5174s/100 iter), loss = 0.100776
I1002 23:30:15.551206 31421 solver.cpp:336]     Train net output #0: loss = 0.100776 (* 1 = 0.100776 loss)
I1002 23:30:15.551215 31421 sgd_solver.cpp:136] Iteration 9900, lr = 0.01, m = 0.9
I1002 23:31:08.842253 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.69 sparsity_achieved=0.677948 iter=10000
I1002 23:31:28.264273 31538 data_reader.cpp:305] Starting prefetch of epoch 2
I1002 23:40:25.246968 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 23:40:25.268800 31421 solver.cpp:352] Sparsity after update:
I1002 23:40:25.280297 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 23:40:25.280371 31421 net.cpp:2312] conv1a_param_0(0.268) 
I1002 23:40:25.280397 31421 net.cpp:2312] conv1b_param_0(0.653) 
I1002 23:40:25.280409 31421 net.cpp:2312] ctx_conv1_param_0(0.689) 
I1002 23:40:25.280431 31421 net.cpp:2312] ctx_conv2_param_0(0.689) 
I1002 23:40:25.280444 31421 net.cpp:2312] ctx_conv3_param_0(0.689) 
I1002 23:40:25.280455 31421 net.cpp:2312] ctx_conv4_param_0(0.689) 
I1002 23:40:25.280465 31421 net.cpp:2312] ctx_final_param_0(0.336) 
I1002 23:40:25.280475 31421 net.cpp:2312] out3a_param_0(0.689) 
I1002 23:40:25.280486 31421 net.cpp:2312] out5a_param_0(0.69) 
I1002 23:40:25.280498 31421 net.cpp:2312] res2a_branch2a_param_0(0.687) 
I1002 23:40:25.280508 31421 net.cpp:2312] res2a_branch2b_param_0(0.652) 
I1002 23:40:25.280517 31421 net.cpp:2312] res3a_branch2a_param_0(0.689) 
I1002 23:40:25.280527 31421 net.cpp:2312] res3a_branch2b_param_0(0.686) 
I1002 23:40:25.280537 31421 net.cpp:2312] res4a_branch2a_param_0(0.689) 
I1002 23:40:25.280549 31421 net.cpp:2312] res4a_branch2b_param_0(0.689) 
I1002 23:40:25.280560 31421 net.cpp:2312] res5a_branch2a_param_0(0.689) 
I1002 23:40:25.280570 31421 net.cpp:2312] res5a_branch2b_param_0(0.689) 
I1002 23:40:25.280581 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.85162e+06/2.69117e+06) 0.688
I1002 23:40:25.311782 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I1002 23:40:25.487190 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I1002 23:40:25.491909 31421 solver.cpp:567] Iteration 10000, Testing net (#0)
I1002 23:40:49.930807 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.95283
I1002 23:40:49.930833 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1002 23:40:49.930841 31421 solver.cpp:659]     Test net output #2: loss = 0.14699 (* 1 = 0.14699 loss)
I1002 23:40:49.930871 31421 solver.cpp:265] [MultiGPU] Tests completed in 24.4383s
I1002 23:40:50.473330 31421 solver.cpp:314] Iteration 10000 (0.157504 iter/s, 634.904s/100 iter), loss = 0.0586577
I1002 23:40:50.473364 31421 solver.cpp:336]     Train net output #0: loss = 0.0586577 (* 1 = 0.0586577 loss)
I1002 23:40:50.473371 31421 sgd_solver.cpp:136] Iteration 10000, lr = 0.01, m = 0.9
I1002 23:41:35.326755 31343 blocking_queue.cpp:40] Waiting for datum
I1002 23:41:48.503402 31421 solver.cpp:314] Iteration 10100 (1.72329 iter/s, 58.0285s/100 iter), loss = 0.0660351
I1002 23:41:48.503443 31421 solver.cpp:336]     Train net output #0: loss = 0.0660351 (* 1 = 0.0660351 loss)
I1002 23:41:48.503448 31421 sgd_solver.cpp:136] Iteration 10100, lr = 0.01, m = 0.9
I1002 23:42:46.224205 31421 solver.cpp:314] Iteration 10200 (1.73253 iter/s, 57.7192s/100 iter), loss = 0.115023
I1002 23:42:46.230607 31421 solver.cpp:336]     Train net output #0: loss = 0.115023 (* 1 = 0.115023 loss)
I1002 23:42:46.230623 31421 sgd_solver.cpp:136] Iteration 10200, lr = 0.01, m = 0.9
I1002 23:43:12.700014 31430 data_reader.cpp:305] Starting prefetch of epoch 6
I1002 23:43:44.425448 31421 solver.cpp:314] Iteration 10300 (1.71822 iter/s, 58.1996s/100 iter), loss = 0.048658
I1002 23:43:44.425516 31421 solver.cpp:336]     Train net output #0: loss = 0.048658 (* 1 = 0.048658 loss)
I1002 23:43:44.425524 31421 sgd_solver.cpp:136] Iteration 10300, lr = 0.01, m = 0.9
I1002 23:44:42.417959 31421 solver.cpp:314] Iteration 10400 (1.72441 iter/s, 57.9909s/100 iter), loss = 0.0602606
I1002 23:44:42.418069 31421 solver.cpp:336]     Train net output #0: loss = 0.0602606 (* 1 = 0.0602606 loss)
I1002 23:44:42.418076 31421 sgd_solver.cpp:136] Iteration 10400, lr = 0.01, m = 0.9
I1002 23:44:48.548200 31342 data_reader.cpp:305] Starting prefetch of epoch 8
I1002 23:45:38.899827 31421 solver.cpp:314] Iteration 10500 (1.77053 iter/s, 56.4803s/100 iter), loss = 0.0591885
I1002 23:45:38.902730 31421 solver.cpp:336]     Train net output #0: loss = 0.0591885 (* 1 = 0.0591885 loss)
I1002 23:45:38.902741 31421 sgd_solver.cpp:136] Iteration 10500, lr = 0.01, m = 0.9
I1002 23:46:28.013164 31421 solver.cpp:314] Iteration 10600 (2.03616 iter/s, 49.1119s/100 iter), loss = 0.0775887
I1002 23:46:28.013253 31421 solver.cpp:336]     Train net output #0: loss = 0.0775886 (* 1 = 0.0775886 loss)
I1002 23:46:28.013263 31421 sgd_solver.cpp:136] Iteration 10600, lr = 0.01, m = 0.9
I1002 23:47:21.484699 31421 solver.cpp:314] Iteration 10700 (1.87021 iter/s, 53.47s/100 iter), loss = 0.0427836
I1002 23:47:21.486110 31421 solver.cpp:336]     Train net output #0: loss = 0.0427835 (* 1 = 0.0427835 loss)
I1002 23:47:21.486119 31421 sgd_solver.cpp:136] Iteration 10700, lr = 0.01, m = 0.9
I1002 23:47:44.320891 31342 data_reader.cpp:305] Starting prefetch of epoch 9
I1002 23:48:16.800213 31421 solver.cpp:314] Iteration 10800 (1.80786 iter/s, 55.314s/100 iter), loss = 0.0806378
I1002 23:48:16.804446 31421 solver.cpp:336]     Train net output #0: loss = 0.0806378 (* 1 = 0.0806378 loss)
I1002 23:48:16.804570 31421 sgd_solver.cpp:136] Iteration 10800, lr = 0.01, m = 0.9
I1002 23:49:09.730855 31421 solver.cpp:314] Iteration 10900 (1.88932 iter/s, 52.9291s/100 iter), loss = 0.044546
I1002 23:49:09.730921 31421 solver.cpp:336]     Train net output #0: loss = 0.044546 (* 1 = 0.044546 loss)
I1002 23:49:09.730927 31421 sgd_solver.cpp:136] Iteration 10900, lr = 0.01, m = 0.9
I1002 23:50:03.069900 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.7 sparsity_achieved=0.688035 iter=11000
I1002 23:59:41.094046 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1002 23:59:41.105623 31421 solver.cpp:352] Sparsity after update:
I1002 23:59:41.109185 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1002 23:59:41.109199 31421 net.cpp:2312] conv1a_param_0(0.276) 
I1002 23:59:41.109213 31421 net.cpp:2312] conv1b_param_0(0.662) 
I1002 23:59:41.109220 31421 net.cpp:2312] ctx_conv1_param_0(0.7) 
I1002 23:59:41.109223 31421 net.cpp:2312] ctx_conv2_param_0(0.7) 
I1002 23:59:41.109226 31421 net.cpp:2312] ctx_conv3_param_0(0.7) 
I1002 23:59:41.109230 31421 net.cpp:2312] ctx_conv4_param_0(0.7) 
I1002 23:59:41.109232 31421 net.cpp:2312] ctx_final_param_0(0.218) 
I1002 23:59:41.109236 31421 net.cpp:2312] out3a_param_0(0.7) 
I1002 23:59:41.109238 31421 net.cpp:2312] out5a_param_0(0.7) 
I1002 23:59:41.109241 31421 net.cpp:2312] res2a_branch2a_param_0(0.697) 
I1002 23:59:41.109244 31421 net.cpp:2312] res2a_branch2b_param_0(0.657) 
I1002 23:59:41.109248 31421 net.cpp:2312] res3a_branch2a_param_0(0.7) 
I1002 23:59:41.109251 31421 net.cpp:2312] res3a_branch2b_param_0(0.696) 
I1002 23:59:41.109254 31421 net.cpp:2312] res4a_branch2a_param_0(0.7) 
I1002 23:59:41.109258 31421 net.cpp:2312] res4a_branch2b_param_0(0.7) 
I1002 23:59:41.109261 31421 net.cpp:2312] res5a_branch2a_param_0(0.699) 
I1002 23:59:41.109264 31421 net.cpp:2312] res5a_branch2b_param_0(0.7) 
I1002 23:59:41.109268 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.87833e+06/2.69117e+06) 0.698
I1002 23:59:41.652225 31421 solver.cpp:314] Iteration 11000 (0.158252 iter/s, 631.904s/100 iter), loss = 0.0577764
I1002 23:59:41.652259 31421 solver.cpp:336]     Train net output #0: loss = 0.0577764 (* 1 = 0.0577764 loss)
I1002 23:59:41.652266 31421 sgd_solver.cpp:136] Iteration 11000, lr = 0.01, m = 0.9
I1003 00:00:18.948949 31430 data_reader.cpp:305] Starting prefetch of epoch 7
I1003 00:00:35.151675 31421 solver.cpp:314] Iteration 11100 (1.86923 iter/s, 53.4979s/100 iter), loss = 0.0793459
I1003 00:00:35.151715 31421 solver.cpp:336]     Train net output #0: loss = 0.0793459 (* 1 = 0.0793459 loss)
I1003 00:00:35.151721 31421 sgd_solver.cpp:136] Iteration 11100, lr = 0.01, m = 0.9
I1003 00:01:28.918470 31421 solver.cpp:314] Iteration 11200 (1.85994 iter/s, 53.7653s/100 iter), loss = 0.0690188
I1003 00:01:28.918551 31421 solver.cpp:336]     Train net output #0: loss = 0.0690188 (* 1 = 0.0690188 loss)
I1003 00:01:28.918560 31421 sgd_solver.cpp:136] Iteration 11200, lr = 0.01, m = 0.9
I1003 00:01:32.807065 31343 blocking_queue.cpp:40] Waiting for datum
I1003 00:01:48.859057 31424 data_reader.cpp:305] Starting prefetch of epoch 4
I1003 00:02:22.608871 31421 solver.cpp:314] Iteration 11300 (1.86258 iter/s, 53.6889s/100 iter), loss = 0.0758412
I1003 00:02:22.619040 31421 solver.cpp:336]     Train net output #0: loss = 0.0758412 (* 1 = 0.0758412 loss)
I1003 00:02:22.619053 31421 sgd_solver.cpp:136] Iteration 11300, lr = 0.01, m = 0.9
I1003 00:03:17.384757 31421 solver.cpp:314] Iteration 11400 (1.82567 iter/s, 54.7743s/100 iter), loss = 0.0542911
I1003 00:03:17.384974 31421 solver.cpp:336]     Train net output #0: loss = 0.0542911 (* 1 = 0.0542911 loss)
I1003 00:03:17.384982 31421 sgd_solver.cpp:136] Iteration 11400, lr = 0.01, m = 0.9
I1003 00:04:11.220211 31421 solver.cpp:314] Iteration 11500 (1.85756 iter/s, 53.8339s/100 iter), loss = 0.0728414
I1003 00:04:11.221510 31421 solver.cpp:336]     Train net output #0: loss = 0.0728414 (* 1 = 0.0728414 loss)
I1003 00:04:11.221521 31421 sgd_solver.cpp:136] Iteration 11500, lr = 0.01, m = 0.9
I1003 00:04:47.403506 31342 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 00:05:04.675420 31421 solver.cpp:314] Iteration 11600 (1.87078 iter/s, 53.4537s/100 iter), loss = 0.0910666
I1003 00:05:04.675457 31421 solver.cpp:336]     Train net output #0: loss = 0.0910666 (* 1 = 0.0910666 loss)
I1003 00:05:04.675463 31421 sgd_solver.cpp:136] Iteration 11600, lr = 0.01, m = 0.9
I1003 00:05:56.981559 31421 solver.cpp:314] Iteration 11700 (1.91188 iter/s, 52.3047s/100 iter), loss = 0.0406111
I1003 00:05:56.981622 31421 solver.cpp:336]     Train net output #0: loss = 0.0406111 (* 1 = 0.0406111 loss)
I1003 00:05:56.981631 31421 sgd_solver.cpp:136] Iteration 11700, lr = 0.01, m = 0.9
I1003 00:06:50.761097 31421 solver.cpp:314] Iteration 11800 (1.8595 iter/s, 53.778s/100 iter), loss = 0.0514176
I1003 00:06:50.761168 31421 solver.cpp:336]     Train net output #0: loss = 0.0514176 (* 1 = 0.0514176 loss)
I1003 00:06:50.761180 31421 sgd_solver.cpp:136] Iteration 11800, lr = 0.01, m = 0.9
I1003 00:07:42.347600 31425 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 00:07:43.850713 31421 solver.cpp:314] Iteration 11900 (1.88366 iter/s, 53.0881s/100 iter), loss = 0.0615476
I1003 00:07:43.850802 31421 solver.cpp:336]     Train net output #0: loss = 0.0615476 (* 1 = 0.0615476 loss)
I1003 00:07:43.850819 31421 sgd_solver.cpp:136] Iteration 11900, lr = 0.01, m = 0.9
I1003 00:08:36.565062 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.71 sparsity_achieved=0.69796 iter=12000
I1003 00:09:02.971148 31536 data_reader.cpp:305] Starting prefetch of epoch 2
I1003 00:18:06.423336 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 00:18:06.463205 31421 solver.cpp:352] Sparsity after update:
I1003 00:18:06.481921 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 00:18:06.482018 31421 net.cpp:2312] conv1a_param_0(0.277) 
I1003 00:18:06.482043 31421 net.cpp:2312] conv1b_param_0(0.67) 
I1003 00:18:06.482053 31421 net.cpp:2312] ctx_conv1_param_0(0.708) 
I1003 00:18:06.482061 31421 net.cpp:2312] ctx_conv2_param_0(0.708) 
I1003 00:18:06.482074 31421 net.cpp:2312] ctx_conv3_param_0(0.708) 
I1003 00:18:06.482085 31421 net.cpp:2312] ctx_conv4_param_0(0.708) 
I1003 00:18:06.482095 31421 net.cpp:2312] ctx_final_param_0(0.338) 
I1003 00:18:06.482105 31421 net.cpp:2312] out3a_param_0(0.708) 
I1003 00:18:06.482116 31421 net.cpp:2312] out5a_param_0(0.71) 
I1003 00:18:06.482126 31421 net.cpp:2312] res2a_branch2a_param_0(0.707) 
I1003 00:18:06.482136 31421 net.cpp:2312] res2a_branch2b_param_0(0.663) 
I1003 00:18:06.482147 31421 net.cpp:2312] res3a_branch2a_param_0(0.708) 
I1003 00:18:06.482158 31421 net.cpp:2312] res3a_branch2b_param_0(0.706) 
I1003 00:18:06.482169 31421 net.cpp:2312] res4a_branch2a_param_0(0.709) 
I1003 00:18:06.482179 31421 net.cpp:2312] res4a_branch2b_param_0(0.708) 
I1003 00:18:06.482188 31421 net.cpp:2312] res5a_branch2a_param_0(0.709) 
I1003 00:18:06.482197 31421 net.cpp:2312] res5a_branch2b_param_0(0.709) 
I1003 00:18:06.482209 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.90498e+06/2.69117e+06) 0.708
I1003 00:18:06.482231 31421 solver.cpp:567] Iteration 12000, Testing net (#0)
I1003 00:18:29.357874 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.953016
I1003 00:18:29.357928 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 00:18:29.357944 31421 solver.cpp:659]     Test net output #2: loss = 0.134425 (* 1 = 0.134425 loss)
I1003 00:18:29.357978 31421 solver.cpp:265] [MultiGPU] Tests completed in 22.8751s
I1003 00:18:29.891108 31421 solver.cpp:314] Iteration 12000 (0.154793 iter/s, 646.022s/100 iter), loss = 0.0447798
I1003 00:18:29.891167 31421 solver.cpp:336]     Train net output #0: loss = 0.0447798 (* 1 = 0.0447798 loss)
I1003 00:18:29.891183 31421 sgd_solver.cpp:136] Iteration 12000, lr = 0.01, m = 0.9
I1003 00:19:04.512248 31342 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 00:19:24.370642 31421 solver.cpp:314] Iteration 12100 (1.8356 iter/s, 54.478s/100 iter), loss = 0.111193
I1003 00:19:24.370674 31421 solver.cpp:336]     Train net output #0: loss = 0.111193 (* 1 = 0.111193 loss)
I1003 00:19:24.370681 31421 sgd_solver.cpp:136] Iteration 12100, lr = 0.01, m = 0.9
I1003 00:20:17.864420 31421 solver.cpp:314] Iteration 12200 (1.86943 iter/s, 53.4923s/100 iter), loss = 0.0580562
I1003 00:20:17.864598 31421 solver.cpp:336]     Train net output #0: loss = 0.0580562 (* 1 = 0.0580562 loss)
I1003 00:20:17.864604 31421 sgd_solver.cpp:136] Iteration 12200, lr = 0.01, m = 0.9
I1003 00:21:12.003595 31421 solver.cpp:314] Iteration 12300 (1.84714 iter/s, 54.1376s/100 iter), loss = 0.0529882
I1003 00:21:12.008213 31421 solver.cpp:336]     Train net output #0: loss = 0.0529882 (* 1 = 0.0529882 loss)
I1003 00:21:12.008224 31421 sgd_solver.cpp:136] Iteration 12300, lr = 0.01, m = 0.9
I1003 00:22:02.333295 31424 data_reader.cpp:305] Starting prefetch of epoch 5
I1003 00:22:06.052232 31421 solver.cpp:314] Iteration 12400 (1.85024 iter/s, 54.0471s/100 iter), loss = 0.0520169
I1003 00:22:06.052256 31421 solver.cpp:336]     Train net output #0: loss = 0.0520168 (* 1 = 0.0520168 loss)
I1003 00:22:06.052263 31421 sgd_solver.cpp:136] Iteration 12400, lr = 0.01, m = 0.9
I1003 00:23:00.764441 31421 solver.cpp:314] Iteration 12500 (1.8278 iter/s, 54.7107s/100 iter), loss = 0.0760447
I1003 00:23:00.768208 31421 solver.cpp:336]     Train net output #0: loss = 0.0760446 (* 1 = 0.0760446 loss)
I1003 00:23:00.768218 31421 sgd_solver.cpp:136] Iteration 12500, lr = 0.01, m = 0.9
I1003 00:23:54.716205 31421 solver.cpp:314] Iteration 12600 (1.85356 iter/s, 53.9502s/100 iter), loss = 0.0397124
I1003 00:23:54.724202 31421 solver.cpp:336]     Train net output #0: loss = 0.0397124 (* 1 = 0.0397124 loss)
I1003 00:23:54.724216 31421 sgd_solver.cpp:136] Iteration 12600, lr = 0.01, m = 0.9
I1003 00:24:47.695338 31421 solver.cpp:314] Iteration 12700 (1.88759 iter/s, 52.9776s/100 iter), loss = 0.144797
I1003 00:24:47.695389 31421 solver.cpp:336]     Train net output #0: loss = 0.144797 (* 1 = 0.144797 loss)
I1003 00:24:47.695394 31421 sgd_solver.cpp:136] Iteration 12700, lr = 0.01, m = 0.9
I1003 00:24:59.248306 31432 data_reader.cpp:305] Starting prefetch of epoch 7
I1003 00:25:33.175987 31421 solver.cpp:314] Iteration 12800 (2.1988 iter/s, 45.4794s/100 iter), loss = 0.0508298
I1003 00:25:33.176062 31421 solver.cpp:336]     Train net output #0: loss = 0.0508298 (* 1 = 0.0508298 loss)
I1003 00:25:33.176069 31421 sgd_solver.cpp:136] Iteration 12800, lr = 0.01, m = 0.9
I1003 00:26:25.729528 31421 solver.cpp:314] Iteration 12900 (1.90287 iter/s, 52.5521s/100 iter), loss = 0.0687333
I1003 00:26:25.729588 31421 solver.cpp:336]     Train net output #0: loss = 0.0687333 (* 1 = 0.0687333 loss)
I1003 00:26:25.729593 31421 sgd_solver.cpp:136] Iteration 12900, lr = 0.01, m = 0.9
I1003 00:27:19.309577 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.72 sparsity_achieved=0.707863 iter=13000
I1003 00:37:53.243867 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 00:37:53.249953 31421 solver.cpp:352] Sparsity after update:
I1003 00:37:53.253855 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 00:37:53.253870 31421 net.cpp:2312] conv1a_param_0(0.277) 
I1003 00:37:53.253883 31421 net.cpp:2312] conv1b_param_0(0.671) 
I1003 00:37:53.253886 31421 net.cpp:2312] ctx_conv1_param_0(0.719) 
I1003 00:37:53.253890 31421 net.cpp:2312] ctx_conv2_param_0(0.719) 
I1003 00:37:53.253892 31421 net.cpp:2312] ctx_conv3_param_0(0.719) 
I1003 00:37:53.253895 31421 net.cpp:2312] ctx_conv4_param_0(0.719) 
I1003 00:37:53.253898 31421 net.cpp:2312] ctx_final_param_0(0.314) 
I1003 00:37:53.253901 31421 net.cpp:2312] out3a_param_0(0.719) 
I1003 00:37:53.253906 31421 net.cpp:2312] out5a_param_0(0.72) 
I1003 00:37:53.253911 31421 net.cpp:2312] res2a_branch2a_param_0(0.718) 
I1003 00:37:53.253916 31421 net.cpp:2312] res2a_branch2b_param_0(0.668) 
I1003 00:37:53.253921 31421 net.cpp:2312] res3a_branch2a_param_0(0.719) 
I1003 00:37:53.253924 31421 net.cpp:2312] res3a_branch2b_param_0(0.715) 
I1003 00:37:53.253929 31421 net.cpp:2312] res4a_branch2a_param_0(0.72) 
I1003 00:37:53.253933 31421 net.cpp:2312] res4a_branch2b_param_0(0.718) 
I1003 00:37:53.253938 31421 net.cpp:2312] res5a_branch2a_param_0(0.719) 
I1003 00:37:53.253942 31421 net.cpp:2312] res5a_branch2b_param_0(0.72) 
I1003 00:37:53.253947 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.93212e+06/2.69117e+06) 0.718
I1003 00:37:53.679150 31421 solver.cpp:314] Iteration 13000 (0.145364 iter/s, 687.93s/100 iter), loss = 0.0539055
I1003 00:37:53.679184 31421 solver.cpp:336]     Train net output #0: loss = 0.0539054 (* 1 = 0.0539054 loss)
I1003 00:37:53.679191 31421 sgd_solver.cpp:136] Iteration 13000, lr = 0.01, m = 0.9
I1003 00:38:22.207499 31344 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 00:38:48.278169 31421 solver.cpp:314] Iteration 13100 (1.83159 iter/s, 54.5974s/100 iter), loss = 0.0774539
I1003 00:38:48.278254 31421 solver.cpp:336]     Train net output #0: loss = 0.0774538 (* 1 = 0.0774538 loss)
I1003 00:38:48.278262 31421 sgd_solver.cpp:136] Iteration 13100, lr = 0.01, m = 0.9
I1003 00:38:51.027230 31343 blocking_queue.cpp:40] Waiting for datum
I1003 00:39:45.033028 31421 solver.cpp:314] Iteration 13200 (1.76201 iter/s, 56.7533s/100 iter), loss = 0.0497513
I1003 00:39:45.033264 31421 solver.cpp:336]     Train net output #0: loss = 0.0497512 (* 1 = 0.0497512 loss)
I1003 00:39:45.033375 31421 sgd_solver.cpp:136] Iteration 13200, lr = 0.01, m = 0.9
I1003 00:39:56.264283 31342 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 00:40:41.900292 31421 solver.cpp:314] Iteration 13300 (1.75853 iter/s, 56.8657s/100 iter), loss = 0.0668129
I1003 00:40:41.900359 31421 solver.cpp:336]     Train net output #0: loss = 0.0668128 (* 1 = 0.0668128 loss)
I1003 00:40:41.900369 31421 sgd_solver.cpp:136] Iteration 13300, lr = 0.01, m = 0.9
I1003 00:41:37.944622 31421 solver.cpp:314] Iteration 13400 (1.78435 iter/s, 56.0427s/100 iter), loss = 0.0515638
I1003 00:41:37.944691 31421 solver.cpp:336]     Train net output #0: loss = 0.0515637 (* 1 = 0.0515637 loss)
I1003 00:41:37.944699 31421 sgd_solver.cpp:136] Iteration 13400, lr = 0.01, m = 0.9
I1003 00:42:33.690515 31421 solver.cpp:314] Iteration 13500 (1.7939 iter/s, 55.7443s/100 iter), loss = 0.0854035
I1003 00:42:33.693640 31421 solver.cpp:336]     Train net output #0: loss = 0.0854034 (* 1 = 0.0854034 loss)
I1003 00:42:33.693648 31421 sgd_solver.cpp:136] Iteration 13500, lr = 0.01, m = 0.9
I1003 00:43:00.151938 31344 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 00:43:26.553093 31421 solver.cpp:314] Iteration 13600 (1.89175 iter/s, 52.8611s/100 iter), loss = 0.0681072
I1003 00:43:26.553154 31421 solver.cpp:336]     Train net output #0: loss = 0.0681071 (* 1 = 0.0681071 loss)
I1003 00:43:26.553165 31421 sgd_solver.cpp:136] Iteration 13600, lr = 0.01, m = 0.9
I1003 00:44:19.786871 31421 solver.cpp:314] Iteration 13700 (1.87856 iter/s, 53.2323s/100 iter), loss = 0.0548488
I1003 00:44:19.794311 31421 solver.cpp:336]     Train net output #0: loss = 0.0548487 (* 1 = 0.0548487 loss)
I1003 00:44:19.794463 31421 sgd_solver.cpp:136] Iteration 13700, lr = 0.01, m = 0.9
I1003 00:45:12.136330 31421 solver.cpp:314] Iteration 13800 (1.91029 iter/s, 52.348s/100 iter), loss = 0.0598893
I1003 00:45:12.144217 31421 solver.cpp:336]     Train net output #0: loss = 0.0598892 (* 1 = 0.0598892 loss)
I1003 00:45:12.144230 31421 sgd_solver.cpp:136] Iteration 13800, lr = 0.01, m = 0.9
I1003 00:45:54.509567 31430 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 00:46:05.260646 31421 solver.cpp:314] Iteration 13900 (1.88243 iter/s, 53.1228s/100 iter), loss = 0.0612105
I1003 00:46:05.260674 31421 solver.cpp:336]     Train net output #0: loss = 0.0612104 (* 1 = 0.0612104 loss)
I1003 00:46:05.260681 31421 sgd_solver.cpp:136] Iteration 13900, lr = 0.01, m = 0.9
I1003 00:46:57.576719 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.73 sparsity_achieved=0.717949 iter=14000
I1003 00:58:16.617394 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 00:58:16.649070 31421 solver.cpp:352] Sparsity after update:
I1003 00:58:16.661986 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 00:58:16.662004 31421 net.cpp:2312] conv1a_param_0(0.285) 
I1003 00:58:16.662010 31421 net.cpp:2312] conv1b_param_0(0.679) 
I1003 00:58:16.662014 31421 net.cpp:2312] ctx_conv1_param_0(0.729) 
I1003 00:58:16.662015 31421 net.cpp:2312] ctx_conv2_param_0(0.729) 
I1003 00:58:16.662016 31421 net.cpp:2312] ctx_conv3_param_0(0.729) 
I1003 00:58:16.662019 31421 net.cpp:2312] ctx_conv4_param_0(0.729) 
I1003 00:58:16.662020 31421 net.cpp:2312] ctx_final_param_0(0.271) 
I1003 00:58:16.662022 31421 net.cpp:2312] out3a_param_0(0.729) 
I1003 00:58:16.662024 31421 net.cpp:2312] out5a_param_0(0.73) 
I1003 00:58:16.662026 31421 net.cpp:2312] res2a_branch2a_param_0(0.728) 
I1003 00:58:16.662029 31421 net.cpp:2312] res2a_branch2b_param_0(0.674) 
I1003 00:58:16.662030 31421 net.cpp:2312] res3a_branch2a_param_0(0.729) 
I1003 00:58:16.662034 31421 net.cpp:2312] res3a_branch2b_param_0(0.724) 
I1003 00:58:16.662035 31421 net.cpp:2312] res4a_branch2a_param_0(0.729) 
I1003 00:58:16.662037 31421 net.cpp:2312] res4a_branch2b_param_0(0.729) 
I1003 00:58:16.662039 31421 net.cpp:2312] res5a_branch2a_param_0(0.729) 
I1003 00:58:16.662040 31421 net.cpp:2312] res5a_branch2b_param_0(0.729) 
I1003 00:58:16.662042 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.95855e+06/2.69117e+06) 0.728
I1003 00:58:16.662055 31421 solver.cpp:567] Iteration 14000, Testing net (#0)
I1003 00:58:31.775271 31405 data_reader.cpp:305] Starting prefetch of epoch 1
I1003 00:58:39.800216 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.953374
I1003 00:58:39.800237 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 00:58:39.800245 31421 solver.cpp:659]     Test net output #2: loss = 0.146746 (* 1 = 0.146746 loss)
I1003 00:58:39.800274 31421 solver.cpp:265] [MultiGPU] Tests completed in 23.1376s
I1003 00:58:40.284667 31421 solver.cpp:314] Iteration 14000 (0.13245 iter/s, 755.003s/100 iter), loss = 0.0905177
I1003 00:58:40.284833 31421 solver.cpp:336]     Train net output #0: loss = 0.0905176 (* 1 = 0.0905176 loss)
I1003 00:58:40.284843 31421 sgd_solver.cpp:136] Iteration 14000, lr = 0.01, m = 0.9
I1003 00:59:37.359951 31421 solver.cpp:314] Iteration 14100 (1.75212 iter/s, 57.0737s/100 iter), loss = 0.0976564
I1003 00:59:37.360146 31421 solver.cpp:336]     Train net output #0: loss = 0.0976563 (* 1 = 0.0976563 loss)
I1003 00:59:37.360169 31421 sgd_solver.cpp:136] Iteration 14100, lr = 0.01, m = 0.9
I1003 01:00:36.157989 31421 solver.cpp:314] Iteration 14200 (1.70079 iter/s, 58.7964s/100 iter), loss = 0.056203
I1003 01:00:36.158066 31421 solver.cpp:336]     Train net output #0: loss = 0.0562029 (* 1 = 0.0562029 loss)
I1003 01:00:36.158074 31421 sgd_solver.cpp:136] Iteration 14200, lr = 0.01, m = 0.9
I1003 01:00:42.706260 31424 data_reader.cpp:305] Starting prefetch of epoch 6
I1003 01:01:26.214946 31431 blocking_queue.cpp:40] Waiting for datum
I1003 01:01:33.236618 31421 solver.cpp:314] Iteration 14300 (1.75202 iter/s, 57.077s/100 iter), loss = 0.0722587
I1003 01:01:33.236649 31421 solver.cpp:336]     Train net output #0: loss = 0.0722586 (* 1 = 0.0722586 loss)
I1003 01:01:33.236654 31421 sgd_solver.cpp:136] Iteration 14300, lr = 0.01, m = 0.9
I1003 01:02:31.428463 31421 solver.cpp:314] Iteration 14400 (1.7185 iter/s, 58.1902s/100 iter), loss = 0.0739032
I1003 01:02:31.428547 31421 solver.cpp:336]     Train net output #0: loss = 0.0739031 (* 1 = 0.0739031 loss)
I1003 01:02:31.428555 31421 sgd_solver.cpp:136] Iteration 14400, lr = 0.01, m = 0.9
I1003 01:03:28.834911 31421 solver.cpp:314] Iteration 14500 (1.74201 iter/s, 57.4048s/100 iter), loss = 0.0713184
I1003 01:03:28.835002 31421 solver.cpp:336]     Train net output #0: loss = 0.0713183 (* 1 = 0.0713183 loss)
I1003 01:03:28.835011 31421 sgd_solver.cpp:136] Iteration 14500, lr = 0.01, m = 0.9
I1003 01:03:50.935662 31344 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 01:04:21.004819 31421 solver.cpp:314] Iteration 14600 (1.91687 iter/s, 52.1684s/100 iter), loss = 0.0751378
I1003 01:04:21.004907 31421 solver.cpp:336]     Train net output #0: loss = 0.0751378 (* 1 = 0.0751378 loss)
I1003 01:04:21.004914 31421 sgd_solver.cpp:136] Iteration 14600, lr = 0.01, m = 0.9
I1003 01:05:14.448070 31421 solver.cpp:314] Iteration 14700 (1.8712 iter/s, 53.4417s/100 iter), loss = 0.0609434
I1003 01:05:14.456218 31421 solver.cpp:336]     Train net output #0: loss = 0.0609433 (* 1 = 0.0609433 loss)
I1003 01:05:14.456231 31421 sgd_solver.cpp:136] Iteration 14700, lr = 0.01, m = 0.9
I1003 01:05:18.352607 31424 data_reader.cpp:305] Starting prefetch of epoch 7
I1003 01:06:08.037900 31421 solver.cpp:314] Iteration 14800 (1.86608 iter/s, 53.5883s/100 iter), loss = 0.0632126
I1003 01:06:08.037981 31421 solver.cpp:336]     Train net output #0: loss = 0.0632125 (* 1 = 0.0632125 loss)
I1003 01:06:08.037989 31421 sgd_solver.cpp:136] Iteration 14800, lr = 0.01, m = 0.9
I1003 01:07:02.056221 31421 solver.cpp:314] Iteration 14900 (1.85128 iter/s, 54.0168s/100 iter), loss = 0.0960844
I1003 01:07:02.060221 31421 solver.cpp:336]     Train net output #0: loss = 0.0960843 (* 1 = 0.0960843 loss)
I1003 01:07:02.060235 31421 sgd_solver.cpp:136] Iteration 14900, lr = 0.01, m = 0.9
I1003 01:07:54.793567 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.74 sparsity_achieved=0.727769 iter=15000
I1003 01:19:29.661206 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 01:19:29.701839 31421 solver.cpp:352] Sparsity after update:
I1003 01:19:29.705894 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 01:19:29.705914 31421 net.cpp:2312] conv1a_param_0(0.286) 
I1003 01:19:29.705927 31421 net.cpp:2312] conv1b_param_0(0.685) 
I1003 01:19:29.705931 31421 net.cpp:2312] ctx_conv1_param_0(0.74) 
I1003 01:19:29.705935 31421 net.cpp:2312] ctx_conv2_param_0(0.74) 
I1003 01:19:29.705938 31421 net.cpp:2312] ctx_conv3_param_0(0.74) 
I1003 01:19:29.705943 31421 net.cpp:2312] ctx_conv4_param_0(0.74) 
I1003 01:19:29.705946 31421 net.cpp:2312] ctx_final_param_0(0.364) 
I1003 01:19:29.705950 31421 net.cpp:2312] out3a_param_0(0.74) 
I1003 01:19:29.705952 31421 net.cpp:2312] out5a_param_0(0.74) 
I1003 01:19:29.705955 31421 net.cpp:2312] res2a_branch2a_param_0(0.738) 
I1003 01:19:29.705958 31421 net.cpp:2312] res2a_branch2b_param_0(0.678) 
I1003 01:19:29.705961 31421 net.cpp:2312] res3a_branch2a_param_0(0.739) 
I1003 01:19:29.705965 31421 net.cpp:2312] res3a_branch2b_param_0(0.733) 
I1003 01:19:29.705967 31421 net.cpp:2312] res4a_branch2a_param_0(0.74) 
I1003 01:19:29.705971 31421 net.cpp:2312] res4a_branch2b_param_0(0.739) 
I1003 01:19:29.705973 31421 net.cpp:2312] res5a_branch2a_param_0(0.739) 
I1003 01:19:29.705976 31421 net.cpp:2312] res5a_branch2b_param_0(0.74) 
I1003 01:19:29.705979 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.98614e+06/2.69117e+06) 0.738
I1003 01:19:30.260866 31421 solver.cpp:314] Iteration 15000 (0.133657 iter/s, 748.184s/100 iter), loss = 0.0551993
I1003 01:19:30.261059 31421 solver.cpp:336]     Train net output #0: loss = 0.0551992 (* 1 = 0.0551992 loss)
I1003 01:19:30.261184 31421 sgd_solver.cpp:136] Iteration 15000, lr = 0.01, m = 0.9
I1003 01:19:51.238512 31425 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 01:20:24.736517 31421 solver.cpp:314] Iteration 15100 (1.83573 iter/s, 54.4741s/100 iter), loss = 0.0933231
I1003 01:20:24.740217 31421 solver.cpp:336]     Train net output #0: loss = 0.093323 (* 1 = 0.093323 loss)
I1003 01:20:24.740226 31421 sgd_solver.cpp:136] Iteration 15100, lr = 0.01, m = 0.9
I1003 01:21:19.643023 31421 solver.cpp:314] Iteration 15200 (1.82133 iter/s, 54.905s/100 iter), loss = 0.0838918
I1003 01:21:19.643105 31421 solver.cpp:336]     Train net output #0: loss = 0.0838917 (* 1 = 0.0838917 loss)
I1003 01:21:19.643115 31421 sgd_solver.cpp:136] Iteration 15200, lr = 0.01, m = 0.9
I1003 01:22:14.526346 31421 solver.cpp:314] Iteration 15300 (1.8221 iter/s, 54.8818s/100 iter), loss = 0.0382713
I1003 01:22:14.526624 31421 solver.cpp:336]     Train net output #0: loss = 0.0382711 (* 1 = 0.0382711 loss)
I1003 01:22:14.526631 31421 sgd_solver.cpp:136] Iteration 15300, lr = 0.01, m = 0.9
I1003 01:22:44.578541 31421 blocking_queue.cpp:40] Data layer prefetch queue empty
I1003 01:22:52.729774 31430 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 01:23:10.750141 31421 solver.cpp:314] Iteration 15400 (1.77866 iter/s, 56.2222s/100 iter), loss = 0.0662412
I1003 01:23:10.750171 31421 solver.cpp:336]     Train net output #0: loss = 0.0662411 (* 1 = 0.0662411 loss)
I1003 01:23:10.750176 31421 sgd_solver.cpp:136] Iteration 15400, lr = 0.01, m = 0.9
I1003 01:24:04.737756 31421 solver.cpp:314] Iteration 15500 (1.85233 iter/s, 53.9861s/100 iter), loss = 0.0685165
I1003 01:24:04.737843 31421 solver.cpp:336]     Train net output #0: loss = 0.0685164 (* 1 = 0.0685164 loss)
I1003 01:24:04.737851 31421 sgd_solver.cpp:136] Iteration 15500, lr = 0.01, m = 0.9
I1003 01:24:56.544205 31421 solver.cpp:314] Iteration 15600 (1.93032 iter/s, 51.805s/100 iter), loss = 0.0804961
I1003 01:24:56.552261 31421 solver.cpp:336]     Train net output #0: loss = 0.080496 (* 1 = 0.080496 loss)
I1003 01:24:56.552287 31421 sgd_solver.cpp:136] Iteration 15600, lr = 0.01, m = 0.9
I1003 01:25:49.425652 31432 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 01:25:50.056217 31421 solver.cpp:314] Iteration 15700 (1.86879 iter/s, 53.5105s/100 iter), loss = 0.0828777
I1003 01:25:50.056298 31421 solver.cpp:336]     Train net output #0: loss = 0.0828776 (* 1 = 0.0828776 loss)
I1003 01:25:50.056316 31421 sgd_solver.cpp:136] Iteration 15700, lr = 0.01, m = 0.9
I1003 01:26:42.962450 31421 solver.cpp:314] Iteration 15800 (1.89019 iter/s, 52.9047s/100 iter), loss = 0.0832862
I1003 01:26:42.962517 31421 solver.cpp:336]     Train net output #0: loss = 0.0832861 (* 1 = 0.0832861 loss)
I1003 01:26:42.962524 31421 sgd_solver.cpp:136] Iteration 15800, lr = 0.01, m = 0.9
I1003 01:27:16.991061 31342 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 01:27:36.797462 31421 solver.cpp:314] Iteration 15900 (1.85758 iter/s, 53.8335s/100 iter), loss = 0.0497273
I1003 01:27:36.797526 31421 solver.cpp:336]     Train net output #0: loss = 0.0497272 (* 1 = 0.0497272 loss)
I1003 01:27:36.797541 31421 sgd_solver.cpp:136] Iteration 15900, lr = 0.01, m = 0.9
I1003 01:28:29.435603 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.75 sparsity_achieved=0.73802 iter=16000
I1003 01:28:33.280114 31543 blocking_queue.cpp:40] Waiting for datum
I1003 01:40:11.339787 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 01:40:11.345353 31421 solver.cpp:352] Sparsity after update:
I1003 01:40:11.363095 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 01:40:11.363131 31421 net.cpp:2312] conv1a_param_0(0.294) 
I1003 01:40:11.363167 31421 net.cpp:2312] conv1b_param_0(0.687) 
I1003 01:40:11.363176 31421 net.cpp:2312] ctx_conv1_param_0(0.748) 
I1003 01:40:11.363184 31421 net.cpp:2312] ctx_conv2_param_0(0.748) 
I1003 01:40:11.363191 31421 net.cpp:2312] ctx_conv3_param_0(0.748) 
I1003 01:40:11.363199 31421 net.cpp:2312] ctx_conv4_param_0(0.748) 
I1003 01:40:11.363206 31421 net.cpp:2312] ctx_final_param_0(0.279) 
I1003 01:40:11.363214 31421 net.cpp:2312] out3a_param_0(0.748) 
I1003 01:40:11.363220 31421 net.cpp:2312] out5a_param_0(0.75) 
I1003 01:40:11.363227 31421 net.cpp:2312] res2a_branch2a_param_0(0.744) 
I1003 01:40:11.363236 31421 net.cpp:2312] res2a_branch2b_param_0(0.683) 
I1003 01:40:11.363245 31421 net.cpp:2312] res3a_branch2a_param_0(0.748) 
I1003 01:40:11.363252 31421 net.cpp:2312] res3a_branch2b_param_0(0.739) 
I1003 01:40:11.363260 31421 net.cpp:2312] res4a_branch2a_param_0(0.749) 
I1003 01:40:11.363267 31421 net.cpp:2312] res4a_branch2b_param_0(0.747) 
I1003 01:40:11.363274 31421 net.cpp:2312] res5a_branch2a_param_0(0.749) 
I1003 01:40:11.363283 31421 net.cpp:2312] res5a_branch2b_param_0(0.749) 
I1003 01:40:11.363291 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.01119e+06/2.69117e+06) 0.747
I1003 01:40:11.363320 31421 solver.cpp:567] Iteration 16000, Testing net (#0)
I1003 01:40:17.720413 31418 data_reader.cpp:305] Starting prefetch of epoch 2
I1003 01:40:34.426466 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.95172
I1003 01:40:34.426488 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 01:40:34.426496 31421 solver.cpp:659]     Test net output #2: loss = 0.136358 (* 1 = 0.136358 loss)
I1003 01:40:34.426527 31421 solver.cpp:265] [MultiGPU] Tests completed in 23.0626s
I1003 01:40:34.856166 31421 solver.cpp:314] Iteration 16000 (0.128529 iter/s, 778.037s/100 iter), loss = 0.0821494
I1003 01:40:34.856209 31421 solver.cpp:336]     Train net output #0: loss = 0.0821493 (* 1 = 0.0821493 loss)
I1003 01:40:34.856217 31421 sgd_solver.cpp:136] Iteration 16000, lr = 0.01, m = 0.9
I1003 01:41:31.045374 31421 solver.cpp:314] Iteration 16100 (1.77975 iter/s, 56.1876s/100 iter), loss = 0.0800158
I1003 01:41:31.045640 31421 solver.cpp:336]     Train net output #0: loss = 0.0800157 (* 1 = 0.0800157 loss)
I1003 01:41:31.045650 31421 sgd_solver.cpp:136] Iteration 16100, lr = 0.01, m = 0.9
I1003 01:42:24.332545 31344 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 01:42:27.494772 31421 solver.cpp:314] Iteration 16200 (1.77155 iter/s, 56.4478s/100 iter), loss = 0.106411
I1003 01:42:27.494796 31421 solver.cpp:336]     Train net output #0: loss = 0.106411 (* 1 = 0.106411 loss)
I1003 01:42:27.494799 31421 sgd_solver.cpp:136] Iteration 16200, lr = 0.01, m = 0.9
I1003 01:43:24.525969 31421 solver.cpp:314] Iteration 16300 (1.75348 iter/s, 57.0296s/100 iter), loss = 0.0611209
I1003 01:43:24.526039 31421 solver.cpp:336]     Train net output #0: loss = 0.0611208 (* 1 = 0.0611208 loss)
I1003 01:43:24.526046 31421 sgd_solver.cpp:136] Iteration 16300, lr = 0.01, m = 0.9
I1003 01:43:58.630453 31425 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 01:44:20.228204 31421 solver.cpp:314] Iteration 16400 (1.79531 iter/s, 55.7007s/100 iter), loss = 0.0543378
I1003 01:44:20.228235 31421 solver.cpp:336]     Train net output #0: loss = 0.0543377 (* 1 = 0.0543377 loss)
I1003 01:44:20.228241 31421 sgd_solver.cpp:136] Iteration 16400, lr = 0.01, m = 0.9
I1003 01:45:13.924031 31421 solver.cpp:314] Iteration 16500 (1.86239 iter/s, 53.6943s/100 iter), loss = 0.0570983
I1003 01:45:13.924098 31421 solver.cpp:336]     Train net output #0: loss = 0.0570982 (* 1 = 0.0570982 loss)
I1003 01:45:13.924106 31421 sgd_solver.cpp:136] Iteration 16500, lr = 0.01, m = 0.9
I1003 01:46:06.908443 31421 solver.cpp:314] Iteration 16600 (1.8874 iter/s, 52.9829s/100 iter), loss = 0.0649512
I1003 01:46:06.908558 31421 solver.cpp:336]     Train net output #0: loss = 0.064951 (* 1 = 0.064951 loss)
I1003 01:46:06.908566 31421 sgd_solver.cpp:136] Iteration 16600, lr = 0.01, m = 0.9
I1003 01:46:50.816854 31430 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 01:46:55.195549 31421 solver.cpp:314] Iteration 16700 (2.071 iter/s, 48.2857s/100 iter), loss = 0.131313
I1003 01:46:55.195587 31421 solver.cpp:336]     Train net output #0: loss = 0.131313 (* 1 = 0.131313 loss)
I1003 01:46:55.195593 31421 sgd_solver.cpp:136] Iteration 16700, lr = 0.01, m = 0.9
I1003 01:47:42.124279 31421 solver.cpp:314] Iteration 16800 (2.13095 iter/s, 46.9274s/100 iter), loss = 0.0856629
I1003 01:47:42.124372 31421 solver.cpp:336]     Train net output #0: loss = 0.0856627 (* 1 = 0.0856627 loss)
I1003 01:47:42.124382 31421 sgd_solver.cpp:136] Iteration 16800, lr = 0.01, m = 0.9
I1003 01:48:35.531265 31421 solver.cpp:314] Iteration 16900 (1.87247 iter/s, 53.4055s/100 iter), loss = 0.0648334
I1003 01:48:35.531311 31421 solver.cpp:336]     Train net output #0: loss = 0.0648332 (* 1 = 0.0648332 loss)
I1003 01:48:35.531316 31421 sgd_solver.cpp:136] Iteration 16900, lr = 0.01, m = 0.9
I1003 01:49:29.548516 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.76 sparsity_achieved=0.747331 iter=17000
I1003 01:55:34.578938 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 01:55:34.583017 31421 solver.cpp:352] Sparsity after update:
I1003 01:55:34.584605 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 01:55:34.584612 31421 net.cpp:2312] conv1a_param_0(0.294) 
I1003 01:55:34.584622 31421 net.cpp:2312] conv1b_param_0(0.692) 
I1003 01:55:34.584625 31421 net.cpp:2312] ctx_conv1_param_0(0.759) 
I1003 01:55:34.584626 31421 net.cpp:2312] ctx_conv2_param_0(0.759) 
I1003 01:55:34.584628 31421 net.cpp:2312] ctx_conv3_param_0(0.759) 
I1003 01:55:34.584630 31421 net.cpp:2312] ctx_conv4_param_0(0.759) 
I1003 01:55:34.584632 31421 net.cpp:2312] ctx_final_param_0(0.323) 
I1003 01:55:34.584635 31421 net.cpp:2312] out3a_param_0(0.759) 
I1003 01:55:34.584636 31421 net.cpp:2312] out5a_param_0(0.76) 
I1003 01:55:34.584638 31421 net.cpp:2312] res2a_branch2a_param_0(0.754) 
I1003 01:55:34.584640 31421 net.cpp:2312] res2a_branch2b_param_0(0.688) 
I1003 01:55:34.584642 31421 net.cpp:2312] res3a_branch2a_param_0(0.758) 
I1003 01:55:34.584643 31421 net.cpp:2312] res3a_branch2b_param_0(0.747) 
I1003 01:55:34.584646 31421 net.cpp:2312] res4a_branch2a_param_0(0.76) 
I1003 01:55:34.584648 31421 net.cpp:2312] res4a_branch2b_param_0(0.757) 
I1003 01:55:34.584650 31421 net.cpp:2312] res5a_branch2a_param_0(0.76) 
I1003 01:55:34.584652 31421 net.cpp:2312] res5a_branch2b_param_0(0.76) 
I1003 01:55:34.584655 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.03918e+06/2.69117e+06) 0.758
I1003 01:55:34.760300 31421 solver.cpp:314] Iteration 17000 (0.23854 iter/s, 419.217s/100 iter), loss = 0.11049
I1003 01:55:34.760324 31421 solver.cpp:336]     Train net output #0: loss = 0.11049 (* 1 = 0.11049 loss)
I1003 01:55:34.760331 31421 sgd_solver.cpp:136] Iteration 17000, lr = 0.01, m = 0.9
I1003 01:55:50.611280 31344 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 01:56:51.673961 31421 solver.cpp:314] Iteration 17100 (1.3002 iter/s, 76.9114s/100 iter), loss = 0.0727533
I1003 01:56:51.674036 31421 solver.cpp:336]     Train net output #0: loss = 0.0727531 (* 1 = 0.0727531 loss)
I1003 01:56:51.674043 31421 sgd_solver.cpp:136] Iteration 17100, lr = 0.01, m = 0.9
I1003 01:58:01.172425 31421 solver.cpp:314] Iteration 17200 (1.43892 iter/s, 69.4965s/100 iter), loss = 0.0808837
I1003 01:58:01.172524 31421 solver.cpp:336]     Train net output #0: loss = 0.0808836 (* 1 = 0.0808836 loss)
I1003 01:58:01.172530 31421 sgd_solver.cpp:136] Iteration 17200, lr = 0.01, m = 0.9
I1003 01:58:17.210418 31431 blocking_queue.cpp:40] Waiting for datum
I1003 01:59:00.952729 31421 solver.cpp:314] Iteration 17300 (1.67284 iter/s, 59.7786s/100 iter), loss = 0.0505025
I1003 01:59:00.952782 31421 solver.cpp:336]     Train net output #0: loss = 0.0505023 (* 1 = 0.0505023 loss)
I1003 01:59:00.952787 31421 sgd_solver.cpp:136] Iteration 17300, lr = 0.01, m = 0.9
I1003 01:59:20.894254 31344 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 01:59:42.785856 31421 solver.cpp:314] Iteration 17400 (2.39052 iter/s, 41.8319s/100 iter), loss = 0.0459142
I1003 01:59:42.785931 31421 solver.cpp:336]     Train net output #0: loss = 0.045914 (* 1 = 0.045914 loss)
I1003 01:59:42.785938 31421 sgd_solver.cpp:136] Iteration 17400, lr = 0.01, m = 0.9
I1003 02:00:07.658404 31421 solver.cpp:314] Iteration 17500 (4.02061 iter/s, 24.8718s/100 iter), loss = 0.0485386
I1003 02:00:07.658430 31421 solver.cpp:336]     Train net output #0: loss = 0.0485384 (* 1 = 0.0485384 loss)
I1003 02:00:07.658437 31421 sgd_solver.cpp:136] Iteration 17500, lr = 0.01, m = 0.9
I1003 02:00:25.546370 31421 solver.cpp:314] Iteration 17600 (5.59051 iter/s, 17.8874s/100 iter), loss = 0.0750148
I1003 02:00:25.546480 31421 solver.cpp:336]     Train net output #0: loss = 0.0750147 (* 1 = 0.0750147 loss)
I1003 02:00:25.546486 31421 sgd_solver.cpp:136] Iteration 17600, lr = 0.01, m = 0.9
I1003 02:00:40.112318 31344 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 02:00:43.560591 31421 solver.cpp:314] Iteration 17700 (5.55133 iter/s, 18.0137s/100 iter), loss = 0.0657073
I1003 02:00:43.560617 31421 solver.cpp:336]     Train net output #0: loss = 0.0657071 (* 1 = 0.0657071 loss)
I1003 02:00:43.560621 31421 sgd_solver.cpp:136] Iteration 17700, lr = 0.01, m = 0.9
I1003 02:01:01.672631 31421 solver.cpp:314] Iteration 17800 (5.52135 iter/s, 18.1115s/100 iter), loss = 0.0599435
I1003 02:01:01.672739 31421 solver.cpp:336]     Train net output #0: loss = 0.0599434 (* 1 = 0.0599434 loss)
I1003 02:01:01.672746 31421 sgd_solver.cpp:136] Iteration 17800, lr = 0.01, m = 0.9
I1003 02:01:23.416827 31421 solver.cpp:314] Iteration 17900 (4.59906 iter/s, 21.7436s/100 iter), loss = 0.0878052
I1003 02:01:23.416853 31421 solver.cpp:336]     Train net output #0: loss = 0.0878051 (* 1 = 0.0878051 loss)
I1003 02:01:23.416858 31421 sgd_solver.cpp:136] Iteration 17900, lr = 0.01, m = 0.9
I1003 02:01:41.496222 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.77 sparsity_achieved=0.757732 iter=18000
I1003 02:01:58.307312 31538 data_reader.cpp:305] Starting prefetch of epoch 3
I1003 02:04:10.766013 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 02:04:10.770089 31421 solver.cpp:352] Sparsity after update:
I1003 02:04:10.771687 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:04:10.771693 31421 net.cpp:2312] conv1a_param_0(0.295) 
I1003 02:04:10.771700 31421 net.cpp:2312] conv1b_param_0(0.699) 
I1003 02:04:10.771703 31421 net.cpp:2312] ctx_conv1_param_0(0.769) 
I1003 02:04:10.771704 31421 net.cpp:2312] ctx_conv2_param_0(0.769) 
I1003 02:04:10.771706 31421 net.cpp:2312] ctx_conv3_param_0(0.769) 
I1003 02:04:10.771708 31421 net.cpp:2312] ctx_conv4_param_0(0.769) 
I1003 02:04:10.771710 31421 net.cpp:2312] ctx_final_param_0(0.382) 
I1003 02:04:10.771713 31421 net.cpp:2312] out3a_param_0(0.769) 
I1003 02:04:10.771713 31421 net.cpp:2312] out5a_param_0(0.77) 
I1003 02:04:10.771716 31421 net.cpp:2312] res2a_branch2a_param_0(0.763) 
I1003 02:04:10.771718 31421 net.cpp:2312] res2a_branch2b_param_0(0.692) 
I1003 02:04:10.771719 31421 net.cpp:2312] res3a_branch2a_param_0(0.768) 
I1003 02:04:10.771721 31421 net.cpp:2312] res3a_branch2b_param_0(0.755) 
I1003 02:04:10.771723 31421 net.cpp:2312] res4a_branch2a_param_0(0.77) 
I1003 02:04:10.771726 31421 net.cpp:2312] res4a_branch2b_param_0(0.767) 
I1003 02:04:10.771730 31421 net.cpp:2312] res5a_branch2a_param_0(0.77) 
I1003 02:04:10.771733 31421 net.cpp:2312] res5a_branch2b_param_0(0.77) 
I1003 02:04:10.771736 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.06648e+06/2.69117e+06) 0.768
I1003 02:04:10.771754 31421 solver.cpp:567] Iteration 18000, Testing net (#0)
I1003 02:04:20.321146 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.945528
I1003 02:04:20.321163 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 02:04:20.321168 31421 solver.cpp:659]     Test net output #2: loss = 0.19594 (* 1 = 0.19594 loss)
I1003 02:04:20.321197 31421 solver.cpp:265] [MultiGPU] Tests completed in 9.54916s
I1003 02:04:20.518293 31421 solver.cpp:314] Iteration 18000 (0.564664 iter/s, 177.096s/100 iter), loss = 0.0582802
I1003 02:04:20.518317 31421 solver.cpp:336]     Train net output #0: loss = 0.0582801 (* 1 = 0.0582801 loss)
I1003 02:04:20.518322 31421 sgd_solver.cpp:136] Iteration 18000, lr = 0.01, m = 0.9
I1003 02:04:39.955363 31421 solver.cpp:314] Iteration 18100 (5.14496 iter/s, 19.4365s/100 iter), loss = 0.0561731
I1003 02:04:39.955387 31421 solver.cpp:336]     Train net output #0: loss = 0.056173 (* 1 = 0.056173 loss)
I1003 02:04:39.955392 31421 sgd_solver.cpp:136] Iteration 18100, lr = 0.01, m = 0.9
I1003 02:05:02.702194 31342 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 02:05:15.956104 31421 solver.cpp:314] Iteration 18200 (2.7778 iter/s, 35.9997s/100 iter), loss = 0.067461
I1003 02:05:15.956137 31421 solver.cpp:336]     Train net output #0: loss = 0.0674609 (* 1 = 0.0674609 loss)
I1003 02:05:15.956141 31421 sgd_solver.cpp:136] Iteration 18200, lr = 0.01, m = 0.9
I1003 02:06:08.452872 31421 solver.cpp:314] Iteration 18300 (1.90493 iter/s, 52.4953s/100 iter), loss = 0.0849458
I1003 02:06:08.452944 31421 solver.cpp:336]     Train net output #0: loss = 0.0849457 (* 1 = 0.0849457 loss)
I1003 02:06:08.452950 31421 sgd_solver.cpp:136] Iteration 18300, lr = 0.01, m = 0.9
I1003 02:07:16.856101 31421 solver.cpp:314] Iteration 18400 (1.46196 iter/s, 68.4012s/100 iter), loss = 0.0595743
I1003 02:07:16.856155 31421 solver.cpp:336]     Train net output #0: loss = 0.0595742 (* 1 = 0.0595742 loss)
I1003 02:07:16.856160 31421 sgd_solver.cpp:136] Iteration 18400, lr = 0.01, m = 0.9
I1003 02:07:56.629640 31421 solver.cpp:314] Iteration 18500 (2.51431 iter/s, 39.7724s/100 iter), loss = 0.0739908
I1003 02:07:56.629694 31421 solver.cpp:336]     Train net output #0: loss = 0.0739907 (* 1 = 0.0739907 loss)
I1003 02:07:56.629699 31421 sgd_solver.cpp:136] Iteration 18500, lr = 0.01, m = 0.9
I1003 02:08:00.630295 31432 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 02:08:34.097654 31421 solver.cpp:314] Iteration 18600 (2.66902 iter/s, 37.4669s/100 iter), loss = 0.0865751
I1003 02:08:34.097759 31421 solver.cpp:336]     Train net output #0: loss = 0.0865751 (* 1 = 0.0865751 loss)
I1003 02:08:34.097766 31421 sgd_solver.cpp:136] Iteration 18600, lr = 0.01, m = 0.9
I1003 02:09:19.044056 31421 solver.cpp:314] Iteration 18700 (2.22494 iter/s, 44.9451s/100 iter), loss = 0.0456375
I1003 02:09:19.044127 31421 solver.cpp:336]     Train net output #0: loss = 0.0456374 (* 1 = 0.0456374 loss)
I1003 02:09:19.044132 31421 sgd_solver.cpp:136] Iteration 18700, lr = 0.01, m = 0.9
I1003 02:09:52.055521 31343 blocking_queue.cpp:40] Waiting for datum
I1003 02:10:11.743543 31421 solver.cpp:314] Iteration 18800 (1.89761 iter/s, 52.698s/100 iter), loss = 0.184354
I1003 02:10:11.743569 31421 solver.cpp:336]     Train net output #0: loss = 0.184353 (* 1 = 0.184353 loss)
I1003 02:10:11.743577 31421 sgd_solver.cpp:136] Iteration 18800, lr = 0.01, m = 0.9
I1003 02:10:27.013471 31432 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 02:10:40.046355 31421 solver.cpp:314] Iteration 18900 (3.53332 iter/s, 28.302s/100 iter), loss = 0.063771
I1003 02:10:40.046376 31421 solver.cpp:336]     Train net output #0: loss = 0.0637709 (* 1 = 0.0637709 loss)
I1003 02:10:40.046380 31421 sgd_solver.cpp:136] Iteration 18900, lr = 0.01, m = 0.9
I1003 02:11:26.321182 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.78 sparsity_achieved=0.767876 iter=19000
I1003 02:13:59.276142 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 02:13:59.280144 31421 solver.cpp:352] Sparsity after update:
I1003 02:13:59.281708 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:13:59.281714 31421 net.cpp:2312] conv1a_param_0(0.304) 
I1003 02:13:59.303794 31421 net.cpp:2312] conv1b_param_0(0.704) 
I1003 02:13:59.303800 31421 net.cpp:2312] ctx_conv1_param_0(0.779) 
I1003 02:13:59.303802 31421 net.cpp:2312] ctx_conv2_param_0(0.78) 
I1003 02:13:59.303805 31421 net.cpp:2312] ctx_conv3_param_0(0.78) 
I1003 02:13:59.303807 31421 net.cpp:2312] ctx_conv4_param_0(0.78) 
I1003 02:13:59.303808 31421 net.cpp:2312] ctx_final_param_0(0.243) 
I1003 02:13:59.303810 31421 net.cpp:2312] out3a_param_0(0.78) 
I1003 02:13:59.303812 31421 net.cpp:2312] out5a_param_0(0.78) 
I1003 02:13:59.303814 31421 net.cpp:2312] res2a_branch2a_param_0(0.773) 
I1003 02:13:59.303817 31421 net.cpp:2312] res2a_branch2b_param_0(0.696) 
I1003 02:13:59.303819 31421 net.cpp:2312] res3a_branch2a_param_0(0.778) 
I1003 02:13:59.303822 31421 net.cpp:2312] res3a_branch2b_param_0(0.762) 
I1003 02:13:59.303823 31421 net.cpp:2312] res4a_branch2a_param_0(0.779) 
I1003 02:13:59.303824 31421 net.cpp:2312] res4a_branch2b_param_0(0.776) 
I1003 02:13:59.303827 31421 net.cpp:2312] res5a_branch2a_param_0(0.78) 
I1003 02:13:59.303828 31421 net.cpp:2312] res5a_branch2b_param_0(0.78) 
I1003 02:13:59.303830 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.09216e+06/2.69117e+06) 0.777
I1003 02:13:59.478374 31421 solver.cpp:314] Iteration 19000 (0.501439 iter/s, 199.426s/100 iter), loss = 0.132035
I1003 02:13:59.478402 31421 solver.cpp:336]     Train net output #0: loss = 0.132035 (* 1 = 0.132035 loss)
I1003 02:13:59.478410 31421 sgd_solver.cpp:136] Iteration 19000, lr = 0.01, m = 0.9
I1003 02:14:35.045315 31421 solver.cpp:314] Iteration 19100 (2.81168 iter/s, 35.5659s/100 iter), loss = 0.058612
I1003 02:14:35.052433 31421 solver.cpp:336]     Train net output #0: loss = 0.0586119 (* 1 = 0.0586119 loss)
I1003 02:14:35.052469 31421 sgd_solver.cpp:136] Iteration 19100, lr = 0.01, m = 0.9
I1003 02:14:46.987747 31430 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 02:14:52.406913 31421 solver.cpp:314] Iteration 19200 (5.76001 iter/s, 17.3611s/100 iter), loss = 0.0904144
I1003 02:14:52.406935 31421 solver.cpp:336]     Train net output #0: loss = 0.0904142 (* 1 = 0.0904142 loss)
I1003 02:14:52.406940 31421 sgd_solver.cpp:136] Iteration 19200, lr = 0.01, m = 0.9
I1003 02:15:12.116942 31421 solver.cpp:314] Iteration 19300 (5.07371 iter/s, 19.7094s/100 iter), loss = 0.0804446
I1003 02:15:12.117003 31421 solver.cpp:336]     Train net output #0: loss = 0.0804445 (* 1 = 0.0804445 loss)
I1003 02:15:12.117010 31421 sgd_solver.cpp:136] Iteration 19300, lr = 0.01, m = 0.9
I1003 02:15:29.878830 31421 solver.cpp:314] Iteration 19400 (5.6302 iter/s, 17.7614s/100 iter), loss = 0.0942607
I1003 02:15:29.878852 31421 solver.cpp:336]     Train net output #0: loss = 0.0942606 (* 1 = 0.0942606 loss)
I1003 02:15:29.878856 31421 sgd_solver.cpp:136] Iteration 19400, lr = 0.01, m = 0.9
I1003 02:15:47.490901 31432 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 02:15:47.654116 31421 solver.cpp:314] Iteration 19500 (5.62595 iter/s, 17.7748s/100 iter), loss = 0.0725195
I1003 02:15:47.654140 31421 solver.cpp:336]     Train net output #0: loss = 0.0725194 (* 1 = 0.0725194 loss)
I1003 02:15:47.654145 31421 sgd_solver.cpp:136] Iteration 19500, lr = 0.01, m = 0.9
I1003 02:16:05.515244 31421 solver.cpp:314] Iteration 19600 (5.59891 iter/s, 17.8606s/100 iter), loss = 0.0802363
I1003 02:16:05.515265 31421 solver.cpp:336]     Train net output #0: loss = 0.0802362 (* 1 = 0.0802362 loss)
I1003 02:16:05.515270 31421 sgd_solver.cpp:136] Iteration 19600, lr = 0.01, m = 0.9
I1003 02:16:17.170219 31342 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 02:16:23.402853 31421 solver.cpp:314] Iteration 19700 (5.59062 iter/s, 17.8871s/100 iter), loss = 0.383669
I1003 02:16:23.402915 31421 solver.cpp:336]     Train net output #0: loss = 0.383669 (* 1 = 0.383669 loss)
I1003 02:16:23.402921 31421 sgd_solver.cpp:136] Iteration 19700, lr = 0.01, m = 0.9
I1003 02:16:41.218569 31421 solver.cpp:314] Iteration 19800 (5.61319 iter/s, 17.8152s/100 iter), loss = 0.0717331
I1003 02:16:41.218600 31421 solver.cpp:336]     Train net output #0: loss = 0.071733 (* 1 = 0.071733 loss)
I1003 02:16:41.218605 31421 sgd_solver.cpp:136] Iteration 19800, lr = 0.01, m = 0.9
I1003 02:16:59.353019 31421 solver.cpp:314] Iteration 19900 (5.51453 iter/s, 18.1339s/100 iter), loss = 0.0658135
I1003 02:16:59.353102 31421 solver.cpp:336]     Train net output #0: loss = 0.0658133 (* 1 = 0.0658133 loss)
I1003 02:16:59.353109 31421 sgd_solver.cpp:136] Iteration 19900, lr = 0.01, m = 0.9
I1003 02:17:16.608007 31430 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 02:17:17.291633 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.79 sparsity_achieved=0.777419 iter=20000
I1003 02:17:40.630986 31538 data_reader.cpp:305] Starting prefetch of epoch 4
I1003 02:19:59.427124 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 02:19:59.431210 31421 solver.cpp:352] Sparsity after update:
I1003 02:19:59.432801 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:19:59.432806 31421 net.cpp:2312] conv1a_param_0(0.305) 
I1003 02:19:59.432816 31421 net.cpp:2312] conv1b_param_0(0.707) 
I1003 02:19:59.432817 31421 net.cpp:2312] ctx_conv1_param_0(0.79) 
I1003 02:19:59.432819 31421 net.cpp:2312] ctx_conv2_param_0(0.79) 
I1003 02:19:59.432821 31421 net.cpp:2312] ctx_conv3_param_0(0.79) 
I1003 02:19:59.432823 31421 net.cpp:2312] ctx_conv4_param_0(0.79) 
I1003 02:19:59.432826 31421 net.cpp:2312] ctx_final_param_0(0.368) 
I1003 02:19:59.432827 31421 net.cpp:2312] out3a_param_0(0.79) 
I1003 02:19:59.432829 31421 net.cpp:2312] out5a_param_0(0.79) 
I1003 02:19:59.432831 31421 net.cpp:2312] res2a_branch2a_param_0(0.781) 
I1003 02:19:59.432832 31421 net.cpp:2312] res2a_branch2b_param_0(0.699) 
I1003 02:19:59.432834 31421 net.cpp:2312] res3a_branch2a_param_0(0.787) 
I1003 02:19:59.432837 31421 net.cpp:2312] res3a_branch2b_param_0(0.769) 
I1003 02:19:59.432838 31421 net.cpp:2312] res4a_branch2a_param_0(0.79) 
I1003 02:19:59.432840 31421 net.cpp:2312] res4a_branch2b_param_0(0.786) 
I1003 02:19:59.432842 31421 net.cpp:2312] res5a_branch2a_param_0(0.79) 
I1003 02:19:59.432844 31421 net.cpp:2312] res5a_branch2b_param_0(0.79) 
I1003 02:19:59.432847 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.11957e+06/2.69117e+06) 0.788
I1003 02:19:59.434514 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I1003 02:19:59.517961 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I1003 02:19:59.522552 31421 solver.cpp:567] Iteration 20000, Testing net (#0)
I1003 02:20:09.159113 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.950281
I1003 02:20:09.159150 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 02:20:09.159155 31421 solver.cpp:659]     Test net output #2: loss = 0.141732 (* 1 = 0.141732 loss)
I1003 02:20:09.159173 31421 solver.cpp:265] [MultiGPU] Tests completed in 9.63634s
I1003 02:20:09.351557 31421 solver.cpp:314] Iteration 20000 (0.526335 iter/s, 189.993s/100 iter), loss = 0.0506351
I1003 02:20:09.351582 31421 solver.cpp:336]     Train net output #0: loss = 0.050635 (* 1 = 0.050635 loss)
I1003 02:20:09.351585 31421 sgd_solver.cpp:136] Iteration 20000, lr = 0.01, m = 0.9
I1003 02:20:26.613706 31421 solver.cpp:314] Iteration 20100 (5.79319 iter/s, 17.2616s/100 iter), loss = 0.0494776
I1003 02:20:26.613729 31421 solver.cpp:336]     Train net output #0: loss = 0.0494775 (* 1 = 0.0494775 loss)
I1003 02:20:26.613734 31421 sgd_solver.cpp:136] Iteration 20100, lr = 0.01, m = 0.9
I1003 02:20:44.594815 31421 solver.cpp:314] Iteration 20200 (5.56156 iter/s, 17.9806s/100 iter), loss = 0.0864101
I1003 02:20:44.594857 31421 solver.cpp:336]     Train net output #0: loss = 0.08641 (* 1 = 0.08641 loss)
I1003 02:20:44.594862 31421 sgd_solver.cpp:136] Iteration 20200, lr = 0.01, m = 0.9
I1003 02:21:02.535770 31421 solver.cpp:314] Iteration 20300 (5.574 iter/s, 17.9404s/100 iter), loss = 0.0675305
I1003 02:21:02.535794 31421 solver.cpp:336]     Train net output #0: loss = 0.0675304 (* 1 = 0.0675304 loss)
I1003 02:21:02.535799 31421 sgd_solver.cpp:136] Iteration 20300, lr = 0.01, m = 0.9
I1003 02:21:07.561380 31432 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 02:21:22.600359 31421 solver.cpp:314] Iteration 20400 (4.98405 iter/s, 20.064s/100 iter), loss = 0.0730721
I1003 02:21:22.600416 31421 solver.cpp:336]     Train net output #0: loss = 0.073072 (* 1 = 0.073072 loss)
I1003 02:21:22.600420 31421 sgd_solver.cpp:136] Iteration 20400, lr = 0.01, m = 0.9
I1003 02:21:55.923327 31421 solver.cpp:314] Iteration 20500 (3.00102 iter/s, 33.322s/100 iter), loss = 0.0675939
I1003 02:21:55.923437 31421 solver.cpp:336]     Train net output #0: loss = 0.0675939 (* 1 = 0.0675939 loss)
I1003 02:21:55.923444 31421 sgd_solver.cpp:136] Iteration 20500, lr = 0.01, m = 0.9
I1003 02:22:14.986495 31421 solver.cpp:314] Iteration 20600 (5.24587 iter/s, 19.0626s/100 iter), loss = 0.0576731
I1003 02:22:14.986522 31421 solver.cpp:336]     Train net output #0: loss = 0.0576731 (* 1 = 0.0576731 loss)
I1003 02:22:14.986527 31421 sgd_solver.cpp:136] Iteration 20600, lr = 0.01, m = 0.9
I1003 02:22:28.759367 31344 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 02:22:36.863258 31421 solver.cpp:314] Iteration 20700 (4.57119 iter/s, 21.8761s/100 iter), loss = 0.0525148
I1003 02:22:36.863281 31421 solver.cpp:336]     Train net output #0: loss = 0.0525148 (* 1 = 0.0525148 loss)
I1003 02:22:36.863286 31421 sgd_solver.cpp:136] Iteration 20700, lr = 0.01, m = 0.9
I1003 02:23:03.146824 31421 solver.cpp:314] Iteration 20800 (3.80477 iter/s, 26.2828s/100 iter), loss = 0.0429896
I1003 02:23:03.146868 31421 solver.cpp:336]     Train net output #0: loss = 0.0429896 (* 1 = 0.0429896 loss)
I1003 02:23:03.146873 31421 sgd_solver.cpp:136] Iteration 20800, lr = 0.01, m = 0.9
I1003 02:23:35.883986 31421 solver.cpp:314] Iteration 20900 (3.05472 iter/s, 32.7362s/100 iter), loss = 0.049774
I1003 02:23:35.884065 31421 solver.cpp:336]     Train net output #0: loss = 0.049774 (* 1 = 0.049774 loss)
I1003 02:23:35.884071 31421 sgd_solver.cpp:136] Iteration 20900, lr = 0.01, m = 0.9
I1003 02:24:25.048070 31430 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 02:24:29.104408 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.8 sparsity_achieved=0.787603 iter=21000
I1003 02:27:13.627601 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 02:27:13.631654 31421 solver.cpp:352] Sparsity after update:
I1003 02:27:13.633261 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:27:13.633268 31421 net.cpp:2312] conv1a_param_0(0.305) 
I1003 02:27:13.633275 31421 net.cpp:2312] conv1b_param_0(0.712) 
I1003 02:27:13.633276 31421 net.cpp:2312] ctx_conv1_param_0(0.798) 
I1003 02:27:13.633278 31421 net.cpp:2312] ctx_conv2_param_0(0.799) 
I1003 02:27:13.633280 31421 net.cpp:2312] ctx_conv3_param_0(0.798) 
I1003 02:27:13.633282 31421 net.cpp:2312] ctx_conv4_param_0(0.799) 
I1003 02:27:13.633285 31421 net.cpp:2312] ctx_final_param_0(0.336) 
I1003 02:27:13.633286 31421 net.cpp:2312] out3a_param_0(0.799) 
I1003 02:27:13.633288 31421 net.cpp:2312] out5a_param_0(0.8) 
I1003 02:27:13.633291 31421 net.cpp:2312] res2a_branch2a_param_0(0.789) 
I1003 02:27:13.633292 31421 net.cpp:2312] res2a_branch2b_param_0(0.703) 
I1003 02:27:13.633294 31421 net.cpp:2312] res3a_branch2a_param_0(0.795) 
I1003 02:27:13.633296 31421 net.cpp:2312] res3a_branch2b_param_0(0.776) 
I1003 02:27:13.633298 31421 net.cpp:2312] res4a_branch2a_param_0(0.799) 
I1003 02:27:13.633301 31421 net.cpp:2312] res4a_branch2b_param_0(0.793) 
I1003 02:27:13.633302 31421 net.cpp:2312] res5a_branch2a_param_0(0.8) 
I1003 02:27:13.633303 31421 net.cpp:2312] res5a_branch2b_param_0(0.799) 
I1003 02:27:13.633306 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.14482e+06/2.69117e+06) 0.797
I1003 02:27:13.807482 31421 solver.cpp:314] Iteration 21000 (0.45889 iter/s, 217.917s/100 iter), loss = 0.078398
I1003 02:27:13.807507 31421 solver.cpp:336]     Train net output #0: loss = 0.0783979 (* 1 = 0.0783979 loss)
I1003 02:27:13.807510 31421 sgd_solver.cpp:136] Iteration 21000, lr = 0.01, m = 0.9
I1003 02:27:43.943400 31421 solver.cpp:314] Iteration 21100 (3.3184 iter/s, 30.135s/100 iter), loss = 0.0521812
I1003 02:27:43.943475 31421 solver.cpp:336]     Train net output #0: loss = 0.0521811 (* 1 = 0.0521811 loss)
I1003 02:27:43.943480 31421 sgd_solver.cpp:136] Iteration 21100, lr = 0.01, m = 0.9
I1003 02:28:03.387811 31421 solver.cpp:314] Iteration 21200 (5.14302 iter/s, 19.4438s/100 iter), loss = 0.0815072
I1003 02:28:03.387853 31421 solver.cpp:336]     Train net output #0: loss = 0.0815072 (* 1 = 0.0815072 loss)
I1003 02:28:03.387858 31421 sgd_solver.cpp:136] Iteration 21200, lr = 0.01, m = 0.9
I1003 02:28:23.738641 31421 solver.cpp:314] Iteration 21300 (4.91395 iter/s, 20.3502s/100 iter), loss = 0.0777676
I1003 02:28:23.738734 31421 solver.cpp:336]     Train net output #0: loss = 0.0777675 (* 1 = 0.0777675 loss)
I1003 02:28:23.738741 31421 sgd_solver.cpp:136] Iteration 21300, lr = 0.01, m = 0.9
I1003 02:28:26.771924 31430 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 02:28:41.411204 31421 solver.cpp:314] Iteration 21400 (5.65865 iter/s, 17.672s/100 iter), loss = 0.0658047
I1003 02:28:41.411231 31421 solver.cpp:336]     Train net output #0: loss = 0.0658046 (* 1 = 0.0658046 loss)
I1003 02:28:41.411234 31421 sgd_solver.cpp:136] Iteration 21400, lr = 0.01, m = 0.9
I1003 02:29:01.123067 31421 solver.cpp:314] Iteration 21500 (5.07324 iter/s, 19.7113s/100 iter), loss = 0.0570446
I1003 02:29:01.123136 31421 solver.cpp:336]     Train net output #0: loss = 0.0570446 (* 1 = 0.0570446 loss)
I1003 02:29:01.123143 31421 sgd_solver.cpp:136] Iteration 21500, lr = 0.01, m = 0.9
I1003 02:29:19.701390 31421 solver.cpp:314] Iteration 21600 (5.38278 iter/s, 18.5778s/100 iter), loss = 0.0907269
I1003 02:29:19.701416 31421 solver.cpp:336]     Train net output #0: loss = 0.0907268 (* 1 = 0.0907268 loss)
I1003 02:29:19.701419 31421 sgd_solver.cpp:136] Iteration 21600, lr = 0.01, m = 0.9
I1003 02:29:28.300514 31432 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 02:29:37.663867 31421 solver.cpp:314] Iteration 21700 (5.56732 iter/s, 17.962s/100 iter), loss = 0.0715578
I1003 02:29:37.663977 31421 solver.cpp:336]     Train net output #0: loss = 0.0715577 (* 1 = 0.0715577 loss)
I1003 02:29:37.663986 31421 sgd_solver.cpp:136] Iteration 21700, lr = 0.01, m = 0.9
I1003 02:29:55.587827 31421 solver.cpp:314] Iteration 21800 (5.57929 iter/s, 17.9234s/100 iter), loss = 0.0673758
I1003 02:29:55.587857 31421 solver.cpp:336]     Train net output #0: loss = 0.0673757 (* 1 = 0.0673757 loss)
I1003 02:29:55.587863 31421 sgd_solver.cpp:136] Iteration 21800, lr = 0.01, m = 0.9
I1003 02:29:57.990048 31424 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 02:30:13.667712 31421 solver.cpp:314] Iteration 21900 (5.53117 iter/s, 18.0794s/100 iter), loss = 0.0541344
I1003 02:30:13.667764 31421 solver.cpp:336]     Train net output #0: loss = 0.0541343 (* 1 = 0.0541343 loss)
I1003 02:30:13.667770 31421 sgd_solver.cpp:136] Iteration 21900, lr = 0.01, m = 0.9
I1003 02:30:31.464463 31421 solver.cpp:428] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.81 sparsity_achieved=0.796986 iter=22000
I1003 02:33:31.068188 31421 net.cpp:2253] All zero weights of convolution layers are frozen
I1003 02:33:31.072160 31421 solver.cpp:352] Sparsity after update:
I1003 02:33:31.073760 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:33:31.073767 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:33:31.074352 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:33:31.074359 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:33:31.074364 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:33:31.074368 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:33:31.074371 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:33:31.074376 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:33:31.074381 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:33:31.074385 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:33:31.074389 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:33:31.074393 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:33:31.074396 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:33:31.074400 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:33:31.074404 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:33:31.074409 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:33:31.074412 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:33:31.074417 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:33:31.074421 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:33:31.093952 31421 solver.cpp:567] Iteration 22000, Testing net (#0)
I1003 02:33:37.436933 31418 data_reader.cpp:305] Starting prefetch of epoch 3
I1003 02:33:40.945473 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.95056
I1003 02:33:40.945495 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 02:33:40.945503 31421 solver.cpp:659]     Test net output #2: loss = 0.149518 (* 1 = 0.149518 loss)
I1003 02:33:40.945523 31421 solver.cpp:265] [MultiGPU] Tests completed in 9.87079s
I1003 02:33:41.142065 31421 solver.cpp:314] Iteration 22000 (0.482001 iter/s, 207.468s/100 iter), loss = 0.0834014
I1003 02:33:41.142086 31421 solver.cpp:336]     Train net output #0: loss = 0.0834013 (* 1 = 0.0834013 loss)
I1003 02:33:41.142091 31421 sgd_solver.cpp:136] Iteration 22000, lr = 0.01, m = 0.9
I1003 02:33:59.096340 31421 solver.cpp:314] Iteration 22100 (5.56987 iter/s, 17.9537s/100 iter), loss = 0.0701849
I1003 02:33:59.096364 31421 solver.cpp:336]     Train net output #0: loss = 0.0701848 (* 1 = 0.0701848 loss)
I1003 02:33:59.096369 31421 sgd_solver.cpp:136] Iteration 22100, lr = 0.01, m = 0.9
I1003 02:34:00.017340 31429 blocking_queue.cpp:40] Waiting for datum
I1003 02:34:18.570523 31421 solver.cpp:314] Iteration 22200 (5.13515 iter/s, 19.4736s/100 iter), loss = 0.0664314
I1003 02:34:18.570605 31421 solver.cpp:336]     Train net output #0: loss = 0.0664313 (* 1 = 0.0664313 loss)
I1003 02:34:18.570612 31421 sgd_solver.cpp:136] Iteration 22200, lr = 0.01, m = 0.9
I1003 02:34:46.961275 31421 solver.cpp:314] Iteration 22300 (3.52238 iter/s, 28.3899s/100 iter), loss = 0.045221
I1003 02:34:46.961319 31421 solver.cpp:336]     Train net output #0: loss = 0.0452209 (* 1 = 0.0452209 loss)
I1003 02:34:46.961329 31421 sgd_solver.cpp:136] Iteration 22300, lr = 0.01, m = 0.9
I1003 02:34:51.506708 31430 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 02:35:43.866991 31421 solver.cpp:314] Iteration 22400 (1.75734 iter/s, 56.9041s/100 iter), loss = 0.0662634
I1003 02:35:43.867056 31421 solver.cpp:336]     Train net output #0: loss = 0.0662633 (* 1 = 0.0662633 loss)
I1003 02:35:43.867063 31421 sgd_solver.cpp:136] Iteration 22400, lr = 0.01, m = 0.9
I1003 02:36:37.336010 31421 solver.cpp:314] Iteration 22500 (1.8703 iter/s, 53.4675s/100 iter), loss = 0.119935
I1003 02:36:37.336081 31421 solver.cpp:336]     Train net output #0: loss = 0.119935 (* 1 = 0.119935 loss)
I1003 02:36:37.336086 31421 sgd_solver.cpp:136] Iteration 22500, lr = 0.01, m = 0.9
I1003 02:37:19.992405 31421 solver.cpp:314] Iteration 22600 (2.34438 iter/s, 42.6552s/100 iter), loss = 0.0502741
I1003 02:37:19.992501 31421 solver.cpp:336]     Train net output #0: loss = 0.050274 (* 1 = 0.050274 loss)
I1003 02:37:19.992507 31421 sgd_solver.cpp:136] Iteration 22600, lr = 0.01, m = 0.9
I1003 02:37:41.394237 31344 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 02:38:06.131506 31421 solver.cpp:314] Iteration 22700 (2.16742 iter/s, 46.1378s/100 iter), loss = 0.0850023
I1003 02:38:06.131583 31421 solver.cpp:336]     Train net output #0: loss = 0.0850022 (* 1 = 0.0850022 loss)
I1003 02:38:06.131588 31421 sgd_solver.cpp:136] Iteration 22700, lr = 0.01, m = 0.9
I1003 02:38:57.430140 31421 solver.cpp:314] Iteration 22800 (1.94943 iter/s, 51.2971s/100 iter), loss = 0.0411337
I1003 02:38:57.431056 31421 solver.cpp:336]     Train net output #0: loss = 0.0411336 (* 1 = 0.0411336 loss)
I1003 02:38:57.431062 31421 sgd_solver.cpp:136] Iteration 22800, lr = 0.01, m = 0.9
I1003 02:39:01.192632 31424 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 02:39:54.378796 31421 solver.cpp:314] Iteration 22900 (1.75602 iter/s, 56.947s/100 iter), loss = 0.0583193
I1003 02:39:54.378854 31421 solver.cpp:336]     Train net output #0: loss = 0.0583193 (* 1 = 0.0583193 loss)
I1003 02:39:54.378859 31421 sgd_solver.cpp:136] Iteration 22900, lr = 0.01, m = 0.9
I1003 02:40:29.722462 31421 solver.cpp:352] Sparsity after update:
I1003 02:40:29.728797 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:40:29.728806 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:40:29.728814 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:40:29.728816 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:40:29.728819 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:40:29.728821 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:40:29.728823 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:40:29.728826 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:40:29.728826 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:40:29.728828 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:40:29.728832 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:40:29.728833 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:40:29.728834 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:40:29.728837 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:40:29.728839 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:40:29.728842 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:40:29.728843 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:40:29.728845 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:40:29.728847 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:40:29.894397 31421 solver.cpp:314] Iteration 23000 (2.81575 iter/s, 35.5146s/100 iter), loss = 0.084796
I1003 02:40:29.894421 31421 solver.cpp:336]     Train net output #0: loss = 0.084796 (* 1 = 0.084796 loss)
I1003 02:40:29.894426 31421 sgd_solver.cpp:136] Iteration 23000, lr = 0.01, m = 0.9
I1003 02:41:19.099514 31421 solver.cpp:314] Iteration 23100 (2.03237 iter/s, 49.2037s/100 iter), loss = 0.0684568
I1003 02:41:19.099567 31421 solver.cpp:336]     Train net output #0: loss = 0.0684568 (* 1 = 0.0684568 loss)
I1003 02:41:19.099572 31421 sgd_solver.cpp:136] Iteration 23100, lr = 0.01, m = 0.9
I1003 02:41:36.278887 31342 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 02:41:39.273692 31431 blocking_queue.cpp:40] Waiting for datum
I1003 02:42:11.158697 31421 solver.cpp:314] Iteration 23200 (1.92095 iter/s, 52.0577s/100 iter), loss = 0.0538729
I1003 02:42:11.158747 31421 solver.cpp:336]     Train net output #0: loss = 0.0538729 (* 1 = 0.0538729 loss)
I1003 02:42:11.158753 31421 sgd_solver.cpp:136] Iteration 23200, lr = 0.01, m = 0.9
I1003 02:42:29.311656 31421 solver.cpp:314] Iteration 23300 (5.5089 iter/s, 18.1524s/100 iter), loss = 0.0397695
I1003 02:42:29.311681 31421 solver.cpp:336]     Train net output #0: loss = 0.0397695 (* 1 = 0.0397695 loss)
I1003 02:42:29.311684 31421 sgd_solver.cpp:136] Iteration 23300, lr = 0.01, m = 0.9
I1003 02:42:47.596806 31421 solver.cpp:314] Iteration 23400 (5.46908 iter/s, 18.2846s/100 iter), loss = 0.0574898
I1003 02:42:47.596861 31421 solver.cpp:336]     Train net output #0: loss = 0.0574898 (* 1 = 0.0574898 loss)
I1003 02:42:47.596866 31421 sgd_solver.cpp:136] Iteration 23400, lr = 0.01, m = 0.9
I1003 02:43:05.546195 31421 solver.cpp:314] Iteration 23500 (5.57138 iter/s, 17.9489s/100 iter), loss = 0.240168
I1003 02:43:05.546218 31421 solver.cpp:336]     Train net output #0: loss = 0.240168 (* 1 = 0.240168 loss)
I1003 02:43:05.546222 31421 sgd_solver.cpp:136] Iteration 23500, lr = 0.01, m = 0.9
I1003 02:43:23.428891 31421 solver.cpp:314] Iteration 23600 (5.59216 iter/s, 17.8822s/100 iter), loss = 0.045782
I1003 02:43:23.428944 31421 solver.cpp:336]     Train net output #0: loss = 0.045782 (* 1 = 0.045782 loss)
I1003 02:43:23.428951 31421 sgd_solver.cpp:136] Iteration 23600, lr = 0.01, m = 0.9
I1003 02:43:29.029186 31430 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 02:43:41.510767 31421 solver.cpp:314] Iteration 23700 (5.53056 iter/s, 18.0814s/100 iter), loss = 0.0532457
I1003 02:43:41.510793 31421 solver.cpp:336]     Train net output #0: loss = 0.0532457 (* 1 = 0.0532457 loss)
I1003 02:43:41.510797 31421 sgd_solver.cpp:136] Iteration 23700, lr = 0.01, m = 0.9
I1003 02:43:58.841554 31424 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 02:43:59.514897 31421 solver.cpp:314] Iteration 23800 (5.55444 iter/s, 18.0036s/100 iter), loss = 0.103468
I1003 02:43:59.514919 31421 solver.cpp:336]     Train net output #0: loss = 0.103468 (* 1 = 0.103468 loss)
I1003 02:43:59.514924 31421 sgd_solver.cpp:136] Iteration 23800, lr = 0.01, m = 0.9
I1003 02:44:17.391185 31421 solver.cpp:314] Iteration 23900 (5.59416 iter/s, 17.8758s/100 iter), loss = 0.057386
I1003 02:44:17.391206 31421 solver.cpp:336]     Train net output #0: loss = 0.0573859 (* 1 = 0.0573859 loss)
I1003 02:44:17.391211 31421 sgd_solver.cpp:136] Iteration 23900, lr = 0.01, m = 0.9
I1003 02:44:35.194162 31421 solver.cpp:352] Sparsity after update:
I1003 02:44:35.195682 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:44:35.195688 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:44:35.195694 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:44:35.195698 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:44:35.195698 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:44:35.195700 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:44:35.195703 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:44:35.195704 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:44:35.195706 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:44:35.195708 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:44:35.195710 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:44:35.195713 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:44:35.195714 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:44:35.195716 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:44:35.195719 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:44:35.195720 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:44:35.195722 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:44:35.195724 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:44:35.195726 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:44:35.216145 31421 solver.cpp:567] Iteration 24000, Testing net (#0)
I1003 02:44:45.760758 31405 data_reader.cpp:305] Starting prefetch of epoch 2
I1003 02:44:45.760758 31536 data_reader.cpp:305] Starting prefetch of epoch 3
I1003 02:44:45.760931 31540 data_reader.cpp:305] Starting prefetch of epoch 1
I1003 02:45:02.228819 31421 blocking_queue.cpp:40] Data layer prefetch queue empty
I1003 02:45:09.345340 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.952029
I1003 02:45:09.345561 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 02:45:09.345607 31421 solver.cpp:659]     Test net output #2: loss = 0.145111 (* 1 = 0.145111 loss)
I1003 02:45:09.345711 31421 solver.cpp:265] [MultiGPU] Tests completed in 34.149s
I1003 02:45:09.559617 31421 solver.cpp:314] Iteration 24000 (1.91692 iter/s, 52.1669s/100 iter), loss = 0.0705159
I1003 02:45:09.559643 31421 solver.cpp:336]     Train net output #0: loss = 0.0705159 (* 1 = 0.0705159 loss)
I1003 02:45:09.559648 31421 sgd_solver.cpp:136] Iteration 24000, lr = 0.01, m = 0.9
I1003 02:45:45.314862 31421 solver.cpp:314] Iteration 24100 (2.79687 iter/s, 35.7542s/100 iter), loss = 0.0879674
I1003 02:45:45.314941 31421 solver.cpp:336]     Train net output #0: loss = 0.0879674 (* 1 = 0.0879674 loss)
I1003 02:45:45.314946 31421 sgd_solver.cpp:136] Iteration 24100, lr = 0.01, m = 0.9
I1003 02:46:03.578476 31421 solver.cpp:314] Iteration 24200 (5.47553 iter/s, 18.2631s/100 iter), loss = 0.14862
I1003 02:46:03.578498 31421 solver.cpp:336]     Train net output #0: loss = 0.14862 (* 1 = 0.14862 loss)
I1003 02:46:03.578503 31421 sgd_solver.cpp:136] Iteration 24200, lr = 0.01, m = 0.9
I1003 02:46:20.167812 31432 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 02:46:21.611415 31421 solver.cpp:314] Iteration 24300 (5.54557 iter/s, 18.0324s/100 iter), loss = 0.0497674
I1003 02:46:21.611438 31421 solver.cpp:336]     Train net output #0: loss = 0.0497674 (* 1 = 0.0497674 loss)
I1003 02:46:21.611443 31421 sgd_solver.cpp:136] Iteration 24300, lr = 0.01, m = 0.9
I1003 02:46:46.625340 31421 solver.cpp:314] Iteration 24400 (3.99789 iter/s, 25.0132s/100 iter), loss = 0.0764501
I1003 02:46:46.625372 31421 solver.cpp:336]     Train net output #0: loss = 0.07645 (* 1 = 0.07645 loss)
I1003 02:46:46.625378 31421 sgd_solver.cpp:136] Iteration 24400, lr = 0.01, m = 0.9
I1003 02:47:19.251534 31421 solver.cpp:314] Iteration 24500 (3.06511 iter/s, 32.6253s/100 iter), loss = 0.0510516
I1003 02:47:19.251606 31421 solver.cpp:336]     Train net output #0: loss = 0.0510515 (* 1 = 0.0510515 loss)
I1003 02:47:19.251612 31421 sgd_solver.cpp:136] Iteration 24500, lr = 0.01, m = 0.9
I1003 02:48:04.980372 31421 solver.cpp:314] Iteration 24600 (2.18687 iter/s, 45.7275s/100 iter), loss = 0.0903011
I1003 02:48:04.980455 31421 solver.cpp:336]     Train net output #0: loss = 0.0903011 (* 1 = 0.0903011 loss)
I1003 02:48:04.980460 31421 sgd_solver.cpp:136] Iteration 24600, lr = 0.01, m = 0.9
I1003 02:48:16.331208 31430 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 02:48:49.556059 31421 solver.cpp:314] Iteration 24700 (2.24344 iter/s, 44.5744s/100 iter), loss = 0.0585044
I1003 02:48:49.556124 31421 solver.cpp:336]     Train net output #0: loss = 0.0585043 (* 1 = 0.0585043 loss)
I1003 02:48:49.556133 31421 sgd_solver.cpp:136] Iteration 24700, lr = 0.01, m = 0.9
I1003 02:49:13.339418 31421 solver.cpp:314] Iteration 24800 (4.20474 iter/s, 23.7827s/100 iter), loss = 0.0696868
I1003 02:49:13.339445 31421 solver.cpp:336]     Train net output #0: loss = 0.0696868 (* 1 = 0.0696868 loss)
I1003 02:49:13.339450 31421 sgd_solver.cpp:136] Iteration 24800, lr = 0.01, m = 0.9
I1003 02:49:55.545189 31421 solver.cpp:314] Iteration 24900 (2.36941 iter/s, 42.2046s/100 iter), loss = 0.0603221
I1003 02:49:55.545301 31421 solver.cpp:336]     Train net output #0: loss = 0.0603221 (* 1 = 0.0603221 loss)
I1003 02:49:55.545310 31421 sgd_solver.cpp:136] Iteration 24900, lr = 0.01, m = 0.9
I1003 02:50:12.839789 31432 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 02:50:29.896631 31421 solver.cpp:352] Sparsity after update:
I1003 02:50:29.906450 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:50:29.906507 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:50:29.906882 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:50:29.906918 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:50:29.906939 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:50:29.906957 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:50:29.906982 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:50:29.907007 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:50:29.907027 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:50:29.907050 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:50:29.907071 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:50:29.907091 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:50:29.907140 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:50:29.907167 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:50:29.907188 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:50:29.907207 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:50:29.907227 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:50:29.907246 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:50:29.907266 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:50:30.158525 31421 solver.cpp:314] Iteration 25000 (2.88914 iter/s, 34.6123s/100 iter), loss = 0.0683709
I1003 02:50:30.158552 31421 solver.cpp:336]     Train net output #0: loss = 0.0683709 (* 1 = 0.0683709 loss)
I1003 02:50:30.158557 31421 sgd_solver.cpp:136] Iteration 25000, lr = 0.01, m = 0.9
I1003 02:50:54.648741 31421 solver.cpp:314] Iteration 25100 (4.08338 iter/s, 24.4895s/100 iter), loss = 0.0561486
I1003 02:50:54.648766 31421 solver.cpp:336]     Train net output #0: loss = 0.0561486 (* 1 = 0.0561486 loss)
I1003 02:50:54.648771 31421 sgd_solver.cpp:136] Iteration 25100, lr = 0.01, m = 0.9
I1003 02:51:12.964843 31421 solver.cpp:314] Iteration 25200 (5.45983 iter/s, 18.3156s/100 iter), loss = 0.0844241
I1003 02:51:12.964984 31421 solver.cpp:336]     Train net output #0: loss = 0.0844241 (* 1 = 0.0844241 loss)
I1003 02:51:12.964993 31421 sgd_solver.cpp:136] Iteration 25200, lr = 0.01, m = 0.9
I1003 02:51:27.982782 31344 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 02:51:30.881420 31421 solver.cpp:314] Iteration 25300 (5.58158 iter/s, 17.9161s/100 iter), loss = 0.0619614
I1003 02:51:30.881443 31421 solver.cpp:336]     Train net output #0: loss = 0.0619614 (* 1 = 0.0619614 loss)
I1003 02:51:30.881448 31421 sgd_solver.cpp:136] Iteration 25300, lr = 0.01, m = 0.9
I1003 02:51:48.923324 31421 solver.cpp:314] Iteration 25400 (5.54281 iter/s, 18.0414s/100 iter), loss = 0.0706613
I1003 02:51:48.923372 31421 solver.cpp:336]     Train net output #0: loss = 0.0706613 (* 1 = 0.0706613 loss)
I1003 02:51:48.923377 31421 sgd_solver.cpp:136] Iteration 25400, lr = 0.01, m = 0.9
I1003 02:51:57.870363 31425 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 02:52:07.049199 31421 solver.cpp:314] Iteration 25500 (5.51713 iter/s, 18.1254s/100 iter), loss = 0.0519066
I1003 02:52:07.049221 31421 solver.cpp:336]     Train net output #0: loss = 0.0519065 (* 1 = 0.0519065 loss)
I1003 02:52:07.049224 31421 sgd_solver.cpp:136] Iteration 25500, lr = 0.01, m = 0.9
I1003 02:52:25.096346 31421 solver.cpp:314] Iteration 25600 (5.5412 iter/s, 18.0466s/100 iter), loss = 0.0711178
I1003 02:52:25.096395 31421 solver.cpp:336]     Train net output #0: loss = 0.0711177 (* 1 = 0.0711177 loss)
I1003 02:52:25.096401 31421 sgd_solver.cpp:136] Iteration 25600, lr = 0.01, m = 0.9
I1003 02:52:43.139111 31421 solver.cpp:314] Iteration 25700 (5.54255 iter/s, 18.0422s/100 iter), loss = 0.0451273
I1003 02:52:43.139132 31421 solver.cpp:336]     Train net output #0: loss = 0.0451273 (* 1 = 0.0451273 loss)
I1003 02:52:43.139137 31421 sgd_solver.cpp:136] Iteration 25700, lr = 0.01, m = 0.9
I1003 02:52:57.518303 31344 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 02:53:01.125285 31421 solver.cpp:314] Iteration 25800 (5.55998 iter/s, 17.9857s/100 iter), loss = 0.0682844
I1003 02:53:01.125308 31421 solver.cpp:336]     Train net output #0: loss = 0.0682844 (* 1 = 0.0682844 loss)
I1003 02:53:01.125315 31421 sgd_solver.cpp:136] Iteration 25800, lr = 0.01, m = 0.9
I1003 02:53:19.238997 31421 solver.cpp:314] Iteration 25900 (5.52084 iter/s, 18.1132s/100 iter), loss = 0.0474848
I1003 02:53:19.239023 31421 solver.cpp:336]     Train net output #0: loss = 0.0474848 (* 1 = 0.0474848 loss)
I1003 02:53:19.239027 31421 sgd_solver.cpp:136] Iteration 25900, lr = 0.01, m = 0.9
I1003 02:53:37.175437 31421 solver.cpp:352] Sparsity after update:
I1003 02:53:37.180882 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:53:37.180889 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:53:37.180896 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:53:37.180898 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:53:37.180901 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:53:37.180902 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:53:37.180904 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:53:37.180907 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:53:37.180908 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:53:37.180910 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:53:37.180912 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:53:37.180914 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:53:37.180917 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:53:37.180918 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:53:37.180920 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:53:37.180922 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:53:37.180924 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:53:37.180927 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:53:37.180928 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:53:37.189883 31421 solver.cpp:567] Iteration 26000, Testing net (#0)
I1003 02:54:01.244933 31536 data_reader.cpp:305] Starting prefetch of epoch 4
I1003 02:54:12.953331 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954964
I1003 02:54:12.953605 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 02:54:12.953651 31421 solver.cpp:659]     Test net output #2: loss = 0.149857 (* 1 = 0.149857 loss)
I1003 02:54:12.953737 31421 solver.cpp:265] [MultiGPU] Tests completed in 35.7718s
I1003 02:54:13.180806 31421 solver.cpp:314] Iteration 26000 (1.8539 iter/s, 53.9403s/100 iter), loss = 0.0816631
I1003 02:54:13.180865 31421 solver.cpp:336]     Train net output #0: loss = 0.0816631 (* 1 = 0.0816631 loss)
I1003 02:54:13.180884 31421 sgd_solver.cpp:136] Iteration 26000, lr = 0.01, m = 0.9
I1003 02:54:30.913120 31421 solver.cpp:314] Iteration 26100 (5.63958 iter/s, 17.7318s/100 iter), loss = 0.0407568
I1003 02:54:30.913146 31421 solver.cpp:336]     Train net output #0: loss = 0.0407568 (* 1 = 0.0407568 loss)
I1003 02:54:30.913153 31421 sgd_solver.cpp:136] Iteration 26100, lr = 0.01, m = 0.9
I1003 02:54:48.635963 31421 solver.cpp:314] Iteration 26200 (5.6426 iter/s, 17.7223s/100 iter), loss = 0.0838406
I1003 02:54:48.637154 31421 solver.cpp:336]     Train net output #0: loss = 0.0838405 (* 1 = 0.0838405 loss)
I1003 02:54:48.637164 31421 sgd_solver.cpp:136] Iteration 26200, lr = 0.01, m = 0.9
I1003 02:55:02.289682 31342 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 02:55:06.532058 31421 solver.cpp:314] Iteration 26300 (5.58797 iter/s, 17.8956s/100 iter), loss = 0.0639794
I1003 02:55:06.532083 31421 solver.cpp:336]     Train net output #0: loss = 0.0639794 (* 1 = 0.0639794 loss)
I1003 02:55:06.532088 31421 sgd_solver.cpp:136] Iteration 26300, lr = 0.01, m = 0.9
I1003 02:55:33.696218 31421 solver.cpp:314] Iteration 26400 (3.68143 iter/s, 27.1634s/100 iter), loss = 0.081527
I1003 02:55:33.696279 31421 solver.cpp:336]     Train net output #0: loss = 0.081527 (* 1 = 0.081527 loss)
I1003 02:55:33.696282 31421 sgd_solver.cpp:136] Iteration 26400, lr = 0.01, m = 0.9
I1003 02:55:58.398353 31421 solver.cpp:314] Iteration 26500 (4.04835 iter/s, 24.7014s/100 iter), loss = 0.0511967
I1003 02:55:58.398381 31421 solver.cpp:336]     Train net output #0: loss = 0.0511967 (* 1 = 0.0511967 loss)
I1003 02:55:58.398386 31421 sgd_solver.cpp:136] Iteration 26500, lr = 0.01, m = 0.9
I1003 02:56:19.633693 31421 solver.cpp:314] Iteration 26600 (4.70927 iter/s, 21.2347s/100 iter), loss = 0.0719319
I1003 02:56:19.633759 31421 solver.cpp:336]     Train net output #0: loss = 0.0719319 (* 1 = 0.0719319 loss)
I1003 02:56:19.633766 31421 sgd_solver.cpp:136] Iteration 26600, lr = 0.01, m = 0.9
I1003 02:56:20.260298 31431 blocking_queue.cpp:40] Waiting for datum
I1003 02:56:20.260298 31429 blocking_queue.cpp:40] Waiting for datum
I1003 02:56:20.920092 31432 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 02:57:00.656116 31421 solver.cpp:314] Iteration 26700 (2.43776 iter/s, 41.0213s/100 iter), loss = 0.0577529
I1003 02:57:00.656203 31421 solver.cpp:336]     Train net output #0: loss = 0.0577528 (* 1 = 0.0577528 loss)
I1003 02:57:00.656208 31421 sgd_solver.cpp:136] Iteration 26700, lr = 0.01, m = 0.9
I1003 02:57:44.011577 31421 solver.cpp:314] Iteration 26800 (2.30658 iter/s, 43.3542s/100 iter), loss = 0.0890718
I1003 02:57:44.011642 31421 solver.cpp:336]     Train net output #0: loss = 0.0890718 (* 1 = 0.0890718 loss)
I1003 02:57:44.011651 31421 sgd_solver.cpp:136] Iteration 26800, lr = 0.01, m = 0.9
I1003 02:58:02.662082 31421 solver.cpp:314] Iteration 26900 (5.36194 iter/s, 18.65s/100 iter), loss = 0.0527325
I1003 02:58:02.662117 31421 solver.cpp:336]     Train net output #0: loss = 0.0527325 (* 1 = 0.0527325 loss)
I1003 02:58:02.662123 31421 sgd_solver.cpp:136] Iteration 26900, lr = 0.01, m = 0.9
I1003 02:58:11.162986 31432 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 02:58:31.602659 31421 solver.cpp:352] Sparsity after update:
I1003 02:58:31.605315 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 02:58:31.605322 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 02:58:31.605489 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 02:58:31.605495 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 02:58:31.605504 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 02:58:31.605509 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 02:58:31.605512 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 02:58:31.605516 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 02:58:31.605521 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 02:58:31.605525 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 02:58:31.605530 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 02:58:31.605535 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 02:58:31.605540 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 02:58:31.605543 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 02:58:31.605547 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 02:58:31.605552 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 02:58:31.605556 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 02:58:31.605561 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 02:58:31.605566 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 02:58:31.773895 31421 solver.cpp:314] Iteration 27000 (3.43513 iter/s, 29.111s/100 iter), loss = 0.114605
I1003 02:58:31.773919 31421 solver.cpp:336]     Train net output #0: loss = 0.114605 (* 1 = 0.114605 loss)
I1003 02:58:31.773924 31421 sgd_solver.cpp:136] Iteration 27000, lr = 0.01, m = 0.9
I1003 02:58:49.657371 31421 solver.cpp:314] Iteration 27100 (5.59191 iter/s, 17.883s/100 iter), loss = 0.0628142
I1003 02:58:49.657397 31421 solver.cpp:336]     Train net output #0: loss = 0.0628142 (* 1 = 0.0628142 loss)
I1003 02:58:49.657402 31421 sgd_solver.cpp:136] Iteration 27100, lr = 0.01, m = 0.9
I1003 02:59:07.588987 31421 solver.cpp:314] Iteration 27200 (5.5769 iter/s, 17.9311s/100 iter), loss = 0.0473792
I1003 02:59:07.589035 31421 solver.cpp:336]     Train net output #0: loss = 0.0473791 (* 1 = 0.0473791 loss)
I1003 02:59:07.589040 31421 sgd_solver.cpp:136] Iteration 27200, lr = 0.01, m = 0.9
I1003 02:59:19.651983 31430 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 02:59:25.586782 31421 solver.cpp:314] Iteration 27300 (5.55639 iter/s, 17.9973s/100 iter), loss = 0.0629694
I1003 02:59:25.586805 31421 solver.cpp:336]     Train net output #0: loss = 0.0629694 (* 1 = 0.0629694 loss)
I1003 02:59:25.586809 31421 sgd_solver.cpp:136] Iteration 27300, lr = 0.01, m = 0.9
I1003 02:59:43.510156 31421 solver.cpp:314] Iteration 27400 (5.57947 iter/s, 17.9229s/100 iter), loss = 0.102453
I1003 02:59:43.510203 31421 solver.cpp:336]     Train net output #0: loss = 0.102453 (* 1 = 0.102453 loss)
I1003 02:59:43.510210 31421 sgd_solver.cpp:136] Iteration 27400, lr = 0.01, m = 0.9
I1003 02:59:49.404582 31342 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 03:00:01.474341 31421 solver.cpp:314] Iteration 27500 (5.56679 iter/s, 17.9637s/100 iter), loss = 0.0587219
I1003 03:00:01.474364 31421 solver.cpp:336]     Train net output #0: loss = 0.0587219 (* 1 = 0.0587219 loss)
I1003 03:00:01.474370 31421 sgd_solver.cpp:136] Iteration 27500, lr = 0.01, m = 0.9
I1003 03:00:19.500046 31421 solver.cpp:314] Iteration 27600 (5.54779 iter/s, 18.0252s/100 iter), loss = 0.0792414
I1003 03:00:19.500094 31421 solver.cpp:336]     Train net output #0: loss = 0.0792415 (* 1 = 0.0792415 loss)
I1003 03:00:19.500099 31421 sgd_solver.cpp:136] Iteration 27600, lr = 0.01, m = 0.9
I1003 03:00:37.591651 31421 solver.cpp:314] Iteration 27700 (5.52758 iter/s, 18.0911s/100 iter), loss = 0.0865118
I1003 03:00:37.591675 31421 solver.cpp:336]     Train net output #0: loss = 0.0865119 (* 1 = 0.0865119 loss)
I1003 03:00:37.591681 31421 sgd_solver.cpp:136] Iteration 27700, lr = 0.01, m = 0.9
I1003 03:00:48.945480 31425 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 03:00:55.485262 31421 solver.cpp:314] Iteration 27800 (5.58875 iter/s, 17.8931s/100 iter), loss = 0.0436942
I1003 03:00:55.485534 31421 solver.cpp:336]     Train net output #0: loss = 0.0436942 (* 1 = 0.0436942 loss)
I1003 03:00:55.485543 31421 sgd_solver.cpp:136] Iteration 27800, lr = 0.01, m = 0.9
I1003 03:01:13.418558 31421 solver.cpp:314] Iteration 27900 (5.57638 iter/s, 17.9328s/100 iter), loss = 0.0685287
I1003 03:01:13.418584 31421 solver.cpp:336]     Train net output #0: loss = 0.0685287 (* 1 = 0.0685287 loss)
I1003 03:01:13.418591 31421 sgd_solver.cpp:136] Iteration 27900, lr = 0.01, m = 0.9
I1003 03:01:31.253854 31421 solver.cpp:352] Sparsity after update:
I1003 03:01:31.261466 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:01:31.261483 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:01:31.261492 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:01:31.261495 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:01:31.261499 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:01:31.261503 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:01:31.261507 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:01:31.261512 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:01:31.261517 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:01:31.261520 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:01:31.261523 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:01:31.261528 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:01:31.261533 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:01:31.261536 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:01:31.261541 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:01:31.261545 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:01:31.261548 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:01:31.261553 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:01:31.261557 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:01:31.386776 31421 solver.cpp:567] Iteration 28000, Testing net (#0)
I1003 03:01:41.822957 31418 data_reader.cpp:305] Starting prefetch of epoch 4
I1003 03:02:06.834828 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.952133
I1003 03:02:06.834880 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:02:06.834887 31421 solver.cpp:659]     Test net output #2: loss = 0.140719 (* 1 = 0.140719 loss)
I1003 03:02:06.834916 31421 solver.cpp:265] [MultiGPU] Tests completed in 35.5723s
I1003 03:02:07.033321 31421 solver.cpp:314] Iteration 28000 (1.86521 iter/s, 53.6133s/100 iter), loss = 0.076656
I1003 03:02:07.033346 31421 solver.cpp:336]     Train net output #0: loss = 0.0766561 (* 1 = 0.0766561 loss)
I1003 03:02:07.033350 31421 sgd_solver.cpp:136] Iteration 28000, lr = 0.01, m = 0.9
I1003 03:02:23.596285 31432 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 03:02:24.688763 31421 solver.cpp:314] Iteration 28100 (5.66414 iter/s, 17.6549s/100 iter), loss = 0.0651258
I1003 03:02:24.688783 31421 solver.cpp:336]     Train net output #0: loss = 0.0651258 (* 1 = 0.0651258 loss)
I1003 03:02:24.688788 31421 sgd_solver.cpp:136] Iteration 28100, lr = 0.01, m = 0.9
I1003 03:02:42.389506 31421 solver.cpp:314] Iteration 28200 (5.64964 iter/s, 17.7002s/100 iter), loss = 0.0737631
I1003 03:02:42.389585 31421 solver.cpp:336]     Train net output #0: loss = 0.0737631 (* 1 = 0.0737631 loss)
I1003 03:02:42.389591 31421 sgd_solver.cpp:136] Iteration 28200, lr = 0.01, m = 0.9
I1003 03:03:00.679183 31421 solver.cpp:314] Iteration 28300 (5.46772 iter/s, 18.2892s/100 iter), loss = 0.0530466
I1003 03:03:00.679208 31421 solver.cpp:336]     Train net output #0: loss = 0.0530466 (* 1 = 0.0530466 loss)
I1003 03:03:00.679213 31421 sgd_solver.cpp:136] Iteration 28300, lr = 0.01, m = 0.9
I1003 03:03:19.523520 31421 solver.cpp:314] Iteration 28400 (5.30678 iter/s, 18.8438s/100 iter), loss = 0.0796833
I1003 03:03:19.523633 31421 solver.cpp:336]     Train net output #0: loss = 0.0796833 (* 1 = 0.0796833 loss)
I1003 03:03:19.523640 31421 sgd_solver.cpp:136] Iteration 28400, lr = 0.01, m = 0.9
I1003 03:03:25.306689 31344 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 03:03:25.306689 31432 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 03:03:44.871744 31421 solver.cpp:314] Iteration 28500 (3.94516 iter/s, 25.3475s/100 iter), loss = 0.0675713
I1003 03:03:44.871769 31421 solver.cpp:336]     Train net output #0: loss = 0.0675713 (* 1 = 0.0675713 loss)
I1003 03:03:44.871773 31421 sgd_solver.cpp:136] Iteration 28500, lr = 0.01, m = 0.9
I1003 03:04:09.445315 31421 solver.cpp:314] Iteration 28600 (4.06953 iter/s, 24.5729s/100 iter), loss = 0.105885
I1003 03:04:09.445376 31421 solver.cpp:336]     Train net output #0: loss = 0.105885 (* 1 = 0.105885 loss)
I1003 03:04:09.445381 31421 sgd_solver.cpp:136] Iteration 28600, lr = 0.01, m = 0.9
I1003 03:04:32.659072 31421 solver.cpp:314] Iteration 28700 (4.30791 iter/s, 23.2131s/100 iter), loss = 0.0906014
I1003 03:04:32.659096 31421 solver.cpp:336]     Train net output #0: loss = 0.0906015 (* 1 = 0.0906015 loss)
I1003 03:04:32.659101 31421 sgd_solver.cpp:136] Iteration 28700, lr = 0.01, m = 0.9
I1003 03:04:42.527796 31432 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 03:04:51.262732 31421 solver.cpp:314] Iteration 28800 (5.37544 iter/s, 18.6031s/100 iter), loss = 0.0634343
I1003 03:04:51.262759 31421 solver.cpp:336]     Train net output #0: loss = 0.0634344 (* 1 = 0.0634344 loss)
I1003 03:04:51.262765 31421 sgd_solver.cpp:136] Iteration 28800, lr = 0.01, m = 0.9
I1003 03:05:22.418967 31421 solver.cpp:314] Iteration 28900 (3.20972 iter/s, 31.1554s/100 iter), loss = 0.127091
I1003 03:05:22.419023 31421 solver.cpp:336]     Train net output #0: loss = 0.127091 (* 1 = 0.127091 loss)
I1003 03:05:22.419028 31421 sgd_solver.cpp:136] Iteration 28900, lr = 0.01, m = 0.9
I1003 03:06:02.606971 31421 solver.cpp:352] Sparsity after update:
I1003 03:06:02.608518 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:06:02.608525 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:06:02.608531 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:06:02.608533 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:06:02.608536 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:06:02.608538 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:06:02.608541 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:06:02.608543 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:06:02.608546 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:06:02.608548 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:06:02.608551 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:06:02.608552 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:06:02.608556 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:06:02.608559 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:06:02.608563 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:06:02.608568 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:06:02.608572 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:06:02.608577 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:06:02.608580 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:06:03.145179 31421 solver.cpp:314] Iteration 29000 (2.45549 iter/s, 40.7251s/100 iter), loss = 0.0624134
I1003 03:06:03.145203 31421 solver.cpp:336]     Train net output #0: loss = 0.0624135 (* 1 = 0.0624135 loss)
I1003 03:06:03.145208 31421 sgd_solver.cpp:136] Iteration 29000, lr = 0.01, m = 0.9
I1003 03:06:44.595682 31432 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 03:06:48.636623 31421 solver.cpp:314] Iteration 29100 (2.19828 iter/s, 45.4901s/100 iter), loss = 0.0657105
I1003 03:06:48.636649 31421 solver.cpp:336]     Train net output #0: loss = 0.0657105 (* 1 = 0.0657105 loss)
I1003 03:06:48.636654 31421 sgd_solver.cpp:136] Iteration 29100, lr = 0.01, m = 0.9
I1003 03:07:14.638181 31421 solver.cpp:314] Iteration 29200 (3.84603 iter/s, 26.0008s/100 iter), loss = 0.0536355
I1003 03:07:14.638298 31421 solver.cpp:336]     Train net output #0: loss = 0.0536355 (* 1 = 0.0536355 loss)
I1003 03:07:14.638305 31421 sgd_solver.cpp:136] Iteration 29200, lr = 0.01, m = 0.9
I1003 03:07:43.958611 31421 solver.cpp:314] Iteration 29300 (3.41069 iter/s, 29.3196s/100 iter), loss = 0.0463909
I1003 03:07:43.958788 31421 solver.cpp:336]     Train net output #0: loss = 0.046391 (* 1 = 0.046391 loss)
I1003 03:07:43.958796 31421 sgd_solver.cpp:136] Iteration 29300, lr = 0.01, m = 0.9
I1003 03:08:12.501405 31421 solver.cpp:314] Iteration 29400 (3.50361 iter/s, 28.542s/100 iter), loss = 0.0542385
I1003 03:08:12.501521 31421 solver.cpp:336]     Train net output #0: loss = 0.0542385 (* 1 = 0.0542385 loss)
I1003 03:08:12.501528 31421 sgd_solver.cpp:136] Iteration 29400, lr = 0.01, m = 0.9
I1003 03:08:17.412186 31432 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 03:08:17.412186 31344 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 03:08:17.412186 31430 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 03:08:34.049263 31421 solver.cpp:314] Iteration 29500 (4.64096 iter/s, 21.5472s/100 iter), loss = 0.0853852
I1003 03:08:34.049283 31421 solver.cpp:336]     Train net output #0: loss = 0.0853852 (* 1 = 0.0853852 loss)
I1003 03:08:34.049288 31421 sgd_solver.cpp:136] Iteration 29500, lr = 0.01, m = 0.9
I1003 03:08:52.113744 31421 solver.cpp:314] Iteration 29600 (5.53588 iter/s, 18.064s/100 iter), loss = 0.0642459
I1003 03:08:52.113847 31421 solver.cpp:336]     Train net output #0: loss = 0.0642459 (* 1 = 0.0642459 loss)
I1003 03:08:52.113854 31421 sgd_solver.cpp:136] Iteration 29600, lr = 0.01, m = 0.9
I1003 03:09:09.996278 31421 solver.cpp:314] Iteration 29700 (5.59221 iter/s, 17.882s/100 iter), loss = 0.0586373
I1003 03:09:09.996301 31421 solver.cpp:336]     Train net output #0: loss = 0.0586373 (* 1 = 0.0586373 loss)
I1003 03:09:09.996305 31421 sgd_solver.cpp:136] Iteration 29700, lr = 0.01, m = 0.9
I1003 03:09:18.362089 31432 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 03:09:28.048033 31421 solver.cpp:314] Iteration 29800 (5.53979 iter/s, 18.0512s/100 iter), loss = 0.118478
I1003 03:09:28.052374 31421 solver.cpp:336]     Train net output #0: loss = 0.118478 (* 1 = 0.118478 loss)
I1003 03:09:28.052436 31421 sgd_solver.cpp:136] Iteration 29800, lr = 0.01, m = 0.9
I1003 03:09:46.019881 31421 solver.cpp:314] Iteration 29900 (5.56442 iter/s, 17.9713s/100 iter), loss = 0.0693299
I1003 03:09:46.019904 31421 solver.cpp:336]     Train net output #0: loss = 0.06933 (* 1 = 0.06933 loss)
I1003 03:09:46.019908 31421 sgd_solver.cpp:136] Iteration 29900, lr = 0.01, m = 0.9
I1003 03:10:03.778676 31421 solver.cpp:352] Sparsity after update:
I1003 03:10:03.780318 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:10:03.780326 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:10:03.780331 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:10:03.780334 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:10:03.780336 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:10:03.780339 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:10:03.780340 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:10:03.780342 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:10:03.780344 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:10:03.780346 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:10:03.780349 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:10:03.780350 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:10:03.780351 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:10:03.780354 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:10:03.780355 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:10:03.780357 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:10:03.780359 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:10:03.780361 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:10:03.780364 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:10:03.785390 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I1003 03:10:04.282768 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I1003 03:10:04.301326 31421 solver.cpp:567] Iteration 30000, Testing net (#0)
I1003 03:10:20.941807 31541 blocking_queue.cpp:40] Waiting for datum
I1003 03:10:27.927279 31418 data_reader.cpp:305] Starting prefetch of epoch 5
I1003 03:10:39.840826 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.952816
I1003 03:10:39.840952 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:10:39.840961 31421 solver.cpp:659]     Test net output #2: loss = 0.151005 (* 1 = 0.151005 loss)
I1003 03:10:39.840994 31421 solver.cpp:265] [MultiGPU] Tests completed in 35.5387s
I1003 03:10:39.926187 31572 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I1003 03:10:39.926198 31573 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I1003 03:10:39.926204 31574 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I1003 03:10:40.030490 31421 solver.cpp:314] Iteration 30000 (1.85154 iter/s, 54.0091s/100 iter), loss = 0.116532
I1003 03:10:40.030517 31421 solver.cpp:336]     Train net output #0: loss = 0.116532 (* 1 = 0.116532 loss)
I1003 03:10:40.030524 31421 sgd_solver.cpp:136] Iteration 30000, lr = 0.001, m = 0.9
I1003 03:10:57.938443 31421 solver.cpp:314] Iteration 30100 (5.58427 iter/s, 17.9074s/100 iter), loss = 0.0572016
I1003 03:10:57.938467 31421 solver.cpp:336]     Train net output #0: loss = 0.0572016 (* 1 = 0.0572016 loss)
I1003 03:10:57.938470 31421 sgd_solver.cpp:136] Iteration 30100, lr = 0.001, m = 0.9
I1003 03:11:15.566901 31421 solver.cpp:314] Iteration 30200 (5.67281 iter/s, 17.6279s/100 iter), loss = 0.0621377
I1003 03:11:15.566977 31421 solver.cpp:336]     Train net output #0: loss = 0.0621377 (* 1 = 0.0621377 loss)
I1003 03:11:15.566983 31421 sgd_solver.cpp:136] Iteration 30200, lr = 0.001, m = 0.9
I1003 03:11:24.243386 31430 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 03:11:43.156430 31421 solver.cpp:314] Iteration 30300 (3.62467 iter/s, 27.5887s/100 iter), loss = 0.0728225
I1003 03:11:43.156456 31421 solver.cpp:336]     Train net output #0: loss = 0.0728225 (* 1 = 0.0728225 loss)
I1003 03:11:43.156462 31421 sgd_solver.cpp:136] Iteration 30300, lr = 0.001, m = 0.9
I1003 03:12:20.600710 31421 solver.cpp:314] Iteration 30400 (2.67071 iter/s, 37.4432s/100 iter), loss = 0.0698083
I1003 03:12:20.600806 31421 solver.cpp:336]     Train net output #0: loss = 0.0698083 (* 1 = 0.0698083 loss)
I1003 03:12:20.600811 31421 sgd_solver.cpp:136] Iteration 30400, lr = 0.001, m = 0.9
I1003 03:12:57.938943 31421 solver.cpp:314] Iteration 30500 (2.6783 iter/s, 37.3372s/100 iter), loss = 0.0768729
I1003 03:12:57.938998 31421 solver.cpp:336]     Train net output #0: loss = 0.076873 (* 1 = 0.076873 loss)
I1003 03:12:57.939004 31421 sgd_solver.cpp:136] Iteration 30500, lr = 0.001, m = 0.9
I1003 03:13:21.566468 31432 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 03:13:34.927081 31421 solver.cpp:314] Iteration 30600 (2.70365 iter/s, 36.9871s/100 iter), loss = 0.0507585
I1003 03:13:34.927170 31421 solver.cpp:336]     Train net output #0: loss = 0.0507585 (* 1 = 0.0507585 loss)
I1003 03:13:34.927175 31421 sgd_solver.cpp:136] Iteration 30600, lr = 0.001, m = 0.9
I1003 03:14:01.707193 31421 solver.cpp:314] Iteration 30700 (3.73422 iter/s, 26.7793s/100 iter), loss = 0.120702
I1003 03:14:01.707222 31421 solver.cpp:336]     Train net output #0: loss = 0.120702 (* 1 = 0.120702 loss)
I1003 03:14:01.707229 31421 sgd_solver.cpp:136] Iteration 30700, lr = 0.001, m = 0.9
I1003 03:14:25.443225 31421 solver.cpp:314] Iteration 30800 (4.21313 iter/s, 23.7353s/100 iter), loss = 0.0603
I1003 03:14:25.443337 31421 solver.cpp:336]     Train net output #0: loss = 0.0603001 (* 1 = 0.0603001 loss)
I1003 03:14:25.443346 31421 sgd_solver.cpp:136] Iteration 30800, lr = 0.001, m = 0.9
I1003 03:15:03.048032 31421 solver.cpp:314] Iteration 30900 (2.65931 iter/s, 37.6037s/100 iter), loss = 0.0799054
I1003 03:15:03.048110 31421 solver.cpp:336]     Train net output #0: loss = 0.0799054 (* 1 = 0.0799054 loss)
I1003 03:15:03.048116 31421 sgd_solver.cpp:136] Iteration 30900, lr = 0.001, m = 0.9
I1003 03:15:03.940708 31430 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 03:15:22.725018 31421 solver.cpp:352] Sparsity after update:
I1003 03:15:22.728008 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:15:22.728018 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:15:22.728286 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:15:22.728291 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:15:22.728296 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:15:22.728299 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:15:22.728303 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:15:22.728307 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:15:22.728310 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:15:22.728315 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:15:22.728318 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:15:22.728322 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:15:22.728327 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:15:22.728330 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:15:22.728335 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:15:22.728339 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:15:22.728343 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:15:22.728348 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:15:22.728351 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:15:22.896912 31421 solver.cpp:314] Iteration 31000 (5.03821 iter/s, 19.8483s/100 iter), loss = 0.0548499
I1003 03:15:22.896939 31421 solver.cpp:336]     Train net output #0: loss = 0.0548499 (* 1 = 0.0548499 loss)
I1003 03:15:22.896946 31421 sgd_solver.cpp:136] Iteration 31000, lr = 0.001, m = 0.9
I1003 03:15:35.256345 31425 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 03:15:40.813747 31421 solver.cpp:314] Iteration 31100 (5.5815 iter/s, 17.9163s/100 iter), loss = 0.0529725
I1003 03:15:40.813772 31421 solver.cpp:336]     Train net output #0: loss = 0.0529725 (* 1 = 0.0529725 loss)
I1003 03:15:40.813776 31421 sgd_solver.cpp:136] Iteration 31100, lr = 0.001, m = 0.9
I1003 03:15:58.729306 31421 solver.cpp:314] Iteration 31200 (5.5819 iter/s, 17.915s/100 iter), loss = 0.137569
I1003 03:15:58.729331 31421 solver.cpp:336]     Train net output #0: loss = 0.137569 (* 1 = 0.137569 loss)
I1003 03:15:58.729336 31421 sgd_solver.cpp:136] Iteration 31200, lr = 0.001, m = 0.9
I1003 03:16:16.787854 31421 solver.cpp:314] Iteration 31300 (5.5377 iter/s, 18.058s/100 iter), loss = 0.0879606
I1003 03:16:16.787955 31421 solver.cpp:336]     Train net output #0: loss = 0.0879606 (* 1 = 0.0879606 loss)
I1003 03:16:16.787959 31421 sgd_solver.cpp:136] Iteration 31300, lr = 0.001, m = 0.9
I1003 03:16:34.507901 31430 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 03:16:34.675853 31421 solver.cpp:314] Iteration 31400 (5.5905 iter/s, 17.8875s/100 iter), loss = 0.0547375
I1003 03:16:34.675874 31421 solver.cpp:336]     Train net output #0: loss = 0.0547375 (* 1 = 0.0547375 loss)
I1003 03:16:34.675879 31421 sgd_solver.cpp:136] Iteration 31400, lr = 0.001, m = 0.9
I1003 03:16:52.781898 31421 solver.cpp:314] Iteration 31500 (5.52317 iter/s, 18.1055s/100 iter), loss = 0.0529994
I1003 03:16:52.781955 31421 solver.cpp:336]     Train net output #0: loss = 0.0529994 (* 1 = 0.0529994 loss)
I1003 03:16:52.781961 31421 sgd_solver.cpp:136] Iteration 31500, lr = 0.001, m = 0.9
I1003 03:17:10.880353 31421 solver.cpp:314] Iteration 31600 (5.52549 iter/s, 18.0979s/100 iter), loss = 0.190829
I1003 03:17:10.880378 31421 solver.cpp:336]     Train net output #0: loss = 0.190829 (* 1 = 0.190829 loss)
I1003 03:17:10.880383 31421 sgd_solver.cpp:136] Iteration 31600, lr = 0.001, m = 0.9
I1003 03:17:28.994935 31421 solver.cpp:314] Iteration 31700 (5.52057 iter/s, 18.1141s/100 iter), loss = 0.0496465
I1003 03:17:28.995035 31421 solver.cpp:336]     Train net output #0: loss = 0.0496465 (* 1 = 0.0496465 loss)
I1003 03:17:28.995041 31421 sgd_solver.cpp:136] Iteration 31700, lr = 0.001, m = 0.9
I1003 03:17:34.348908 31432 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 03:17:46.928584 31421 solver.cpp:314] Iteration 31800 (5.57627 iter/s, 17.9331s/100 iter), loss = 0.0722441
I1003 03:17:46.928606 31421 solver.cpp:336]     Train net output #0: loss = 0.0722441 (* 1 = 0.0722441 loss)
I1003 03:17:46.928611 31421 sgd_solver.cpp:136] Iteration 31800, lr = 0.001, m = 0.9
I1003 03:18:04.005410 31342 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 03:18:04.871268 31421 solver.cpp:314] Iteration 31900 (5.57346 iter/s, 17.9422s/100 iter), loss = 0.0374984
I1003 03:18:04.871290 31421 solver.cpp:336]     Train net output #0: loss = 0.0374985 (* 1 = 0.0374985 loss)
I1003 03:18:04.871294 31421 sgd_solver.cpp:136] Iteration 31900, lr = 0.001, m = 0.9
I1003 03:18:22.608700 31421 solver.cpp:352] Sparsity after update:
I1003 03:18:22.613595 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:18:22.613601 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:18:22.613607 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:18:22.613610 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:18:22.613612 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:18:22.613615 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:18:22.613616 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:18:22.613617 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:18:22.613620 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:18:22.613621 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:18:22.613623 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:18:22.613626 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:18:22.613627 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:18:22.613629 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:18:22.613631 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:18:22.613633 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:18:22.613636 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:18:22.613637 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:18:22.613639 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:18:22.613651 31421 solver.cpp:567] Iteration 32000, Testing net (#0)
I1003 03:18:55.717219 31536 data_reader.cpp:305] Starting prefetch of epoch 5
I1003 03:18:56.131974 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.953032
I1003 03:18:56.131994 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:18:56.131999 31421 solver.cpp:659]     Test net output #2: loss = 0.14126 (* 1 = 0.14126 loss)
I1003 03:18:56.132035 31421 solver.cpp:265] [MultiGPU] Tests completed in 33.5174s
I1003 03:18:56.328816 31421 solver.cpp:314] Iteration 32000 (1.9434 iter/s, 51.4561s/100 iter), loss = 0.064645
I1003 03:18:56.328847 31421 solver.cpp:336]     Train net output #0: loss = 0.0646451 (* 1 = 0.0646451 loss)
I1003 03:18:56.328855 31421 sgd_solver.cpp:136] Iteration 32000, lr = 0.001, m = 0.9
I1003 03:19:14.413594 31421 solver.cpp:314] Iteration 32100 (5.52967 iter/s, 18.0843s/100 iter), loss = 0.0832882
I1003 03:19:14.413619 31421 solver.cpp:336]     Train net output #0: loss = 0.0832882 (* 1 = 0.0832882 loss)
I1003 03:19:14.413624 31421 sgd_solver.cpp:136] Iteration 32100, lr = 0.001, m = 0.9
I1003 03:19:32.299556 31421 solver.cpp:314] Iteration 32200 (5.59114 iter/s, 17.8855s/100 iter), loss = 0.0580186
I1003 03:19:32.300662 31421 solver.cpp:336]     Train net output #0: loss = 0.0580186 (* 1 = 0.0580186 loss)
I1003 03:19:32.300671 31421 sgd_solver.cpp:136] Iteration 32200, lr = 0.001, m = 0.9
I1003 03:19:37.557711 31432 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 03:19:53.032373 31421 solver.cpp:314] Iteration 32300 (4.82341 iter/s, 20.7322s/100 iter), loss = 0.0745319
I1003 03:19:53.032395 31421 solver.cpp:336]     Train net output #0: loss = 0.074532 (* 1 = 0.074532 loss)
I1003 03:19:53.032399 31421 sgd_solver.cpp:136] Iteration 32300, lr = 0.001, m = 0.9
I1003 03:20:14.853298 31421 solver.cpp:314] Iteration 32400 (4.58289 iter/s, 21.8203s/100 iter), loss = 0.0590518
I1003 03:20:14.853396 31421 solver.cpp:336]     Train net output #0: loss = 0.0590519 (* 1 = 0.0590519 loss)
I1003 03:20:14.853404 31421 sgd_solver.cpp:136] Iteration 32400, lr = 0.001, m = 0.9
I1003 03:20:53.343689 31421 solver.cpp:314] Iteration 32500 (2.59812 iter/s, 38.4893s/100 iter), loss = 0.0579181
I1003 03:20:53.343767 31421 solver.cpp:336]     Train net output #0: loss = 0.0579182 (* 1 = 0.0579182 loss)
I1003 03:20:53.343773 31421 sgd_solver.cpp:136] Iteration 32500, lr = 0.001, m = 0.9
I1003 03:21:03.278106 31432 data_reader.cpp:305] Starting prefetch of epoch 27
I1003 03:21:11.128273 31421 solver.cpp:314] Iteration 32600 (5.62301 iter/s, 17.7841s/100 iter), loss = 0.0481369
I1003 03:21:11.128298 31421 solver.cpp:336]     Train net output #0: loss = 0.048137 (* 1 = 0.048137 loss)
I1003 03:21:11.128303 31421 sgd_solver.cpp:136] Iteration 32600, lr = 0.001, m = 0.9
I1003 03:21:32.342700 31421 solver.cpp:314] Iteration 32700 (4.71391 iter/s, 21.2138s/100 iter), loss = 0.0659857
I1003 03:21:32.342762 31421 solver.cpp:336]     Train net output #0: loss = 0.0659858 (* 1 = 0.0659858 loss)
I1003 03:21:32.342768 31421 sgd_solver.cpp:136] Iteration 32700, lr = 0.001, m = 0.9
I1003 03:21:50.542027 31421 solver.cpp:314] Iteration 32800 (5.49486 iter/s, 18.1988s/100 iter), loss = 0.073883
I1003 03:21:50.542053 31421 solver.cpp:336]     Train net output #0: loss = 0.0738831 (* 1 = 0.0738831 loss)
I1003 03:21:50.542057 31421 sgd_solver.cpp:136] Iteration 32800, lr = 0.001, m = 0.9
I1003 03:22:06.136116 31430 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 03:22:08.479254 31421 solver.cpp:314] Iteration 32900 (5.57516 iter/s, 17.9367s/100 iter), loss = 0.0812887
I1003 03:22:08.479281 31421 solver.cpp:336]     Train net output #0: loss = 0.0812888 (* 1 = 0.0812888 loss)
I1003 03:22:08.479286 31421 sgd_solver.cpp:136] Iteration 32900, lr = 0.001, m = 0.9
I1003 03:22:26.538941 31421 solver.cpp:352] Sparsity after update:
I1003 03:22:26.546105 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:22:26.546113 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:22:26.546119 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:22:26.546121 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:22:26.546123 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:22:26.546125 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:22:26.546128 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:22:26.546129 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:22:26.546131 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:22:26.546133 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:22:26.546135 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:22:26.546138 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:22:26.546139 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:22:26.546141 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:22:26.546144 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:22:26.546145 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:22:26.546147 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:22:26.546149 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:22:26.546152 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:22:26.714004 31421 solver.cpp:314] Iteration 33000 (5.48419 iter/s, 18.2342s/100 iter), loss = 0.0634056
I1003 03:22:26.714026 31421 solver.cpp:336]     Train net output #0: loss = 0.0634057 (* 1 = 0.0634057 loss)
I1003 03:22:26.714031 31421 sgd_solver.cpp:136] Iteration 33000, lr = 0.001, m = 0.9
I1003 03:22:36.120990 31342 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 03:22:44.646221 31421 solver.cpp:314] Iteration 33100 (5.57671 iter/s, 17.9317s/100 iter), loss = 0.0678609
I1003 03:22:44.646294 31421 solver.cpp:336]     Train net output #0: loss = 0.0678611 (* 1 = 0.0678611 loss)
I1003 03:22:44.646301 31421 sgd_solver.cpp:136] Iteration 33100, lr = 0.001, m = 0.9
I1003 03:23:02.742837 31421 solver.cpp:314] Iteration 33200 (5.52605 iter/s, 18.0961s/100 iter), loss = 0.0919129
I1003 03:23:02.742861 31421 solver.cpp:336]     Train net output #0: loss = 0.091913 (* 1 = 0.091913 loss)
I1003 03:23:02.742866 31421 sgd_solver.cpp:136] Iteration 33200, lr = 0.001, m = 0.9
I1003 03:23:20.699728 31421 solver.cpp:314] Iteration 33300 (5.56905 iter/s, 17.9564s/100 iter), loss = 0.069532
I1003 03:23:20.699838 31421 solver.cpp:336]     Train net output #0: loss = 0.0695321 (* 1 = 0.0695321 loss)
I1003 03:23:20.699846 31421 sgd_solver.cpp:136] Iteration 33300, lr = 0.001, m = 0.9
I1003 03:23:35.713506 31342 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 03:23:38.742374 31421 solver.cpp:314] Iteration 33400 (5.54258 iter/s, 18.0421s/100 iter), loss = 0.0806118
I1003 03:23:38.742398 31421 solver.cpp:336]     Train net output #0: loss = 0.0806119 (* 1 = 0.0806119 loss)
I1003 03:23:38.742401 31421 sgd_solver.cpp:136] Iteration 33400, lr = 0.001, m = 0.9
I1003 03:23:56.724624 31421 solver.cpp:314] Iteration 33500 (5.56119 iter/s, 17.9817s/100 iter), loss = 0.0682516
I1003 03:23:56.724730 31421 solver.cpp:336]     Train net output #0: loss = 0.0682517 (* 1 = 0.0682517 loss)
I1003 03:23:56.724737 31421 sgd_solver.cpp:136] Iteration 33500, lr = 0.001, m = 0.9
I1003 03:24:14.873266 31421 solver.cpp:314] Iteration 33600 (5.51021 iter/s, 18.1481s/100 iter), loss = 0.0665137
I1003 03:24:14.873288 31421 solver.cpp:336]     Train net output #0: loss = 0.0665137 (* 1 = 0.0665137 loss)
I1003 03:24:14.873294 31421 sgd_solver.cpp:136] Iteration 33600, lr = 0.001, m = 0.9
I1003 03:24:32.860121 31421 solver.cpp:314] Iteration 33700 (5.55977 iter/s, 17.9863s/100 iter), loss = 0.133424
I1003 03:24:32.860229 31421 solver.cpp:336]     Train net output #0: loss = 0.133424 (* 1 = 0.133424 loss)
I1003 03:24:32.860236 31421 sgd_solver.cpp:136] Iteration 33700, lr = 0.001, m = 0.9
I1003 03:24:35.254286 31432 data_reader.cpp:305] Starting prefetch of epoch 28
I1003 03:24:51.626446 31421 solver.cpp:314] Iteration 33800 (5.32884 iter/s, 18.7658s/100 iter), loss = 0.0563067
I1003 03:24:51.626471 31421 solver.cpp:336]     Train net output #0: loss = 0.0563068 (* 1 = 0.0563068 loss)
I1003 03:24:51.626477 31421 sgd_solver.cpp:136] Iteration 33800, lr = 0.001, m = 0.9
I1003 03:25:05.900679 31425 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 03:25:09.908721 31421 solver.cpp:314] Iteration 33900 (5.46993 iter/s, 18.2818s/100 iter), loss = 0.0588991
I1003 03:25:09.908743 31421 solver.cpp:336]     Train net output #0: loss = 0.0588992 (* 1 = 0.0588992 loss)
I1003 03:25:09.908748 31421 sgd_solver.cpp:136] Iteration 33900, lr = 0.001, m = 0.9
I1003 03:25:27.895578 31421 solver.cpp:352] Sparsity after update:
I1003 03:25:27.900822 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:25:27.900830 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:25:27.900836 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:25:27.900840 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:25:27.900842 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:25:27.900846 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:25:27.900851 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:25:27.900854 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:25:27.900859 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:25:27.900863 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:25:27.900866 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:25:27.900871 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:25:27.900874 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:25:27.900878 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:25:27.900882 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:25:27.900885 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:25:27.900888 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:25:27.900892 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:25:27.900897 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:25:27.900909 31421 solver.cpp:567] Iteration 34000, Testing net (#0)
I1003 03:26:04.218430 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.956177
I1003 03:26:04.218524 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:26:04.218533 31421 solver.cpp:659]     Test net output #2: loss = 0.14644 (* 1 = 0.14644 loss)
I1003 03:26:04.218566 31421 solver.cpp:265] [MultiGPU] Tests completed in 36.3166s
I1003 03:26:04.398528 31421 solver.cpp:314] Iteration 34000 (1.83526 iter/s, 54.4883s/100 iter), loss = 0.149597
I1003 03:26:04.398551 31421 solver.cpp:336]     Train net output #0: loss = 0.149597 (* 1 = 0.149597 loss)
I1003 03:26:04.398557 31421 sgd_solver.cpp:136] Iteration 34000, lr = 0.001, m = 0.9
I1003 03:26:17.978127 31424 data_reader.cpp:305] Starting prefetch of epoch 11
I1003 03:26:34.454984 31421 solver.cpp:314] Iteration 34100 (3.32717 iter/s, 30.0556s/100 iter), loss = 0.0537767
I1003 03:26:34.455432 31421 solver.cpp:336]     Train net output #0: loss = 0.0537768 (* 1 = 0.0537768 loss)
I1003 03:26:34.455437 31421 sgd_solver.cpp:136] Iteration 34100, lr = 0.001, m = 0.9
I1003 03:26:56.503501 31421 solver.cpp:314] Iteration 34200 (4.53558 iter/s, 22.0479s/100 iter), loss = 0.0438392
I1003 03:26:56.503532 31421 solver.cpp:336]     Train net output #0: loss = 0.0438392 (* 1 = 0.0438392 loss)
I1003 03:26:56.503540 31421 sgd_solver.cpp:136] Iteration 34200, lr = 0.001, m = 0.9
I1003 03:27:17.507519 31421 solver.cpp:314] Iteration 34300 (4.76113 iter/s, 21.0034s/100 iter), loss = 0.0576043
I1003 03:27:17.507570 31421 solver.cpp:336]     Train net output #0: loss = 0.0576044 (* 1 = 0.0576044 loss)
I1003 03:27:17.507578 31421 sgd_solver.cpp:136] Iteration 34300, lr = 0.001, m = 0.9
I1003 03:27:30.812963 31432 data_reader.cpp:305] Starting prefetch of epoch 29
I1003 03:27:35.354348 31421 solver.cpp:314] Iteration 34400 (5.60339 iter/s, 17.8463s/100 iter), loss = 0.0595434
I1003 03:27:35.354372 31421 solver.cpp:336]     Train net output #0: loss = 0.0595434 (* 1 = 0.0595434 loss)
I1003 03:27:35.354375 31421 sgd_solver.cpp:136] Iteration 34400, lr = 0.001, m = 0.9
I1003 03:27:53.675667 31421 solver.cpp:314] Iteration 34500 (5.45828 iter/s, 18.3208s/100 iter), loss = 0.0858702
I1003 03:27:53.675722 31421 solver.cpp:336]     Train net output #0: loss = 0.0858703 (* 1 = 0.0858703 loss)
I1003 03:27:53.675727 31421 sgd_solver.cpp:136] Iteration 34500, lr = 0.001, m = 0.9
I1003 03:28:04.411919 31431 blocking_queue.cpp:40] Waiting for datum
I1003 03:28:13.309959 31421 solver.cpp:314] Iteration 34600 (5.09327 iter/s, 19.6337s/100 iter), loss = 0.0806415
I1003 03:28:13.309986 31421 solver.cpp:336]     Train net output #0: loss = 0.0806416 (* 1 = 0.0806416 loss)
I1003 03:28:13.309991 31421 sgd_solver.cpp:136] Iteration 34600, lr = 0.001, m = 0.9
I1003 03:28:31.605801 31421 solver.cpp:314] Iteration 34700 (5.46588 iter/s, 18.2953s/100 iter), loss = 0.0465477
I1003 03:28:31.605856 31421 solver.cpp:336]     Train net output #0: loss = 0.0465477 (* 1 = 0.0465477 loss)
I1003 03:28:31.605862 31421 sgd_solver.cpp:136] Iteration 34700, lr = 0.001, m = 0.9
I1003 03:28:32.528127 31430 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 03:28:49.604480 31421 solver.cpp:314] Iteration 34800 (5.55612 iter/s, 17.9982s/100 iter), loss = 0.0593952
I1003 03:28:49.604501 31421 solver.cpp:336]     Train net output #0: loss = 0.0593953 (* 1 = 0.0593953 loss)
I1003 03:28:49.604506 31421 sgd_solver.cpp:136] Iteration 34800, lr = 0.001, m = 0.9
I1003 03:29:07.562734 31421 solver.cpp:314] Iteration 34900 (5.56863 iter/s, 17.9577s/100 iter), loss = 0.0781869
I1003 03:29:07.562827 31421 solver.cpp:336]     Train net output #0: loss = 0.078187 (* 1 = 0.078187 loss)
I1003 03:29:07.562834 31421 sgd_solver.cpp:136] Iteration 34900, lr = 0.001, m = 0.9
I1003 03:29:25.332818 31421 solver.cpp:352] Sparsity after update:
I1003 03:29:25.337534 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:29:25.337544 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:29:25.337815 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:29:25.337822 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:29:25.337826 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:29:25.337829 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:29:25.337833 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:29:25.337836 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:29:25.337841 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:29:25.337843 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:29:25.337847 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:29:25.337852 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:29:25.337859 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:29:25.337864 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:29:25.337868 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:29:25.337872 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:29:25.337877 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:29:25.337880 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:29:25.337883 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:29:25.510438 31421 solver.cpp:314] Iteration 35000 (5.5719 iter/s, 17.9472s/100 iter), loss = 0.0645877
I1003 03:29:25.510462 31421 solver.cpp:336]     Train net output #0: loss = 0.0645878 (* 1 = 0.0645878 loss)
I1003 03:29:25.510466 31421 sgd_solver.cpp:136] Iteration 35000, lr = 0.001, m = 0.9
I1003 03:29:31.802799 31430 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 03:29:43.474966 31421 solver.cpp:314] Iteration 35100 (5.56668 iter/s, 17.964s/100 iter), loss = 0.0698618
I1003 03:29:43.475427 31421 solver.cpp:336]     Train net output #0: loss = 0.0698618 (* 1 = 0.0698618 loss)
I1003 03:29:43.475436 31421 sgd_solver.cpp:136] Iteration 35100, lr = 0.001, m = 0.9
I1003 03:30:01.428086 31421 solver.cpp:314] Iteration 35200 (5.57022 iter/s, 17.9526s/100 iter), loss = 0.047786
I1003 03:30:01.428112 31421 solver.cpp:336]     Train net output #0: loss = 0.0477861 (* 1 = 0.0477861 loss)
I1003 03:30:01.428117 31421 sgd_solver.cpp:136] Iteration 35200, lr = 0.001, m = 0.9
I1003 03:30:01.642874 31342 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 03:30:19.532680 31421 solver.cpp:314] Iteration 35300 (5.52362 iter/s, 18.1041s/100 iter), loss = 0.0650393
I1003 03:30:19.537595 31421 solver.cpp:336]     Train net output #0: loss = 0.0650394 (* 1 = 0.0650394 loss)
I1003 03:30:19.537642 31421 sgd_solver.cpp:136] Iteration 35300, lr = 0.001, m = 0.9
I1003 03:30:37.524905 31421 solver.cpp:314] Iteration 35400 (5.55811 iter/s, 17.9917s/100 iter), loss = 0.0686572
I1003 03:30:37.524930 31421 solver.cpp:336]     Train net output #0: loss = 0.0686572 (* 1 = 0.0686572 loss)
I1003 03:30:37.524935 31421 sgd_solver.cpp:136] Iteration 35400, lr = 0.001, m = 0.9
I1003 03:30:55.685583 31421 solver.cpp:314] Iteration 35500 (5.50656 iter/s, 18.1602s/100 iter), loss = 0.0411513
I1003 03:30:55.685628 31421 solver.cpp:336]     Train net output #0: loss = 0.0411513 (* 1 = 0.0411513 loss)
I1003 03:30:55.685633 31421 sgd_solver.cpp:136] Iteration 35500, lr = 0.001, m = 0.9
I1003 03:31:01.357908 31425 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 03:31:13.670078 31421 solver.cpp:314] Iteration 35600 (5.5605 iter/s, 17.984s/100 iter), loss = 0.0680612
I1003 03:31:13.670099 31421 solver.cpp:336]     Train net output #0: loss = 0.0680613 (* 1 = 0.0680613 loss)
I1003 03:31:13.670104 31421 sgd_solver.cpp:136] Iteration 35600, lr = 0.001, m = 0.9
I1003 03:31:31.741070 31421 solver.cpp:314] Iteration 35700 (5.53389 iter/s, 18.0705s/100 iter), loss = 0.0514696
I1003 03:31:31.741139 31421 solver.cpp:336]     Train net output #0: loss = 0.0514697 (* 1 = 0.0514697 loss)
I1003 03:31:31.741145 31421 sgd_solver.cpp:136] Iteration 35700, lr = 0.001, m = 0.9
I1003 03:31:49.794270 31421 solver.cpp:314] Iteration 35800 (5.53934 iter/s, 18.0527s/100 iter), loss = 0.0577536
I1003 03:31:49.794297 31421 solver.cpp:336]     Train net output #0: loss = 0.0577536 (* 1 = 0.0577536 loss)
I1003 03:31:49.794301 31421 sgd_solver.cpp:136] Iteration 35800, lr = 0.001, m = 0.9
I1003 03:32:00.881351 31430 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 03:32:07.682390 31421 solver.cpp:314] Iteration 35900 (5.59046 iter/s, 17.8876s/100 iter), loss = 0.0703751
I1003 03:32:07.682446 31421 solver.cpp:336]     Train net output #0: loss = 0.0703751 (* 1 = 0.0703751 loss)
I1003 03:32:07.682451 31421 sgd_solver.cpp:136] Iteration 35900, lr = 0.001, m = 0.9
I1003 03:32:25.436446 31421 solver.cpp:352] Sparsity after update:
I1003 03:32:25.442476 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:32:25.442486 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:32:25.442492 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:32:25.442494 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:32:25.442497 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:32:25.442498 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:32:25.442500 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:32:25.442502 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:32:25.442504 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:32:25.442507 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:32:25.442509 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:32:25.442512 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:32:25.442513 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:32:25.442515 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:32:25.442517 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:32:25.442519 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:32:25.442522 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:32:25.442523 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:32:25.442525 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:32:25.564862 31421 solver.cpp:567] Iteration 36000, Testing net (#0)
I1003 03:32:57.713251 31418 data_reader.cpp:305] Starting prefetch of epoch 6
I1003 03:32:58.358033 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.953173
I1003 03:32:58.358057 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:32:58.358062 31421 solver.cpp:659]     Test net output #2: loss = 0.144475 (* 1 = 0.144475 loss)
I1003 03:32:58.358141 31421 solver.cpp:265] [MultiGPU] Tests completed in 32.9147s
I1003 03:32:58.537467 31421 solver.cpp:314] Iteration 36000 (1.96643 iter/s, 50.8537s/100 iter), loss = 0.0771745
I1003 03:32:58.537497 31421 solver.cpp:336]     Train net output #0: loss = 0.0771745 (* 1 = 0.0771745 loss)
I1003 03:32:58.537503 31421 sgd_solver.cpp:136] Iteration 36000, lr = 0.001, m = 0.9
I1003 03:33:03.370579 31425 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 03:33:16.218086 31421 solver.cpp:314] Iteration 36100 (5.65607 iter/s, 17.6801s/100 iter), loss = 0.229052
I1003 03:33:16.218109 31421 solver.cpp:336]     Train net output #0: loss = 0.229052 (* 1 = 0.229052 loss)
I1003 03:33:16.218114 31421 sgd_solver.cpp:136] Iteration 36100, lr = 0.001, m = 0.9
I1003 03:33:34.638816 31421 solver.cpp:314] Iteration 36200 (5.42882 iter/s, 18.4202s/100 iter), loss = 0.0458
I1003 03:33:34.638905 31421 solver.cpp:336]     Train net output #0: loss = 0.0458001 (* 1 = 0.0458001 loss)
I1003 03:33:34.638911 31421 sgd_solver.cpp:136] Iteration 36200, lr = 0.001, m = 0.9
I1003 03:33:52.466639 31421 solver.cpp:314] Iteration 36300 (5.60937 iter/s, 17.8273s/100 iter), loss = 0.0545806
I1003 03:33:52.466663 31421 solver.cpp:336]     Train net output #0: loss = 0.0545807 (* 1 = 0.0545807 loss)
I1003 03:33:52.466667 31421 sgd_solver.cpp:136] Iteration 36300, lr = 0.001, m = 0.9
I1003 03:34:02.988147 31432 data_reader.cpp:305] Starting prefetch of epoch 30
I1003 03:34:10.461560 31421 solver.cpp:314] Iteration 36400 (5.55728 iter/s, 17.9944s/100 iter), loss = 0.0556791
I1003 03:34:10.461655 31421 solver.cpp:336]     Train net output #0: loss = 0.0556792 (* 1 = 0.0556792 loss)
I1003 03:34:10.461660 31421 sgd_solver.cpp:136] Iteration 36400, lr = 0.001, m = 0.9
I1003 03:34:28.454349 31421 solver.cpp:314] Iteration 36500 (5.55794 iter/s, 17.9923s/100 iter), loss = 0.0896153
I1003 03:34:28.454372 31421 solver.cpp:336]     Train net output #0: loss = 0.0896153 (* 1 = 0.0896153 loss)
I1003 03:34:28.454376 31421 sgd_solver.cpp:136] Iteration 36500, lr = 0.001, m = 0.9
I1003 03:34:46.601778 31421 solver.cpp:314] Iteration 36600 (5.51058 iter/s, 18.1469s/100 iter), loss = 0.0607579
I1003 03:34:46.601830 31421 solver.cpp:336]     Train net output #0: loss = 0.0607579 (* 1 = 0.0607579 loss)
I1003 03:34:46.601837 31421 sgd_solver.cpp:136] Iteration 36600, lr = 0.001, m = 0.9
I1003 03:35:02.976877 31432 data_reader.cpp:305] Starting prefetch of epoch 31
I1003 03:35:05.111343 31421 solver.cpp:314] Iteration 36700 (5.40276 iter/s, 18.509s/100 iter), loss = 0.0634885
I1003 03:35:05.111366 31421 solver.cpp:336]     Train net output #0: loss = 0.0634886 (* 1 = 0.0634886 loss)
I1003 03:35:05.111369 31421 sgd_solver.cpp:136] Iteration 36700, lr = 0.001, m = 0.9
I1003 03:35:23.142765 31421 solver.cpp:314] Iteration 36800 (5.54603 iter/s, 18.0309s/100 iter), loss = 0.0535533
I1003 03:35:23.142814 31421 solver.cpp:336]     Train net output #0: loss = 0.0535534 (* 1 = 0.0535534 loss)
I1003 03:35:23.142822 31421 sgd_solver.cpp:136] Iteration 36800, lr = 0.001, m = 0.9
I1003 03:35:41.202853 31421 solver.cpp:314] Iteration 36900 (5.53723 iter/s, 18.0596s/100 iter), loss = 0.0641594
I1003 03:35:41.202881 31421 solver.cpp:336]     Train net output #0: loss = 0.0641595 (* 1 = 0.0641595 loss)
I1003 03:35:41.202888 31421 sgd_solver.cpp:136] Iteration 36900, lr = 0.001, m = 0.9
I1003 03:35:59.202261 31421 solver.cpp:352] Sparsity after update:
I1003 03:35:59.220264 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:35:59.220281 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:35:59.220290 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:35:59.220294 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:35:59.220297 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:35:59.220300 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:35:59.220304 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:35:59.220306 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:35:59.220309 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:35:59.220314 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:35:59.220316 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:35:59.220319 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:35:59.220324 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:35:59.220326 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:35:59.220330 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:35:59.220333 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:35:59.220337 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:35:59.220341 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:35:59.220345 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:35:59.385589 31421 solver.cpp:314] Iteration 37000 (5.49988 iter/s, 18.1822s/100 iter), loss = 0.061838
I1003 03:35:59.385615 31421 solver.cpp:336]     Train net output #0: loss = 0.061838 (* 1 = 0.061838 loss)
I1003 03:35:59.385622 31421 sgd_solver.cpp:136] Iteration 37000, lr = 0.001, m = 0.9
I1003 03:36:02.784059 31432 data_reader.cpp:305] Starting prefetch of epoch 32
I1003 03:36:17.345230 31421 solver.cpp:314] Iteration 37100 (5.5682 iter/s, 17.9591s/100 iter), loss = 0.0787457
I1003 03:36:17.345259 31421 solver.cpp:336]     Train net output #0: loss = 0.0787458 (* 1 = 0.0787458 loss)
I1003 03:36:17.345266 31421 sgd_solver.cpp:136] Iteration 37100, lr = 0.001, m = 0.9
I1003 03:36:32.378743 31424 data_reader.cpp:305] Starting prefetch of epoch 12
I1003 03:36:35.220945 31421 solver.cpp:314] Iteration 37200 (5.59434 iter/s, 17.8752s/100 iter), loss = 0.053098
I1003 03:36:35.220969 31421 solver.cpp:336]     Train net output #0: loss = 0.053098 (* 1 = 0.053098 loss)
I1003 03:36:35.220975 31421 sgd_solver.cpp:136] Iteration 37200, lr = 0.001, m = 0.9
I1003 03:36:53.262308 31421 solver.cpp:314] Iteration 37300 (5.54297 iter/s, 18.0409s/100 iter), loss = 0.072608
I1003 03:36:53.262332 31421 solver.cpp:336]     Train net output #0: loss = 0.072608 (* 1 = 0.072608 loss)
I1003 03:36:53.262336 31421 sgd_solver.cpp:136] Iteration 37300, lr = 0.001, m = 0.9
I1003 03:37:11.336526 31421 solver.cpp:314] Iteration 37400 (5.5329 iter/s, 18.0737s/100 iter), loss = 0.0889154
I1003 03:37:11.336616 31421 solver.cpp:336]     Train net output #0: loss = 0.0889154 (* 1 = 0.0889154 loss)
I1003 03:37:11.336622 31421 sgd_solver.cpp:136] Iteration 37400, lr = 0.001, m = 0.9
I1003 03:37:29.399000 31421 solver.cpp:314] Iteration 37500 (5.5365 iter/s, 18.062s/100 iter), loss = 0.0484382
I1003 03:37:29.399025 31421 solver.cpp:336]     Train net output #0: loss = 0.0484383 (* 1 = 0.0484383 loss)
I1003 03:37:29.399029 31421 sgd_solver.cpp:136] Iteration 37500, lr = 0.001, m = 0.9
I1003 03:37:32.053397 31344 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 03:37:47.238384 31421 solver.cpp:314] Iteration 37600 (5.60573 iter/s, 17.8389s/100 iter), loss = 0.034862
I1003 03:37:47.238433 31421 solver.cpp:336]     Train net output #0: loss = 0.034862 (* 1 = 0.034862 loss)
I1003 03:37:47.238438 31421 sgd_solver.cpp:136] Iteration 37600, lr = 0.001, m = 0.9
I1003 03:38:05.278326 31421 solver.cpp:314] Iteration 37700 (5.54341 iter/s, 18.0394s/100 iter), loss = 0.0667958
I1003 03:38:05.278348 31421 solver.cpp:336]     Train net output #0: loss = 0.0667958 (* 1 = 0.0667958 loss)
I1003 03:38:05.278353 31421 sgd_solver.cpp:136] Iteration 37700, lr = 0.001, m = 0.9
I1003 03:38:23.335752 31421 solver.cpp:314] Iteration 37800 (5.53804 iter/s, 18.0569s/100 iter), loss = 0.048932
I1003 03:38:23.335846 31421 solver.cpp:336]     Train net output #0: loss = 0.0489321 (* 1 = 0.0489321 loss)
I1003 03:38:23.335855 31421 sgd_solver.cpp:136] Iteration 37800, lr = 0.001, m = 0.9
I1003 03:38:31.603847 31430 data_reader.cpp:305] Starting prefetch of epoch 27
I1003 03:38:42.140702 31421 solver.cpp:314] Iteration 37900 (5.3179 iter/s, 18.8044s/100 iter), loss = 0.0729606
I1003 03:38:42.140728 31421 solver.cpp:336]     Train net output #0: loss = 0.0729606 (* 1 = 0.0729606 loss)
I1003 03:38:42.140733 31421 sgd_solver.cpp:136] Iteration 37900, lr = 0.001, m = 0.9
I1003 03:39:00.670786 31421 solver.cpp:352] Sparsity after update:
I1003 03:39:00.674731 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:39:00.674743 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:39:00.674753 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:39:00.674758 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:39:00.674762 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:39:00.674767 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:39:00.674769 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:39:00.674773 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:39:00.674777 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:39:00.674780 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:39:00.674784 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:39:00.674788 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:39:00.674800 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:39:00.674808 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:39:00.674813 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:39:00.674820 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:39:00.674825 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:39:00.674829 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:39:00.674834 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:39:00.674847 31421 solver.cpp:567] Iteration 38000, Testing net (#0)
I1003 03:39:21.660717 31405 data_reader.cpp:305] Starting prefetch of epoch 3
I1003 03:39:22.178335 31423 blocking_queue.cpp:40] Data layer prefetch queue empty
I1003 03:39:32.305065 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.95615
I1003 03:39:32.305323 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:39:32.305333 31421 solver.cpp:659]     Test net output #2: loss = 0.149245 (* 1 = 0.149245 loss)
I1003 03:39:32.305371 31421 solver.cpp:265] [MultiGPU] Tests completed in 31.6296s
I1003 03:39:32.502677 31421 solver.cpp:314] Iteration 38000 (1.98568 iter/s, 50.3606s/100 iter), loss = 0.0723429
I1003 03:39:32.502709 31421 solver.cpp:336]     Train net output #0: loss = 0.0723429 (* 1 = 0.0723429 loss)
I1003 03:39:32.502714 31421 sgd_solver.cpp:136] Iteration 38000, lr = 0.001, m = 0.9
I1003 03:39:51.167387 31421 solver.cpp:314] Iteration 38100 (5.35786 iter/s, 18.6642s/100 iter), loss = 0.0766822
I1003 03:39:51.167409 31421 solver.cpp:336]     Train net output #0: loss = 0.0766822 (* 1 = 0.0766822 loss)
I1003 03:39:51.167414 31421 sgd_solver.cpp:136] Iteration 38100, lr = 0.001, m = 0.9
I1003 03:40:04.756497 31342 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 03:40:09.014452 31421 solver.cpp:314] Iteration 38200 (5.60332 iter/s, 17.8466s/100 iter), loss = 0.080982
I1003 03:40:09.014475 31421 solver.cpp:336]     Train net output #0: loss = 0.0809821 (* 1 = 0.0809821 loss)
I1003 03:40:09.014480 31421 sgd_solver.cpp:136] Iteration 38200, lr = 0.001, m = 0.9
I1003 03:40:27.061661 31421 solver.cpp:314] Iteration 38300 (5.54118 iter/s, 18.0467s/100 iter), loss = 0.0631165
I1003 03:40:27.061681 31421 solver.cpp:336]     Train net output #0: loss = 0.0631165 (* 1 = 0.0631165 loss)
I1003 03:40:27.061686 31421 sgd_solver.cpp:136] Iteration 38300, lr = 0.001, m = 0.9
I1003 03:40:44.953260 31421 solver.cpp:314] Iteration 38400 (5.58937 iter/s, 17.8911s/100 iter), loss = 0.0585887
I1003 03:40:44.953366 31421 solver.cpp:336]     Train net output #0: loss = 0.0585888 (* 1 = 0.0585888 loss)
I1003 03:40:44.953373 31421 sgd_solver.cpp:136] Iteration 38400, lr = 0.001, m = 0.9
I1003 03:41:02.956768 31421 solver.cpp:314] Iteration 38500 (5.55463 iter/s, 18.003s/100 iter), loss = 0.0606758
I1003 03:41:02.956791 31421 solver.cpp:336]     Train net output #0: loss = 0.0606758 (* 1 = 0.0606758 loss)
I1003 03:41:02.956797 31421 sgd_solver.cpp:136] Iteration 38500, lr = 0.001, m = 0.9
I1003 03:41:04.068331 31432 data_reader.cpp:305] Starting prefetch of epoch 33
I1003 03:41:21.523468 31421 solver.cpp:314] Iteration 38600 (5.38614 iter/s, 18.5662s/100 iter), loss = 0.0575865
I1003 03:41:21.523520 31421 solver.cpp:336]     Train net output #0: loss = 0.0575866 (* 1 = 0.0575866 loss)
I1003 03:41:21.523525 31421 sgd_solver.cpp:136] Iteration 38600, lr = 0.001, m = 0.9
I1003 03:41:34.522467 31424 data_reader.cpp:305] Starting prefetch of epoch 13
I1003 03:41:39.706187 31421 solver.cpp:314] Iteration 38700 (5.49988 iter/s, 18.1822s/100 iter), loss = 0.079792
I1003 03:41:39.706208 31421 solver.cpp:336]     Train net output #0: loss = 0.079792 (* 1 = 0.079792 loss)
I1003 03:41:39.706212 31421 sgd_solver.cpp:136] Iteration 38700, lr = 0.001, m = 0.9
I1003 03:41:57.816232 31421 solver.cpp:314] Iteration 38800 (5.52195 iter/s, 18.1095s/100 iter), loss = 0.0671362
I1003 03:41:57.816293 31421 solver.cpp:336]     Train net output #0: loss = 0.0671363 (* 1 = 0.0671363 loss)
I1003 03:41:57.816298 31421 sgd_solver.cpp:136] Iteration 38800, lr = 0.001, m = 0.9
I1003 03:42:15.832568 31421 solver.cpp:314] Iteration 38900 (5.55067 iter/s, 18.0158s/100 iter), loss = 0.139231
I1003 03:42:15.832594 31421 solver.cpp:336]     Train net output #0: loss = 0.139231 (* 1 = 0.139231 loss)
I1003 03:42:15.832599 31421 sgd_solver.cpp:136] Iteration 38900, lr = 0.001, m = 0.9
I1003 03:42:33.717669 31421 solver.cpp:352] Sparsity after update:
I1003 03:42:33.724305 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:42:33.724314 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:42:33.724323 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:42:33.724326 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:42:33.724329 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:42:33.724333 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:42:33.724336 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:42:33.724339 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:42:33.724342 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:42:33.724345 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:42:33.724349 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:42:33.724352 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:42:33.724357 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:42:33.724361 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:42:33.724364 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:42:33.724369 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:42:33.724371 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:42:33.724375 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:42:33.724380 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:42:33.896211 31421 solver.cpp:314] Iteration 39000 (5.53614 iter/s, 18.0631s/100 iter), loss = 0.0591847
I1003 03:42:33.896236 31421 solver.cpp:336]     Train net output #0: loss = 0.0591847 (* 1 = 0.0591847 loss)
I1003 03:42:33.896242 31421 sgd_solver.cpp:136] Iteration 39000, lr = 0.001, m = 0.9
I1003 03:42:34.290513 31425 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 03:42:52.034127 31421 solver.cpp:314] Iteration 39100 (5.51347 iter/s, 18.1374s/100 iter), loss = 0.032017
I1003 03:42:52.034152 31421 solver.cpp:336]     Train net output #0: loss = 0.032017 (* 1 = 0.032017 loss)
I1003 03:42:52.034159 31421 sgd_solver.cpp:136] Iteration 39100, lr = 0.001, m = 0.9
I1003 03:43:09.870664 31421 solver.cpp:314] Iteration 39200 (5.60663 iter/s, 17.836s/100 iter), loss = 0.0865268
I1003 03:43:09.870723 31421 solver.cpp:336]     Train net output #0: loss = 0.0865269 (* 1 = 0.0865269 loss)
I1003 03:43:09.870729 31421 sgd_solver.cpp:136] Iteration 39200, lr = 0.001, m = 0.9
I1003 03:43:27.931807 31421 solver.cpp:314] Iteration 39300 (5.5369 iter/s, 18.0606s/100 iter), loss = 0.0566416
I1003 03:43:27.931833 31421 solver.cpp:336]     Train net output #0: loss = 0.0566416 (* 1 = 0.0566416 loss)
I1003 03:43:27.931839 31421 sgd_solver.cpp:136] Iteration 39300, lr = 0.001, m = 0.9
I1003 03:43:33.897807 31424 data_reader.cpp:305] Starting prefetch of epoch 14
I1003 03:43:45.976733 31421 solver.cpp:314] Iteration 39400 (5.54188 iter/s, 18.0444s/100 iter), loss = 0.0515938
I1003 03:43:45.976799 31421 solver.cpp:336]     Train net output #0: loss = 0.0515939 (* 1 = 0.0515939 loss)
I1003 03:43:45.976806 31421 sgd_solver.cpp:136] Iteration 39400, lr = 0.001, m = 0.9
I1003 03:44:03.857359 31342 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 03:44:04.182060 31421 solver.cpp:314] Iteration 39500 (5.49305 iter/s, 18.2048s/100 iter), loss = 0.0555017
I1003 03:44:04.182082 31421 solver.cpp:336]     Train net output #0: loss = 0.0555018 (* 1 = 0.0555018 loss)
I1003 03:44:04.182087 31421 sgd_solver.cpp:136] Iteration 39500, lr = 0.001, m = 0.9
I1003 03:44:22.208791 31421 solver.cpp:314] Iteration 39600 (5.54747 iter/s, 18.0262s/100 iter), loss = 0.0612543
I1003 03:44:22.208876 31421 solver.cpp:336]     Train net output #0: loss = 0.0612543 (* 1 = 0.0612543 loss)
I1003 03:44:22.208884 31421 sgd_solver.cpp:136] Iteration 39600, lr = 0.001, m = 0.9
I1003 03:44:40.243914 31421 solver.cpp:314] Iteration 39700 (5.54489 iter/s, 18.0346s/100 iter), loss = 0.0676348
I1003 03:44:40.243939 31421 solver.cpp:336]     Train net output #0: loss = 0.0676348 (* 1 = 0.0676348 loss)
I1003 03:44:40.243943 31421 sgd_solver.cpp:136] Iteration 39700, lr = 0.001, m = 0.9
I1003 03:44:58.487586 31421 solver.cpp:314] Iteration 39800 (5.48151 iter/s, 18.2432s/100 iter), loss = 0.0547275
I1003 03:44:58.487673 31421 solver.cpp:336]     Train net output #0: loss = 0.0547276 (* 1 = 0.0547276 loss)
I1003 03:44:58.487679 31421 sgd_solver.cpp:136] Iteration 39800, lr = 0.001, m = 0.9
I1003 03:45:03.676836 31425 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 03:45:16.903427 31421 solver.cpp:314] Iteration 39900 (5.43026 iter/s, 18.4153s/100 iter), loss = 0.0672185
I1003 03:45:16.903473 31421 solver.cpp:336]     Train net output #0: loss = 0.0672185 (* 1 = 0.0672185 loss)
I1003 03:45:16.903487 31421 sgd_solver.cpp:136] Iteration 39900, lr = 0.001, m = 0.9
I1003 03:45:35.356264 31421 solver.cpp:352] Sparsity after update:
I1003 03:45:35.362712 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:45:35.362725 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:45:35.362735 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:45:35.362740 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:45:35.362743 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:45:35.362748 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:45:35.362753 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:45:35.362757 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:45:35.362761 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:45:35.362766 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:45:35.362771 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:45:35.362774 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:45:35.362778 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:45:35.362783 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:45:35.362788 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:45:35.362792 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:45:35.362797 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:45:35.362802 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:45:35.362807 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:45:35.362819 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.caffemodel
I1003 03:45:36.077518 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.solverstate
I1003 03:45:36.097224 31421 solver.cpp:567] Iteration 40000, Testing net (#0)
I1003 03:45:43.904400 31538 data_reader.cpp:305] Starting prefetch of epoch 5
I1003 03:45:43.904400 31418 data_reader.cpp:305] Starting prefetch of epoch 7
I1003 03:46:04.846693 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954174
I1003 03:46:04.846721 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:46:04.846730 31421 solver.cpp:659]     Test net output #2: loss = 0.139666 (* 1 = 0.139666 loss)
I1003 03:46:04.846762 31421 solver.cpp:265] [MultiGPU] Tests completed in 28.7487s
I1003 03:46:05.037513 31421 solver.cpp:314] Iteration 40000 (2.07759 iter/s, 48.1328s/100 iter), loss = 0.0758151
I1003 03:46:05.037537 31421 solver.cpp:336]     Train net output #0: loss = 0.0758151 (* 1 = 0.0758151 loss)
I1003 03:46:05.037541 31421 sgd_solver.cpp:136] Iteration 40000, lr = 0.001, m = 0.9
I1003 03:46:23.367388 31421 solver.cpp:314] Iteration 40100 (5.45573 iter/s, 18.3294s/100 iter), loss = 0.0571211
I1003 03:46:23.367447 31421 solver.cpp:336]     Train net output #0: loss = 0.0571211 (* 1 = 0.0571211 loss)
I1003 03:46:23.367455 31421 sgd_solver.cpp:136] Iteration 40100, lr = 0.001, m = 0.9
I1003 03:46:34.141249 31342 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 03:46:41.728402 31421 solver.cpp:314] Iteration 40200 (5.44648 iter/s, 18.3605s/100 iter), loss = 0.042824
I1003 03:46:41.728451 31421 solver.cpp:336]     Train net output #0: loss = 0.0428241 (* 1 = 0.0428241 loss)
I1003 03:46:41.728463 31421 sgd_solver.cpp:136] Iteration 40200, lr = 0.001, m = 0.9
I1003 03:47:00.327252 31421 solver.cpp:314] Iteration 40300 (5.37683 iter/s, 18.5983s/100 iter), loss = 0.0710723
I1003 03:47:00.327332 31421 solver.cpp:336]     Train net output #0: loss = 0.0710724 (* 1 = 0.0710724 loss)
I1003 03:47:00.327338 31421 sgd_solver.cpp:136] Iteration 40300, lr = 0.001, m = 0.9
I1003 03:47:18.646715 31421 solver.cpp:314] Iteration 40400 (5.45883 iter/s, 18.3189s/100 iter), loss = 0.0753401
I1003 03:47:18.646739 31421 solver.cpp:336]     Train net output #0: loss = 0.0753402 (* 1 = 0.0753402 loss)
I1003 03:47:18.646744 31421 sgd_solver.cpp:136] Iteration 40400, lr = 0.001, m = 0.9
I1003 03:47:35.231549 31430 data_reader.cpp:305] Starting prefetch of epoch 28
I1003 03:47:37.046022 31421 solver.cpp:314] Iteration 40500 (5.43514 iter/s, 18.3988s/100 iter), loss = 0.0691486
I1003 03:47:37.046052 31421 solver.cpp:336]     Train net output #0: loss = 0.0691487 (* 1 = 0.0691487 loss)
I1003 03:47:37.046059 31421 sgd_solver.cpp:136] Iteration 40500, lr = 0.001, m = 0.9
I1003 03:47:55.429437 31421 solver.cpp:314] Iteration 40600 (5.43984 iter/s, 18.3829s/100 iter), loss = 0.0687619
I1003 03:47:55.429466 31421 solver.cpp:336]     Train net output #0: loss = 0.0687619 (* 1 = 0.0687619 loss)
I1003 03:47:55.429476 31421 sgd_solver.cpp:136] Iteration 40600, lr = 0.001, m = 0.9
I1003 03:48:05.489001 31425 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 03:48:13.661402 31421 solver.cpp:314] Iteration 40700 (5.48503 iter/s, 18.2315s/100 iter), loss = 0.0670742
I1003 03:48:13.661427 31421 solver.cpp:336]     Train net output #0: loss = 0.0670743 (* 1 = 0.0670743 loss)
I1003 03:48:13.661430 31421 sgd_solver.cpp:136] Iteration 40700, lr = 0.001, m = 0.9
I1003 03:48:32.055313 31421 solver.cpp:314] Iteration 40800 (5.43674 iter/s, 18.3934s/100 iter), loss = 0.106305
I1003 03:48:32.055353 31421 solver.cpp:336]     Train net output #0: loss = 0.106305 (* 1 = 0.106305 loss)
I1003 03:48:32.055362 31421 sgd_solver.cpp:136] Iteration 40800, lr = 0.001, m = 0.9
I1003 03:48:50.554989 31421 solver.cpp:314] Iteration 40900 (5.40565 iter/s, 18.4992s/100 iter), loss = 0.0589703
I1003 03:48:50.555042 31421 solver.cpp:336]     Train net output #0: loss = 0.0589704 (* 1 = 0.0589704 loss)
I1003 03:48:50.555047 31421 sgd_solver.cpp:136] Iteration 40900, lr = 0.001, m = 0.9
I1003 03:49:06.218158 31425 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 03:49:08.777549 31421 solver.cpp:352] Sparsity after update:
I1003 03:49:08.788529 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:49:08.788542 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:49:08.788550 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:49:08.788554 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:49:08.788558 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:49:08.788560 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:49:08.788564 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:49:08.788568 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:49:08.788570 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:49:08.788573 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:49:08.788578 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:49:08.788580 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:49:08.788583 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:49:08.788589 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:49:08.788594 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:49:08.788599 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:49:08.788601 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:49:08.788604 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:49:08.788609 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:49:08.961484 31421 solver.cpp:314] Iteration 41000 (5.43302 iter/s, 18.406s/100 iter), loss = 0.0700809
I1003 03:49:08.961504 31421 solver.cpp:336]     Train net output #0: loss = 0.0700809 (* 1 = 0.0700809 loss)
I1003 03:49:08.961508 31421 sgd_solver.cpp:136] Iteration 41000, lr = 0.001, m = 0.9
I1003 03:49:27.058707 31421 solver.cpp:314] Iteration 41100 (5.52586 iter/s, 18.0967s/100 iter), loss = 0.0502586
I1003 03:49:27.059100 31421 solver.cpp:336]     Train net output #0: loss = 0.0502587 (* 1 = 0.0502587 loss)
I1003 03:49:27.059109 31421 sgd_solver.cpp:136] Iteration 41100, lr = 0.001, m = 0.9
I1003 03:49:45.164242 31421 solver.cpp:314] Iteration 41200 (5.52333 iter/s, 18.105s/100 iter), loss = 0.0537312
I1003 03:49:45.164265 31421 solver.cpp:336]     Train net output #0: loss = 0.0537313 (* 1 = 0.0537313 loss)
I1003 03:49:45.164269 31421 sgd_solver.cpp:136] Iteration 41200, lr = 0.001, m = 0.9
I1003 03:50:03.133107 31421 solver.cpp:314] Iteration 41300 (5.56534 iter/s, 17.9684s/100 iter), loss = 0.0303365
I1003 03:50:03.133157 31421 solver.cpp:336]     Train net output #0: loss = 0.0303366 (* 1 = 0.0303366 loss)
I1003 03:50:03.133162 31421 sgd_solver.cpp:136] Iteration 41300, lr = 0.001, m = 0.9
I1003 03:50:06.027951 31344 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 03:50:21.299471 31421 solver.cpp:314] Iteration 41400 (5.50483 iter/s, 18.1659s/100 iter), loss = 0.106231
I1003 03:50:21.299496 31421 solver.cpp:336]     Train net output #0: loss = 0.106231 (* 1 = 0.106231 loss)
I1003 03:50:21.299500 31421 sgd_solver.cpp:136] Iteration 41400, lr = 0.001, m = 0.9
I1003 03:50:35.921216 31425 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 03:50:39.301919 31421 solver.cpp:314] Iteration 41500 (5.55495 iter/s, 18.0019s/100 iter), loss = 0.075599
I1003 03:50:39.301944 31421 solver.cpp:336]     Train net output #0: loss = 0.0755991 (* 1 = 0.0755991 loss)
I1003 03:50:39.301949 31421 sgd_solver.cpp:136] Iteration 41500, lr = 0.001, m = 0.9
I1003 03:50:57.296234 31421 solver.cpp:314] Iteration 41600 (5.55747 iter/s, 17.9938s/100 iter), loss = 0.0455245
I1003 03:50:57.296257 31421 solver.cpp:336]     Train net output #0: loss = 0.0455246 (* 1 = 0.0455246 loss)
I1003 03:50:57.296260 31421 sgd_solver.cpp:136] Iteration 41600, lr = 0.001, m = 0.9
I1003 03:51:15.268803 31421 solver.cpp:314] Iteration 41700 (5.56419 iter/s, 17.9721s/100 iter), loss = 0.099684
I1003 03:51:15.268856 31421 solver.cpp:336]     Train net output #0: loss = 0.099684 (* 1 = 0.099684 loss)
I1003 03:51:15.268860 31421 sgd_solver.cpp:136] Iteration 41700, lr = 0.001, m = 0.9
I1003 03:51:33.335095 31421 solver.cpp:314] Iteration 41800 (5.53533 iter/s, 18.0658s/100 iter), loss = 0.0574632
I1003 03:51:33.335119 31421 solver.cpp:336]     Train net output #0: loss = 0.0574632 (* 1 = 0.0574632 loss)
I1003 03:51:33.335124 31421 sgd_solver.cpp:136] Iteration 41800, lr = 0.001, m = 0.9
I1003 03:51:35.534510 31342 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 03:51:51.458510 31421 solver.cpp:314] Iteration 41900 (5.51788 iter/s, 18.1229s/100 iter), loss = 0.0764297
I1003 03:51:51.458588 31421 solver.cpp:336]     Train net output #0: loss = 0.0764297 (* 1 = 0.0764297 loss)
I1003 03:51:51.458593 31421 sgd_solver.cpp:136] Iteration 41900, lr = 0.001, m = 0.9
I1003 03:52:09.257302 31421 solver.cpp:352] Sparsity after update:
I1003 03:52:09.262526 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:52:09.262534 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:52:09.262540 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:52:09.262542 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:52:09.262544 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:52:09.262547 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:52:09.262548 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:52:09.262550 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:52:09.262552 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:52:09.262554 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:52:09.262557 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:52:09.262557 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:52:09.262559 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:52:09.262562 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:52:09.262563 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:52:09.262565 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:52:09.262567 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:52:09.262569 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:52:09.262572 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:52:09.262580 31421 solver.cpp:567] Iteration 42000, Testing net (#0)
I1003 03:52:31.991456 31542 data_reader.cpp:305] Starting prefetch of epoch 1
I1003 03:52:43.653012 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.956329
I1003 03:52:43.653039 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 03:52:43.653045 31421 solver.cpp:659]     Test net output #2: loss = 0.149905 (* 1 = 0.149905 loss)
I1003 03:52:43.653074 31421 solver.cpp:265] [MultiGPU] Tests completed in 34.3895s
I1003 03:52:43.839648 31421 solver.cpp:314] Iteration 42000 (1.90914 iter/s, 52.3797s/100 iter), loss = 0.0531978
I1003 03:52:43.839675 31421 solver.cpp:336]     Train net output #0: loss = 0.0531978 (* 1 = 0.0531978 loss)
I1003 03:52:43.839680 31421 sgd_solver.cpp:136] Iteration 42000, lr = 0.001, m = 0.9
I1003 03:53:01.601133 31421 solver.cpp:314] Iteration 42100 (5.63032 iter/s, 17.761s/100 iter), loss = 0.0462639
I1003 03:53:01.601157 31421 solver.cpp:336]     Train net output #0: loss = 0.046264 (* 1 = 0.046264 loss)
I1003 03:53:01.601162 31421 sgd_solver.cpp:136] Iteration 42100, lr = 0.001, m = 0.9
I1003 03:53:20.087664 31421 solver.cpp:314] Iteration 42200 (5.4095 iter/s, 18.486s/100 iter), loss = 0.0512613
I1003 03:53:20.087715 31421 solver.cpp:336]     Train net output #0: loss = 0.0512614 (* 1 = 0.0512614 loss)
I1003 03:53:20.087720 31421 sgd_solver.cpp:136] Iteration 42200, lr = 0.001, m = 0.9
I1003 03:54:24.896526 31421 solver.cpp:314] Iteration 42300 (1.54304 iter/s, 64.807s/100 iter), loss = 0.068456
I1003 03:54:24.896721 31421 solver.cpp:336]     Train net output #0: loss = 0.068456 (* 1 = 0.068456 loss)
I1003 03:54:24.896728 31421 sgd_solver.cpp:136] Iteration 42300, lr = 0.001, m = 0.9
I1003 03:54:27.896034 31432 data_reader.cpp:305] Starting prefetch of epoch 34
I1003 03:55:08.555575 31421 solver.cpp:314] Iteration 42400 (2.29054 iter/s, 43.6578s/100 iter), loss = 0.0579366
I1003 03:55:08.555655 31421 solver.cpp:336]     Train net output #0: loss = 0.0579366 (* 1 = 0.0579366 loss)
I1003 03:55:08.555661 31421 sgd_solver.cpp:136] Iteration 42400, lr = 0.001, m = 0.9
I1003 03:55:35.378167 31424 data_reader.cpp:305] Starting prefetch of epoch 15
I1003 03:55:40.262328 31421 solver.cpp:314] Iteration 42500 (3.15399 iter/s, 31.7058s/100 iter), loss = 0.0507374
I1003 03:55:40.262403 31421 solver.cpp:336]     Train net output #0: loss = 0.0507375 (* 1 = 0.0507375 loss)
I1003 03:55:40.262408 31421 sgd_solver.cpp:136] Iteration 42500, lr = 0.001, m = 0.9
I1003 03:55:58.742406 31421 solver.cpp:314] Iteration 42600 (5.41139 iter/s, 18.4795s/100 iter), loss = 0.081387
I1003 03:55:58.742436 31421 solver.cpp:336]     Train net output #0: loss = 0.0813871 (* 1 = 0.0813871 loss)
I1003 03:55:58.742442 31421 sgd_solver.cpp:136] Iteration 42600, lr = 0.001, m = 0.9
I1003 03:56:16.925611 31421 solver.cpp:314] Iteration 42700 (5.49974 iter/s, 18.1827s/100 iter), loss = 0.0500748
I1003 03:56:16.925721 31421 solver.cpp:336]     Train net output #0: loss = 0.0500748 (* 1 = 0.0500748 loss)
I1003 03:56:16.925729 31421 sgd_solver.cpp:136] Iteration 42700, lr = 0.001, m = 0.9
I1003 03:56:35.378549 31421 solver.cpp:314] Iteration 42800 (5.41935 iter/s, 18.4524s/100 iter), loss = 0.0531251
I1003 03:56:35.378646 31421 solver.cpp:336]     Train net output #0: loss = 0.0531252 (* 1 = 0.0531252 loss)
I1003 03:56:35.378705 31421 sgd_solver.cpp:136] Iteration 42800, lr = 0.001, m = 0.9
I1003 03:56:36.035246 31425 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 03:56:54.145237 31421 solver.cpp:314] Iteration 42900 (5.32874 iter/s, 18.7662s/100 iter), loss = 0.0489523
I1003 03:56:54.145319 31421 solver.cpp:336]     Train net output #0: loss = 0.0489524 (* 1 = 0.0489524 loss)
I1003 03:56:54.145328 31421 sgd_solver.cpp:136] Iteration 42900, lr = 0.001, m = 0.9
I1003 03:57:12.721307 31421 solver.cpp:352] Sparsity after update:
I1003 03:57:12.736901 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 03:57:12.736913 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 03:57:12.736920 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 03:57:12.736923 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 03:57:12.736925 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 03:57:12.736927 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 03:57:12.736929 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 03:57:12.736930 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 03:57:12.736932 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 03:57:12.736934 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 03:57:12.736937 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 03:57:12.736938 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 03:57:12.736940 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 03:57:12.736943 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 03:57:12.736944 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 03:57:12.736946 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 03:57:12.736949 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 03:57:12.736953 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 03:57:12.736956 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 03:57:12.905900 31421 solver.cpp:314] Iteration 43000 (5.33045 iter/s, 18.7601s/100 iter), loss = 0.0450752
I1003 03:57:12.905921 31421 solver.cpp:336]     Train net output #0: loss = 0.0450752 (* 1 = 0.0450752 loss)
I1003 03:57:12.905925 31421 sgd_solver.cpp:136] Iteration 43000, lr = 0.001, m = 0.9
I1003 03:57:31.544574 31421 solver.cpp:314] Iteration 43100 (5.36534 iter/s, 18.6381s/100 iter), loss = 0.0814828
I1003 03:57:31.544663 31421 solver.cpp:336]     Train net output #0: loss = 0.0814828 (* 1 = 0.0814828 loss)
I1003 03:57:31.544682 31421 sgd_solver.cpp:136] Iteration 43100, lr = 0.001, m = 0.9
I1003 03:57:37.777894 31425 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 03:57:50.060892 31421 solver.cpp:314] Iteration 43200 (5.40079 iter/s, 18.5158s/100 iter), loss = 0.080112
I1003 03:57:50.060916 31421 solver.cpp:336]     Train net output #0: loss = 0.080112 (* 1 = 0.080112 loss)
I1003 03:57:50.060923 31421 sgd_solver.cpp:136] Iteration 43200, lr = 0.001, m = 0.9
I1003 03:58:08.728299 31421 solver.cpp:314] Iteration 43300 (5.35708 iter/s, 18.6669s/100 iter), loss = 0.0484095
I1003 03:58:08.728364 31421 solver.cpp:336]     Train net output #0: loss = 0.0484095 (* 1 = 0.0484095 loss)
I1003 03:58:08.728371 31421 sgd_solver.cpp:136] Iteration 43300, lr = 0.001, m = 0.9
I1003 03:58:27.393322 31421 solver.cpp:314] Iteration 43400 (5.35777 iter/s, 18.6645s/100 iter), loss = 0.0802072
I1003 03:58:27.393347 31421 solver.cpp:336]     Train net output #0: loss = 0.0802072 (* 1 = 0.0802072 loss)
I1003 03:58:27.393352 31421 sgd_solver.cpp:136] Iteration 43400, lr = 0.001, m = 0.9
I1003 03:58:39.584098 31344 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 03:58:45.968786 31421 solver.cpp:314] Iteration 43500 (5.3836 iter/s, 18.5749s/100 iter), loss = 0.066752
I1003 03:58:45.968811 31421 solver.cpp:336]     Train net output #0: loss = 0.066752 (* 1 = 0.066752 loss)
I1003 03:58:45.968816 31421 sgd_solver.cpp:136] Iteration 43500, lr = 0.001, m = 0.9
I1003 03:59:04.568954 31421 solver.cpp:314] Iteration 43600 (5.37645 iter/s, 18.5996s/100 iter), loss = 0.0575238
I1003 03:59:04.569241 31421 solver.cpp:336]     Train net output #0: loss = 0.0575238 (* 1 = 0.0575238 loss)
I1003 03:59:04.569350 31421 sgd_solver.cpp:136] Iteration 43600, lr = 0.001, m = 0.9
I1003 03:59:10.196583 31425 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 03:59:23.236346 31421 solver.cpp:314] Iteration 43700 (5.35709 iter/s, 18.6669s/100 iter), loss = 0.0933479
I1003 03:59:23.236374 31421 solver.cpp:336]     Train net output #0: loss = 0.0933479 (* 1 = 0.0933479 loss)
I1003 03:59:23.236382 31421 sgd_solver.cpp:136] Iteration 43700, lr = 0.001, m = 0.9
I1003 03:59:41.879257 31421 solver.cpp:314] Iteration 43800 (5.36412 iter/s, 18.6424s/100 iter), loss = 0.0371611
I1003 03:59:41.879313 31421 solver.cpp:336]     Train net output #0: loss = 0.0371612 (* 1 = 0.0371612 loss)
I1003 03:59:41.879318 31421 sgd_solver.cpp:136] Iteration 43800, lr = 0.001, m = 0.9
I1003 04:00:00.085216 31421 solver.cpp:314] Iteration 43900 (5.49286 iter/s, 18.2055s/100 iter), loss = 0.0433463
I1003 04:00:00.085238 31421 solver.cpp:336]     Train net output #0: loss = 0.0433463 (* 1 = 0.0433463 loss)
I1003 04:00:00.085242 31421 sgd_solver.cpp:136] Iteration 43900, lr = 0.001, m = 0.9
I1003 04:00:11.292980 31425 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 04:00:18.442839 31421 solver.cpp:352] Sparsity after update:
I1003 04:00:18.450037 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:00:18.450047 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:00:18.450052 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:00:18.450054 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:00:18.450057 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:00:18.450058 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:00:18.450060 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:00:18.450062 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:00:18.450064 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:00:18.450067 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:00:18.450068 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:00:18.450070 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:00:18.450073 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:00:18.450074 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:00:18.450076 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:00:18.450078 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:00:18.450080 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:00:18.450081 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:00:18.450083 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:00:18.450093 31421 solver.cpp:567] Iteration 44000, Testing net (#0)
I1003 04:00:50.345975 31538 data_reader.cpp:305] Starting prefetch of epoch 6
I1003 04:00:51.119060 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.953912
I1003 04:00:51.119086 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:00:51.119091 31421 solver.cpp:659]     Test net output #2: loss = 0.141729 (* 1 = 0.141729 loss)
I1003 04:00:51.119119 31421 solver.cpp:265] [MultiGPU] Tests completed in 32.6681s
I1003 04:00:51.310401 31421 solver.cpp:314] Iteration 44000 (1.95222 iter/s, 51.2237s/100 iter), loss = 0.0826923
I1003 04:00:51.310606 31421 solver.cpp:336]     Train net output #0: loss = 0.0826924 (* 1 = 0.0826924 loss)
I1003 04:00:51.310673 31421 sgd_solver.cpp:136] Iteration 44000, lr = 0.001, m = 0.9
I1003 04:01:09.916193 31421 solver.cpp:314] Iteration 44100 (5.37481 iter/s, 18.6053s/100 iter), loss = 0.0619027
I1003 04:01:09.916440 31421 solver.cpp:336]     Train net output #0: loss = 0.0619027 (* 1 = 0.0619027 loss)
I1003 04:01:09.916672 31421 sgd_solver.cpp:136] Iteration 44100, lr = 0.001, m = 0.9
I1003 04:01:28.316462 31421 solver.cpp:314] Iteration 44200 (5.43485 iter/s, 18.3998s/100 iter), loss = 0.0638922
I1003 04:01:28.316510 31421 solver.cpp:336]     Train net output #0: loss = 0.0638923 (* 1 = 0.0638923 loss)
I1003 04:01:28.316516 31421 sgd_solver.cpp:136] Iteration 44200, lr = 0.001, m = 0.9
I1003 04:01:44.717072 31344 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 04:01:46.228152 31421 solver.cpp:314] Iteration 44300 (5.58311 iter/s, 17.9112s/100 iter), loss = 0.0456991
I1003 04:01:46.228185 31421 solver.cpp:336]     Train net output #0: loss = 0.0456991 (* 1 = 0.0456991 loss)
I1003 04:01:46.228190 31421 sgd_solver.cpp:136] Iteration 44300, lr = 0.001, m = 0.9
I1003 04:02:05.395690 31421 solver.cpp:314] Iteration 44400 (5.2173 iter/s, 19.167s/100 iter), loss = 0.0526884
I1003 04:02:05.395768 31421 solver.cpp:336]     Train net output #0: loss = 0.0526884 (* 1 = 0.0526884 loss)
I1003 04:02:05.395774 31421 sgd_solver.cpp:136] Iteration 44400, lr = 0.001, m = 0.9
I1003 04:02:37.541280 31421 solver.cpp:314] Iteration 44500 (3.11093 iter/s, 32.1447s/100 iter), loss = 0.0614563
I1003 04:02:37.548290 31421 solver.cpp:336]     Train net output #0: loss = 0.0614563 (* 1 = 0.0614563 loss)
I1003 04:02:37.548321 31421 sgd_solver.cpp:136] Iteration 44500, lr = 0.001, m = 0.9
I1003 04:02:55.292954 31421 solver.cpp:314] Iteration 44600 (5.63343 iter/s, 17.7512s/100 iter), loss = 0.0593725
I1003 04:02:55.292980 31421 solver.cpp:336]     Train net output #0: loss = 0.0593725 (* 1 = 0.0593725 loss)
I1003 04:02:55.292984 31421 sgd_solver.cpp:136] Iteration 44600, lr = 0.001, m = 0.9
I1003 04:02:59.089702 31432 data_reader.cpp:305] Starting prefetch of epoch 35
I1003 04:03:13.337585 31421 solver.cpp:314] Iteration 44700 (5.54197 iter/s, 18.0441s/100 iter), loss = 0.0587179
I1003 04:03:13.337637 31421 solver.cpp:336]     Train net output #0: loss = 0.0587179 (* 1 = 0.0587179 loss)
I1003 04:03:13.337641 31421 sgd_solver.cpp:136] Iteration 44700, lr = 0.001, m = 0.9
I1003 04:03:31.402077 31421 solver.cpp:314] Iteration 44800 (5.53588 iter/s, 18.064s/100 iter), loss = 0.0805681
I1003 04:03:31.402102 31421 solver.cpp:336]     Train net output #0: loss = 0.0805682 (* 1 = 0.0805682 loss)
I1003 04:03:31.402107 31421 sgd_solver.cpp:136] Iteration 44800, lr = 0.001, m = 0.9
I1003 04:03:49.401746 31421 solver.cpp:314] Iteration 44900 (5.55581 iter/s, 17.9992s/100 iter), loss = 0.0487053
I1003 04:03:49.401796 31421 solver.cpp:336]     Train net output #0: loss = 0.0487053 (* 1 = 0.0487053 loss)
I1003 04:03:49.401803 31421 sgd_solver.cpp:136] Iteration 44900, lr = 0.001, m = 0.9
I1003 04:03:58.713557 31432 data_reader.cpp:305] Starting prefetch of epoch 36
I1003 04:03:58.713557 31430 data_reader.cpp:305] Starting prefetch of epoch 29
I1003 04:04:07.280414 31421 solver.cpp:352] Sparsity after update:
I1003 04:04:07.296353 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:04:07.296372 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:04:07.296381 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:04:07.296385 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:04:07.296388 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:04:07.296391 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:04:07.296394 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:04:07.296398 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:04:07.296401 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:04:07.296404 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:04:07.296408 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:04:07.296411 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:04:07.296414 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:04:07.296417 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:04:07.296422 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:04:07.296424 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:04:07.296429 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:04:07.296433 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:04:07.296437 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:04:07.368016 31572 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I1003 04:04:07.368016 31574 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I1003 04:04:07.368016 31573 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I1003 04:04:07.469054 31421 solver.cpp:314] Iteration 45000 (5.53501 iter/s, 18.0668s/100 iter), loss = 0.0679213
I1003 04:04:07.469077 31421 solver.cpp:336]     Train net output #0: loss = 0.0679213 (* 1 = 0.0679213 loss)
I1003 04:04:07.469081 31421 sgd_solver.cpp:136] Iteration 45000, lr = 0.0001, m = 0.9
I1003 04:04:25.487462 31421 solver.cpp:314] Iteration 45100 (5.55004 iter/s, 18.0179s/100 iter), loss = 0.09772
I1003 04:04:25.487538 31421 solver.cpp:336]     Train net output #0: loss = 0.09772 (* 1 = 0.09772 loss)
I1003 04:04:25.487545 31421 sgd_solver.cpp:136] Iteration 45100, lr = 0.0001, m = 0.9
I1003 04:04:28.567009 31342 data_reader.cpp:305] Starting prefetch of epoch 27
I1003 04:04:43.521589 31421 solver.cpp:314] Iteration 45200 (5.5452 iter/s, 18.0336s/100 iter), loss = 0.0602957
I1003 04:04:43.521613 31421 solver.cpp:336]     Train net output #0: loss = 0.0602957 (* 1 = 0.0602957 loss)
I1003 04:04:43.521617 31421 sgd_solver.cpp:136] Iteration 45200, lr = 0.0001, m = 0.9
I1003 04:05:01.649878 31421 solver.cpp:314] Iteration 45300 (5.5164 iter/s, 18.1278s/100 iter), loss = 0.0687958
I1003 04:05:01.649981 31421 solver.cpp:336]     Train net output #0: loss = 0.0687958 (* 1 = 0.0687958 loss)
I1003 04:05:01.649987 31421 sgd_solver.cpp:136] Iteration 45300, lr = 0.0001, m = 0.9
I1003 04:05:19.701155 31421 solver.cpp:314] Iteration 45400 (5.53993 iter/s, 18.0508s/100 iter), loss = 0.062739
I1003 04:05:19.701176 31421 solver.cpp:336]     Train net output #0: loss = 0.0627391 (* 1 = 0.0627391 loss)
I1003 04:05:19.701180 31421 sgd_solver.cpp:136] Iteration 45400, lr = 0.0001, m = 0.9
I1003 04:05:28.446377 31424 data_reader.cpp:305] Starting prefetch of epoch 16
I1003 04:05:37.960222 31421 solver.cpp:314] Iteration 45500 (5.47689 iter/s, 18.2585s/100 iter), loss = 0.0700723
I1003 04:05:37.960381 31421 solver.cpp:336]     Train net output #0: loss = 0.0700724 (* 1 = 0.0700724 loss)
I1003 04:05:37.960405 31421 sgd_solver.cpp:136] Iteration 45500, lr = 0.0001, m = 0.9
I1003 04:05:56.132400 31421 solver.cpp:314] Iteration 45600 (5.50307 iter/s, 18.1717s/100 iter), loss = 0.0858811
I1003 04:05:56.132421 31421 solver.cpp:336]     Train net output #0: loss = 0.0858812 (* 1 = 0.0858812 loss)
I1003 04:05:56.132426 31421 sgd_solver.cpp:136] Iteration 45600, lr = 0.0001, m = 0.9
I1003 04:06:14.269961 31421 solver.cpp:314] Iteration 45700 (5.51357 iter/s, 18.1371s/100 iter), loss = 0.0388822
I1003 04:06:14.270041 31421 solver.cpp:336]     Train net output #0: loss = 0.0388823 (* 1 = 0.0388823 loss)
I1003 04:06:14.270046 31421 sgd_solver.cpp:136] Iteration 45700, lr = 0.0001, m = 0.9
I1003 04:06:28.382076 31344 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 04:06:32.373262 31421 solver.cpp:314] Iteration 45800 (5.52401 iter/s, 18.1028s/100 iter), loss = 0.11174
I1003 04:06:32.373287 31421 solver.cpp:336]     Train net output #0: loss = 0.11174 (* 1 = 0.11174 loss)
I1003 04:06:32.373291 31421 sgd_solver.cpp:136] Iteration 45800, lr = 0.0001, m = 0.9
I1003 04:06:50.507421 31421 solver.cpp:314] Iteration 45900 (5.51461 iter/s, 18.1336s/100 iter), loss = 0.039389
I1003 04:06:50.507467 31421 solver.cpp:336]     Train net output #0: loss = 0.0393891 (* 1 = 0.0393891 loss)
I1003 04:06:50.507472 31421 sgd_solver.cpp:136] Iteration 45900, lr = 0.0001, m = 0.9
I1003 04:06:58.535590 31342 data_reader.cpp:305] Starting prefetch of epoch 28
I1003 04:07:08.495154 31421 solver.cpp:352] Sparsity after update:
I1003 04:07:08.500732 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:07:08.500741 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:07:08.500749 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:07:08.500753 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:07:08.500756 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:07:08.500759 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:07:08.500763 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:07:08.500766 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:07:08.500771 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:07:08.500773 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:07:08.500777 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:07:08.500780 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:07:08.500783 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:07:08.500787 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:07:08.500790 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:07:08.500793 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:07:08.500797 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:07:08.500800 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:07:08.500804 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:07:08.500818 31421 solver.cpp:567] Iteration 46000, Testing net (#0)
I1003 04:07:42.096355 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.956549
I1003 04:07:42.100229 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:07:42.100250 31421 solver.cpp:659]     Test net output #2: loss = 0.150399 (* 1 = 0.150399 loss)
I1003 04:07:42.100288 31421 solver.cpp:265] [MultiGPU] Tests completed in 33.5985s
I1003 04:07:42.302523 31421 solver.cpp:314] Iteration 46000 (1.93074 iter/s, 51.7937s/100 iter), loss = 0.0573523
I1003 04:07:42.302548 31421 solver.cpp:336]     Train net output #0: loss = 0.0573524 (* 1 = 0.0573524 loss)
I1003 04:07:42.302552 31421 sgd_solver.cpp:136] Iteration 46000, lr = 0.0001, m = 0.9
I1003 04:08:00.608244 31421 solver.cpp:314] Iteration 46100 (5.46293 iter/s, 18.3052s/100 iter), loss = 0.0431741
I1003 04:08:00.608270 31421 solver.cpp:336]     Train net output #0: loss = 0.0431742 (* 1 = 0.0431742 loss)
I1003 04:08:00.608274 31421 sgd_solver.cpp:136] Iteration 46100, lr = 0.0001, m = 0.9
I1003 04:08:02.477799 31424 data_reader.cpp:305] Starting prefetch of epoch 17
I1003 04:08:34.010016 31421 solver.cpp:314] Iteration 46200 (2.99394 iter/s, 33.4008s/100 iter), loss = 0.0407323
I1003 04:08:34.010118 31421 solver.cpp:336]     Train net output #0: loss = 0.0407324 (* 1 = 0.0407324 loss)
I1003 04:08:34.010123 31421 sgd_solver.cpp:136] Iteration 46200, lr = 0.0001, m = 0.9
I1003 04:08:51.736088 31421 solver.cpp:314] Iteration 46300 (5.64157 iter/s, 17.7256s/100 iter), loss = 0.0558009
I1003 04:08:51.736114 31421 solver.cpp:336]     Train net output #0: loss = 0.055801 (* 1 = 0.055801 loss)
I1003 04:08:51.736117 31421 sgd_solver.cpp:136] Iteration 46300, lr = 0.0001, m = 0.9
I1003 04:09:10.099503 31421 solver.cpp:314] Iteration 46400 (5.44576 iter/s, 18.3629s/100 iter), loss = 0.060168
I1003 04:09:10.099606 31421 solver.cpp:336]     Train net output #0: loss = 0.0601681 (* 1 = 0.0601681 loss)
I1003 04:09:10.099612 31421 sgd_solver.cpp:136] Iteration 46400, lr = 0.0001, m = 0.9
I1003 04:09:18.849154 31342 data_reader.cpp:305] Starting prefetch of epoch 29
I1003 04:09:29.631191 31421 solver.cpp:314] Iteration 46500 (5.12003 iter/s, 19.5311s/100 iter), loss = 0.0839926
I1003 04:09:29.631222 31421 solver.cpp:336]     Train net output #0: loss = 0.0839927 (* 1 = 0.0839927 loss)
I1003 04:09:29.631225 31421 sgd_solver.cpp:136] Iteration 46500, lr = 0.0001, m = 0.9
I1003 04:09:47.678314 31421 solver.cpp:314] Iteration 46600 (5.5412 iter/s, 18.0466s/100 iter), loss = 0.0391723
I1003 04:09:47.678390 31421 solver.cpp:336]     Train net output #0: loss = 0.0391724 (* 1 = 0.0391724 loss)
I1003 04:09:47.678395 31421 sgd_solver.cpp:136] Iteration 46600, lr = 0.0001, m = 0.9
I1003 04:10:05.757570 31421 solver.cpp:314] Iteration 46700 (5.53136 iter/s, 18.0788s/100 iter), loss = 0.0708861
I1003 04:10:05.757593 31421 solver.cpp:336]     Train net output #0: loss = 0.0708862 (* 1 = 0.0708862 loss)
I1003 04:10:05.757598 31421 sgd_solver.cpp:136] Iteration 46700, lr = 0.0001, m = 0.9
I1003 04:10:18.361366 31424 data_reader.cpp:305] Starting prefetch of epoch 18
I1003 04:10:23.707283 31421 solver.cpp:314] Iteration 46800 (5.57128 iter/s, 17.9492s/100 iter), loss = 0.0795747
I1003 04:10:23.707310 31421 solver.cpp:336]     Train net output #0: loss = 0.0795748 (* 1 = 0.0795748 loss)
I1003 04:10:23.707315 31421 sgd_solver.cpp:136] Iteration 46800, lr = 0.0001, m = 0.9
I1003 04:10:41.823868 31421 solver.cpp:314] Iteration 46900 (5.51996 iter/s, 18.1161s/100 iter), loss = 0.0745495
I1003 04:10:41.823891 31421 solver.cpp:336]     Train net output #0: loss = 0.0745496 (* 1 = 0.0745496 loss)
I1003 04:10:41.823896 31421 sgd_solver.cpp:136] Iteration 46900, lr = 0.0001, m = 0.9
I1003 04:10:59.721334 31421 solver.cpp:352] Sparsity after update:
I1003 04:10:59.729841 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:10:59.729851 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:10:59.729859 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:10:59.729863 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:10:59.729867 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:10:59.729872 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:10:59.729877 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:10:59.729882 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:10:59.729887 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:10:59.729890 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:10:59.729894 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:10:59.729898 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:10:59.729902 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:10:59.729907 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:10:59.729912 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:10:59.729915 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:10:59.729920 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:10:59.729924 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:10:59.729928 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:10:59.899394 31421 solver.cpp:314] Iteration 47000 (5.5325 iter/s, 18.075s/100 iter), loss = 0.044159
I1003 04:10:59.899418 31421 solver.cpp:336]     Train net output #0: loss = 0.0441591 (* 1 = 0.0441591 loss)
I1003 04:10:59.899423 31421 sgd_solver.cpp:136] Iteration 47000, lr = 0.0001, m = 0.9
I1003 04:11:17.922416 31421 solver.cpp:314] Iteration 47100 (5.54861 iter/s, 18.0225s/100 iter), loss = 0.0531182
I1003 04:11:17.922441 31421 solver.cpp:336]     Train net output #0: loss = 0.0531183 (* 1 = 0.0531183 loss)
I1003 04:11:17.922444 31421 sgd_solver.cpp:136] Iteration 47100, lr = 0.0001, m = 0.9
I1003 04:11:18.124950 31430 data_reader.cpp:305] Starting prefetch of epoch 30
I1003 04:11:35.838460 31421 solver.cpp:314] Iteration 47200 (5.58175 iter/s, 17.9155s/100 iter), loss = 0.0561419
I1003 04:11:35.838515 31421 solver.cpp:336]     Train net output #0: loss = 0.056142 (* 1 = 0.056142 loss)
I1003 04:11:35.838522 31421 sgd_solver.cpp:136] Iteration 47200, lr = 0.0001, m = 0.9
I1003 04:11:53.815117 31421 solver.cpp:314] Iteration 47300 (5.56292 iter/s, 17.9762s/100 iter), loss = 0.175163
I1003 04:11:53.815143 31421 solver.cpp:336]     Train net output #0: loss = 0.175163 (* 1 = 0.175163 loss)
I1003 04:11:53.815147 31421 sgd_solver.cpp:136] Iteration 47300, lr = 0.0001, m = 0.9
I1003 04:12:11.935003 31421 solver.cpp:314] Iteration 47400 (5.51895 iter/s, 18.1194s/100 iter), loss = 0.0371403
I1003 04:12:11.935052 31421 solver.cpp:336]     Train net output #0: loss = 0.0371404 (* 1 = 0.0371404 loss)
I1003 04:12:11.935057 31421 sgd_solver.cpp:136] Iteration 47400, lr = 0.0001, m = 0.9
I1003 04:12:17.589207 31344 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 04:12:30.170094 31421 solver.cpp:314] Iteration 47500 (5.48409 iter/s, 18.2346s/100 iter), loss = 0.0464191
I1003 04:12:30.170116 31421 solver.cpp:336]     Train net output #0: loss = 0.0464192 (* 1 = 0.0464192 loss)
I1003 04:12:30.170121 31421 sgd_solver.cpp:136] Iteration 47500, lr = 0.0001, m = 0.9
I1003 04:12:47.788017 31424 data_reader.cpp:305] Starting prefetch of epoch 19
I1003 04:12:48.464197 31421 solver.cpp:314] Iteration 47600 (5.46639 iter/s, 18.2936s/100 iter), loss = 0.0766432
I1003 04:12:48.464223 31421 solver.cpp:336]     Train net output #0: loss = 0.0766433 (* 1 = 0.0766433 loss)
I1003 04:12:48.464227 31421 sgd_solver.cpp:136] Iteration 47600, lr = 0.0001, m = 0.9
I1003 04:13:07.164499 31421 solver.cpp:314] Iteration 47700 (5.34766 iter/s, 18.6998s/100 iter), loss = 0.0604724
I1003 04:13:07.164521 31421 solver.cpp:336]     Train net output #0: loss = 0.0604724 (* 1 = 0.0604724 loss)
I1003 04:13:07.164525 31421 sgd_solver.cpp:136] Iteration 47700, lr = 0.0001, m = 0.9
I1003 04:13:26.040573 31421 solver.cpp:314] Iteration 47800 (5.29786 iter/s, 18.8755s/100 iter), loss = 0.0806953
I1003 04:13:26.040647 31421 solver.cpp:336]     Train net output #0: loss = 0.0806954 (* 1 = 0.0806954 loss)
I1003 04:13:26.040652 31421 sgd_solver.cpp:136] Iteration 47800, lr = 0.0001, m = 0.9
I1003 04:13:44.775722 31421 solver.cpp:314] Iteration 47900 (5.33771 iter/s, 18.7346s/100 iter), loss = 0.0674223
I1003 04:13:44.775746 31421 solver.cpp:336]     Train net output #0: loss = 0.0674223 (* 1 = 0.0674223 loss)
I1003 04:13:44.775750 31421 sgd_solver.cpp:136] Iteration 47900, lr = 0.0001, m = 0.9
I1003 04:13:49.719537 31432 data_reader.cpp:305] Starting prefetch of epoch 37
I1003 04:14:03.119119 31421 solver.cpp:352] Sparsity after update:
I1003 04:14:03.124047 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:14:03.124060 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:14:03.124068 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:14:03.124071 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:14:03.124074 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:14:03.124078 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:14:03.124079 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:14:03.124083 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:14:03.124085 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:14:03.124089 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:14:03.124091 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:14:03.124094 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:14:03.124097 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:14:03.124100 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:14:03.124104 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:14:03.124106 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:14:03.124109 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:14:03.124112 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:14:03.124114 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:14:03.124126 31421 solver.cpp:567] Iteration 48000, Testing net (#0)
I1003 04:14:34.385291 31418 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 04:14:34.862368 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954281
I1003 04:14:34.862393 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:14:34.862401 31421 solver.cpp:659]     Test net output #2: loss = 0.142194 (* 1 = 0.142194 loss)
I1003 04:14:34.862432 31421 solver.cpp:265] [MultiGPU] Tests completed in 31.7374s
I1003 04:14:35.064002 31421 solver.cpp:314] Iteration 48000 (1.98859 iter/s, 50.2869s/100 iter), loss = 0.0790616
I1003 04:14:35.064033 31421 solver.cpp:336]     Train net output #0: loss = 0.0790616 (* 1 = 0.0790616 loss)
I1003 04:14:35.064039 31421 sgd_solver.cpp:136] Iteration 48000, lr = 0.0001, m = 0.9
I1003 04:14:52.047709 31425 data_reader.cpp:305] Starting prefetch of epoch 27
I1003 04:14:53.437840 31421 solver.cpp:314] Iteration 48100 (5.44268 iter/s, 18.3733s/100 iter), loss = 0.0368448
I1003 04:14:53.437865 31421 solver.cpp:336]     Train net output #0: loss = 0.0368449 (* 1 = 0.0368449 loss)
I1003 04:14:53.437870 31421 sgd_solver.cpp:136] Iteration 48100, lr = 0.0001, m = 0.9
I1003 04:15:11.484658 31421 solver.cpp:314] Iteration 48200 (5.5413 iter/s, 18.0463s/100 iter), loss = 0.0578437
I1003 04:15:11.484715 31421 solver.cpp:336]     Train net output #0: loss = 0.0578437 (* 1 = 0.0578437 loss)
I1003 04:15:11.484720 31421 sgd_solver.cpp:136] Iteration 48200, lr = 0.0001, m = 0.9
I1003 04:15:29.755698 31421 solver.cpp:314] Iteration 48300 (5.4733 iter/s, 18.2705s/100 iter), loss = 0.0347656
I1003 04:15:29.755741 31421 solver.cpp:336]     Train net output #0: loss = 0.0347657 (* 1 = 0.0347657 loss)
I1003 04:15:29.755750 31421 sgd_solver.cpp:136] Iteration 48300, lr = 0.0001, m = 0.9
I1003 04:15:47.970640 31421 solver.cpp:314] Iteration 48400 (5.49015 iter/s, 18.2144s/100 iter), loss = 0.0538796
I1003 04:15:47.970708 31421 solver.cpp:336]     Train net output #0: loss = 0.0538797 (* 1 = 0.0538797 loss)
I1003 04:15:47.970715 31421 sgd_solver.cpp:136] Iteration 48400, lr = 0.0001, m = 0.9
I1003 04:15:52.151224 31344 data_reader.cpp:305] Starting prefetch of epoch 27
I1003 04:16:06.024958 31421 solver.cpp:314] Iteration 48500 (5.539 iter/s, 18.0538s/100 iter), loss = 0.0479651
I1003 04:16:06.024978 31421 solver.cpp:336]     Train net output #0: loss = 0.0479651 (* 1 = 0.0479651 loss)
I1003 04:16:06.024983 31421 sgd_solver.cpp:136] Iteration 48500, lr = 0.0001, m = 0.9
I1003 04:16:23.944185 31421 solver.cpp:314] Iteration 48600 (5.58076 iter/s, 17.9187s/100 iter), loss = 0.0485004
I1003 04:16:23.944262 31421 solver.cpp:336]     Train net output #0: loss = 0.0485005 (* 1 = 0.0485005 loss)
I1003 04:16:23.944267 31421 sgd_solver.cpp:136] Iteration 48600, lr = 0.0001, m = 0.9
I1003 04:16:41.980841 31421 solver.cpp:314] Iteration 48700 (5.54442 iter/s, 18.0361s/100 iter), loss = 0.0454081
I1003 04:16:41.980975 31421 solver.cpp:336]     Train net output #0: loss = 0.0454082 (* 1 = 0.0454082 loss)
I1003 04:16:41.980998 31421 sgd_solver.cpp:136] Iteration 48700, lr = 0.0001, m = 0.9
I1003 04:16:51.568200 31432 data_reader.cpp:305] Starting prefetch of epoch 38
I1003 04:17:00.153504 31421 solver.cpp:314] Iteration 48800 (5.50292 iter/s, 18.1722s/100 iter), loss = 0.0868279
I1003 04:17:00.153609 31421 solver.cpp:336]     Train net output #0: loss = 0.086828 (* 1 = 0.086828 loss)
I1003 04:17:00.153615 31421 sgd_solver.cpp:136] Iteration 48800, lr = 0.0001, m = 0.9
I1003 04:17:18.193255 31421 solver.cpp:314] Iteration 48900 (5.54347 iter/s, 18.0392s/100 iter), loss = 0.0551686
I1003 04:17:18.193279 31421 solver.cpp:336]     Train net output #0: loss = 0.0551687 (* 1 = 0.0551687 loss)
I1003 04:17:18.193284 31421 sgd_solver.cpp:136] Iteration 48900, lr = 0.0001, m = 0.9
I1003 04:17:21.676163 31425 data_reader.cpp:305] Starting prefetch of epoch 28
I1003 04:17:36.121600 31421 solver.cpp:352] Sparsity after update:
I1003 04:17:36.126624 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:17:36.126634 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:17:36.126643 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:17:36.126647 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:17:36.126651 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:17:36.126653 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:17:36.126657 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:17:36.126660 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:17:36.126663 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:17:36.126667 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:17:36.126670 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:17:36.126673 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:17:36.126677 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:17:36.126680 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:17:36.126683 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:17:36.126687 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:17:36.126691 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:17:36.126695 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:17:36.126699 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:17:36.296430 31421 solver.cpp:314] Iteration 49000 (5.52405 iter/s, 18.1027s/100 iter), loss = 0.0729641
I1003 04:17:36.296458 31421 solver.cpp:336]     Train net output #0: loss = 0.0729642 (* 1 = 0.0729642 loss)
I1003 04:17:36.296465 31421 sgd_solver.cpp:136] Iteration 49000, lr = 0.0001, m = 0.9
I1003 04:17:54.346153 31421 solver.cpp:314] Iteration 49100 (5.54041 iter/s, 18.0492s/100 iter), loss = 0.0650119
I1003 04:17:54.346182 31421 solver.cpp:336]     Train net output #0: loss = 0.065012 (* 1 = 0.065012 loss)
I1003 04:17:54.346189 31421 sgd_solver.cpp:136] Iteration 49100, lr = 0.0001, m = 0.9
I1003 04:18:12.530575 31421 solver.cpp:314] Iteration 49200 (5.49937 iter/s, 18.1839s/100 iter), loss = 0.0653471
I1003 04:18:12.530647 31421 solver.cpp:336]     Train net output #0: loss = 0.0653472 (* 1 = 0.0653472 loss)
I1003 04:18:12.530652 31421 sgd_solver.cpp:136] Iteration 49200, lr = 0.0001, m = 0.9
I1003 04:18:21.379700 31344 data_reader.cpp:305] Starting prefetch of epoch 28
I1003 04:18:30.541620 31421 solver.cpp:314] Iteration 49300 (5.5523 iter/s, 18.0105s/100 iter), loss = 0.0745813
I1003 04:18:30.541646 31421 solver.cpp:336]     Train net output #0: loss = 0.0745813 (* 1 = 0.0745813 loss)
I1003 04:18:30.541649 31421 sgd_solver.cpp:136] Iteration 49300, lr = 0.0001, m = 0.9
I1003 04:18:48.538529 31421 solver.cpp:314] Iteration 49400 (5.55667 iter/s, 17.9964s/100 iter), loss = 0.0659974
I1003 04:18:48.538579 31421 solver.cpp:336]     Train net output #0: loss = 0.0659975 (* 1 = 0.0659975 loss)
I1003 04:18:48.538584 31421 sgd_solver.cpp:136] Iteration 49400, lr = 0.0001, m = 0.9
I1003 04:19:06.459858 31421 solver.cpp:314] Iteration 49500 (5.5801 iter/s, 17.9208s/100 iter), loss = 0.0471753
I1003 04:19:06.459884 31421 solver.cpp:336]     Train net output #0: loss = 0.0471753 (* 1 = 0.0471753 loss)
I1003 04:19:06.459888 31421 sgd_solver.cpp:136] Iteration 49500, lr = 0.0001, m = 0.9
I1003 04:19:20.943017 31344 data_reader.cpp:305] Starting prefetch of epoch 29
I1003 04:19:24.554721 31421 solver.cpp:314] Iteration 49600 (5.52659 iter/s, 18.0944s/100 iter), loss = 0.05532
I1003 04:19:24.554744 31421 solver.cpp:336]     Train net output #0: loss = 0.05532 (* 1 = 0.05532 loss)
I1003 04:19:24.554747 31421 sgd_solver.cpp:136] Iteration 49600, lr = 0.0001, m = 0.9
I1003 04:19:42.673226 31421 solver.cpp:314] Iteration 49700 (5.51937 iter/s, 18.118s/100 iter), loss = 0.0615955
I1003 04:19:42.673249 31421 solver.cpp:336]     Train net output #0: loss = 0.0615955 (* 1 = 0.0615955 loss)
I1003 04:19:42.673252 31421 sgd_solver.cpp:136] Iteration 49700, lr = 0.0001, m = 0.9
I1003 04:19:50.928256 31424 data_reader.cpp:305] Starting prefetch of epoch 20
I1003 04:20:00.880475 31421 solver.cpp:314] Iteration 49800 (5.49247 iter/s, 18.2067s/100 iter), loss = 0.0723774
I1003 04:20:00.880528 31421 solver.cpp:336]     Train net output #0: loss = 0.0723774 (* 1 = 0.0723774 loss)
I1003 04:20:00.880533 31421 sgd_solver.cpp:136] Iteration 49800, lr = 0.0001, m = 0.9
I1003 04:20:19.063550 31421 solver.cpp:314] Iteration 49900 (5.49977 iter/s, 18.1826s/100 iter), loss = 0.0432173
I1003 04:20:19.063575 31421 solver.cpp:336]     Train net output #0: loss = 0.0432173 (* 1 = 0.0432173 loss)
I1003 04:20:19.063578 31421 sgd_solver.cpp:136] Iteration 49900, lr = 0.0001, m = 0.9
I1003 04:20:37.007233 31421 solver.cpp:352] Sparsity after update:
I1003 04:20:37.013346 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:20:37.013356 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:20:37.013366 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:20:37.013370 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:20:37.013375 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:20:37.013380 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:20:37.013384 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:20:37.013389 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:20:37.013393 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:20:37.013397 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:20:37.013402 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:20:37.013406 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:20:37.013411 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:20:37.013414 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:20:37.013419 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:20:37.013423 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:20:37.013427 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:20:37.013432 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:20:37.013434 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:20:37.013447 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.caffemodel
I1003 04:20:37.338184 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.solverstate
I1003 04:20:37.350793 31421 solver.cpp:567] Iteration 50000, Testing net (#0)
I1003 04:20:58.162299 31536 data_reader.cpp:305] Starting prefetch of epoch 6
I1003 04:20:58.162299 31540 data_reader.cpp:305] Starting prefetch of epoch 2
I1003 04:20:58.162400 31405 data_reader.cpp:305] Starting prefetch of epoch 4
I1003 04:21:09.013528 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.956547
I1003 04:21:09.013651 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:21:09.013671 31421 solver.cpp:659]     Test net output #2: loss = 0.150599 (* 1 = 0.150599 loss)
I1003 04:21:09.013767 31421 solver.cpp:265] [MultiGPU] Tests completed in 31.6621s
I1003 04:21:09.223701 31421 solver.cpp:314] Iteration 50000 (1.99367 iter/s, 50.1587s/100 iter), loss = 0.0564096
I1003 04:21:09.223744 31421 solver.cpp:336]     Train net output #0: loss = 0.0564096 (* 1 = 0.0564096 loss)
I1003 04:21:09.223754 31421 sgd_solver.cpp:136] Iteration 50000, lr = 0.0001, m = 0.9
I1003 04:21:27.656038 31421 solver.cpp:314] Iteration 50100 (5.4254 iter/s, 18.4318s/100 iter), loss = 0.071107
I1003 04:21:27.656064 31421 solver.cpp:336]     Train net output #0: loss = 0.071107 (* 1 = 0.071107 loss)
I1003 04:21:27.656069 31421 sgd_solver.cpp:136] Iteration 50100, lr = 0.0001, m = 0.9
I1003 04:21:46.502351 31421 solver.cpp:314] Iteration 50200 (5.30623 iter/s, 18.8458s/100 iter), loss = 0.0745268
I1003 04:21:46.502488 31421 solver.cpp:336]     Train net output #0: loss = 0.0745269 (* 1 = 0.0745269 loss)
I1003 04:21:46.502506 31421 sgd_solver.cpp:136] Iteration 50200, lr = 0.0001, m = 0.9
I1003 04:21:54.266970 31424 data_reader.cpp:305] Starting prefetch of epoch 21
I1003 04:22:05.660851 31421 solver.cpp:314] Iteration 50300 (5.21976 iter/s, 19.158s/100 iter), loss = 0.0390984
I1003 04:22:05.660878 31421 solver.cpp:336]     Train net output #0: loss = 0.0390985 (* 1 = 0.0390985 loss)
I1003 04:22:05.660882 31421 sgd_solver.cpp:136] Iteration 50300, lr = 0.0001, m = 0.9
I1003 04:22:27.136826 31421 solver.cpp:314] Iteration 50400 (4.6565 iter/s, 21.4754s/100 iter), loss = 0.0526523
I1003 04:22:27.136893 31421 solver.cpp:336]     Train net output #0: loss = 0.0526524 (* 1 = 0.0526524 loss)
I1003 04:22:27.136899 31421 sgd_solver.cpp:136] Iteration 50400, lr = 0.0001, m = 0.9
I1003 04:22:29.799149 31429 blocking_queue.cpp:40] Waiting for datum
I1003 04:22:46.564995 31421 solver.cpp:314] Iteration 50500 (5.14731 iter/s, 19.4276s/100 iter), loss = 0.071462
I1003 04:22:46.565016 31421 solver.cpp:336]     Train net output #0: loss = 0.071462 (* 1 = 0.071462 loss)
I1003 04:22:46.565019 31421 sgd_solver.cpp:136] Iteration 50500, lr = 0.0001, m = 0.9
I1003 04:22:59.682976 31424 data_reader.cpp:305] Starting prefetch of epoch 22
I1003 04:23:05.142482 31421 solver.cpp:314] Iteration 50600 (5.38301 iter/s, 18.577s/100 iter), loss = 0.0666321
I1003 04:23:05.142505 31421 solver.cpp:336]     Train net output #0: loss = 0.0666322 (* 1 = 0.0666322 loss)
I1003 04:23:05.142513 31421 sgd_solver.cpp:136] Iteration 50600, lr = 0.0001, m = 0.9
I1003 04:23:25.866430 31421 solver.cpp:314] Iteration 50700 (4.82547 iter/s, 20.7234s/100 iter), loss = 0.0436774
I1003 04:23:25.866456 31421 solver.cpp:336]     Train net output #0: loss = 0.0436774 (* 1 = 0.0436774 loss)
I1003 04:23:25.866463 31421 sgd_solver.cpp:136] Iteration 50700, lr = 0.0001, m = 0.9
I1003 04:23:44.268246 31421 solver.cpp:314] Iteration 50800 (5.43441 iter/s, 18.4013s/100 iter), loss = 0.0863718
I1003 04:23:44.272255 31421 solver.cpp:336]     Train net output #0: loss = 0.0863718 (* 1 = 0.0863718 loss)
I1003 04:23:44.272294 31421 sgd_solver.cpp:136] Iteration 50800, lr = 0.0001, m = 0.9
I1003 04:24:02.845733 31421 solver.cpp:314] Iteration 50900 (5.38301 iter/s, 18.577s/100 iter), loss = 0.0615302
I1003 04:24:02.845755 31421 solver.cpp:336]     Train net output #0: loss = 0.0615302 (* 1 = 0.0615302 loss)
I1003 04:24:02.845759 31421 sgd_solver.cpp:136] Iteration 50900, lr = 0.0001, m = 0.9
I1003 04:24:03.222256 31432 data_reader.cpp:305] Starting prefetch of epoch 39
I1003 04:24:20.974340 31421 solver.cpp:352] Sparsity after update:
I1003 04:24:20.981218 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:24:20.981226 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:24:20.981235 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:24:20.981240 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:24:20.981242 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:24:20.981246 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:24:20.981251 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:24:20.981256 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:24:20.981259 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:24:20.981264 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:24:20.981268 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:24:20.981272 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:24:20.981276 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:24:20.981281 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:24:20.981284 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:24:20.981289 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:24:20.981293 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:24:20.981298 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:24:20.981302 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:24:21.149161 31421 solver.cpp:314] Iteration 51000 (5.46361 iter/s, 18.3029s/100 iter), loss = 0.0245457
I1003 04:24:21.149183 31421 solver.cpp:336]     Train net output #0: loss = 0.0245457 (* 1 = 0.0245457 loss)
I1003 04:24:21.149188 31421 sgd_solver.cpp:136] Iteration 51000, lr = 0.0001, m = 0.9
I1003 04:24:33.347093 31425 data_reader.cpp:305] Starting prefetch of epoch 29
I1003 04:24:39.193042 31421 solver.cpp:314] Iteration 51100 (5.5422 iter/s, 18.0434s/100 iter), loss = 0.0768082
I1003 04:24:39.193065 31421 solver.cpp:336]     Train net output #0: loss = 0.0768082 (* 1 = 0.0768082 loss)
I1003 04:24:39.193070 31421 sgd_solver.cpp:136] Iteration 51100, lr = 0.0001, m = 0.9
I1003 04:24:57.329188 31421 solver.cpp:314] Iteration 51200 (5.514 iter/s, 18.1356s/100 iter), loss = 0.06817
I1003 04:24:57.329233 31421 solver.cpp:336]     Train net output #0: loss = 0.06817 (* 1 = 0.06817 loss)
I1003 04:24:57.329238 31421 sgd_solver.cpp:136] Iteration 51200, lr = 0.0001, m = 0.9
I1003 04:25:15.374676 31421 solver.cpp:314] Iteration 51300 (5.54171 iter/s, 18.045s/100 iter), loss = 0.0711622
I1003 04:25:15.374697 31421 solver.cpp:336]     Train net output #0: loss = 0.0711622 (* 1 = 0.0711622 loss)
I1003 04:25:15.374701 31421 sgd_solver.cpp:136] Iteration 51300, lr = 0.0001, m = 0.9
I1003 04:25:33.104866 31342 data_reader.cpp:305] Starting prefetch of epoch 30
I1003 04:25:33.437058 31421 solver.cpp:314] Iteration 51400 (5.53652 iter/s, 18.0619s/100 iter), loss = 0.0635163
I1003 04:25:33.437078 31421 solver.cpp:336]     Train net output #0: loss = 0.0635163 (* 1 = 0.0635163 loss)
I1003 04:25:33.437083 31421 sgd_solver.cpp:136] Iteration 51400, lr = 0.0001, m = 0.9
I1003 04:25:51.572903 31421 solver.cpp:314] Iteration 51500 (5.5141 iter/s, 18.1353s/100 iter), loss = 0.0399281
I1003 04:25:51.572927 31421 solver.cpp:336]     Train net output #0: loss = 0.0399281 (* 1 = 0.0399281 loss)
I1003 04:25:51.572932 31421 sgd_solver.cpp:136] Iteration 51500, lr = 0.0001, m = 0.9
I1003 04:26:09.635462 31421 solver.cpp:314] Iteration 51600 (5.53647 iter/s, 18.062s/100 iter), loss = 0.065724
I1003 04:26:09.635574 31421 solver.cpp:336]     Train net output #0: loss = 0.0657241 (* 1 = 0.0657241 loss)
I1003 04:26:09.635583 31421 sgd_solver.cpp:136] Iteration 51600, lr = 0.0001, m = 0.9
I1003 04:26:27.645081 31421 solver.cpp:314] Iteration 51700 (5.55274 iter/s, 18.0091s/100 iter), loss = 0.0589473
I1003 04:26:27.645108 31421 solver.cpp:336]     Train net output #0: loss = 0.0589474 (* 1 = 0.0589474 loss)
I1003 04:26:27.645115 31421 sgd_solver.cpp:136] Iteration 51700, lr = 0.0001, m = 0.9
I1003 04:26:32.733850 31344 data_reader.cpp:305] Starting prefetch of epoch 30
I1003 04:26:45.720489 31421 solver.cpp:314] Iteration 51800 (5.53253 iter/s, 18.0749s/100 iter), loss = 0.0655212
I1003 04:26:45.720541 31421 solver.cpp:336]     Train net output #0: loss = 0.0655212 (* 1 = 0.0655212 loss)
I1003 04:26:45.720547 31421 sgd_solver.cpp:136] Iteration 51800, lr = 0.0001, m = 0.9
I1003 04:27:03.710953 31421 solver.cpp:314] Iteration 51900 (5.55866 iter/s, 17.99s/100 iter), loss = 0.0700266
I1003 04:27:03.710976 31421 solver.cpp:336]     Train net output #0: loss = 0.0700266 (* 1 = 0.0700266 loss)
I1003 04:27:03.710980 31421 sgd_solver.cpp:136] Iteration 51900, lr = 0.0001, m = 0.9
I1003 04:27:22.063570 31421 solver.cpp:352] Sparsity after update:
I1003 04:27:22.067723 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:27:22.067731 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:27:22.067739 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:27:22.067744 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:27:22.067746 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:27:22.067751 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:27:22.067754 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:27:22.067759 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:27:22.067761 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:27:22.067765 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:27:22.067769 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:27:22.067771 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:27:22.067775 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:27:22.067778 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:27:22.067782 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:27:22.067785 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:27:22.067788 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:27:22.067791 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:27:22.067795 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:27:22.067807 31421 solver.cpp:567] Iteration 52000, Testing net (#0)
I1003 04:27:32.437278 31538 data_reader.cpp:305] Starting prefetch of epoch 7
I1003 04:27:55.712082 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954505
I1003 04:27:55.712159 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:27:55.712168 31421 solver.cpp:659]     Test net output #2: loss = 0.141657 (* 1 = 0.141657 loss)
I1003 04:27:55.712215 31421 solver.cpp:265] [MultiGPU] Tests completed in 33.6435s
I1003 04:27:55.925832 31421 solver.cpp:314] Iteration 52000 (1.91522 iter/s, 52.2134s/100 iter), loss = 0.0555319
I1003 04:27:55.925854 31421 solver.cpp:336]     Train net output #0: loss = 0.0555319 (* 1 = 0.0555319 loss)
I1003 04:27:55.925858 31421 sgd_solver.cpp:136] Iteration 52000, lr = 0.0001, m = 0.9
I1003 04:28:07.048125 31430 data_reader.cpp:305] Starting prefetch of epoch 31
I1003 04:28:14.539409 31421 solver.cpp:314] Iteration 52100 (5.37258 iter/s, 18.613s/100 iter), loss = 0.032559
I1003 04:28:14.539444 31421 solver.cpp:336]     Train net output #0: loss = 0.032559 (* 1 = 0.032559 loss)
I1003 04:28:14.539451 31421 sgd_solver.cpp:136] Iteration 52100, lr = 0.0001, m = 0.9
I1003 04:28:33.286936 31421 solver.cpp:314] Iteration 52200 (5.33419 iter/s, 18.747s/100 iter), loss = 0.0615326
I1003 04:28:33.287065 31421 solver.cpp:336]     Train net output #0: loss = 0.0615326 (* 1 = 0.0615326 loss)
I1003 04:28:33.287071 31421 sgd_solver.cpp:136] Iteration 52200, lr = 0.0001, m = 0.9
I1003 04:28:37.781819 31424 data_reader.cpp:305] Starting prefetch of epoch 23
I1003 04:28:52.177836 31421 solver.cpp:314] Iteration 52300 (5.2937 iter/s, 18.8904s/100 iter), loss = 0.077958
I1003 04:28:52.185987 31421 solver.cpp:336]     Train net output #0: loss = 0.077958 (* 1 = 0.077958 loss)
I1003 04:28:52.186055 31421 sgd_solver.cpp:136] Iteration 52300, lr = 0.0001, m = 0.9
I1003 04:29:10.618305 31421 solver.cpp:314] Iteration 52400 (5.42301 iter/s, 18.4399s/100 iter), loss = 0.0499904
I1003 04:29:10.618402 31421 solver.cpp:336]     Train net output #0: loss = 0.0499905 (* 1 = 0.0499905 loss)
I1003 04:29:10.618408 31421 sgd_solver.cpp:136] Iteration 52400, lr = 0.0001, m = 0.9
I1003 04:29:29.317945 31421 solver.cpp:314] Iteration 52500 (5.34785 iter/s, 18.6991s/100 iter), loss = 0.0754595
I1003 04:29:29.317975 31421 solver.cpp:336]     Train net output #0: loss = 0.0754596 (* 1 = 0.0754596 loss)
I1003 04:29:29.317981 31421 sgd_solver.cpp:136] Iteration 52500, lr = 0.0001, m = 0.9
I1003 04:29:39.551484 31432 data_reader.cpp:305] Starting prefetch of epoch 40
I1003 04:29:47.935355 31421 solver.cpp:314] Iteration 52600 (5.37147 iter/s, 18.6169s/100 iter), loss = 0.0838642
I1003 04:29:47.935477 31421 solver.cpp:336]     Train net output #0: loss = 0.0838643 (* 1 = 0.0838643 loss)
I1003 04:29:47.935487 31421 sgd_solver.cpp:136] Iteration 52600, lr = 0.0001, m = 0.9
I1003 04:30:06.595572 31421 solver.cpp:314] Iteration 52700 (5.35914 iter/s, 18.6597s/100 iter), loss = 0.095234
I1003 04:30:06.595593 31421 solver.cpp:336]     Train net output #0: loss = 0.095234 (* 1 = 0.095234 loss)
I1003 04:30:06.595597 31421 sgd_solver.cpp:136] Iteration 52700, lr = 0.0001, m = 0.9
I1003 04:30:24.930677 31421 solver.cpp:314] Iteration 52800 (5.45417 iter/s, 18.3346s/100 iter), loss = 0.0724619
I1003 04:30:24.930785 31421 solver.cpp:336]     Train net output #0: loss = 0.0724619 (* 1 = 0.0724619 loss)
I1003 04:30:24.930791 31421 sgd_solver.cpp:136] Iteration 52800, lr = 0.0001, m = 0.9
I1003 04:30:40.983285 31430 data_reader.cpp:305] Starting prefetch of epoch 32
I1003 04:30:43.705607 31421 solver.cpp:314] Iteration 52900 (5.3264 iter/s, 18.7744s/100 iter), loss = 0.0506076
I1003 04:30:43.705631 31421 solver.cpp:336]     Train net output #0: loss = 0.0506076 (* 1 = 0.0506076 loss)
I1003 04:30:43.705636 31421 sgd_solver.cpp:136] Iteration 52900, lr = 0.0001, m = 0.9
I1003 04:31:01.950636 31421 solver.cpp:352] Sparsity after update:
I1003 04:31:01.969646 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:31:01.969662 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:31:01.969671 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:31:01.969672 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:31:01.969674 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:31:01.969676 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:31:01.969678 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:31:01.969681 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:31:01.969682 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:31:01.969684 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:31:01.969686 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:31:01.969688 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:31:01.969691 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:31:01.969692 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:31:01.969694 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:31:01.969696 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:31:01.969698 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:31:01.969700 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:31:01.969702 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:31:02.135833 31421 solver.cpp:314] Iteration 53000 (5.42602 iter/s, 18.4297s/100 iter), loss = 0.045856
I1003 04:31:02.135852 31421 solver.cpp:336]     Train net output #0: loss = 0.045856 (* 1 = 0.045856 loss)
I1003 04:31:02.135856 31421 sgd_solver.cpp:136] Iteration 53000, lr = 0.0001, m = 0.9
I1003 04:31:11.448228 31425 data_reader.cpp:305] Starting prefetch of epoch 30
I1003 04:31:20.338757 31421 solver.cpp:314] Iteration 53100 (5.49378 iter/s, 18.2024s/100 iter), loss = 0.0551363
I1003 04:31:20.338783 31421 solver.cpp:336]     Train net output #0: loss = 0.0551363 (* 1 = 0.0551363 loss)
I1003 04:31:20.338786 31421 sgd_solver.cpp:136] Iteration 53100, lr = 0.0001, m = 0.9
I1003 04:31:39.024749 31421 solver.cpp:314] Iteration 53200 (5.35175 iter/s, 18.6855s/100 iter), loss = 0.0521297
I1003 04:31:39.024821 31421 solver.cpp:336]     Train net output #0: loss = 0.0521297 (* 1 = 0.0521297 loss)
I1003 04:31:39.024826 31421 sgd_solver.cpp:136] Iteration 53200, lr = 0.0001, m = 0.9
I1003 04:31:57.403933 31421 solver.cpp:314] Iteration 53300 (5.44109 iter/s, 18.3787s/100 iter), loss = 0.0629329
I1003 04:31:57.403957 31421 solver.cpp:336]     Train net output #0: loss = 0.0629329 (* 1 = 0.0629329 loss)
I1003 04:31:57.403962 31421 sgd_solver.cpp:136] Iteration 53300, lr = 0.0001, m = 0.9
I1003 04:32:12.288383 31424 data_reader.cpp:305] Starting prefetch of epoch 24
I1003 04:32:15.753837 31421 solver.cpp:314] Iteration 53400 (5.44977 iter/s, 18.3494s/100 iter), loss = 0.0637054
I1003 04:32:15.753860 31421 solver.cpp:336]     Train net output #0: loss = 0.0637054 (* 1 = 0.0637054 loss)
I1003 04:32:15.753865 31421 sgd_solver.cpp:136] Iteration 53400, lr = 0.0001, m = 0.9
I1003 04:32:34.245817 31421 solver.cpp:314] Iteration 53500 (5.4079 iter/s, 18.4915s/100 iter), loss = 0.056163
I1003 04:32:34.245842 31421 solver.cpp:336]     Train net output #0: loss = 0.056163 (* 1 = 0.056163 loss)
I1003 04:32:34.245847 31421 sgd_solver.cpp:136] Iteration 53500, lr = 0.0001, m = 0.9
I1003 04:32:52.626325 31421 solver.cpp:314] Iteration 53600 (5.4407 iter/s, 18.38s/100 iter), loss = 0.0773894
I1003 04:32:52.626379 31421 solver.cpp:336]     Train net output #0: loss = 0.0773894 (* 1 = 0.0773894 loss)
I1003 04:32:52.626385 31421 sgd_solver.cpp:136] Iteration 53600, lr = 0.0001, m = 0.9
I1003 04:33:10.860888 31421 solver.cpp:314] Iteration 53700 (5.48425 iter/s, 18.234s/100 iter), loss = 0.0618052
I1003 04:33:10.860914 31421 solver.cpp:336]     Train net output #0: loss = 0.0618052 (* 1 = 0.0618052 loss)
I1003 04:33:10.860921 31421 sgd_solver.cpp:136] Iteration 53700, lr = 0.0001, m = 0.9
I1003 04:33:13.034261 31430 data_reader.cpp:305] Starting prefetch of epoch 33
I1003 04:33:29.172585 31421 solver.cpp:314] Iteration 53800 (5.46114 iter/s, 18.3112s/100 iter), loss = 0.0529311
I1003 04:33:29.172665 31421 solver.cpp:336]     Train net output #0: loss = 0.052931 (* 1 = 0.052931 loss)
I1003 04:33:29.172672 31421 sgd_solver.cpp:136] Iteration 53800, lr = 0.0001, m = 0.9
I1003 04:33:43.535938 31424 data_reader.cpp:305] Starting prefetch of epoch 25
I1003 04:33:47.778578 31421 solver.cpp:314] Iteration 53900 (5.37476 iter/s, 18.6055s/100 iter), loss = 0.049964
I1003 04:33:47.778602 31421 solver.cpp:336]     Train net output #0: loss = 0.049964 (* 1 = 0.049964 loss)
I1003 04:33:47.778607 31421 sgd_solver.cpp:136] Iteration 53900, lr = 0.0001, m = 0.9
I1003 04:34:05.961827 31421 solver.cpp:352] Sparsity after update:
I1003 04:34:05.966758 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:34:05.966766 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:34:05.966773 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:34:05.966774 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:34:05.966776 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:34:05.966778 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:34:05.966780 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:34:05.966783 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:34:05.966784 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:34:05.966786 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:34:05.966789 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:34:05.966790 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:34:05.966792 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:34:05.966794 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:34:05.966796 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:34:05.966799 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:34:05.966799 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:34:05.966801 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:34:05.966804 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:34:05.966812 31421 solver.cpp:567] Iteration 54000, Testing net (#0)
I1003 04:34:38.803628 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.95664
I1003 04:34:38.803773 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:34:38.803782 31421 solver.cpp:659]     Test net output #2: loss = 0.150659 (* 1 = 0.150659 loss)
I1003 04:34:38.803812 31421 solver.cpp:265] [MultiGPU] Tests completed in 32.8361s
I1003 04:34:38.983011 31421 solver.cpp:314] Iteration 54000 (1.95301 iter/s, 51.203s/100 iter), loss = 0.06459
I1003 04:34:38.983039 31421 solver.cpp:336]     Train net output #0: loss = 0.06459 (* 1 = 0.06459 loss)
I1003 04:34:38.983045 31421 sgd_solver.cpp:136] Iteration 54000, lr = 0.0001, m = 0.9
I1003 04:34:46.665391 31425 data_reader.cpp:305] Starting prefetch of epoch 31
I1003 04:34:57.071849 31421 solver.cpp:314] Iteration 54100 (5.52843 iter/s, 18.0883s/100 iter), loss = 0.0606066
I1003 04:34:57.071871 31421 solver.cpp:336]     Train net output #0: loss = 0.0606066 (* 1 = 0.0606066 loss)
I1003 04:34:57.071877 31421 sgd_solver.cpp:136] Iteration 54100, lr = 0.0001, m = 0.9
I1003 04:35:15.107059 31421 solver.cpp:314] Iteration 54200 (5.54487 iter/s, 18.0347s/100 iter), loss = 0.0687894
I1003 04:35:15.107131 31421 solver.cpp:336]     Train net output #0: loss = 0.0687894 (* 1 = 0.0687894 loss)
I1003 04:35:15.107136 31421 sgd_solver.cpp:136] Iteration 54200, lr = 0.0001, m = 0.9
I1003 04:35:33.471101 31421 solver.cpp:314] Iteration 54300 (5.44558 iter/s, 18.3635s/100 iter), loss = 0.0803049
I1003 04:35:33.471127 31421 solver.cpp:336]     Train net output #0: loss = 0.0803049 (* 1 = 0.0803049 loss)
I1003 04:35:33.471132 31421 sgd_solver.cpp:136] Iteration 54300, lr = 0.0001, m = 0.9
I1003 04:35:46.848333 31344 data_reader.cpp:305] Starting prefetch of epoch 31
I1003 04:35:51.815014 31421 solver.cpp:314] Iteration 54400 (5.45155 iter/s, 18.3434s/100 iter), loss = 0.0422836
I1003 04:35:51.815042 31421 solver.cpp:336]     Train net output #0: loss = 0.0422836 (* 1 = 0.0422836 loss)
I1003 04:35:51.815048 31421 sgd_solver.cpp:136] Iteration 54400, lr = 0.0001, m = 0.9
I1003 04:36:10.149509 31421 solver.cpp:314] Iteration 54500 (5.45436 iter/s, 18.334s/100 iter), loss = 0.106297
I1003 04:36:10.149564 31421 solver.cpp:336]     Train net output #0: loss = 0.106297 (* 1 = 0.106297 loss)
I1003 04:36:10.149588 31421 sgd_solver.cpp:136] Iteration 54500, lr = 0.0001, m = 0.9
I1003 04:36:28.510565 31421 solver.cpp:314] Iteration 54600 (5.44646 iter/s, 18.3605s/100 iter), loss = 0.0559606
I1003 04:36:28.510670 31421 solver.cpp:336]     Train net output #0: loss = 0.0559605 (* 1 = 0.0559605 loss)
I1003 04:36:28.510676 31421 sgd_solver.cpp:136] Iteration 54600, lr = 0.0001, m = 0.9
I1003 04:36:47.052649 31421 solver.cpp:314] Iteration 54700 (5.39329 iter/s, 18.5416s/100 iter), loss = 0.0698166
I1003 04:36:47.052671 31421 solver.cpp:336]     Train net output #0: loss = 0.0698166 (* 1 = 0.0698166 loss)
I1003 04:36:47.052675 31421 sgd_solver.cpp:136] Iteration 54700, lr = 0.0001, m = 0.9
I1003 04:36:47.596056 31432 data_reader.cpp:305] Starting prefetch of epoch 41
I1003 04:37:05.369448 31421 solver.cpp:314] Iteration 54800 (5.45962 iter/s, 18.3163s/100 iter), loss = 0.0568425
I1003 04:37:05.372264 31421 solver.cpp:336]     Train net output #0: loss = 0.0568425 (* 1 = 0.0568425 loss)
I1003 04:37:05.372284 31421 sgd_solver.cpp:136] Iteration 54800, lr = 0.0001, m = 0.9
I1003 04:37:18.265452 31425 data_reader.cpp:305] Starting prefetch of epoch 32
I1003 04:37:24.050760 31421 solver.cpp:314] Iteration 54900 (5.35309 iter/s, 18.6808s/100 iter), loss = 0.0717707
I1003 04:37:24.050786 31421 solver.cpp:336]     Train net output #0: loss = 0.0717707 (* 1 = 0.0717707 loss)
I1003 04:37:24.050792 31421 sgd_solver.cpp:136] Iteration 54900, lr = 0.0001, m = 0.9
I1003 04:37:42.524448 31421 solver.cpp:352] Sparsity after update:
I1003 04:37:42.542034 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:37:42.542055 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:37:42.542065 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:37:42.542069 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:37:42.542073 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:37:42.542075 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:37:42.542078 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:37:42.542083 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:37:42.542085 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:37:42.542090 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:37:42.542095 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:37:42.542099 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:37:42.542104 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:37:42.542107 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:37:42.542111 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:37:42.542114 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:37:42.542117 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:37:42.542120 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:37:42.542124 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:37:42.711896 31421 solver.cpp:314] Iteration 55000 (5.35888 iter/s, 18.6606s/100 iter), loss = 0.098173
I1003 04:37:42.711925 31421 solver.cpp:336]     Train net output #0: loss = 0.098173 (* 1 = 0.098173 loss)
I1003 04:37:42.711931 31421 sgd_solver.cpp:136] Iteration 55000, lr = 0.0001, m = 0.9
I1003 04:38:01.331404 31421 solver.cpp:314] Iteration 55100 (5.37086 iter/s, 18.619s/100 iter), loss = 0.0912839
I1003 04:38:01.331467 31421 solver.cpp:336]     Train net output #0: loss = 0.0912838 (* 1 = 0.0912838 loss)
I1003 04:38:01.331483 31421 sgd_solver.cpp:136] Iteration 55100, lr = 0.0001, m = 0.9
I1003 04:38:19.820127 31430 data_reader.cpp:305] Starting prefetch of epoch 34
I1003 04:38:19.977973 31421 solver.cpp:314] Iteration 55200 (5.36307 iter/s, 18.646s/100 iter), loss = 0.0475248
I1003 04:38:19.977998 31421 solver.cpp:336]     Train net output #0: loss = 0.0475248 (* 1 = 0.0475248 loss)
I1003 04:38:19.978003 31421 sgd_solver.cpp:136] Iteration 55200, lr = 0.0001, m = 0.9
I1003 04:38:38.731616 31421 solver.cpp:314] Iteration 55300 (5.33245 iter/s, 18.7531s/100 iter), loss = 0.0742206
I1003 04:38:38.731642 31421 solver.cpp:336]     Train net output #0: loss = 0.0742206 (* 1 = 0.0742206 loss)
I1003 04:38:38.731645 31421 sgd_solver.cpp:136] Iteration 55300, lr = 0.0001, m = 0.9
I1003 04:38:57.486435 31421 solver.cpp:314] Iteration 55400 (5.33211 iter/s, 18.7543s/100 iter), loss = 0.0444036
I1003 04:38:57.486512 31421 solver.cpp:336]     Train net output #0: loss = 0.0444035 (* 1 = 0.0444035 loss)
I1003 04:38:57.486522 31421 sgd_solver.cpp:136] Iteration 55400, lr = 0.0001, m = 0.9
I1003 04:39:16.026049 31421 solver.cpp:314] Iteration 55500 (5.39401 iter/s, 18.5391s/100 iter), loss = 0.0636085
I1003 04:39:16.026077 31421 solver.cpp:336]     Train net output #0: loss = 0.0636084 (* 1 = 0.0636084 loss)
I1003 04:39:16.026082 31421 sgd_solver.cpp:136] Iteration 55500, lr = 0.0001, m = 0.9
I1003 04:39:21.606782 31430 data_reader.cpp:305] Starting prefetch of epoch 35
I1003 04:39:34.700548 31421 solver.cpp:314] Iteration 55600 (5.35505 iter/s, 18.674s/100 iter), loss = 0.0684283
I1003 04:39:34.700610 31421 solver.cpp:336]     Train net output #0: loss = 0.0684283 (* 1 = 0.0684283 loss)
I1003 04:39:34.700615 31421 sgd_solver.cpp:136] Iteration 55600, lr = 0.0001, m = 0.9
I1003 04:39:52.620054 31342 data_reader.cpp:305] Starting prefetch of epoch 31
I1003 04:39:53.497464 31421 solver.cpp:314] Iteration 55700 (5.32017 iter/s, 18.7964s/100 iter), loss = 0.0366449
I1003 04:39:53.497491 31421 solver.cpp:336]     Train net output #0: loss = 0.0366449 (* 1 = 0.0366449 loss)
I1003 04:39:53.497496 31421 sgd_solver.cpp:136] Iteration 55700, lr = 0.0001, m = 0.9
I1003 04:40:12.045310 31421 solver.cpp:314] Iteration 55800 (5.39161 iter/s, 18.5473s/100 iter), loss = 0.0540016
I1003 04:40:12.045464 31421 solver.cpp:336]     Train net output #0: loss = 0.0540016 (* 1 = 0.0540016 loss)
I1003 04:40:12.045470 31421 sgd_solver.cpp:136] Iteration 55800, lr = 0.0001, m = 0.9
I1003 04:40:30.685999 31421 solver.cpp:314] Iteration 55900 (5.36476 iter/s, 18.6402s/100 iter), loss = 0.0978191
I1003 04:40:30.686024 31421 solver.cpp:336]     Train net output #0: loss = 0.0978191 (* 1 = 0.0978191 loss)
I1003 04:40:30.686028 31421 sgd_solver.cpp:136] Iteration 55900, lr = 0.0001, m = 0.9
I1003 04:40:49.323851 31421 solver.cpp:352] Sparsity after update:
I1003 04:40:49.332655 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:40:49.332711 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:40:49.332736 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:40:49.332747 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:40:49.332758 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:40:49.332769 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:40:49.332779 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:40:49.332790 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:40:49.332800 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:40:49.332810 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:40:49.332821 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:40:49.332831 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:40:49.332842 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:40:49.332852 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:40:49.332864 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:40:49.332873 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:40:49.332883 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:40:49.332895 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:40:49.332906 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:40:49.332927 31421 solver.cpp:567] Iteration 56000, Testing net (#0)
I1003 04:40:59.903434 31418 data_reader.cpp:305] Starting prefetch of epoch 9
I1003 04:41:23.606403 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954444
I1003 04:41:23.606498 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:41:23.606505 31421 solver.cpp:659]     Test net output #2: loss = 0.142075 (* 1 = 0.142075 loss)
I1003 04:41:23.606534 31421 solver.cpp:265] [MultiGPU] Tests completed in 34.2726s
I1003 04:41:23.785995 31421 solver.cpp:314] Iteration 56000 (1.88329 iter/s, 53.0985s/100 iter), loss = 0.062305
I1003 04:41:23.786070 31421 solver.cpp:336]     Train net output #0: loss = 0.062305 (* 1 = 0.062305 loss)
I1003 04:41:23.786077 31421 sgd_solver.cpp:136] Iteration 56000, lr = 0.0001, m = 0.9
I1003 04:41:28.718176 31344 data_reader.cpp:305] Starting prefetch of epoch 32
I1003 04:41:42.363162 31421 solver.cpp:314] Iteration 56100 (5.38311 iter/s, 18.5766s/100 iter), loss = 0.0783949
I1003 04:41:42.363184 31421 solver.cpp:336]     Train net output #0: loss = 0.0783949 (* 1 = 0.0783949 loss)
I1003 04:41:42.363190 31421 sgd_solver.cpp:136] Iteration 56100, lr = 0.0001, m = 0.9
I1003 04:42:00.844251 31421 solver.cpp:314] Iteration 56200 (5.41109 iter/s, 18.4806s/100 iter), loss = 0.064279
I1003 04:42:00.844306 31421 solver.cpp:336]     Train net output #0: loss = 0.064279 (* 1 = 0.064279 loss)
I1003 04:42:00.844312 31421 sgd_solver.cpp:136] Iteration 56200, lr = 0.0001, m = 0.9
I1003 04:42:19.407923 31421 solver.cpp:314] Iteration 56300 (5.38702 iter/s, 18.5631s/100 iter), loss = 0.0592143
I1003 04:42:19.407948 31421 solver.cpp:336]     Train net output #0: loss = 0.0592143 (* 1 = 0.0592143 loss)
I1003 04:42:19.407951 31421 sgd_solver.cpp:136] Iteration 56300, lr = 0.0001, m = 0.9
I1003 04:42:30.222635 31344 data_reader.cpp:305] Starting prefetch of epoch 33
I1003 04:42:38.500051 31421 solver.cpp:314] Iteration 56400 (5.23792 iter/s, 19.0916s/100 iter), loss = 0.0840459
I1003 04:42:38.500319 31421 solver.cpp:336]     Train net output #0: loss = 0.0840459 (* 1 = 0.0840459 loss)
I1003 04:42:38.500367 31421 sgd_solver.cpp:136] Iteration 56400, lr = 0.0001, m = 0.9
I1003 04:42:57.213318 31421 solver.cpp:314] Iteration 56500 (5.34395 iter/s, 18.7127s/100 iter), loss = 0.067001
I1003 04:42:57.213349 31421 solver.cpp:336]     Train net output #0: loss = 0.067001 (* 1 = 0.067001 loss)
I1003 04:42:57.213354 31421 sgd_solver.cpp:136] Iteration 56500, lr = 0.0001, m = 0.9
I1003 04:43:01.177708 31425 data_reader.cpp:305] Starting prefetch of epoch 33
I1003 04:43:17.702406 31421 solver.cpp:314] Iteration 56600 (4.88078 iter/s, 20.4885s/100 iter), loss = 0.0520548
I1003 04:43:17.702476 31421 solver.cpp:336]     Train net output #0: loss = 0.0520548 (* 1 = 0.0520548 loss)
I1003 04:43:17.702482 31421 sgd_solver.cpp:136] Iteration 56600, lr = 0.0001, m = 0.9
I1003 04:43:36.153640 31421 solver.cpp:314] Iteration 56700 (5.41984 iter/s, 18.4507s/100 iter), loss = 0.0791756
I1003 04:43:36.153672 31421 solver.cpp:336]     Train net output #0: loss = 0.0791755 (* 1 = 0.0791755 loss)
I1003 04:43:36.153679 31421 sgd_solver.cpp:136] Iteration 56700, lr = 0.0001, m = 0.9
I1003 04:43:54.798033 31421 solver.cpp:314] Iteration 56800 (5.3637 iter/s, 18.6439s/100 iter), loss = 0.0680377
I1003 04:43:54.798158 31421 solver.cpp:336]     Train net output #0: loss = 0.0680377 (* 1 = 0.0680377 loss)
I1003 04:43:54.798176 31421 sgd_solver.cpp:136] Iteration 56800, lr = 0.0001, m = 0.9
I1003 04:44:04.528740 31344 data_reader.cpp:305] Starting prefetch of epoch 34
I1003 04:44:13.455615 31421 solver.cpp:314] Iteration 56900 (5.3599 iter/s, 18.6571s/100 iter), loss = 0.0507054
I1003 04:44:13.455639 31421 solver.cpp:336]     Train net output #0: loss = 0.0507053 (* 1 = 0.0507053 loss)
I1003 04:44:13.455643 31421 sgd_solver.cpp:136] Iteration 56900, lr = 0.0001, m = 0.9
I1003 04:44:31.780263 31421 solver.cpp:352] Sparsity after update:
I1003 04:44:31.788230 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:44:31.788244 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:44:31.788259 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:44:31.788264 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:44:31.788269 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:44:31.788275 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:44:31.788280 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:44:31.788285 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:44:31.788291 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:44:31.788297 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:44:31.788302 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:44:31.788308 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:44:31.788314 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:44:31.788319 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:44:31.788326 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:44:31.788331 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:44:31.788336 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:44:31.788341 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:44:31.788348 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:44:31.957056 31421 solver.cpp:314] Iteration 57000 (5.40514 iter/s, 18.5009s/100 iter), loss = 0.0798807
I1003 04:44:31.957077 31421 solver.cpp:336]     Train net output #0: loss = 0.0798806 (* 1 = 0.0798806 loss)
I1003 04:44:31.957082 31421 sgd_solver.cpp:136] Iteration 57000, lr = 0.0001, m = 0.9
I1003 04:44:50.506937 31421 solver.cpp:314] Iteration 57100 (5.39102 iter/s, 18.5494s/100 iter), loss = 0.0802635
I1003 04:44:50.506963 31421 solver.cpp:336]     Train net output #0: loss = 0.0802634 (* 1 = 0.0802634 loss)
I1003 04:44:50.506969 31421 sgd_solver.cpp:136] Iteration 57100, lr = 0.0001, m = 0.9
I1003 04:45:06.021492 31430 data_reader.cpp:305] Starting prefetch of epoch 36
I1003 04:45:09.173727 31421 solver.cpp:314] Iteration 57200 (5.35726 iter/s, 18.6663s/100 iter), loss = 0.0531083
I1003 04:45:09.173751 31421 solver.cpp:336]     Train net output #0: loss = 0.0531083 (* 1 = 0.0531083 loss)
I1003 04:45:09.173758 31421 sgd_solver.cpp:136] Iteration 57200, lr = 0.0001, m = 0.9
I1003 04:45:27.606680 31421 solver.cpp:314] Iteration 57300 (5.42522 iter/s, 18.4324s/100 iter), loss = 0.0729941
I1003 04:45:27.606705 31421 solver.cpp:336]     Train net output #0: loss = 0.0729941 (* 1 = 0.0729941 loss)
I1003 04:45:27.606711 31421 sgd_solver.cpp:136] Iteration 57300, lr = 0.0001, m = 0.9
I1003 04:45:36.589118 31424 data_reader.cpp:305] Starting prefetch of epoch 26
I1003 04:45:46.182437 31421 solver.cpp:314] Iteration 57400 (5.38351 iter/s, 18.5752s/100 iter), loss = 0.0540562
I1003 04:45:46.182457 31421 solver.cpp:336]     Train net output #0: loss = 0.0540562 (* 1 = 0.0540562 loss)
I1003 04:45:46.182462 31421 sgd_solver.cpp:136] Iteration 57400, lr = 0.0001, m = 0.9
I1003 04:46:04.591068 31421 solver.cpp:314] Iteration 57500 (5.43239 iter/s, 18.4081s/100 iter), loss = 0.101076
I1003 04:46:04.591094 31421 solver.cpp:336]     Train net output #0: loss = 0.101076 (* 1 = 0.101076 loss)
I1003 04:46:04.591097 31421 sgd_solver.cpp:136] Iteration 57500, lr = 0.0001, m = 0.9
I1003 04:46:23.444368 31421 solver.cpp:314] Iteration 57600 (5.30426 iter/s, 18.8528s/100 iter), loss = 0.0473646
I1003 04:46:23.444422 31421 solver.cpp:336]     Train net output #0: loss = 0.0473646 (* 1 = 0.0473646 loss)
I1003 04:46:23.444427 31421 sgd_solver.cpp:136] Iteration 57600, lr = 0.0001, m = 0.9
I1003 04:46:38.011075 31342 data_reader.cpp:305] Starting prefetch of epoch 32
I1003 04:46:42.023905 31421 solver.cpp:314] Iteration 57700 (5.38242 iter/s, 18.579s/100 iter), loss = 0.0669133
I1003 04:46:42.023926 31421 solver.cpp:336]     Train net output #0: loss = 0.0669133 (* 1 = 0.0669133 loss)
I1003 04:46:42.023929 31421 sgd_solver.cpp:136] Iteration 57700, lr = 0.0001, m = 0.9
I1003 04:47:00.459306 31421 solver.cpp:314] Iteration 57800 (5.4245 iter/s, 18.4349s/100 iter), loss = 0.0349147
I1003 04:47:00.459362 31421 solver.cpp:336]     Train net output #0: loss = 0.0349147 (* 1 = 0.0349147 loss)
I1003 04:47:00.459368 31421 sgd_solver.cpp:136] Iteration 57800, lr = 0.0001, m = 0.9
I1003 04:47:18.865180 31421 solver.cpp:314] Iteration 57900 (5.4332 iter/s, 18.4054s/100 iter), loss = 0.0376645
I1003 04:47:18.865205 31421 solver.cpp:336]     Train net output #0: loss = 0.0376645 (* 1 = 0.0376645 loss)
I1003 04:47:18.865211 31421 sgd_solver.cpp:136] Iteration 57900, lr = 0.0001, m = 0.9
I1003 04:47:36.951697 31421 solver.cpp:352] Sparsity after update:
I1003 04:47:36.959807 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:47:36.959823 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:47:36.959830 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:47:36.959832 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:47:36.959834 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:47:36.959836 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:47:36.959838 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:47:36.959841 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:47:36.959842 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:47:36.959844 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:47:36.959846 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:47:36.959848 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:47:36.959851 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:47:36.959852 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:47:36.959854 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:47:36.959856 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:47:36.959858 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:47:36.959859 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:47:36.959861 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:47:36.959872 31421 solver.cpp:567] Iteration 58000, Testing net (#0)
I1003 04:47:58.156420 31538 data_reader.cpp:305] Starting prefetch of epoch 8
I1003 04:47:58.156420 31418 data_reader.cpp:305] Starting prefetch of epoch 10
I1003 04:48:10.329270 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.956471
I1003 04:48:10.329416 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:48:10.329426 31421 solver.cpp:659]     Test net output #2: loss = 0.151452 (* 1 = 0.151452 loss)
I1003 04:48:10.329457 31421 solver.cpp:265] [MultiGPU] Tests completed in 33.3686s
I1003 04:48:10.526583 31421 solver.cpp:314] Iteration 58000 (1.93574 iter/s, 51.66s/100 iter), loss = 0.0373138
I1003 04:48:10.526607 31421 solver.cpp:336]     Train net output #0: loss = 0.0373138 (* 1 = 0.0373138 loss)
I1003 04:48:10.526612 31421 sgd_solver.cpp:136] Iteration 58000, lr = 0.0001, m = 0.9
I1003 04:48:28.968068 31421 solver.cpp:314] Iteration 58100 (5.42271 iter/s, 18.441s/100 iter), loss = 0.0585022
I1003 04:48:28.968096 31421 solver.cpp:336]     Train net output #0: loss = 0.0585022 (* 1 = 0.0585022 loss)
I1003 04:48:28.968101 31421 sgd_solver.cpp:136] Iteration 58100, lr = 0.0001, m = 0.9
I1003 04:48:42.769836 31430 data_reader.cpp:305] Starting prefetch of epoch 37
I1003 04:48:47.501729 31421 solver.cpp:314] Iteration 58200 (5.39574 iter/s, 18.5331s/100 iter), loss = 0.0730604
I1003 04:48:47.501754 31421 solver.cpp:336]     Train net output #0: loss = 0.0730604 (* 1 = 0.0730604 loss)
I1003 04:48:47.501757 31421 sgd_solver.cpp:136] Iteration 58200, lr = 0.0001, m = 0.9
I1003 04:49:05.964186 31421 solver.cpp:314] Iteration 58300 (5.41656 iter/s, 18.4619s/100 iter), loss = 0.0322086
I1003 04:49:05.964223 31421 solver.cpp:336]     Train net output #0: loss = 0.0322085 (* 1 = 0.0322085 loss)
I1003 04:49:05.964233 31421 sgd_solver.cpp:136] Iteration 58300, lr = 0.0001, m = 0.9
I1003 04:49:13.248227 31342 data_reader.cpp:305] Starting prefetch of epoch 33
I1003 04:49:24.229436 31421 solver.cpp:314] Iteration 58400 (5.47503 iter/s, 18.2647s/100 iter), loss = 0.0774039
I1003 04:49:24.229455 31421 solver.cpp:336]     Train net output #0: loss = 0.0774039 (* 1 = 0.0774039 loss)
I1003 04:49:24.229460 31421 sgd_solver.cpp:136] Iteration 58400, lr = 0.0001, m = 0.9
I1003 04:49:42.687590 31421 solver.cpp:314] Iteration 58500 (5.41781 iter/s, 18.4576s/100 iter), loss = 0.0343278
I1003 04:49:42.687615 31421 solver.cpp:336]     Train net output #0: loss = 0.0343278 (* 1 = 0.0343278 loss)
I1003 04:49:42.687619 31421 sgd_solver.cpp:136] Iteration 58500, lr = 0.0001, m = 0.9
I1003 04:50:01.202232 31421 solver.cpp:314] Iteration 58600 (5.40128 iter/s, 18.5141s/100 iter), loss = 0.0819527
I1003 04:50:01.202286 31421 solver.cpp:336]     Train net output #0: loss = 0.0819527 (* 1 = 0.0819527 loss)
I1003 04:50:01.202292 31421 sgd_solver.cpp:136] Iteration 58600, lr = 0.0001, m = 0.9
I1003 04:50:14.249254 31430 data_reader.cpp:305] Starting prefetch of epoch 38
I1003 04:50:19.838191 31421 solver.cpp:314] Iteration 58700 (5.36612 iter/s, 18.6354s/100 iter), loss = 0.0909734
I1003 04:50:19.838217 31421 solver.cpp:336]     Train net output #0: loss = 0.0909733 (* 1 = 0.0909733 loss)
I1003 04:50:19.838220 31421 sgd_solver.cpp:136] Iteration 58700, lr = 0.0001, m = 0.9
I1003 04:50:38.198048 31421 solver.cpp:314] Iteration 58800 (5.44682 iter/s, 18.3593s/100 iter), loss = 0.0769827
I1003 04:50:38.198103 31421 solver.cpp:336]     Train net output #0: loss = 0.0769826 (* 1 = 0.0769826 loss)
I1003 04:50:38.198109 31421 sgd_solver.cpp:136] Iteration 58800, lr = 0.0001, m = 0.9
I1003 04:50:56.349416 31421 solver.cpp:314] Iteration 58900 (5.50938 iter/s, 18.1509s/100 iter), loss = 0.0603425
I1003 04:50:56.349470 31421 solver.cpp:336]     Train net output #0: loss = 0.0603425 (* 1 = 0.0603425 loss)
I1003 04:50:56.349485 31421 sgd_solver.cpp:136] Iteration 58900, lr = 0.0001, m = 0.9
I1003 04:51:14.456986 31421 solver.cpp:352] Sparsity after update:
I1003 04:51:14.474443 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:51:14.474485 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:51:14.474506 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:51:14.474514 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:51:14.474519 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:51:14.474525 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:51:14.474532 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:51:14.474544 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:51:14.474552 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:51:14.474561 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:51:14.474570 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:51:14.474578 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:51:14.474586 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:51:14.474594 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:51:14.474601 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:51:14.474609 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:51:14.474614 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:51:14.474620 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:51:14.474627 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:51:14.644608 31421 solver.cpp:314] Iteration 59000 (5.46607 iter/s, 18.2947s/100 iter), loss = 0.051271
I1003 04:51:14.644635 31421 solver.cpp:336]     Train net output #0: loss = 0.0512709 (* 1 = 0.0512709 loss)
I1003 04:51:14.644640 31421 sgd_solver.cpp:136] Iteration 59000, lr = 0.0001, m = 0.9
I1003 04:51:14.862233 31432 data_reader.cpp:305] Starting prefetch of epoch 42
I1003 04:51:33.224531 31421 solver.cpp:314] Iteration 59100 (5.3823 iter/s, 18.5794s/100 iter), loss = 0.0511745
I1003 04:51:33.224572 31421 solver.cpp:336]     Train net output #0: loss = 0.0511745 (* 1 = 0.0511745 loss)
I1003 04:51:33.224581 31421 sgd_solver.cpp:136] Iteration 59100, lr = 0.0001, m = 0.9
I1003 04:51:45.448487 31342 data_reader.cpp:305] Starting prefetch of epoch 34
I1003 04:51:51.479981 31421 solver.cpp:314] Iteration 59200 (5.47797 iter/s, 18.2549s/100 iter), loss = 0.0573218
I1003 04:51:51.480003 31421 solver.cpp:336]     Train net output #0: loss = 0.0573217 (* 1 = 0.0573217 loss)
I1003 04:51:51.480010 31421 sgd_solver.cpp:136] Iteration 59200, lr = 0.0001, m = 0.9
I1003 04:52:10.146661 31421 solver.cpp:314] Iteration 59300 (5.35729 iter/s, 18.6661s/100 iter), loss = 0.0406125
I1003 04:52:10.146703 31421 solver.cpp:336]     Train net output #0: loss = 0.0406124 (* 1 = 0.0406124 loss)
I1003 04:52:10.146718 31421 sgd_solver.cpp:136] Iteration 59300, lr = 0.0001, m = 0.9
I1003 04:52:28.999989 31421 solver.cpp:314] Iteration 59400 (5.30425 iter/s, 18.8528s/100 iter), loss = 0.0599206
I1003 04:52:29.000046 31421 solver.cpp:336]     Train net output #0: loss = 0.0599205 (* 1 = 0.0599205 loss)
I1003 04:52:29.000053 31421 sgd_solver.cpp:136] Iteration 59400, lr = 0.0001, m = 0.9
I1003 04:52:47.279844 31342 data_reader.cpp:305] Starting prefetch of epoch 35
I1003 04:52:47.988731 31421 solver.cpp:314] Iteration 59500 (5.26643 iter/s, 18.9882s/100 iter), loss = 0.0580426
I1003 04:52:47.988755 31421 solver.cpp:336]     Train net output #0: loss = 0.0580425 (* 1 = 0.0580425 loss)
I1003 04:52:47.988760 31421 sgd_solver.cpp:136] Iteration 59500, lr = 0.0001, m = 0.9
I1003 04:53:06.358415 31421 solver.cpp:314] Iteration 59600 (5.4439 iter/s, 18.3692s/100 iter), loss = 0.0481525
I1003 04:53:06.358472 31421 solver.cpp:336]     Train net output #0: loss = 0.0481525 (* 1 = 0.0481525 loss)
I1003 04:53:06.358477 31421 sgd_solver.cpp:136] Iteration 59600, lr = 0.0001, m = 0.9
I1003 04:53:24.793087 31421 solver.cpp:314] Iteration 59700 (5.42471 iter/s, 18.4342s/100 iter), loss = 0.0788875
I1003 04:53:24.793110 31421 solver.cpp:336]     Train net output #0: loss = 0.0788875 (* 1 = 0.0788875 loss)
I1003 04:53:24.793115 31421 sgd_solver.cpp:136] Iteration 59700, lr = 0.0001, m = 0.9
I1003 04:53:43.863682 31421 solver.cpp:314] Iteration 59800 (5.24382 iter/s, 19.0701s/100 iter), loss = 0.0889491
I1003 04:53:43.863798 31421 solver.cpp:336]     Train net output #0: loss = 0.088949 (* 1 = 0.088949 loss)
I1003 04:53:43.863809 31421 sgd_solver.cpp:136] Iteration 59800, lr = 0.0001, m = 0.9
I1003 04:53:48.944692 31342 data_reader.cpp:305] Starting prefetch of epoch 36
I1003 04:54:02.489040 31421 solver.cpp:314] Iteration 59900 (5.36918 iter/s, 18.6248s/100 iter), loss = 0.0769066
I1003 04:54:02.489086 31421 solver.cpp:336]     Train net output #0: loss = 0.0769065 (* 1 = 0.0769065 loss)
I1003 04:54:02.489100 31421 sgd_solver.cpp:136] Iteration 59900, lr = 0.0001, m = 0.9
I1003 04:54:20.653769 31421 solver.cpp:314] Iteration 59999 (5.45027 iter/s, 18.1642s/99 iter), loss = 0.0607646
I1003 04:54:20.653879 31421 solver.cpp:336]     Train net output #0: loss = 0.0607646 (* 1 = 0.0607646 loss)
I1003 04:54:20.655886 31421 solver.cpp:352] Sparsity after update:
I1003 04:54:20.657364 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:54:20.657371 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:54:20.657374 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:54:20.657377 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:54:20.657379 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:54:20.657382 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:54:20.657383 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:54:20.657387 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:54:20.657388 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:54:20.657392 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:54:20.657393 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:54:20.657397 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:54:20.657398 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:54:20.657400 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:54:20.657402 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:54:20.657404 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:54:20.657407 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:54:20.657408 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:54:20.657410 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:54:20.657418 31421 solver.cpp:829] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I1003 04:54:20.867831 31421 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.solverstate
I1003 04:54:20.881556 31421 solver.cpp:376] Sparsity after training:
I1003 04:54:20.883962 31421 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I1003 04:54:20.883977 31421 net.cpp:2312] conv1a_param_0(0.312) 
I1003 04:54:20.883987 31421 net.cpp:2312] conv1b_param_0(0.715) 
I1003 04:54:20.883993 31421 net.cpp:2312] ctx_conv1_param_0(0.808) 
I1003 04:54:20.884001 31421 net.cpp:2312] ctx_conv2_param_0(0.808) 
I1003 04:54:20.884009 31421 net.cpp:2312] ctx_conv3_param_0(0.809) 
I1003 04:54:20.884017 31421 net.cpp:2312] ctx_conv4_param_0(0.809) 
I1003 04:54:20.884024 31421 net.cpp:2312] ctx_final_param_0(0.347) 
I1003 04:54:20.884032 31421 net.cpp:2312] out3a_param_0(0.809) 
I1003 04:54:20.884039 31421 net.cpp:2312] out5a_param_0(0.81) 
I1003 04:54:20.884047 31421 net.cpp:2312] res2a_branch2a_param_0(0.797) 
I1003 04:54:20.884054 31421 net.cpp:2312] res2a_branch2b_param_0(0.706) 
I1003 04:54:20.884061 31421 net.cpp:2312] res3a_branch2a_param_0(0.804) 
I1003 04:54:20.884069 31421 net.cpp:2312] res3a_branch2b_param_0(0.781) 
I1003 04:54:20.884076 31421 net.cpp:2312] res4a_branch2a_param_0(0.81) 
I1003 04:54:20.884083 31421 net.cpp:2312] res4a_branch2b_param_0(0.802) 
I1003 04:54:20.884088 31421 net.cpp:2312] res5a_branch2a_param_0(0.81) 
I1003 04:54:20.884094 31421 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I1003 04:54:20.884101 31421 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1715e+06/2.69117e+06) 0.807
I1003 04:54:20.947700 31421 solver.cpp:542] Iteration 60000, loss = 0.0483267
I1003 04:54:20.947722 31421 solver.cpp:567] Iteration 60000, Testing net (#0)
I1003 04:54:31.029481 31542 data_reader.cpp:305] Starting prefetch of epoch 2
I1003 04:54:54.553784 31421 solver.cpp:659]     Test net output #0: accuracy/top1 = 0.954073
I1003 04:54:54.553876 31421 solver.cpp:659]     Test net output #1: accuracy/top5 = 1
I1003 04:54:54.553884 31421 solver.cpp:659]     Test net output #2: loss = 0.143172 (* 1 = 0.143172 loss)
I1003 04:54:54.780504 31268 parallel.cpp:71] Root Solver performance on device 0: 2.021 * 6 = 12.13 img/sec (60000 itr in 2.968e+04 sec)
I1003 04:54:54.780572 31268 parallel.cpp:76]      Solver performance on device 1: 2.021 * 6 = 12.13 img/sec (60000 itr in 2.968e+04 sec)
I1003 04:54:54.780594 31268 parallel.cpp:76]      Solver performance on device 2: 2.021 * 6 = 12.13 img/sec (60000 itr in 2.968e+04 sec)
I1003 04:54:54.780604 31268 parallel.cpp:79] Overall multi-GPU performance: 36.3854 img/sec
I1003 04:54:56.542780 31268 caffe.cpp:253] Optimization Done in 8h 15m 33s
