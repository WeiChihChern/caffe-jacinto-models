I0511 21:45:42.178694   380 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 21:45:42 2020
I0511 21:45:42.430066   380 caffe.cpp:904] CuDNN version: 7605
I0511 21:45:42.430071   380 caffe.cpp:905] CuBLAS version: 10202
I0511 21:45:42.430074   380 caffe.cpp:906] CUDA version: 10020
I0511 21:45:42.430076   380 caffe.cpp:907] CUDA driver version: 10020
I0511 21:45:42.430079   380 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/solver.prototxt
[3]: --weights=/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
[4]: --gpu
[5]: 0
I0511 21:45:42.451269   380 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 21:45:42.451297   380 gpu_memory.cpp:107] Total memory: 16900227072, Free: 12089163776, dev_info[0]: total=16900227072 free=12089163776
I0511 21:45:42.451499   380 caffe.cpp:226] Using GPUs 0
I0511 21:45:42.451627   380 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 21:45:42.451696   380 solver.cpp:41] Solver data type: FLOAT
I0511 21:45:42.459705   380 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/test.prototxt"
test_iter: 107
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 20000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 1e-05
snapshot: 1000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 1
type: "Adam"
display_sparsity: 2000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.75
sparsity_step_factor: 0.05
sparsity_step_iter: 2000
sparsity_start_iter: 0
sparsity_start_factor: 0.5
sparsity_threshold_maxratio: 0.2
sparsity_itr_increment_bfr_applying: true
sparsity_threshold_value_max: 0.2
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 21:45:42.459931   380 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/train.prototxt
I0511 21:45:42.461521   380 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 48
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 21:45:42.462453   380 net.cpp:110] Using FLOAT as default forward math type
I0511 21:45:42.462471   380 net.cpp:116] Using FLOAT as default backward math type
I0511 21:45:42.462481   380 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 21:45:42.462487   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.462604   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:42.463037   385 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 21:45:42.463055   380 net.cpp:200] Created Layer data (0)
I0511 21:45:42.463073   380 net.cpp:542] data -> data
I0511 21:45:42.463105   380 net.cpp:542] data -> label
I0511 21:45:42.463129   380 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 48
I0511 21:45:42.463271   380 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 21:45:42.463645   386 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.463793   387 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.464022   388 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.464269   389 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.466291   380 annotated_data_layer.cpp:105] output data size: 48,3,320,768
I0511 21:45:42.466593   380 annotated_data_layer.cpp:150] [0] Output data size: 48, 3, 320, 768
I0511 21:45:42.466675   380 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 21:45:42.467247   390 data_layer.cpp:105] [0] Parser threads: 4
I0511 21:45:42.467260   390 data_layer.cpp:107] [0] Transformer threads: 4
I0511 21:45:42.467676   380 net.cpp:260] Setting up data
I0511 21:45:42.468137   380 net.cpp:267] TRAIN Top shape for layer 0 'data' 48 3 320 768 (35389440)
I0511 21:45:42.468184   380 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 21:45:42.468197   380 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 21:45:42.468204   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468215   380 net.cpp:200] Created Layer data_data_0_split (1)
I0511 21:45:42.468223   380 net.cpp:572] data_data_0_split <- data
I0511 21:45:42.468240   380 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 21:45:42.468250   380 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 21:45:42.468257   380 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 21:45:42.468266   380 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 21:45:42.468274   380 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 21:45:42.468282   380 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 21:45:42.468291   380 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 21:45:42.468395   380 net.cpp:260] Setting up data_data_0_split
I0511 21:45:42.468401   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468410   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468417   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468423   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468431   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468438   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468446   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468456   380 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 21:45:42.468461   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468477   380 net.cpp:200] Created Layer data/bias (2)
I0511 21:45:42.468482   380 net.cpp:572] data/bias <- data_data_0_split_0
I0511 21:45:42.468487   380 net.cpp:542] data/bias -> data/bias
I0511 21:45:42.468626   380 net.cpp:260] Setting up data/bias
I0511 21:45:42.468631   380 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 48 3 320 768 (35389440)
I0511 21:45:42.468650   380 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 21:45:42.468655   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468713   380 net.cpp:200] Created Layer conv1a (3)
I0511 21:45:42.468737   380 net.cpp:572] conv1a <- data/bias
I0511 21:45:42.468744   380 net.cpp:542] conv1a -> conv1a
I0511 21:45:45.975857   380 net.cpp:260] Setting up conv1a
I0511 21:45:45.975996   380 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 48 32 160 384 (94371840)
I0511 21:45:45.976083   380 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 21:45:45.976140   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.976202   380 net.cpp:200] Created Layer conv1a/bn (4)
I0511 21:45:45.976253   380 net.cpp:572] conv1a/bn <- conv1a
I0511 21:45:45.976305   380 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 21:45:45.977051   380 net.cpp:260] Setting up conv1a/bn
I0511 21:45:45.977120   380 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 48 32 160 384 (94371840)
I0511 21:45:45.977187   380 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 21:45:45.977236   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.977293   380 net.cpp:200] Created Layer conv1a/relu (5)
I0511 21:45:45.977344   380 net.cpp:572] conv1a/relu <- conv1a
I0511 21:45:45.977393   380 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 21:45:45.977460   380 net.cpp:260] Setting up conv1a/relu
I0511 21:45:45.977507   380 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 48 32 160 384 (94371840)
I0511 21:45:45.977560   380 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 21:45:45.977607   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.977668   380 net.cpp:200] Created Layer conv1b (6)
I0511 21:45:45.977715   380 net.cpp:572] conv1b <- conv1a
I0511 21:45:45.977762   380 net.cpp:542] conv1b -> conv1b
I0511 21:45:45.978826   380 net.cpp:260] Setting up conv1b
I0511 21:45:45.978894   380 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 48 32 160 384 (94371840)
I0511 21:45:45.978955   380 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 21:45:45.979003   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.979058   380 net.cpp:200] Created Layer conv1b/bn (7)
I0511 21:45:45.979104   380 net.cpp:572] conv1b/bn <- conv1b
I0511 21:45:45.979152   380 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 21:45:45.979830   380 net.cpp:260] Setting up conv1b/bn
I0511 21:45:45.979894   380 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 48 32 160 384 (94371840)
I0511 21:45:45.979957   380 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 21:45:45.980005   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980057   380 net.cpp:200] Created Layer conv1b/relu (8)
I0511 21:45:45.980104   380 net.cpp:572] conv1b/relu <- conv1b
I0511 21:45:45.980154   380 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 21:45:45.980206   380 net.cpp:260] Setting up conv1b/relu
I0511 21:45:45.980252   380 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 48 32 160 384 (94371840)
I0511 21:45:45.980304   380 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 21:45:45.980351   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980408   380 net.cpp:200] Created Layer pool1 (9)
I0511 21:45:45.980456   380 net.cpp:572] pool1 <- conv1b
I0511 21:45:45.980505   380 net.cpp:542] pool1 -> pool1
I0511 21:45:45.980669   380 net.cpp:260] Setting up pool1
I0511 21:45:45.980722   380 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 48 32 80 192 (23592960)
I0511 21:45:45.980798   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 21:45:45.980866   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980964   380 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 21:45:45.981036   380 net.cpp:572] res2a_branch2a <- pool1
I0511 21:45:45.981120   380 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 21:45:45.983037   380 net.cpp:260] Setting up res2a_branch2a
I0511 21:45:45.983139   380 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 48 64 80 192 (47185920)
I0511 21:45:45.983222   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.983291   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.983369   380 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 21:45:45.983438   380 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 21:45:45.983511   380 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 21:45:45.984113   380 net.cpp:260] Setting up res2a_branch2a/bn
I0511 21:45:45.984203   380 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 48 64 80 192 (47185920)
I0511 21:45:45.984287   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.984355   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.984432   380 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 21:45:45.984504   380 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 21:45:45.984578   380 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 21:45:45.984658   380 net.cpp:260] Setting up res2a_branch2a/relu
I0511 21:45:45.984731   380 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 48 64 80 192 (47185920)
I0511 21:45:45.984807   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 21:45:45.984879   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.984964   380 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 21:45:45.985038   380 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 21:45:45.985110   380 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 21:45:45.985870   380 net.cpp:260] Setting up res2a_branch2b
I0511 21:45:45.985962   380 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 48 64 80 192 (47185920)
I0511 21:45:45.986039   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:45.986106   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.986179   380 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 21:45:45.986251   380 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 21:45:45.986320   380 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 21:45:45.986932   380 net.cpp:260] Setting up res2a_branch2b/bn
I0511 21:45:45.987011   380 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 48 64 80 192 (47185920)
I0511 21:45:45.987125   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 21:45:45.987234   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.987335   380 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 21:45:45.987393   380 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 21:45:45.987434   380 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 21:45:45.987500   380 net.cpp:260] Setting up res2a_branch2b/relu
I0511 21:45:45.987567   380 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 48 64 80 192 (47185920)
I0511 21:45:45.987640   380 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 21:45:45.987706   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.987782   380 net.cpp:200] Created Layer pool2 (16)
I0511 21:45:45.987823   380 net.cpp:572] pool2 <- res2a_branch2b
I0511 21:45:45.987891   380 net.cpp:542] pool2 -> pool2
I0511 21:45:45.988015   380 net.cpp:260] Setting up pool2
I0511 21:45:45.988024   380 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 48 64 40 96 (11796480)
I0511 21:45:45.988039   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 21:45:45.988078   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.988111   380 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 21:45:45.988134   380 net.cpp:572] res3a_branch2a <- pool2
I0511 21:45:45.988162   380 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 21:45:45.989650   380 net.cpp:260] Setting up res3a_branch2a
I0511 21:45:45.989665   380 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 48 128 40 96 (23592960)
I0511 21:45:45.989681   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.989709   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.989737   380 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 21:45:45.989759   380 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 21:45:45.989784   380 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 21:45:45.990322   380 net.cpp:260] Setting up res3a_branch2a/bn
I0511 21:45:45.990348   380 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 48 128 40 96 (23592960)
I0511 21:45:45.990391   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.990414   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.990445   380 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 21:45:45.990468   380 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 21:45:45.990492   380 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 21:45:45.990523   380 net.cpp:260] Setting up res3a_branch2a/relu
I0511 21:45:45.990545   380 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 48 128 40 96 (23592960)
I0511 21:45:45.990574   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 21:45:45.990598   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.990633   380 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 21:45:45.990658   380 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 21:45:45.990684   380 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 21:45:45.991925   380 net.cpp:260] Setting up res3a_branch2b
I0511 21:45:45.991955   380 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 48 128 40 96 (23592960)
I0511 21:45:45.991991   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:45.992015   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992045   380 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 21:45:45.992069   380 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 21:45:45.992095   380 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 21:45:45.992636   380 net.cpp:260] Setting up res3a_branch2b/bn
I0511 21:45:45.992664   380 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 48 128 40 96 (23592960)
I0511 21:45:45.992700   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 21:45:45.992725   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992753   380 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 21:45:45.992779   380 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 21:45:45.992808   380 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 21:45:45.992835   380 net.cpp:260] Setting up res3a_branch2b/relu
I0511 21:45:45.992857   380 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 48 128 40 96 (23592960)
I0511 21:45:45.992885   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:45.992910   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992942   380 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 21:45:45.992978   380 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 21:45:45.993003   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:45.993028   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:45.993108   380 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 21:45:45.993130   380 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 48 128 40 96 (23592960)
I0511 21:45:45.993157   380 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 48 128 40 96 (23592960)
I0511 21:45:45.993188   380 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 21:45:45.993211   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.993239   380 net.cpp:200] Created Layer pool3 (24)
I0511 21:45:45.993264   380 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:45.993295   380 net.cpp:542] pool3 -> pool3
I0511 21:45:45.993388   380 net.cpp:260] Setting up pool3
I0511 21:45:45.993412   380 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 48 128 20 48 (5898240)
I0511 21:45:45.993443   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 21:45:45.993466   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.993505   380 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 21:45:45.993530   380 net.cpp:572] res4a_branch2a <- pool3
I0511 21:45:45.993552   380 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 21:45:45.999234   380 net.cpp:260] Setting up res4a_branch2a
I0511 21:45:45.999248   380 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 48 256 20 48 (11796480)
I0511 21:45:45.999266   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.999291   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.999317   380 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 21:45:45.999338   380 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 21:45:45.999361   380 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 21:45:45.999864   380 net.cpp:260] Setting up res4a_branch2a/bn
I0511 21:45:45.999891   380 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 48 256 20 48 (11796480)
I0511 21:45:45.999927   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.999950   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.999975   380 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 21:45:45.999996   380 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 21:45:46.000020   380 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 21:45:46.000046   380 net.cpp:260] Setting up res4a_branch2a/relu
I0511 21:45:46.000066   380 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 48 256 20 48 (11796480)
I0511 21:45:46.000095   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 21:45:46.000116   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.000149   380 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 21:45:46.000171   380 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 21:45:46.000192   380 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 21:45:46.003631   380 net.cpp:260] Setting up res4a_branch2b
I0511 21:45:46.003715   380 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 48 256 20 48 (11796480)
I0511 21:45:46.003814   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.003958   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.004076   380 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 21:45:46.004227   380 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 21:45:46.004328   380 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 21:45:46.004923   380 net.cpp:260] Setting up res4a_branch2b/bn
I0511 21:45:46.005030   380 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 48 256 20 48 (11796480)
I0511 21:45:46.005167   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.005256   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.005412   380 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 21:45:46.005511   380 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 21:45:46.005609   380 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 21:45:46.005707   380 net.cpp:260] Setting up res4a_branch2b/relu
I0511 21:45:46.005795   380 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 48 256 20 48 (11796480)
I0511 21:45:46.005895   380 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 21:45:46.005991   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.006093   380 net.cpp:200] Created Layer pool4 (31)
I0511 21:45:46.006182   380 net.cpp:572] pool4 <- res4a_branch2b
I0511 21:45:46.006276   380 net.cpp:542] pool4 -> pool4
I0511 21:45:46.006440   380 net.cpp:260] Setting up pool4
I0511 21:45:46.006531   380 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 48 256 10 24 (2949120)
I0511 21:45:46.006630   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 21:45:46.006723   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.006829   380 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 21:45:46.006923   380 net.cpp:572] res5a_branch2a <- pool4
I0511 21:45:46.007017   380 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 21:45:46.033661   380 net.cpp:260] Setting up res5a_branch2a
I0511 21:45:46.033818   380 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 48 512 10 24 (5898240)
I0511 21:45:46.033932   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.034029   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.034132   380 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 21:45:46.034229   380 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 21:45:46.034322   380 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 21:45:46.034942   380 net.cpp:260] Setting up res5a_branch2a/bn
I0511 21:45:46.035054   380 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 48 512 10 24 (5898240)
I0511 21:45:46.035174   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.035272   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.035370   380 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 21:45:46.035463   380 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 21:45:46.035558   380 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 21:45:46.035663   380 net.cpp:260] Setting up res5a_branch2a/relu
I0511 21:45:46.035751   380 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 48 512 10 24 (5898240)
I0511 21:45:46.035849   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 21:45:46.035946   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.036059   380 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 21:45:46.036154   380 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 21:45:46.036262   380 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 21:45:46.050024   380 net.cpp:260] Setting up res5a_branch2b
I0511 21:45:46.050174   380 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 48 512 10 24 (5898240)
I0511 21:45:46.050323   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.050472   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.050627   380 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 21:45:46.050777   380 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 21:45:46.050921   380 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 21:45:46.051609   380 net.cpp:260] Setting up res5a_branch2b/bn
I0511 21:45:46.051766   380 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 48 512 10 24 (5898240)
I0511 21:45:46.051926   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.052069   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.052215   380 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 21:45:46.052353   380 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 21:45:46.052496   380 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 21:45:46.052642   380 net.cpp:260] Setting up res5a_branch2b/relu
I0511 21:45:46.052781   380 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 48 512 10 24 (5898240)
I0511 21:45:46.052929   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.053071   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.053216   380 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 21:45:46.053313   380 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 21:45:46.053467   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.053563   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.053714   380 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 21:45:46.053807   380 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 48 512 10 24 (5898240)
I0511 21:45:46.053907   380 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 48 512 10 24 (5898240)
I0511 21:45:46.054003   380 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 21:45:46.054095   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.054193   380 net.cpp:200] Created Layer pool6 (39)
I0511 21:45:46.054284   380 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.054378   380 net.cpp:542] pool6 -> pool6
I0511 21:45:46.054543   380 net.cpp:260] Setting up pool6
I0511 21:45:46.054633   380 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 48 512 5 12 (1474560)
I0511 21:45:46.054731   380 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 21:45:46.054769   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.054919   380 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 21:45:46.054956   380 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 21:45:46.055104   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 21:45:46.055197   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 21:45:46.055338   380 net.cpp:260] Setting up pool6_pool6_0_split
I0511 21:45:46.055377   380 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 48 512 5 12 (1474560)
I0511 21:45:46.055523   380 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 48 512 5 12 (1474560)
I0511 21:45:46.055624   380 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 21:45:46.055719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.055830   380 net.cpp:200] Created Layer pool7 (41)
I0511 21:45:46.055920   380 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 21:45:46.056013   380 net.cpp:542] pool7 -> pool7
I0511 21:45:46.056177   380 net.cpp:260] Setting up pool7
I0511 21:45:46.056267   380 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 48 512 3 6 (442368)
I0511 21:45:46.056363   380 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 21:45:46.056455   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.056550   380 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 21:45:46.056641   380 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 21:45:46.056733   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 21:45:46.056830   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 21:45:46.056968   380 net.cpp:260] Setting up pool7_pool7_0_split
I0511 21:45:46.057008   380 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 48 512 3 6 (442368)
I0511 21:45:46.057157   380 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 48 512 3 6 (442368)
I0511 21:45:46.057253   380 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 21:45:46.057349   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.057448   380 net.cpp:200] Created Layer pool8 (43)
I0511 21:45:46.057538   380 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 21:45:46.057631   380 net.cpp:542] pool8 -> pool8
I0511 21:45:46.057792   380 net.cpp:260] Setting up pool8
I0511 21:45:46.057829   380 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 48 512 2 3 (147456)
I0511 21:45:46.057977   380 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 21:45:46.058069   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.058164   380 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 21:45:46.058254   380 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 21:45:46.058348   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 21:45:46.058449   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 21:45:46.058588   380 net.cpp:260] Setting up pool8_pool8_0_split
I0511 21:45:46.058676   380 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 48 512 2 3 (147456)
I0511 21:45:46.058717   380 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 48 512 2 3 (147456)
I0511 21:45:46.058866   380 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 21:45:46.058957   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.059054   380 net.cpp:200] Created Layer pool9 (45)
I0511 21:45:46.059146   380 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 21:45:46.059239   380 net.cpp:542] pool9 -> pool9
I0511 21:45:46.059397   380 net.cpp:260] Setting up pool9
I0511 21:45:46.059434   380 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 48 512 1 2 (49152)
I0511 21:45:46.059582   380 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 21:45:46.059674   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.059778   380 net.cpp:200] Created Layer ctx_output1 (46)
I0511 21:45:46.059870   380 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.059963   380 net.cpp:542] ctx_output1 -> ctx_output1
I0511 21:45:46.061087   380 net.cpp:260] Setting up ctx_output1
I0511 21:45:46.061134   380 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 48 256 40 96 (47185920)
I0511 21:45:46.061308   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 21:45:46.061408   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.061507   380 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 21:45:46.061607   380 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 21:45:46.061699   380 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 21:45:46.061795   380 net.cpp:260] Setting up ctx_output1/relu
I0511 21:45:46.061831   380 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 48 256 40 96 (47185920)
I0511 21:45:46.061981   380 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 21:45:46.062077   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.062173   380 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 21:45:46.062265   380 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 21:45:46.062358   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.062453   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.062547   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.062705   380 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 21:45:46.062793   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.062888   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.062983   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.063079   380 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 21:45:46.063117   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.063273   380 net.cpp:200] Created Layer ctx_output2 (49)
I0511 21:45:46.063366   380 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.063460   380 net.cpp:542] ctx_output2 -> ctx_output2
I0511 21:45:46.066623   380 net.cpp:260] Setting up ctx_output2
I0511 21:45:46.066678   380 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 48 256 10 24 (2949120)
I0511 21:45:46.066854   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 21:45:46.066946   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.067040   380 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 21:45:46.067133   380 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 21:45:46.067226   380 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 21:45:46.067322   380 net.cpp:260] Setting up ctx_output2/relu
I0511 21:45:46.067359   380 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 48 256 10 24 (2949120)
I0511 21:45:46.067519   380 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 21:45:46.067556   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.067706   380 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 21:45:46.067797   380 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 21:45:46.067891   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.067986   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.068080   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.068248   380 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 21:45:46.068338   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068434   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068538   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068645   380 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 21:45:46.068735   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.068838   380 net.cpp:200] Created Layer ctx_output3 (52)
I0511 21:45:46.068930   380 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 21:45:46.069022   380 net.cpp:542] ctx_output3 -> ctx_output3
I0511 21:45:46.073102   380 net.cpp:260] Setting up ctx_output3
I0511 21:45:46.073158   380 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 48 256 5 12 (737280)
I0511 21:45:46.073339   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 21:45:46.073432   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.073529   380 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 21:45:46.073619   380 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 21:45:46.073719   380 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 21:45:46.073817   380 net.cpp:260] Setting up ctx_output3/relu
I0511 21:45:46.073853   380 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 48 256 5 12 (737280)
I0511 21:45:46.074003   380 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 21:45:46.074095   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.074190   380 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 21:45:46.074281   380 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 21:45:46.074374   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.074470   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.074564   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.074734   380 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 21:45:46.074823   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.074867   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.075016   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.075111   380 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 21:45:46.075202   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.075306   380 net.cpp:200] Created Layer ctx_output4 (55)
I0511 21:45:46.075398   380 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 21:45:46.075491   380 net.cpp:542] ctx_output4 -> ctx_output4
I0511 21:45:46.078663   380 net.cpp:260] Setting up ctx_output4
I0511 21:45:46.078716   380 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 48 256 3 6 (221184)
I0511 21:45:46.078891   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 21:45:46.078982   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.079085   380 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 21:45:46.079190   380 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 21:45:46.079282   380 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 21:45:46.079377   380 net.cpp:260] Setting up ctx_output4/relu
I0511 21:45:46.079414   380 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 48 256 3 6 (221184)
I0511 21:45:46.079563   380 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 21:45:46.079660   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.079771   380 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 21:45:46.079861   380 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 21:45:46.079955   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.080050   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.080145   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.080315   380 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 21:45:46.080353   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080502   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080596   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080693   380 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 21:45:46.080785   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.080886   380 net.cpp:200] Created Layer ctx_output5 (58)
I0511 21:45:46.080977   380 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 21:45:46.081073   380 net.cpp:542] ctx_output5 -> ctx_output5
I0511 21:45:46.084260   380 net.cpp:260] Setting up ctx_output5
I0511 21:45:46.084313   380 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 48 256 2 3 (73728)
I0511 21:45:46.084503   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 21:45:46.084615   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.084722   380 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 21:45:46.084827   380 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 21:45:46.084933   380 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 21:45:46.085047   380 net.cpp:260] Setting up ctx_output5/relu
I0511 21:45:46.085150   380 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 48 256 2 3 (73728)
I0511 21:45:46.085258   380 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 21:45:46.085372   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.085481   380 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 21:45:46.085584   380 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 21:45:46.085697   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.085804   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.085912   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.086104   380 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 21:45:46.086196   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086262   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086362   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086472   380 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 21:45:46.086575   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.086688   380 net.cpp:200] Created Layer ctx_output6 (61)
I0511 21:45:46.086789   380 net.cpp:572] ctx_output6 <- pool9
I0511 21:45:46.086896   380 net.cpp:542] ctx_output6 -> ctx_output6
I0511 21:45:46.090193   380 net.cpp:260] Setting up ctx_output6
I0511 21:45:46.090324   380 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 48 256 1 2 (24576)
I0511 21:45:46.090458   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 21:45:46.090564   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.090674   380 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 21:45:46.090790   380 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 21:45:46.090888   380 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 21:45:46.090988   380 net.cpp:260] Setting up ctx_output6/relu
I0511 21:45:46.091085   380 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 48 256 1 2 (24576)
I0511 21:45:46.091187   380 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 21:45:46.091233   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.091320   380 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 21:45:46.091395   380 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 21:45:46.091470   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.091586   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.091665   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.091835   380 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 21:45:46.091935   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092027   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092083   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092175   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.092252   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.092373   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 21:45:46.092412   380 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.092486   380 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 21:45:46.093057   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 21:45:46.093102   380 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 48 16 40 96 (2949120)
I0511 21:45:46.093197   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.093273   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.093400   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 21:45:46.093439   380 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 21:45:46.093513   380 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.093767   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.093806   380 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 48 40 96 16 (2949120)
I0511 21:45:46.093888   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.093960   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.094038   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 21:45:46.094113   380 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.094185   380 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.105696   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.105799   380 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 48 61440 (2949120)
I0511 21:45:46.105975   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.106060   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.106199   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 21:45:46.106241   380 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.106333   380 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 21:45:46.107059   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 21:45:46.107113   380 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 48 16 40 96 (2949120)
I0511 21:45:46.107220   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.107307   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.107393   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 21:45:46.107478   380 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 21:45:46.107556   380 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.107822   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.107859   380 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 48 40 96 16 (2949120)
I0511 21:45:46.107944   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.108016   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.108135   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 21:45:46.108176   380 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.108255   380 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.119660   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.119756   380 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 48 61440 (2949120)
I0511 21:45:46.119905   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.119998   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.120129   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 21:45:46.120170   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.120251   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 21:45:46.120337   380 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 21:45:46.120528   380 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 21:45:46.120566   380 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 21:45:46.120648   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.120726   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.120862   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 21:45:46.120901   380 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.120995   380 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 21:45:46.121706   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 21:45:46.121758   380 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 48 24 10 24 (276480)
I0511 21:45:46.121932   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.122030   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.122141   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 21:45:46.122201   380 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 21:45:46.122285   380 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.122604   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.122642   380 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 48 10 24 24 (276480)
I0511 21:45:46.122735   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.122822   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.122898   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 21:45:46.122974   380 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.123047   380 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.125253   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.125352   380 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 48 5760 (276480)
I0511 21:45:46.125524   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.125562   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.125718   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 21:45:46.125816   380 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.125914   380 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 21:45:46.126530   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 21:45:46.126663   380 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 48 24 10 24 (276480)
I0511 21:45:46.126796   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.126924   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.127043   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 21:45:46.127157   380 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 21:45:46.127271   380 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.127538   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.127648   380 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 48 10 24 24 (276480)
I0511 21:45:46.127768   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.127882   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.127997   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 21:45:46.128110   380 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.128227   380 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.130420   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.130568   380 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 48 5760 (276480)
I0511 21:45:46.130698   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.130818   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.130939   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 21:45:46.131054   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.131170   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 21:45:46.131295   380 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 21:45:46.131480   380 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 21:45:46.131603   380 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 21:45:46.131722   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.131834   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.131961   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 21:45:46.132081   380 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.132196   380 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 21:45:46.132845   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 21:45:46.132970   380 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 48 24 5 12 (69120)
I0511 21:45:46.133101   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.133219   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.133354   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 21:45:46.133474   380 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 21:45:46.133594   380 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.133863   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.133973   380 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 48 5 12 24 (69120)
I0511 21:45:46.134090   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.134205   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.134323   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 21:45:46.134441   380 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.134557   380 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.134778   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.134896   380 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 48 1440 (69120)
I0511 21:45:46.135020   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.135133   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.135258   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 21:45:46.135378   380 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.135502   380 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 21:45:46.136158   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 21:45:46.136289   380 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 48 24 5 12 (69120)
I0511 21:45:46.136430   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.136559   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.136689   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 21:45:46.136819   380 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 21:45:46.136945   380 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.137234   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.137357   380 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 48 5 12 24 (69120)
I0511 21:45:46.137495   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.137621   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.137766   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 21:45:46.137907   380 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.138031   380 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.138275   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.138401   380 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 48 1440 (69120)
I0511 21:45:46.138540   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.138731   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.138880   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 21:45:46.139012   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.139144   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 21:45:46.139277   380 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 21:45:46.139461   380 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 21:45:46.139581   380 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 21:45:46.139719   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.139848   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.139997   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 21:45:46.140130   380 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.140261   380 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 21:45:46.140954   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 21:45:46.141096   380 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 48 24 3 6 (20736)
I0511 21:45:46.141242   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.141424   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.141563   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 21:45:46.141693   380 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 21:45:46.141816   380 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142097   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142216   380 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 48 3 6 24 (20736)
I0511 21:45:46.142343   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.142464   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.142596   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 21:45:46.142724   380 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142848   380 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.143070   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.143190   380 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 48 432 (20736)
I0511 21:45:46.143317   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.143437   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.143571   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 21:45:46.143699   380 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.143822   380 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 21:45:46.144482   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 21:45:46.144627   380 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 48 24 3 6 (20736)
I0511 21:45:46.144768   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.144896   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.145025   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 21:45:46.145150   380 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 21:45:46.145275   380 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.145601   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.145720   380 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 48 3 6 24 (20736)
I0511 21:45:46.145853   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.145977   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.146102   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 21:45:46.146226   380 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.146358   380 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.146584   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.146703   380 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 48 432 (20736)
I0511 21:45:46.146828   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.146950   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.147076   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 21:45:46.147200   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.147326   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 21:45:46.147450   380 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 21:45:46.147594   380 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 21:45:46.147718   380 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 21:45:46.147843   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.147964   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.148097   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 21:45:46.148226   380 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.148351   380 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 21:45:46.148959   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 21:45:46.149087   380 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 48 16 2 3 (4608)
I0511 21:45:46.149227   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.149360   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.149490   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 21:45:46.149613   380 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 21:45:46.149737   380 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150003   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150120   380 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 48 2 3 16 (4608)
I0511 21:45:46.150246   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.150380   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.150561   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 21:45:46.150683   380 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150806   380 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.151016   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.151134   380 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 48 96 (4608)
I0511 21:45:46.151259   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.151379   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.151515   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 21:45:46.151640   380 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.151764   380 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 21:45:46.152308   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 21:45:46.152436   380 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 48 16 2 3 (4608)
I0511 21:45:46.152575   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.152704   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.152832   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 21:45:46.152954   380 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 21:45:46.153077   380 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.153362   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.153486   380 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 48 2 3 16 (4608)
I0511 21:45:46.153626   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.153754   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.153880   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 21:45:46.154002   380 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.154127   380 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.154338   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.154458   380 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 48 96 (4608)
I0511 21:45:46.154584   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.154707   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.154834   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 21:45:46.154956   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.155079   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 21:45:46.155205   380 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 21:45:46.155349   380 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 21:45:46.155473   380 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 21:45:46.155601   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.155721   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.155853   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 21:45:46.155978   380 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.156123   380 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 21:45:46.156688   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 21:45:46.156814   380 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 48 16 1 2 (1536)
I0511 21:45:46.156895   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.157047   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.157157   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 21:45:46.157285   380 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 21:45:46.157423   380 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.157701   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.157819   380 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 48 1 2 16 (1536)
I0511 21:45:46.157944   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.158064   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.158187   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 21:45:46.158306   380 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.158428   380 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.158632   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.158746   380 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 48 32 (1536)
I0511 21:45:46.158871   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.158989   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.159121   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 21:45:46.159245   380 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.159366   380 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 21:45:46.159935   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 21:45:46.160064   380 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 48 16 1 2 (1536)
I0511 21:45:46.160202   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.160320   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.160451   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 21:45:46.160579   380 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 21:45:46.160701   380 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.160975   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.161094   380 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 48 1 2 16 (1536)
I0511 21:45:46.161221   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.161346   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.161469   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 21:45:46.161590   380 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.161712   380 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.161918   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.162036   380 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 48 32 (1536)
I0511 21:45:46.162160   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.162290   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.162439   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 21:45:46.162559   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.162680   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 21:45:46.162801   380 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 21:45:46.162945   380 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 21:45:46.163069   380 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 21:45:46.163192   380 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 21:45:46.163311   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.163439   380 net.cpp:200] Created Layer mbox_loc (106)
I0511 21:45:46.163566   380 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.163697   380 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.163825   380 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.163942   380 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.164057   380 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.164175   380 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.164288   380 net.cpp:542] mbox_loc -> mbox_loc
I0511 21:45:46.164431   380 net.cpp:260] Setting up mbox_loc
I0511 21:45:46.164548   380 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 48 69200 (3321600)
I0511 21:45:46.164669   380 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 21:45:46.164783   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.164899   380 net.cpp:200] Created Layer mbox_conf (107)
I0511 21:45:46.165016   380 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.165133   380 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.165249   380 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.165369   380 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.165485   380 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.165601   380 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.165715   380 net.cpp:542] mbox_conf -> mbox_conf
I0511 21:45:46.165858   380 net.cpp:260] Setting up mbox_conf
I0511 21:45:46.165977   380 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 48 69200 (3321600)
I0511 21:45:46.166096   380 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 21:45:46.166211   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.166330   380 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 21:45:46.166442   380 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 21:45:46.166559   380 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 21:45:46.166682   380 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 21:45:46.166795   380 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 21:45:46.166910   380 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 21:45:46.167024   380 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 21:45:46.167140   380 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 21:45:46.167284   380 net.cpp:260] Setting up mbox_priorbox
I0511 21:45:46.167402   380 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 21:45:46.167520   380 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 21:45:46.167635   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.167769   380 net.cpp:200] Created Layer mbox_loss (109)
I0511 21:45:46.167887   380 net.cpp:572] mbox_loss <- mbox_loc
I0511 21:45:46.168011   380 net.cpp:572] mbox_loss <- mbox_conf
I0511 21:45:46.168125   380 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 21:45:46.168241   380 net.cpp:572] mbox_loss <- label
I0511 21:45:46.168356   380 net.cpp:542] mbox_loss -> mbox_loss
I0511 21:45:46.168560   380 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 21:45:46.168670   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.168933   380 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 21:45:46.169044   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.169318   380 net.cpp:260] Setting up mbox_loss
I0511 21:45:46.169435   380 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 21:45:46.169550   380 net.cpp:271]     with loss weight 1
I0511 21:45:46.169692   380 net.cpp:336] mbox_loss needs backward computation.
I0511 21:45:46.169809   380 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 21:45:46.169929   380 net.cpp:336] mbox_conf needs backward computation.
I0511 21:45:46.170044   380 net.cpp:336] mbox_loc needs backward computation.
I0511 21:45:46.170168   380 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.170290   380 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.170406   380 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.170519   380 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 21:45:46.170637   380 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.170750   380 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.170866   380 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 21:45:46.170981   380 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.171095   380 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.171211   380 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.171324   380 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 21:45:46.171439   380 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.171553   380 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.171669   380 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 21:45:46.171783   380 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.171900   380 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.172013   380 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.172127   380 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 21:45:46.172241   380 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.172358   380 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.172472   380 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 21:45:46.172588   380 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.172704   380 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.172818   380 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.172940   380 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 21:45:46.173054   380 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.173168   380 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.173306   380 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 21:45:46.173427   380 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.173544   380 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.173657   380 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.173771   380 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 21:45:46.173887   380 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.174002   380 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.174115   380 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 21:45:46.174237   380 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.174353   380 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.174468   380 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.174583   380 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 21:45:46.174697   380 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.174813   380 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.174927   380 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 21:45:46.175045   380 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 21:45:46.175160   380 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 21:45:46.175274   380 net.cpp:336] ctx_output6 needs backward computation.
I0511 21:45:46.175390   380 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 21:45:46.175505   380 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 21:45:46.175618   380 net.cpp:336] ctx_output5 needs backward computation.
I0511 21:45:46.175736   380 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 21:45:46.175849   380 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 21:45:46.175963   380 net.cpp:336] ctx_output4 needs backward computation.
I0511 21:45:46.176079   380 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 21:45:46.176194   380 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 21:45:46.176308   380 net.cpp:336] ctx_output3 needs backward computation.
I0511 21:45:46.176422   380 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 21:45:46.176537   380 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 21:45:46.176653   380 net.cpp:336] ctx_output2 needs backward computation.
I0511 21:45:46.176767   380 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 21:45:46.176882   380 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 21:45:46.176997   380 net.cpp:336] ctx_output1 needs backward computation.
I0511 21:45:46.177114   380 net.cpp:336] pool9 needs backward computation.
I0511 21:45:46.177227   380 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 21:45:46.177368   380 net.cpp:336] pool8 needs backward computation.
I0511 21:45:46.177484   380 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 21:45:46.177623   380 net.cpp:336] pool7 needs backward computation.
I0511 21:45:46.177727   380 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 21:45:46.177829   380 net.cpp:336] pool6 needs backward computation.
I0511 21:45:46.177930   380 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 21:45:46.178032   380 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 21:45:46.178128   380 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 21:45:46.178228   380 net.cpp:336] res5a_branch2b needs backward computation.
I0511 21:45:46.178328   380 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 21:45:46.178433   380 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 21:45:46.178547   380 net.cpp:336] res5a_branch2a needs backward computation.
I0511 21:45:46.178644   380 net.cpp:336] pool4 needs backward computation.
I0511 21:45:46.178746   380 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 21:45:46.178843   380 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 21:45:46.178938   380 net.cpp:336] res4a_branch2b needs backward computation.
I0511 21:45:46.179033   380 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 21:45:46.179129   380 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 21:45:46.179224   380 net.cpp:336] res4a_branch2a needs backward computation.
I0511 21:45:46.179322   380 net.cpp:336] pool3 needs backward computation.
I0511 21:45:46.179422   380 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 21:45:46.179523   380 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 21:45:46.179625   380 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 21:45:46.179726   380 net.cpp:336] res3a_branch2b needs backward computation.
I0511 21:45:46.179828   380 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 21:45:46.179924   380 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 21:45:46.180019   380 net.cpp:336] res3a_branch2a needs backward computation.
I0511 21:45:46.180114   380 net.cpp:336] pool2 needs backward computation.
I0511 21:45:46.180210   380 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 21:45:46.180305   380 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 21:45:46.180402   380 net.cpp:336] res2a_branch2b needs backward computation.
I0511 21:45:46.180498   380 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 21:45:46.180598   380 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 21:45:46.180703   380 net.cpp:336] res2a_branch2a needs backward computation.
I0511 21:45:46.180805   380 net.cpp:336] pool1 needs backward computation.
I0511 21:45:46.180902   380 net.cpp:336] conv1b/relu needs backward computation.
I0511 21:45:46.180997   380 net.cpp:336] conv1b/bn needs backward computation.
I0511 21:45:46.181095   380 net.cpp:336] conv1b needs backward computation.
I0511 21:45:46.181190   380 net.cpp:336] conv1a/relu needs backward computation.
I0511 21:45:46.181285   380 net.cpp:336] conv1a/bn needs backward computation.
I0511 21:45:46.181437   380 net.cpp:336] conv1a needs backward computation.
I0511 21:45:46.181555   380 net.cpp:338] data/bias does not need backward computation.
I0511 21:45:46.181674   380 net.cpp:338] data_data_0_split does not need backward computation.
I0511 21:45:46.181789   380 net.cpp:338] data does not need backward computation.
I0511 21:45:46.181905   380 net.cpp:380] This network produces output mbox_loss
I0511 21:45:46.182181   380 net.cpp:403] Top memory (TRAIN) required for data: 7231391112 diff: 7231391112
I0511 21:45:46.182294   380 net.cpp:406] Bottom memory (TRAIN) required for data: 7231391104 diff: 7231391104
I0511 21:45:46.182407   380 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 3130294272 diff: 3130294272
I0511 21:45:46.182523   380 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 21:45:46.182636   380 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 21:45:46.182757   380 net.cpp:421] Network initialization done.
I0511 21:45:46.184695   380 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/test.prototxt
I0511 21:45:46.185166   380 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 21:45:46.190017   380 net.cpp:110] Using FLOAT as default forward math type
I0511 21:45:46.190050   380 net.cpp:116] Using FLOAT as default backward math type
I0511 21:45:46.190078   380 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 21:45:46.190105   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.190148   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.190302   380 net.cpp:200] Created Layer data (0)
I0511 21:45:46.190325   380 net.cpp:542] data -> data
I0511 21:45:46.190349   380 net.cpp:542] data -> label
I0511 21:45:46.190379   380 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 21:45:46.190418   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.201643   423 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 21:45:46.203445   380 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 21:45:46.203574   380 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 21:45:46.203630   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.203825   380 net.cpp:260] Setting up data
I0511 21:45:46.203841   380 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 21:45:46.203862   380 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 21:45:46.203877   380 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 21:45:46.203887   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.203908   380 net.cpp:200] Created Layer data_data_0_split (1)
I0511 21:45:46.203920   380 net.cpp:572] data_data_0_split <- data
I0511 21:45:46.203929   380 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 21:45:46.203941   380 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 21:45:46.203949   380 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 21:45:46.203964   380 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 21:45:46.203980   380 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 21:45:46.203995   380 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 21:45:46.204003   380 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 21:45:46.204187   380 net.cpp:260] Setting up data_data_0_split
I0511 21:45:46.204202   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204221   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204237   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204252   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204259   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204272   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204288   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204308   380 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 21:45:46.204313   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.204330   380 net.cpp:200] Created Layer data/bias (2)
I0511 21:45:46.204344   380 net.cpp:572] data/bias <- data_data_0_split_0
I0511 21:45:46.204355   380 net.cpp:542] data/bias -> data/bias
I0511 21:45:46.204576   380 net.cpp:260] Setting up data/bias
I0511 21:45:46.204589   380 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 21:45:46.204613   380 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 21:45:46.204627   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.204653   380 net.cpp:200] Created Layer conv1a (3)
I0511 21:45:46.204665   380 net.cpp:572] conv1a <- data/bias
I0511 21:45:46.204682   380 net.cpp:542] conv1a -> conv1a
I0511 21:45:46.206666   424 data_layer.cpp:105] (0) Parser threads: 1
I0511 21:45:46.206688   380 net.cpp:260] Setting up conv1a
I0511 21:45:46.207042   380 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 21:45:46.207024   424 data_layer.cpp:107] (0) Transformer threads: 1
I0511 21:45:46.207072   380 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 21:45:46.207108   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.207129   380 net.cpp:200] Created Layer conv1a/bn (4)
I0511 21:45:46.207142   380 net.cpp:572] conv1a/bn <- conv1a
I0511 21:45:46.207170   380 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 21:45:46.207883   380 net.cpp:260] Setting up conv1a/bn
I0511 21:45:46.207899   380 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 21:45:46.207928   380 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 21:45:46.207938   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.207947   380 net.cpp:200] Created Layer conv1a/relu (5)
I0511 21:45:46.207957   380 net.cpp:572] conv1a/relu <- conv1a
I0511 21:45:46.207967   380 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 21:45:46.207984   380 net.cpp:260] Setting up conv1a/relu
I0511 21:45:46.207996   380 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 21:45:46.208011   380 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 21:45:46.208019   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.208043   380 net.cpp:200] Created Layer conv1b (6)
I0511 21:45:46.208055   380 net.cpp:572] conv1b <- conv1a
I0511 21:45:46.208063   380 net.cpp:542] conv1b -> conv1b
I0511 21:45:46.208582   380 net.cpp:260] Setting up conv1b
I0511 21:45:46.208597   380 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 21:45:46.208617   380 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 21:45:46.208628   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.208652   380 net.cpp:200] Created Layer conv1b/bn (7)
I0511 21:45:46.208662   380 net.cpp:572] conv1b/bn <- conv1b
I0511 21:45:46.208668   380 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 21:45:46.209357   380 net.cpp:260] Setting up conv1b/bn
I0511 21:45:46.209374   380 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 21:45:46.209400   380 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 21:45:46.209410   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209420   380 net.cpp:200] Created Layer conv1b/relu (8)
I0511 21:45:46.209434   380 net.cpp:572] conv1b/relu <- conv1b
I0511 21:45:46.209451   380 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 21:45:46.209465   380 net.cpp:260] Setting up conv1b/relu
I0511 21:45:46.209476   380 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 21:45:46.209494   380 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 21:45:46.209509   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209527   380 net.cpp:200] Created Layer pool1 (9)
I0511 21:45:46.209537   380 net.cpp:572] pool1 <- conv1b
I0511 21:45:46.209543   380 net.cpp:542] pool1 -> pool1
I0511 21:45:46.209640   380 net.cpp:260] Setting up pool1
I0511 21:45:46.209650   380 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 21:45:46.209668   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 21:45:46.209681   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209703   380 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 21:45:46.209712   380 net.cpp:572] res2a_branch2a <- pool1
I0511 21:45:46.209720   380 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 21:45:46.210548   380 net.cpp:260] Setting up res2a_branch2a
I0511 21:45:46.210561   380 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 21:45:46.210582   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.210593   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.210613   380 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 21:45:46.210626   380 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 21:45:46.210634   380 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 21:45:46.211292   380 net.cpp:260] Setting up res2a_branch2a/bn
I0511 21:45:46.211303   380 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 21:45:46.211334   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.211347   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.211364   380 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 21:45:46.211370   380 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 21:45:46.211381   380 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 21:45:46.211401   380 net.cpp:260] Setting up res2a_branch2a/relu
I0511 21:45:46.211413   380 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 21:45:46.211428   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 21:45:46.211434   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.211458   380 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 21:45:46.211470   380 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 21:45:46.211478   380 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 21:45:46.227989   380 net.cpp:260] Setting up res2a_branch2b
I0511 21:45:46.228031   380 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 21:45:46.228070   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.228085   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.228121   380 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 21:45:46.228145   380 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 21:45:46.228171   380 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 21:45:46.229591   380 net.cpp:260] Setting up res2a_branch2b/bn
I0511 21:45:46.229984   380 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 21:45:46.230072   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.230096   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230127   380 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 21:45:46.230162   380 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 21:45:46.230197   380 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 21:45:46.230238   380 net.cpp:260] Setting up res2a_branch2b/relu
I0511 21:45:46.230262   380 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 21:45:46.230300   380 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 21:45:46.230325   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230386   380 net.cpp:200] Created Layer pool2 (16)
I0511 21:45:46.230409   380 net.cpp:572] pool2 <- res2a_branch2b
I0511 21:45:46.230446   380 net.cpp:542] pool2 -> pool2
I0511 21:45:46.230635   380 net.cpp:260] Setting up pool2
I0511 21:45:46.230660   380 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 21:45:46.230698   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 21:45:46.230721   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230783   380 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 21:45:46.230795   380 net.cpp:572] res3a_branch2a <- pool2
I0511 21:45:46.230823   380 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 21:45:46.235287   380 net.cpp:260] Setting up res3a_branch2a
I0511 21:45:46.235301   380 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 21:45:46.235317   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.235348   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.235366   380 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 21:45:46.235378   380 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 21:45:46.235384   380 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 21:45:46.235874   380 net.cpp:260] Setting up res3a_branch2a/bn
I0511 21:45:46.235885   380 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 21:45:46.235910   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.235919   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.235929   380 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 21:45:46.235942   380 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 21:45:46.235956   380 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 21:45:46.235965   380 net.cpp:260] Setting up res3a_branch2a/relu
I0511 21:45:46.235973   380 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 21:45:46.235987   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 21:45:46.235993   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.236018   380 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 21:45:46.236028   380 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 21:45:46.236035   380 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 21:45:46.237156   380 net.cpp:260] Setting up res3a_branch2b
I0511 21:45:46.237172   380 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 21:45:46.237191   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.237201   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.237211   380 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 21:45:46.237221   380 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 21:45:46.237227   380 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 21:45:46.246433   380 net.cpp:260] Setting up res3a_branch2b/bn
I0511 21:45:46.246552   380 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 21:45:46.246776   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.246965   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.247051   380 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 21:45:46.247100   380 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 21:45:46.247114   380 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 21:45:46.247614   380 net.cpp:260] Setting up res3a_branch2b/relu
I0511 21:45:46.247885   380 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 21:45:46.248051   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.248260   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.248576   380 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 21:45:46.248718   380 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 21:45:46.248903   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:46.249301   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.250378   380 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 21:45:46.250589   380 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 21:45:46.250723   380 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 21:45:46.251240   380 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 21:45:46.251564   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.251822   380 net.cpp:200] Created Layer pool3 (24)
I0511 21:45:46.251963   380 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:46.252144   380 net.cpp:542] pool3 -> pool3
I0511 21:45:46.253098   380 net.cpp:260] Setting up pool3
I0511 21:45:46.253113   380 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 21:45:46.253127   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 21:45:46.253136   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.253295   380 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 21:45:46.253309   380 net.cpp:572] res4a_branch2a <- pool3
I0511 21:45:46.253324   380 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 21:45:46.280545   380 net.cpp:260] Setting up res4a_branch2a
I0511 21:45:46.280565   380 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 21:45:46.280586   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.280592   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.280612   380 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 21:45:46.280623   380 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 21:45:46.289446   380 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 21:45:46.290066   380 net.cpp:260] Setting up res4a_branch2a/bn
I0511 21:45:46.290174   380 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 21:45:46.290297   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.290405   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.290518   380 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 21:45:46.290627   380 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 21:45:46.290735   380 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 21:45:46.290845   380 net.cpp:260] Setting up res4a_branch2a/relu
I0511 21:45:46.290949   380 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 21:45:46.291060   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 21:45:46.291167   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.291285   380 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 21:45:46.291391   380 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 21:45:46.291496   380 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 21:45:46.295001   380 net.cpp:260] Setting up res4a_branch2b
I0511 21:45:46.309317   380 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 21:45:46.309464   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.309574   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.309691   380 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 21:45:46.309803   380 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 21:45:46.309914   380 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 21:45:46.310549   380 net.cpp:260] Setting up res4a_branch2b/bn
I0511 21:45:46.310658   380 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 21:45:46.310782   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.310899   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.311014   380 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 21:45:46.311136   380 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 21:45:46.311244   380 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 21:45:46.311352   380 net.cpp:260] Setting up res4a_branch2b/relu
I0511 21:45:46.311461   380 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 21:45:46.311573   380 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 21:45:46.311681   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.311796   380 net.cpp:200] Created Layer pool4 (31)
I0511 21:45:46.311904   380 net.cpp:572] pool4 <- res4a_branch2b
I0511 21:45:46.312014   380 net.cpp:542] pool4 -> pool4
I0511 21:45:46.312203   380 net.cpp:260] Setting up pool4
I0511 21:45:46.312309   380 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 21:45:46.312422   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 21:45:46.312531   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.312652   380 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 21:45:46.312760   380 net.cpp:572] res5a_branch2a <- pool4
I0511 21:45:46.312870   380 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 21:45:46.362547   380 net.cpp:260] Setting up res5a_branch2a
I0511 21:45:46.362637   380 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 21:45:46.362727   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.362792   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.362861   380 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 21:45:46.362923   380 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 21:45:46.362987   380 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 21:45:46.363510   380 net.cpp:260] Setting up res5a_branch2a/bn
I0511 21:45:46.363582   380 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 21:45:46.363656   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.363718   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.363782   380 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 21:45:46.363843   380 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 21:45:46.363906   380 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 21:45:46.363970   380 net.cpp:260] Setting up res5a_branch2a/relu
I0511 21:45:46.364030   380 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 21:45:46.364094   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 21:45:46.364156   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.364226   380 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 21:45:46.364289   380 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 21:45:46.364351   380 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 21:45:46.377768   380 net.cpp:260] Setting up res5a_branch2b
I0511 21:45:46.377907   380 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 21:45:46.378005   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.378073   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.378150   380 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 21:45:46.378221   380 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 21:45:46.378293   380 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 21:45:46.378808   380 net.cpp:260] Setting up res5a_branch2b/bn
I0511 21:45:46.378886   380 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 21:45:46.378983   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.379052   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.379124   380 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 21:45:46.379196   380 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 21:45:46.379271   380 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 21:45:46.379344   380 net.cpp:260] Setting up res5a_branch2b/relu
I0511 21:45:46.379412   380 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 21:45:46.379489   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.379559   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.379632   380 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 21:45:46.379701   380 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 21:45:46.379776   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.379850   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.379967   380 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 21:45:46.380038   380 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 21:45:46.380115   380 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 21:45:46.380187   380 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 21:45:46.380256   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.380331   380 net.cpp:200] Created Layer pool6 (39)
I0511 21:45:46.380400   380 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.380470   380 net.cpp:542] pool6 -> pool6
I0511 21:45:46.380605   380 net.cpp:260] Setting up pool6
I0511 21:45:46.380674   380 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 21:45:46.380751   380 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 21:45:46.380825   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.380897   380 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 21:45:46.380965   380 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 21:45:46.381037   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 21:45:46.381110   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 21:45:46.381222   380 net.cpp:260] Setting up pool6_pool6_0_split
I0511 21:45:46.381295   380 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 21:45:46.381371   380 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 21:45:46.381444   380 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 21:45:46.381513   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.381588   380 net.cpp:200] Created Layer pool7 (41)
I0511 21:45:46.381657   380 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 21:45:46.381726   380 net.cpp:542] pool7 -> pool7
I0511 21:45:46.381860   380 net.cpp:260] Setting up pool7
I0511 21:45:46.381929   380 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 21:45:46.382002   380 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 21:45:46.382071   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.382148   380 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 21:45:46.382220   380 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 21:45:46.382299   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 21:45:46.382373   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 21:45:46.382485   380 net.cpp:260] Setting up pool7_pool7_0_split
I0511 21:45:46.382553   380 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 21:45:46.382624   380 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 21:45:46.382699   380 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 21:45:46.382766   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.382839   380 net.cpp:200] Created Layer pool8 (43)
I0511 21:45:46.382908   380 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 21:45:46.382978   380 net.cpp:542] pool8 -> pool8
I0511 21:45:46.383110   380 net.cpp:260] Setting up pool8
I0511 21:45:46.383183   380 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 21:45:46.383256   380 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 21:45:46.383324   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.383396   380 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 21:45:46.383466   380 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 21:45:46.383535   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 21:45:46.383606   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 21:45:46.383713   380 net.cpp:260] Setting up pool8_pool8_0_split
I0511 21:45:46.383780   380 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 21:45:46.383852   380 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 21:45:46.383924   380 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 21:45:46.383992   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.384066   380 net.cpp:200] Created Layer pool9 (45)
I0511 21:45:46.384135   380 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 21:45:46.384209   380 net.cpp:542] pool9 -> pool9
I0511 21:45:46.384338   380 net.cpp:260] Setting up pool9
I0511 21:45:46.384407   380 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 21:45:46.384479   380 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 21:45:46.384550   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.384631   380 net.cpp:200] Created Layer ctx_output1 (46)
I0511 21:45:46.384701   380 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.384773   380 net.cpp:542] ctx_output1 -> ctx_output1
I0511 21:45:46.385855   380 net.cpp:260] Setting up ctx_output1
I0511 21:45:46.385944   380 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 21:45:46.386016   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 21:45:46.386077   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.386142   380 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 21:45:46.386204   380 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 21:45:46.386265   380 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 21:45:46.386330   380 net.cpp:260] Setting up ctx_output1/relu
I0511 21:45:46.386390   380 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 21:45:46.386454   380 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 21:45:46.386518   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.386590   380 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 21:45:46.386669   380 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 21:45:46.386750   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.386822   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.386893   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.387023   380 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 21:45:46.387091   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387164   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387236   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387310   380 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 21:45:46.387382   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.387460   380 net.cpp:200] Created Layer ctx_output2 (49)
I0511 21:45:46.387531   380 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.387601   380 net.cpp:542] ctx_output2 -> ctx_output2
I0511 21:45:46.391471   380 net.cpp:260] Setting up ctx_output2
I0511 21:45:46.391567   380 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 21:45:46.391642   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 21:45:46.391705   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.391768   380 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 21:45:46.391830   380 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 21:45:46.391893   380 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 21:45:46.391958   380 net.cpp:260] Setting up ctx_output2/relu
I0511 21:45:46.392019   380 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 21:45:46.392084   380 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 21:45:46.392145   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.392210   380 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 21:45:46.392271   380 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 21:45:46.392333   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.392397   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.392462   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.392585   380 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 21:45:46.392647   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392711   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392774   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392839   380 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 21:45:46.392899   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.392969   380 net.cpp:200] Created Layer ctx_output3 (52)
I0511 21:45:46.393033   380 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 21:45:46.393095   380 net.cpp:542] ctx_output3 -> ctx_output3
I0511 21:45:46.396152   380 net.cpp:260] Setting up ctx_output3
I0511 21:45:46.396239   380 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 21:45:46.396322   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 21:45:46.396394   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.396458   380 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 21:45:46.396520   380 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 21:45:46.396582   380 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 21:45:46.396646   380 net.cpp:260] Setting up ctx_output3/relu
I0511 21:45:46.396706   380 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 21:45:46.404789   380 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 21:45:46.404901   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.404991   380 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 21:45:46.405125   380 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 21:45:46.405211   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.405300   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.405388   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.405566   380 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 21:45:46.405740   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.405831   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.405916   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.406003   380 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 21:45:46.406085   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.406177   380 net.cpp:200] Created Layer ctx_output4 (55)
I0511 21:45:46.406268   380 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 21:45:46.406350   380 net.cpp:542] ctx_output4 -> ctx_output4
I0511 21:45:46.409499   380 net.cpp:260] Setting up ctx_output4
I0511 21:45:46.412668   380 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 21:45:46.412770   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 21:45:46.412847   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.412923   380 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 21:45:46.413003   380 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 21:45:46.413077   380 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 21:45:46.413152   380 net.cpp:260] Setting up ctx_output4/relu
I0511 21:45:46.413226   380 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 21:45:46.413303   380 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 21:45:46.413377   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.413450   380 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 21:45:46.413522   380 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 21:45:46.413596   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.413669   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.413745   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.413892   380 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 21:45:46.414038   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414124   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414219   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414297   380 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 21:45:46.414368   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.414449   380 net.cpp:200] Created Layer ctx_output5 (58)
I0511 21:45:46.414530   380 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 21:45:46.414602   380 net.cpp:542] ctx_output5 -> ctx_output5
I0511 21:45:46.417708   380 net.cpp:260] Setting up ctx_output5
I0511 21:45:46.420851   380 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 21:45:46.420959   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 21:45:46.421047   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.421136   380 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 21:45:46.421221   380 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 21:45:46.421301   380 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 21:45:46.421387   380 net.cpp:260] Setting up ctx_output5/relu
I0511 21:45:46.421470   380 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 21:45:46.421561   380 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 21:45:46.421645   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.421730   380 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 21:45:46.421815   380 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 21:45:46.421896   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.421986   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.422076   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.422235   380 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 21:45:46.422394   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422482   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422571   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422660   380 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 21:45:46.422740   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.422830   380 net.cpp:200] Created Layer ctx_output6 (61)
I0511 21:45:46.422920   380 net.cpp:572] ctx_output6 <- pool9
I0511 21:45:46.423007   380 net.cpp:542] ctx_output6 -> ctx_output6
I0511 21:45:46.427263   380 net.cpp:260] Setting up ctx_output6
I0511 21:45:46.431545   380 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 21:45:46.431653   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 21:45:46.431731   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.431807   380 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 21:45:46.431881   380 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 21:45:46.431954   380 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 21:45:46.432031   380 net.cpp:260] Setting up ctx_output6/relu
I0511 21:45:46.432104   380 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 21:45:46.432181   380 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 21:45:46.432261   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.432353   380 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 21:45:46.432427   380 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 21:45:46.432497   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.432571   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.432646   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.432790   380 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 21:45:46.432934   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433012   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433086   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433162   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.433234   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.433321   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 21:45:46.433405   380 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.433480   380 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 21:45:46.433954   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 21:45:46.434432   380 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 21:45:46.434518   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.434594   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.434672   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 21:45:46.434748   380 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 21:45:46.434821   380 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435019   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435214   380 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 21:45:46.435290   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.435359   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.435434   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 21:45:46.435506   380 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435580   380 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.439497   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.440151   380 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 21:45:46.440250   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.440335   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.440433   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 21:45:46.440531   380 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.440614   380 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 21:45:46.441155   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 21:45:46.441709   380 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 21:45:46.441807   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.441905   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.442016   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 21:45:46.442101   380 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 21:45:46.442183   380 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.442391   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.442602   380 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 21:45:46.442690   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.442773   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.442858   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 21:45:46.442943   380 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.443024   380 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.445452   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.447906   380 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 21:45:46.447994   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.448065   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.448143   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 21:45:46.448220   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.448293   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 21:45:46.448367   380 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 21:45:46.448479   380 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 21:45:46.448587   380 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 21:45:46.448662   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.448734   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.448812   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 21:45:46.448892   380 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.448964   380 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 21:45:46.449771   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 21:45:46.450063   380 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 21:45:46.450150   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.450225   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.450304   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 21:45:46.450381   380 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 21:45:46.450453   380 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.450654   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.450851   380 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 21:45:46.450928   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.451002   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.451074   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 21:45:46.451156   380 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.451232   380 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.451414   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.451576   380 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 21:45:46.451651   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.451722   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.451802   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 21:45:46.451884   380 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.451954   380 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 21:45:46.452471   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 21:45:46.452996   380 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 21:45:46.453079   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.453153   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.453228   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 21:45:46.453307   380 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 21:45:46.453382   380 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.453584   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.453783   380 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 21:45:46.453862   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.453933   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.454005   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 21:45:46.454082   380 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.454157   380 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.454301   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.454447   380 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 21:45:46.454524   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.454596   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.454669   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 21:45:46.454748   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.454824   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 21:45:46.454898   380 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 21:45:46.455003   380 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 21:45:46.455102   380 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 21:45:46.455179   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.455250   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.455328   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 21:45:46.455406   380 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.455482   380 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 21:45:46.456003   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 21:45:46.456535   380 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 21:45:46.456629   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.456719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.456799   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 21:45:46.456872   380 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 21:45:46.456944   380 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457147   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457350   380 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 21:45:46.457448   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.457530   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.457618   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 21:45:46.457708   380 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457789   380 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.457948   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.458106   380 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 21:45:46.458191   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.458273   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.458364   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 21:45:46.458456   380 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.458536   380 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 21:45:46.459113   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 21:45:46.459692   380 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 21:45:46.459789   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.459877   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.459962   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 21:45:46.460048   380 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 21:45:46.460134   380 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460350   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460566   380 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 21:45:46.460656   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.460737   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.460824   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 21:45:46.460907   380 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460992   380 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.461140   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.461285   380 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 21:45:46.461385   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.461467   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.461553   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 21:45:46.461644   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.461731   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 21:45:46.461838   380 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 21:45:46.461951   380 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 21:45:46.462060   380 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 21:45:46.462146   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.462226   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.462316   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 21:45:46.462404   380 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.462500   380 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 21:45:46.463073   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 21:45:46.463649   380 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 21:45:46.463747   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.463832   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.463920   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 21:45:46.464005   380 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 21:45:46.464087   380 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464315   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464537   380 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 21:45:46.464624   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.464704   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.464789   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 21:45:46.464874   380 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464958   380 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.465103   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.465245   380 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 21:45:46.465339   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.465421   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.465513   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 21:45:46.465602   380 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.465690   380 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 21:45:46.466259   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 21:45:46.466832   380 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 21:45:46.466926   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.467011   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.467101   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 21:45:46.467187   380 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 21:45:46.467270   380 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.467486   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.467700   380 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 21:45:46.467792   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.467875   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.467947   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 21:45:46.468020   380 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.468091   380 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.468228   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.468359   380 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 21:45:46.468444   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.468516   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.468591   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 21:45:46.468667   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.468739   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 21:45:46.468812   380 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 21:45:46.468909   380 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 21:45:46.469012   380 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 21:45:46.469100   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.469182   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.469272   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 21:45:46.469368   380 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.469451   380 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 21:45:46.469986   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 21:45:46.470520   380 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 21:45:46.470602   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.470674   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.470753   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 21:45:46.470830   380 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 21:45:46.470904   380 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471099   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471292   380 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 21:45:46.471370   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.471439   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.471513   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 21:45:46.471583   380 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471654   380 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.471788   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.471920   380 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 21:45:46.471994   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.472064   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.472142   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 21:45:46.472230   380 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.472318   380 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 21:45:46.472765   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 21:45:46.473215   380 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 21:45:46.473309   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.473377   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.473453   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 21:45:46.473531   380 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 21:45:46.473603   380 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.473795   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.473985   380 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 21:45:46.474062   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.474133   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.474203   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 21:45:46.474277   380 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.474347   380 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.474483   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.474617   380 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 21:45:46.474692   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.474761   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.474841   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 21:45:46.474916   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.474989   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 21:45:46.475062   380 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 21:45:46.475153   380 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 21:45:46.475246   380 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 21:45:46.475322   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.475391   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.475471   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 21:45:46.475553   380 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.475625   380 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 21:45:46.476089   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 21:45:46.476559   380 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 21:45:46.476644   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.476719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.476794   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 21:45:46.476868   380 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 21:45:46.476940   380 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477128   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477321   380 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 21:45:46.477439   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.477522   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.477610   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 21:45:46.477694   380 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477774   380 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.477926   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.478076   380 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 21:45:46.478161   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.478241   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.478338   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 21:45:46.478428   380 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.478510   380 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 21:45:46.479022   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 21:45:46.479907   380 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 21:45:46.479992   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.480057   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.480134   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 21:45:46.480199   380 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 21:45:46.480270   380 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480475   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480545   380 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 21:45:46.480613   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.480684   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.480751   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 21:45:46.480821   380 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480887   380 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.481024   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.481168   380 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 21:45:46.481252   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.481359   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.481446   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 21:45:46.481534   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.481616   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 21:45:46.481703   380 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 21:45:46.481806   380 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 21:45:46.481905   380 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 21:45:46.481990   380 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 21:45:46.482070   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.482169   380 net.cpp:200] Created Layer mbox_loc (106)
I0511 21:45:46.482254   380 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.482771   380 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.482846   380 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.482913   380 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.482985   380 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.483052   380 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.483121   380 net.cpp:542] mbox_loc -> mbox_loc
I0511 21:45:46.483220   380 net.cpp:260] Setting up mbox_loc
I0511 21:45:46.483289   380 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 21:45:46.483359   380 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 21:45:46.483428   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.483502   380 net.cpp:200] Created Layer mbox_conf (107)
I0511 21:45:46.483567   380 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.483639   380 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.483705   380 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.483775   380 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.483842   380 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.483916   380 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.483990   380 net.cpp:542] mbox_conf -> mbox_conf
I0511 21:45:46.484086   380 net.cpp:260] Setting up mbox_conf
I0511 21:45:46.484158   380 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 21:45:46.484230   380 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 21:45:46.484308   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.485137   380 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 21:45:46.485206   380 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 21:45:46.485270   380 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 21:45:46.485337   380 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 21:45:46.485410   380 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 21:45:46.485477   380 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 21:45:46.485549   380 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 21:45:46.485615   380 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 21:45:46.485723   380 net.cpp:260] Setting up mbox_priorbox
I0511 21:45:46.485792   380 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 21:45:46.485867   380 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 21:45:46.485934   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.486013   380 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 21:45:46.486090   380 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 21:45:46.486160   380 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 21:45:46.486246   380 net.cpp:260] Setting up mbox_conf_reshape
I0511 21:45:46.486306   380 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 21:45:46.486371   380 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 21:45:46.486433   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.486502   380 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 21:45:46.486562   380 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 21:45:46.486625   380 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 21:45:46.486780   380 net.cpp:260] Setting up mbox_conf_softmax
I0511 21:45:46.486853   380 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 21:45:46.486937   380 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 21:45:46.487015   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.487083   380 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 21:45:46.487144   380 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 21:45:46.487207   380 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 21:45:46.489516   380 net.cpp:260] Setting up mbox_conf_flatten
I0511 21:45:46.491750   380 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 21:45:46.491843   380 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 21:45:46.491915   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.492012   380 net.cpp:200] Created Layer detection_out (112)
I0511 21:45:46.492110   380 net.cpp:572] detection_out <- mbox_loc
I0511 21:45:46.492182   380 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 21:45:46.492255   380 net.cpp:572] detection_out <- mbox_priorbox
I0511 21:45:46.492327   380 net.cpp:542] detection_out -> detection_out
I0511 21:45:46.493100   380 net.cpp:260] Setting up detection_out
I0511 21:45:46.493896   380 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 21:45:46.493995   380 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 21:45:46.494489   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.494583   380 net.cpp:200] Created Layer detection_eval (113)
I0511 21:45:46.494654   380 net.cpp:572] detection_eval <- detection_out
I0511 21:45:46.494729   380 net.cpp:572] detection_eval <- label
I0511 21:45:46.494801   380 net.cpp:542] detection_eval -> detection_eval
I0511 21:45:46.495478   380 net.cpp:260] Setting up detection_eval
I0511 21:45:46.495854   380 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 21:45:46.495945   380 net.cpp:338] detection_eval does not need backward computation.
I0511 21:45:46.496026   380 net.cpp:338] detection_out does not need backward computation.
I0511 21:45:46.496107   380 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 21:45:46.496193   380 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 21:45:46.496277   380 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 21:45:46.496361   380 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 21:45:46.496450   380 net.cpp:338] mbox_conf does not need backward computation.
I0511 21:45:46.496537   380 net.cpp:338] mbox_loc does not need backward computation.
I0511 21:45:46.496623   380 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.496711   380 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.496793   380 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.496876   380 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 21:45:46.496958   380 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.497040   380 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.497125   380 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 21:45:46.497206   380 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.497298   380 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.497386   380 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.497470   380 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 21:45:46.497556   380 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.497648   380 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.497746   380 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 21:45:46.497828   380 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.497911   380 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.497992   380 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.498075   380 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 21:45:46.498162   380 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.498245   380 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.498327   380 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 21:45:46.498409   380 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.498494   380 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.498580   380 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.498663   380 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 21:45:46.498745   380 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.498831   380 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.498914   380 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 21:45:46.498994   380 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.499079   380 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.499161   380 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.499243   380 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 21:45:46.499325   380 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.499408   380 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.499491   380 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 21:45:46.499572   380 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.499644   380 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.499716   380 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.499790   380 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 21:45:46.499861   380 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.499936   380 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.500006   380 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 21:45:46.500082   380 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 21:45:46.500157   380 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 21:45:46.500228   380 net.cpp:338] ctx_output6 does not need backward computation.
I0511 21:45:46.500303   380 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 21:45:46.500373   380 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 21:45:46.500447   380 net.cpp:338] ctx_output5 does not need backward computation.
I0511 21:45:46.500517   380 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 21:45:46.500589   380 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 21:45:46.500667   380 net.cpp:338] ctx_output4 does not need backward computation.
I0511 21:45:46.500741   380 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 21:45:46.500828   380 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 21:45:46.500898   380 net.cpp:338] ctx_output3 does not need backward computation.
I0511 21:45:46.500969   380 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 21:45:46.501044   380 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 21:45:46.501114   380 net.cpp:338] ctx_output2 does not need backward computation.
I0511 21:45:46.501188   380 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 21:45:46.501260   380 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 21:45:46.501341   380 net.cpp:338] ctx_output1 does not need backward computation.
I0511 21:45:46.501426   380 net.cpp:338] pool9 does not need backward computation.
I0511 21:45:46.501509   380 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 21:45:46.501593   380 net.cpp:338] pool8 does not need backward computation.
I0511 21:45:46.501680   380 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 21:45:46.501763   380 net.cpp:338] pool7 does not need backward computation.
I0511 21:45:46.501844   380 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 21:45:46.501925   380 net.cpp:338] pool6 does not need backward computation.
I0511 21:45:46.502009   380 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 21:45:46.502091   380 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 21:45:46.502174   380 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 21:45:46.502256   380 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 21:45:46.502337   380 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 21:45:46.502418   380 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 21:45:46.502499   380 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 21:45:46.502578   380 net.cpp:338] pool4 does not need backward computation.
I0511 21:45:46.502660   380 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 21:45:46.502746   380 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 21:45:46.502825   380 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 21:45:46.502907   380 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 21:45:46.502991   380 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 21:45:46.503070   380 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 21:45:46.503139   380 net.cpp:338] pool3 does not need backward computation.
I0511 21:45:46.503211   380 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 21:45:46.503281   380 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 21:45:46.503356   380 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 21:45:46.503429   380 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 21:45:46.503504   380 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 21:45:46.503573   380 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 21:45:46.503643   380 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 21:45:46.503716   380 net.cpp:338] pool2 does not need backward computation.
I0511 21:45:46.503787   380 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 21:45:46.503859   380 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 21:45:46.503932   380 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 21:45:46.504002   380 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 21:45:46.504077   380 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 21:45:46.504164   380 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 21:45:46.504235   380 net.cpp:338] pool1 does not need backward computation.
I0511 21:45:46.504304   380 net.cpp:338] conv1b/relu does not need backward computation.
I0511 21:45:46.504379   380 net.cpp:338] conv1b/bn does not need backward computation.
I0511 21:45:46.504448   380 net.cpp:338] conv1b does not need backward computation.
I0511 21:45:46.504523   380 net.cpp:338] conv1a/relu does not need backward computation.
I0511 21:45:46.504591   380 net.cpp:338] conv1a/bn does not need backward computation.
I0511 21:45:46.504660   380 net.cpp:338] conv1a does not need backward computation.
I0511 21:45:46.504734   380 net.cpp:338] data/bias does not need backward computation.
I0511 21:45:46.504808   380 net.cpp:338] data_data_0_split does not need backward computation.
I0511 21:45:46.504880   380 net.cpp:338] data does not need backward computation.
I0511 21:45:46.504953   380 net.cpp:380] This network produces output detection_eval
I0511 21:45:46.505162   380 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 21:45:46.505416   380 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 21:45:46.505486   380 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 21:45:46.505555   380 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 21:45:46.505623   380 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 21:45:46.505692   380 net.cpp:421] Network initialization done.
I0511 21:45:46.506050   380 solver.cpp:55] Solver scaffolding done.
I0511 21:45:46.513011   380 caffe.cpp:158] Finetuning from /workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
I0511 21:45:46.527025   380 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 21:45:46.534432   380 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 21:45:46.534554   380 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 21:45:46.534783   380 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 21:45:46.534942   380 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.535185   380 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 21:45:46.535269   380 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 21:45:46.535419   380 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.535650   380 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 21:45:46.535728   380 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 21:45:46.535809   380 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.535976   380 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.536211   380 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.536289   380 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.536443   380 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.536674   380 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.536751   380 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 21:45:46.536826   380 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.537063   380 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.537307   380 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.537396   380 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.537587   380 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.537837   380 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.537915   380 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.537992   380 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 21:45:46.538069   380 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.538545   380 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.538815   380 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.538892   380 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.539188   380 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.539433   380 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.539515   380 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 21:45:46.539593   380 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.540920   380 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.541182   380 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.541261   380 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.542013   380 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.542279   380 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.542358   380 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.542434   380 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 21:45:46.542512   380 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 21:45:46.542590   380 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 21:45:46.542667   380 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 21:45:46.542744   380 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 21:45:46.542821   380 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 21:45:46.542898   380 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 21:45:46.542975   380 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 21:45:46.543145   380 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 21:45:46.543216   380 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 21:45:46.543285   380 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 21:45:46.543354   380 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 21:45:46.543648   380 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 21:45:46.543731   380 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 21:45:46.543803   380 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 21:45:46.543871   380 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 21:45:46.544159   380 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 21:45:46.544242   380 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 21:45:46.544312   380 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 21:45:46.544381   380 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 21:45:46.544672   380 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 21:45:46.544754   380 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 21:45:46.544839   380 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 21:45:46.544929   380 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 21:45:46.545212   380 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 21:45:46.545298   380 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 21:45:46.545372   380 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 21:45:46.545442   380 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 21:45:46.545727   380 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 21:45:46.545809   380 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 21:45:46.545881   380 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 21:45:46.545949   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.546025   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 21:45:46.546768   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.547029   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.547103   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.547173   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.547241   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 21:45:46.549500   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.550314   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 21:45:46.551229   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.551368   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.551482   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.551561   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.551640   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.551717   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 21:45:46.552065   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.552281   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.552363   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.552440   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.552518   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 21:45:46.559976   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.560619   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 21:45:46.561362   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.561511   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.561622   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.561691   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.561758   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.561825   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 21:45:46.562155   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.562381   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.562458   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.562525   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.562593   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 21:45:46.564708   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.565276   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 21:45:46.566826   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.567023   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.567152   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.567235   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.567313   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.567390   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 21:45:46.567761   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.567975   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.568055   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.568136   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.568222   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 21:45:46.594996   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.598479   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 21:45:46.599906   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.600064   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.600219   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.600311   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.600399   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.600487   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 21:45:46.600895   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.601161   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.601245   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.601333   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.601418   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 21:45:46.604529   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.605144   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 21:45:46.606446   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.606596   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.606735   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.606819   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.606909   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.606995   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 21:45:46.607367   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.607615   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.607702   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.607790   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.607924   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 21:45:46.610924   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.611598   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 21:45:46.612929   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.613085   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.613231   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.613323   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.613415   380 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 21:45:46.613502   380 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 21:45:46.613587   380 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 21:45:46.613677   380 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 21:45:46.630904   380 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 21:45:46.643815   380 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 21:45:46.643945   380 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 21:45:46.644137   380 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 21:45:46.644341   380 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.644647   380 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 21:45:46.644735   380 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 21:45:46.644918   380 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.645222   380 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 21:45:46.645305   380 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 21:45:46.645390   380 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.645604   380 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.645902   380 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.645987   380 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.646188   380 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.646478   380 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.646566   380 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 21:45:46.646651   380 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.646950   380 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.647256   380 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.647346   380 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.647585   380 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.647878   380 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.647965   380 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.648064   380 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 21:45:46.648183   380 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.648825   380 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.649161   380 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.649250   380 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.649665   380 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.649991   380 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.650080   380 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 21:45:46.650172   380 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.658277   380 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.658674   380 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.658762   380 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.659781   380 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.660140   380 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.660233   380 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.660318   380 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 21:45:46.660400   380 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 21:45:46.660488   380 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 21:45:46.660567   380 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 21:45:46.660645   380 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 21:45:46.660727   380 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 21:45:46.660807   380 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 21:45:46.660894   380 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 21:45:46.661178   380 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 21:45:46.661275   380 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 21:45:46.661379   380 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 21:45:46.661473   380 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 21:45:46.661918   380 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 21:45:46.662030   380 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 21:45:46.662133   380 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 21:45:46.662228   380 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 21:45:46.662672   380 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 21:45:46.662775   380 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 21:45:46.662859   380 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 21:45:46.662938   380 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 21:45:46.663318   380 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 21:45:46.663414   380 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 21:45:46.663503   380 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 21:45:46.663583   380 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 21:45:46.663957   380 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 21:45:46.664050   380 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 21:45:46.664145   380 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 21:45:46.664270   380 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 21:45:46.664722   380 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 21:45:46.664824   380 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 21:45:46.664909   380 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 21:45:46.664991   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.665079   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 21:45:46.665654   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.666043   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.666131   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.666224   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.666311   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 21:45:46.669927   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.670670   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 21:45:46.672155   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.672310   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.672458   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.672547   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.672642   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.672730   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 21:45:46.673192   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.673476   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.673569   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.673660   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.673746   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 21:45:46.677266   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.677953   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 21:45:46.679345   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.679528   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.679678   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.679766   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.679850   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.679940   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 21:45:46.680398   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.680662   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.680747   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.680824   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.680905   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 21:45:46.684139   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.684760   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 21:45:46.686022   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.686182   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.686327   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.686410   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.686488   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.686571   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 21:45:46.686995   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.687269   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.687357   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.687438   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.687517   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 21:45:46.690886   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.691555   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 21:45:46.692900   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.693017   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.693118   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.693197   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.693270   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.693367   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 21:45:46.693657   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.693861   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.693940   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.694015   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.694093   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 21:45:46.696245   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.696812   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 21:45:46.697512   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.697643   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.697746   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.697820   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.697890   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.697962   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 21:45:46.698228   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.698418   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.698490   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.698559   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.698630   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 21:45:46.700592   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.701179   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 21:45:46.701872   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.702011   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.702121   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.702199   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.702273   380 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 21:45:46.702347   380 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 21:45:46.702420   380 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 21:45:46.702495   380 net.cpp:1137] Ignoring source layer mbox_loss
I0511 21:45:46.702904   380 caffe.cpp:260] Starting Optimization
I0511 21:45:46.703333   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 21:45:46.719657   380 solver.cpp:455] Solving ssdJacintoNetV2
I0511 21:45:46.720780   380 solver.cpp:456] Learning Rate Policy: poly
I0511 21:45:46.720927   380 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 21:45:46.725401   380 solver.cpp:269] Initial Test started...
I0511 21:45:46.729791   380 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 21:45:46.733114   380 net.cpp:1071] Ignoring source layer mbox_loss
I0511 21:45:46.750828   425 common.cpp:528] NVML initialized, thread 425
I0511 21:45:46.801403   425 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 425
I0511 21:45:52.641427   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:46:21.546025   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:46:22.080286   380 solver.cpp:749] class AP 1: 0
I0511 21:46:22.096787   380 solver.cpp:749] class AP 2: 0
I0511 21:46:22.122843   380 solver.cpp:749] class AP 3: 0
I0511 21:46:22.122872   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0
I0511 21:46:22.122942   380 solver.cpp:274] Initial Test completed in 35.3925s
I0511 21:46:22.998708   380 solver.cpp:360] Iteration 0 (0.875677 s), 0/1129.4ep, loss = 17.8434
I0511 21:46:22.998790   380 solver.cpp:378]     Train net output #0: mbox_loss = 17.8434 (* 1 = 17.8434 loss)
I0511 21:46:22.998824   380 sgd_solver.cpp:172] Iteration 0, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I0511 21:46:23.004179   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.5 sparsity_achieved=0.584013 iter=0
W0511 21:46:23.004243   380 net.cpp:2654] conv1a ni=3 no=32
W0511 21:46:23.933059   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.229583
W0511 21:46:23.933159   380 net.cpp:2654] conv1b ni=32 no=32
W0511 21:46:24.291654   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.497396
W0511 21:46:24.291754   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0511 21:46:24.548342   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.5
W0511 21:46:24.548440   380 net.cpp:2654] res2a_branch2b ni=64 no=64
I0511 21:46:25.367794   386 data_reader.cpp:320] Restarting data pre-fetching
W0511 21:46:26.278247   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.489041
W0511 21:46:26.278638   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0511 21:46:26.911547   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.499932
W0511 21:46:26.911657   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0511 21:46:27.630895   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.499891
W0511 21:46:27.631026   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0511 21:46:28.542709   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.499736
W0511 21:46:28.542829   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0511 21:46:29.339414   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.498535
W0511 21:46:29.339459   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0511 21:46:30.473320   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.496552
W0511 21:46:30.473347   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0511 21:46:31.159669   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.564619
W0511 21:46:31.159693   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0511 21:46:31.159698   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0511 21:46:31.159704   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0511 21:46:31.159709   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0511 21:46:31.159714   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0511 21:46:31.159720   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0511 21:46:31.159729   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159735   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0511 21:46:31.159742   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159765   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159772   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159781   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159790   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159798   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159804   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159811   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0511 21:46:31.159817   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159826   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0511 21:46:31.159837   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 21:46:31.162729   380 solver.cpp:391] Sparsity after update:
I0511 21:46:31.163635   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0511 21:46:31.163641   380 net.cpp:2780] conv1a_param_0(0.23) 
I0511 21:46:31.163666   380 net.cpp:2780] conv1b_param_0(0.497) 
I0511 21:46:31.163671   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163681   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163688   380 net.cpp:2780] ctx_output1_param_0(0) 
I0511 21:46:31.163693   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163697   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163702   380 net.cpp:2780] ctx_output2_param_0(0) 
I0511 21:46:31.163707   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163712   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163718   380 net.cpp:2780] ctx_output3_param_0(0) 
I0511 21:46:31.163724   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163731   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163736   380 net.cpp:2780] ctx_output4_param_0(0) 
I0511 21:46:31.163741   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163746   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163753   380 net.cpp:2780] ctx_output5_param_0(0) 
I0511 21:46:31.163758   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163763   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163769   380 net.cpp:2780] ctx_output6_param_0(0) 
I0511 21:46:31.163775   380 net.cpp:2780] res2a_branch2a_param_0(0.5) 
I0511 21:46:31.163781   380 net.cpp:2780] res2a_branch2b_param_0(0.489) 
I0511 21:46:31.163786   380 net.cpp:2780] res3a_branch2a_param_0(0.5) 
I0511 21:46:31.163794   380 net.cpp:2780] res3a_branch2b_param_0(0.5) 
I0511 21:46:31.163800   380 net.cpp:2780] res4a_branch2a_param_0(0.5) 
I0511 21:46:31.163807   380 net.cpp:2780] res4a_branch2b_param_0(0.499) 
I0511 21:46:31.163812   380 net.cpp:2780] res5a_branch2a_param_0(0.497) 
I0511 21:46:31.163817   380 net.cpp:2780] res5a_branch2b_param_0(0.565) 
I0511 21:46:31.163822   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.21038e+06/3.10435e+06) 0.39
I0511 21:46:31.901993   380 solver.cpp:360] Iteration 1 (8.90313 s), 0.1/1129.4ep, loss = 16.3147
I0511 21:46:31.902081   380 solver.cpp:378]     Train net output #0: mbox_loss = 14.786 (* 1 = 14.786 loss)
I0511 21:46:32.064534   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 1.01G, req 0.74M)	t: 0 0 5.56
I0511 21:46:32.289000   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 1.01G, req 0.74M)	t: 0 2.5 5.65
I0511 21:46:32.534826   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 1 	(avail 1.01G, req 0.74M)	t: 0 2.52 7.91
I0511 21:46:32.650166   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 1.01G, req 0.74M)	t: 0 0.87 2.28
I0511 21:46:32.823280   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.32G 64/1 6 4 5 	(avail 0.69G, req 0.32G)	t: 0 1.9 3.22
I0511 21:46:32.931272   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.32G 128/4 6 4 0 	(avail 0.69G, req 0.32G)	t: 0 0.38 1.03
I0511 21:46:33.075250   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.32G 128/1 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 1.42 1.49
I0511 21:46:33.155385   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.32G 256/4 6 4 5 	(avail 0.69G, req 0.32G)	t: 0 0.32 0.49
I0511 21:46:33.314446   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.32G 256/1 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 1.28 1.28
I0511 21:46:33.387312   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.32G 512/4 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 0.26 0.29
I0511 21:46:33.495271   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.32G 128/1 1 1 0 	(avail 0.69G, req 0.32G)	t: 0 1.45 2.37
I0511 21:46:33.551244   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.32G 512/1 1 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.38 0.47
I0511 21:46:33.607254   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.32G 512/1 1 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.11 0.18
I0511 21:46:33.663237   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.32G 512/1 0 1 1 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.07
I0511 21:46:33.715263   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.32G 512/1 0 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.05 0.05
I0511 21:46:33.763232   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.32G 512/1 0 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.05 0.04
I0511 21:46:33.855271   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.32G 256/1 1 1 1 	(avail 0.69G, req 0.32G)	t: 0 0.73 2.05
I0511 21:46:33.951265   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.32G 256/1 1 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.74 2.07
I0511 21:46:33.999212   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.11
I0511 21:46:34.047281   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.11
I0511 21:46:34.087225   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.04
I0511 21:46:34.131233   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.03
I0511 21:46:34.171270   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.02
I0511 21:46:34.219235   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.267227   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.307215   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.355239   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.395215   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:35.128628   380 solver.cpp:360] Iteration 2 (3.22657 s), 0.1/1129.4ep, loss = 15.6573
I0511 21:46:35.128664   380 solver.cpp:378]     Train net output #0: mbox_loss = 14.3426 (* 1 = 14.3426 loss)
I0511 21:47:15.048736   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:47:53.209851   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:48:31.739748   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:49:10.025204   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:49:48.905861   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:50:07.818722   380 solver.cpp:354] Iteration 100 (0.460771 iter/s, 212.687s/98 iter), 5.6/1129.4ep, loss = 5.08009
I0511 21:50:07.819167   380 solver.cpp:378]     Train net output #0: mbox_loss = 5.69373 (* 1 = 5.69373 loss)
I0511 21:50:07.819371   380 sgd_solver.cpp:172] Iteration 100, lr = 0.00098015, m = 0.9, wd = 1e-05, gs = 1
I0511 21:50:43.786126   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:51:20.762719   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:52:04.629755   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:52:46.639966   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:53:30.403527   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:54:07.693262   380 solver.cpp:354] Iteration 200 (0.416889 iter/s, 239.872s/100 iter), 11.3/1129.4ep, loss = 4.06675
I0511 21:54:07.693459   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.85671 (* 1 = 3.85671 loss)
I0511 21:54:07.693507   380 sgd_solver.cpp:172] Iteration 200, lr = 0.000960596, m = 0.9, wd = 1e-05, gs = 1
I0511 21:54:13.241771   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:54:52.132915   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:55:37.447532   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:56:16.683847   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:57:02.713503   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:57:41.708932   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:58:01.006506   380 solver.cpp:354] Iteration 300 (0.428612 iter/s, 233.311s/100 iter), 16.9/1129.4ep, loss = 3.94508
I0511 21:58:01.006546   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.88616 (* 1 = 3.88616 loss)
I0511 21:58:01.006554   380 sgd_solver.cpp:172] Iteration 300, lr = 0.000941337, m = 0.9, wd = 1e-05, gs = 1
I0511 21:58:26.044611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:59:08.337473   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:59:45.354251   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:00:32.321157   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:05.956228   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:52.184914   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:57.573454   380 solver.cpp:354] Iteration 400 (0.422716 iter/s, 236.566s/100 iter), 22.6/1129.4ep, loss = 3.69972
I0511 22:01:57.573590   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.5853 (* 1 = 3.5853 loss)
I0511 22:01:57.573602   380 sgd_solver.cpp:172] Iteration 400, lr = 0.000922368, m = 0.9, wd = 1e-05, gs = 1
I0511 22:02:32.082953   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:03:14.627357   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:03:58.124053   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:04:38.828341   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:05:31.923715   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:02.066817   380 solver.cpp:637] Iteration 500, Testing net (#0)
I0511 22:06:08.480397   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:25.735750   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:25.989441   380 solver.cpp:749] class AP 1: 0.851463
I0511 22:06:26.010355   380 solver.cpp:749] class AP 2: 0.844034
I0511 22:06:26.013630   380 solver.cpp:749] class AP 3: 0.902044
I0511 22:06:26.013648   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.865847
I0511 22:06:26.013690   380 solver.cpp:284] Tests completed in 268.439s
I0511 22:06:26.698017   380 solver.cpp:354] Iteration 500 (0.372524 iter/s, 268.439s/100 iter), 28.2/1129.4ep, loss = 3.50596
I0511 22:06:26.698105   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.11099 (* 1 = 3.11099 loss)
I0511 22:06:26.698137   380 sgd_solver.cpp:172] Iteration 500, lr = 0.000903688, m = 0.9, wd = 1e-05, gs = 1
I0511 22:06:58.158004   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:07:35.389096   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:08:26.342031   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:07.359182   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:46.397260   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:59.481369   380 solver.cpp:354] Iteration 600 (0.469964 iter/s, 212.782s/100 iter), 33.9/1129.4ep, loss = 3.43911
I0511 22:09:59.481544   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.24994 (* 1 = 3.24994 loss)
I0511 22:09:59.481590   380 sgd_solver.cpp:172] Iteration 600, lr = 0.000885293, m = 0.9, wd = 1e-05, gs = 1
I0511 22:10:29.885285   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:11:11.987272   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:11:57.356365   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:12:37.101845   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:13:17.148389   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:14:02.032563   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:14:14.627190   380 solver.cpp:354] Iteration 700 (0.391934 iter/s, 255.145s/100 iter), 39.5/1129.4ep, loss = 3.36716
I0511 22:14:14.627347   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.26968 (* 1 = 3.26968 loss)
I0511 22:14:14.627391   380 sgd_solver.cpp:172] Iteration 700, lr = 0.00086718, m = 0.9, wd = 1e-05, gs = 1
I0511 22:14:37.594638   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:15:34.038246   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:16:11.919057   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:16:57.985312   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:17:38.787438   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:18:12.441474   380 solver.cpp:354] Iteration 800 (0.420496 iter/s, 237.814s/100 iter), 45.2/1129.4ep, loss = 3.20753
I0511 22:18:12.441648   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.62432 (* 1 = 3.62432 loss)
I0511 22:18:12.441720   380 sgd_solver.cpp:172] Iteration 800, lr = 0.000849347, m = 0.9, wd = 1e-05, gs = 1
I0511 22:18:19.131451   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:18:59.130355   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:19:38.248065   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:20:22.736457   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:20:59.781296   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:21:45.405323   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:22:04.609818   380 solver.cpp:354] Iteration 900 (0.430722 iter/s, 232.168s/100 iter), 50.8/1129.4ep, loss = 3.20883
I0511 22:22:04.610272   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.09849 (* 1 = 3.09849 loss)
I0511 22:22:04.610445   380 sgd_solver.cpp:172] Iteration 900, lr = 0.00083179, m = 0.9, wd = 1e-05, gs = 1
I0511 22:22:24.244216   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:23:14.587445   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:23:59.035549   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:24:37.702049   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:25:18.616324   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:25:59.149740   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:26:00.239434   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_1000.caffemodel
I0511 22:26:00.318750   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_1000.solverstate
I0511 22:26:00.352358   380 solver.cpp:637] Iteration 1000, Testing net (#0)
I0511 22:26:26.882102   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:26:27.479652   380 solver.cpp:749] class AP 1: 0.881073
I0511 22:26:27.533767   380 solver.cpp:749] class AP 2: 0.842335
I0511 22:26:27.539366   380 solver.cpp:749] class AP 3: 0.90211
I0511 22:26:27.539384   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.875172
I0511 22:26:27.539441   380 solver.cpp:284] Tests completed in 262.929s
I0511 22:26:28.131887   380 solver.cpp:354] Iteration 1000 (0.38033 iter/s, 262.929s/100 iter), 56.5/1129.4ep, loss = 3.18494
I0511 22:26:28.132262   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.06059 (* 1 = 3.06059 loss)
I0511 22:26:28.132462   380 sgd_solver.cpp:172] Iteration 1000, lr = 0.000814506, m = 0.9, wd = 1e-05, gs = 1
I0511 22:26:45.552937   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:27:26.108860   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:28:07.312378   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:28:57.275434   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:29:31.142695   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:30:00.924348   380 solver.cpp:354] Iteration 1100 (0.469942 iter/s, 212.792s/100 iter), 62.1/1129.4ep, loss = 3.14656
I0511 22:30:00.924497   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.01286 (* 1 = 3.01286 loss)
I0511 22:30:00.924542   380 sgd_solver.cpp:172] Iteration 1100, lr = 0.000797494, m = 0.9, wd = 1e-05, gs = 1
I0511 22:30:26.362645   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:31:05.960283   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:31:46.105351   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:32:30.511308   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:33:09.913506   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:33:55.848616   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:34:19.987020   380 solver.cpp:354] Iteration 1200 (0.386008 iter/s, 259.062s/100 iter), 67.8/1129.4ep, loss = 3.10649
I0511 22:34:19.987246   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.09124 (* 1 = 3.09124 loss)
I0511 22:34:19.987310   380 sgd_solver.cpp:172] Iteration 1200, lr = 0.000780749, m = 0.9, wd = 1e-05, gs = 1
I0511 22:34:32.029151   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:35:16.130746   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:35:57.764910   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:36:53.717509   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:37:33.048996   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:38:07.328758   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:38:09.071995   380 solver.cpp:354] Iteration 1300 (0.43652 iter/s, 229.085s/100 iter), 73.4/1129.4ep, loss = 3.04207
I0511 22:38:09.072032   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53374 (* 1 = 2.53374 loss)
I0511 22:38:09.072043   380 sgd_solver.cpp:172] Iteration 1300, lr = 0.000764269, m = 0.9, wd = 1e-05, gs = 1
I0511 22:38:51.081827   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:39:27.380702   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:40:15.551990   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:40:56.110198   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:41:44.151913   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:42:04.743002   380 solver.cpp:354] Iteration 1400 (0.424321 iter/s, 235.671s/100 iter), 79.1/1129.4ep, loss = 2.9028
I0511 22:42:04.743041   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.16925 (* 1 = 3.16925 loss)
I0511 22:42:04.743050   380 sgd_solver.cpp:172] Iteration 1400, lr = 0.000748052, m = 0.9, wd = 1e-05, gs = 1
I0511 22:42:23.253594   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:43:03.677249   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:43:48.909335   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:44:30.940543   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:11.500739   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:49.746834   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:57.984078   380 solver.cpp:637] Iteration 1500, Testing net (#0)
I0511 22:46:27.310767   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:46:27.983259   380 solver.cpp:749] class AP 1: 0.88929
I0511 22:46:27.994613   380 solver.cpp:749] class AP 2: 0.868738
I0511 22:46:27.998314   380 solver.cpp:749] class AP 3: 0.901085
I0511 22:46:27.998335   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.886371
I0511 22:46:27.998386   380 solver.cpp:284] Tests completed in 263.255s
I0511 22:46:28.861436   380 solver.cpp:354] Iteration 1500 (0.37986 iter/s, 263.255s/100 iter), 84.7/1129.4ep, loss = 2.88776
I0511 22:46:28.861608   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.7638 (* 1 = 2.7638 loss)
I0511 22:46:28.861662   380 sgd_solver.cpp:172] Iteration 1500, lr = 0.000732094, m = 0.9, wd = 1e-05, gs = 1
I0511 22:46:38.012153   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:47:21.871023   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:47:56.417906   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:48:41.931571   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:49:19.163460   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:49:59.534721   380 solver.cpp:354] Iteration 1600 (0.474671 iter/s, 210.672s/100 iter), 90.4/1129.4ep, loss = 2.79613
I0511 22:49:59.535276   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.83263 (* 1 = 2.83263 loss)
I0511 22:49:59.535456   380 sgd_solver.cpp:172] Iteration 1600, lr = 0.000716393, m = 0.9, wd = 1e-05, gs = 1
I0511 22:50:06.111376   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:50:40.529433   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:51:31.187630   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:52:13.043351   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:52:52.419611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:53:34.819511   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:53:57.594475   380 solver.cpp:354] Iteration 1700 (0.420065 iter/s, 238.058s/100 iter), 96/1129.4ep, loss = 2.84702
I0511 22:53:57.594514   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.999 (* 1 = 2.999 loss)
I0511 22:53:57.594524   380 sgd_solver.cpp:172] Iteration 1700, lr = 0.000700946, m = 0.9, wd = 1e-05, gs = 1
I0511 22:54:11.547266   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:54:53.910516   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:55:32.299279   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:56:15.066823   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:56:56.904923   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:57:34.897579   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:57:52.261819   380 solver.cpp:354] Iteration 1800 (0.426137 iter/s, 234.666s/100 iter), 101.6/1129.4ep, loss = 2.88403
I0511 22:57:52.262017   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.06196 (* 1 = 3.06196 loss)
I0511 22:57:52.262084   380 sgd_solver.cpp:172] Iteration 1800, lr = 0.00068575, m = 0.9, wd = 1e-05, gs = 1
I0511 22:58:31.483157   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:59:11.234608   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:59:47.568879   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:00:21.941752   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:01:00.834185   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:01:40.379721   380 solver.cpp:354] Iteration 1900 (0.438372 iter/s, 228.117s/100 iter), 107.3/1129.4ep, loss = 2.81459
I0511 23:01:40.379788   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60571 (* 1 = 2.60571 loss)
I0511 23:01:40.379798   380 sgd_solver.cpp:172] Iteration 1900, lr = 0.000670802, m = 0.9, wd = 1e-05, gs = 1
I0511 23:01:45.764611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:02:23.773350   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:03:15.836768   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:03:52.831061   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:04:49.744917   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:05:25.983264   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:05:41.228611   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 23:05:41.286837   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 23:05:41.326206   380 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 23:06:10.085602   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:06:10.563458   380 solver.cpp:749] class AP 1: 0.897847
I0511 23:06:10.566098   380 solver.cpp:749] class AP 2: 0.880573
I0511 23:06:10.567783   380 solver.cpp:749] class AP 3: 0.901148
I0511 23:06:10.567796   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893189
I0511 23:06:10.567833   380 solver.cpp:284] Tests completed in 270.187s
I0511 23:06:11.180995   380 solver.cpp:354] Iteration 2000 (0.370114 iter/s, 270.187s/100 iter), 112.9/1129.4ep, loss = 2.90048
I0511 23:06:11.181115   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61702 (* 1 = 2.61702 loss)
I0511 23:06:11.181162   380 sgd_solver.cpp:172] Iteration 2000, lr = 0.0006561, m = 0.9, wd = 1e-05, gs = 1
I0511 23:06:11.182655   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.55 sparsity_achieved=0.389898 iter=2000
W0511 23:06:11.182711   380 net.cpp:2654] conv1a ni=3 no=32
W0511 23:06:12.070410   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.25
W0511 23:06:12.070803   380 net.cpp:2654] conv1b ni=32 no=32
W0511 23:06:12.661587   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.538628
W0511 23:06:12.661717   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0511 23:06:13.434237   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.548611
W0511 23:06:13.434377   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0511 23:06:15.373550   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.54069
W0511 23:06:15.373591   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0511 23:06:16.388242   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.548611
W0511 23:06:16.388270   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0511 23:06:17.769129   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.548611
W0511 23:06:17.769151   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0511 23:06:18.676594   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.549479
W0511 23:06:18.676614   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0511 23:06:19.744892   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.547092
W0511 23:06:19.744912   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0511 23:06:21.542587   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.546757
W0511 23:06:21.542609   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0511 23:06:22.659111   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.549274
W0511 23:06:22.659132   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0511 23:06:22.659137   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0511 23:06:22.659143   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0511 23:06:22.659149   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0511 23:06:22.659154   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0511 23:06:22.659160   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0511 23:06:22.659173   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659179   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0511 23:06:22.659201   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659209   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659222   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659229   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659236   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659245   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659250   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659257   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0511 23:06:22.659272   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659278   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0511 23:06:22.659289   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 23:06:22.662211   380 solver.cpp:391] Sparsity after update:
I0511 23:06:22.663183   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0511 23:06:22.663192   380 net.cpp:2780] conv1a_param_0(0.25) 
I0511 23:06:22.663200   380 net.cpp:2780] conv1b_param_0(0.539) 
I0511 23:06:22.663218   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663223   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663242   380 net.cpp:2780] ctx_output1_param_0(0) 
I0511 23:06:22.663246   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663249   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663254   380 net.cpp:2780] ctx_output2_param_0(0) 
I0511 23:06:22.663259   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663262   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663269   380 net.cpp:2780] ctx_output3_param_0(0) 
I0511 23:06:22.663275   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663278   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663285   380 net.cpp:2780] ctx_output4_param_0(0) 
I0511 23:06:22.663290   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663295   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663300   380 net.cpp:2780] ctx_output5_param_0(0) 
I0511 23:06:22.663305   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663309   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663314   380 net.cpp:2780] ctx_output6_param_0(0) 
I0511 23:06:22.663319   380 net.cpp:2780] res2a_branch2a_param_0(0.549) 
I0511 23:06:22.663324   380 net.cpp:2780] res2a_branch2b_param_0(0.541) 
I0511 23:06:22.663329   380 net.cpp:2780] res3a_branch2a_param_0(0.549) 
I0511 23:06:22.663332   380 net.cpp:2780] res3a_branch2b_param_0(0.549) 
I0511 23:06:22.663336   380 net.cpp:2780] res4a_branch2a_param_0(0.549) 
I0511 23:06:22.663339   380 net.cpp:2780] res4a_branch2b_param_0(0.547) 
I0511 23:06:22.663347   380 net.cpp:2780] res5a_branch2a_param_0(0.547) 
I0511 23:06:22.663352   380 net.cpp:2780] res5a_branch2b_param_0(0.549) 
I0511 23:06:22.663355   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.28928e+06/3.10435e+06) 0.415
I0511 23:06:26.968063   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:07:08.982769   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:07:48.924650   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:08:32.735319   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:08.995749   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:48.425508   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:54.001981   380 solver.cpp:354] Iteration 2100 (0.448792 iter/s, 222.82s/100 iter), 118.6/1129.4ep, loss = 2.7656
I0511 23:09:54.002017   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61097 (* 1 = 2.61097 loss)
I0511 23:09:54.002027   380 sgd_solver.cpp:172] Iteration 2100, lr = 0.000641641, m = 0.9, wd = 1e-05, gs = 1
I0511 23:10:25.882692   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:11:07.183223   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:11:49.188369   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:12:26.681581   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:13:26.537501   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:13:56.643184   380 solver.cpp:354] Iteration 2200 (0.412132 iter/s, 242.64s/100 iter), 124.2/1129.4ep, loss = 2.86298
I0511 23:13:56.643621   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.95602 (* 1 = 2.95602 loss)
I0511 23:13:56.643819   380 sgd_solver.cpp:172] Iteration 2200, lr = 0.000627422, m = 0.9, wd = 1e-05, gs = 1
I0511 23:14:04.083290   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:14:51.854902   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:15:29.150980   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:16:12.861479   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:16:49.339495   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:17:27.793597   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:17:58.672216   380 solver.cpp:354] Iteration 2300 (0.413175 iter/s, 242.028s/100 iter), 129.9/1129.4ep, loss = 2.77348
I0511 23:17:58.672421   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.77719 (* 1 = 2.77719 loss)
I0511 23:17:58.672482   380 sgd_solver.cpp:172] Iteration 2300, lr = 0.000613441, m = 0.9, wd = 1e-05, gs = 1
I0511 23:18:13.328398   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:18:52.190073   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:19:45.094656   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:20:23.562669   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:21:10.713533   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:21:49.973322   380 solver.cpp:354] Iteration 2400 (0.432338 iter/s, 231.3s/100 iter), 135.5/1129.4ep, loss = 2.72122
I0511 23:21:49.973412   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47747 (* 1 = 2.47747 loss)
I0511 23:21:49.973438   380 sgd_solver.cpp:172] Iteration 2400, lr = 0.000599695, m = 0.9, wd = 1e-05, gs = 1
I0511 23:21:52.867935   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:22:28.632994   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:23:10.311792   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:23:49.549851   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:24:32.578976   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:25:12.752198   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:25:45.686859   380 solver.cpp:637] Iteration 2500, Testing net (#0)
I0511 23:25:53.278416   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:26:10.875247   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:26:11.730819   380 solver.cpp:749] class AP 1: 0.899247
I0511 23:26:11.732950   380 solver.cpp:749] class AP 2: 0.881324
I0511 23:26:11.733897   380 solver.cpp:749] class AP 3: 0.902666
I0511 23:26:11.733909   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894412
I0511 23:26:11.733942   380 solver.cpp:284] Tests completed in 261.762s
I0511 23:26:12.395884   380 solver.cpp:354] Iteration 2500 (0.382026 iter/s, 261.762s/100 iter), 141.2/1129.4ep, loss = 2.84025
I0511 23:26:12.395969   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.9878 (* 1 = 2.9878 loss)
I0511 23:26:12.396001   380 sgd_solver.cpp:172] Iteration 2500, lr = 0.000586182, m = 0.9, wd = 1e-05, gs = 1
I0511 23:26:45.407070   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:27:20.784235   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:28:10.744758   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:28:51.855731   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:29:27.997948   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:29:43.873634   380 solver.cpp:354] Iteration 2600 (0.472861 iter/s, 211.479s/100 iter), 146.8/1129.4ep, loss = 2.74006
I0511 23:29:43.873716   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.6806 (* 1 = 2.6806 loss)
I0511 23:29:43.873742   380 sgd_solver.cpp:172] Iteration 2600, lr = 0.000572898, m = 0.9, wd = 1e-05, gs = 1
I0511 23:30:06.048714   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:30:46.617630   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:31:28.477247   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:32:10.740487   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:02.929312   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:37.388928   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:45.903798   380 solver.cpp:354] Iteration 2700 (0.413171 iter/s, 242.031s/100 iter), 152.5/1129.4ep, loss = 2.64169
I0511 23:33:45.904040   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.68634 (* 1 = 2.68634 loss)
I0511 23:33:45.904109   380 sgd_solver.cpp:172] Iteration 2700, lr = 0.000559841, m = 0.9, wd = 1e-05, gs = 1
I0511 23:34:29.028403   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:35:08.894178   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:35:47.070574   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:36:26.629539   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:37:04.359081   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:37:42.269634   380 solver.cpp:354] Iteration 2800 (0.423072 iter/s, 236.366s/100 iter), 158.1/1129.4ep, loss = 2.65224
I0511 23:37:42.270206   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.36787 (* 1 = 2.36787 loss)
I0511 23:37:42.270313   380 sgd_solver.cpp:172] Iteration 2800, lr = 0.000547008, m = 0.9, wd = 1e-05, gs = 1
I0511 23:37:51.326196   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:38:33.341886   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:39:20.142961   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:39:59.192663   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:40:42.338551   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:41:28.958895   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:41:33.686410   380 solver.cpp:354] Iteration 2900 (0.432121 iter/s, 231.417s/100 iter), 163.8/1129.4ep, loss = 2.63049
I0511 23:41:33.686769   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.90685 (* 1 = 2.90685 loss)
I0511 23:41:33.686784   380 sgd_solver.cpp:172] Iteration 2900, lr = 0.000534398, m = 0.9, wd = 1e-05, gs = 1
I0511 23:42:05.466763   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:42:45.522639   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:43:22.156185   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:44:07.320704   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:44:44.338208   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:34.165438   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:34.394577   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_3000.caffemodel
I0511 23:45:34.418658   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_3000.solverstate
I0511 23:45:34.442899   380 solver.cpp:637] Iteration 3000, Testing net (#0)
I0511 23:45:58.098038   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:58.667392   380 solver.cpp:749] class AP 1: 0.888234
I0511 23:45:58.669116   380 solver.cpp:749] class AP 2: 0.881476
I0511 23:45:58.669879   380 solver.cpp:749] class AP 3: 0.903607
I0511 23:45:58.669893   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891106
I0511 23:45:58.669929   380 solver.cpp:284] Tests completed in 264.984s
I0511 23:45:59.380784   380 solver.cpp:354] Iteration 3000 (0.377382 iter/s, 264.984s/100 iter), 169.4/1129.4ep, loss = 2.7778
I0511 23:45:59.380868   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.73399 (* 1 = 2.73399 loss)
I0511 23:45:59.380894   380 sgd_solver.cpp:172] Iteration 3000, lr = 0.000522006, m = 0.9, wd = 1e-05, gs = 1
I0511 23:46:19.517340   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:46:53.087132   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:47:35.475322   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:48:12.615372   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:49:04.852743   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:49:24.120959   380 solver.cpp:354] Iteration 3100 (0.488424 iter/s, 204.74s/100 iter), 175.1/1129.4ep, loss = 2.64164
I0511 23:49:24.121253   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53339 (* 1 = 2.53339 loss)
I0511 23:49:24.121384   380 sgd_solver.cpp:172] Iteration 3100, lr = 0.000509832, m = 0.9, wd = 1e-05, gs = 1
I0511 23:49:43.378356   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:50:23.451146   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:51:11.076329   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:51:48.454010   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:52:34.068723   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:53:14.722208   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:53:23.689450   380 solver.cpp:354] Iteration 3200 (0.417417 iter/s, 239.568s/100 iter), 180.7/1129.4ep, loss = 2.67375
I0511 23:53:23.689833   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.36061 (* 1 = 2.36061 loss)
I0511 23:53:23.690014   380 sgd_solver.cpp:172] Iteration 3200, lr = 0.000497871, m = 0.9, wd = 1e-05, gs = 1
I0511 23:53:51.517578   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:54:31.763139   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:55:11.999732   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:55:57.817662   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:56:33.243101   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:57:17.805027   380 solver.cpp:354] Iteration 3300 (0.42714 iter/s, 234.115s/100 iter), 186.4/1129.4ep, loss = 2.66382
I0511 23:57:17.805342   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.14024 (* 1 = 3.14024 loss)
I0511 23:57:17.805407   380 sgd_solver.cpp:172] Iteration 3300, lr = 0.000486123, m = 0.9, wd = 1e-05, gs = 1
I0511 23:57:23.981336   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:58:01.414300   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:58:37.225128   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:59:20.610926   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:59:58.795346   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:00:44.572506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:01:06.861644   380 solver.cpp:354] Iteration 3400 (0.436575 iter/s, 229.056s/100 iter), 192/1129.4ep, loss = 2.56589
I0512 00:01:06.862188   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46831 (* 1 = 2.46831 loss)
I0512 00:01:06.862443   380 sgd_solver.cpp:172] Iteration 3400, lr = 0.000474583, m = 0.9, wd = 1e-05, gs = 1
I0512 00:01:21.509789   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:02:09.187813   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:02:45.658895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:03:30.086712   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:04:10.998262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:04:50.490295   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:05:02.011380   380 solver.cpp:637] Iteration 3500, Testing net (#0)
I0512 00:05:28.300626   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:05:28.933239   380 solver.cpp:749] class AP 1: 0.88368
I0512 00:05:28.934778   380 solver.cpp:749] class AP 2: 0.872317
I0512 00:05:28.935155   380 solver.cpp:749] class AP 3: 0.903052
I0512 00:05:28.935163   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.886349
I0512 00:05:28.935191   380 solver.cpp:284] Tests completed in 262.073s
I0512 00:05:29.544144   380 solver.cpp:354] Iteration 3500 (0.381573 iter/s, 262.073s/100 iter), 197.6/1129.4ep, loss = 2.6441
I0512 00:05:29.544229   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.82891 (* 1 = 2.82891 loss)
I0512 00:05:29.544260   380 sgd_solver.cpp:172] Iteration 3500, lr = 0.00046325, m = 0.9, wd = 1e-05, gs = 1
I0512 00:05:41.379496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:06:19.428290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:06:57.669991   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:07:40.457116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:08:26.791290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:09:03.554411   380 solver.cpp:354] Iteration 3600 (0.467268 iter/s, 214.01s/100 iter), 203.3/1129.4ep, loss = 2.56633
I0512 00:09:03.554484   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.93594 (* 1 = 2.93594 loss)
I0512 00:09:03.554495   380 sgd_solver.cpp:172] Iteration 3600, lr = 0.000452122, m = 0.9, wd = 1e-05, gs = 1
I0512 00:09:09.457468   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:09:43.959610   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:10:44.147754   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:11:20.719260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:12:01.509384   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:12:42.186445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:13:10.385424   380 solver.cpp:354] Iteration 3700 (0.405136 iter/s, 246.831s/100 iter), 208.9/1129.4ep, loss = 2.52452
I0512 00:13:10.385459   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.65433 (* 1 = 2.65433 loss)
I0512 00:13:10.385468   380 sgd_solver.cpp:172] Iteration 3700, lr = 0.000441195, m = 0.9, wd = 1e-05, gs = 1
I0512 00:13:18.944260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:14:03.477363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:14:40.823678   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:15:26.753348   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:16:02.798437   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:17:04.629142   380 solver.cpp:354] Iteration 3800 (0.426907 iter/s, 234.243s/100 iter), 214.6/1129.4ep, loss = 2.51966
I0512 00:17:04.629246   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.03686 (* 1 = 3.03686 loss)
I0512 00:17:04.629273   380 sgd_solver.cpp:172] Iteration 3800, lr = 0.000430467, m = 0.9, wd = 1e-05, gs = 1
I0512 00:17:05.309358   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:17:47.298697   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:18:24.579331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:19:01.347045   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:19:41.815564   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:20:22.512291   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:20:58.006129   380 solver.cpp:354] Iteration 3900 (0.428492 iter/s, 233.377s/100 iter), 220.2/1129.4ep, loss = 2.57888
I0512 00:20:58.006188   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58013 (* 1 = 2.58013 loss)
I0512 00:20:58.006197   380 sgd_solver.cpp:172] Iteration 3900, lr = 0.000419936, m = 0.9, wd = 1e-05, gs = 1
I0512 00:20:59.617907   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:21:44.414579   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:22:25.717340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:23:13.284507   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:23:56.977728   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:24:36.236737   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:24:47.099014   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0512 00:24:47.154350   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0512 00:24:47.220252   380 solver.cpp:637] Iteration 4000, Testing net (#0)
I0512 00:25:17.687211   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:25:18.384301   380 solver.cpp:749] class AP 1: 0.897955
I0512 00:25:18.385728   380 solver.cpp:749] class AP 2: 0.882179
I0512 00:25:18.386346   380 solver.cpp:749] class AP 3: 0.900449
I0512 00:25:18.386356   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893528
I0512 00:25:18.386390   380 solver.cpp:284] Tests completed in 260.38s
I0512 00:25:18.999925   380 solver.cpp:354] Iteration 4000 (0.384054 iter/s, 260.38s/100 iter), 225.9/1129.4ep, loss = 2.54187
I0512 00:25:19.000003   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43002 (* 1 = 2.43002 loss)
I0512 00:25:19.000034   380 sgd_solver.cpp:172] Iteration 4000, lr = 0.0004096, m = 0.9, wd = 1e-05, gs = 1
I0512 00:25:19.001554   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.6 sparsity_achieved=0.415315 iter=4000
W0512 00:25:19.001595   380 net.cpp:2654] conv1a ni=3 no=32
W0512 00:25:19.999111   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.267917
W0512 00:25:19.999209   380 net.cpp:2654] conv1b ni=32 no=32
W0512 00:25:21.056505   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.590712
W0512 00:25:21.056607   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 00:25:22.421207   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.597222
W0512 00:25:22.421568   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 00:25:24.674854   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.580187
W0512 00:25:24.674901   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 00:25:26.327927   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.598958
W0512 00:25:26.327955   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 00:25:28.556594   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.597222
W0512 00:25:28.556618   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 00:25:29.489646   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.599826
W0512 00:25:29.489667   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 00:25:30.545923   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.597243
W0512 00:25:30.545943   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 00:25:32.338028   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.596279
W0512 00:25:32.338049   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 00:25:33.474155   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.599789
W0512 00:25:33.474177   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 00:25:33.474185   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 00:25:33.474211   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 00:25:33.474217   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 00:25:33.474229   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 00:25:33.474236   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 00:25:33.474251   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474263   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 00:25:33.474275   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474287   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474299   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474313   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474324   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474335   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474349   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474359   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 00:25:33.474365   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474373   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 00:25:33.474387   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 00:25:33.477280   380 solver.cpp:391] Sparsity after update:
I0512 00:25:33.478197   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 00:25:33.478204   380 net.cpp:2780] conv1a_param_0(0.268) 
I0512 00:25:33.478214   380 net.cpp:2780] conv1b_param_0(0.591) 
I0512 00:25:33.478235   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478241   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478265   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 00:25:33.478269   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478274   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478281   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 00:25:33.478286   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478291   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478296   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 00:25:33.478302   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478307   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478312   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 00:25:33.478317   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478320   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478327   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 00:25:33.478330   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478335   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478343   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 00:25:33.478345   380 net.cpp:2780] res2a_branch2a_param_0(0.597) 
I0512 00:25:33.478349   380 net.cpp:2780] res2a_branch2b_param_0(0.58) 
I0512 00:25:33.478353   380 net.cpp:2780] res3a_branch2a_param_0(0.599) 
I0512 00:25:33.478355   380 net.cpp:2780] res3a_branch2b_param_0(0.597) 
I0512 00:25:33.478360   380 net.cpp:2780] res4a_branch2a_param_0(0.6) 
I0512 00:25:33.478365   380 net.cpp:2780] res4a_branch2b_param_0(0.597) 
I0512 00:25:33.478370   380 net.cpp:2780] res5a_branch2a_param_0(0.596) 
I0512 00:25:33.478377   380 net.cpp:2780] res5a_branch2b_param_0(0.6) 
I0512 00:25:33.478381   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.40667e+06/3.10435e+06) 0.453
I0512 00:25:41.890031   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:26:19.934027   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:27:00.383478   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:27:39.531344   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:15.345149   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:56.722363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:59.517132   380 solver.cpp:354] Iteration 4100 (0.45348 iter/s, 220.517s/100 iter), 231.5/1129.4ep, loss = 2.5599
I0512 00:28:59.517355   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.5621 (* 1 = 2.5621 loss)
I0512 00:28:59.517426   380 sgd_solver.cpp:172] Iteration 4100, lr = 0.000399456, m = 0.9, wd = 1e-05, gs = 1
I0512 00:29:35.432925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:30:22.431565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:30:59.449494   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:31:59.734853   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:32:36.922135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:33:08.756563   380 solver.cpp:354] Iteration 4200 (0.401221 iter/s, 249.239s/100 iter), 237.2/1129.4ep, loss = 2.58784
I0512 00:33:08.757025   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.50377 (* 1 = 2.50377 loss)
I0512 00:33:08.757138   380 sgd_solver.cpp:172] Iteration 4200, lr = 0.000389501, m = 0.9, wd = 1e-05, gs = 1
I0512 00:33:15.244516   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:33:53.878259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:34:29.457478   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:35:45.235251   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:36:51.147622   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:38:06.437019   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:38:49.161453   380 solver.cpp:354] Iteration 4300 (0.293768 iter/s, 340.405s/100 iter), 242.8/1129.4ep, loss = 2.64839
I0512 00:38:49.161515   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.20717 (* 1 = 3.20717 loss)
I0512 00:38:49.161525   380 sgd_solver.cpp:172] Iteration 4300, lr = 0.000379733, m = 0.9, wd = 1e-05, gs = 1
I0512 00:39:08.013336   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:40:46.569418   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:41:52.466670   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:42:58.383314   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:43:59.158247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:45:03.231246   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:45:06.409447   380 solver.cpp:354] Iteration 4400 (0.265078 iter/s, 377.248s/100 iter), 248.5/1129.4ep, loss = 2.42696
I0512 00:45:06.410135   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71346 (* 1 = 2.71346 loss)
I0512 00:45:06.410580   380 sgd_solver.cpp:172] Iteration 4400, lr = 0.000370151, m = 0.9, wd = 1e-05, gs = 1
I0512 00:46:11.114528   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:47:15.570771   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:48:13.562254   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:49:20.250169   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:50:28.279299   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:51:22.421309   380 solver.cpp:637] Iteration 4500, Testing net (#0)
I0512 00:52:09.145686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:52:11.678988   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:52:12.886446   380 solver.cpp:749] class AP 1: 0.901306
I0512 00:52:12.888061   380 solver.cpp:749] class AP 2: 0.887872
I0512 00:52:12.888891   380 solver.cpp:749] class AP 3: 0.900151
I0512 00:52:12.888979   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896443
I0512 00:52:12.889042   380 solver.cpp:284] Tests completed in 426.479s
I0512 00:52:13.905397   380 solver.cpp:354] Iteration 4500 (0.234478 iter/s, 426.479s/100 iter), 254.1/1129.4ep, loss = 2.47644
I0512 00:52:13.905475   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.28363 (* 1 = 2.28363 loss)
I0512 00:52:13.905989   380 sgd_solver.cpp:172] Iteration 4500, lr = 0.00036075, m = 0.9, wd = 1e-05, gs = 1
I0512 00:53:06.347507   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:54:28.477331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:55:34.395138   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:56:34.613667   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:57:43.642853   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:58:05.879189   380 solver.cpp:354] Iteration 4600 (0.284112 iter/s, 351.974s/100 iter), 259.8/1129.4ep, loss = 2.48582
I0512 00:58:05.880385   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53226 (* 1 = 2.53226 loss)
I0512 00:58:05.881044   380 sgd_solver.cpp:172] Iteration 4600, lr = 0.00035153, m = 0.9, wd = 1e-05, gs = 1
I0512 00:58:50.233384   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:59:58.286686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:01:04.093536   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:02:33.850375   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:03:43.130002   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:04:57.689337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:05:00.288084   380 solver.cpp:354] Iteration 4700 (0.241308 iter/s, 414.409s/100 iter), 265.4/1129.4ep, loss = 2.6142
I0512 01:05:00.288306   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.74339 (* 1 = 2.74339 loss)
I0512 01:05:00.288378   380 sgd_solver.cpp:172] Iteration 4700, lr = 0.000342488, m = 0.9, wd = 1e-05, gs = 1
I0512 01:06:17.668298   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:07:17.538868   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:08:32.459085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:09:35.858275   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:10:41.118965   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:11:32.169776   380 solver.cpp:354] Iteration 4800 (0.25518 iter/s, 391.881s/100 iter), 271.1/1129.4ep, loss = 2.37495
I0512 01:11:32.170899   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0665 (* 1 = 2.0665 loss)
I0512 01:11:32.171406   380 sgd_solver.cpp:172] Iteration 4800, lr = 0.000333622, m = 0.9, wd = 1e-05, gs = 1
I0512 01:11:48.141237   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:13:04.379415   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:14:17.595906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:15:19.940905   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:16:50.792737   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:17:57.526757   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:18:02.896693   380 solver.cpp:354] Iteration 4900 (0.255934 iter/s, 390.726s/100 iter), 276.7/1129.4ep, loss = 2.47929
I0512 01:18:02.897406   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71888 (* 1 = 2.71888 loss)
I0512 01:18:02.897801   380 sgd_solver.cpp:172] Iteration 4900, lr = 0.000324929, m = 0.9, wd = 1e-05, gs = 1
I0512 01:19:00.090277   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:20:11.640247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:21:12.851711   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:22:18.052014   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:23:17.748912   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:24:24.445372   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_5000.caffemodel
I0512 01:24:24.561967   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_5000.solverstate
I0512 01:24:24.635025   380 solver.cpp:637] Iteration 5000, Testing net (#0)
I0512 01:24:32.267335   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:25:13.569141   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:25:15.028043   380 solver.cpp:749] class AP 1: 0.897915
I0512 01:25:15.029959   380 solver.cpp:749] class AP 2: 0.877443
I0512 01:25:15.030586   380 solver.cpp:749] class AP 3: 0.900239
I0512 01:25:15.030632   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891866
I0512 01:25:15.030709   380 solver.cpp:284] Tests completed in 432.133s
I0512 01:25:15.869117   380 solver.cpp:354] Iteration 5000 (0.23141 iter/s, 432.133s/100 iter), 282.4/1129.4ep, loss = 2.42633
I0512 01:25:15.870133   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46512 (* 1 = 2.46512 loss)
I0512 01:25:15.870707   380 sgd_solver.cpp:172] Iteration 5000, lr = 0.000316406, m = 0.9, wd = 1e-05, gs = 1
I0512 01:25:46.670099   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:27:08.021355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:28:21.935257   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:29:23.425238   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:30:21.727969   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:30:46.149371   380 solver.cpp:354] Iteration 5100 (0.302774 iter/s, 330.28s/100 iter), 288/1129.4ep, loss = 2.59005
I0512 01:30:46.149914   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.67602 (* 1 = 2.67602 loss)
I0512 01:30:46.150158   380 sgd_solver.cpp:172] Iteration 5100, lr = 0.000308053, m = 0.9, wd = 1e-05, gs = 1
I0512 01:31:04.598172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:31:44.917948   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:32:24.953783   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:33:15.006157   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:00.359247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:37.900871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:48.811710   380 solver.cpp:354] Iteration 5200 (0.412096 iter/s, 242.662s/100 iter), 293.6/1129.4ep, loss = 2.47679
I0512 01:34:48.811821   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72997 (* 1 = 2.72997 loss)
I0512 01:34:48.811863   380 sgd_solver.cpp:172] Iteration 5200, lr = 0.000299866, m = 0.9, wd = 1e-05, gs = 1
I0512 01:35:22.147125   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:35:59.396337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:36:52.705267   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:37:28.802376   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:38:06.915724   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:38:44.870774   380 solver.cpp:354] Iteration 5300 (0.423623 iter/s, 236.059s/100 iter), 299.3/1129.4ep, loss = 2.4971
I0512 01:38:44.871619   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47255 (* 1 = 2.47255 loss)
I0512 01:38:44.871984   380 sgd_solver.cpp:172] Iteration 5300, lr = 0.000291843, m = 0.9, wd = 1e-05, gs = 1
I0512 01:38:45.288408   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:39:23.111728   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:40:13.466006   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:40:54.115346   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:41:38.392027   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:42:18.859532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:42:49.373077   380 solver.cpp:354] Iteration 5400 (0.408996 iter/s, 244.501s/100 iter), 304.9/1129.4ep, loss = 2.40164
I0512 01:42:49.373296   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18969 (* 1 = 2.18969 loss)
I0512 01:42:49.373342   380 sgd_solver.cpp:172] Iteration 5400, lr = 0.000283982, m = 0.9, wd = 1e-05, gs = 1
I0512 01:43:12.293882   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:43:56.938295   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:44:34.046520   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:45:13.077294   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:45:50.584296   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:46:42.368672   380 solver.cpp:637] Iteration 5500, Testing net (#0)
I0512 01:46:45.264559   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:47:11.358120   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:47:12.487283   380 solver.cpp:749] class AP 1: 0.895555
I0512 01:47:12.487982   380 solver.cpp:749] class AP 2: 0.875854
I0512 01:47:12.488297   380 solver.cpp:749] class AP 3: 0.902538
I0512 01:47:12.488303   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891316
I0512 01:47:12.488332   380 solver.cpp:284] Tests completed in 263.114s
I0512 01:47:13.092278   380 solver.cpp:354] Iteration 5500 (0.380063 iter/s, 263.114s/100 iter), 310.6/1129.4ep, loss = 2.44502
I0512 01:47:13.093457   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.5611 (* 1 = 2.5611 loss)
I0512 01:47:13.093531   380 sgd_solver.cpp:172] Iteration 5500, lr = 0.000276282, m = 0.9, wd = 1e-05, gs = 1
I0512 01:47:26.430277   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:48:07.747608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:48:49.451440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:49:26.062801   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:50:23.291360   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:51:02.309056   380 solver.cpp:354] Iteration 5600 (0.43627 iter/s, 229.216s/100 iter), 316.2/1129.4ep, loss = 2.4636
I0512 01:51:02.309507   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47253 (* 1 = 2.47253 loss)
I0512 01:51:02.309653   380 sgd_solver.cpp:172] Iteration 5600, lr = 0.000268739, m = 0.9, wd = 1e-05, gs = 1
I0512 01:51:07.505848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:52:05.297349   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:52:40.907613   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:53:20.685048   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:53:58.284906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:54:39.758596   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:55:10.406332   380 solver.cpp:354] Iteration 5700 (0.403069 iter/s, 248.097s/100 iter), 321.9/1129.4ep, loss = 2.35871
I0512 01:55:10.406992   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46216 (* 1 = 2.46216 loss)
I0512 01:55:10.407331   380 sgd_solver.cpp:172] Iteration 5700, lr = 0.000261351, m = 0.9, wd = 1e-05, gs = 1
I0512 01:55:24.810925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:56:05.006271   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:56:49.403934   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:57:30.045353   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:58:35.759140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:59:14.579017   380 solver.cpp:354] Iteration 5800 (0.409547 iter/s, 244.172s/100 iter), 327.5/1129.4ep, loss = 2.36464
I0512 01:59:14.579270   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35028 (* 1 = 2.35028 loss)
I0512 01:59:14.579339   380 sgd_solver.cpp:172] Iteration 5800, lr = 0.000254117, m = 0.9, wd = 1e-05, gs = 1
I0512 01:59:17.267081   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:59:55.256178   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:00:31.821322   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:01:07.483500   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:01:53.408804   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:02:34.153854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:03:07.722275   380 solver.cpp:354] Iteration 5900 (0.428922 iter/s, 233.143s/100 iter), 333.2/1129.4ep, loss = 2.51474
I0512 02:03:07.722615   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46806 (* 1 = 2.46806 loss)
I0512 02:03:07.722698   380 sgd_solver.cpp:172] Iteration 5900, lr = 0.000247034, m = 0.9, wd = 1e-05, gs = 1
I0512 02:03:14.772049   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:04:01.791177   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:04:39.074381   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:05:34.150161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:12.605933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:53.141966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:59.611079   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0512 02:06:59.651274   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0512 02:06:59.691298   380 solver.cpp:637] Iteration 6000, Testing net (#0)
I0512 02:07:29.223470   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:07:30.474630   380 solver.cpp:749] class AP 1: 0.898383
I0512 02:07:30.475436   380 solver.cpp:749] class AP 2: 0.883382
I0512 02:07:30.475797   380 solver.cpp:749] class AP 3: 0.901935
I0512 02:07:30.475805   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894566
I0512 02:07:30.475832   380 solver.cpp:284] Tests completed in 262.753s
I0512 02:07:31.042992   380 solver.cpp:354] Iteration 6000 (0.380586 iter/s, 262.753s/100 iter), 338.8/1129.4ep, loss = 2.40074
I0512 02:07:31.043082   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25514 (* 1 = 2.25514 loss)
I0512 02:07:31.043117   380 sgd_solver.cpp:172] Iteration 6000, lr = 0.0002401, m = 0.9, wd = 1e-05, gs = 1
I0512 02:07:31.044647   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.65 sparsity_achieved=0.453127 iter=6000
W0512 02:07:31.044697   380 net.cpp:2654] conv1a ni=3 no=32
W0512 02:07:32.137281   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.285
W0512 02:07:32.137385   380 net.cpp:2654] conv1b ni=32 no=32
W0512 02:07:33.770866   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.624132
W0512 02:07:33.771011   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 02:07:35.376111   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.648763
W0512 02:07:35.376155   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 02:07:37.960913   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.610135
W0512 02:07:37.960958   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 02:07:40.682441   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.649306
W0512 02:07:40.682487   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 02:07:44.198565   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.648953
W0512 02:07:44.198611   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 02:07:45.186856   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.649306
W0512 02:07:45.186897   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 02:07:46.380535   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.647393
W0512 02:07:46.380578   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 02:07:48.179569   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.645804
W0512 02:07:48.179612   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 02:07:49.414454   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.649255
W0512 02:07:49.414499   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 02:07:49.414506   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 02:07:49.414511   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 02:07:49.414517   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 02:07:49.414523   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 02:07:49.414532   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 02:07:49.414539   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414549   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 02:07:49.414557   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414564   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414574   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414583   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414592   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414598   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414608   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414616   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 02:07:49.414624   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414633   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 02:07:49.414645   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 02:07:49.417568   380 solver.cpp:391] Sparsity after update:
I0512 02:07:49.418488   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 02:07:49.418496   380 net.cpp:2780] conv1a_param_0(0.285) 
I0512 02:07:49.418504   380 net.cpp:2780] conv1b_param_0(0.624) 
I0512 02:07:49.418524   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418529   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418531   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 02:07:49.418535   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418542   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418547   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 02:07:49.418552   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418558   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418563   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 02:07:49.418568   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418579   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418584   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 02:07:49.418589   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418594   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418599   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 02:07:49.418629   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418639   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418645   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 02:07:49.418649   380 net.cpp:2780] res2a_branch2a_param_0(0.649) 
I0512 02:07:49.418655   380 net.cpp:2780] res2a_branch2b_param_0(0.61) 
I0512 02:07:49.418660   380 net.cpp:2780] res3a_branch2a_param_0(0.649) 
I0512 02:07:49.418664   380 net.cpp:2780] res3a_branch2b_param_0(0.649) 
I0512 02:07:49.418669   380 net.cpp:2780] res4a_branch2a_param_0(0.649) 
I0512 02:07:49.418674   380 net.cpp:2780] res4a_branch2b_param_0(0.647) 
I0512 02:07:49.418682   380 net.cpp:2780] res5a_branch2a_param_0(0.646) 
I0512 02:07:49.418687   380 net.cpp:2780] res5a_branch2b_param_0(0.649) 
I0512 02:07:49.418691   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.52322e+06/3.10435e+06) 0.491
I0512 02:07:57.337417   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:08:35.339231   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:09:16.781379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:09:57.599200   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:10:40.014683   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:11:18.504251   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:11:21.065613   380 solver.cpp:354] Iteration 6100 (0.434741 iter/s, 230.022s/100 iter), 344.5/1129.4ep, loss = 2.37823
I0512 02:11:21.065686   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29026 (* 1 = 2.29026 loss)
I0512 02:11:21.065716   380 sgd_solver.cpp:172] Iteration 6100, lr = 0.000233313, m = 0.9, wd = 1e-05, gs = 1
I0512 02:12:00.484292   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:12:41.341153   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:13:31.993492   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:14:20.075188   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:15:00.228516   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:15:31.409224   380 solver.cpp:354] Iteration 6200 (0.39945 iter/s, 250.344s/100 iter), 350.1/1129.4ep, loss = 2.38103
I0512 02:15:31.409317   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0734 (* 1 = 2.0734 loss)
I0512 02:15:31.409343   380 sgd_solver.cpp:172] Iteration 6200, lr = 0.000226671, m = 0.9, wd = 1e-05, gs = 1
I0512 02:15:40.641471   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:16:17.975491   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:17:06.512168   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:17:46.844496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:18:25.919242   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:19:03.070468   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:19:19.619683   380 solver.cpp:354] Iteration 6300 (0.43819 iter/s, 228.211s/100 iter), 355.8/1129.4ep, loss = 2.44095
I0512 02:19:19.620252   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47413 (* 1 = 2.47413 loss)
I0512 02:19:19.620535   380 sgd_solver.cpp:172] Iteration 6300, lr = 0.000220172, m = 0.9, wd = 1e-05, gs = 1
I0512 02:19:42.132400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:20:31.852154   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:21:12.622151   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:21:53.386695   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:22:32.827358   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:23:17.012738   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:23:19.670917   380 solver.cpp:354] Iteration 6400 (0.416577 iter/s, 240.052s/100 iter), 361.4/1129.4ep, loss = 2.54247
I0512 02:23:19.671205   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53932 (* 1 = 2.53932 loss)
I0512 02:23:19.671219   380 sgd_solver.cpp:172] Iteration 6400, lr = 0.000213814, m = 0.9, wd = 1e-05, gs = 1
I0512 02:24:07.153723   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:24:46.166132   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:25:28.350947   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:26:04.832834   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:26:52.673930   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:16.427119   380 solver.cpp:637] Iteration 6500, Testing net (#0)
I0512 02:27:32.325964   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:41.824787   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:42.891902   380 solver.cpp:749] class AP 1: 0.899231
I0512 02:27:42.892781   380 solver.cpp:749] class AP 2: 0.876477
I0512 02:27:42.893106   380 solver.cpp:749] class AP 3: 0.900726
I0512 02:27:42.893113   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892145
I0512 02:27:42.893142   380 solver.cpp:284] Tests completed in 263.223s
I0512 02:27:43.521646   380 solver.cpp:354] Iteration 6500 (0.379907 iter/s, 263.223s/100 iter), 367.1/1129.4ep, loss = 2.38855
I0512 02:27:43.521730   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.23517 (* 1 = 2.23517 loss)
I0512 02:27:43.521760   380 sgd_solver.cpp:172] Iteration 6500, lr = 0.000207594, m = 0.9, wd = 1e-05, gs = 1
I0512 02:28:20.457262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:29:09.182498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:29:44.098693   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:30:26.720405   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:31:03.733336   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:31:13.881167   380 solver.cpp:354] Iteration 6600 (0.475376 iter/s, 210.36s/100 iter), 372.7/1129.4ep, loss = 2.47914
I0512 02:31:13.881321   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.06515 (* 1 = 2.06515 loss)
I0512 02:31:13.881364   380 sgd_solver.cpp:172] Iteration 6600, lr = 0.000201511, m = 0.9, wd = 1e-05, gs = 1
I0512 02:31:47.213681   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:32:26.804457   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:33:05.777323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:33:48.916040   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:34:25.905556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:35:08.179304   380 solver.cpp:354] Iteration 6700 (0.426807 iter/s, 234.298s/100 iter), 378.4/1129.4ep, loss = 2.54409
I0512 02:35:08.179548   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.89707 (* 1 = 2.89707 loss)
I0512 02:35:08.179615   380 sgd_solver.cpp:172] Iteration 6700, lr = 0.000195563, m = 0.9, wd = 1e-05, gs = 1
I0512 02:35:13.747311   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:35:56.936542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:36:38.242431   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:37:19.163228   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:37:55.278175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:38:40.550518   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:39:02.777071   380 solver.cpp:354] Iteration 6800 (0.426262 iter/s, 234.598s/100 iter), 384/1129.4ep, loss = 2.4659
I0512 02:39:02.777297   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55173 (* 1 = 2.55173 loss)
I0512 02:39:02.777372   380 sgd_solver.cpp:172] Iteration 6800, lr = 0.000189747, m = 0.9, wd = 1e-05, gs = 1
I0512 02:39:16.613379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:40:05.811794   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:40:43.814064   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:41:31.436228   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:42:14.553663   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:42:50.486626   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:43:02.620867   380 solver.cpp:354] Iteration 6900 (0.416938 iter/s, 239.844s/100 iter), 389.6/1129.4ep, loss = 2.30128
I0512 02:43:02.621024   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.21489 (* 1 = 2.21489 loss)
I0512 02:43:02.621069   380 sgd_solver.cpp:172] Iteration 6900, lr = 0.000184062, m = 0.9, wd = 1e-05, gs = 1
I0512 02:43:37.003266   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:44:13.428620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:45:02.111227   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:45:39.607476   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:46:19.494508   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:46:56.615309   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_7000.caffemodel
I0512 02:46:56.647401   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_7000.solverstate
I0512 02:46:56.711889   380 solver.cpp:637] Iteration 7000, Testing net (#0)
I0512 02:47:03.149209   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:47:26.042374   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:47:27.137480   380 solver.cpp:749] class AP 1: 0.897499
I0512 02:47:27.138204   380 solver.cpp:749] class AP 2: 0.884294
I0512 02:47:27.138494   380 solver.cpp:749] class AP 3: 0.901668
I0512 02:47:27.138504   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894487
I0512 02:47:27.138535   380 solver.cpp:284] Tests completed in 264.517s
I0512 02:47:27.695593   380 solver.cpp:354] Iteration 7000 (0.378047 iter/s, 264.517s/100 iter), 395.3/1129.4ep, loss = 2.31362
I0512 02:47:27.695677   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.21658 (* 1 = 2.21658 loss)
I0512 02:47:27.695708   380 sgd_solver.cpp:172] Iteration 7000, lr = 0.000178506, m = 0.9, wd = 1e-05, gs = 1
I0512 02:47:46.806356   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:48:34.450616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:49:09.251739   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:01.704924   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:40.268707   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:55.102490   380 solver.cpp:354] Iteration 7100 (0.482144 iter/s, 207.407s/100 iter), 400.9/1129.4ep, loss = 2.39177
I0512 02:50:55.102576   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.2713 (* 1 = 2.2713 loss)
I0512 02:50:55.102607   380 sgd_solver.cpp:172] Iteration 7100, lr = 0.000173077, m = 0.9, wd = 1e-05, gs = 1
I0512 02:51:24.675582   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:52:05.977023   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:52:47.077607   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:53:29.852808   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:54:07.301862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:54:51.198400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:55:01.143505   380 solver.cpp:354] Iteration 7200 (0.406437 iter/s, 246.041s/100 iter), 406.6/1129.4ep, loss = 2.45712
I0512 02:55:01.144268   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3552 (* 1 = 2.3552 loss)
I0512 02:55:01.144668   380 sgd_solver.cpp:172] Iteration 7200, lr = 0.000167772, m = 0.9, wd = 1e-05, gs = 1
I0512 02:55:27.655175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:56:15.547619   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:57:00.134402   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:57:37.289343   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:58:23.646422   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:58:56.136005   380 solver.cpp:354] Iteration 7300 (0.425546 iter/s, 234.992s/100 iter), 412.2/1129.4ep, loss = 2.34368
I0512 02:58:56.136322   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35763 (* 1 = 2.35763 loss)
I0512 02:58:56.136392   380 sgd_solver.cpp:172] Iteration 7300, lr = 0.00016259, m = 0.9, wd = 1e-05, gs = 1
I0512 02:59:03.511423   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:59:47.841972   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:00:28.883803   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:01:07.289131   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:01:52.231138   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:02:27.931105   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:02:55.459163   380 solver.cpp:354] Iteration 7400 (0.417845 iter/s, 239.323s/100 iter), 417.9/1129.4ep, loss = 2.28563
I0512 03:02:55.459203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31655 (* 1 = 2.31655 loss)
I0512 03:02:55.459213   380 sgd_solver.cpp:172] Iteration 7400, lr = 0.00015753, m = 0.9, wd = 1e-05, gs = 1
I0512 03:03:25.003631   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:04:01.221992   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:04:47.544104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:05:24.232769   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:06:01.468278   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:06:44.811024   380 solver.cpp:637] Iteration 7500, Testing net (#0)
I0512 03:06:47.519542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:07:13.366176   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:07:14.527379   380 solver.cpp:749] class AP 1: 0.895763
I0512 03:07:14.528084   380 solver.cpp:749] class AP 2: 0.884477
I0512 03:07:14.528374   380 solver.cpp:749] class AP 3: 0.900954
I0512 03:07:14.528383   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893731
I0512 03:07:14.528416   380 solver.cpp:284] Tests completed in 259.069s
I0512 03:07:15.087018   380 solver.cpp:354] Iteration 7500 (0.385997 iter/s, 259.069s/100 iter), 423.5/1129.4ep, loss = 2.39279
I0512 03:07:15.087107   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18968 (* 1 = 2.18968 loss)
I0512 03:07:15.087139   380 sgd_solver.cpp:172] Iteration 7500, lr = 0.000152588, m = 0.9, wd = 1e-05, gs = 1
I0512 03:07:26.689069   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:08:10.824625   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:08:45.930652   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:09:34.342669   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:10:09.833281   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:10:40.802503   380 solver.cpp:354] Iteration 7600 (0.486109 iter/s, 205.715s/100 iter), 429.2/1129.4ep, loss = 2.18991
I0512 03:10:40.802843   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19425 (* 1 = 2.19425 loss)
I0512 03:10:40.802923   380 sgd_solver.cpp:172] Iteration 7600, lr = 0.000147763, m = 0.9, wd = 1e-05, gs = 1
I0512 03:10:56.908025   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:11:37.434936   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:12:14.655290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:12:59.465934   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:13:33.625195   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:14:12.815861   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:14:41.563577   380 solver.cpp:354] Iteration 7700 (0.41535 iter/s, 240.761s/100 iter), 434.8/1129.4ep, loss = 2.40904
I0512 03:14:41.563728   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31752 (* 1 = 2.31752 loss)
I0512 03:14:41.563771   380 sgd_solver.cpp:172] Iteration 7700, lr = 0.000143054, m = 0.9, wd = 1e-05, gs = 1
I0512 03:14:55.705982   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:15:45.825004   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:16:29.093302   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:17:04.945158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:17:48.602969   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:18:27.172235   380 solver.cpp:354] Iteration 7800 (0.443246 iter/s, 225.608s/100 iter), 440.5/1129.4ep, loss = 2.32555
I0512 03:18:27.172441   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.05796 (* 1 = 2.05796 loss)
I0512 03:18:27.172492   380 sgd_solver.cpp:172] Iteration 7800, lr = 0.000138458, m = 0.9, wd = 1e-05, gs = 1
I0512 03:18:27.567354   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:19:07.835121   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:19:48.661317   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:20:28.069588   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:21:08.481639   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:21:43.226600   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:22:19.123342   380 solver.cpp:354] Iteration 7900 (0.431126 iter/s, 231.951s/100 iter), 446.1/1129.4ep, loss = 2.40362
I0512 03:22:19.123687   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.54337 (* 1 = 2.54337 loss)
I0512 03:22:19.123773   380 sgd_solver.cpp:172] Iteration 7900, lr = 0.000133974, m = 0.9, wd = 1e-05, gs = 1
I0512 03:22:36.546411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:23:16.599068   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:24:05.626708   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:24:43.629340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:25:24.582979   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:09.663314   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:18.795955   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0512 03:26:18.824919   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0512 03:26:18.868686   380 solver.cpp:637] Iteration 8000, Testing net (#0)
I0512 03:26:44.520768   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:45.819016   380 solver.cpp:749] class AP 1: 0.897608
I0512 03:26:45.819759   380 solver.cpp:749] class AP 2: 0.885291
I0512 03:26:45.820050   380 solver.cpp:749] class AP 3: 0.901436
I0512 03:26:45.820057   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894778
I0512 03:26:45.820087   380 solver.cpp:284] Tests completed in 266.691s
I0512 03:26:46.517526   380 solver.cpp:354] Iteration 8000 (0.374965 iter/s, 266.691s/100 iter), 451.8/1129.4ep, loss = 2.36739
I0512 03:26:46.517626   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40254 (* 1 = 2.40254 loss)
I0512 03:26:46.517653   380 sgd_solver.cpp:172] Iteration 8000, lr = 0.0001296, m = 0.9, wd = 1e-05, gs = 1
I0512 03:26:46.519505   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.7 sparsity_achieved=0.490671 iter=8000
W0512 03:26:46.519564   380 net.cpp:2654] conv1a ni=3 no=32
W0512 03:26:47.884352   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.302083
W0512 03:26:47.884447   380 net.cpp:2654] conv1b ni=32 no=32
W0512 03:26:49.876003   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.652344
W0512 03:26:49.876109   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 03:26:51.816432   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.693251
W0512 03:26:51.816476   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 03:26:54.638629   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.632161
W0512 03:26:54.638676   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 03:26:58.599370   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.699653
W0512 03:26:58.599416   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 03:27:03.321456   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.693115
W0512 03:27:03.321501   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 03:27:04.541266   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.699653
W0512 03:27:04.541312   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 03:27:06.170826   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.697544
W0512 03:27:06.170872   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 03:27:08.111204   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.695323
W0512 03:27:08.111256   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 03:27:09.490093   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.699592
W0512 03:27:09.490136   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 03:27:09.490142   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 03:27:09.490149   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 03:27:09.490154   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 03:27:09.490159   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 03:27:09.490164   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 03:27:09.490170   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490177   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 03:27:09.490183   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490190   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490211   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490219   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490229   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490236   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490244   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490250   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 03:27:09.490257   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490263   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 03:27:09.490272   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 03:27:09.493151   380 solver.cpp:391] Sparsity after update:
I0512 03:27:09.494045   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 03:27:09.494052   380 net.cpp:2780] conv1a_param_0(0.302) 
I0512 03:27:09.494060   380 net.cpp:2780] conv1b_param_0(0.652) 
I0512 03:27:09.494082   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494086   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494089   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 03:27:09.494092   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494096   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494099   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 03:27:09.494102   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494105   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494108   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 03:27:09.494113   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494117   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494128   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 03:27:09.494133   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494138   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494141   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 03:27:09.494146   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494151   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494156   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 03:27:09.494160   380 net.cpp:2780] res2a_branch2a_param_0(0.693) 
I0512 03:27:09.494168   380 net.cpp:2780] res2a_branch2b_param_0(0.632) 
I0512 03:27:09.494174   380 net.cpp:2780] res3a_branch2a_param_0(0.7) 
I0512 03:27:09.494179   380 net.cpp:2780] res3a_branch2b_param_0(0.693) 
I0512 03:27:09.494184   380 net.cpp:2780] res4a_branch2a_param_0(0.7) 
I0512 03:27:09.494189   380 net.cpp:2780] res4a_branch2b_param_0(0.698) 
I0512 03:27:09.494192   380 net.cpp:2780] res5a_branch2a_param_0(0.695) 
I0512 03:27:09.494197   380 net.cpp:2780] res5a_branch2b_param_0(0.7) 
I0512 03:27:09.494202   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.64003e+06/3.10435e+06) 0.528
I0512 03:27:17.393595   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:28:00.671474   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:28:34.868084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:29:22.881727   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:30:02.886854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:30:39.690645   380 solver.cpp:354] Iteration 8100 (0.428874 iter/s, 233.169s/100 iter), 457.4/1129.4ep, loss = 2.49695
I0512 03:30:39.690950   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.39197 (* 1 = 2.39197 loss)
I0512 03:30:39.691006   380 sgd_solver.cpp:172] Iteration 8100, lr = 0.000125334, m = 0.9, wd = 1e-05, gs = 1
I0512 03:30:41.673918   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:31:20.868306   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:32:00.966837   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:33:00.779994   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:33:37.856308   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:34:16.923619   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:34:49.851321   380 solver.cpp:354] Iteration 8200 (0.399749 iter/s, 250.157s/100 iter), 463.1/1129.4ep, loss = 2.53674
I0512 03:34:49.851662   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33144 (* 1 = 2.33144 loss)
I0512 03:34:49.851744   380 sgd_solver.cpp:172] Iteration 8200, lr = 0.000121174, m = 0.9, wd = 1e-05, gs = 1
I0512 03:35:00.245226   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:35:38.543448   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:36:26.458299   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:37:04.579043   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:37:40.072616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:38:17.089570   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:38:41.628803   380 solver.cpp:354] Iteration 8300 (0.431453 iter/s, 231.775s/100 iter), 468.7/1129.4ep, loss = 2.33841
I0512 03:38:41.629285   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.84869 (* 1 = 1.84869 loss)
I0512 03:38:41.629484   380 sgd_solver.cpp:172] Iteration 8300, lr = 0.000117118, m = 0.9, wd = 1e-05, gs = 1
I0512 03:39:14.493449   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:39:55.888578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:40:31.644776   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:41:19.291715   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:41:56.028820   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:42:38.358232   380 solver.cpp:354] Iteration 8400 (0.422427 iter/s, 236.727s/100 iter), 474.4/1129.4ep, loss = 2.3373
I0512 03:42:38.358451   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11929 (* 1 = 2.11929 loss)
I0512 03:42:38.358513   380 sgd_solver.cpp:172] Iteration 8400, lr = 0.000113165, m = 0.9, wd = 1e-05, gs = 1
I0512 03:42:43.653321   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:43:21.214802   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:43:58.160298   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:44:38.720536   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:45:18.189263   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:13.061955   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:29.102859   380 solver.cpp:637] Iteration 8500, Testing net (#0)
I0512 03:46:54.475260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:58.953804   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:47:00.598454   380 solver.cpp:749] class AP 1: 0.89757
I0512 03:47:00.599237   380 solver.cpp:749] class AP 2: 0.882906
I0512 03:47:00.599557   380 solver.cpp:749] class AP 3: 0.900166
I0512 03:47:00.599567   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893547
I0512 03:47:00.599599   380 solver.cpp:284] Tests completed in 262.239s
I0512 03:47:01.413116   380 solver.cpp:354] Iteration 8500 (0.381331 iter/s, 262.239s/100 iter), 480/1129.4ep, loss = 2.42511
I0512 03:47:01.413203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52729 (* 1 = 2.52729 loss)
I0512 03:47:01.413234   380 sgd_solver.cpp:172] Iteration 8500, lr = 0.000109313, m = 0.9, wd = 1e-05, gs = 1
I0512 03:47:46.690104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:48:23.432672   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:49:06.078362   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:49:47.503825   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:50:26.453498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:50:41.045670   380 solver.cpp:354] Iteration 8600 (0.455309 iter/s, 219.631s/100 iter), 485.6/1129.4ep, loss = 2.47804
I0512 03:50:41.045874   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55095 (* 1 = 2.55095 loss)
I0512 03:50:41.045920   380 sgd_solver.cpp:172] Iteration 8600, lr = 0.00010556, m = 0.9, wd = 1e-05, gs = 1
I0512 03:51:07.445318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:51:43.803124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:52:42.218966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:53:19.591488   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:53:53.739470   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:54:31.832521   380 solver.cpp:354] Iteration 8700 (0.433303 iter/s, 230.786s/100 iter), 491.3/1129.4ep, loss = 2.17636
I0512 03:54:31.832587   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.01066 (* 1 = 2.01066 loss)
I0512 03:54:31.832597   380 sgd_solver.cpp:172] Iteration 8700, lr = 0.000101905, m = 0.9, wd = 1e-05, gs = 1
I0512 03:54:33.863315   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:55:13.862351   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:56:06.395097   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:56:42.319245   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:57:23.943122   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:58:02.073696   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:58:25.842339   380 solver.cpp:354] Iteration 8800 (0.427333 iter/s, 234.009s/100 iter), 496.9/1129.4ep, loss = 2.26597
I0512 03:58:25.842375   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47366 (* 1 = 2.47366 loss)
I0512 03:58:25.842386   380 sgd_solver.cpp:172] Iteration 8800, lr = 9.8345e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 03:58:46.940827   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:59:31.486565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:00:10.049578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:00:52.036797   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:01:30.084602   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:02:19.907476   380 solver.cpp:354] Iteration 8900 (0.427228 iter/s, 234.067s/100 iter), 502.6/1129.4ep, loss = 2.32096
I0512 04:02:19.907789   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.63535 (* 1 = 2.63535 loss)
I0512 04:02:19.907861   380 sgd_solver.cpp:172] Iteration 8900, lr = 9.48794e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:02:21.475105   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:02:55.891996   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:03:37.904000   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:04:22.565506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:04:57.132908   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:05:45.763237   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:17.981604   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_9000.caffemodel
I0512 04:06:18.005462   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_9000.solverstate
I0512 04:06:18.027801   380 solver.cpp:637] Iteration 9000, Testing net (#0)
I0512 04:06:24.600473   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:41.067543   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:43.336433   380 solver.cpp:749] class AP 1: 0.897619
I0512 04:06:43.337236   380 solver.cpp:749] class AP 2: 0.886354
I0512 04:06:43.337545   380 solver.cpp:749] class AP 3: 0.900405
I0512 04:06:43.337553   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894793
I0512 04:06:43.337582   380 solver.cpp:284] Tests completed in 263.431s
I0512 04:06:43.981307   380 solver.cpp:354] Iteration 9000 (0.379606 iter/s, 263.431s/100 iter), 508.2/1129.4ep, loss = 2.34535
I0512 04:06:43.981343   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.12829 (* 1 = 2.12829 loss)
I0512 04:06:43.981351   380 sgd_solver.cpp:172] Iteration 9000, lr = 9.15063e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:07:15.315498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:07:51.825415   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:08:35.767640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:09:20.775972   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:09:56.172760   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:10:16.714072   380 solver.cpp:354] Iteration 9100 (0.470073 iter/s, 212.733s/100 iter), 513.9/1129.4ep, loss = 2.28868
I0512 04:10:16.714788   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.93488 (* 1 = 1.93488 loss)
I0512 04:10:16.715176   380 sgd_solver.cpp:172] Iteration 9100, lr = 8.82238e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:10:44.757681   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:11:24.644364   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:12:08.173871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:12:44.703150   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:13:31.190140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:14:17.660177   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:14:18.881845   380 solver.cpp:354] Iteration 9200 (0.412937 iter/s, 242.168s/100 iter), 519.5/1129.4ep, loss = 2.38187
I0512 04:14:18.882496   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46838 (* 1 = 2.46838 loss)
I0512 04:14:18.882824   380 sgd_solver.cpp:172] Iteration 9200, lr = 8.50305e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:14:58.226096   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:15:46.145756   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:16:23.331537   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:17:16.906608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:17:56.628084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:18:28.268833   380 solver.cpp:354] Iteration 9300 (0.400984 iter/s, 249.387s/100 iter), 525.2/1129.4ep, loss = 2.47833
I0512 04:18:28.269058   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.79426 (* 1 = 2.79426 loss)
I0512 04:18:28.269119   380 sgd_solver.cpp:172] Iteration 9300, lr = 8.19247e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:18:36.292224   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:19:15.597388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:19:54.318579   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:20:40.433630   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:21:18.065428   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:22:00.226513   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:22:23.438215   380 solver.cpp:354] Iteration 9400 (0.425226 iter/s, 235.169s/100 iter), 530.8/1129.4ep, loss = 2.34775
I0512 04:22:23.438251   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.57279 (* 1 = 2.57279 loss)
I0512 04:22:23.438261   380 sgd_solver.cpp:172] Iteration 9400, lr = 7.89048e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:22:39.044164   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:23:30.671339   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:24:15.297966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:24:52.300066   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:25:33.650758   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:09.076153   380 solver.cpp:637] Iteration 9500, Testing net (#0)
I0512 04:26:10.033074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:36.053133   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:37.611305   380 solver.cpp:749] class AP 1: 0.897164
I0512 04:26:37.612017   380 solver.cpp:749] class AP 2: 0.887014
I0512 04:26:37.612320   380 solver.cpp:749] class AP 3: 0.902297
I0512 04:26:37.612327   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895492
I0512 04:26:37.612354   380 solver.cpp:284] Tests completed in 254.174s
I0512 04:26:38.291482   380 solver.cpp:354] Iteration 9500 (0.393432 iter/s, 254.174s/100 iter), 536.5/1129.4ep, loss = 2.32927
I0512 04:26:38.291617   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18902 (* 1 = 2.18902 loss)
I0512 04:26:38.291666   380 sgd_solver.cpp:172] Iteration 9500, lr = 7.59691e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:26:55.160392   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:27:34.971447   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:28:15.618667   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:29:01.073982   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:29:41.493643   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:30:10.876461   380 solver.cpp:354] Iteration 9600 (0.470401 iter/s, 212.584s/100 iter), 542.1/1129.4ep, loss = 2.38155
I0512 04:30:10.876710   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38257 (* 1 = 2.38257 loss)
I0512 04:30:10.876776   380 sgd_solver.cpp:172] Iteration 9600, lr = 7.31161e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:30:37.339706   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:31:16.071599   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:32:01.316895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:32:39.146456   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:33:17.844205   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:33:55.999173   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:34:18.285456   380 solver.cpp:354] Iteration 9700 (0.40419 iter/s, 247.408s/100 iter), 547.8/1129.4ep, loss = 2.36669
I0512 04:34:18.285892   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41858 (* 1 = 2.41858 loss)
I0512 04:34:18.286020   380 sgd_solver.cpp:172] Iteration 9700, lr = 7.03443e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:34:34.332509   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:35:22.874950   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:36:01.235574   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:36:55.050454   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:37:32.878162   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:38:10.755548   380 solver.cpp:354] Iteration 9800 (0.430164 iter/s, 232.469s/100 iter), 553.4/1129.4ep, loss = 2.38713
I0512 04:38:10.755614   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33087 (* 1 = 2.33087 loss)
I0512 04:38:10.755622   380 sgd_solver.cpp:172] Iteration 9800, lr = 6.7652e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:38:11.685073   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:38:58.482578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:39:33.765489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:40:16.352944   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:40:53.765167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:41:39.234714   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:42:07.209664   380 solver.cpp:354] Iteration 9900 (0.422916 iter/s, 236.453s/100 iter), 559.1/1129.4ep, loss = 2.29045
I0512 04:42:07.209703   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22078 (* 1 = 2.22078 loss)
I0512 04:42:07.209713   380 sgd_solver.cpp:172] Iteration 9900, lr = 6.50378e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:42:18.775606   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:43:06.571552   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:43:50.888077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:44:27.324867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:05.549017   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:45.848542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:56.049854   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0512 04:45:56.080005   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0512 04:45:56.104878   380 solver.cpp:637] Iteration 10000, Testing net (#0)
I0512 04:46:23.322026   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:46:25.711717   380 solver.cpp:749] class AP 1: 0.896814
I0512 04:46:25.712582   380 solver.cpp:749] class AP 2: 0.889558
I0512 04:46:25.712939   380 solver.cpp:749] class AP 3: 0.900951
I0512 04:46:25.712949   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895774
I0512 04:46:25.712985   380 solver.cpp:284] Tests completed in 258.503s
I0512 04:46:26.315979   380 solver.cpp:354] Iteration 10000 (0.386843 iter/s, 258.503s/100 iter), 564.7/1129.4ep, loss = 2.3947
I0512 04:46:26.316064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35071 (* 1 = 2.35071 loss)
I0512 04:46:26.316097   380 sgd_solver.cpp:172] Iteration 10000, lr = 6.25e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:46:26.317803   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.75 sparsity_achieved=0.528301 iter=10000
W0512 04:46:26.317849   380 net.cpp:2654] conv1a ni=3 no=32
W0512 04:46:27.700212   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.319583
W0512 04:46:27.700350   380 net.cpp:2654] conv1b ni=32 no=32
W0512 04:46:29.601946   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.667101
W0512 04:46:29.601994   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 04:46:31.954355   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.733778
W0512 04:46:31.954398   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 04:46:34.964366   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.645833
W0512 04:46:34.964411   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 04:46:40.177001   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.74608
W0512 04:46:40.177047   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 04:46:45.732903   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.724745
W0512 04:46:45.732949   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 04:46:47.585765   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.75
W0512 04:46:47.585808   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 04:46:50.289683   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.747694
W0512 04:46:50.289726   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 04:46:52.512779   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.74526
W0512 04:46:52.512823   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 04:46:54.126011   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.749939
W0512 04:46:54.126070   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 04:46:54.126075   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 04:46:54.126081   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 04:46:54.126087   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 04:46:54.126092   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 04:46:54.126106   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 04:46:54.126111   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126118   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 04:46:54.126140   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126147   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126158   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126166   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126173   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126179   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126190   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126196   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 04:46:54.126204   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126209   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 04:46:54.126222   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 04:46:54.129120   380 solver.cpp:391] Sparsity after update:
I0512 04:46:54.130077   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 04:46:54.130086   380 net.cpp:2780] conv1a_param_0(0.32) 
I0512 04:46:54.130095   380 net.cpp:2780] conv1b_param_0(0.667) 
I0512 04:46:54.130097   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130101   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130105   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 04:46:54.130108   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130111   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130115   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 04:46:54.130118   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130121   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130125   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 04:46:54.130128   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130132   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130139   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 04:46:54.130144   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130152   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130156   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 04:46:54.130162   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130167   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130172   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 04:46:54.130179   380 net.cpp:2780] res2a_branch2a_param_0(0.734) 
I0512 04:46:54.130184   380 net.cpp:2780] res2a_branch2b_param_0(0.646) 
I0512 04:46:54.130189   380 net.cpp:2780] res3a_branch2a_param_0(0.746) 
I0512 04:46:54.130195   380 net.cpp:2780] res3a_branch2b_param_0(0.725) 
I0512 04:46:54.130198   380 net.cpp:2780] res4a_branch2a_param_0(0.75) 
I0512 04:46:54.130203   380 net.cpp:2780] res4a_branch2b_param_0(0.748) 
I0512 04:46:54.130205   380 net.cpp:2780] res5a_branch2a_param_0(0.745) 
I0512 04:46:54.130210   380 net.cpp:2780] res5a_branch2b_param_0(0.75) 
I0512 04:46:54.130214   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.75642e+06/3.10435e+06) 0.566
I0512 04:47:04.026461   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:47:42.293543   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:48:18.007519   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:49:02.739923   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:49:38.108216   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:50:17.342411   380 solver.cpp:354] Iteration 10100 (0.432852 iter/s, 231.026s/100 iter), 570.4/1129.4ep, loss = 2.32927
I0512 04:50:17.342597   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22941 (* 1 = 2.22941 loss)
I0512 04:50:17.342640   380 sgd_solver.cpp:172] Iteration 10100, lr = 6.00373e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:50:20.613549   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:50:58.246646   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:51:52.230434   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:52:35.205265   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:53:13.295640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:53:56.651862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:54:24.401233   380 solver.cpp:354] Iteration 10200 (0.404763 iter/s, 247.058s/100 iter), 576/1129.4ep, loss = 2.43863
I0512 04:54:24.401412   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.0371 (* 1 = 3.0371 loss)
I0512 04:54:24.401465   380 sgd_solver.cpp:172] Iteration 10200, lr = 5.7648e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:54:35.641506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:55:13.492697   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:55:52.306445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:56:31.541483   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:57:17.623597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:57:54.230746   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:58:13.791709   380 solver.cpp:354] Iteration 10300 (0.435939 iter/s, 229.39s/100 iter), 581.6/1129.4ep, loss = 2.43791
I0512 04:58:13.791882   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.70717 (* 1 = 2.70717 loss)
I0512 04:58:13.791939   380 sgd_solver.cpp:172] Iteration 10300, lr = 5.53308e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:58:54.651705   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:59:29.620061   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:00:07.606716   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:00:44.238610   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:01:21.053959   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:01:59.582717   380 solver.cpp:354] Iteration 10400 (0.442889 iter/s, 225.79s/100 iter), 587.3/1129.4ep, loss = 2.21844
I0512 05:01:59.583503   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.08876 (* 1 = 2.08876 loss)
I0512 05:01:59.583925   380 sgd_solver.cpp:172] Iteration 10400, lr = 5.30842e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:02:01.327734   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:02:38.461591   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:03:30.491304   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:04:06.099306   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:01.590224   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:39.963915   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:53.941879   380 solver.cpp:637] Iteration 10500, Testing net (#0)
I0512 05:06:18.894109   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:06:21.693877   380 solver.cpp:749] class AP 1: 0.895421
I0512 05:06:21.708896   380 solver.cpp:749] class AP 2: 0.890704
I0512 05:06:21.711741   380 solver.cpp:749] class AP 3: 0.901217
I0512 05:06:21.712734   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895781
I0512 05:06:21.712834   380 solver.cpp:284] Tests completed in 262.129s
I0512 05:06:22.824540   380 solver.cpp:354] Iteration 10500 (0.381491 iter/s, 262.129s/100 iter), 592.9/1129.4ep, loss = 2.3693
I0512 05:06:22.824630   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47519 (* 1 = 2.47519 loss)
I0512 05:06:22.824659   380 sgd_solver.cpp:172] Iteration 10500, lr = 5.09067e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:06:26.919333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:07:07.914069   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:07:43.175319   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:08:35.711709   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:09:14.252532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:09:55.657963   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:10:01.209481   380 solver.cpp:354] Iteration 10600 (0.457901 iter/s, 218.388s/100 iter), 598.6/1129.4ep, loss = 2.35791
I0512 05:10:01.209532   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.97221 (* 1 = 1.97221 loss)
I0512 05:10:01.209545   380 sgd_solver.cpp:172] Iteration 10600, lr = 4.87968e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:10:28.937144   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:11:16.933604   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:12:00.187927   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:12:35.605412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:13:22.758742   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:13:52.295043   380 solver.cpp:354] Iteration 10700 (0.432735 iter/s, 231.088s/100 iter), 604.2/1129.4ep, loss = 2.21944
I0512 05:13:52.295637   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.42109 (* 1 = 2.42109 loss)
I0512 05:13:52.295953   380 sgd_solver.cpp:172] Iteration 10700, lr = 4.67532e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:13:59.257160   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:14:45.193357   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:15:24.771397   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:16:04.701340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:16:45.088380   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:17:27.546627   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:17:51.631371   380 solver.cpp:354] Iteration 10800 (0.417819 iter/s, 239.338s/100 iter), 609.9/1129.4ep, loss = 2.34322
I0512 05:17:51.631536   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22657 (* 1 = 2.22657 loss)
I0512 05:17:51.631585   380 sgd_solver.cpp:172] Iteration 10800, lr = 4.47746e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:18:15.397620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:18:51.053941   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:19:37.359869   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:20:18.237088   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:04.266841   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:45.601585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:47.690296   380 solver.cpp:354] Iteration 10900 (0.423621 iter/s, 236.06s/100 iter), 615.5/1129.4ep, loss = 2.26057
I0512 05:21:47.690331   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.28003 (* 1 = 2.28003 loss)
I0512 05:21:47.690341   380 sgd_solver.cpp:172] Iteration 10900, lr = 4.28593e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:22:22.958495   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:23:09.518605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:23:48.710981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:24:33.674439   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:25:06.079530   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:25:36.023643   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_11000.caffemodel
I0512 05:25:36.087622   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_11000.solverstate
I0512 05:25:36.153710   380 solver.cpp:637] Iteration 11000, Testing net (#0)
I0512 05:25:46.254866   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:26:02.649729   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:26:05.895247   380 solver.cpp:749] class AP 1: 0.896072
I0512 05:26:05.896275   380 solver.cpp:749] class AP 2: 0.890738
I0512 05:26:05.896700   380 solver.cpp:749] class AP 3: 0.900932
I0512 05:26:05.896709   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895914
I0512 05:26:05.896744   380 solver.cpp:284] Tests completed in 258.207s
I0512 05:26:06.641871   380 solver.cpp:354] Iteration 11000 (0.387286 iter/s, 258.207s/100 iter), 621.2/1129.4ep, loss = 2.2689
I0512 05:26:06.642454   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.56788 (* 1 = 2.56788 loss)
I0512 05:26:06.642767   380 sgd_solver.cpp:172] Iteration 11000, lr = 4.10062e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:26:41.267556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:27:17.095515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:28:08.419981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:28:48.638711   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:29:24.349315   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:29:43.101438   380 solver.cpp:354] Iteration 11100 (0.461979 iter/s, 216.46s/100 iter), 626.8/1129.4ep, loss = 2.43016
I0512 05:29:43.101524   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40675 (* 1 = 2.40675 loss)
I0512 05:29:43.101557   380 sgd_solver.cpp:172] Iteration 11100, lr = 3.92139e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:30:03.228891   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:30:42.014852   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:31:25.862293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:32:05.414085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:32:57.188159   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:33:33.602154   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:33:42.497421   380 solver.cpp:354] Iteration 11200 (0.417718 iter/s, 239.396s/100 iter), 632.5/1129.4ep, loss = 2.2224
I0512 05:33:42.497505   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72 (* 1 = 2.72 loss)
I0512 05:33:42.497535   380 sgd_solver.cpp:172] Iteration 11200, lr = 3.7481e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:34:30.818600   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:35:08.251355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:35:43.669617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:36:22.749773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:37:01.802333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:37:34.361750   380 solver.cpp:354] Iteration 11300 (0.431286 iter/s, 231.865s/100 iter), 638.1/1129.4ep, loss = 2.29065
I0512 05:37:34.361814   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18509 (* 1 = 2.18509 loss)
I0512 05:37:34.361824   380 sgd_solver.cpp:172] Iteration 11300, lr = 3.58061e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:37:40.772616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:38:16.627167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:39:01.740345   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:39:41.658077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:40:26.172933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:41:11.747084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:41:21.665264   380 solver.cpp:354] Iteration 11400 (0.43994 iter/s, 227.304s/100 iter), 643.8/1129.4ep, loss = 2.35193
I0512 05:41:21.665997   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58476 (* 1 = 2.58476 loss)
I0512 05:41:21.666395   380 sgd_solver.cpp:172] Iteration 11400, lr = 3.4188e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:41:49.021987   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:42:31.677724   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:43:09.831244   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:43:48.723028   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:44:28.003379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:11.438060   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:11.715428   380 solver.cpp:637] Iteration 11500, Testing net (#0)
I0512 05:45:37.323577   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:40.198287   380 solver.cpp:749] class AP 1: 0.897706
I0512 05:45:40.198969   380 solver.cpp:749] class AP 2: 0.889887
I0512 05:45:40.199286   380 solver.cpp:749] class AP 3: 0.901156
I0512 05:45:40.199296   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89625
I0512 05:45:40.199331   380 solver.cpp:284] Tests completed in 258.537s
I0512 05:45:40.945160   380 solver.cpp:354] Iteration 11500 (0.386793 iter/s, 258.537s/100 iter), 649.4/1129.4ep, loss = 2.3099
I0512 05:45:40.945250   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0581 (* 1 = 2.0581 loss)
I0512 05:45:40.945281   380 sgd_solver.cpp:172] Iteration 11500, lr = 3.26254e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:45:58.905357   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:46:36.544039   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:47:25.804399   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:48:04.763736   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:48:50.102066   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:49:15.950212   380 solver.cpp:354] Iteration 11600 (0.465102 iter/s, 215.006s/100 iter), 655.1/1129.4ep, loss = 2.36105
I0512 05:49:15.950249   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.20784 (* 1 = 2.20784 loss)
I0512 05:49:15.950259   380 sgd_solver.cpp:172] Iteration 11600, lr = 3.1117e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:49:32.186085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:50:10.271440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:50:51.499954   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:51:28.598505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:52:19.084802   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:52:59.000892   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:53:16.934952   380 solver.cpp:354] Iteration 11700 (0.414962 iter/s, 240.986s/100 iter), 660.7/1129.4ep, loss = 2.39388
I0512 05:53:16.935112   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38527 (* 1 = 2.38527 loss)
I0512 05:53:16.935122   380 sgd_solver.cpp:172] Iteration 11700, lr = 2.96615e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:53:47.080124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:54:24.476706   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:55:10.370474   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:55:57.592562   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:56:36.655004   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:57:13.084990   380 solver.cpp:354] Iteration 11800 (0.423458 iter/s, 236.151s/100 iter), 666.4/1129.4ep, loss = 2.40543
I0512 05:57:13.085055   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41194 (* 1 = 2.41194 loss)
I0512 05:57:13.085065   380 sgd_solver.cpp:172] Iteration 11800, lr = 2.82576e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:57:16.590603   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:57:52.495090   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:58:33.725603   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:59:13.370117   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:59:51.943583   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:00:50.892933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:01:07.639163   380 solver.cpp:354] Iteration 11900 (0.426339 iter/s, 234.555s/100 iter), 672/1129.4ep, loss = 2.33039
I0512 06:01:07.639201   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3926 (* 1 = 2.3926 loss)
I0512 06:01:07.639211   380 sgd_solver.cpp:172] Iteration 11900, lr = 2.69042e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:01:31.972064   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:02:20.921489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:03:03.102632   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:03:39.798557   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:04:22.278528   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:04:58.035933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:05:17.578611   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_12000.caffemodel
I0512 06:05:17.667927   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_12000.solverstate
I0512 06:05:17.748509   380 solver.cpp:637] Iteration 12000, Testing net (#0)
I0512 06:05:37.238816   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:05:39.338567   380 solver.cpp:749] class AP 1: 0.897155
I0512 06:05:39.339229   380 solver.cpp:749] class AP 2: 0.890343
I0512 06:05:39.339509   380 solver.cpp:749] class AP 3: 0.901765
I0512 06:05:39.339514   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896421
I0512 06:05:39.339540   380 solver.cpp:284] Tests completed in 271.701s
I0512 06:05:39.938904   380 solver.cpp:354] Iteration 12000 (0.368051 iter/s, 271.701s/100 iter), 677.6/1129.4ep, loss = 2.26669
I0512 06:05:39.938992   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43762 (* 1 = 2.43762 loss)
I0512 06:05:39.939023   380 sgd_solver.cpp:172] Iteration 12000, lr = 2.56e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:05:39.940562   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.8 sparsity_achieved=0.565792 iter=12000
W0512 06:05:39.940603   380 net.cpp:2654] conv1a ni=3 no=32
W0512 06:05:41.443749   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.335
W0512 06:05:41.443835   380 net.cpp:2654] conv1b ni=32 no=32
W0512 06:05:43.122349   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.671007
W0512 06:05:43.122392   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 06:05:45.833979   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.762967
W0512 06:05:45.834020   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 06:05:48.915009   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.651476
W0512 06:05:48.915053   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 06:05:54.849905   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.773465
W0512 06:05:54.849951   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 06:06:00.813643   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.742459
W0512 06:06:00.813690   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 06:06:03.962257   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.799479
W0512 06:06:03.962302   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 06:06:08.405756   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.796061
W0512 06:06:08.405803   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 06:06:11.206773   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.794759
W0512 06:06:11.206817   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 06:06:13.264230   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.799391
W0512 06:06:13.264277   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 06:06:13.264283   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 06:06:13.264289   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 06:06:13.264295   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 06:06:13.264300   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 06:06:13.264307   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 06:06:13.264312   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264320   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 06:06:13.264326   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264333   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264355   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264361   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264369   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264379   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264384   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264391   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 06:06:13.264400   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264406   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 06:06:13.264416   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 06:06:13.267336   380 solver.cpp:391] Sparsity after update:
I0512 06:06:13.268277   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 06:06:13.268286   380 net.cpp:2780] conv1a_param_0(0.335) 
I0512 06:06:13.268296   380 net.cpp:2780] conv1b_param_0(0.671) 
I0512 06:06:13.268298   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268306   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268308   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 06:06:13.268311   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268316   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268318   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 06:06:13.268322   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268326   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268334   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 06:06:13.268339   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268343   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268348   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 06:06:13.268355   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268359   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268364   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 06:06:13.268369   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268373   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268381   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 06:06:13.268386   380 net.cpp:2780] res2a_branch2a_param_0(0.763) 
I0512 06:06:13.268393   380 net.cpp:2780] res2a_branch2b_param_0(0.651) 
I0512 06:06:13.268398   380 net.cpp:2780] res3a_branch2a_param_0(0.773) 
I0512 06:06:13.268402   380 net.cpp:2780] res3a_branch2b_param_0(0.742) 
I0512 06:06:13.268409   380 net.cpp:2780] res4a_branch2a_param_0(0.799) 
I0512 06:06:13.268442   380 net.cpp:2780] res4a_branch2b_param_0(0.796) 
I0512 06:06:13.268452   380 net.cpp:2780] res5a_branch2a_param_0(0.795) 
I0512 06:06:13.268458   380 net.cpp:2780] res5a_branch2b_param_0(0.799) 
I0512 06:06:13.268463   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.86901e+06/3.10435e+06) 0.602
I0512 06:06:26.517848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:07:02.980505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:07:45.765640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:08:21.696214   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:09:01.983026   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:09:37.656339   380 solver.cpp:354] Iteration 12100 (0.420666 iter/s, 237.718s/100 iter), 683.3/1129.4ep, loss = 2.30271
I0512 06:09:37.657096   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43908 (* 1 = 2.43908 loss)
I0512 06:09:37.657335   380 sgd_solver.cpp:172] Iteration 12100, lr = 2.43438e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:09:43.331024   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:10:23.222741   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:11:21.117360   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:11:57.352170   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:12:36.948956   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:13:14.694136   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:13:42.489498   380 solver.cpp:354] Iteration 12200 (0.408441 iter/s, 244.834s/100 iter), 688.9/1129.4ep, loss = 2.34875
I0512 06:13:42.489533   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19593 (* 1 = 2.19593 loss)
I0512 06:13:42.489542   380 sgd_solver.cpp:172] Iteration 12200, lr = 2.31344e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:13:57.892750   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:14:41.506403   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:15:18.580929   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:16:06.041558   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:16:41.908457   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:17:35.333668   380 solver.cpp:354] Iteration 12300 (0.429475 iter/s, 232.842s/100 iter), 694.6/1129.4ep, loss = 2.39844
I0512 06:17:35.334223   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.26889 (* 1 = 2.26889 loss)
I0512 06:17:35.334553   380 sgd_solver.cpp:172] Iteration 12300, lr = 2.19706e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:17:36.900149   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:18:16.666059   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:18:52.242817   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:19:32.508054   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:20:11.345551   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:21:01.066597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:21:32.221345   380 solver.cpp:354] Iteration 12400 (0.422145 iter/s, 236.885s/100 iter), 700.2/1129.4ep, loss = 2.34175
I0512 06:21:32.221438   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.14213 (* 1 = 2.14213 loss)
I0512 06:21:32.221469   380 sgd_solver.cpp:172] Iteration 12400, lr = 2.08514e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:21:36.863664   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:22:30.779036   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:23:08.783572   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:23:50.718506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:24:31.727022   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:09.870867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:31.636656   380 solver.cpp:637] Iteration 12500, Testing net (#0)
I0512 06:25:55.800508   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:58.212220   380 solver.cpp:749] class AP 1: 0.896631
I0512 06:25:58.213078   380 solver.cpp:749] class AP 2: 0.884702
I0512 06:25:58.213413   380 solver.cpp:749] class AP 3: 0.901848
I0512 06:25:58.213421   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894394
I0512 06:25:58.213450   380 solver.cpp:284] Tests completed in 265.991s
I0512 06:25:58.916282   380 solver.cpp:354] Iteration 12500 (0.375953 iter/s, 265.991s/100 iter), 705.9/1129.4ep, loss = 2.3806
I0512 06:25:58.916401   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33346 (* 1 = 2.33346 loss)
I0512 06:25:58.916447   380 sgd_solver.cpp:172] Iteration 12500, lr = 1.97754e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:26:05.879794   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:26:48.089718   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:27:23.452514   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:28:01.666285   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:28:40.225848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:29:24.613023   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:29:28.099177   380 solver.cpp:354] Iteration 12600 (0.478052 iter/s, 209.182s/100 iter), 711.5/1129.4ep, loss = 2.28146
I0512 06:29:28.099916   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.1506 (* 1 = 2.1506 loss)
I0512 06:29:28.100299   380 sgd_solver.cpp:172] Iteration 12600, lr = 1.87416e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:30:03.520756   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:30:52.604082   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:31:31.773980   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:32:25.838932   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:33:05.975986   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:33:34.286411   380 solver.cpp:354] Iteration 12700 (0.406196 iter/s, 246.187s/100 iter), 717.2/1129.4ep, loss = 2.43517
I0512 06:33:34.286657   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48013 (* 1 = 2.48013 loss)
I0512 06:33:34.286748   380 sgd_solver.cpp:172] Iteration 12700, lr = 1.77489e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:33:40.941913   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:34:17.781616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:34:55.426512   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:35:48.387617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:36:26.879323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:37:04.952895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:37:24.351830   380 solver.cpp:354] Iteration 12800 (0.43466 iter/s, 230.065s/100 iter), 722.8/1129.4ep, loss = 2.29932
I0512 06:37:24.352488   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11013 (* 1 = 2.11013 loss)
I0512 06:37:24.352850   380 sgd_solver.cpp:172] Iteration 12800, lr = 1.67962e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:37:44.789419   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:38:34.387388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:39:18.631997   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:39:58.429492   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:40:34.252135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:41:13.095304   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:41:15.594710   380 solver.cpp:354] Iteration 12900 (0.432446 iter/s, 231.243s/100 iter), 728.5/1129.4ep, loss = 2.33723
I0512 06:41:15.594748   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35788 (* 1 = 2.35788 loss)
I0512 06:41:15.594758   380 sgd_solver.cpp:172] Iteration 12900, lr = 1.58823e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:41:56.351114   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:42:34.818361   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:43:17.867835   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:44:04.965185   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:44:39.564811   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:08.764317   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_13000.caffemodel
I0512 06:45:08.822512   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_13000.solverstate
I0512 06:45:08.884050   380 solver.cpp:637] Iteration 13000, Testing net (#0)
I0512 06:45:33.703161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:35.911193   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:38.684885   380 solver.cpp:749] class AP 1: 0.897355
I0512 06:45:38.685725   380 solver.cpp:749] class AP 2: 0.88589
I0512 06:45:38.686061   380 solver.cpp:749] class AP 3: 0.902037
I0512 06:45:38.686069   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895094
I0512 06:45:38.686097   380 solver.cpp:284] Tests completed in 263.091s
I0512 06:45:39.281121   380 solver.cpp:354] Iteration 13000 (0.380096 iter/s, 263.091s/100 iter), 734.1/1129.4ep, loss = 2.44566
I0512 06:45:39.281203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.42635 (* 1 = 2.42635 loss)
I0512 06:45:39.281234   380 sgd_solver.cpp:172] Iteration 13000, lr = 1.50063e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:46:08.765684   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:46:57.449867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:47:42.440268   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:48:20.579772   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:48:58.451648   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:49:06.276201   380 solver.cpp:354] Iteration 13100 (0.483104 iter/s, 206.995s/100 iter), 739.8/1129.4ep, loss = 2.44823
I0512 06:49:06.276286   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60307 (* 1 = 2.60307 loss)
I0512 06:49:06.276316   380 sgd_solver.cpp:172] Iteration 13100, lr = 1.4167e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:49:36.681020   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:50:20.691504   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:50:59.741583   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:51:43.453512   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:52:21.239766   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:53:19.637573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:53:20.796137   380 solver.cpp:354] Iteration 13200 (0.392894 iter/s, 254.522s/100 iter), 745.4/1129.4ep, loss = 2.46841
I0512 06:53:20.796175   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46276 (* 1 = 2.46276 loss)
I0512 06:53:20.796183   380 sgd_solver.cpp:172] Iteration 13200, lr = 1.33634e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:54:05.901422   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:54:43.672833   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:55:21.113030   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:55:56.085239   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:56:34.869011   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:57:06.451875   380 solver.cpp:354] Iteration 13300 (0.443151 iter/s, 225.657s/100 iter), 751.1/1129.4ep, loss = 2.34491
I0512 06:57:06.452064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17581 (* 1 = 2.17581 loss)
I0512 06:57:06.452111   380 sgd_solver.cpp:172] Iteration 13300, lr = 1.25944e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:57:17.110515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:57:54.811944   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:58:41.091114   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:59:16.049782   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:12.797559   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:50.825961   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:54.735842   380 solver.cpp:354] Iteration 13400 (0.438049 iter/s, 228.285s/100 iter), 756.7/1129.4ep, loss = 2.37319
I0512 07:00:54.735980   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3782 (* 1 = 2.3782 loss)
I0512 07:00:54.736027   380 sgd_solver.cpp:172] Iteration 13400, lr = 1.18592e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:01:30.692363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:02:10.166597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:02:46.698156   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:03:28.340123   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:04:04.508785   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:04:43.612576   380 solver.cpp:637] Iteration 13500, Testing net (#0)
I0512 07:04:48.126879   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:05:10.335208   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:05:12.436916   380 solver.cpp:749] class AP 1: 0.895944
I0512 07:05:12.437710   380 solver.cpp:749] class AP 2: 0.885567
I0512 07:05:12.438035   380 solver.cpp:749] class AP 3: 0.901646
I0512 07:05:12.438042   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894386
I0512 07:05:12.438071   380 solver.cpp:284] Tests completed in 257.703s
I0512 07:05:13.064388   380 solver.cpp:354] Iteration 13500 (0.388044 iter/s, 257.703s/100 iter), 762.4/1129.4ep, loss = 2.30305
I0512 07:05:13.064474   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55962 (* 1 = 2.55962 loss)
I0512 07:05:13.064507   380 sgd_solver.cpp:172] Iteration 13500, lr = 1.11566e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:05:31.713337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:06:15.545054   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:06:55.901111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:07:34.915421   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:08:31.364018   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:08:45.467353   380 solver.cpp:354] Iteration 13600 (0.470802 iter/s, 212.403s/100 iter), 768/1129.4ep, loss = 2.43224
I0512 07:08:45.467612   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.6149 (* 1 = 2.6149 loss)
I0512 07:08:45.467680   380 sgd_solver.cpp:172] Iteration 13600, lr = 1.04858e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:09:09.037259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:09:48.372699   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:10:25.051229   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:11:13.652639   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:11:52.639192   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:12:30.236826   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:12:44.709638   380 solver.cpp:354] Iteration 13700 (0.417986 iter/s, 239.243s/100 iter), 773.6/1129.4ep, loss = 2.34681
I0512 07:12:44.709672   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.37087 (* 1 = 2.37087 loss)
I0512 07:12:44.709681   380 sgd_solver.cpp:172] Iteration 13700, lr = 9.8456e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:13:16.381981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:13:53.819025   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:14:47.908991   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:15:24.681938   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:16:04.766158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:16:39.083851   380 solver.cpp:354] Iteration 13800 (0.426668 iter/s, 234.375s/100 iter), 779.3/1129.4ep, loss = 2.36962
I0512 07:16:39.083916   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.51439 (* 1 = 2.51439 loss)
I0512 07:16:39.083927   380 sgd_solver.cpp:172] Iteration 13800, lr = 9.23521e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:16:43.347702   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:17:18.843503   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:18:06.169550   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:18:46.751183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:19:28.314239   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:20:07.549818   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:20:32.880179   380 solver.cpp:354] Iteration 13900 (0.427722 iter/s, 233.797s/100 iter), 784.9/1129.4ep, loss = 2.3397
I0512 07:20:32.880704   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11196 (* 1 = 2.11196 loss)
I0512 07:20:32.880937   380 sgd_solver.cpp:172] Iteration 13900, lr = 8.65365e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:21:03.613611   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:21:44.093758   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:22:19.291666   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:23:03.619459   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:23:40.908710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:19.048044   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:28.633548   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_14000.caffemodel
I0512 07:24:28.688812   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_14000.solverstate
I0512 07:24:28.737344   380 solver.cpp:637] Iteration 14000, Testing net (#0)
I0512 07:24:50.037305   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:52.355204   380 solver.cpp:749] class AP 1: 0.89628
I0512 07:24:52.356040   380 solver.cpp:749] class AP 2: 0.88667
I0512 07:24:52.356384   380 solver.cpp:749] class AP 3: 0.901667
I0512 07:24:52.356395   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894872
I0512 07:24:52.356429   380 solver.cpp:284] Tests completed in 259.474s
I0512 07:24:52.981204   380 solver.cpp:354] Iteration 14000 (0.385395 iter/s, 259.474s/100 iter), 790.6/1129.4ep, loss = 2.39895
I0512 07:24:52.981297   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46555 (* 1 = 2.46555 loss)
I0512 07:24:52.981331   380 sgd_solver.cpp:172] Iteration 14000, lr = 8.1e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:24:52.983011   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.85 sparsity_achieved=0.602061 iter=14000
W0512 07:24:52.983054   380 net.cpp:2654] conv1a ni=3 no=32
W0512 07:24:54.762243   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.3425
W0512 07:24:54.762342   380 net.cpp:2654] conv1b ni=32 no=32
W0512 07:24:56.442049   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.674479
W0512 07:24:56.442095   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 07:24:59.412950   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.780599
W0512 07:24:59.412993   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 07:25:02.505936   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.65332
W0512 07:25:02.505980   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 07:25:08.671644   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.783596
W0512 07:25:08.671689   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 07:25:14.824723   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.750217
W0512 07:25:14.824767   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 07:25:20.207489   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.84947
W0512 07:25:20.207548   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 07:25:27.279160   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.843174
W0512 07:25:27.279202   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 07:25:31.233297   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.844131
W0512 07:25:31.233340   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 07:25:34.229763   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.849526
W0512 07:25:34.229809   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 07:25:34.229815   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 07:25:34.229821   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 07:25:34.229826   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 07:25:34.229832   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 07:25:34.229838   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 07:25:34.229845   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229851   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 07:25:34.229861   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229871   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229880   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229889   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229916   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229930   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229941   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229957   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 07:25:34.229970   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229982   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 07:25:34.229993   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 07:25:34.232905   380 solver.cpp:391] Sparsity after update:
I0512 07:25:34.233840   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 07:25:34.233850   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 07:25:34.233858   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 07:25:34.233861   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233865   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233868   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 07:25:34.233872   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233876   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233883   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 07:25:34.233888   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233893   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233901   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 07:25:34.233904   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233909   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233913   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 07:25:34.233922   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233927   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233932   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 07:25:34.233935   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233938   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233943   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 07:25:34.233947   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 07:25:34.233952   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 07:25:34.233958   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 07:25:34.233963   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 07:25:34.233968   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 07:25:34.233973   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 07:25:34.233978   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 07:25:34.234004   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 07:25:34.234009   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 07:25:46.099409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:26:24.533906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:27:08.471392   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:27:43.094532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:28:29.307065   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:29:00.848176   380 solver.cpp:354] Iteration 14100 (0.403451 iter/s, 247.862s/100 iter), 796.2/1129.4ep, loss = 2.5574
I0512 07:29:00.848471   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29777 (* 1 = 2.29777 loss)
I0512 07:29:00.848538   380 sgd_solver.cpp:172] Iteration 14100, lr = 7.57335e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:29:05.882730   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:30:04.028123   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:30:44.361773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:31:22.282876   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:32:03.865661   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:32:39.793854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:33:10.702345   380 solver.cpp:354] Iteration 14200 (0.400239 iter/s, 249.85s/100 iter), 801.9/1129.4ep, loss = 2.47844
I0512 07:33:10.703030   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58026 (* 1 = 2.58026 loss)
I0512 07:33:10.703384   380 sgd_solver.cpp:172] Iteration 14200, lr = 7.07281e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:33:24.347329   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:34:02.782104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:34:47.743248   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:35:24.586400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:36:22.143801   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:37:02.439826   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:37:03.478307   380 solver.cpp:354] Iteration 14300 (0.429602 iter/s, 232.773s/100 iter), 807.5/1129.4ep, loss = 2.39447
I0512 07:37:03.478458   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52669 (* 1 = 2.52669 loss)
I0512 07:37:03.478509   380 sgd_solver.cpp:172] Iteration 14300, lr = 6.5975e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:37:38.083612   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:38:15.703184   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:38:52.380204   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:39:40.429337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:40:19.911423   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:40:50.818274   380 solver.cpp:354] Iteration 14400 (0.439874 iter/s, 227.338s/100 iter), 813.2/1129.4ep, loss = 2.50824
I0512 07:40:50.818390   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.49054 (* 1 = 2.49054 loss)
I0512 07:40:50.818434   380 sgd_solver.cpp:172] Iteration 14400, lr = 6.14656e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:40:58.933917   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:41:37.699744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:42:16.515658   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:43:16.743243   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:43:52.033319   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:44:32.242260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:44:45.479857   380 solver.cpp:637] Iteration 14500, Testing net (#0)
I0512 07:45:08.862388   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:45:12.184242   380 solver.cpp:749] class AP 1: 0.894689
I0512 07:45:12.185427   380 solver.cpp:749] class AP 2: 0.884244
I0512 07:45:12.185783   380 solver.cpp:749] class AP 3: 0.901387
I0512 07:45:12.185791   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89344
I0512 07:45:12.185820   380 solver.cpp:284] Tests completed in 261.366s
I0512 07:45:12.900179   380 solver.cpp:354] Iteration 14500 (0.382605 iter/s, 261.366s/100 iter), 818.8/1129.4ep, loss = 2.38669
I0512 07:45:12.900267   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.4358 (* 1 = 2.4358 loss)
I0512 07:45:12.900295   380 sgd_solver.cpp:172] Iteration 14500, lr = 5.71914e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:45:22.118444   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:45:59.421556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:46:46.104092   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:47:24.289927   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:04.250116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:39.939463   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:49.415818   380 solver.cpp:354] Iteration 14600 (0.461863 iter/s, 216.514s/100 iter), 824.5/1129.4ep, loss = 2.30774
I0512 07:48:49.416157   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43388 (* 1 = 2.43388 loss)
I0512 07:48:49.416244   380 sgd_solver.cpp:172] Iteration 14600, lr = 5.31441e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:49:33.194670   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:50:11.710112   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:50:51.186725   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:51:34.303781   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:52:09.476140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:52:40.223341   380 solver.cpp:354] Iteration 14700 (0.433264 iter/s, 230.806s/100 iter), 830.1/1129.4ep, loss = 2.47492
I0512 07:52:40.223424   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22872 (* 1 = 2.22872 loss)
I0512 07:52:40.223451   380 sgd_solver.cpp:172] Iteration 14700, lr = 4.93155e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:52:56.503324   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:53:40.316382   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:54:23.106197   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:55:04.873273   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:55:49.470324   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:56:32.330438   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:56:44.101650   380 solver.cpp:354] Iteration 14800 (0.410043 iter/s, 243.877s/100 iter), 835.8/1129.4ep, loss = 2.38121
I0512 07:56:44.101953   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40385 (* 1 = 2.40385 loss)
I0512 07:56:44.102036   380 sgd_solver.cpp:172] Iteration 14800, lr = 4.56976e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:57:13.446024   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:58:00.623440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:58:41.222637   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:59:21.740432   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:01.214862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:39.803169   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:40.230898   380 solver.cpp:354] Iteration 14900 (0.423495 iter/s, 236.13s/100 iter), 841.4/1129.4ep, loss = 2.32013
I0512 08:00:40.231585   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.04751 (* 1 = 2.04751 loss)
I0512 08:00:40.231972   380 sgd_solver.cpp:172] Iteration 14900, lr = 4.22825e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:01:24.912009   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:02:03.143669   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:02:54.398586   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:03:34.460433   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:04:21.644702   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:04:43.264290   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_15000.caffemodel
I0512 08:04:43.337749   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_15000.solverstate
I0512 08:04:43.399899   380 solver.cpp:637] Iteration 15000, Testing net (#0)
I0512 08:05:02.137131   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:05:05.637182   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:05:09.070629   380 solver.cpp:749] class AP 1: 0.893947
I0512 08:05:09.071787   380 solver.cpp:749] class AP 2: 0.882938
I0512 08:05:09.072137   380 solver.cpp:749] class AP 3: 0.901647
I0512 08:05:09.072144   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892844
I0512 08:05:09.072175   380 solver.cpp:284] Tests completed in 268.842s
I0512 08:05:09.737620   380 solver.cpp:354] Iteration 15000 (0.371965 iter/s, 268.842s/100 iter), 847.1/1129.4ep, loss = 2.59361
I0512 08:05:09.737712   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71177 (* 1 = 2.71177 loss)
I0512 08:05:09.737746   380 sgd_solver.cpp:172] Iteration 15000, lr = 3.90625e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:05:43.523984   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:06:26.522555   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:07:02.259291   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:07:51.357620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:08:30.935556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:08:49.985224   380 solver.cpp:354] Iteration 15100 (0.454034 iter/s, 220.248s/100 iter), 852.7/1129.4ep, loss = 2.38181
I0512 08:08:49.985416   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.2876 (* 1 = 2.2876 loss)
I0512 08:08:49.985479   380 sgd_solver.cpp:172] Iteration 15100, lr = 3.603e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:09:12.115409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:09:54.221241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:10:39.264596   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:11:26.965469   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:12:04.211906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:12:46.289477   380 solver.cpp:354] Iteration 15200 (0.423183 iter/s, 236.304s/100 iter), 858.4/1129.4ep, loss = 2.4371
I0512 08:12:46.290117   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.80906 (* 1 = 2.80906 loss)
I0512 08:12:46.290447   380 sgd_solver.cpp:172] Iteration 15200, lr = 3.31776e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:12:55.527381   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:13:32.113687   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:14:05.849428   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:14:45.373320   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:15:21.321030   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:16:15.836139   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:16:33.305686   380 solver.cpp:354] Iteration 15300 (0.440497 iter/s, 227.016s/100 iter), 864/1129.4ep, loss = 2.35677
I0512 08:16:33.305727   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.65949 (* 1 = 2.65949 loss)
I0512 08:16:33.305737   380 sgd_solver.cpp:172] Iteration 15300, lr = 3.0498e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:16:51.347573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:17:42.169284   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:18:17.864292   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:19:01.127099   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:19:47.540293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:20:22.977674   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:20:27.777370   380 solver.cpp:354] Iteration 15400 (0.426491 iter/s, 234.471s/100 iter), 869.6/1129.4ep, loss = 2.44611
I0512 08:20:27.777546   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61646 (* 1 = 2.61646 loss)
I0512 08:20:27.777606   380 sgd_solver.cpp:172] Iteration 15400, lr = 2.79841e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:21:02.634498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:21:39.585217   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:22:24.819844   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:23:01.597182   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:23:54.346920   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:29.915454   380 solver.cpp:637] Iteration 15500, Testing net (#0)
I0512 08:24:35.203483   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:50.184361   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:52.788005   380 solver.cpp:749] class AP 1: 0.894075
I0512 08:24:52.789157   380 solver.cpp:749] class AP 2: 0.883818
I0512 08:24:52.789511   380 solver.cpp:749] class AP 3: 0.901737
I0512 08:24:52.789520   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89321
I0512 08:24:52.789551   380 solver.cpp:284] Tests completed in 265.012s
I0512 08:24:53.500522   380 solver.cpp:354] Iteration 15500 (0.377342 iter/s, 265.012s/100 iter), 875.3/1129.4ep, loss = 2.42094
I0512 08:24:53.500592   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38818 (* 1 = 2.38818 loss)
I0512 08:24:53.500622   380 sgd_solver.cpp:172] Iteration 15500, lr = 2.56289e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:25:12.217156   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:25:58.856359   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:26:34.289676   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:27:32.933523   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:28:09.246408   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:28:23.760172   380 solver.cpp:354] Iteration 15600 (0.475603 iter/s, 210.259s/100 iter), 880.9/1129.4ep, loss = 2.39625
I0512 08:28:23.760283   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.4466 (* 1 = 2.4466 loss)
I0512 08:28:23.760327   380 sgd_solver.cpp:172] Iteration 15600, lr = 2.34256e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:28:50.703773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:29:33.323573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:30:11.193820   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:31:02.298489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:31:41.053411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:32:16.280588   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:32:24.060652   380 solver.cpp:354] Iteration 15700 (0.416147 iter/s, 240.3s/100 iter), 886.6/1129.4ep, loss = 2.40741
I0512 08:32:24.061125   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48293 (* 1 = 2.48293 loss)
I0512 08:32:24.061213   380 sgd_solver.cpp:172] Iteration 15700, lr = 2.13675e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:32:52.615475   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:33:40.227411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:34:25.732493   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:35:06.001135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:35:56.376116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:36:29.090582   380 solver.cpp:354] Iteration 15800 (0.408112 iter/s, 245.031s/100 iter), 892.2/1129.4ep, loss = 2.35173
I0512 08:36:29.091447   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.10538 (* 1 = 2.10538 loss)
I0512 08:36:29.091814   380 sgd_solver.cpp:172] Iteration 15800, lr = 1.94481e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:36:36.692628   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:37:21.156144   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:37:59.355221   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:38:39.619859   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:39:24.415330   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:40:05.484552   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:40:23.683187   380 solver.cpp:354] Iteration 15900 (0.42627 iter/s, 234.593s/100 iter), 897.9/1129.4ep, loss = 2.45922
I0512 08:40:23.683353   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.32499 (* 1 = 2.32499 loss)
I0512 08:40:23.683400   380 sgd_solver.cpp:172] Iteration 15900, lr = 1.7661e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:40:46.294355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:41:26.560863   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:42:12.992933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:42:48.878226   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:43:31.637800   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:16.927175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:26.361054   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_16000.caffemodel
I0512 08:44:26.382162   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_16000.solverstate
I0512 08:44:26.404278   380 solver.cpp:637] Iteration 16000, Testing net (#0)
I0512 08:44:45.449192   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:48.496340   380 solver.cpp:749] class AP 1: 0.893932
I0512 08:44:48.497509   380 solver.cpp:749] class AP 2: 0.883147
I0512 08:44:48.497857   380 solver.cpp:749] class AP 3: 0.901828
I0512 08:44:48.497864   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892969
I0512 08:44:48.497893   380 solver.cpp:284] Tests completed in 264.815s
I0512 08:44:49.228394   380 solver.cpp:354] Iteration 16000 (0.377622 iter/s, 264.815s/100 iter), 903.5/1129.4ep, loss = 2.38821
I0512 08:44:49.228482   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60551 (* 1 = 2.60551 loss)
I0512 08:44:49.228514   380 sgd_solver.cpp:172] Iteration 16000, lr = 1.6e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:44:49.230512   380 solver.cpp:391] Sparsity after update:
I0512 08:44:49.232025   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 08:44:49.232057   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 08:44:49.232089   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 08:44:49.232117   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232146   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232175   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 08:44:49.232203   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232232   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232260   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 08:44:49.232290   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232318   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232347   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 08:44:49.232375   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232404   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232434   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 08:44:49.232462   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232491   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232520   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 08:44:49.232548   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232578   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232605   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 08:44:49.232635   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 08:44:49.232663   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 08:44:49.232692   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 08:44:49.232720   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 08:44:49.232749   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 08:44:49.232777   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 08:44:49.232807   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 08:44:49.232836   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 08:44:49.232864   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 08:45:01.285671   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:45:39.896585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:46:18.314790   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:47:10.651660   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:47:52.706176   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:48:24.228271   380 solver.cpp:354] Iteration 16100 (0.465116 iter/s, 215s/100 iter), 909.2/1129.4ep, loss = 2.5473
I0512 08:48:24.228338   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.98722 (* 1 = 2.98722 loss)
I0512 08:48:24.228348   380 sgd_solver.cpp:172] Iteration 16100, lr = 1.4459e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:48:37.516037   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:49:19.842875   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:50:00.832288   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:50:37.076860   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:51:14.554430   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:51:58.141252   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:52:24.244592   380 solver.cpp:354] Iteration 16200 (0.416638 iter/s, 240.016s/100 iter), 914.8/1129.4ep, loss = 2.35982
I0512 08:52:24.244797   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38771 (* 1 = 2.38771 loss)
I0512 08:52:24.244860   380 sgd_solver.cpp:172] Iteration 16200, lr = 1.30321e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:52:37.518167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:53:24.339283   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:54:09.353605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:54:48.211962   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:55:41.459074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:56:17.799016   380 solver.cpp:354] Iteration 16300 (0.428166 iter/s, 233.554s/100 iter), 920.5/1129.4ep, loss = 2.42821
I0512 08:56:17.799552   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52797 (* 1 = 2.52797 loss)
I0512 08:56:17.799749   380 sgd_solver.cpp:172] Iteration 16300, lr = 1.17135e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:56:17.957899   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:56:55.372046   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:57:33.934849   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:58:09.779541   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:58:50.629307   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:59:30.157141   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:00:09.487685   380 solver.cpp:354] Iteration 16400 (0.431614 iter/s, 231.689s/100 iter), 926.1/1129.4ep, loss = 2.34306
I0512 09:00:09.488273   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.1992 (* 1 = 2.1992 loss)
I0512 09:00:09.488590   380 sgd_solver.cpp:172] Iteration 16400, lr = 1.04976e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 09:00:27.970585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:01:08.162022   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:01:56.319124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:02:36.146018   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:03:12.089181   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:03:52.189498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:04:08.930269   380 solver.cpp:637] Iteration 16500, Testing net (#0)
I0512 09:04:31.769407   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:04:35.871034   380 solver.cpp:749] class AP 1: 0.894414
I0512 09:04:35.872736   380 solver.cpp:749] class AP 2: 0.881759
I0512 09:04:35.873374   380 solver.cpp:749] class AP 3: 0.901836
I0512 09:04:35.873437   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892669
I0512 09:04:35.873618   380 solver.cpp:284] Tests completed in 266.386s
I0512 09:04:36.761648   380 solver.cpp:354] Iteration 16500 (0.375396 iter/s, 266.386s/100 iter), 931.8/1129.4ep, loss = 2.44951
I0512 09:04:36.761726   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29817 (* 1 = 2.29817 loss)
I0512 09:04:36.761777   380 sgd_solver.cpp:172] Iteration 16500, lr = 9.37891e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:04:47.350136   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:05:31.553633   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:06:06.055147   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:06:49.699376   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:07:30.675356   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:08:09.775640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:08:11.336812   380 solver.cpp:354] Iteration 16600 (0.466038 iter/s, 214.575s/100 iter), 937.4/1129.4ep, loss = 2.29924
I0512 09:08:11.337090   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25949 (* 1 = 2.25949 loss)
I0512 09:08:11.337174   380 sgd_solver.cpp:172] Iteration 16600, lr = 8.3521e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:08:58.117007   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:09:31.857744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:10:24.973925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:11:02.719388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:11:46.422344   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:12:08.214195   380 solver.cpp:354] Iteration 16700 (0.422159 iter/s, 236.877s/100 iter), 943.1/1129.4ep, loss = 2.34044
I0512 09:12:08.214231   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25168 (* 1 = 2.25168 loss)
I0512 09:12:08.214241   380 sgd_solver.cpp:172] Iteration 16700, lr = 7.41201e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:12:25.287760   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:12:59.633983   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:13:44.129186   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:14:23.907331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:15:17.726172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:15:56.456158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:16:07.331291   380 solver.cpp:354] Iteration 16800 (0.418205 iter/s, 239.117s/100 iter), 948.7/1129.4ep, loss = 2.46709
I0512 09:16:07.331331   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.51219 (* 1 = 2.51219 loss)
I0512 09:16:07.331341   380 sgd_solver.cpp:172] Iteration 16800, lr = 6.5536e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:16:38.833515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:17:20.532500   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:17:56.674775   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:18:43.436904   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:19:24.144241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:20:00.027390   380 solver.cpp:354] Iteration 16900 (0.429745 iter/s, 232.696s/100 iter), 954.4/1129.4ep, loss = 2.38701
I0512 09:20:00.028015   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55418 (* 1 = 2.55418 loss)
I0512 09:20:00.028208   380 sgd_solver.cpp:172] Iteration 16900, lr = 5.772e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:20:04.087844   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:20:42.352347   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:21:36.486074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:22:16.480111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:22:49.602617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:23:33.124745   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:23:57.040663   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_17000.caffemodel
I0512 09:23:57.119530   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_17000.solverstate
I0512 09:23:57.178508   380 solver.cpp:637] Iteration 17000, Testing net (#0)
I0512 09:24:09.690515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:24:18.671679   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:24:22.396333   380 solver.cpp:749] class AP 1: 0.893847
I0512 09:24:22.397408   380 solver.cpp:749] class AP 2: 0.883152
I0512 09:24:22.397742   380 solver.cpp:749] class AP 3: 0.902081
I0512 09:24:22.397748   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893027
I0512 09:24:22.397776   380 solver.cpp:284] Tests completed in 262.37s
I0512 09:24:22.997289   380 solver.cpp:354] Iteration 17000 (0.381141 iter/s, 262.37s/100 iter), 960/1129.4ep, loss = 2.37627
I0512 09:24:22.997375   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.27167 (* 1 = 2.27167 loss)
I0512 09:24:22.997406   380 sgd_solver.cpp:172] Iteration 17000, lr = 5.0625e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:25:11.451094   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:25:48.409762   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:26:32.423051   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:15.355129   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:52.065673   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:58.965366   380 solver.cpp:354] Iteration 17100 (0.463032 iter/s, 215.968s/100 iter), 965.6/1129.4ep, loss = 2.53495
I0512 09:27:58.965456   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.94788 (* 1 = 2.94788 loss)
I0512 09:27:58.965487   380 sgd_solver.cpp:172] Iteration 17100, lr = 4.4205e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:28:31.583958   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:29:08.700513   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:29:57.060436   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:30:36.301640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:31:22.720608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:32:01.204000   380 solver.cpp:354] Iteration 17200 (0.412816 iter/s, 242.238s/100 iter), 971.3/1129.4ep, loss = 2.50831
I0512 09:32:01.204064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19645 (* 1 = 2.19645 loss)
I0512 09:32:01.204077   380 sgd_solver.cpp:172] Iteration 17200, lr = 3.8416e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:32:05.907874   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:32:43.711859   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:33:35.113812   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:34:11.718183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:34:54.357329   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:35:31.596019   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:35:51.828601   380 solver.cpp:354] Iteration 17300 (0.433605 iter/s, 230.624s/100 iter), 976.9/1129.4ep, loss = 2.41484
I0512 09:35:51.828850   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.32872 (* 1 = 2.32872 loss)
I0512 09:35:51.828946   380 sgd_solver.cpp:172] Iteration 17300, lr = 3.32151e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:36:04.833793   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:36:43.360210   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:37:19.799355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:38:11.991829   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:38:48.852129   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:39:44.566401   380 solver.cpp:354] Iteration 17400 (0.429668 iter/s, 232.738s/100 iter), 982.6/1129.4ep, loss = 2.41857
I0512 09:39:44.566473   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.68868 (* 1 = 2.68868 loss)
I0512 09:39:44.566483   380 sgd_solver.cpp:172] Iteration 17400, lr = 2.8561e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:39:45.958361   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:40:19.623339   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:40:58.618078   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:41:38.603543   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:42:15.619128   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:42:55.659443   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:30.294371   380 solver.cpp:637] Iteration 17500, Testing net (#0)
I0512 09:43:34.333338   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:55.230094   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:59.798846   380 solver.cpp:749] class AP 1: 0.894015
I0512 09:43:59.799955   380 solver.cpp:749] class AP 2: 0.883818
I0512 09:43:59.800318   380 solver.cpp:749] class AP 3: 0.902187
I0512 09:43:59.800331   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89334
I0512 09:43:59.800367   380 solver.cpp:284] Tests completed in 255.233s
I0512 09:44:00.401837   380 solver.cpp:354] Iteration 17500 (0.391799 iter/s, 255.233s/100 iter), 988.2/1129.4ep, loss = 2.41114
I0512 09:44:00.401960   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.34293 (* 1 = 2.34293 loss)
I0512 09:44:00.401973   380 sgd_solver.cpp:172] Iteration 17500, lr = 2.44141e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:44:31.086401   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:45:10.028558   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:45:58.613703   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:46:46.429435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:47:20.131259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:47:36.915020   380 solver.cpp:354] Iteration 17600 (0.461868 iter/s, 216.512s/100 iter), 993.9/1129.4ep, loss = 2.34302
I0512 09:47:36.915184   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19788 (* 1 = 2.19788 loss)
I0512 09:47:36.915230   380 sgd_solver.cpp:172] Iteration 17600, lr = 2.0736e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:48:01.992120   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:48:37.623883   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:49:13.399333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:49:50.513161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:50:32.550293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:51:18.771561   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:51:32.709905   380 solver.cpp:354] Iteration 17700 (0.424099 iter/s, 235.794s/100 iter), 999.5/1129.4ep, loss = 2.38956
I0512 09:51:32.710103   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41128 (* 1 = 2.41128 loss)
I0512 09:51:32.710168   380 sgd_solver.cpp:172] Iteration 17700, lr = 1.74901e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:51:54.833321   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:52:55.531157   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:53:35.126605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:54:12.918452   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:54:51.484804   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:55:26.470865   380 solver.cpp:354] Iteration 17800 (0.427789 iter/s, 233.76s/100 iter), 1005.2/1129.4ep, loss = 2.5572
I0512 09:55:26.471468   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48266 (* 1 = 2.48266 loss)
I0512 09:55:26.471725   380 sgd_solver.cpp:172] Iteration 17800, lr = 1.4641e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:55:31.733242   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:56:12.667940   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:56:52.708580   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:57:40.627449   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:58:15.290505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:59:12.145421   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:59:20.464395   380 solver.cpp:354] Iteration 17900 (0.427363 iter/s, 233.993s/100 iter), 1010.8/1129.4ep, loss = 2.30859
I0512 09:59:20.464648   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.45775 (* 1 = 2.45775 loss)
I0512 09:59:20.464715   380 sgd_solver.cpp:172] Iteration 17900, lr = 1.21551e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:59:49.537147   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:00:23.347481   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:01:06.467250   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:01:46.742223   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:02:27.568100   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:03.545318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:04.961740   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_18000.caffemodel
I0512 10:03:04.999786   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_18000.solverstate
I0512 10:03:05.038211   380 solver.cpp:637] Iteration 18000, Testing net (#0)
I0512 10:03:31.070262   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:35.863893   380 solver.cpp:749] class AP 1: 0.894205
I0512 10:03:35.864966   380 solver.cpp:749] class AP 2: 0.883221
I0512 10:03:35.865311   380 solver.cpp:749] class AP 3: 0.902125
I0512 10:03:35.865319   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893184
I0512 10:03:35.865352   380 solver.cpp:284] Tests completed in 255.4s
I0512 10:03:36.538617   380 solver.cpp:354] Iteration 18000 (0.391542 iter/s, 255.4s/100 iter), 1016.5/1129.4ep, loss = 2.364
I0512 10:03:36.538779   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72345 (* 1 = 2.72345 loss)
I0512 10:03:36.538792   380 sgd_solver.cpp:172] Iteration 18000, lr = 1e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 10:03:36.541278   380 solver.cpp:391] Sparsity after update:
I0512 10:03:36.543077   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 10:03:36.543113   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 10:03:36.543145   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 10:03:36.543171   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543198   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543226   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 10:03:36.543251   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543277   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543303   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 10:03:36.543329   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543355   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543380   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 10:03:36.543407   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543433   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543459   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 10:03:36.543485   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543511   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543537   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 10:03:36.543563   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543591   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543617   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 10:03:36.543642   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 10:03:36.543668   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 10:03:36.543695   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 10:03:36.543731   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 10:03:36.543757   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 10:03:36.543782   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 10:03:36.543808   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 10:03:36.543833   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 10:03:36.543859   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 10:03:53.986996   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:04:34.130010   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:05:15.419371   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:05:55.690867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:06:35.354212   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:07:06.036717   380 solver.cpp:354] Iteration 18100 (0.477332 iter/s, 209.498s/100 iter), 1022.1/1129.4ep, loss = 2.37254
I0512 10:07:06.037008   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33276 (* 1 = 2.33276 loss)
I0512 10:07:06.037099   380 sgd_solver.cpp:172] Iteration 18100, lr = 8.14507e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:07:30.109714   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:08:10.033643   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:08:56.287663   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:09:36.800096   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:10:15.656172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:11:00.226097   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:11:18.488560   380 solver.cpp:354] Iteration 18200 (0.396116 iter/s, 252.451s/100 iter), 1027.8/1129.4ep, loss = 2.41399
I0512 10:11:18.488835   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35256 (* 1 = 2.35256 loss)
I0512 10:11:18.488921   380 sgd_solver.cpp:172] Iteration 18200, lr = 6.56099e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:11:37.381693   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:12:27.789412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:13:07.988039   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:13:57.189648   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:14:37.208496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:15:16.227200   380 solver.cpp:354] Iteration 18300 (0.420631 iter/s, 237.738s/100 iter), 1033.4/1129.4ep, loss = 2.35758
I0512 10:15:16.271337   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.95332 (* 1 = 1.95332 loss)
I0512 10:15:16.271955   380 sgd_solver.cpp:172] Iteration 18300, lr = 5.22006e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:15:16.977077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:15:58.747262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:16:38.960530   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:17:18.091311   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:17:56.294893   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:18:41.618063   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:19:08.334026   380 solver.cpp:354] Iteration 18400 (0.430834 iter/s, 232.108s/100 iter), 1039.1/1129.4ep, loss = 2.42412
I0512 10:19:08.334183   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.63416 (* 1 = 2.63416 loss)
I0512 10:19:08.334226   380 sgd_solver.cpp:172] Iteration 18400, lr = 4.096e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:19:23.928412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:20:11.579933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:20:55.875021   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:21:32.531742   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:22:21.297595   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:22:58.824362   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:23:02.914670   380 solver.cpp:637] Iteration 18500, Testing net (#0)
I0512 10:23:27.618016   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:23:32.076793   380 solver.cpp:749] class AP 1: 0.894244
I0512 10:23:32.078614   380 solver.cpp:749] class AP 2: 0.883184
I0512 10:23:32.079195   380 solver.cpp:749] class AP 3: 0.902082
I0512 10:23:32.079210   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89317
I0512 10:23:32.079277   380 solver.cpp:284] Tests completed in 263.747s
I0512 10:23:32.924772   380 solver.cpp:354] Iteration 18500 (0.379152 iter/s, 263.747s/100 iter), 1044.7/1129.4ep, loss = 2.42639
I0512 10:23:32.924883   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.13619 (* 1 = 2.13619 loss)
I0512 10:23:32.924927   380 sgd_solver.cpp:172] Iteration 18500, lr = 3.16406e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:23:41.474529   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:24:24.268353   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:25:02.933275   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:25:50.584750   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:26:25.249871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:27:14.985824   380 solver.cpp:354] Iteration 18600 (0.450325 iter/s, 222.062s/100 iter), 1050.4/1129.4ep, loss = 2.50778
I0512 10:27:14.986133   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72977 (* 1 = 2.72977 loss)
I0512 10:27:14.986200   380 sgd_solver.cpp:172] Iteration 18600, lr = 2.401e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:27:19.936694   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:27:57.307183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:28:42.810482   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:29:24.149341   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:30:02.309686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:30:39.263618   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:31:14.248764   380 solver.cpp:354] Iteration 18700 (0.417949 iter/s, 239.263s/100 iter), 1056/1129.4ep, loss = 2.36643
I0512 10:31:14.249168   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19184 (* 1 = 2.19184 loss)
I0512 10:31:14.249269   380 sgd_solver.cpp:172] Iteration 18700, lr = 1.78506e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:31:20.953889   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:32:01.320109   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:32:39.821368   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:33:27.452232   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:34:10.262470   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:34:48.508263   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:35:04.294538   380 solver.cpp:354] Iteration 18800 (0.434696 iter/s, 230.046s/100 iter), 1061.6/1129.4ep, loss = 2.50304
I0512 10:35:04.295138   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.9241 (* 1 = 2.9241 loss)
I0512 10:35:04.295331   380 sgd_solver.cpp:172] Iteration 18800, lr = 1.296e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:35:40.580126   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:36:18.823345   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:36:58.233330   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:37:38.596127   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:38:24.585521   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:39:01.600636   380 solver.cpp:354] Iteration 18900 (0.421396 iter/s, 237.306s/100 iter), 1067.3/1129.4ep, loss = 2.45993
I0512 10:39:01.600919   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17704 (* 1 = 2.17704 loss)
I0512 10:39:01.601016   380 sgd_solver.cpp:172] Iteration 18900, lr = 9.15063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:39:06.669787   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:39:43.263815   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:40:35.636435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:41:15.616729   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:42:08.310307   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:42:44.338111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:43:00.499802   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_19000.caffemodel
I0512 10:43:00.522817   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_19000.solverstate
I0512 10:43:00.546180   380 solver.cpp:637] Iteration 19000, Testing net (#0)
I0512 10:43:24.714201   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:43:28.755618   380 solver.cpp:749] class AP 1: 0.893837
I0512 10:43:28.756716   380 solver.cpp:749] class AP 2: 0.882615
I0512 10:43:28.757050   380 solver.cpp:749] class AP 3: 0.901859
I0512 10:43:28.757056   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89277
I0512 10:43:28.757083   380 solver.cpp:284] Tests completed in 267.157s
I0512 10:43:29.377202   380 solver.cpp:354] Iteration 19000 (0.374312 iter/s, 267.157s/100 iter), 1072.9/1129.4ep, loss = 2.3911
I0512 10:43:29.377315   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.45187 (* 1 = 2.45187 loss)
I0512 10:43:29.377348   380 sgd_solver.cpp:172] Iteration 19000, lr = 6.25001e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:43:32.705907   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:44:16.508692   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:44:51.424710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:45:31.700912   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:46:08.219813   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:46:53.901805   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:47:03.135761   380 solver.cpp:354] Iteration 19100 (0.46782 iter/s, 213.757s/100 iter), 1078.6/1129.4ep, loss = 2.47819
I0512 10:47:03.135973   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.73547 (* 1 = 2.73547 loss)
I0512 10:47:03.136044   380 sgd_solver.cpp:172] Iteration 19100, lr = 4.10063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:47:33.655812   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:48:22.380635   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:49:04.559005   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:49:43.341435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:50:35.243722   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:51:07.066197   380 solver.cpp:354] Iteration 19200 (0.409953 iter/s, 243.93s/100 iter), 1084.2/1129.4ep, loss = 2.46651
I0512 10:51:07.066262   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.66086 (* 1 = 2.66086 loss)
I0512 10:51:07.066273   380 sgd_solver.cpp:172] Iteration 19200, lr = 2.56001e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:51:15.088189   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:51:55.549378   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:52:34.900723   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:53:11.360589   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:53:53.550539   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:54:33.451581   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:54:59.043887   380 solver.cpp:354] Iteration 19300 (0.431076 iter/s, 231.978s/100 iter), 1089.9/1129.4ep, loss = 2.5041
I0512 10:54:59.044142   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31234 (* 1 = 2.31234 loss)
I0512 10:54:59.044237   380 sgd_solver.cpp:172] Iteration 19300, lr = 1.50063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:55:16.332653   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:55:52.994753   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:56:46.064904   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:57:23.968318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:58:07.646241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:58:53.409446   380 solver.cpp:354] Iteration 19400 (0.426684 iter/s, 234.366s/100 iter), 1095.5/1129.4ep, loss = 2.36279
I0512 10:58:53.409543   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.44068 (* 1 = 2.44068 loss)
I0512 10:58:53.409570   380 sgd_solver.cpp:172] Iteration 19400, lr = 8.09997e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 10:58:53.814441   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:59:31.142905   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:00:11.775565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:00:51.498445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:01:34.773409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:02:15.129990   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:02:46.427273   380 solver.cpp:637] Iteration 19500, Testing net (#0)
I0512 11:02:57.369012   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:03:12.871645   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:03:18.023680   380 solver.cpp:749] class AP 1: 0.893981
I0512 11:03:18.024809   380 solver.cpp:749] class AP 2: 0.878386
I0512 11:03:18.025156   380 solver.cpp:749] class AP 3: 0.901904
I0512 11:03:18.025166   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891424
I0512 11:03:18.025200   380 solver.cpp:284] Tests completed in 264.616s
I0512 11:03:18.631800   380 solver.cpp:354] Iteration 19500 (0.377906 iter/s, 264.616s/100 iter), 1101.2/1129.4ep, loss = 2.40557
I0512 11:03:18.631891   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19637 (* 1 = 2.19637 loss)
I0512 11:03:18.631924   380 sgd_solver.cpp:172] Iteration 19500, lr = 3.90624e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 11:03:50.350059   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:04:31.066613   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:05:22.685827   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:05:59.506305   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:06:46.420635   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:07:00.657367   380 solver.cpp:354] Iteration 19600 (0.450399 iter/s, 222.026s/100 iter), 1106.8/1129.4ep, loss = 2.5909
I0512 11:07:00.657501   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.86336 (* 1 = 2.86336 loss)
I0512 11:07:00.657531   380 sgd_solver.cpp:172] Iteration 19600, lr = 1.59999e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 11:07:26.213730   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:08:05.588981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:08:48.882877   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:09:28.033928   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:15.373303   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:52.054744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:54.259781   380 solver.cpp:354] Iteration 19700 (0.428078 iter/s, 233.602s/100 iter), 1112.5/1129.4ep, loss = 2.26548
I0512 11:10:54.259948   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.14124 (* 1 = 2.14124 loss)
I0512 11:10:54.259999   380 sgd_solver.cpp:172] Iteration 19700, lr = 5.06248e-11, m = 0.9, wd = 1e-05, gs = 1
I0512 11:11:34.855651   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:12:19.291388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:12:57.851460   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:13:41.991153   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:14:18.974550   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:14:50.000511   380 solver.cpp:354] Iteration 19800 (0.424195 iter/s, 235.741s/100 iter), 1118.1/1129.4ep, loss = 2.31879
I0512 11:14:50.000788   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17575 (* 1 = 2.17575 loss)
I0512 11:14:50.000876   380 sgd_solver.cpp:172] Iteration 19800, lr = 9.99996e-12, m = 0.9, wd = 1e-05, gs = 1
I0512 11:15:05.379460   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:15:41.667323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:16:32.575320   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:17:11.074219   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:17:49.139986   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:18:33.495342   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:18:45.050544   380 solver.cpp:354] Iteration 19900 (0.425441 iter/s, 235.05s/100 iter), 1123.8/1129.4ep, loss = 2.4196
I0512 11:18:45.051076   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29033 (* 1 = 2.29033 loss)
I0512 11:18:45.051331   380 sgd_solver.cpp:172] Iteration 19900, lr = 6.24998e-13, m = 0.9, wd = 1e-05, gs = 1
I0512 11:19:08.698869   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:19:57.257997   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:20:36.218704   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:21:14.165791   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:21:55.740710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:22:40.995616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:22:41.113509   380 solver.cpp:354] Iteration 19999 (0.41938 iter/s, 236.063s/99 iter), 1129.4/1129.4ep, loss = 2.47274
I0512 11:22:41.114009   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3165 (* 1 = 2.3165 loss)
I0512 11:22:41.115195   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.caffemodel
I0512 11:22:41.173271   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.solverstate
I0512 11:22:41.241598   380 solver.cpp:421] Sparsity after training:
I0512 11:22:41.251746   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 11:22:41.252666   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 11:22:41.252876   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 11:22:41.253069   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 11:22:41.253253   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 11:22:41.253446   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 11:22:41.253631   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 11:22:41.253814   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 11:22:41.253998   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 11:22:41.254182   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 11:22:41.254364   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 11:22:41.254549   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 11:22:41.254734   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 11:22:41.254920   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 11:22:41.255105   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 11:22:41.255288   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 11:22:41.255466   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 11:22:41.255650   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 11:22:41.255831   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 11:22:41.256014   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 11:22:41.256202   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 11:22:41.256395   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 11:22:41.256578   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 11:22:41.256772   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 11:22:41.256953   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 11:22:41.257115   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 11:22:41.257279   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 11:22:41.257448   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 11:22:41.257609   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 11:22:41.257769   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 11:22:42.144740   380 solver.cpp:503] Iteration 20000, loss = 2.44613
I0512 11:22:42.157303   380 solver.cpp:637] Iteration 20000, Testing net (#0)
I0512 11:23:04.479737   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:23:05.758611   422 blocking_queue.cpp:40] Data layer prefetch queue empty
I0512 11:23:08.859124   380 solver.cpp:749] class AP 1: 0.894072
I0512 11:23:08.860196   380 solver.cpp:749] class AP 2: 0.883149
I0512 11:23:08.860532   380 solver.cpp:749] class AP 3: 0.901858
I0512 11:23:08.860538   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893027
I0512 11:23:08.860563   380 caffe.cpp:268] Solver performance on device 0: 0.4086 * 48 = 19.61 img/sec (20000 itr in 4.894e+04 sec)
I0512 11:23:08.860569   380 caffe.cpp:271] Optimization Done in 13h 37m 26s
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument
*** Aborted at 1589282588 (unix time) try "date -d @1589282588" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x17c) received by PID 380 (TID 0x7f2fc7fff700) from PID 380; stack trace: ***
    @     0x7f32025c1f20 (unknown)
    @     0x7f32025c1e97 gsignal
    @     0x7f32025c3801 abort
    @     0x7f32066cd84a __gnu_cxx::__verbose_terminate_handler()
    @     0x7f32066cbf47 __cxxabiv1::__terminate()
    @     0x7f32066cbf7d std::terminate()
    @     0x7f32066cc15a __cxa_throw
    @     0x7f32041720ac boost::throw_exception<>()
    @     0x7f320417219d boost::mutex::lock()
    @     0x7f3204173020 boost::unique_lock<>::lock()
    @     0x7f320460a89f caffe::BlockingQueue<>::push()
    @     0x7f32041f149e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7f320422b476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7f320419f7ee caffe::InternalThread::entry()
    @     0x7f32041a154b boost::detail::thread_data<>::run()
    @     0x7f32037d97ee thread_proxy
    @     0x7f320236b6db start_thread
    @     0x7f32026a488f clone
    @                0x0 (unknown)
