I0511 10:37:52.968363   286 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 10:37:52 2020
I0511 10:37:53.372486   286 caffe.cpp:904] CuDNN version: 7605
I0511 10:37:53.372524   286 caffe.cpp:905] CuBLAS version: 10202
I0511 10:37:53.372560   286 caffe.cpp:906] CUDA version: 10020
I0511 10:37:53.372596   286 caffe.cpp:907] CUDA driver version: 10020
I0511 10:37:53.372632   286 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/solver.prototxt
[3]: --weights=/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
[4]: --gpu
[5]: 0
I0511 10:37:53.406301   286 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 10:37:53.406407   286 gpu_memory.cpp:107] Total memory: 16900227072, Free: 11730550784, dev_info[0]: total=16900227072 free=11730550784
I0511 10:37:53.406695   286 caffe.cpp:226] Using GPUs 0
I0511 10:37:53.406881   286 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 10:37:53.406996   286 solver.cpp:41] Solver data type: FLOAT
I0511 10:37:53.419385   286 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/test.prototxt"
test_iter: 107
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 0.0005
snapshot: 2000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 2
type: "Adam"
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 10:37:53.419783   286 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/train.prototxt
I0511 10:37:53.422184   286 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 16
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 10:37:53.424973   286 net.cpp:110] Using FLOAT as default forward math type
I0511 10:37:53.425005   286 net.cpp:116] Using FLOAT as default backward math type
I0511 10:37:53.425037   286 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 10:37:53.425060   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.425202   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:37:53.425680   291 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 10:37:53.429348   286 net.cpp:200] Created Layer data (0)
I0511 10:37:53.429368   286 net.cpp:542] data -> data
I0511 10:37:53.429399   286 net.cpp:542] data -> label
I0511 10:37:53.429457   286 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 16
I0511 10:37:53.429569   286 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 10:37:53.437425   292 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.445333   293 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.465340   294 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.469344   295 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.496456   286 annotated_data_layer.cpp:105] output data size: 16,3,320,768
I0511 10:37:53.497028   286 annotated_data_layer.cpp:150] [0] Output data size: 16, 3, 320, 768
I0511 10:37:53.497166   286 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 10:37:53.505786   296 data_layer.cpp:105] [0] Parser threads: 4
I0511 10:37:53.505829   296 data_layer.cpp:107] [0] Transformer threads: 4
I0511 10:37:53.553561   286 net.cpp:260] Setting up data
I0511 10:37:53.553637   286 net.cpp:267] TRAIN Top shape for layer 0 'data' 16 3 320 768 (11796480)
I0511 10:37:53.553712   286 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 10:37:53.553766   286 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 10:37:53.553802   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.553840   286 net.cpp:200] Created Layer data_data_0_split (1)
I0511 10:37:53.553910   286 net.cpp:572] data_data_0_split <- data
I0511 10:37:53.553964   286 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 10:37:53.554014   286 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 10:37:53.554069   286 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 10:37:53.554131   286 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 10:37:53.554159   286 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 10:37:53.554214   286 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 10:37:53.554246   286 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 10:37:53.554447   286 net.cpp:260] Setting up data_data_0_split
I0511 10:37:53.554484   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554533   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554599   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554643   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554708   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554770   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554829   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554913   286 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 10:37:53.554952   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.555014   286 net.cpp:200] Created Layer data/bias (2)
I0511 10:37:53.555065   286 net.cpp:572] data/bias <- data_data_0_split_0
I0511 10:37:53.555112   286 net.cpp:542] data/bias -> data/bias
I0511 10:37:53.561565   286 net.cpp:260] Setting up data/bias
I0511 10:37:53.561621   286 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 16 3 320 768 (11796480)
I0511 10:37:53.565732   286 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 10:37:53.565774   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.565829   286 net.cpp:200] Created Layer conv1a (3)
I0511 10:37:53.565857   286 net.cpp:572] conv1a <- data/bias
I0511 10:37:53.565886   286 net.cpp:542] conv1a -> conv1a
I0511 10:37:59.451964   286 net.cpp:260] Setting up conv1a
I0511 10:37:59.465309   286 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 16 32 160 384 (31457280)
I0511 10:37:59.465442   286 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 10:37:59.465523   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.465617   286 net.cpp:200] Created Layer conv1a/bn (4)
I0511 10:37:59.465699   286 net.cpp:572] conv1a/bn <- conv1a
I0511 10:37:59.465804   286 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 10:37:59.466562   286 net.cpp:260] Setting up conv1a/bn
I0511 10:37:59.466614   286 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 16 32 160 384 (31457280)
I0511 10:37:59.466962   286 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 10:37:59.467013   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.467269   286 net.cpp:200] Created Layer conv1a/relu (5)
I0511 10:37:59.467329   286 net.cpp:572] conv1a/relu <- conv1a
I0511 10:37:59.467588   286 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 10:37:59.467789   286 net.cpp:260] Setting up conv1a/relu
I0511 10:37:59.467974   286 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 16 32 160 384 (31457280)
I0511 10:37:59.468058   286 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 10:37:59.468140   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.468256   286 net.cpp:200] Created Layer conv1b (6)
I0511 10:37:59.468392   286 net.cpp:572] conv1b <- conv1a
I0511 10:37:59.468518   286 net.cpp:542] conv1b -> conv1b
I0511 10:37:59.469379   286 net.cpp:260] Setting up conv1b
I0511 10:37:59.469506   286 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 16 32 160 384 (31457280)
I0511 10:37:59.469660   286 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 10:37:59.469785   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.469920   286 net.cpp:200] Created Layer conv1b/bn (7)
I0511 10:37:59.470041   286 net.cpp:572] conv1b/bn <- conv1b
I0511 10:37:59.470170   286 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 10:37:59.470826   286 net.cpp:260] Setting up conv1b/bn
I0511 10:37:59.470933   286 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 16 32 160 384 (31457280)
I0511 10:37:59.471078   286 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 10:37:59.471200   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.471329   286 net.cpp:200] Created Layer conv1b/relu (8)
I0511 10:37:59.471457   286 net.cpp:572] conv1b/relu <- conv1b
I0511 10:37:59.471585   286 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 10:37:59.471714   286 net.cpp:260] Setting up conv1b/relu
I0511 10:37:59.471837   286 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 16 32 160 384 (31457280)
I0511 10:37:59.472045   286 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 10:37:59.472086   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.472327   286 net.cpp:200] Created Layer pool1 (9)
I0511 10:37:59.472543   286 net.cpp:572] pool1 <- conv1b
I0511 10:37:59.472643   286 net.cpp:542] pool1 -> pool1
I0511 10:37:59.472937   286 net.cpp:260] Setting up pool1
I0511 10:37:59.473031   286 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 16 32 80 192 (7864320)
I0511 10:37:59.473181   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 10:37:59.473318   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.473501   286 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 10:37:59.473603   286 net.cpp:572] res2a_branch2a <- pool1
I0511 10:37:59.473745   286 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 10:37:59.475531   286 net.cpp:260] Setting up res2a_branch2a
I0511 10:37:59.475644   286 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 16 64 80 192 (15728640)
I0511 10:37:59.475800   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.475946   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.476089   286 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 10:37:59.476217   286 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 10:37:59.476331   286 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 10:37:59.476917   286 net.cpp:260] Setting up res2a_branch2a/bn
I0511 10:37:59.477020   286 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 16 64 80 192 (15728640)
I0511 10:37:59.477159   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.477272   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.477425   286 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 10:37:59.477542   286 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 10:37:59.477661   286 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 10:37:59.477784   286 net.cpp:260] Setting up res2a_branch2a/relu
I0511 10:37:59.477896   286 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 16 64 80 192 (15728640)
I0511 10:37:59.478019   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 10:37:59.478137   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.478271   286 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 10:37:59.478390   286 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 10:37:59.478509   286 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 10:37:59.479214   286 net.cpp:260] Setting up res2a_branch2b
I0511 10:37:59.479326   286 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 16 64 80 192 (15728640)
I0511 10:37:59.479466   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.479590   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.479787   286 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 10:37:59.479828   286 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 10:37:59.480096   286 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 10:37:59.480671   286 net.cpp:260] Setting up res2a_branch2b/bn
I0511 10:37:59.480715   286 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 16 64 80 192 (15728640)
I0511 10:37:59.480903   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.480942   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.481098   286 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 10:37:59.481137   286 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 10:37:59.481298   286 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 10:37:59.481403   286 net.cpp:260] Setting up res2a_branch2b/relu
I0511 10:37:59.481441   286 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 16 64 80 192 (15728640)
I0511 10:37:59.481597   286 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 10:37:59.481645   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.481691   286 net.cpp:200] Created Layer pool2 (16)
I0511 10:37:59.481793   286 net.cpp:572] pool2 <- res2a_branch2b
I0511 10:37:59.481997   286 net.cpp:542] pool2 -> pool2
I0511 10:37:59.482169   286 net.cpp:260] Setting up pool2
I0511 10:37:59.482208   286 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 16 64 40 96 (3932160)
I0511 10:37:59.482313   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 10:37:59.482456   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.482568   286 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 10:37:59.482607   286 net.cpp:572] res3a_branch2a <- pool2
I0511 10:37:59.482715   286 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 10:37:59.484709   286 net.cpp:260] Setting up res3a_branch2a
I0511 10:37:59.484831   286 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 16 128 40 96 (7864320)
I0511 10:37:59.484941   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.484982   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.485136   286 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 10:37:59.485174   286 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 10:37:59.485332   286 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 10:37:59.485805   286 net.cpp:260] Setting up res3a_branch2a/bn
I0511 10:37:59.485906   286 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 16 128 40 96 (7864320)
I0511 10:37:59.486021   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.486063   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.486212   286 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 10:37:59.486250   286 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 10:37:59.486397   286 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 10:37:59.486439   286 net.cpp:260] Setting up res3a_branch2a/relu
I0511 10:37:59.486595   286 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 16 128 40 96 (7864320)
I0511 10:37:59.486641   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 10:37:59.486805   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.486927   286 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 10:37:59.487046   286 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 10:37:59.487154   286 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 10:37:59.488355   286 net.cpp:260] Setting up res3a_branch2b
I0511 10:37:59.488399   286 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 16 128 40 96 (7864320)
I0511 10:37:59.488584   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.488688   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.488801   286 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 10:37:59.488955   286 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 10:37:59.489008   286 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 10:37:59.497689   286 net.cpp:260] Setting up res3a_branch2b/bn
I0511 10:37:59.497738   286 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 16 128 40 96 (7864320)
I0511 10:37:59.497947   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.497995   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.498129   286 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 10:37:59.498368   286 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 10:37:59.498533   286 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 10:37:59.498705   286 net.cpp:260] Setting up res3a_branch2b/relu
I0511 10:37:59.498870   286 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 16 128 40 96 (7864320)
I0511 10:37:59.499037   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 10:37:59.499202   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.499375   286 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 10:37:59.499539   286 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 10:37:59.499709   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:37:59.499903   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:37:59.500131   286 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 10:37:59.500305   286 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 10:37:59.500485   286 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 10:37:59.500660   286 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 10:37:59.500834   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.501019   286 net.cpp:200] Created Layer pool3 (24)
I0511 10:37:59.501199   286 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:37:59.501394   286 net.cpp:542] pool3 -> pool3
I0511 10:37:59.501638   286 net.cpp:260] Setting up pool3
I0511 10:37:59.501819   286 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 16 128 20 48 (1966080)
I0511 10:37:59.502001   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 10:37:59.502171   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.502355   286 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 10:37:59.502528   286 net.cpp:572] res4a_branch2a <- pool3
I0511 10:37:59.502709   286 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 10:37:59.578085   286 net.cpp:260] Setting up res4a_branch2a
I0511 10:37:59.578179   286 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 16 256 20 48 (3932160)
I0511 10:37:59.578251   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.578287   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.578341   286 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 10:37:59.578379   286 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 10:37:59.578419   286 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 10:37:59.578893   286 net.cpp:260] Setting up res4a_branch2a/bn
I0511 10:37:59.578943   286 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 16 256 20 48 (3932160)
I0511 10:37:59.578996   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.579032   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.579071   286 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 10:37:59.579107   286 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 10:37:59.579140   286 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 10:37:59.579180   286 net.cpp:260] Setting up res4a_branch2a/relu
I0511 10:37:59.579212   286 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 16 256 20 48 (3932160)
I0511 10:37:59.579249   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 10:37:59.579284   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.579330   286 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 10:37:59.579365   286 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 10:37:59.579417   286 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 10:37:59.582883   286 net.cpp:260] Setting up res4a_branch2b
I0511 10:37:59.589303   286 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 16 256 20 48 (3932160)
I0511 10:37:59.589373   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.589411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.589454   286 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 10:37:59.589490   286 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 10:37:59.589535   286 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 10:37:59.590063   286 net.cpp:260] Setting up res4a_branch2b/bn
I0511 10:37:59.590102   286 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 16 256 20 48 (3932160)
I0511 10:37:59.590152   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.590188   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.590238   286 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 10:37:59.590282   286 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 10:37:59.590327   286 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 10:37:59.590374   286 net.cpp:260] Setting up res4a_branch2b/relu
I0511 10:37:59.590416   286 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 16 256 20 48 (3932160)
I0511 10:37:59.590463   286 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 10:37:59.590507   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.590556   286 net.cpp:200] Created Layer pool4 (31)
I0511 10:37:59.590600   286 net.cpp:572] pool4 <- res4a_branch2b
I0511 10:37:59.590644   286 net.cpp:542] pool4 -> pool4
I0511 10:37:59.590750   286 net.cpp:260] Setting up pool4
I0511 10:37:59.590867   286 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 16 256 10 24 (983040)
I0511 10:37:59.590960   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 10:37:59.591061   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.591166   286 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 10:37:59.591254   286 net.cpp:572] res5a_branch2a <- pool4
I0511 10:37:59.591392   286 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 10:37:59.694778   286 net.cpp:260] Setting up res5a_branch2a
I0511 10:37:59.702265   286 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 16 512 10 24 (1966080)
I0511 10:37:59.702711   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.703001   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.703267   286 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 10:37:59.703541   286 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 10:37:59.703794   286 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 10:37:59.704558   286 net.cpp:260] Setting up res5a_branch2a/bn
I0511 10:37:59.705312   286 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 16 512 10 24 (1966080)
I0511 10:37:59.705631   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.705883   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.706144   286 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 10:37:59.706401   286 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 10:37:59.706660   286 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 10:37:59.706919   286 net.cpp:260] Setting up res5a_branch2a/relu
I0511 10:37:59.707173   286 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 16 512 10 24 (1966080)
I0511 10:37:59.707437   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 10:37:59.707686   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.707957   286 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 10:37:59.708221   286 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 10:37:59.708467   286 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 10:37:59.752246   286 net.cpp:260] Setting up res5a_branch2b
I0511 10:37:59.758381   286 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 16 512 10 24 (1966080)
I0511 10:37:59.758863   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.759181   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.759562   286 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 10:37:59.759912   286 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 10:37:59.760257   286 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 10:37:59.761179   286 net.cpp:260] Setting up res5a_branch2b/bn
I0511 10:37:59.762125   286 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 16 512 10 24 (1966080)
I0511 10:37:59.762462   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.762768   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.763053   286 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 10:37:59.763346   286 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 10:37:59.763672   286 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 10:37:59.764040   286 net.cpp:260] Setting up res5a_branch2b/relu
I0511 10:37:59.764394   286 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 16 512 10 24 (1966080)
I0511 10:37:59.764755   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 10:37:59.765107   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.765468   286 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 10:37:59.765825   286 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 10:37:59.766185   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:37:59.766541   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:37:59.767026   286 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 10:37:59.767511   286 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 10:37:59.767911   286 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 10:37:59.768313   286 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 10:37:59.768699   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.769016   286 net.cpp:200] Created Layer pool6 (39)
I0511 10:37:59.769263   286 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:37:59.769557   286 net.cpp:542] pool6 -> pool6
I0511 10:37:59.770010   286 net.cpp:260] Setting up pool6
I0511 10:37:59.770399   286 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 16 512 5 12 (491520)
I0511 10:37:59.770747   286 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 10:37:59.771081   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.771420   286 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 10:37:59.771731   286 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 10:37:59.772058   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 10:37:59.772393   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 10:37:59.772864   286 net.cpp:260] Setting up pool6_pool6_0_split
I0511 10:37:59.773277   286 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 10:37:59.773625   286 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 10:37:59.773916   286 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 10:37:59.774183   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.774462   286 net.cpp:200] Created Layer pool7 (41)
I0511 10:37:59.774735   286 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 10:37:59.775028   286 net.cpp:542] pool7 -> pool7
I0511 10:37:59.775413   286 net.cpp:260] Setting up pool7
I0511 10:37:59.775795   286 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 16 512 3 6 (147456)
I0511 10:37:59.776063   286 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 10:37:59.776333   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.776621   286 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 10:37:59.776892   286 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 10:37:59.777154   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 10:37:59.777469   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 10:37:59.777859   286 net.cpp:260] Setting up pool7_pool7_0_split
I0511 10:37:59.778190   286 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 10:37:59.778463   286 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 10:37:59.778748   286 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 10:37:59.779024   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.779300   286 net.cpp:200] Created Layer pool8 (43)
I0511 10:37:59.779575   286 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 10:37:59.779851   286 net.cpp:542] pool8 -> pool8
I0511 10:37:59.780205   286 net.cpp:260] Setting up pool8
I0511 10:37:59.780552   286 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 16 512 2 3 (49152)
I0511 10:37:59.780839   286 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 10:37:59.781111   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.781422   286 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 10:37:59.781716   286 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 10:37:59.782016   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 10:37:59.782303   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 10:37:59.782629   286 net.cpp:260] Setting up pool8_pool8_0_split
I0511 10:37:59.782953   286 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 10:37:59.783234   286 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 10:37:59.783515   286 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 10:37:59.783794   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.784070   286 net.cpp:200] Created Layer pool9 (45)
I0511 10:37:59.784350   286 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 10:37:59.784620   286 net.cpp:542] pool9 -> pool9
I0511 10:37:59.784987   286 net.cpp:260] Setting up pool9
I0511 10:37:59.785331   286 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 16 512 1 2 (16384)
I0511 10:37:59.785663   286 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 10:37:59.785959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.786276   286 net.cpp:200] Created Layer ctx_output1 (46)
I0511 10:37:59.786576   286 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:37:59.786847   286 net.cpp:542] ctx_output1 -> ctx_output1
I0511 10:37:59.788192   286 net.cpp:260] Setting up ctx_output1
I0511 10:37:59.789578   286 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 16 256 40 96 (15728640)
I0511 10:37:59.789927   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 10:37:59.790195   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.790480   286 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 10:37:59.790767   286 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 10:37:59.791036   286 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 10:37:59.791319   286 net.cpp:260] Setting up ctx_output1/relu
I0511 10:37:59.791595   286 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 16 256 40 96 (15728640)
I0511 10:37:59.791908   286 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 10:37:59.792178   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.792452   286 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 10:37:59.792727   286 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 10:37:59.793002   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 10:37:59.793315   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 10:37:59.793637   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 10:37:59.794057   286 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 10:37:59.794401   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.794689   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.794984   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.795286   286 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 10:37:59.795585   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.795894   286 net.cpp:200] Created Layer ctx_output2 (49)
I0511 10:37:59.796196   286 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:37:59.796505   286 net.cpp:542] ctx_output2 -> ctx_output2
I0511 10:37:59.812373   286 net.cpp:260] Setting up ctx_output2
I0511 10:37:59.815264   286 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 16 256 10 24 (983040)
I0511 10:37:59.815621   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 10:37:59.815928   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.816241   286 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 10:37:59.816520   286 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 10:37:59.816821   286 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 10:37:59.817113   286 net.cpp:260] Setting up ctx_output2/relu
I0511 10:37:59.817438   286 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 16 256 10 24 (983040)
I0511 10:37:59.817778   286 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 10:37:59.818084   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.818385   286 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 10:37:59.818691   286 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 10:37:59.819000   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 10:37:59.819286   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 10:37:59.819591   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 10:37:59.820000   286 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 10:37:59.820354   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.820664   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.820971   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.821257   286 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 10:37:59.821599   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.821977   286 net.cpp:200] Created Layer ctx_output3 (52)
I0511 10:37:59.822280   286 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 10:37:59.822566   286 net.cpp:542] ctx_output3 -> ctx_output3
I0511 10:37:59.826793   286 net.cpp:260] Setting up ctx_output3
I0511 10:37:59.831095   286 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 16 256 5 12 (245760)
I0511 10:37:59.831465   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 10:37:59.831773   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.832083   286 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 10:37:59.832367   286 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 10:37:59.832654   286 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 10:37:59.832963   286 net.cpp:260] Setting up ctx_output3/relu
I0511 10:37:59.833254   286 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 16 256 5 12 (245760)
I0511 10:37:59.833595   286 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 10:37:59.833923   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.834221   286 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 10:37:59.834528   286 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 10:37:59.834816   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 10:37:59.835124   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 10:37:59.835417   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 10:37:59.835784   286 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 10:37:59.841333   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.841737   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.842072   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.842381   286 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 10:37:59.842669   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.842983   286 net.cpp:200] Created Layer ctx_output4 (55)
I0511 10:37:59.843279   286 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 10:37:59.843565   286 net.cpp:542] ctx_output4 -> ctx_output4
I0511 10:37:59.846942   286 net.cpp:260] Setting up ctx_output4
I0511 10:37:59.850342   286 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 16 256 3 6 (73728)
I0511 10:37:59.850690   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 10:37:59.850993   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.851298   286 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 10:37:59.851577   286 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 10:37:59.851882   286 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 10:37:59.852186   286 net.cpp:260] Setting up ctx_output4/relu
I0511 10:37:59.852486   286 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 16 256 3 6 (73728)
I0511 10:37:59.852778   286 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 10:37:59.853080   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.853415   286 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 10:37:59.853745   286 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 10:37:59.854048   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 10:37:59.854344   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 10:37:59.854668   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 10:37:59.855083   286 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 10:37:59.855448   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.855753   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.856036   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.856329   286 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 10:37:59.856614   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.856938   286 net.cpp:200] Created Layer ctx_output5 (58)
I0511 10:37:59.857235   286 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 10:37:59.857579   286 net.cpp:542] ctx_output5 -> ctx_output5
I0511 10:37:59.860971   286 net.cpp:260] Setting up ctx_output5
I0511 10:37:59.871109   286 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 16 256 2 3 (24576)
I0511 10:37:59.871464   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 10:37:59.871742   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.872062   286 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 10:37:59.872340   286 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 10:37:59.872620   286 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 10:37:59.872925   286 net.cpp:260] Setting up ctx_output5/relu
I0511 10:37:59.873195   286 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 16 256 2 3 (24576)
I0511 10:37:59.873533   286 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 10:37:59.873859   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.874179   286 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 10:37:59.874442   286 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 10:37:59.874747   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 10:37:59.875025   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 10:37:59.875317   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 10:37:59.875706   286 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 10:37:59.876070   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876379   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876654   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876945   286 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 10:37:59.877220   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.877590   286 net.cpp:200] Created Layer ctx_output6 (61)
I0511 10:37:59.877919   286 net.cpp:572] ctx_output6 <- pool9
I0511 10:37:59.878196   286 net.cpp:542] ctx_output6 -> ctx_output6
I0511 10:37:59.881597   286 net.cpp:260] Setting up ctx_output6
I0511 10:37:59.885067   286 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 16 256 1 2 (8192)
I0511 10:37:59.885434   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 10:37:59.885772   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.886109   286 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 10:37:59.886387   286 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 10:37:59.886701   286 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 10:37:59.886992   286 net.cpp:260] Setting up ctx_output6/relu
I0511 10:37:59.887290   286 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 16 256 1 2 (8192)
I0511 10:37:59.887578   286 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 10:37:59.887873   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.888156   286 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 10:37:59.888442   286 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 10:37:59.888722   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 10:37:59.889024   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 10:37:59.889334   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 10:37:59.889760   286 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 10:37:59.890172   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.890452   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.890740   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.891036   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.891322   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.891654   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 10:37:59.891928   286 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 10:37:59.892212   286 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 10:37:59.893016   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 10:37:59.893841   286 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 16 16 40 96 (983040)
I0511 10:37:59.894181   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.894474   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.894781   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 10:37:59.895064   286 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 10:37:59.895386   286 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.901461   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.901991   286 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 16 40 96 16 (983040)
I0511 10:37:59.902295   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.902575   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.902873   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 10:37:59.903172   286 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.903443   286 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 10:37:59.907702   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 10:37:59.912034   286 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 16 61440 (983040)
I0511 10:37:59.912406   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.912678   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.912983   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 10:37:59.913252   286 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 10:37:59.913609   286 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 10:37:59.914451   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 10:37:59.914754   286 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 16 16 40 96 (983040)
I0511 10:37:59.915050   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.915329   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.915621   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 10:37:59.915894   286 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 10:37:59.916167   286 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.916594   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.916891   286 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 16 40 96 16 (983040)
I0511 10:37:59.917168   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.917469   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.917778   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 10:37:59.918076   286 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.918344   286 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 10:37:59.922487   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 10:37:59.929324   286 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 16 61440 (983040)
I0511 10:37:59.929725   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.930029   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.930330   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 10:37:59.930595   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 10:37:59.930864   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 10:37:59.931128   286 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 10:37:59.931461   286 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 10:37:59.931733   286 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 10:37:59.932013   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.932278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.932559   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 10:37:59.932828   286 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 10:37:59.933104   286 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 10:37:59.934026   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 10:37:59.934317   286 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 16 24 10 24 (92160)
I0511 10:37:59.934614   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.934883   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.935155   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 10:37:59.935422   286 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 10:37:59.935698   286 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.936118   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.936398   286 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 16 10 24 24 (92160)
I0511 10:37:59.936692   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.936959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.937227   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 10:37:59.937526   286 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.937825   286 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 10:37:59.939034   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 10:37:59.939340   286 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 16 5760 (92160)
I0511 10:37:59.939623   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.939899   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.940181   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 10:37:59.940449   286 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 10:37:59.940718   286 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 10:37:59.941567   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 10:37:59.957307   286 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 16 24 10 24 (92160)
I0511 10:37:59.957728   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.958016   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.958307   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 10:37:59.958581   286 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 10:37:59.958848   286 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.959281   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.959558   286 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 16 10 24 24 (92160)
I0511 10:37:59.959837   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.960108   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.960374   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 10:37:59.960640   286 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.960913   286 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 10:37:59.961323   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 10:37:59.961643   286 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 16 5760 (92160)
I0511 10:37:59.961948   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.962214   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.962496   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 10:37:59.962762   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 10:37:59.963029   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 10:37:59.963297   286 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 10:37:59.963649   286 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 10:37:59.963917   286 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 10:37:59.964195   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.964466   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.964759   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 10:37:59.965018   286 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 10:37:59.965302   286 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 10:37:59.966109   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 10:37:59.966403   286 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 16 24 5 12 (23040)
I0511 10:37:59.966687   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.966953   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.967223   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 10:37:59.967491   286 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 10:37:59.967767   286 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.968178   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.968446   286 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 16 5 12 24 (23040)
I0511 10:37:59.968724   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.968992   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.969266   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 10:37:59.969578   286 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.969880   286 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 10:37:59.970299   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 10:37:59.970567   286 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 16 1440 (23040)
I0511 10:37:59.970839   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.971101   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.971374   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 10:37:59.971650   286 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 10:37:59.971922   286 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 10:37:59.972685   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 10:37:59.972981   286 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 16 24 5 12 (23040)
I0511 10:37:59.973268   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.973546   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.973821   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 10:37:59.974090   286 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 10:37:59.974371   286 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.974797   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.975065   286 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 16 5 12 24 (23040)
I0511 10:37:59.975337   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.975606   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.975886   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 10:37:59.976148   286 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.976416   286 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 10:37:59.976792   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 10:37:59.977082   286 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 16 1440 (23040)
I0511 10:37:59.977385   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.977677   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.977982   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 10:37:59.978278   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 10:37:59.978580   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 10:37:59.978849   286 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 10:37:59.979177   286 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 10:37:59.979439   286 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 10:37:59.979705   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.979969   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.980254   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 10:37:59.980523   286 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 10:37:59.980790   286 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 10:37:59.981556   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 10:37:59.981854   286 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 16 24 3 6 (6912)
I0511 10:37:59.982136   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.982411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.982668   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 10:37:59.982913   286 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 10:37:59.983155   286 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.983547   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.983800   286 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 16 3 6 24 (6912)
I0511 10:37:59.984050   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.984292   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.984539   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 10:37:59.984788   286 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.985031   286 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 10:37:59.985378   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 10:37:59.985666   286 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 16 432 (6912)
I0511 10:37:59.985949   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.986232   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.986521   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 10:37:59.986765   286 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 10:37:59.987013   286 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 10:37:59.987733   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 10:37:59.988004   286 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 16 24 3 6 (6912)
I0511 10:37:59.988271   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.988525   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.988788   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 10:37:59.989037   286 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 10:37:59.989286   286 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.989693   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.989938   286 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 16 3 6 24 (6912)
I0511 10:37:59.990186   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.990432   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.990676   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 10:37:59.990914   286 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.991154   286 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 10:37:59.991489   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 10:37:59.991739   286 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 16 432 (6912)
I0511 10:37:59.991981   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.992216   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.992476   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 10:37:59.992748   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 10:37:59.993005   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 10:37:59.993259   286 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 10:37:59.993592   286 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 10:37:59.993876   286 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 10:37:59.994168   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.994451   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.994733   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 10:37:59.994989   286 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 10:37:59.995249   286 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 10:37:59.995911   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 10:37:59.996191   286 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 16 16 2 3 (1536)
I0511 10:37:59.996464   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.996731   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.996990   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 10:37:59.997242   286 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 10:37:59.997510   286 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.997900   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.998159   286 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 16 2 3 16 (1536)
I0511 10:37:59.998421   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.998688   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.998946   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 10:37:59.999204   286 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.999472   286 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 10:37:59.999819   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 10:38:00.000079   286 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 16 96 (1536)
I0511 10:38:00.000346   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.000604   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.000869   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 10:38:00.001127   286 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:00.001404   286 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 10:38:00.002177   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 10:38:00.002485   286 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 16 16 2 3 (1536)
I0511 10:38:00.002768   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.003026   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.003296   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 10:38:00.003549   286 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 10:38:00.003804   286 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.004204   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.004463   286 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 16 2 3 16 (1536)
I0511 10:38:00.004722   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:00.004974   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.005235   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 10:38:00.005504   286 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.005764   286 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.006109   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.006381   286 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 16 96 (1536)
I0511 10:38:00.006639   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:00.006891   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.007151   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 10:38:00.007412   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:00.007671   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 10:38:00.007928   286 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 10:38:00.008208   286 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 10:38:00.008472   286 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 10:38:00.008744   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 10:38:00.008997   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.009260   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 10:38:00.009550   286 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:00.009846   286 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 10:38:00.010608   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 10:38:00.010890   286 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 16 16 1 2 (512)
I0511 10:38:00.011173   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:00.011438   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.011713   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 10:38:00.011984   286 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 10:38:00.012233   286 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.012606   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.012878   286 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 16 1 2 16 (512)
I0511 10:38:00.013146   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:00.013414   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.013679   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 10:38:00.013931   286 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.014197   286 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.014521   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.014786   286 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 16 32 (512)
I0511 10:38:00.015044   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.015311   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.015579   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 10:38:00.015839   286 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:00.016088   286 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 10:38:00.016752   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 10:38:00.017030   286 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 16 16 1 2 (512)
I0511 10:38:00.017315   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.017604   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.017906   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 10:38:00.018198   286 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 10:38:00.018492   286 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.018899   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.019171   286 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 16 1 2 16 (512)
I0511 10:38:00.019436   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:00.019692   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.019964   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 10:38:00.020226   286 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.020473   286 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.020792   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.021062   286 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 16 32 (512)
I0511 10:38:00.021327   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:00.021601   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.021878   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 10:38:00.022174   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:00.022459   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 10:38:00.022728   286 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 10:38:00.023007   286 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 10:38:00.023267   286 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 10:38:00.023528   286 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 10:38:00.023793   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.024053   286 net.cpp:200] Created Layer mbox_loc (106)
I0511 10:38:00.024310   286 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.024557   286 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 10:38:00.024827   286 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 10:38:00.025084   286 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 10:38:00.025331   286 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 10:38:00.025593   286 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.025854   286 net.cpp:542] mbox_loc -> mbox_loc
I0511 10:38:00.026161   286 net.cpp:260] Setting up mbox_loc
I0511 10:38:00.026427   286 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 16 69200 (1107200)
I0511 10:38:00.026690   286 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 10:38:00.026950   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.027223   286 net.cpp:200] Created Layer mbox_conf (107)
I0511 10:38:00.027482   286 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 10:38:00.027734   286 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 10:38:00.027990   286 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 10:38:00.028264   286 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 10:38:00.028525   286 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.028774   286 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.029032   286 net.cpp:542] mbox_conf -> mbox_conf
I0511 10:38:00.029369   286 net.cpp:260] Setting up mbox_conf
I0511 10:38:00.029661   286 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 16 69200 (1107200)
I0511 10:38:00.029956   286 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 10:38:00.030241   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.030530   286 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 10:38:00.030788   286 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 10:38:00.031039   286 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 10:38:00.031308   286 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 10:38:00.031569   286 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 10:38:00.031816   286 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 10:38:00.032065   286 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 10:38:00.032338   286 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 10:38:00.032634   286 net.cpp:260] Setting up mbox_priorbox
I0511 10:38:00.032892   286 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 10:38:00.033147   286 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 10:38:00.033418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.033694   286 net.cpp:200] Created Layer mbox_loss (109)
I0511 10:38:00.033951   286 net.cpp:572] mbox_loss <- mbox_loc
I0511 10:38:00.034198   286 net.cpp:572] mbox_loss <- mbox_conf
I0511 10:38:00.034459   286 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 10:38:00.034723   286 net.cpp:572] mbox_loss <- label
I0511 10:38:00.034971   286 net.cpp:542] mbox_loss -> mbox_loss
I0511 10:38:00.035310   286 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 10:38:00.035596   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.036000   286 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 10:38:00.036267   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.036676   286 net.cpp:260] Setting up mbox_loss
I0511 10:38:00.036952   286 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 10:38:00.037214   286 net.cpp:271]     with loss weight 1
I0511 10:38:00.037533   286 net.cpp:336] mbox_loss needs backward computation.
I0511 10:38:00.037822   286 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 10:38:00.038110   286 net.cpp:336] mbox_conf needs backward computation.
I0511 10:38:00.038395   286 net.cpp:336] mbox_loc needs backward computation.
I0511 10:38:00.038672   286 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.038923   286 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.039167   286 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.039422   286 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 10:38:00.039683   286 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.039942   286 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.040195   286 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 10:38:00.040462   286 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.040725   286 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.040988   286 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.041234   286 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 10:38:00.041489   286 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.041738   286 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.042002   286 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 10:38:00.042248   286 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.042497   286 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.042742   286 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.043005   286 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 10:38:00.043264   286 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.043509   286 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.043752   286 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 10:38:00.044003   286 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.044267   286 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.044531   286 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.044780   286 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 10:38:00.045027   286 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.045285   286 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.045579   286 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 10:38:00.045876   286 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.046154   286 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.046449   286 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.046692   286 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 10:38:00.046941   286 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.047171   286 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.047397   286 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 10:38:00.047756   286 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.047983   286 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.048216   286 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.048446   286 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 10:38:00.048677   286 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.048908   286 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.049108   286 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 10:38:00.049237   286 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 10:38:00.049373   286 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 10:38:00.049424   286 net.cpp:336] ctx_output6 needs backward computation.
I0511 10:38:00.049638   286 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 10:38:00.049688   286 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 10:38:00.049913   286 net.cpp:336] ctx_output5 needs backward computation.
I0511 10:38:00.050032   286 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 10:38:00.050163   286 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 10:38:00.050212   286 net.cpp:336] ctx_output4 needs backward computation.
I0511 10:38:00.050420   286 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 10:38:00.050544   286 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 10:38:00.050669   286 net.cpp:336] ctx_output3 needs backward computation.
I0511 10:38:00.050806   286 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 10:38:00.050942   286 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 10:38:00.051070   286 net.cpp:336] ctx_output2 needs backward computation.
I0511 10:38:00.051198   286 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 10:38:00.051246   286 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 10:38:00.051457   286 net.cpp:336] ctx_output1 needs backward computation.
I0511 10:38:00.051587   286 net.cpp:336] pool9 needs backward computation.
I0511 10:38:00.051717   286 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 10:38:00.051765   286 net.cpp:336] pool8 needs backward computation.
I0511 10:38:00.051975   286 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 10:38:00.052104   286 net.cpp:336] pool7 needs backward computation.
I0511 10:38:00.052235   286 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 10:38:00.052366   286 net.cpp:336] pool6 needs backward computation.
I0511 10:38:00.052495   286 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 10:38:00.052625   286 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 10:38:00.052757   286 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 10:38:00.052886   286 net.cpp:336] res5a_branch2b needs backward computation.
I0511 10:38:00.053017   286 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 10:38:00.053148   286 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 10:38:00.053280   286 net.cpp:336] res5a_branch2a needs backward computation.
I0511 10:38:00.053432   286 net.cpp:336] pool4 needs backward computation.
I0511 10:38:00.053573   286 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 10:38:00.053717   286 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 10:38:00.053866   286 net.cpp:336] res4a_branch2b needs backward computation.
I0511 10:38:00.054002   286 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 10:38:00.054141   286 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 10:38:00.054276   286 net.cpp:336] res4a_branch2a needs backward computation.
I0511 10:38:00.054399   286 net.cpp:336] pool3 needs backward computation.
I0511 10:38:00.054523   286 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 10:38:00.054649   286 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 10:38:00.054774   286 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 10:38:00.054823   286 net.cpp:336] res3a_branch2b needs backward computation.
I0511 10:38:00.055017   286 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 10:38:00.055140   286 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 10:38:00.055260   286 net.cpp:336] res3a_branch2a needs backward computation.
I0511 10:38:00.055380   286 net.cpp:336] pool2 needs backward computation.
I0511 10:38:00.055502   286 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 10:38:00.055552   286 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 10:38:00.055749   286 net.cpp:336] res2a_branch2b needs backward computation.
I0511 10:38:00.055873   286 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 10:38:00.056002   286 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 10:38:00.056123   286 net.cpp:336] res2a_branch2a needs backward computation.
I0511 10:38:00.056244   286 net.cpp:336] pool1 needs backward computation.
I0511 10:38:00.056295   286 net.cpp:336] conv1b/relu needs backward computation.
I0511 10:38:00.056496   286 net.cpp:336] conv1b/bn needs backward computation.
I0511 10:38:00.056547   286 net.cpp:336] conv1b needs backward computation.
I0511 10:38:00.056748   286 net.cpp:336] conv1a/relu needs backward computation.
I0511 10:38:00.056874   286 net.cpp:336] conv1a/bn needs backward computation.
I0511 10:38:00.056924   286 net.cpp:336] conv1a needs backward computation.
I0511 10:38:00.057122   286 net.cpp:338] data/bias does not need backward computation.
I0511 10:38:00.057248   286 net.cpp:338] data_data_0_split does not need backward computation.
I0511 10:38:00.057404   286 net.cpp:338] data does not need backward computation.
I0511 10:38:00.057541   286 net.cpp:380] This network produces output mbox_loss
I0511 10:38:00.057866   286 net.cpp:403] Top memory (TRAIN) required for data: 2411201928 diff: 2411201928
I0511 10:38:00.057905   286 net.cpp:406] Bottom memory (TRAIN) required for data: 2411201920 diff: 2411201920
I0511 10:38:00.058104   286 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 1043431424 diff: 1043431424
I0511 10:38:00.058224   286 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 10:38:00.058344   286 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 10:38:00.058463   286 net.cpp:421] Network initialization done.
I0511 10:38:00.060263   286 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/test.prototxt
I0511 10:38:00.073693   286 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 10:38:00.101864   286 net.cpp:110] Using FLOAT as default forward math type
I0511 10:38:00.139780   286 net.cpp:116] Using FLOAT as default backward math type
I0511 10:38:00.140223   286 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 10:38:00.140452   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.140709   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.141223   286 net.cpp:200] Created Layer data (0)
I0511 10:38:00.142010   286 net.cpp:542] data -> data
I0511 10:38:00.142218   286 net.cpp:542] data -> label
I0511 10:38:00.142448   286 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 10:38:00.142716   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.160323   329 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 10:38:00.164703   286 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 10:38:00.164973   286 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 10:38:00.165200   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.165462   286 net.cpp:260] Setting up data
I0511 10:38:00.165611   286 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 10:38:00.165777   286 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 10:38:00.165928   286 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 10:38:00.166074   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.166208   286 net.cpp:200] Created Layer data_data_0_split (1)
I0511 10:38:00.166332   286 net.cpp:572] data_data_0_split <- data
I0511 10:38:00.166435   286 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 10:38:00.166496   286 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 10:38:00.166589   286 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 10:38:00.166683   286 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 10:38:00.166741   286 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 10:38:00.166834   286 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 10:38:00.166893   286 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 10:38:00.167165   286 net.cpp:260] Setting up data_data_0_split
I0511 10:38:00.167212   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167279   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167380   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167474   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167533   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167624   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167719   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167779   286 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 10:38:00.167866   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.167945   286 net.cpp:200] Created Layer data/bias (2)
I0511 10:38:00.168018   286 net.cpp:572] data/bias <- data_data_0_split_0
I0511 10:38:00.168081   286 net.cpp:542] data/bias -> data/bias
I0511 10:38:00.168406   286 net.cpp:260] Setting up data/bias
I0511 10:38:00.168479   286 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 10:38:00.168551   286 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 10:38:00.168646   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.168756   286 net.cpp:200] Created Layer conv1a (3)
I0511 10:38:00.168850   286 net.cpp:572] conv1a <- data/bias
I0511 10:38:00.168910   286 net.cpp:542] conv1a -> conv1a
I0511 10:38:00.173552   330 data_layer.cpp:105] (0) Parser threads: 1
I0511 10:38:00.173635   330 data_layer.cpp:107] (0) Transformer threads: 1
I0511 10:38:00.173625   286 net.cpp:260] Setting up conv1a
I0511 10:38:00.236510   286 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 10:38:00.236716   286 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 10:38:00.236826   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.236943   286 net.cpp:200] Created Layer conv1a/bn (4)
I0511 10:38:00.237097   286 net.cpp:572] conv1a/bn <- conv1a
I0511 10:38:00.237200   286 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 10:38:00.284126   286 net.cpp:260] Setting up conv1a/bn
I0511 10:38:00.284152   286 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 10:38:00.284184   286 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 10:38:00.284191   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284234   286 net.cpp:200] Created Layer conv1a/relu (5)
I0511 10:38:00.284241   286 net.cpp:572] conv1a/relu <- conv1a
I0511 10:38:00.284250   286 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 10:38:00.284260   286 net.cpp:260] Setting up conv1a/relu
I0511 10:38:00.284265   286 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 10:38:00.284273   286 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 10:38:00.284278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284296   286 net.cpp:200] Created Layer conv1b (6)
I0511 10:38:00.284301   286 net.cpp:572] conv1b <- conv1a
I0511 10:38:00.284307   286 net.cpp:542] conv1b -> conv1b
I0511 10:38:00.284718   286 net.cpp:260] Setting up conv1b
I0511 10:38:00.284729   286 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 10:38:00.284746   286 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 10:38:00.284752   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284762   286 net.cpp:200] Created Layer conv1b/bn (7)
I0511 10:38:00.284768   286 net.cpp:572] conv1b/bn <- conv1b
I0511 10:38:00.284775   286 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 10:38:00.285197   286 net.cpp:260] Setting up conv1b/bn
I0511 10:38:00.285218   286 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 10:38:00.285272   286 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 10:38:00.285296   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285320   286 net.cpp:200] Created Layer conv1b/relu (8)
I0511 10:38:00.285337   286 net.cpp:572] conv1b/relu <- conv1b
I0511 10:38:00.285354   286 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 10:38:00.285375   286 net.cpp:260] Setting up conv1b/relu
I0511 10:38:00.285389   286 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 10:38:00.285413   286 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 10:38:00.285429   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285454   286 net.cpp:200] Created Layer pool1 (9)
I0511 10:38:00.285467   286 net.cpp:572] pool1 <- conv1b
I0511 10:38:00.285485   286 net.cpp:542] pool1 -> pool1
I0511 10:38:00.285624   286 net.cpp:260] Setting up pool1
I0511 10:38:00.285638   286 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 10:38:00.285660   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 10:38:00.285676   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285707   286 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 10:38:00.285723   286 net.cpp:572] res2a_branch2a <- pool1
I0511 10:38:00.285739   286 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 10:38:00.287034   286 net.cpp:260] Setting up res2a_branch2a
I0511 10:38:00.287050   286 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 10:38:00.287086   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.287101   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.287124   286 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 10:38:00.287138   286 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 10:38:00.287155   286 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 10:38:00.288167   286 net.cpp:260] Setting up res2a_branch2a/bn
I0511 10:38:00.288182   286 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 10:38:00.288225   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.288278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.288298   286 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 10:38:00.288312   286 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 10:38:00.288329   286 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 10:38:00.288350   286 net.cpp:260] Setting up res2a_branch2a/relu
I0511 10:38:00.288363   286 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 10:38:00.288388   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 10:38:00.288401   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.288431   286 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 10:38:00.288446   286 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 10:38:00.288462   286 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 10:38:00.292881   286 net.cpp:260] Setting up res2a_branch2b
I0511 10:38:00.293035   286 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 10:38:00.293248   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.293433   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.293599   286 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 10:38:00.293740   286 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 10:38:00.293881   286 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 10:38:00.294829   286 net.cpp:260] Setting up res2a_branch2b/bn
I0511 10:38:00.294916   286 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 10:38:00.295183   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.295375   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.295568   286 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 10:38:00.295761   286 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 10:38:00.295887   286 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 10:38:00.296032   286 net.cpp:260] Setting up res2a_branch2b/relu
I0511 10:38:00.296162   286 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 10:38:00.296303   286 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 10:38:00.296440   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.296586   286 net.cpp:200] Created Layer pool2 (16)
I0511 10:38:00.296721   286 net.cpp:572] pool2 <- res2a_branch2b
I0511 10:38:00.296861   286 net.cpp:542] pool2 -> pool2
I0511 10:38:00.297103   286 net.cpp:260] Setting up pool2
I0511 10:38:00.297224   286 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 10:38:00.297375   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 10:38:00.297508   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.297652   286 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 10:38:00.297777   286 net.cpp:572] res3a_branch2a <- pool2
I0511 10:38:00.297909   286 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 10:38:00.300310   286 net.cpp:260] Setting up res3a_branch2a
I0511 10:38:00.300524   286 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 10:38:00.300673   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.300793   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.300966   286 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 10:38:00.301105   286 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 10:38:00.301194   286 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 10:38:00.301964   286 net.cpp:260] Setting up res3a_branch2a/bn
I0511 10:38:00.305519   286 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 10:38:00.305819   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.306018   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.306151   286 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 10:38:00.306288   286 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 10:38:00.306413   286 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 10:38:00.306550   286 net.cpp:260] Setting up res3a_branch2a/relu
I0511 10:38:00.306694   286 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 10:38:00.306833   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 10:38:00.306962   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.307097   286 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 10:38:00.307262   286 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 10:38:00.307386   286 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 10:38:00.308964   286 net.cpp:260] Setting up res3a_branch2b
I0511 10:38:00.313462   286 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 10:38:00.313647   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.313791   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.313925   286 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 10:38:00.314065   286 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 10:38:00.314185   286 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 10:38:00.321985   286 net.cpp:260] Setting up res3a_branch2b/bn
I0511 10:38:00.322772   286 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 10:38:00.323020   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.323264   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.323441   286 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 10:38:00.323627   286 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 10:38:00.323793   286 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 10:38:00.323976   286 net.cpp:260] Setting up res3a_branch2b/relu
I0511 10:38:00.324156   286 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 10:38:00.324348   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 10:38:00.324515   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.324681   286 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 10:38:00.324870   286 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 10:38:00.325032   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:38:00.325199   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:38:00.325417   286 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 10:38:00.325734   286 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 10:38:00.325875   286 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 10:38:00.326009   286 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 10:38:00.326130   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.326264   286 net.cpp:200] Created Layer pool3 (24)
I0511 10:38:00.326409   286 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:38:00.326514   286 net.cpp:542] pool3 -> pool3
I0511 10:38:00.326699   286 net.cpp:260] Setting up pool3
I0511 10:38:00.327030   286 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 10:38:00.327154   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 10:38:00.327256   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.327366   286 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 10:38:00.327491   286 net.cpp:572] res4a_branch2a <- pool3
I0511 10:38:00.327592   286 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 10:38:00.359237   286 net.cpp:260] Setting up res4a_branch2a
I0511 10:38:00.372788   286 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 10:38:00.373016   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.373152   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.373265   286 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 10:38:00.373409   286 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 10:38:00.373504   286 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 10:38:00.374991   286 net.cpp:260] Setting up res4a_branch2a/bn
I0511 10:38:00.376276   286 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 10:38:00.376410   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.376525   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.376621   286 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 10:38:00.376788   286 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 10:38:00.376873   286 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 10:38:00.376981   286 net.cpp:260] Setting up res4a_branch2a/relu
I0511 10:38:00.377132   286 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 10:38:00.377230   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 10:38:00.377382   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.377481   286 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 10:38:00.377676   286 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 10:38:00.377753   286 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 10:38:00.381403   286 net.cpp:260] Setting up res4a_branch2b
I0511 10:38:00.409235   286 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 10:38:00.409476   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.409621   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.409763   286 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 10:38:00.409902   286 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 10:38:00.410006   286 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 10:38:00.410929   286 net.cpp:260] Setting up res4a_branch2b/bn
I0511 10:38:00.413265   286 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 10:38:00.413422   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.413566   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.413708   286 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 10:38:00.413805   286 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 10:38:00.413940   286 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 10:38:00.414050   286 net.cpp:260] Setting up res4a_branch2b/relu
I0511 10:38:00.414196   286 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 10:38:00.414384   286 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 10:38:00.414489   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.414613   286 net.cpp:200] Created Layer pool4 (31)
I0511 10:38:00.414752   286 net.cpp:572] pool4 <- res4a_branch2b
I0511 10:38:00.414855   286 net.cpp:542] pool4 -> pool4
I0511 10:38:00.415100   286 net.cpp:260] Setting up pool4
I0511 10:38:00.415486   286 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 10:38:00.415629   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 10:38:00.415730   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.415884   286 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 10:38:00.416031   286 net.cpp:572] res5a_branch2a <- pool4
I0511 10:38:00.416136   286 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 10:38:00.563432   286 net.cpp:260] Setting up res5a_branch2a
I0511 10:38:00.570969   286 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 10:38:00.571703   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.571997   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.572183   286 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 10:38:00.572372   286 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 10:38:00.572612   286 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 10:38:00.573835   286 net.cpp:260] Setting up res5a_branch2a/bn
I0511 10:38:00.576297   286 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 10:38:00.576719   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.577056   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.577350   286 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 10:38:00.577684   286 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 10:38:00.577986   286 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 10:38:00.578306   286 net.cpp:260] Setting up res5a_branch2a/relu
I0511 10:38:00.578636   286 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 10:38:00.585314   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 10:38:00.585705   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.586031   286 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 10:38:00.586378   286 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 10:38:00.586674   286 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 10:38:00.703346   286 net.cpp:260] Setting up res5a_branch2b
I0511 10:38:00.716594   286 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 10:38:00.717247   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.717890   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.718536   286 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 10:38:00.719210   286 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 10:38:00.719818   286 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 10:38:00.720806   286 net.cpp:260] Setting up res5a_branch2b/bn
I0511 10:38:00.723659   286 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 10:38:00.724120   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.724508   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.724864   286 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 10:38:00.725219   286 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 10:38:00.725749   286 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 10:38:00.726313   286 net.cpp:260] Setting up res5a_branch2b/relu
I0511 10:38:00.726874   286 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 10:38:00.727447   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 10:38:00.727969   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.728497   286 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 10:38:00.729038   286 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 10:38:00.729553   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:38:00.730093   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:38:00.730782   286 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 10:38:00.732031   286 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 10:38:00.732620   286 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 10:38:00.733178   286 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 10:38:00.733645   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.733929   286 net.cpp:200] Created Layer pool6 (39)
I0511 10:38:00.734226   286 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:38:00.734499   286 net.cpp:542] pool6 -> pool6
I0511 10:38:00.734931   286 net.cpp:260] Setting up pool6
I0511 10:38:00.736011   286 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 10:38:00.736325   286 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 10:38:00.736589   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.736857   286 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 10:38:00.737136   286 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 10:38:00.737391   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 10:38:00.737692   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 10:38:00.738068   286 net.cpp:260] Setting up pool6_pool6_0_split
I0511 10:38:00.738816   286 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 10:38:00.739123   286 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 10:38:00.739426   286 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 10:38:00.739682   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.739948   286 net.cpp:200] Created Layer pool7 (41)
I0511 10:38:00.740243   286 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 10:38:00.740463   286 net.cpp:542] pool7 -> pool7
I0511 10:38:00.740705   286 net.cpp:260] Setting up pool7
I0511 10:38:00.741253   286 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 10:38:00.741466   286 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 10:38:00.741647   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.741816   286 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 10:38:00.742002   286 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 10:38:00.742166   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 10:38:00.742347   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 10:38:00.742597   286 net.cpp:260] Setting up pool7_pool7_0_split
I0511 10:38:00.743060   286 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 10:38:00.743328   286 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 10:38:00.743525   286 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 10:38:00.743698   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.743867   286 net.cpp:200] Created Layer pool8 (43)
I0511 10:38:00.744055   286 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 10:38:00.744220   286 net.cpp:542] pool8 -> pool8
I0511 10:38:00.744480   286 net.cpp:260] Setting up pool8
I0511 10:38:00.745043   286 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 10:38:00.745244   286 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 10:38:00.745425   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.745620   286 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 10:38:00.745817   286 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 10:38:00.745999   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 10:38:00.746189   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 10:38:00.746438   286 net.cpp:260] Setting up pool8_pool8_0_split
I0511 10:38:00.746917   286 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 10:38:00.747118   286 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 10:38:00.747320   286 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 10:38:00.747503   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.747689   286 net.cpp:200] Created Layer pool9 (45)
I0511 10:38:00.747886   286 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 10:38:00.748059   286 net.cpp:542] pool9 -> pool9
I0511 10:38:00.748311   286 net.cpp:260] Setting up pool9
I0511 10:38:00.748843   286 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 10:38:00.749049   286 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 10:38:00.749225   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.749434   286 net.cpp:200] Created Layer ctx_output1 (46)
I0511 10:38:00.749675   286 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:38:00.749852   286 net.cpp:542] ctx_output1 -> ctx_output1
I0511 10:38:00.751111   286 net.cpp:260] Setting up ctx_output1
I0511 10:38:00.756677   286 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 10:38:00.757061   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 10:38:00.757411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.757840   286 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 10:38:00.758211   286 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 10:38:00.758512   286 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 10:38:00.758862   286 net.cpp:260] Setting up ctx_output1/relu
I0511 10:38:00.759196   286 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 10:38:00.759539   286 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 10:38:00.759840   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.760138   286 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 10:38:00.760450   286 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 10:38:00.760742   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 10:38:00.761060   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 10:38:00.761397   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 10:38:00.761997   286 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 10:38:00.763012   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.763362   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.763697   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.764024   286 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 10:38:00.764314   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.764631   286 net.cpp:200] Created Layer ctx_output2 (49)
I0511 10:38:00.765009   286 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:38:00.765303   286 net.cpp:542] ctx_output2 -> ctx_output2
I0511 10:38:00.785568   286 net.cpp:260] Setting up ctx_output2
I0511 10:38:00.796783   286 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 10:38:00.797606   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 10:38:00.798049   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.798391   286 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 10:38:00.798707   286 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 10:38:00.799026   286 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 10:38:00.799343   286 net.cpp:260] Setting up ctx_output2/relu
I0511 10:38:00.799687   286 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 10:38:00.799973   286 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 10:38:00.800173   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.800361   286 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 10:38:00.800559   286 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 10:38:00.800806   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 10:38:00.801097   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 10:38:00.801417   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 10:38:00.801928   286 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 10:38:00.803295   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803515   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803692   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803872   286 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 10:38:00.803959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.804122   286 net.cpp:200] Created Layer ctx_output3 (52)
I0511 10:38:00.804332   286 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 10:38:00.804495   286 net.cpp:542] ctx_output3 -> ctx_output3
I0511 10:38:00.807998   286 net.cpp:260] Setting up ctx_output3
I0511 10:38:00.853966   286 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 10:38:00.854054   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 10:38:00.854094   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.854138   286 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 10:38:00.854307   286 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 10:38:00.854369   286 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 10:38:00.854499   286 net.cpp:260] Setting up ctx_output3/relu
I0511 10:38:00.854560   286 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 10:38:00.854614   286 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 10:38:00.856178   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.856326   286 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 10:38:00.856459   286 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 10:38:00.856592   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 10:38:00.856725   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 10:38:00.856856   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 10:38:00.857107   286 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 10:38:00.857239   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857378   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857513   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857647   286 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 10:38:00.857782   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.857921   286 net.cpp:200] Created Layer ctx_output4 (55)
I0511 10:38:00.858053   286 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 10:38:00.858180   286 net.cpp:542] ctx_output4 -> ctx_output4
I0511 10:38:00.861441   286 net.cpp:260] Setting up ctx_output4
I0511 10:38:00.861624   286 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 10:38:00.861768   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 10:38:00.861899   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.862032   286 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 10:38:00.862161   286 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 10:38:00.862291   286 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 10:38:00.862423   286 net.cpp:260] Setting up ctx_output4/relu
I0511 10:38:00.862551   286 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 10:38:00.862684   286 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 10:38:00.862818   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.862957   286 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 10:38:00.863085   286 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 10:38:00.863217   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 10:38:00.863346   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 10:38:00.863478   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 10:38:00.863682   286 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 10:38:00.863811   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.863945   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.864074   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.864207   286 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 10:38:00.864341   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.864490   286 net.cpp:200] Created Layer ctx_output5 (58)
I0511 10:38:00.864634   286 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 10:38:00.864852   286 net.cpp:542] ctx_output5 -> ctx_output5
I0511 10:38:00.868456   286 net.cpp:260] Setting up ctx_output5
I0511 10:38:00.892488   286 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 10:38:00.893256   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 10:38:00.893589   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.893831   286 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 10:38:00.894083   286 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 10:38:00.894318   286 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 10:38:00.894584   286 net.cpp:260] Setting up ctx_output5/relu
I0511 10:38:00.894876   286 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 10:38:00.895146   286 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 10:38:00.895357   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.895589   286 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 10:38:00.895853   286 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 10:38:00.896073   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 10:38:00.896307   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:00.896549   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:00.896899   286 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 10:38:00.897919   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898227   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898476   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898728   286 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 10:38:00.898949   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.899190   286 net.cpp:200] Created Layer ctx_output6 (61)
I0511 10:38:00.899886   286 net.cpp:572] ctx_output6 <- pool9
I0511 10:38:00.900143   286 net.cpp:542] ctx_output6 -> ctx_output6
I0511 10:38:00.904469   286 net.cpp:260] Setting up ctx_output6
I0511 10:38:00.914566   286 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 10:38:00.921326   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 10:38:00.921558   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.921710   286 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 10:38:00.921867   286 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 10:38:00.922020   286 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 10:38:00.922179   286 net.cpp:260] Setting up ctx_output6/relu
I0511 10:38:00.922334   286 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 10:38:00.922490   286 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 10:38:00.922634   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.922761   286 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 10:38:00.922881   286 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 10:38:00.923010   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:00.923146   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:00.923323   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:00.923676   286 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 10:38:00.923897   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924033   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924175   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924319   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 10:38:00.924453   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.924603   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 10:38:00.924772   286 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 10:38:00.924907   286 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 10:38:00.925599   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 10:38:00.926632   286 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 10:38:00.926802   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:00.926968   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.927125   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 10:38:00.927274   286 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 10:38:00.927417   286 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.927764   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.928125   286 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 10:38:00.928275   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:00.928418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.928560   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 10:38:00.928707   286 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.928859   286 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.931531   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.995368   286 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 10:38:00.995499   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.995635   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.996258   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 10:38:00.996297   286 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 10:38:00.996336   286 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 10:38:00.996858   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 10:38:00.996901   286 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 10:38:00.996950   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.996986   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.997026   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 10:38:00.997061   286 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 10:38:01.001762   286 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003091   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003160   286 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 10:38:01.003233   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.003342   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.003415   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 10:38:01.003512   286 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003571   286 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.007719   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.007805   286 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 10:38:01.007864   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.007908   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.007977   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 10:38:01.008116   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 10:38:01.008242   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 10:38:01.008378   286 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 10:38:01.008595   286 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 10:38:01.008671   286 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 10:38:01.008769   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.008944   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.009074   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 10:38:01.009162   286 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 10:38:01.009327   286 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 10:38:01.010057   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 10:38:01.010139   286 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 10:38:01.010243   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.010421   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.010550   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 10:38:01.010668   286 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 10:38:01.010792   286 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011106   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011178   286 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 10:38:01.011273   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.011447   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.011570   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 10:38:01.011689   286 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011811   286 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.012079   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.012151   286 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 10:38:01.012308   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.012419   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.012580   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 10:38:01.012702   286 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 10:38:01.012825   286 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 10:38:01.013501   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 10:38:01.013582   286 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 10:38:01.013746   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.013869   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.013996   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 10:38:01.014117   286 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 10:38:01.014237   286 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.014554   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.014626   286 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 10:38:01.014721   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.014904   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.015027   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 10:38:01.015146   286 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.015269   286 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.015532   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.015604   286 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 10:38:01.015697   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.015875   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.016002   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 10:38:01.016121   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 10:38:01.016243   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 10:38:01.016364   286 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 10:38:01.016566   286 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 10:38:01.016638   286 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 10:38:01.016786   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.016906   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.017035   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 10:38:01.017155   286 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 10:38:01.017277   286 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 10:38:01.017979   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 10:38:01.018059   286 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 10:38:01.018162   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.018340   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.018467   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 10:38:01.018632   286 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 10:38:01.018764   286 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019129   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019202   286 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 10:38:01.019336   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.019464   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.019587   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 10:38:01.019748   286 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019822   286 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.020148   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.020220   286 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 10:38:01.020474   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.020546   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.020682   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 10:38:01.020843   286 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 10:38:01.020972   286 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 10:38:01.021735   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 10:38:01.021816   286 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 10:38:01.022091   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.022186   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.022320   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 10:38:01.022481   286 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 10:38:01.022608   286 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023000   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023072   286 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 10:38:01.023289   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.023397   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.023525   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 10:38:01.023663   286 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023833   286 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.024147   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.024217   286 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 10:38:01.024353   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.024487   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.024657   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 10:38:01.024780   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 10:38:01.024917   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 10:38:01.025084   286 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 10:38:01.025353   286 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 10:38:01.025436   286 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 10:38:01.033692   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.033830   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.034090   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 10:38:01.034365   286 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 10:38:01.034440   286 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 10:38:01.035321   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 10:38:01.036125   286 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 10:38:01.036221   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.036479   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.036602   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 10:38:01.036777   286 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 10:38:01.041563   286 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.041996   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.042387   286 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 10:38:01.042600   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.042681   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.042845   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 10:38:01.042994   286 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.043151   286 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.043577   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.043860   286 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 10:38:01.043954   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.044111   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.044279   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 10:38:01.044360   286 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 10:38:01.044531   286 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 10:38:01.045389   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 10:38:01.046135   286 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 10:38:01.046314   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.046407   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.046576   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 10:38:01.046659   286 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 10:38:01.046815   286 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.047209   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.047474   286 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 10:38:01.047552   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.047694   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.047881   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 10:38:01.048049   286 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.048224   286 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.048606   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.048838   286 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 10:38:01.048918   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.049057   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.049238   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 10:38:01.049414   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 10:38:01.049567   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 10:38:01.049634   286 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 10:38:01.049844   286 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 10:38:01.050012   286 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 10:38:01.050119   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.050230   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.050340   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 10:38:01.050597   286 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 10:38:01.050678   286 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 10:38:01.051476   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 10:38:01.052158   286 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 10:38:01.052243   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.052417   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.052587   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 10:38:01.052740   286 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 10:38:01.052808   286 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.053140   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.053472   286 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 10:38:01.053550   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.053763   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.053886   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 10:38:01.054059   286 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.054227   286 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.054481   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.054741   286 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 10:38:01.054818   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.054955   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.055100   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 10:38:01.055243   286 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:01.055456   286 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 10:38:01.056166   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 10:38:01.056797   286 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 10:38:01.056887   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.057072   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.057251   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 10:38:01.057420   286 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 10:38:01.057567   286 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.057929   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.058221   286 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 10:38:01.058292   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.058440   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.058503   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 10:38:01.058616   286 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.058717   286 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.058956   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.059193   286 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 10:38:01.059270   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.059418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.059568   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 10:38:01.059666   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:01.059819   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 10:38:01.059964   286 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 10:38:01.060174   286 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 10:38:01.065316   286 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 10:38:01.065474   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.065591   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.065871   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 10:38:01.065935   286 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:01.066057   286 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 10:38:01.066812   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 10:38:01.067467   286 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 10:38:01.067553   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.067764   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.067901   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 10:38:01.068085   286 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 10:38:01.068341   286 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.068719   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.069054   286 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 10:38:01.069150   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.069320   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.069591   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 10:38:01.069686   286 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.069890   286 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.070278   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.070530   286 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 10:38:01.070632   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.070816   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.071074   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 10:38:01.071166   286 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:01.071343   286 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 10:38:01.072058   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 10:38:01.072712   286 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 10:38:01.072793   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.072934   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.073125   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 10:38:01.073278   286 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 10:38:01.073446   286 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.073894   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.074204   286 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 10:38:01.074295   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.074442   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.074676   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 10:38:01.074789   286 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.074926   286 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.075320   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.075564   286 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 10:38:01.075661   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.075822   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.075947   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 10:38:01.076059   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:01.076195   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 10:38:01.076347   286 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 10:38:01.076622   286 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 10:38:01.076746   286 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 10:38:01.076815   286 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 10:38:01.076876   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.076925   286 net.cpp:200] Created Layer mbox_loc (106)
I0511 10:38:01.076975   286 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 10:38:01.077023   286 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.077072   286 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.077121   286 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.077173   286 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.077239   286 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.077283   286 net.cpp:542] mbox_loc -> mbox_loc
I0511 10:38:01.077378   286 net.cpp:260] Setting up mbox_loc
I0511 10:38:01.077456   286 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 10:38:01.077512   286 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 10:38:01.077556   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.077603   286 net.cpp:200] Created Layer mbox_conf (107)
I0511 10:38:01.077649   286 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.077697   286 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.077746   286 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.077795   286 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.077842   286 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.077889   286 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.077935   286 net.cpp:542] mbox_conf -> mbox_conf
I0511 10:38:01.078016   286 net.cpp:260] Setting up mbox_conf
I0511 10:38:01.078094   286 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 10:38:01.078148   286 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 10:38:01.078197   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.078245   286 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 10:38:01.078292   286 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 10:38:01.078339   286 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 10:38:01.078388   286 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 10:38:01.078436   286 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 10:38:01.078483   286 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 10:38:01.078531   286 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 10:38:01.078577   286 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 10:38:01.078649   286 net.cpp:260] Setting up mbox_priorbox
I0511 10:38:01.078718   286 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 10:38:01.078769   286 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 10:38:01.078814   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.078869   286 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 10:38:01.078922   286 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 10:38:01.078969   286 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 10:38:01.079041   286 net.cpp:260] Setting up mbox_conf_reshape
I0511 10:38:01.079111   286 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 10:38:01.079159   286 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 10:38:01.079200   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.079250   286 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 10:38:01.079300   286 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 10:38:01.079342   286 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 10:38:01.079469   286 net.cpp:260] Setting up mbox_conf_softmax
I0511 10:38:01.079599   286 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 10:38:01.079653   286 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 10:38:01.079697   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.079743   286 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 10:38:01.079794   286 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 10:38:01.079839   286 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 10:38:01.082798   286 net.cpp:260] Setting up mbox_conf_flatten
I0511 10:38:01.086413   286 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 10:38:01.086676   286 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 10:38:01.086910   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.087154   286 net.cpp:200] Created Layer detection_out (112)
I0511 10:38:01.089323   286 net.cpp:572] detection_out <- mbox_loc
I0511 10:38:01.089581   286 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 10:38:01.089674   286 net.cpp:572] detection_out <- mbox_priorbox
I0511 10:38:01.090101   286 net.cpp:542] detection_out -> detection_out
I0511 10:38:01.090975   286 net.cpp:260] Setting up detection_out
I0511 10:38:01.091850   286 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 10:38:01.091948   286 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 10:38:01.092154   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.092577   286 net.cpp:200] Created Layer detection_eval (113)
I0511 10:38:01.092790   286 net.cpp:572] detection_eval <- detection_out
I0511 10:38:01.092880   286 net.cpp:572] detection_eval <- label
I0511 10:38:01.093076   286 net.cpp:542] detection_eval -> detection_eval
I0511 10:38:01.093842   286 net.cpp:260] Setting up detection_eval
I0511 10:38:01.094563   286 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 10:38:01.094794   286 net.cpp:338] detection_eval does not need backward computation.
I0511 10:38:01.094879   286 net.cpp:338] detection_out does not need backward computation.
I0511 10:38:01.095168   286 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 10:38:01.095257   286 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 10:38:01.095538   286 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 10:38:01.095626   286 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 10:38:01.095911   286 net.cpp:338] mbox_conf does not need backward computation.
I0511 10:38:01.096004   286 net.cpp:338] mbox_loc does not need backward computation.
I0511 10:38:01.096196   286 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.096505   286 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.096588   286 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.096873   286 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 10:38:01.096959   286 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.097246   286 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.097338   286 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 10:38:01.097625   286 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.097714   286 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.097999   286 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.098083   286 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 10:38:01.098351   286 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.098585   286 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.098680   286 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 10:38:01.098964   286 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.099064   286 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.099285   286 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.099573   286 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 10:38:01.099660   286 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.099933   286 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.100028   286 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 10:38:01.100324   286 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.100414   286 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.100698   286 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.100778   286 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 10:38:01.101047   286 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.101265   286 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.101362   286 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 10:38:01.101797   286 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.102041   286 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.102277   286 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.103260   286 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 10:38:01.103477   286 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.103550   286 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.103826   286 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 10:38:01.104034   286 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.104112   286 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.104434   286 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.104504   286 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 10:38:01.104782   286 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.104984   286 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.105187   286 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 10:38:01.105263   286 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 10:38:01.105618   286 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 10:38:01.105690   286 net.cpp:338] ctx_output6 does not need backward computation.
I0511 10:38:01.105962   286 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 10:38:01.106161   286 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 10:38:01.106359   286 net.cpp:338] ctx_output5 does not need backward computation.
I0511 10:38:01.106432   286 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 10:38:01.106757   286 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 10:38:01.106827   286 net.cpp:338] ctx_output4 does not need backward computation.
I0511 10:38:01.107106   286 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 10:38:01.107311   286 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 10:38:01.107515   286 net.cpp:338] ctx_output3 does not need backward computation.
I0511 10:38:01.107592   286 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 10:38:01.107831   286 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 10:38:01.108152   286 net.cpp:338] ctx_output2 does not need backward computation.
I0511 10:38:01.108224   286 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 10:38:01.108547   286 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 10:38:01.108616   286 net.cpp:338] ctx_output1 does not need backward computation.
I0511 10:38:01.108891   286 net.cpp:338] pool9 does not need backward computation.
I0511 10:38:01.109091   286 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 10:38:01.109165   286 net.cpp:338] pool8 does not need backward computation.
I0511 10:38:01.109483   286 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 10:38:01.109683   286 net.cpp:338] pool7 does not need backward computation.
I0511 10:38:01.109756   286 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 10:38:01.110021   286 net.cpp:338] pool6 does not need backward computation.
I0511 10:38:01.110220   286 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 10:38:01.110422   286 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 10:38:01.110493   286 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 10:38:01.110752   286 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 10:38:01.110944   286 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 10:38:01.111141   286 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 10:38:01.111212   286 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 10:38:01.111485   286 net.cpp:338] pool4 does not need backward computation.
I0511 10:38:01.111692   286 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 10:38:01.111779   286 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 10:38:01.112089   286 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 10:38:01.112166   286 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 10:38:01.112476   286 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 10:38:01.112568   286 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 10:38:01.112799   286 net.cpp:338] pool3 does not need backward computation.
I0511 10:38:01.113103   286 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 10:38:01.113327   286 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 10:38:01.113412   286 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 10:38:01.121309   286 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 10:38:01.121418   286 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 10:38:01.121649   286 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 10:38:01.121954   286 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 10:38:01.122041   286 net.cpp:338] pool2 does not need backward computation.
I0511 10:38:01.122339   286 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 10:38:01.122416   286 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 10:38:01.122604   286 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 10:38:01.122911   286 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 10:38:01.123003   286 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 10:38:01.123298   286 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 10:38:01.123389   286 net.cpp:338] pool1 does not need backward computation.
I0511 10:38:01.123682   286 net.cpp:338] conv1b/relu does not need backward computation.
I0511 10:38:01.123781   286 net.cpp:338] conv1b/bn does not need backward computation.
I0511 10:38:01.124011   286 net.cpp:338] conv1b does not need backward computation.
I0511 10:38:01.124308   286 net.cpp:338] conv1a/relu does not need backward computation.
I0511 10:38:01.124399   286 net.cpp:338] conv1a/bn does not need backward computation.
I0511 10:38:01.124693   286 net.cpp:338] conv1a does not need backward computation.
I0511 10:38:01.124775   286 net.cpp:338] data/bias does not need backward computation.
I0511 10:38:01.125072   286 net.cpp:338] data_data_0_split does not need backward computation.
I0511 10:38:01.125170   286 net.cpp:338] data does not need backward computation.
I0511 10:38:01.125361   286 net.cpp:380] This network produces output detection_eval
I0511 10:38:01.125874   286 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 10:38:01.126257   286 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 10:38:01.126338   286 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 10:38:01.126636   286 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 10:38:01.126725   286 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 10:38:01.127008   286 net.cpp:421] Network initialization done.
I0511 10:38:01.127588   286 solver.cpp:55] Solver scaffolding done.
I0511 10:38:01.134549   286 caffe.cpp:158] Finetuning from /workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
I0511 10:38:01.166608   286 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 10:38:01.168434   286 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 10:38:01.168687   286 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 10:38:01.169045   286 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 10:38:01.169394   286 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.169845   286 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 10:38:01.169935   286 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 10:38:01.170405   286 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.170811   286 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 10:38:01.170994   286 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 10:38:01.171176   286 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.171464   286 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.171808   286 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.171887   286 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.172248   286 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.172611   286 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.172691   286 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 10:38:01.172857   286 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.173296   286 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.173667   286 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.173751   286 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.174124   286 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.174484   286 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.174561   286 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.174814   286 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 10:38:01.175009   286 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.175540   286 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.175928   286 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.176091   286 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.176482   286 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.176846   286 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.177006   286 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 10:38:01.177176   286 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.178481   286 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.178851   286 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.179016   286 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.179925   286 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.180318   286 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.180486   286 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.180663   286 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 10:38:01.180835   286 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 10:38:01.180999   286 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 10:38:01.181160   286 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 10:38:01.181329   286 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 10:38:01.181493   286 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 10:38:01.181664   286 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 10:38:01.181828   286 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 10:38:01.182121   286 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 10:38:01.182288   286 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 10:38:01.182462   286 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 10:38:01.182631   286 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 10:38:01.183017   286 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 10:38:01.183194   286 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 10:38:01.183369   286 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 10:38:01.183539   286 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 10:38:01.183950   286 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 10:38:01.184123   286 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 10:38:01.184290   286 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 10:38:01.184465   286 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 10:38:01.184850   286 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 10:38:01.185022   286 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 10:38:01.185194   286 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 10:38:01.185366   286 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 10:38:01.185762   286 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 10:38:01.185933   286 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 10:38:01.186108   286 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 10:38:01.186296   286 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 10:38:01.186681   286 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 10:38:01.186849   286 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 10:38:01.187016   286 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 10:38:01.187181   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.187356   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 10:38:01.188225   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.188688   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.188854   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.189029   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.189196   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 10:38:01.189911   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.190171   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 10:38:01.190552   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.190805   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.191025   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.191185   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.191355   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.191519   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 10:38:01.191956   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.192281   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.192445   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.192625   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.192786   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 10:38:01.195355   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.196084   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 10:38:01.196985   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.197255   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.197477   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.197638   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.197811   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.198031   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 10:38:01.198453   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.198796   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.198962   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.199127   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.199317   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 10:38:01.201678   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.202448   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 10:38:01.203383   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.203635   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.203831   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.203982   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.204149   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.204309   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 10:38:01.204739   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.205052   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.205209   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.205415   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.205565   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 10:38:01.208089   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.208870   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 10:38:01.209805   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.210052   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.210258   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.210422   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.210597   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.210768   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 10:38:01.211201   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.211534   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.211699   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.211879   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.212046   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 10:38:01.212749   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.213001   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 10:38:01.213376   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.213618   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.213819   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.213979   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.214154   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.214368   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 10:38:01.214773   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.215095   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.215256   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.215468   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.215624   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 10:38:01.216327   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.216609   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 10:38:01.216974   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.217208   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.217423   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.217581   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.217789   286 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 10:38:01.217947   286 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 10:38:01.218113   286 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 10:38:01.218281   286 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 10:38:01.226122   286 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 10:38:01.259639   286 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 10:38:01.259920   286 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 10:38:01.260269   286 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 10:38:01.260607   286 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.261065   286 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 10:38:01.261265   286 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 10:38:01.261592   286 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.262035   286 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 10:38:01.262204   286 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 10:38:01.262380   286 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.262717   286 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.263142   286 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.263317   286 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.263643   286 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.273527   286 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.273838   286 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 10:38:01.274055   286 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.274770   286 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.275532   286 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.275728   286 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.276331   286 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.277051   286 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.277247   286 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.277442   286 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 10:38:01.277660   286 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.278848   286 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.279575   286 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.279785   286 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.280678   286 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.281399   286 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.281605   286 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 10:38:01.281803   286 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.285168   286 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.285991   286 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.286219   286 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.288219   286 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.289085   286 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.289299   286 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.289515   286 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 10:38:01.289700   286 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 10:38:01.289882   286 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 10:38:01.290081   286 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 10:38:01.290271   286 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 10:38:01.290452   286 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 10:38:01.290635   286 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 10:38:01.290822   286 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 10:38:01.291348   286 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 10:38:01.291553   286 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 10:38:01.291736   286 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 10:38:01.291929   286 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 10:38:01.292723   286 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 10:38:01.292903   286 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 10:38:01.293149   286 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 10:38:01.293285   286 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 10:38:01.294036   286 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 10:38:01.294201   286 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 10:38:01.294376   286 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 10:38:01.294589   286 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 10:38:01.295296   286 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 10:38:01.295464   286 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 10:38:01.295600   286 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 10:38:01.295737   286 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 10:38:01.296484   286 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 10:38:01.296715   286 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 10:38:01.296912   286 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 10:38:01.297065   286 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 10:38:01.297871   286 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 10:38:01.298100   286 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 10:38:01.298367   286 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 10:38:01.298622   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.298801   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 10:38:01.305400   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.306962   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.307109   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.307308   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.307507   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 10:38:01.309159   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.309569   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 10:38:01.310184   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.310328   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.310472   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.310600   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.310724   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.310847   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 10:38:01.311291   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.311669   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.311801   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.311996   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.312127   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 10:38:01.324643   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.326035   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 10:38:01.329679   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.330022   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.330471   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.330860   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.331136   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.331310   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 10:38:01.332109   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.332574   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.332746   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.332899   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.333058   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 10:38:01.337882   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.338722   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 10:38:01.340797   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.341045   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.341317   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.341480   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.341631   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.341784   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 10:38:01.342450   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.342924   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.343081   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.343235   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.343379   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 10:38:01.354245   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.355154   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 10:38:01.358006   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.358327   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.358742   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.358916   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.359082   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.359253   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 10:38:01.360002   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.360553   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.360744   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.360913   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.361079   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 10:38:01.362339   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.362622   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 10:38:01.363209   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.363447   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.363703   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.363850   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.363993   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.364146   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 10:38:01.364729   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.365173   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.365340   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.365602   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.365751   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 10:38:01.366809   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.367069   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 10:38:01.367630   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.367913   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.368168   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.368326   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.368482   286 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 10:38:01.368618   286 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 10:38:01.368772   286 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 10:38:01.368923   286 net.cpp:1137] Ignoring source layer mbox_loss
I0511 10:38:01.369447   286 caffe.cpp:260] Starting Optimization
I0511 10:38:01.370682   286 solver.cpp:455] Solving ssdJacintoNetV2
I0511 10:38:01.370862   286 solver.cpp:456] Learning Rate Policy: poly
I0511 10:38:01.371042   286 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 10:38:01.375810   286 solver.cpp:269] Initial Test started...
I0511 10:38:01.395205   286 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 10:38:01.399256   286 net.cpp:1071] Ignoring source layer mbox_loss
I0511 10:38:01.394722   331 common.cpp:528] NVML initialized, thread 331
I0511 10:38:01.595722   331 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 331
I0511 10:38:28.091308   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:38:28.412811   286 solver.cpp:749] class AP 1: 0
I0511 10:38:28.419812   286 solver.cpp:749] class AP 2: 0
I0511 10:38:28.434264   286 solver.cpp:749] class AP 3: 0
I0511 10:38:28.434299   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0
I0511 10:38:28.434365   286 solver.cpp:274] Initial Test completed in 27.0391s
I0511 10:38:29.393462   286 solver.cpp:360] Iteration 0 (0.959058 s), loss = 17.6562
I0511 10:38:29.393839   286 solver.cpp:378]     Train net output #0: mbox_loss = 17.8346 (* 1 = 17.8346 loss)
I0511 10:38:29.393970   286 sgd_solver.cpp:172] Iteration 0, lr = 0.01, m = 0.9, wd = 0.0005, gs = 1
I0511 10:38:29.541718   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 7.07G, req 0.74M)	t: 0 0 1.91
I0511 10:38:29.812618   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.92 1.99
I0511 10:38:30.201333   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.96 2.71
I0511 10:38:30.466063   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.33 0.78
I0511 10:38:30.853801   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.11G 64/1 6 4 5 	(avail 6.96G, req 0.11G)	t: 0 0.65 1.1
I0511 10:38:31.122493   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.11G 128/4 6 4 0 	(avail 6.96G, req 0.11G)	t: 0 0.2 0.43
I0511 10:38:31.424767   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.11G 128/1 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.54 0.58
I0511 10:38:31.657657   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.11G 256/4 6 4 5 	(avail 6.96G, req 0.11G)	t: 0 0.18 0.23
I0511 10:38:31.942916   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.11G 256/1 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.55 0.5
I0511 10:38:32.143879   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.11G 512/4 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.11 0.11
I0511 10:38:32.630931   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.11G 128/1 1 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.41 0.82
I0511 10:38:32.894946   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.11G 512/1 1 1 0 	(avail 6.95G, req 0.11G)	t: 0 0.15 0.2
I0511 10:38:33.206898   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.11G 512/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.07 0.09
I0511 10:38:33.467043   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.11G 512/1 0 0 1 	(avail 6.96G, req 0.11G)	t: 0 0.05 0.06
I0511 10:38:33.677325   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.11G 512/1 0 0 3 	(avail 6.96G, req 0.11G)	t: 0 0.05 0.05
I0511 10:38:33.866238   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.11G 512/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.06 0.06
I0511 10:38:34.167804   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.26 0.72
I0511 10:38:34.478067   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.11G 256/1 1 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.26 0.71
I0511 10:38:34.679678   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.14 0.08
I0511 10:38:34.853935   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.09 0.11
I0511 10:38:35.038169   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.01 0.02
I0511 10:38:35.225252   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.07 0.03
I0511 10:38:35.401845   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.584975   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.95G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.765408   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.952461   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.130211   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.308430   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.95G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.956701   286 solver.cpp:360] Iteration 1 (7.56243 s), loss = 15.8703
I0511 10:38:36.956981   286 solver.cpp:378]     Train net output #0: mbox_loss = 13.5519 (* 1 = 13.5519 loss)
I0511 10:38:38.168962   286 solver.cpp:360] Iteration 2 (1.21224 s), loss = 14.956
I0511 10:38:38.169925   286 solver.cpp:378]     Train net output #0: mbox_loss = 13.8961 (* 1 = 13.8961 loss)
I0511 10:38:57.125018   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:40:03.165524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:41:14.132916   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:42:28.609467   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:42:48.614398   286 solver.cpp:354] Iteration 100 (0.391304 iter/s, 250.445s/98 iter), 3.8/376.5ep, loss = 4.91637
I0511 10:42:48.615317   286 solver.cpp:378]     Train net output #0: mbox_loss = 4.19354 (* 1 = 4.19354 loss)
I0511 10:42:48.615847   286 sgd_solver.cpp:172] Iteration 100, lr = 0.00960596, m = 0.9, wd = 0.0005, gs = 1
I0511 10:43:35.426385   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:44:40.301086   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:45:52.520774   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:47:07.786118   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:47:10.175945   286 solver.cpp:354] Iteration 200 (0.382319 iter/s, 261.562s/100 iter), 7.5/376.5ep, loss = 4.07285
I0511 10:47:10.176024   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.58295 (* 1 = 3.58295 loss)
I0511 10:47:10.176046   286 sgd_solver.cpp:172] Iteration 200, lr = 0.00922368, m = 0.9, wd = 0.0005, gs = 1
I0511 10:48:14.055537   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:49:22.252074   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:50:40.964177   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:51:42.024085   286 solver.cpp:354] Iteration 300 (0.367852 iter/s, 271.848s/100 iter), 11.3/376.5ep, loss = 4.21286
I0511 10:51:42.024226   286 solver.cpp:378]     Train net output #0: mbox_loss = 4.17682 (* 1 = 4.17682 loss)
I0511 10:51:42.024268   286 sgd_solver.cpp:172] Iteration 300, lr = 0.00885293, m = 0.9, wd = 0.0005, gs = 1
I0511 10:51:48.562729   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:52:54.873471   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:54:12.802685   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:55:23.551412   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:56:05.629467   286 solver.cpp:354] Iteration 400 (0.379355 iter/s, 263.605s/100 iter), 15.1/376.5ep, loss = 3.72909
I0511 10:56:05.629534   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.93742 (* 1 = 3.93742 loss)
I0511 10:56:05.629544   286 sgd_solver.cpp:172] Iteration 400, lr = 0.00849346, m = 0.9, wd = 0.0005, gs = 1
I0511 10:56:29.952297   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:57:43.743602   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:58:51.682006   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:59:57.250562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:00:23.382565   286 solver.cpp:354] Iteration 500 (0.387968 iter/s, 257.753s/100 iter), 18.8/376.5ep, loss = 3.60061
I0511 11:00:23.382637   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.24536 (* 1 = 3.24536 loss)
I0511 11:00:23.382660   286 sgd_solver.cpp:172] Iteration 500, lr = 0.00814506, m = 0.9, wd = 0.0005, gs = 1
I0511 11:01:11.306205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:02:20.201948   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:03:25.341446   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:04:37.361199   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:04:48.727902   286 solver.cpp:354] Iteration 600 (0.376868 iter/s, 265.345s/100 iter), 22.6/376.5ep, loss = 3.71362
I0511 11:04:48.727995   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.55587 (* 1 = 3.55587 loss)
I0511 11:04:48.728022   286 sgd_solver.cpp:172] Iteration 600, lr = 0.00780749, m = 0.9, wd = 0.0005, gs = 1
I0511 11:05:48.521507   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:07:02.944953   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:08:09.233359   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:09:16.780077   286 solver.cpp:354] Iteration 700 (0.373062 iter/s, 268.052s/100 iter), 26.4/376.5ep, loss = 3.65001
I0511 11:09:16.780711   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.73428 (* 1 = 3.73428 loss)
I0511 11:09:16.780903   286 sgd_solver.cpp:172] Iteration 700, lr = 0.00748052, m = 0.9, wd = 0.0005, gs = 1
I0511 11:09:26.408531   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:10:34.786350   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:11:41.689963   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:12:56.029769   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:13:43.569540   286 solver.cpp:354] Iteration 800 (0.374828 iter/s, 266.789s/100 iter), 30.1/376.5ep, loss = 3.43742
I0511 11:13:43.570178   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.4464 (* 1 = 3.4464 loss)
I0511 11:13:43.570397   286 sgd_solver.cpp:172] Iteration 800, lr = 0.00716393, m = 0.9, wd = 0.0005, gs = 1
I0511 11:14:06.653177   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:15:16.172699   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:16:30.039755   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:17:44.701989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:18:09.307843   286 solver.cpp:354] Iteration 900 (0.37631 iter/s, 265.738s/100 iter), 33.9/376.5ep, loss = 3.52763
I0511 11:18:09.308257   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.65744 (* 1 = 3.65744 loss)
I0511 11:18:09.308470   286 sgd_solver.cpp:172] Iteration 900, lr = 0.0068575, m = 0.9, wd = 0.0005, gs = 1
I0511 11:18:48.855034   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:20:03.712283   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:21:15.829341   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:22:26.632393   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:22:37.554395   286 solver.cpp:354] Iteration 1000 (0.372791 iter/s, 268.247s/100 iter), 37.6/376.5ep, loss = 3.44066
I0511 11:22:37.554469   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.49448 (* 1 = 3.49448 loss)
I0511 11:22:37.555598   286 sgd_solver.cpp:172] Iteration 1000, lr = 0.006561, m = 0.9, wd = 0.0005, gs = 1
I0511 11:23:37.579844   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:24:54.242961   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:26:09.785871   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:27:11.479239   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:27:12.089705   286 solver.cpp:354] Iteration 1100 (0.364251 iter/s, 274.536s/100 iter), 41.4/376.5ep, loss = 3.35575
I0511 11:27:12.089983   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.20541 (* 1 = 3.20541 loss)
I0511 11:27:12.090056   286 sgd_solver.cpp:172] Iteration 1100, lr = 0.00627422, m = 0.9, wd = 0.0005, gs = 1
I0511 11:28:22.553395   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:29:36.227958   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:30:40.405364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:31:31.089360   286 solver.cpp:354] Iteration 1200 (0.386101 iter/s, 259s/100 iter), 45.2/376.5ep, loss = 3.31291
I0511 11:31:31.089628   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.32026 (* 1 = 3.32026 loss)
I0511 11:31:31.089767   286 sgd_solver.cpp:172] Iteration 1200, lr = 0.00599695, m = 0.9, wd = 0.0005, gs = 1
I0511 11:31:49.211495   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:33:00.732100   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:34:11.613571   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:35:23.095744   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:35:52.320159   286 solver.cpp:354] Iteration 1300 (0.382803 iter/s, 261.231s/100 iter), 48.9/376.5ep, loss = 3.1759
I0511 11:35:52.320235   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.83433 (* 1 = 3.83433 loss)
I0511 11:35:52.320259   286 sgd_solver.cpp:172] Iteration 1300, lr = 0.00572898, m = 0.9, wd = 0.0005, gs = 1
I0511 11:36:29.350693   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:37:43.388226   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:38:55.138619   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:40:05.133401   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:40:23.446975   286 solver.cpp:354] Iteration 1400 (0.368831 iter/s, 271.127s/100 iter), 52.7/376.5ep, loss = 3.24382
I0511 11:40:23.447376   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.0333 (* 1 = 3.0333 loss)
I0511 11:40:23.447553   286 sgd_solver.cpp:172] Iteration 1400, lr = 0.00547008, m = 0.9, wd = 0.0005, gs = 1
I0511 11:41:20.986205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:42:29.725340   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:43:33.881989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:44:45.873347   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:44:47.280021   286 solver.cpp:354] Iteration 1500 (0.379028 iter/s, 263.833s/100 iter), 56.5/376.5ep, loss = 3.09131
I0511 11:44:47.280622   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.15936 (* 1 = 3.15936 loss)
I0511 11:44:47.280884   286 sgd_solver.cpp:172] Iteration 1500, lr = 0.00522006, m = 0.9, wd = 0.0005, gs = 1
I0511 11:46:05.818781   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:47:07.989359   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:48:17.968626   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:49:22.973845   286 solver.cpp:354] Iteration 1600 (0.362721 iter/s, 275.694s/100 iter), 60.2/376.5ep, loss = 3.17959
I0511 11:49:22.973912   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.86817 (* 1 = 2.86817 loss)
I0511 11:49:22.973922   286 sgd_solver.cpp:172] Iteration 1600, lr = 0.00497871, m = 0.9, wd = 0.0005, gs = 1
I0511 11:49:42.087678   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:50:47.660218   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:51:56.369282   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:53:06.144986   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:53:48.199604   286 solver.cpp:354] Iteration 1700 (0.377038 iter/s, 265.226s/100 iter), 64/376.5ep, loss = 3.00944
I0511 11:53:48.200031   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.78802 (* 1 = 2.78802 loss)
I0511 11:53:48.200186   286 sgd_solver.cpp:172] Iteration 1700, lr = 0.00474583, m = 0.9, wd = 0.0005, gs = 1
I0511 11:54:25.107169   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:55:33.825628   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:56:51.222885   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:58:07.273593   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:58:28.588940   286 solver.cpp:354] Iteration 1800 (0.356649 iter/s, 280.388s/100 iter), 67.8/376.5ep, loss = 3.05312
I0511 11:58:28.589234   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.0944 (* 1 = 3.0944 loss)
I0511 11:58:28.589326   286 sgd_solver.cpp:172] Iteration 1800, lr = 0.00452122, m = 0.9, wd = 0.0005, gs = 1
I0511 11:59:16.565518   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:00:21.824263   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:01:41.926645   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:02:47.576617   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:02:52.693686   286 solver.cpp:354] Iteration 1900 (0.37864 iter/s, 264.103s/100 iter), 71.5/376.5ep, loss = 3.15033
I0511 12:02:52.693955   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.22117 (* 1 = 3.22117 loss)
I0511 12:02:52.693980   286 sgd_solver.cpp:172] Iteration 1900, lr = 0.00430467, m = 0.9, wd = 0.0005, gs = 1
I0511 12:03:53.067049   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:05:04.221717   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:06:15.752265   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:07:15.737447   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 12:07:16.003279   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 12:07:16.088572   286 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 12:07:39.658536   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:07:40.059329   286 solver.cpp:749] class AP 1: 0.879249
I0511 12:07:40.110666   286 solver.cpp:749] class AP 2: 0.840918
I0511 12:07:40.112437   286 solver.cpp:749] class AP 3: 0.906466
I0511 12:07:40.112879   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.875544
I0511 12:07:40.112967   286 solver.cpp:284] Tests completed in 287.418s
I0511 12:07:40.950157   286 solver.cpp:354] Iteration 2000 (0.347925 iter/s, 287.418s/100 iter), 75.3/376.5ep, loss = 3.07947
I0511 12:07:40.950234   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.79876 (* 1 = 2.79876 loss)
I0511 12:07:40.950276   286 sgd_solver.cpp:172] Iteration 2000, lr = 0.004096, m = 0.9, wd = 0.0005, gs = 1
I0511 12:07:44.587966   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:08:55.205845   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:10:08.024942   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:11:23.318500   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:12:05.289585   286 solver.cpp:354] Iteration 2100 (0.378303 iter/s, 264.339s/100 iter), 79.1/376.5ep, loss = 3.1227
I0511 12:12:05.289705   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.05706 (* 1 = 3.05706 loss)
I0511 12:12:05.289732   286 sgd_solver.cpp:172] Iteration 2100, lr = 0.00389501, m = 0.9, wd = 0.0005, gs = 1
I0511 12:12:27.215154   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:13:38.954730   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:14:46.994252   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:15:59.583885   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:16:27.190752   286 solver.cpp:354] Iteration 2200 (0.381825 iter/s, 261.9s/100 iter), 82.8/376.5ep, loss = 3.05287
I0511 12:16:27.190817   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.07876 (* 1 = 3.07876 loss)
I0511 12:16:27.190838   286 sgd_solver.cpp:172] Iteration 2200, lr = 0.00370151, m = 0.9, wd = 0.0005, gs = 1
I0511 12:17:07.749131   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:18:12.230006   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:19:31.211865   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:20:39.364607   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:20:48.393659   286 solver.cpp:354] Iteration 2300 (0.382845 iter/s, 261.202s/100 iter), 86.6/376.5ep, loss = 2.79292
I0511 12:20:48.394317   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.48514 (* 1 = 2.48514 loss)
I0511 12:20:48.394619   286 sgd_solver.cpp:172] Iteration 2300, lr = 0.0035153, m = 0.9, wd = 0.0005, gs = 1
I0511 12:21:44.641352   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:23:00.002321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:24:05.931144   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:25:09.035069   286 solver.cpp:354] Iteration 2400 (0.38367 iter/s, 260.641s/100 iter), 90.4/376.5ep, loss = 2.7932
I0511 12:25:09.035487   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.40063 (* 1 = 2.40063 loss)
I0511 12:25:09.035624   286 sgd_solver.cpp:172] Iteration 2400, lr = 0.00333622, m = 0.9, wd = 0.0005, gs = 1
I0511 12:25:14.365535   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:26:21.527201   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:27:34.652740   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:28:51.651880   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:29:33.386966   286 solver.cpp:354] Iteration 2500 (0.378284 iter/s, 264.352s/100 iter), 94.1/376.5ep, loss = 2.81492
I0511 12:29:33.387517   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.60854 (* 1 = 2.60854 loss)
I0511 12:29:33.387727   286 sgd_solver.cpp:172] Iteration 2500, lr = 0.00316406, m = 0.9, wd = 0.0005, gs = 1
I0511 12:29:54.993562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:31:01.488196   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:32:17.283821   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:33:21.858769   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:33:49.713783   286 solver.cpp:354] Iteration 2600 (0.390125 iter/s, 256.328s/100 iter), 97.9/376.5ep, loss = 2.73197
I0511 12:33:49.713941   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.54406 (* 1 = 2.54406 loss)
I0511 12:33:49.713989   286 sgd_solver.cpp:172] Iteration 2600, lr = 0.00299866, m = 0.9, wd = 0.0005, gs = 1
I0511 12:34:29.731086   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:35:45.453665   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:36:53.442992   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:37:58.900590   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:38:16.697453   286 solver.cpp:354] Iteration 2700 (0.374554 iter/s, 266.984s/100 iter), 101.6/376.5ep, loss = 2.93363
I0511 12:38:16.697540   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.18382 (* 1 = 3.18382 loss)
I0511 12:38:16.697566   286 sgd_solver.cpp:172] Iteration 2700, lr = 0.00283982, m = 0.9, wd = 0.0005, gs = 1
I0511 12:39:13.403147   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:40:26.884312   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:41:37.105988   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:42:36.481858   286 solver.cpp:354] Iteration 2800 (0.384934 iter/s, 259.785s/100 iter), 105.4/376.5ep, loss = 2.71231
I0511 12:42:36.482138   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.96847 (* 1 = 2.96847 loss)
I0511 12:42:36.482244   286 sgd_solver.cpp:172] Iteration 2800, lr = 0.00268739, m = 0.9, wd = 0.0005, gs = 1
I0511 12:42:37.723563   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:43:46.591173   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:45:01.515321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:46:09.190704   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:47:06.386873   286 solver.cpp:354] Iteration 2900 (0.370501 iter/s, 269.905s/100 iter), 109.2/376.5ep, loss = 2.74402
I0511 12:47:06.386988   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.80988 (* 1 = 2.80988 loss)
I0511 12:47:06.387012   286 sgd_solver.cpp:172] Iteration 2900, lr = 0.00254117, m = 0.9, wd = 0.0005, gs = 1
I0511 12:47:26.112315   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:48:37.512287   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:49:43.472292   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:50:57.740178   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:51:35.410475   286 solver.cpp:354] Iteration 3000 (0.371715 iter/s, 269.023s/100 iter), 112.9/376.5ep, loss = 2.8412
I0511 12:51:35.410542   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.64109 (* 1 = 2.64109 loss)
I0511 12:51:35.410553   286 sgd_solver.cpp:172] Iteration 3000, lr = 0.002401, m = 0.9, wd = 0.0005, gs = 1
I0511 12:52:14.139542   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:53:29.024706   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:54:41.149220   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:55:49.354791   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:56:07.841018   286 solver.cpp:354] Iteration 3100 (0.367066 iter/s, 272.43s/100 iter), 116.7/376.5ep, loss = 2.75702
I0511 12:56:07.841323   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39492 (* 1 = 2.39492 loss)
I0511 12:56:07.841435   286 sgd_solver.cpp:172] Iteration 3100, lr = 0.00226671, m = 0.9, wd = 0.0005, gs = 1
I0511 12:56:54.763296   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:58:11.073791   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:59:17.272364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:00:26.318444   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:00:29.234131   286 solver.cpp:354] Iteration 3200 (0.382566 iter/s, 261.393s/100 iter), 120.5/376.5ep, loss = 2.63678
I0511 13:00:29.234170   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.46977 (* 1 = 2.46977 loss)
I0511 13:00:29.234181   286 sgd_solver.cpp:172] Iteration 3200, lr = 0.00213814, m = 0.9, wd = 0.0005, gs = 1
I0511 13:01:40.847404   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:02:44.592392   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:03:56.883589   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:04:53.933248   286 solver.cpp:354] Iteration 3300 (0.377789 iter/s, 264.698s/100 iter), 124.2/376.5ep, loss = 2.74173
I0511 13:04:53.937497   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.10632 (* 1 = 2.10632 loss)
I0511 13:04:53.937575   286 sgd_solver.cpp:172] Iteration 3300, lr = 0.00201511, m = 0.9, wd = 0.0005, gs = 1
I0511 13:05:09.040370   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:06:14.338225   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:07:25.205103   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:08:39.808761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:09:17.926232   286 solver.cpp:354] Iteration 3400 (0.378801 iter/s, 263.991s/100 iter), 128/376.5ep, loss = 2.649
I0511 13:09:17.927139   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.8199 (* 1 = 2.8199 loss)
I0511 13:09:17.927505   286 sgd_solver.cpp:172] Iteration 3400, lr = 0.00189747, m = 0.9, wd = 0.0005, gs = 1
I0511 13:09:53.793624   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:10:58.517418   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:12:16.417186   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:13:27.841019   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:13:50.945454   286 solver.cpp:354] Iteration 3500 (0.366277 iter/s, 273.018s/100 iter), 131.8/376.5ep, loss = 2.67151
I0511 13:13:50.945530   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.23974 (* 1 = 2.23974 loss)
I0511 13:13:50.945556   286 sgd_solver.cpp:172] Iteration 3500, lr = 0.00178506, m = 0.9, wd = 0.0005, gs = 1
I0511 13:14:35.788682   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:15:48.467964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:16:53.009790   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:18:04.037165   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:18:06.838940   286 solver.cpp:354] Iteration 3600 (0.390789 iter/s, 255.892s/100 iter), 135.5/376.5ep, loss = 2.57451
I0511 13:18:06.839455   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.47927 (* 1 = 2.47927 loss)
I0511 13:18:06.839705   286 sgd_solver.cpp:172] Iteration 3600, lr = 0.00167772, m = 0.9, wd = 0.0005, gs = 1
I0511 13:19:10.187364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:20:17.900319   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:21:29.689242   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:22:31.841965   286 solver.cpp:354] Iteration 3700 (0.377356 iter/s, 265.002s/100 iter), 139.3/376.5ep, loss = 2.60107
I0511 13:22:31.842711   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.14181 (* 1 = 2.14181 loss)
I0511 13:22:31.842969   286 sgd_solver.cpp:172] Iteration 3700, lr = 0.0015753, m = 0.9, wd = 0.0005, gs = 1
I0511 13:22:47.849494   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:23:49.027128   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:24:56.902205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:26:08.242411   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:26:52.545734   286 solver.cpp:354] Iteration 3800 (0.383579 iter/s, 260.702s/100 iter), 143.1/376.5ep, loss = 2.76805
I0511 13:26:52.545801   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.64585 (* 1 = 2.64585 loss)
I0511 13:26:52.545812   286 sgd_solver.cpp:172] Iteration 3800, lr = 0.00147763, m = 0.9, wd = 0.0005, gs = 1
I0511 13:27:20.809334   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:28:26.125339   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:29:41.801555   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:30:51.501336   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:31:15.801491   286 solver.cpp:354] Iteration 3900 (0.37986 iter/s, 263.255s/100 iter), 146.8/376.5ep, loss = 2.58987
I0511 13:31:15.801527   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.53969 (* 1 = 2.53969 loss)
I0511 13:31:15.801539   286 sgd_solver.cpp:172] Iteration 3900, lr = 0.00138458, m = 0.9, wd = 0.0005, gs = 1
I0511 13:31:57.397296   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:33:08.280045   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:34:19.864984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:24.890027   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:30.135825   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0511 13:35:30.173681   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0511 13:35:30.217507   286 solver.cpp:637] Iteration 4000, Testing net (#0)
I0511 13:35:57.209758   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:57.714069   286 solver.cpp:749] class AP 1: 0.895331
I0511 13:35:57.718536   286 solver.cpp:749] class AP 2: 0.884352
I0511 13:35:57.719480   286 solver.cpp:749] class AP 3: 0.903
I0511 13:35:57.719511   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894228
I0511 13:35:57.719575   286 solver.cpp:284] Tests completed in 281.917s
I0511 13:35:58.392904   286 solver.cpp:354] Iteration 4000 (0.354714 iter/s, 281.917s/100 iter), 150.6/376.5ep, loss = 2.55356
I0511 13:35:58.393193   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.09378 (* 1 = 2.09378 loss)
I0511 13:35:58.393275   286 sgd_solver.cpp:172] Iteration 4000, lr = 0.001296, m = 0.9, wd = 0.0005, gs = 1
I0511 13:36:50.674672   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:37:53.737460   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:39:16.183795   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:40:17.428336   286 solver.cpp:354] Iteration 4100 (0.386046 iter/s, 259.036s/100 iter), 154.4/376.5ep, loss = 2.51746
I0511 13:40:17.429203   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.36696 (* 1 = 2.36696 loss)
I0511 13:40:17.429647   286 sgd_solver.cpp:172] Iteration 4100, lr = 0.00121174, m = 0.9, wd = 0.0005, gs = 1
I0511 13:40:22.139886   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:41:26.006645   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:42:44.228413   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:43:50.919600   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:44:40.386477   286 solver.cpp:354] Iteration 4200 (0.380288 iter/s, 262.959s/100 iter), 158.1/376.5ep, loss = 2.48454
I0511 13:44:40.387066   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.95867 (* 1 = 1.95867 loss)
I0511 13:44:40.387343   286 sgd_solver.cpp:172] Iteration 4200, lr = 0.00113165, m = 0.9, wd = 0.0005, gs = 1
I0511 13:45:00.134527   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:46:11.440686   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:47:33.638960   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:48:40.566258   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:49:10.108214   286 solver.cpp:354] Iteration 4300 (0.370752 iter/s, 269.722s/100 iter), 161.9/376.5ep, loss = 2.66537
I0511 13:49:10.108485   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.17375 (* 1 = 2.17375 loss)
I0511 13:49:10.108569   286 sgd_solver.cpp:172] Iteration 4300, lr = 0.0010556, m = 0.9, wd = 0.0005, gs = 1
I0511 13:49:54.331159   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:50:59.843852   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:52:08.072981   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:53:22.533658   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:53:32.761421   286 solver.cpp:354] Iteration 4400 (0.38073 iter/s, 262.653s/100 iter), 165.6/376.5ep, loss = 2.55626
I0511 13:53:32.762244   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.09898 (* 1 = 3.09898 loss)
I0511 13:53:32.762508   286 sgd_solver.cpp:172] Iteration 4400, lr = 0.00098345, m = 0.9, wd = 0.0005, gs = 1
I0511 13:54:32.260608   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:55:37.447028   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:56:43.742321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:57:51.340132   286 solver.cpp:354] Iteration 4500 (0.38673 iter/s, 258.579s/100 iter), 169.4/376.5ep, loss = 2.53083
I0511 13:57:51.340680   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.38264 (* 1 = 2.38264 loss)
I0511 13:57:51.340816   286 sgd_solver.cpp:172] Iteration 4500, lr = 0.000915063, m = 0.9, wd = 0.0005, gs = 1
I0511 13:57:54.133273   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:59:02.404615   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:00:11.371378   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:01:19.137918   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:02:08.944617   286 solver.cpp:354] Iteration 4600 (0.388192 iter/s, 257.604s/100 iter), 173.2/376.5ep, loss = 2.55919
I0511 14:02:08.945158   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.2639 (* 1 = 2.2639 loss)
I0511 14:02:08.945295   286 sgd_solver.cpp:172] Iteration 4600, lr = 0.000850305, m = 0.9, wd = 0.0005, gs = 1
I0511 14:02:25.426777   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:03:36.698139   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:04:52.299738   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:05:57.266844   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:06:32.370049   286 solver.cpp:354] Iteration 4700 (0.379615 iter/s, 263.425s/100 iter), 176.9/376.5ep, loss = 2.26435
I0511 14:06:32.370859   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.24497 (* 1 = 2.24497 loss)
I0511 14:06:32.371233   286 sgd_solver.cpp:172] Iteration 4700, lr = 0.000789048, m = 0.9, wd = 0.0005, gs = 1
I0511 14:07:12.517341   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:08:16.817073   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:09:27.969321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:10:31.794914   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:10:54.785575   286 solver.cpp:354] Iteration 4800 (0.381076 iter/s, 262.415s/100 iter), 180.7/376.5ep, loss = 2.51465
I0511 14:10:54.785679   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.63854 (* 1 = 2.63854 loss)
I0511 14:10:54.785713   286 sgd_solver.cpp:172] Iteration 4800, lr = 0.000731161, m = 0.9, wd = 0.0005, gs = 1
I0511 14:11:47.349313   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:12:51.029984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:14:02.539785   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:15:11.173334   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:15:12.481969   286 solver.cpp:354] Iteration 4900 (0.388054 iter/s, 257.696s/100 iter), 184.5/376.5ep, loss = 2.46885
I0511 14:15:12.482333   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.61849 (* 1 = 2.61849 loss)
I0511 14:15:12.482445   286 sgd_solver.cpp:172] Iteration 4900, lr = 0.00067652, m = 0.9, wd = 0.0005, gs = 1
I0511 14:16:16.027968   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:17:19.601358   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:18:28.823508   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:19:19.600805   286 solver.cpp:354] Iteration 5000 (0.404664 iter/s, 247.118s/100 iter), 188.2/376.5ep, loss = 2.35858
I0511 14:19:19.600908   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.06529 (* 1 = 2.06529 loss)
I0511 14:19:19.600934   286 sgd_solver.cpp:172] Iteration 5000, lr = 0.000625, m = 0.9, wd = 0.0005, gs = 1
I0511 14:19:37.514231   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:20:42.713737   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:21:57.599524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:23:11.385015   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:23:48.169528   286 solver.cpp:354] Iteration 5100 (0.372345 iter/s, 268.568s/100 iter), 192/376.5ep, loss = 2.45979
I0511 14:23:48.170099   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.56464 (* 1 = 2.56464 loss)
I0511 14:23:48.170296   286 sgd_solver.cpp:172] Iteration 5100, lr = 0.00057648, m = 0.9, wd = 0.0005, gs = 1
I0511 14:24:14.310308   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:25:27.188686   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:26:38.342267   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:27:43.074115   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:28:08.558845   286 solver.cpp:354] Iteration 5200 (0.384041 iter/s, 260.389s/100 iter), 195.8/376.5ep, loss = 2.47466
I0511 14:28:08.558895   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.50955 (* 1 = 2.50955 loss)
I0511 14:28:08.558910   286 sgd_solver.cpp:172] Iteration 5200, lr = 0.000530842, m = 0.9, wd = 0.0005, gs = 1
I0511 14:28:52.875831   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:30:09.653620   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:31:12.377820   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:32:23.693809   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:32:30.000942   286 solver.cpp:354] Iteration 5300 (0.382494 iter/s, 261.442s/100 iter), 199.5/376.5ep, loss = 2.40503
I0511 14:32:30.001298   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.30751 (* 1 = 2.30751 loss)
I0511 14:32:30.001462   286 sgd_solver.cpp:172] Iteration 5300, lr = 0.000487968, m = 0.9, wd = 0.0005, gs = 1
I0511 14:33:29.752796   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:34:38.402200   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:35:49.980350   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:36:55.671003   286 solver.cpp:354] Iteration 5400 (0.376407 iter/s, 265.67s/100 iter), 203.3/376.5ep, loss = 2.47235
I0511 14:36:55.671073   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.21606 (* 1 = 2.21606 loss)
I0511 14:36:55.671084   286 sgd_solver.cpp:172] Iteration 5400, lr = 0.000447745, m = 0.9, wd = 0.0005, gs = 1
I0511 14:37:06.020602   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:38:10.276371   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:39:15.951314   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:40:32.351894   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:41:13.609359   286 solver.cpp:354] Iteration 5500 (0.38769 iter/s, 257.938s/100 iter), 207.1/376.5ep, loss = 2.31745
I0511 14:41:13.609762   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.76545 (* 1 = 2.76545 loss)
I0511 14:41:13.609951   286 sgd_solver.cpp:172] Iteration 5500, lr = 0.000410062, m = 0.9, wd = 0.0005, gs = 1
I0511 14:41:38.267742   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:42:51.352392   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:44:05.462697   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:45:15.655130   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:45:38.377336   286 solver.cpp:354] Iteration 5600 (0.37769 iter/s, 264.768s/100 iter), 210.8/376.5ep, loss = 2.37671
I0511 14:45:38.377447   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.80275 (* 1 = 2.80275 loss)
I0511 14:45:38.377499   286 sgd_solver.cpp:172] Iteration 5600, lr = 0.00037481, m = 0.9, wd = 0.0005, gs = 1
I0511 14:46:20.286691   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:47:37.883761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:48:46.160190   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:49:55.102929   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:50:08.614735   286 solver.cpp:354] Iteration 5700 (0.370047 iter/s, 270.236s/100 iter), 214.6/376.5ep, loss = 2.14596
I0511 14:50:08.614796   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.96386 (* 1 = 1.96386 loss)
I0511 14:50:08.614816   286 sgd_solver.cpp:172] Iteration 5700, lr = 0.00034188, m = 0.9, wd = 0.0005, gs = 1
I0511 14:51:06.059387   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:52:22.318501   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:53:28.155045   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:54:29.501780   286 solver.cpp:354] Iteration 5800 (0.38331 iter/s, 260.886s/100 iter), 218.4/376.5ep, loss = 2.20686
I0511 14:54:29.501899   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.12483 (* 1 = 2.12483 loss)
I0511 14:54:29.501921   286 sgd_solver.cpp:172] Iteration 5800, lr = 0.00031117, m = 0.9, wd = 0.0005, gs = 1
I0511 14:54:41.173198   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:55:52.591112   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:56:57.947305   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:58:11.827960   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:58:56.322983   286 solver.cpp:354] Iteration 5900 (0.374784 iter/s, 266.82s/100 iter), 222.1/376.5ep, loss = 2.23025
I0511 14:58:56.323608   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.0907 (* 1 = 2.0907 loss)
I0511 14:58:56.323763   286 sgd_solver.cpp:172] Iteration 5900, lr = 0.000282576, m = 0.9, wd = 0.0005, gs = 1
I0511 14:59:21.182163   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:00:23.018683   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:01:40.175355   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:02:50.411329   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:03:17.114616   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0511 15:03:17.172052   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0511 15:03:17.193516   286 solver.cpp:637] Iteration 6000, Testing net (#0)
I0511 15:03:41.279762   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:03:41.845378   286 solver.cpp:749] class AP 1: 0.902398
I0511 15:03:41.846396   286 solver.cpp:749] class AP 2: 0.892614
I0511 15:03:41.846835   286 solver.cpp:749] class AP 3: 0.902081
I0511 15:03:41.846863   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899031
I0511 15:03:41.846915   286 solver.cpp:284] Tests completed in 285.523s
I0511 15:03:42.425446   286 solver.cpp:354] Iteration 6000 (0.350235 iter/s, 285.523s/100 iter), 225.9/376.5ep, loss = 2.27116
I0511 15:03:42.425484   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39021 (* 1 = 2.39021 loss)
I0511 15:03:42.425494   286 sgd_solver.cpp:172] Iteration 6000, lr = 0.000256, m = 0.9, wd = 0.0005, gs = 1
I0511 15:04:09.587678   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:05:21.828974   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:06:27.871925   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:07:34.535059   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:07:50.633746   286 solver.cpp:354] Iteration 6100 (0.402889 iter/s, 248.208s/100 iter), 229.6/376.5ep, loss = 2.33219
I0511 15:07:50.634197   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.59976 (* 1 = 2.59976 loss)
I0511 15:07:50.634369   286 sgd_solver.cpp:172] Iteration 6100, lr = 0.000231344, m = 0.9, wd = 0.0005, gs = 1
I0511 15:08:42.834233   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:09:56.460775   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:10:59.864198   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:12:10.109671   286 solver.cpp:354] Iteration 6200 (0.385393 iter/s, 259.475s/100 iter), 233.4/376.5ep, loss = 2.30922
I0511 15:12:10.109740   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.06719 (* 1 = 2.06719 loss)
I0511 15:12:10.109751   286 sgd_solver.cpp:172] Iteration 6200, lr = 0.000208514, m = 0.9, wd = 0.0005, gs = 1
I0511 15:12:12.339005   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:13:24.473868   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:14:31.629825   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:15:43.173020   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:16:32.450592   286 solver.cpp:354] Iteration 6300 (0.381184 iter/s, 262.34s/100 iter), 237.2/376.5ep, loss = 2.28051
I0511 15:16:32.450721   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.97659 (* 1 = 1.97659 loss)
I0511 15:16:32.450765   286 sgd_solver.cpp:172] Iteration 6300, lr = 0.000187416, m = 0.9, wd = 0.0005, gs = 1
I0511 15:16:50.844229   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:18:02.489755   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:19:11.244990   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:20:25.167714   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:20:58.709304   286 solver.cpp:354] Iteration 6400 (0.375575 iter/s, 266.258s/100 iter), 240.9/376.5ep, loss = 2.245
I0511 15:20:58.709363   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39626 (* 1 = 2.39626 loss)
I0511 15:20:58.709373   286 sgd_solver.cpp:172] Iteration 6400, lr = 0.000167962, m = 0.9, wd = 0.0005, gs = 1
I0511 15:21:35.598964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:22:57.373780   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:24:06.000193   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:25:10.659116   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:25:31.005918   286 solver.cpp:354] Iteration 6500 (0.367247 iter/s, 272.296s/100 iter), 244.7/376.5ep, loss = 2.49009
I0511 15:25:31.006278   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.52787 (* 1 = 2.52787 loss)
I0511 15:25:31.006395   286 sgd_solver.cpp:172] Iteration 6500, lr = 0.000150063, m = 0.9, wd = 0.0005, gs = 1
I0511 15:26:26.402189   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:27:33.935775   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:28:43.717872   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:29:52.436280   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:29:55.120820   286 solver.cpp:354] Iteration 6600 (0.378624 iter/s, 264.115s/100 iter), 248.5/376.5ep, loss = 2.33857
I0511 15:29:55.121644   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.55622 (* 1 = 2.55622 loss)
I0511 15:29:55.121953   286 sgd_solver.cpp:172] Iteration 6600, lr = 0.000133634, m = 0.9, wd = 0.0005, gs = 1
I0511 15:31:04.691828   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:32:11.757402   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:33:15.932106   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:34:10.471763   286 solver.cpp:354] Iteration 6700 (0.391619 iter/s, 255.351s/100 iter), 252.2/376.5ep, loss = 2.36313
I0511 15:34:10.471865   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.8176 (* 1 = 1.8176 loss)
I0511 15:34:10.471891   286 sgd_solver.cpp:172] Iteration 6700, lr = 0.000118592, m = 0.9, wd = 0.0005, gs = 1
I0511 15:34:28.992126   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:35:32.187165   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:36:38.552619   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:37:45.253664   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:38:24.437515   286 solver.cpp:354] Iteration 6800 (0.393755 iter/s, 253.965s/100 iter), 256/376.5ep, loss = 2.11028
I0511 15:38:24.437613   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.22305 (* 1 = 2.22305 loss)
I0511 15:38:24.437639   286 sgd_solver.cpp:172] Iteration 6800, lr = 0.000104858, m = 0.9, wd = 0.0005, gs = 1
I0511 15:39:03.429338   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:40:07.609115   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:41:13.936862   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:42:25.769069   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:42:48.835839   286 solver.cpp:354] Iteration 6900 (0.378218 iter/s, 264.398s/100 iter), 259.8/376.5ep, loss = 2.27571
I0511 15:42:48.836088   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.91882 (* 1 = 1.91882 loss)
I0511 15:42:48.836158   286 sgd_solver.cpp:172] Iteration 6900, lr = 9.23521e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:43:36.997524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:44:40.528015   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:45:55.457073   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:47:08.861306   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:47:10.037616   286 solver.cpp:354] Iteration 7000 (0.382847 iter/s, 261.201s/100 iter), 263.5/376.5ep, loss = 2.21986
I0511 15:47:10.037907   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.45303 (* 1 = 2.45303 loss)
I0511 15:47:10.038018   286 sgd_solver.cpp:172] Iteration 7000, lr = 8.1e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:48:13.642457   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:49:31.888128   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:50:44.515708   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:51:43.701668   286 solver.cpp:354] Iteration 7100 (0.365413 iter/s, 273.663s/100 iter), 267.3/376.5ep, loss = 2.16657
I0511 15:51:43.701923   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.15395 (* 1 = 2.15395 loss)
I0511 15:51:43.702021   286 sgd_solver.cpp:172] Iteration 7100, lr = 7.07281e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:51:55.925788   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:53:05.393335   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:54:10.114141   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:55:29.308032   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:56:10.454062   286 solver.cpp:354] Iteration 7200 (0.37488 iter/s, 266.752s/100 iter), 271.1/376.5ep, loss = 2.25104
I0511 15:56:10.455046   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.45581 (* 1 = 2.45581 loss)
I0511 15:56:10.455458   286 sgd_solver.cpp:172] Iteration 7200, lr = 6.14656e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:56:42.564884   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:57:47.640079   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:59:03.445338   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:00:14.926611   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:00:39.265496   286 solver.cpp:354] Iteration 7300 (0.372007 iter/s, 268.812s/100 iter), 274.8/376.5ep, loss = 2.27763
I0511 16:00:39.265856   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.21352 (* 1 = 2.21352 loss)
I0511 16:00:39.266031   286 sgd_solver.cpp:172] Iteration 7300, lr = 5.31441e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:01:21.630378   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:02:30.629869   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:03:49.762984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:04:56.008975   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:05:03.062042   286 solver.cpp:354] Iteration 7400 (0.379079 iter/s, 263.797s/100 iter), 278.6/376.5ep, loss = 2.1906
I0511 16:05:03.062125   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.68163 (* 1 = 1.68163 loss)
I0511 16:05:03.062153   286 sgd_solver.cpp:172] Iteration 7400, lr = 4.56976e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:05:59.355573   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:07:15.925882   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:08:21.498852   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:09:24.217327   286 solver.cpp:354] Iteration 7500 (0.382914 iter/s, 261.155s/100 iter), 282.4/376.5ep, loss = 2.13837
I0511 16:09:24.218309   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.2895 (* 1 = 2.2895 loss)
I0511 16:09:24.218569   286 sgd_solver.cpp:172] Iteration 7500, lr = 3.90625e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:09:27.092594   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:10:30.863638   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:11:49.645085   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:13:00.644201   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:13:29.997458   286 solver.cpp:354] Iteration 7600 (0.406868 iter/s, 245.78s/100 iter), 286.1/376.5ep, loss = 2.27851
I0511 16:13:29.997714   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.3643 (* 1 = 2.3643 loss)
I0511 16:13:29.997809   286 sgd_solver.cpp:172] Iteration 7600, lr = 3.31776e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:13:42.776939   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:14:28.675060   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:15:12.805907   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:15:51.212491   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:16:10.499958   286 solver.cpp:354] Iteration 7700 (0.623044 iter/s, 160.502s/100 iter), 289.9/376.5ep, loss = 2.2783
I0511 16:16:10.499995   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.98052 (* 1 = 1.98052 loss)
I0511 16:16:10.500005   286 sgd_solver.cpp:172] Iteration 7700, lr = 2.79841e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:16:33.372638   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:17:14.421397   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:03.723328   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:47.234839   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:51.817420   286 solver.cpp:354] Iteration 7800 (0.619896 iter/s, 161.317s/100 iter), 293.6/376.5ep, loss = 2.10944
I0511 16:18:51.817495   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.15876 (* 1 = 2.15876 loss)
I0511 16:18:51.817521   286 sgd_solver.cpp:172] Iteration 7800, lr = 2.34256e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:19:27.776983   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:20:11.649794   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:20:56.553231   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:21:35.426523   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:21:36.680392   286 solver.cpp:354] Iteration 7900 (0.606565 iter/s, 164.863s/100 iter), 297.4/376.5ep, loss = 2.17191
I0511 16:21:36.680541   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.021 (* 1 = 2.021 loss)
I0511 16:21:36.680604   286 sgd_solver.cpp:172] Iteration 7900, lr = 1.94481e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:22:17.673382   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:23:00.965333   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:23:42.828294   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:24:09.244841   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0511 16:24:09.284216   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0511 16:24:09.359680   286 solver.cpp:637] Iteration 8000, Testing net (#0)
I0511 16:24:25.265329   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:24:25.702939   286 solver.cpp:749] class AP 1: 0.902042
I0511 16:24:25.703631   286 solver.cpp:749] class AP 2: 0.887863
I0511 16:24:25.703860   286 solver.cpp:749] class AP 3: 0.90274
I0511 16:24:25.703871   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897548
I0511 16:24:25.703908   286 solver.cpp:284] Tests completed in 169.023s
I0511 16:24:26.115075   286 solver.cpp:354] Iteration 8000 (0.591634 iter/s, 169.023s/100 iter), 301.2/376.5ep, loss = 2.23565
I0511 16:24:26.115190   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.26163 (* 1 = 2.26163 loss)
I0511 16:24:26.115236   286 sgd_solver.cpp:172] Iteration 8000, lr = 1.6e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:24:30.135054   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:25:14.574992   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:25:56.786598   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:26:41.331065   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:27:00.310946   286 solver.cpp:354] Iteration 8100 (0.648526 iter/s, 154.196s/100 iter), 304.9/376.5ep, loss = 2.14424
I0511 16:27:00.311252   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.96885 (* 1 = 1.96885 loss)
I0511 16:27:00.311309   286 sgd_solver.cpp:172] Iteration 8100, lr = 1.30321e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:27:22.615207   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:28:09.342959   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:28:48.916963   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:29:27.535276   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:29:40.441097   286 solver.cpp:354] Iteration 8200 (0.624493 iter/s, 160.13s/100 iter), 308.7/376.5ep, loss = 2.25428
I0511 16:29:40.441215   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.04646 (* 1 = 2.04646 loss)
I0511 16:29:40.441258   286 sgd_solver.cpp:172] Iteration 8200, lr = 1.04976e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:30:13.592989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:30:55.455257   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:31:36.515295   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:32:20.302894   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:32:20.665565   286 solver.cpp:354] Iteration 8300 (0.62413 iter/s, 160.223s/100 iter), 312.5/376.5ep, loss = 2.29563
I0511 16:32:20.665751   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.40904 (* 1 = 2.40904 loss)
I0511 16:32:20.665818   286 sgd_solver.cpp:172] Iteration 8300, lr = 8.3521e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:33:02.484475   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:33:43.240209   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:34:24.105861   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:35:02.028978   286 solver.cpp:354] Iteration 8400 (0.619724 iter/s, 161.362s/100 iter), 316.2/376.5ep, loss = 2.28401
I0511 16:35:02.029556   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.86963 (* 1 = 1.86963 loss)
I0511 16:35:02.029690   286 sgd_solver.cpp:172] Iteration 8400, lr = 6.5536e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:35:11.009938   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:35:52.894030   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:36:35.760008   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:37:17.757217   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:37:42.382684   286 solver.cpp:354] Iteration 8500 (0.623625 iter/s, 160.353s/100 iter), 320/376.5ep, loss = 2.27398
I0511 16:37:42.382864   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.18477 (* 1 = 2.18477 loss)
I0511 16:37:42.382912   286 sgd_solver.cpp:172] Iteration 8500, lr = 5.0625e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:38:03.450585   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:38:43.457687   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:39:22.327143   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:40:07.794312   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:40:20.838984   286 solver.cpp:354] Iteration 8600 (0.631092 iter/s, 158.456s/100 iter), 323.8/376.5ep, loss = 2.39464
I0511 16:40:20.839390   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.02272 (* 1 = 2.02272 loss)
I0511 16:40:20.839588   286 sgd_solver.cpp:172] Iteration 8600, lr = 3.8416e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:40:46.781332   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:41:30.368785   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:10.501636   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:57.561727   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:58.886313   286 solver.cpp:354] Iteration 8700 (0.632725 iter/s, 158.047s/100 iter), 327.5/376.5ep, loss = 2.13683
I0511 16:42:58.886471   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.16945 (* 1 = 2.16945 loss)
I0511 16:42:58.886520   286 sgd_solver.cpp:172] Iteration 8700, lr = 2.8561e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:43:39.298666   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:44:19.123512   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:45:01.745896   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:45:34.876058   286 solver.cpp:354] Iteration 8800 (0.64107 iter/s, 155.989s/100 iter), 331.3/376.5ep, loss = 2.40312
I0511 16:45:34.876492   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.04759 (* 1 = 2.04759 loss)
I0511 16:45:34.876618   286 sgd_solver.cpp:172] Iteration 8800, lr = 2.0736e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:45:43.850401   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:46:22.865329   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:47:11.569540   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:47:57.133761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:48:22.820315   286 solver.cpp:354] Iteration 8900 (0.595438 iter/s, 167.944s/100 iter), 335.1/376.5ep, loss = 2.17821
I0511 16:48:22.820353   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.20312 (* 1 = 2.20312 loss)
I0511 16:48:22.820361   286 sgd_solver.cpp:172] Iteration 8900, lr = 1.4641e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:48:36.715961   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:49:20.383297   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:00.097582   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:42.608451   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:58.479560   286 solver.cpp:354] Iteration 9000 (0.642431 iter/s, 155.659s/100 iter), 338.8/376.5ep, loss = 2.18224
I0511 16:50:58.479645   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.73082 (* 1 = 1.73082 loss)
I0511 16:50:58.479669   286 sgd_solver.cpp:172] Iteration 9000, lr = 1e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:51:21.580065   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:52:03.864205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:52:43.513609   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:53:28.449131   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:53:34.075006   286 solver.cpp:354] Iteration 9100 (0.642694 iter/s, 155.595s/100 iter), 342.6/376.5ep, loss = 2.19036
I0511 16:53:34.075166   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.82579 (* 1 = 1.82579 loss)
I0511 16:53:34.075212   286 sgd_solver.cpp:172] Iteration 9100, lr = 6.56099e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:54:11.565376   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:54:53.837738   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:55:34.529760   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:56:15.418911   286 solver.cpp:354] Iteration 9200 (0.619796 iter/s, 161.343s/100 iter), 346.4/376.5ep, loss = 2.23599
I0511 16:56:15.419200   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.90666 (* 1 = 1.90666 loss)
I0511 16:56:15.419309   286 sgd_solver.cpp:172] Iteration 9200, lr = 4.096e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:56:21.277537   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:57:02.657733   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:57:44.023488   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:58:25.042374   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:58:59.089303   286 solver.cpp:354] Iteration 9300 (0.610986 iter/s, 163.67s/100 iter), 350.1/376.5ep, loss = 2.07393
I0511 16:58:59.089525   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.6072 (* 1 = 1.6072 loss)
I0511 16:58:59.089589   286 sgd_solver.cpp:172] Iteration 9300, lr = 2.401e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:59:13.115172   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:59:52.247352   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:00:35.480310   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:01:18.659698   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:01:37.360014   286 solver.cpp:354] Iteration 9400 (0.631831 iter/s, 158.27s/100 iter), 353.9/376.5ep, loss = 2.28215
I0511 17:01:37.360424   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.17892 (* 1 = 2.17892 loss)
I0511 17:01:37.360540   286 sgd_solver.cpp:172] Iteration 9400, lr = 1.296e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 17:02:00.776641   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:02:43.891194   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:03:26.324103   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:04:05.488236   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:04:14.910394   286 solver.cpp:354] Iteration 9500 (0.634718 iter/s, 157.55s/100 iter), 357.6/376.5ep, loss = 2.12974
I0511 17:04:14.910800   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.07233 (* 1 = 3.07233 loss)
I0511 17:04:14.910974   286 sgd_solver.cpp:172] Iteration 9500, lr = 6.25001e-08, m = 0.9, wd = 0.0005, gs = 1
I0511 17:04:47.470029   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:05:35.667562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:06:13.618160   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:06:52.747225   286 solver.cpp:354] Iteration 9600 (0.633562 iter/s, 157.838s/100 iter), 361.4/376.5ep, loss = 2.39994
I0511 17:06:52.747494   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.44933 (* 1 = 2.44933 loss)
I0511 17:06:52.747506   286 sgd_solver.cpp:172] Iteration 9600, lr = 2.56001e-08, m = 0.9, wd = 0.0005, gs = 1
I0511 17:06:53.927964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:07:40.285562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:08:23.153424   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:09:04.857872   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:09:38.679633   286 solver.cpp:354] Iteration 9700 (0.602652 iter/s, 165.933s/100 iter), 365.2/376.5ep, loss = 2.32112
I0511 17:09:38.680035   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.88492 (* 1 = 1.88492 loss)
I0511 17:09:38.680127   286 sgd_solver.cpp:172] Iteration 9700, lr = 8.09997e-09, m = 0.9, wd = 0.0005, gs = 1
I0511 17:09:45.193013   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:10:33.109450   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:11:11.886011   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:11:54.116119   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:12:16.107566   286 solver.cpp:354] Iteration 9800 (0.63521 iter/s, 157.428s/100 iter), 368.9/376.5ep, loss = 2.38863
I0511 17:12:16.107741   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.29323 (* 1 = 2.29323 loss)
I0511 17:12:16.107795   286 sgd_solver.cpp:172] Iteration 9800, lr = 1.59999e-09, m = 0.9, wd = 0.0005, gs = 1
I0511 17:12:40.061962   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:13:19.355566   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:13:57.506289   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:14:42.720100   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:14:53.041469   286 solver.cpp:354] Iteration 9900 (0.63721 iter/s, 156.934s/100 iter), 372.7/376.5ep, loss = 2.44318
I0511 17:14:53.041887   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.37752 (* 1 = 2.37752 loss)
I0511 17:14:53.042093   286 sgd_solver.cpp:172] Iteration 9900, lr = 9.99996e-11, m = 0.9, wd = 0.0005, gs = 1
I0511 17:15:22.521019   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:16:04.028110   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:16:46.902962   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:26.762310   286 solver.cpp:354] Iteration 9999 (0.644024 iter/s, 153.721s/99 iter), 376.4/376.5ep, loss = 2.18223
I0511 17:17:26.762776   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.80593 (* 1 = 1.80593 loss)
I0511 17:17:26.762920   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 17:17:26.830179   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0511 17:17:27.047933   286 solver.cpp:503] Iteration 10000, loss = 2.2178
I0511 17:17:27.048238   286 solver.cpp:637] Iteration 10000, Testing net (#0)
I0511 17:17:27.323174   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:41.535437   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:42.039341   286 solver.cpp:749] class AP 1: 0.902412
I0511 17:17:42.040043   286 solver.cpp:749] class AP 2: 0.8876
I0511 17:17:42.040272   286 solver.cpp:749] class AP 3: 0.902822
I0511 17:17:42.040278   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897611
I0511 17:17:42.040299   286 caffe.cpp:268] Solver performance on device 0: 0.4179 * 16 = 13.37 img/sec (10000 itr in 2.393e+04 sec)
I0511 17:17:42.040307   286 caffe.cpp:271] Optimization Done in 6h 39m 50s
caffe.bin: ../nptl/pthread_mutex_lock.c:115: __pthread_mutex_lock: Assertion `mutex->__data.__owner == 0' failed.
*** Aborted at 1589217462 (unix time) try "date -d @1589217462" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x11e) received by PID 286 (TID 0x7fef47fff700) from PID 286; stack trace: ***
    @     0x7ff19793bf20 (unknown)
    @     0x7ff19793be97 gsignal
    @     0x7ff19793d801 abort
    @     0x7ff19792d39a (unknown)
    @     0x7ff19792d412 __assert_fail
    @     0x7ff1976e8208 __GI___pthread_mutex_lock
    @     0x7ff1994ec128 boost::mutex::lock()
    @     0x7ff1994ed020 boost::unique_lock<>::lock()
    @     0x7ff19998489f caffe::BlockingQueue<>::push()
    @     0x7ff19956b49e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7ff1995a5476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7ff1995197ee caffe::InternalThread::entry()
    @     0x7ff19951b54b boost::detail::thread_data<>::run()
    @     0x7ff198b537ee thread_proxy
    @     0x7ff1976e56db start_thread
    @     0x7ff197a1e88f clone
    @                0x0 (unknown)
