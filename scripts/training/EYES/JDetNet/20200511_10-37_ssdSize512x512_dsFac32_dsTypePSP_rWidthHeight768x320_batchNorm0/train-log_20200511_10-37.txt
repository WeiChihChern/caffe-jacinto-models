Logging output to training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/train-log_20200511_10-37.txt
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial
{'type':'Adam','base_lr':1e-2,'max_iter':10000,'lr_policy':'poly','power':4.0,'stepvalue':[30000,45000,300000],'weight_decay':0.0005}
{'config_name':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial','model_name':'ssdJacintoNetV2','dataset':'EYES','gpus':'0','train_data':'/workspace/data/EYES/lmdb/EYES_trainval_lmdb','test_data':'/workspace/data/EYES/lmdb/official_test_850images','name_size_file':'/workspace/caffe-jacinto/data/EYES/test_name_size.txt','label_map_file':'/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt','num_test_image':850,'num_classes':4,'min_ratio':10,'max_ratio':90,
'log_space_steps':2,'use_difficult_gt':0,'ignore_difficult_gt':0,'evaluate_difficult_gt':0,'pretrain_model':'/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel','use_image_list':0,'shuffle':0,'num_output':8,'resize_width':768,'resize_height':320,'crop_width':768,'crop_height':320,'batch_size':16,'aspect_ratios_type':1,'ssd_size':'512x512','small_objs':1,'min_dim':368,'concat_reg_head':0,'fully_conv_at_end':0,'first_hd_same_op_ch':1,'ker_mbox_loc_conf':1,'base_nw_3_head':0,'reg_head_at_ds8':1,'ds_fac':32,'ds_type':'PSP','rhead_name_non_linear':0,'force_color':0,'num_intermediate':512,'use_batchnorm_mbox':0,'chop_num_heads':0}
caffe_root = :  /workspace/caffe-jacinto/build/tools/caffe.bin
config_param.ds_fac : 32
config_param.stride_list : [2, 2, 2, 2, 2]
num_gpus: 1  gpulist: ['0']
min_dim = 368
ratio_step_size: 20
minsizes = [14.72, 36.8, 110.4, 184.0, 257.6, 331.2]
maxsizes = [36.8, 110.4, 184.0, 257.6, 331.2, 404.8]
ARs: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/train.prototxt
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg
{'snapshot':1000,'test_interval':500,'type':'Adam','base_lr':1e-3,'max_iter':10000,'lr_policy':'poly','power':4.0,'stepvalue':[30000,45000,300000],'regularization_type':'L1','weight_decay':1e-5}
{'config_name':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg','model_name':'ssdJacintoNetV2','dataset':'EYES','gpus':'0','train_data':'/workspace/data/EYES/lmdb/EYES_trainval_lmdb','test_data':'/workspace/data/EYES/lmdb/official_test_850images','name_size_file':'/workspace/caffe-jacinto/data/EYES/test_name_size.txt','label_map_file':'/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt','num_test_image':850,'num_classes':4,'min_ratio':10,'max_ratio':90,
'log_space_steps':2,'use_difficult_gt':0,'ignore_difficult_gt':0,'evaluate_difficult_gt':0,'pretrain_model':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel','use_image_list':0,'shuffle':0,'num_output':8,'resize_width':768,'resize_height':320,'crop_width':768,'crop_height':320,'batch_size':16,'aspect_ratios_type':1,'ssd_size':'512x512','small_objs':1,'min_dim':368,'concat_reg_head':0,
'fully_conv_at_end':0,'first_hd_same_op_ch':1,'ker_mbox_loc_conf':1,'base_nw_3_head':0,'reg_head_at_ds8':1,'ds_fac':32,'ds_type':'PSP','rhead_name_non_linear':0,'force_color':0,'num_intermediate':512,'use_batchnorm_mbox':0,'chop_num_heads':0}
caffe_root = :  /workspace/caffe-jacinto/build/tools/caffe.bin
config_param.ds_fac : 32
config_param.stride_list : [2, 2, 2, 2, 2]
num_gpus: 1  gpulist: ['0']
min_dim = 368
ratio_step_size: 20
minsizes = [14.72, 36.8, 110.4, 184.0, 257.6, 331.2]
maxsizes = [36.8, 110.4, 184.0, 257.6, 331.2, 404.8]
ARs: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/train.prototxt
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse
{'snapshot':1000,'test_interval':500,'type':'Adam','base_lr':1e-3,'max_iter':20000,'lr_policy':'poly','power':4.0,'stepvalue':[30000,45000,300000],'regularization_type':'L1','weight_decay':1e-5,'sparse_mode':1,'display_sparsity':2000,'sparsity_target':0.75,'sparsity_start_iter':0,'sparsity_start_factor':0.5,'sparsity_step_iter':2000,'sparsity_step_factor':0.05,'sparsity_itr_increment_bfr_applying':1,'sparsity_threshold_maxratio':0.2,'sparsity_threshold_value_max':0.2}
{'config_name':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse','model_name':'ssdJacintoNetV2','dataset':'EYES','gpus':'0','train_data':'/workspace/data/EYES/lmdb/EYES_trainval_lmdb','test_data':'/workspace/data/EYES/lmdb/official_test_850images','name_size_file':'/workspace/caffe-jacinto/data/EYES/test_name_size.txt','label_map_file':'/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt','num_test_image':850,'num_classes':4,'min_ratio':10,'max_ratio':90,
'log_space_steps':2,'use_difficult_gt':0,'ignore_difficult_gt':0,'evaluate_difficult_gt':0,'pretrain_model':'/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel','use_image_list':0,'shuffle':0,'num_output':8,'resize_width':768,'resize_height':320,'crop_width':768,'crop_height':320,'batch_size':48,'aspect_ratios_type':1,'ssd_size':'512x512','small_objs':1,'min_dim':368,'concat_reg_head':0,
'fully_conv_at_end':0,'first_hd_same_op_ch':1,'ker_mbox_loc_conf':1,'base_nw_3_head':0,'reg_head_at_ds8':1,'ds_fac':32,'ds_type':'PSP','rhead_name_non_linear':0,'force_color':0,'num_intermediate':512,'use_batchnorm_mbox':0, 'chop_num_heads':0}
caffe_root = :  /workspace/caffe-jacinto/build/tools/caffe.bin
config_param.ds_fac : 32
config_param.stride_list : [2, 2, 2, 2, 2]
num_gpus: 1  gpulist: ['0']
min_dim = 368
ratio_step_size: 20
minsizes = [14.72, 36.8, 110.4, 184.0, 257.6, 331.2]
maxsizes = [36.8, 110.4, 184.0, 257.6, 331.2, 404.8]
ARs: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/train.prototxt
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test
{'type':'Adam','base_lr':1e-3,'max_iter':20000,'lr_policy':'poly','power':4.0,'stepvalue':[30000,45000,300000],'regularization_type':'L1','weight_decay':1e-5,'sparse_mode':1,'display_sparsity':1000}
{'config_name':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test','model_name':'ssdJacintoNetV2','dataset':'EYES','gpus':'0','train_data':'/workspace/data/EYES/lmdb/EYES_trainval_lmdb','test_data':'/workspace/data/EYES/lmdb/official_test_850images','name_size_file':'/workspace/caffe-jacinto/data/EYES/test_name_size.txt','label_map_file':'/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt','num_test_image':850,'num_classes':4,'min_ratio':10,'max_ratio':90,
'log_space_steps':2,'use_difficult_gt':0,'ignore_difficult_gt':0,'evaluate_difficult_gt':0,'pretrain_model':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.caffemodel','use_image_list':0,'shuffle':0,'num_output':8,'resize_width':768,'resize_height':320,'crop_width':768,'crop_height':320,'batch_size':48,'test_batch_size':10,'caffe_cmd':'test_detection','display_sparsity':1,'aspect_ratios_type':1,'ssd_size':'512x512','small_objs':1,'min_dim':368,'concat_reg_head':0,
'fully_conv_at_end':0,'first_hd_same_op_ch':1,'ker_mbox_loc_conf':1,'base_nw_3_head':0,'reg_head_at_ds8':1,'ds_fac':32,'ds_type':'PSP','rhead_name_non_linear':0,'force_color':0,'num_intermediate':512,'use_batchnorm_mbox':0,'chop_num_heads':0}
caffe_root = :  /workspace/caffe-jacinto/build/tools/caffe.bin
config_param.ds_fac : 32
config_param.stride_list : [2, 2, 2, 2, 2]
num_gpus: 1  gpulist: ['0']
min_dim = 368
ratio_step_size: 20
minsizes = [14.72, 36.8, 110.4, 184.0, 257.6, 331.2]
maxsizes = [36.8, 110.4, 184.0, 257.6, 331.2, 404.8]
ARs: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test/train.prototxt
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test_quantize
{'type':'Adam','base_lr':1e-3,'max_iter':20000,'lr_policy':'poly','power':4.0,'stepvalue':[30000,45000,300000],'regularization_type':'L1','weight_decay':1e-5,'sparse_mode':1,'display_sparsity':1000}
{'config_name':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test_quantize','model_name':'ssdJacintoNetV2','dataset':'EYES','gpus':'0','train_data':'/workspace/data/EYES/lmdb/EYES_trainval_lmdb','test_data':'/workspace/data/EYES/lmdb/official_test_850images','name_size_file':'/workspace/caffe-jacinto/data/EYES/test_name_size.txt','label_map_file':'/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt','num_test_image':850,'num_classes':4,'min_ratio':10,'max_ratio':90,
'log_space_steps':2,'use_difficult_gt':0,'ignore_difficult_gt':0,'evaluate_difficult_gt':0,'pretrain_model':'training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test/EYES_ssdJacintoNetV2_iter_20000.caffemodel','use_image_list':0,'shuffle':0,'num_output':8,'resize_width':768,'resize_height':320,'crop_width':768,'crop_height':320,'batch_size':48,'test_batch_size':10,'caffe_cmd':'test_detection','aspect_ratios_type':1,'ssd_size':'512x512','small_objs':1,'min_dim':368,'concat_reg_head':0,
'fully_conv_at_end':0,'first_hd_same_op_ch':1,'ker_mbox_loc_conf':1,'base_nw_3_head':0,'reg_head_at_ds8':1,'ds_fac':32,'ds_type':'PSP','rhead_name_non_linear':0,'force_color':0,'num_intermediate':512,'use_batchnorm_mbox':0,'chop_num_heads':0}
caffe_root = :  /workspace/caffe-jacinto/build/tools/caffe.bin
config_param.ds_fac : 32
config_param.stride_list : [2, 2, 2, 2, 2]
num_gpus: 1  gpulist: ['0']
min_dim = 368
ratio_step_size: 20
minsizes = [14.72, 36.8, 110.4, 184.0, 257.6, 331.2]
maxsizes = [36.8, 110.4, 184.0, 257.6, 331.2, 404.8]
ARs: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test_quantize/train.prototxt
I0511 10:37:52.968363   286 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 10:37:52 2020
I0511 10:37:53.372486   286 caffe.cpp:904] CuDNN version: 7605
I0511 10:37:53.372524   286 caffe.cpp:905] CuBLAS version: 10202
I0511 10:37:53.372560   286 caffe.cpp:906] CUDA version: 10020
I0511 10:37:53.372596   286 caffe.cpp:907] CUDA driver version: 10020
I0511 10:37:53.372632   286 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/solver.prototxt
[3]: --weights=/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
[4]: --gpu
[5]: 0
I0511 10:37:53.406301   286 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 10:37:53.406407   286 gpu_memory.cpp:107] Total memory: 16900227072, Free: 11730550784, dev_info[0]: total=16900227072 free=11730550784
I0511 10:37:53.406695   286 caffe.cpp:226] Using GPUs 0
I0511 10:37:53.406881   286 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 10:37:53.406996   286 solver.cpp:41] Solver data type: FLOAT
I0511 10:37:53.419385   286 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/test.prototxt"
test_iter: 107
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 0.0005
snapshot: 2000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 2
type: "Adam"
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 10:37:53.419783   286 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/train.prototxt
I0511 10:37:53.422184   286 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 16
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 10:37:53.424973   286 net.cpp:110] Using FLOAT as default forward math type
I0511 10:37:53.425005   286 net.cpp:116] Using FLOAT as default backward math type
I0511 10:37:53.425037   286 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 10:37:53.425060   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.425202   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:37:53.425680   291 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 10:37:53.429348   286 net.cpp:200] Created Layer data (0)
I0511 10:37:53.429368   286 net.cpp:542] data -> data
I0511 10:37:53.429399   286 net.cpp:542] data -> label
I0511 10:37:53.429457   286 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 16
I0511 10:37:53.429569   286 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 10:37:53.437425   292 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.445333   293 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.465340   294 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.469344   295 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 10:37:53.496456   286 annotated_data_layer.cpp:105] output data size: 16,3,320,768
I0511 10:37:53.497028   286 annotated_data_layer.cpp:150] [0] Output data size: 16, 3, 320, 768
I0511 10:37:53.497166   286 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 10:37:53.505786   296 data_layer.cpp:105] [0] Parser threads: 4
I0511 10:37:53.505829   296 data_layer.cpp:107] [0] Transformer threads: 4
I0511 10:37:53.553561   286 net.cpp:260] Setting up data
I0511 10:37:53.553637   286 net.cpp:267] TRAIN Top shape for layer 0 'data' 16 3 320 768 (11796480)
I0511 10:37:53.553712   286 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 10:37:53.553766   286 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 10:37:53.553802   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.553840   286 net.cpp:200] Created Layer data_data_0_split (1)
I0511 10:37:53.553910   286 net.cpp:572] data_data_0_split <- data
I0511 10:37:53.553964   286 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 10:37:53.554014   286 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 10:37:53.554069   286 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 10:37:53.554131   286 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 10:37:53.554159   286 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 10:37:53.554214   286 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 10:37:53.554246   286 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 10:37:53.554447   286 net.cpp:260] Setting up data_data_0_split
I0511 10:37:53.554484   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554533   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554599   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554643   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554708   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554770   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554829   286 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 10:37:53.554913   286 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 10:37:53.554952   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.555014   286 net.cpp:200] Created Layer data/bias (2)
I0511 10:37:53.555065   286 net.cpp:572] data/bias <- data_data_0_split_0
I0511 10:37:53.555112   286 net.cpp:542] data/bias -> data/bias
I0511 10:37:53.561565   286 net.cpp:260] Setting up data/bias
I0511 10:37:53.561621   286 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 16 3 320 768 (11796480)
I0511 10:37:53.565732   286 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 10:37:53.565774   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:53.565829   286 net.cpp:200] Created Layer conv1a (3)
I0511 10:37:53.565857   286 net.cpp:572] conv1a <- data/bias
I0511 10:37:53.565886   286 net.cpp:542] conv1a -> conv1a
I0511 10:37:59.451964   286 net.cpp:260] Setting up conv1a
I0511 10:37:59.465309   286 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 16 32 160 384 (31457280)
I0511 10:37:59.465442   286 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 10:37:59.465523   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.465617   286 net.cpp:200] Created Layer conv1a/bn (4)
I0511 10:37:59.465699   286 net.cpp:572] conv1a/bn <- conv1a
I0511 10:37:59.465804   286 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 10:37:59.466562   286 net.cpp:260] Setting up conv1a/bn
I0511 10:37:59.466614   286 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 16 32 160 384 (31457280)
I0511 10:37:59.466962   286 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 10:37:59.467013   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.467269   286 net.cpp:200] Created Layer conv1a/relu (5)
I0511 10:37:59.467329   286 net.cpp:572] conv1a/relu <- conv1a
I0511 10:37:59.467588   286 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 10:37:59.467789   286 net.cpp:260] Setting up conv1a/relu
I0511 10:37:59.467974   286 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 16 32 160 384 (31457280)
I0511 10:37:59.468058   286 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 10:37:59.468140   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.468256   286 net.cpp:200] Created Layer conv1b (6)
I0511 10:37:59.468392   286 net.cpp:572] conv1b <- conv1a
I0511 10:37:59.468518   286 net.cpp:542] conv1b -> conv1b
I0511 10:37:59.469379   286 net.cpp:260] Setting up conv1b
I0511 10:37:59.469506   286 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 16 32 160 384 (31457280)
I0511 10:37:59.469660   286 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 10:37:59.469785   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.469920   286 net.cpp:200] Created Layer conv1b/bn (7)
I0511 10:37:59.470041   286 net.cpp:572] conv1b/bn <- conv1b
I0511 10:37:59.470170   286 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 10:37:59.470826   286 net.cpp:260] Setting up conv1b/bn
I0511 10:37:59.470933   286 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 16 32 160 384 (31457280)
I0511 10:37:59.471078   286 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 10:37:59.471200   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.471329   286 net.cpp:200] Created Layer conv1b/relu (8)
I0511 10:37:59.471457   286 net.cpp:572] conv1b/relu <- conv1b
I0511 10:37:59.471585   286 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 10:37:59.471714   286 net.cpp:260] Setting up conv1b/relu
I0511 10:37:59.471837   286 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 16 32 160 384 (31457280)
I0511 10:37:59.472045   286 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 10:37:59.472086   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.472327   286 net.cpp:200] Created Layer pool1 (9)
I0511 10:37:59.472543   286 net.cpp:572] pool1 <- conv1b
I0511 10:37:59.472643   286 net.cpp:542] pool1 -> pool1
I0511 10:37:59.472937   286 net.cpp:260] Setting up pool1
I0511 10:37:59.473031   286 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 16 32 80 192 (7864320)
I0511 10:37:59.473181   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 10:37:59.473318   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.473501   286 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 10:37:59.473603   286 net.cpp:572] res2a_branch2a <- pool1
I0511 10:37:59.473745   286 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 10:37:59.475531   286 net.cpp:260] Setting up res2a_branch2a
I0511 10:37:59.475644   286 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 16 64 80 192 (15728640)
I0511 10:37:59.475800   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.475946   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.476089   286 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 10:37:59.476217   286 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 10:37:59.476331   286 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 10:37:59.476917   286 net.cpp:260] Setting up res2a_branch2a/bn
I0511 10:37:59.477020   286 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 16 64 80 192 (15728640)
I0511 10:37:59.477159   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.477272   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.477425   286 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 10:37:59.477542   286 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 10:37:59.477661   286 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 10:37:59.477784   286 net.cpp:260] Setting up res2a_branch2a/relu
I0511 10:37:59.477896   286 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 16 64 80 192 (15728640)
I0511 10:37:59.478019   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 10:37:59.478137   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.478271   286 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 10:37:59.478390   286 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 10:37:59.478509   286 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 10:37:59.479214   286 net.cpp:260] Setting up res2a_branch2b
I0511 10:37:59.479326   286 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 16 64 80 192 (15728640)
I0511 10:37:59.479466   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.479590   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.479787   286 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 10:37:59.479828   286 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 10:37:59.480096   286 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 10:37:59.480671   286 net.cpp:260] Setting up res2a_branch2b/bn
I0511 10:37:59.480715   286 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 16 64 80 192 (15728640)
I0511 10:37:59.480903   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.480942   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.481098   286 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 10:37:59.481137   286 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 10:37:59.481298   286 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 10:37:59.481403   286 net.cpp:260] Setting up res2a_branch2b/relu
I0511 10:37:59.481441   286 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 16 64 80 192 (15728640)
I0511 10:37:59.481597   286 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 10:37:59.481645   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.481691   286 net.cpp:200] Created Layer pool2 (16)
I0511 10:37:59.481793   286 net.cpp:572] pool2 <- res2a_branch2b
I0511 10:37:59.481997   286 net.cpp:542] pool2 -> pool2
I0511 10:37:59.482169   286 net.cpp:260] Setting up pool2
I0511 10:37:59.482208   286 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 16 64 40 96 (3932160)
I0511 10:37:59.482313   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 10:37:59.482456   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.482568   286 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 10:37:59.482607   286 net.cpp:572] res3a_branch2a <- pool2
I0511 10:37:59.482715   286 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 10:37:59.484709   286 net.cpp:260] Setting up res3a_branch2a
I0511 10:37:59.484831   286 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 16 128 40 96 (7864320)
I0511 10:37:59.484941   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.484982   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.485136   286 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 10:37:59.485174   286 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 10:37:59.485332   286 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 10:37:59.485805   286 net.cpp:260] Setting up res3a_branch2a/bn
I0511 10:37:59.485906   286 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 16 128 40 96 (7864320)
I0511 10:37:59.486021   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.486063   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.486212   286 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 10:37:59.486250   286 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 10:37:59.486397   286 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 10:37:59.486439   286 net.cpp:260] Setting up res3a_branch2a/relu
I0511 10:37:59.486595   286 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 16 128 40 96 (7864320)
I0511 10:37:59.486641   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 10:37:59.486805   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.486927   286 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 10:37:59.487046   286 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 10:37:59.487154   286 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 10:37:59.488355   286 net.cpp:260] Setting up res3a_branch2b
I0511 10:37:59.488399   286 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 16 128 40 96 (7864320)
I0511 10:37:59.488584   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.488688   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.488801   286 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 10:37:59.488955   286 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 10:37:59.489008   286 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 10:37:59.497689   286 net.cpp:260] Setting up res3a_branch2b/bn
I0511 10:37:59.497738   286 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 16 128 40 96 (7864320)
I0511 10:37:59.497947   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.497995   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.498129   286 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 10:37:59.498368   286 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 10:37:59.498533   286 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 10:37:59.498705   286 net.cpp:260] Setting up res3a_branch2b/relu
I0511 10:37:59.498870   286 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 16 128 40 96 (7864320)
I0511 10:37:59.499037   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 10:37:59.499202   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.499375   286 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 10:37:59.499539   286 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 10:37:59.499709   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:37:59.499903   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:37:59.500131   286 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 10:37:59.500305   286 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 10:37:59.500485   286 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 10:37:59.500660   286 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 10:37:59.500834   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.501019   286 net.cpp:200] Created Layer pool3 (24)
I0511 10:37:59.501199   286 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:37:59.501394   286 net.cpp:542] pool3 -> pool3
I0511 10:37:59.501638   286 net.cpp:260] Setting up pool3
I0511 10:37:59.501819   286 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 16 128 20 48 (1966080)
I0511 10:37:59.502001   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 10:37:59.502171   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.502355   286 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 10:37:59.502528   286 net.cpp:572] res4a_branch2a <- pool3
I0511 10:37:59.502709   286 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 10:37:59.578085   286 net.cpp:260] Setting up res4a_branch2a
I0511 10:37:59.578179   286 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 16 256 20 48 (3932160)
I0511 10:37:59.578251   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.578287   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.578341   286 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 10:37:59.578379   286 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 10:37:59.578419   286 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 10:37:59.578893   286 net.cpp:260] Setting up res4a_branch2a/bn
I0511 10:37:59.578943   286 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 16 256 20 48 (3932160)
I0511 10:37:59.578996   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.579032   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.579071   286 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 10:37:59.579107   286 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 10:37:59.579140   286 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 10:37:59.579180   286 net.cpp:260] Setting up res4a_branch2a/relu
I0511 10:37:59.579212   286 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 16 256 20 48 (3932160)
I0511 10:37:59.579249   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 10:37:59.579284   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.579330   286 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 10:37:59.579365   286 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 10:37:59.579417   286 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 10:37:59.582883   286 net.cpp:260] Setting up res4a_branch2b
I0511 10:37:59.589303   286 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 16 256 20 48 (3932160)
I0511 10:37:59.589373   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.589411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.589454   286 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 10:37:59.589490   286 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 10:37:59.589535   286 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 10:37:59.590063   286 net.cpp:260] Setting up res4a_branch2b/bn
I0511 10:37:59.590102   286 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 16 256 20 48 (3932160)
I0511 10:37:59.590152   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.590188   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.590238   286 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 10:37:59.590282   286 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 10:37:59.590327   286 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 10:37:59.590374   286 net.cpp:260] Setting up res4a_branch2b/relu
I0511 10:37:59.590416   286 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 16 256 20 48 (3932160)
I0511 10:37:59.590463   286 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 10:37:59.590507   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.590556   286 net.cpp:200] Created Layer pool4 (31)
I0511 10:37:59.590600   286 net.cpp:572] pool4 <- res4a_branch2b
I0511 10:37:59.590644   286 net.cpp:542] pool4 -> pool4
I0511 10:37:59.590750   286 net.cpp:260] Setting up pool4
I0511 10:37:59.590867   286 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 16 256 10 24 (983040)
I0511 10:37:59.590960   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 10:37:59.591061   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.591166   286 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 10:37:59.591254   286 net.cpp:572] res5a_branch2a <- pool4
I0511 10:37:59.591392   286 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 10:37:59.694778   286 net.cpp:260] Setting up res5a_branch2a
I0511 10:37:59.702265   286 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 16 512 10 24 (1966080)
I0511 10:37:59.702711   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 10:37:59.703001   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.703267   286 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 10:37:59.703541   286 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 10:37:59.703794   286 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 10:37:59.704558   286 net.cpp:260] Setting up res5a_branch2a/bn
I0511 10:37:59.705312   286 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 16 512 10 24 (1966080)
I0511 10:37:59.705631   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 10:37:59.705883   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.706144   286 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 10:37:59.706401   286 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 10:37:59.706660   286 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 10:37:59.706919   286 net.cpp:260] Setting up res5a_branch2a/relu
I0511 10:37:59.707173   286 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 16 512 10 24 (1966080)
I0511 10:37:59.707437   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 10:37:59.707686   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.707957   286 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 10:37:59.708221   286 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 10:37:59.708467   286 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 10:37:59.752246   286 net.cpp:260] Setting up res5a_branch2b
I0511 10:37:59.758381   286 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 16 512 10 24 (1966080)
I0511 10:37:59.758863   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 10:37:59.759181   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.759562   286 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 10:37:59.759912   286 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 10:37:59.760257   286 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 10:37:59.761179   286 net.cpp:260] Setting up res5a_branch2b/bn
I0511 10:37:59.762125   286 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 16 512 10 24 (1966080)
I0511 10:37:59.762462   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 10:37:59.762768   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.763053   286 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 10:37:59.763346   286 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 10:37:59.763672   286 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 10:37:59.764040   286 net.cpp:260] Setting up res5a_branch2b/relu
I0511 10:37:59.764394   286 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 16 512 10 24 (1966080)
I0511 10:37:59.764755   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 10:37:59.765107   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.765468   286 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 10:37:59.765825   286 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 10:37:59.766185   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:37:59.766541   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:37:59.767026   286 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 10:37:59.767511   286 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 10:37:59.767911   286 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 10:37:59.768313   286 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 10:37:59.768699   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.769016   286 net.cpp:200] Created Layer pool6 (39)
I0511 10:37:59.769263   286 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:37:59.769557   286 net.cpp:542] pool6 -> pool6
I0511 10:37:59.770010   286 net.cpp:260] Setting up pool6
I0511 10:37:59.770399   286 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 16 512 5 12 (491520)
I0511 10:37:59.770747   286 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 10:37:59.771081   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.771420   286 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 10:37:59.771731   286 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 10:37:59.772058   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 10:37:59.772393   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 10:37:59.772864   286 net.cpp:260] Setting up pool6_pool6_0_split
I0511 10:37:59.773277   286 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 10:37:59.773625   286 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 10:37:59.773916   286 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 10:37:59.774183   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.774462   286 net.cpp:200] Created Layer pool7 (41)
I0511 10:37:59.774735   286 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 10:37:59.775028   286 net.cpp:542] pool7 -> pool7
I0511 10:37:59.775413   286 net.cpp:260] Setting up pool7
I0511 10:37:59.775795   286 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 16 512 3 6 (147456)
I0511 10:37:59.776063   286 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 10:37:59.776333   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.776621   286 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 10:37:59.776892   286 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 10:37:59.777154   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 10:37:59.777469   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 10:37:59.777859   286 net.cpp:260] Setting up pool7_pool7_0_split
I0511 10:37:59.778190   286 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 10:37:59.778463   286 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 10:37:59.778748   286 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 10:37:59.779024   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.779300   286 net.cpp:200] Created Layer pool8 (43)
I0511 10:37:59.779575   286 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 10:37:59.779851   286 net.cpp:542] pool8 -> pool8
I0511 10:37:59.780205   286 net.cpp:260] Setting up pool8
I0511 10:37:59.780552   286 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 16 512 2 3 (49152)
I0511 10:37:59.780839   286 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 10:37:59.781111   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.781422   286 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 10:37:59.781716   286 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 10:37:59.782016   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 10:37:59.782303   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 10:37:59.782629   286 net.cpp:260] Setting up pool8_pool8_0_split
I0511 10:37:59.782953   286 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 10:37:59.783234   286 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 10:37:59.783515   286 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 10:37:59.783794   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.784070   286 net.cpp:200] Created Layer pool9 (45)
I0511 10:37:59.784350   286 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 10:37:59.784620   286 net.cpp:542] pool9 -> pool9
I0511 10:37:59.784987   286 net.cpp:260] Setting up pool9
I0511 10:37:59.785331   286 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 16 512 1 2 (16384)
I0511 10:37:59.785663   286 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 10:37:59.785959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.786276   286 net.cpp:200] Created Layer ctx_output1 (46)
I0511 10:37:59.786576   286 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:37:59.786847   286 net.cpp:542] ctx_output1 -> ctx_output1
I0511 10:37:59.788192   286 net.cpp:260] Setting up ctx_output1
I0511 10:37:59.789578   286 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 16 256 40 96 (15728640)
I0511 10:37:59.789927   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 10:37:59.790195   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.790480   286 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 10:37:59.790767   286 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 10:37:59.791036   286 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 10:37:59.791319   286 net.cpp:260] Setting up ctx_output1/relu
I0511 10:37:59.791595   286 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 16 256 40 96 (15728640)
I0511 10:37:59.791908   286 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 10:37:59.792178   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.792452   286 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 10:37:59.792727   286 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 10:37:59.793002   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 10:37:59.793315   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 10:37:59.793637   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 10:37:59.794057   286 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 10:37:59.794401   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.794689   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.794984   286 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 10:37:59.795286   286 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 10:37:59.795585   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.795894   286 net.cpp:200] Created Layer ctx_output2 (49)
I0511 10:37:59.796196   286 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:37:59.796505   286 net.cpp:542] ctx_output2 -> ctx_output2
I0511 10:37:59.812373   286 net.cpp:260] Setting up ctx_output2
I0511 10:37:59.815264   286 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 16 256 10 24 (983040)
I0511 10:37:59.815621   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 10:37:59.815928   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.816241   286 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 10:37:59.816520   286 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 10:37:59.816821   286 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 10:37:59.817113   286 net.cpp:260] Setting up ctx_output2/relu
I0511 10:37:59.817438   286 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 16 256 10 24 (983040)
I0511 10:37:59.817778   286 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 10:37:59.818084   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.818385   286 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 10:37:59.818691   286 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 10:37:59.819000   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 10:37:59.819286   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 10:37:59.819591   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 10:37:59.820000   286 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 10:37:59.820354   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.820664   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.820971   286 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 10:37:59.821257   286 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 10:37:59.821599   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.821977   286 net.cpp:200] Created Layer ctx_output3 (52)
I0511 10:37:59.822280   286 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 10:37:59.822566   286 net.cpp:542] ctx_output3 -> ctx_output3
I0511 10:37:59.826793   286 net.cpp:260] Setting up ctx_output3
I0511 10:37:59.831095   286 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 16 256 5 12 (245760)
I0511 10:37:59.831465   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 10:37:59.831773   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.832083   286 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 10:37:59.832367   286 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 10:37:59.832654   286 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 10:37:59.832963   286 net.cpp:260] Setting up ctx_output3/relu
I0511 10:37:59.833254   286 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 16 256 5 12 (245760)
I0511 10:37:59.833595   286 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 10:37:59.833923   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.834221   286 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 10:37:59.834528   286 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 10:37:59.834816   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 10:37:59.835124   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 10:37:59.835417   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 10:37:59.835784   286 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 10:37:59.841333   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.841737   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.842072   286 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 10:37:59.842381   286 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 10:37:59.842669   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.842983   286 net.cpp:200] Created Layer ctx_output4 (55)
I0511 10:37:59.843279   286 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 10:37:59.843565   286 net.cpp:542] ctx_output4 -> ctx_output4
I0511 10:37:59.846942   286 net.cpp:260] Setting up ctx_output4
I0511 10:37:59.850342   286 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 16 256 3 6 (73728)
I0511 10:37:59.850690   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 10:37:59.850993   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.851298   286 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 10:37:59.851577   286 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 10:37:59.851882   286 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 10:37:59.852186   286 net.cpp:260] Setting up ctx_output4/relu
I0511 10:37:59.852486   286 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 16 256 3 6 (73728)
I0511 10:37:59.852778   286 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 10:37:59.853080   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.853415   286 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 10:37:59.853745   286 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 10:37:59.854048   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 10:37:59.854344   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 10:37:59.854668   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 10:37:59.855083   286 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 10:37:59.855448   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.855753   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.856036   286 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 10:37:59.856329   286 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 10:37:59.856614   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.856938   286 net.cpp:200] Created Layer ctx_output5 (58)
I0511 10:37:59.857235   286 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 10:37:59.857579   286 net.cpp:542] ctx_output5 -> ctx_output5
I0511 10:37:59.860971   286 net.cpp:260] Setting up ctx_output5
I0511 10:37:59.871109   286 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 16 256 2 3 (24576)
I0511 10:37:59.871464   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 10:37:59.871742   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.872062   286 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 10:37:59.872340   286 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 10:37:59.872620   286 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 10:37:59.872925   286 net.cpp:260] Setting up ctx_output5/relu
I0511 10:37:59.873195   286 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 16 256 2 3 (24576)
I0511 10:37:59.873533   286 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 10:37:59.873859   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.874179   286 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 10:37:59.874442   286 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 10:37:59.874747   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 10:37:59.875025   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 10:37:59.875317   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 10:37:59.875706   286 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 10:37:59.876070   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876379   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876654   286 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 10:37:59.876945   286 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 10:37:59.877220   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.877590   286 net.cpp:200] Created Layer ctx_output6 (61)
I0511 10:37:59.877919   286 net.cpp:572] ctx_output6 <- pool9
I0511 10:37:59.878196   286 net.cpp:542] ctx_output6 -> ctx_output6
I0511 10:37:59.881597   286 net.cpp:260] Setting up ctx_output6
I0511 10:37:59.885067   286 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 16 256 1 2 (8192)
I0511 10:37:59.885434   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 10:37:59.885772   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.886109   286 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 10:37:59.886387   286 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 10:37:59.886701   286 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 10:37:59.886992   286 net.cpp:260] Setting up ctx_output6/relu
I0511 10:37:59.887290   286 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 16 256 1 2 (8192)
I0511 10:37:59.887578   286 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 10:37:59.887873   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.888156   286 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 10:37:59.888442   286 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 10:37:59.888722   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 10:37:59.889024   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 10:37:59.889334   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 10:37:59.889760   286 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 10:37:59.890172   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.890452   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.890740   286 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 10:37:59.891036   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.891322   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.891654   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 10:37:59.891928   286 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 10:37:59.892212   286 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 10:37:59.893016   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 10:37:59.893841   286 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 16 16 40 96 (983040)
I0511 10:37:59.894181   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.894474   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.894781   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 10:37:59.895064   286 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 10:37:59.895386   286 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.901461   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.901991   286 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 16 40 96 16 (983040)
I0511 10:37:59.902295   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.902575   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.902873   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 10:37:59.903172   286 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 10:37:59.903443   286 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 10:37:59.907702   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 10:37:59.912034   286 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 16 61440 (983040)
I0511 10:37:59.912406   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.912678   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.912983   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 10:37:59.913252   286 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 10:37:59.913609   286 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 10:37:59.914451   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 10:37:59.914754   286 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 16 16 40 96 (983040)
I0511 10:37:59.915050   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.915329   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.915621   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 10:37:59.915894   286 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 10:37:59.916167   286 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.916594   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.916891   286 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 16 40 96 16 (983040)
I0511 10:37:59.917168   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.917469   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.917778   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 10:37:59.918076   286 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 10:37:59.918344   286 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 10:37:59.922487   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 10:37:59.929324   286 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 16 61440 (983040)
I0511 10:37:59.929725   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.930029   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.930330   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 10:37:59.930595   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 10:37:59.930864   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 10:37:59.931128   286 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 10:37:59.931461   286 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 10:37:59.931733   286 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 10:37:59.932013   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.932278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.932559   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 10:37:59.932828   286 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 10:37:59.933104   286 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 10:37:59.934026   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 10:37:59.934317   286 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 16 24 10 24 (92160)
I0511 10:37:59.934614   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.934883   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.935155   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 10:37:59.935422   286 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 10:37:59.935698   286 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.936118   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.936398   286 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 16 10 24 24 (92160)
I0511 10:37:59.936692   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.936959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.937227   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 10:37:59.937526   286 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 10:37:59.937825   286 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 10:37:59.939034   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 10:37:59.939340   286 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 16 5760 (92160)
I0511 10:37:59.939623   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.939899   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.940181   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 10:37:59.940449   286 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 10:37:59.940718   286 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 10:37:59.941567   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 10:37:59.957307   286 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 16 24 10 24 (92160)
I0511 10:37:59.957728   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.958016   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.958307   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 10:37:59.958581   286 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 10:37:59.958848   286 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.959281   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.959558   286 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 16 10 24 24 (92160)
I0511 10:37:59.959837   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.960108   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.960374   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 10:37:59.960640   286 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 10:37:59.960913   286 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 10:37:59.961323   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 10:37:59.961643   286 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 16 5760 (92160)
I0511 10:37:59.961948   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.962214   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.962496   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 10:37:59.962762   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 10:37:59.963029   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 10:37:59.963297   286 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 10:37:59.963649   286 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 10:37:59.963917   286 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 10:37:59.964195   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.964466   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.964759   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 10:37:59.965018   286 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 10:37:59.965302   286 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 10:37:59.966109   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 10:37:59.966403   286 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 16 24 5 12 (23040)
I0511 10:37:59.966687   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.966953   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.967223   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 10:37:59.967491   286 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 10:37:59.967767   286 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.968178   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.968446   286 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 16 5 12 24 (23040)
I0511 10:37:59.968724   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.968992   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.969266   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 10:37:59.969578   286 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 10:37:59.969880   286 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 10:37:59.970299   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 10:37:59.970567   286 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 16 1440 (23040)
I0511 10:37:59.970839   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.971101   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.971374   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 10:37:59.971650   286 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 10:37:59.971922   286 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 10:37:59.972685   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 10:37:59.972981   286 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 16 24 5 12 (23040)
I0511 10:37:59.973268   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.973546   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.973821   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 10:37:59.974090   286 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 10:37:59.974371   286 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.974797   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.975065   286 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 16 5 12 24 (23040)
I0511 10:37:59.975337   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.975606   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.975886   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 10:37:59.976148   286 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 10:37:59.976416   286 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 10:37:59.976792   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 10:37:59.977082   286 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 16 1440 (23040)
I0511 10:37:59.977385   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.977677   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.977982   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 10:37:59.978278   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 10:37:59.978580   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 10:37:59.978849   286 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 10:37:59.979177   286 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 10:37:59.979439   286 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 10:37:59.979705   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.979969   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.980254   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 10:37:59.980523   286 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 10:37:59.980790   286 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 10:37:59.981556   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 10:37:59.981854   286 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 16 24 3 6 (6912)
I0511 10:37:59.982136   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.982411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.982668   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 10:37:59.982913   286 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 10:37:59.983155   286 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.983547   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.983800   286 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 16 3 6 24 (6912)
I0511 10:37:59.984050   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.984292   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.984539   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 10:37:59.984788   286 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 10:37:59.985031   286 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 10:37:59.985378   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 10:37:59.985666   286 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 16 432 (6912)
I0511 10:37:59.985949   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 10:37:59.986232   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.986521   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 10:37:59.986765   286 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 10:37:59.987013   286 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 10:37:59.987733   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 10:37:59.988004   286 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 16 24 3 6 (6912)
I0511 10:37:59.988271   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 10:37:59.988525   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.988788   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 10:37:59.989037   286 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 10:37:59.989286   286 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.989693   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.989938   286 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 16 3 6 24 (6912)
I0511 10:37:59.990186   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:37:59.990432   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.990676   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 10:37:59.990914   286 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 10:37:59.991154   286 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 10:37:59.991489   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 10:37:59.991739   286 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 16 432 (6912)
I0511 10:37:59.991981   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:37:59.992216   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.992476   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 10:37:59.992748   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 10:37:59.993005   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 10:37:59.993259   286 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 10:37:59.993592   286 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 10:37:59.993876   286 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 10:37:59.994168   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 10:37:59.994451   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.994733   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 10:37:59.994989   286 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 10:37:59.995249   286 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 10:37:59.995911   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 10:37:59.996191   286 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 16 16 2 3 (1536)
I0511 10:37:59.996464   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 10:37:59.996731   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.996990   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 10:37:59.997242   286 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 10:37:59.997510   286 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.997900   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.998159   286 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 16 2 3 16 (1536)
I0511 10:37:59.998421   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:37:59.998688   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:37:59.998946   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 10:37:59.999204   286 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 10:37:59.999472   286 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 10:37:59.999819   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 10:38:00.000079   286 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 16 96 (1536)
I0511 10:38:00.000346   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.000604   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.000869   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 10:38:00.001127   286 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:00.001404   286 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 10:38:00.002177   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 10:38:00.002485   286 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 16 16 2 3 (1536)
I0511 10:38:00.002768   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.003026   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.003296   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 10:38:00.003549   286 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 10:38:00.003804   286 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.004204   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.004463   286 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 16 2 3 16 (1536)
I0511 10:38:00.004722   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:00.004974   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.005235   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 10:38:00.005504   286 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 10:38:00.005764   286 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.006109   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.006381   286 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 16 96 (1536)
I0511 10:38:00.006639   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:00.006891   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.007151   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 10:38:00.007412   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:00.007671   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 10:38:00.007928   286 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 10:38:00.008208   286 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 10:38:00.008472   286 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 10:38:00.008744   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 10:38:00.008997   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.009260   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 10:38:00.009550   286 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:00.009846   286 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 10:38:00.010608   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 10:38:00.010890   286 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 16 16 1 2 (512)
I0511 10:38:00.011173   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:00.011438   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.011713   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 10:38:00.011984   286 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 10:38:00.012233   286 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.012606   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.012878   286 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 16 1 2 16 (512)
I0511 10:38:00.013146   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:00.013414   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.013679   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 10:38:00.013931   286 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 10:38:00.014197   286 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.014521   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.014786   286 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 16 32 (512)
I0511 10:38:00.015044   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.015311   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.015579   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 10:38:00.015839   286 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:00.016088   286 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 10:38:00.016752   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 10:38:00.017030   286 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 16 16 1 2 (512)
I0511 10:38:00.017315   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.017604   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.017906   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 10:38:00.018198   286 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 10:38:00.018492   286 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.018899   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.019171   286 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 16 1 2 16 (512)
I0511 10:38:00.019436   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:00.019692   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.019964   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 10:38:00.020226   286 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 10:38:00.020473   286 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.020792   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.021062   286 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 16 32 (512)
I0511 10:38:00.021327   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:00.021601   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.021878   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 10:38:00.022174   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:00.022459   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 10:38:00.022728   286 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 10:38:00.023007   286 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 10:38:00.023267   286 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 10:38:00.023528   286 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 10:38:00.023793   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.024053   286 net.cpp:200] Created Layer mbox_loc (106)
I0511 10:38:00.024310   286 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.024557   286 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 10:38:00.024827   286 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 10:38:00.025084   286 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 10:38:00.025331   286 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 10:38:00.025593   286 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 10:38:00.025854   286 net.cpp:542] mbox_loc -> mbox_loc
I0511 10:38:00.026161   286 net.cpp:260] Setting up mbox_loc
I0511 10:38:00.026427   286 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 16 69200 (1107200)
I0511 10:38:00.026690   286 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 10:38:00.026950   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.027223   286 net.cpp:200] Created Layer mbox_conf (107)
I0511 10:38:00.027482   286 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 10:38:00.027734   286 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 10:38:00.027990   286 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 10:38:00.028264   286 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 10:38:00.028525   286 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 10:38:00.028774   286 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 10:38:00.029032   286 net.cpp:542] mbox_conf -> mbox_conf
I0511 10:38:00.029369   286 net.cpp:260] Setting up mbox_conf
I0511 10:38:00.029661   286 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 16 69200 (1107200)
I0511 10:38:00.029956   286 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 10:38:00.030241   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.030530   286 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 10:38:00.030788   286 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 10:38:00.031039   286 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 10:38:00.031308   286 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 10:38:00.031569   286 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 10:38:00.031816   286 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 10:38:00.032065   286 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 10:38:00.032338   286 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 10:38:00.032634   286 net.cpp:260] Setting up mbox_priorbox
I0511 10:38:00.032892   286 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 10:38:00.033147   286 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 10:38:00.033418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.033694   286 net.cpp:200] Created Layer mbox_loss (109)
I0511 10:38:00.033951   286 net.cpp:572] mbox_loss <- mbox_loc
I0511 10:38:00.034198   286 net.cpp:572] mbox_loss <- mbox_conf
I0511 10:38:00.034459   286 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 10:38:00.034723   286 net.cpp:572] mbox_loss <- label
I0511 10:38:00.034971   286 net.cpp:542] mbox_loss -> mbox_loss
I0511 10:38:00.035310   286 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 10:38:00.035596   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.036000   286 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 10:38:00.036267   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.036676   286 net.cpp:260] Setting up mbox_loss
I0511 10:38:00.036952   286 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 10:38:00.037214   286 net.cpp:271]     with loss weight 1
I0511 10:38:00.037533   286 net.cpp:336] mbox_loss needs backward computation.
I0511 10:38:00.037822   286 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 10:38:00.038110   286 net.cpp:336] mbox_conf needs backward computation.
I0511 10:38:00.038395   286 net.cpp:336] mbox_loc needs backward computation.
I0511 10:38:00.038672   286 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.038923   286 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.039167   286 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.039422   286 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 10:38:00.039683   286 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.039942   286 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.040195   286 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 10:38:00.040462   286 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.040725   286 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.040988   286 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.041234   286 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 10:38:00.041489   286 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.041738   286 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.042002   286 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 10:38:00.042248   286 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.042497   286 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.042742   286 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.043005   286 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 10:38:00.043264   286 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.043509   286 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.043752   286 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 10:38:00.044003   286 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.044267   286 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.044531   286 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.044780   286 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 10:38:00.045027   286 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.045285   286 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.045579   286 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 10:38:00.045876   286 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.046154   286 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.046449   286 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.046692   286 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 10:38:00.046941   286 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.047171   286 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.047397   286 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 10:38:00.047756   286 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 10:38:00.047983   286 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 10:38:00.048216   286 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 10:38:00.048446   286 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 10:38:00.048677   286 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 10:38:00.048908   286 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 10:38:00.049108   286 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 10:38:00.049237   286 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 10:38:00.049373   286 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 10:38:00.049424   286 net.cpp:336] ctx_output6 needs backward computation.
I0511 10:38:00.049638   286 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 10:38:00.049688   286 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 10:38:00.049913   286 net.cpp:336] ctx_output5 needs backward computation.
I0511 10:38:00.050032   286 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 10:38:00.050163   286 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 10:38:00.050212   286 net.cpp:336] ctx_output4 needs backward computation.
I0511 10:38:00.050420   286 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 10:38:00.050544   286 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 10:38:00.050669   286 net.cpp:336] ctx_output3 needs backward computation.
I0511 10:38:00.050806   286 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 10:38:00.050942   286 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 10:38:00.051070   286 net.cpp:336] ctx_output2 needs backward computation.
I0511 10:38:00.051198   286 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 10:38:00.051246   286 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 10:38:00.051457   286 net.cpp:336] ctx_output1 needs backward computation.
I0511 10:38:00.051587   286 net.cpp:336] pool9 needs backward computation.
I0511 10:38:00.051717   286 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 10:38:00.051765   286 net.cpp:336] pool8 needs backward computation.
I0511 10:38:00.051975   286 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 10:38:00.052104   286 net.cpp:336] pool7 needs backward computation.
I0511 10:38:00.052235   286 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 10:38:00.052366   286 net.cpp:336] pool6 needs backward computation.
I0511 10:38:00.052495   286 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 10:38:00.052625   286 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 10:38:00.052757   286 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 10:38:00.052886   286 net.cpp:336] res5a_branch2b needs backward computation.
I0511 10:38:00.053017   286 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 10:38:00.053148   286 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 10:38:00.053280   286 net.cpp:336] res5a_branch2a needs backward computation.
I0511 10:38:00.053432   286 net.cpp:336] pool4 needs backward computation.
I0511 10:38:00.053573   286 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 10:38:00.053717   286 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 10:38:00.053866   286 net.cpp:336] res4a_branch2b needs backward computation.
I0511 10:38:00.054002   286 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 10:38:00.054141   286 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 10:38:00.054276   286 net.cpp:336] res4a_branch2a needs backward computation.
I0511 10:38:00.054399   286 net.cpp:336] pool3 needs backward computation.
I0511 10:38:00.054523   286 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 10:38:00.054649   286 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 10:38:00.054774   286 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 10:38:00.054823   286 net.cpp:336] res3a_branch2b needs backward computation.
I0511 10:38:00.055017   286 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 10:38:00.055140   286 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 10:38:00.055260   286 net.cpp:336] res3a_branch2a needs backward computation.
I0511 10:38:00.055380   286 net.cpp:336] pool2 needs backward computation.
I0511 10:38:00.055502   286 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 10:38:00.055552   286 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 10:38:00.055749   286 net.cpp:336] res2a_branch2b needs backward computation.
I0511 10:38:00.055873   286 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 10:38:00.056002   286 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 10:38:00.056123   286 net.cpp:336] res2a_branch2a needs backward computation.
I0511 10:38:00.056244   286 net.cpp:336] pool1 needs backward computation.
I0511 10:38:00.056295   286 net.cpp:336] conv1b/relu needs backward computation.
I0511 10:38:00.056496   286 net.cpp:336] conv1b/bn needs backward computation.
I0511 10:38:00.056547   286 net.cpp:336] conv1b needs backward computation.
I0511 10:38:00.056748   286 net.cpp:336] conv1a/relu needs backward computation.
I0511 10:38:00.056874   286 net.cpp:336] conv1a/bn needs backward computation.
I0511 10:38:00.056924   286 net.cpp:336] conv1a needs backward computation.
I0511 10:38:00.057122   286 net.cpp:338] data/bias does not need backward computation.
I0511 10:38:00.057248   286 net.cpp:338] data_data_0_split does not need backward computation.
I0511 10:38:00.057404   286 net.cpp:338] data does not need backward computation.
I0511 10:38:00.057541   286 net.cpp:380] This network produces output mbox_loss
I0511 10:38:00.057866   286 net.cpp:403] Top memory (TRAIN) required for data: 2411201928 diff: 2411201928
I0511 10:38:00.057905   286 net.cpp:406] Bottom memory (TRAIN) required for data: 2411201920 diff: 2411201920
I0511 10:38:00.058104   286 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 1043431424 diff: 1043431424
I0511 10:38:00.058224   286 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 10:38:00.058344   286 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 10:38:00.058463   286 net.cpp:421] Network initialization done.
I0511 10:38:00.060263   286 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/test.prototxt
I0511 10:38:00.073693   286 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 10:38:00.101864   286 net.cpp:110] Using FLOAT as default forward math type
I0511 10:38:00.139780   286 net.cpp:116] Using FLOAT as default backward math type
I0511 10:38:00.140223   286 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 10:38:00.140452   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.140709   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.141223   286 net.cpp:200] Created Layer data (0)
I0511 10:38:00.142010   286 net.cpp:542] data -> data
I0511 10:38:00.142218   286 net.cpp:542] data -> label
I0511 10:38:00.142448   286 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 10:38:00.142716   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.160323   329 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 10:38:00.164703   286 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 10:38:00.164973   286 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 10:38:00.165200   286 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 10:38:00.165462   286 net.cpp:260] Setting up data
I0511 10:38:00.165611   286 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 10:38:00.165777   286 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 10:38:00.165928   286 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 10:38:00.166074   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.166208   286 net.cpp:200] Created Layer data_data_0_split (1)
I0511 10:38:00.166332   286 net.cpp:572] data_data_0_split <- data
I0511 10:38:00.166435   286 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 10:38:00.166496   286 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 10:38:00.166589   286 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 10:38:00.166683   286 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 10:38:00.166741   286 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 10:38:00.166834   286 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 10:38:00.166893   286 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 10:38:00.167165   286 net.cpp:260] Setting up data_data_0_split
I0511 10:38:00.167212   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167279   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167380   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167474   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167533   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167624   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167719   286 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 10:38:00.167779   286 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 10:38:00.167866   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.167945   286 net.cpp:200] Created Layer data/bias (2)
I0511 10:38:00.168018   286 net.cpp:572] data/bias <- data_data_0_split_0
I0511 10:38:00.168081   286 net.cpp:542] data/bias -> data/bias
I0511 10:38:00.168406   286 net.cpp:260] Setting up data/bias
I0511 10:38:00.168479   286 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 10:38:00.168551   286 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 10:38:00.168646   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.168756   286 net.cpp:200] Created Layer conv1a (3)
I0511 10:38:00.168850   286 net.cpp:572] conv1a <- data/bias
I0511 10:38:00.168910   286 net.cpp:542] conv1a -> conv1a
I0511 10:38:00.173552   330 data_layer.cpp:105] (0) Parser threads: 1
I0511 10:38:00.173635   330 data_layer.cpp:107] (0) Transformer threads: 1
I0511 10:38:00.173625   286 net.cpp:260] Setting up conv1a
I0511 10:38:00.236510   286 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 10:38:00.236716   286 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 10:38:00.236826   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.236943   286 net.cpp:200] Created Layer conv1a/bn (4)
I0511 10:38:00.237097   286 net.cpp:572] conv1a/bn <- conv1a
I0511 10:38:00.237200   286 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 10:38:00.284126   286 net.cpp:260] Setting up conv1a/bn
I0511 10:38:00.284152   286 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 10:38:00.284184   286 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 10:38:00.284191   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284234   286 net.cpp:200] Created Layer conv1a/relu (5)
I0511 10:38:00.284241   286 net.cpp:572] conv1a/relu <- conv1a
I0511 10:38:00.284250   286 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 10:38:00.284260   286 net.cpp:260] Setting up conv1a/relu
I0511 10:38:00.284265   286 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 10:38:00.284273   286 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 10:38:00.284278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284296   286 net.cpp:200] Created Layer conv1b (6)
I0511 10:38:00.284301   286 net.cpp:572] conv1b <- conv1a
I0511 10:38:00.284307   286 net.cpp:542] conv1b -> conv1b
I0511 10:38:00.284718   286 net.cpp:260] Setting up conv1b
I0511 10:38:00.284729   286 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 10:38:00.284746   286 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 10:38:00.284752   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.284762   286 net.cpp:200] Created Layer conv1b/bn (7)
I0511 10:38:00.284768   286 net.cpp:572] conv1b/bn <- conv1b
I0511 10:38:00.284775   286 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 10:38:00.285197   286 net.cpp:260] Setting up conv1b/bn
I0511 10:38:00.285218   286 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 10:38:00.285272   286 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 10:38:00.285296   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285320   286 net.cpp:200] Created Layer conv1b/relu (8)
I0511 10:38:00.285337   286 net.cpp:572] conv1b/relu <- conv1b
I0511 10:38:00.285354   286 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 10:38:00.285375   286 net.cpp:260] Setting up conv1b/relu
I0511 10:38:00.285389   286 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 10:38:00.285413   286 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 10:38:00.285429   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285454   286 net.cpp:200] Created Layer pool1 (9)
I0511 10:38:00.285467   286 net.cpp:572] pool1 <- conv1b
I0511 10:38:00.285485   286 net.cpp:542] pool1 -> pool1
I0511 10:38:00.285624   286 net.cpp:260] Setting up pool1
I0511 10:38:00.285638   286 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 10:38:00.285660   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 10:38:00.285676   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.285707   286 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 10:38:00.285723   286 net.cpp:572] res2a_branch2a <- pool1
I0511 10:38:00.285739   286 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 10:38:00.287034   286 net.cpp:260] Setting up res2a_branch2a
I0511 10:38:00.287050   286 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 10:38:00.287086   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.287101   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.287124   286 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 10:38:00.287138   286 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 10:38:00.287155   286 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 10:38:00.288167   286 net.cpp:260] Setting up res2a_branch2a/bn
I0511 10:38:00.288182   286 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 10:38:00.288225   286 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.288278   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.288298   286 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 10:38:00.288312   286 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 10:38:00.288329   286 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 10:38:00.288350   286 net.cpp:260] Setting up res2a_branch2a/relu
I0511 10:38:00.288363   286 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 10:38:00.288388   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 10:38:00.288401   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.288431   286 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 10:38:00.288446   286 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 10:38:00.288462   286 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 10:38:00.292881   286 net.cpp:260] Setting up res2a_branch2b
I0511 10:38:00.293035   286 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 10:38:00.293248   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.293433   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.293599   286 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 10:38:00.293740   286 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 10:38:00.293881   286 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 10:38:00.294829   286 net.cpp:260] Setting up res2a_branch2b/bn
I0511 10:38:00.294916   286 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 10:38:00.295183   286 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.295375   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.295568   286 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 10:38:00.295761   286 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 10:38:00.295887   286 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 10:38:00.296032   286 net.cpp:260] Setting up res2a_branch2b/relu
I0511 10:38:00.296162   286 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 10:38:00.296303   286 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 10:38:00.296440   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.296586   286 net.cpp:200] Created Layer pool2 (16)
I0511 10:38:00.296721   286 net.cpp:572] pool2 <- res2a_branch2b
I0511 10:38:00.296861   286 net.cpp:542] pool2 -> pool2
I0511 10:38:00.297103   286 net.cpp:260] Setting up pool2
I0511 10:38:00.297224   286 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 10:38:00.297375   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 10:38:00.297508   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.297652   286 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 10:38:00.297777   286 net.cpp:572] res3a_branch2a <- pool2
I0511 10:38:00.297909   286 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 10:38:00.300310   286 net.cpp:260] Setting up res3a_branch2a
I0511 10:38:00.300524   286 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 10:38:00.300673   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.300793   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.300966   286 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 10:38:00.301105   286 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 10:38:00.301194   286 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 10:38:00.301964   286 net.cpp:260] Setting up res3a_branch2a/bn
I0511 10:38:00.305519   286 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 10:38:00.305819   286 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.306018   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.306151   286 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 10:38:00.306288   286 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 10:38:00.306413   286 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 10:38:00.306550   286 net.cpp:260] Setting up res3a_branch2a/relu
I0511 10:38:00.306694   286 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 10:38:00.306833   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 10:38:00.306962   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.307097   286 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 10:38:00.307262   286 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 10:38:00.307386   286 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 10:38:00.308964   286 net.cpp:260] Setting up res3a_branch2b
I0511 10:38:00.313462   286 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 10:38:00.313647   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.313791   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.313925   286 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 10:38:00.314065   286 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 10:38:00.314185   286 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 10:38:00.321985   286 net.cpp:260] Setting up res3a_branch2b/bn
I0511 10:38:00.322772   286 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 10:38:00.323020   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.323264   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.323441   286 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 10:38:00.323627   286 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 10:38:00.323793   286 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 10:38:00.323976   286 net.cpp:260] Setting up res3a_branch2b/relu
I0511 10:38:00.324156   286 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 10:38:00.324348   286 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 10:38:00.324515   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.324681   286 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 10:38:00.324870   286 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 10:38:00.325032   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:38:00.325199   286 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:38:00.325417   286 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 10:38:00.325734   286 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 10:38:00.325875   286 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 10:38:00.326009   286 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 10:38:00.326130   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.326264   286 net.cpp:200] Created Layer pool3 (24)
I0511 10:38:00.326409   286 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 10:38:00.326514   286 net.cpp:542] pool3 -> pool3
I0511 10:38:00.326699   286 net.cpp:260] Setting up pool3
I0511 10:38:00.327030   286 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 10:38:00.327154   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 10:38:00.327256   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.327366   286 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 10:38:00.327491   286 net.cpp:572] res4a_branch2a <- pool3
I0511 10:38:00.327592   286 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 10:38:00.359237   286 net.cpp:260] Setting up res4a_branch2a
I0511 10:38:00.372788   286 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 10:38:00.373016   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.373152   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.373265   286 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 10:38:00.373409   286 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 10:38:00.373504   286 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 10:38:00.374991   286 net.cpp:260] Setting up res4a_branch2a/bn
I0511 10:38:00.376276   286 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 10:38:00.376410   286 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.376525   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.376621   286 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 10:38:00.376788   286 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 10:38:00.376873   286 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 10:38:00.376981   286 net.cpp:260] Setting up res4a_branch2a/relu
I0511 10:38:00.377132   286 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 10:38:00.377230   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 10:38:00.377382   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.377481   286 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 10:38:00.377676   286 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 10:38:00.377753   286 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 10:38:00.381403   286 net.cpp:260] Setting up res4a_branch2b
I0511 10:38:00.409235   286 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 10:38:00.409476   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.409621   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.409763   286 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 10:38:00.409902   286 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 10:38:00.410006   286 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 10:38:00.410929   286 net.cpp:260] Setting up res4a_branch2b/bn
I0511 10:38:00.413265   286 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 10:38:00.413422   286 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.413566   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.413708   286 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 10:38:00.413805   286 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 10:38:00.413940   286 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 10:38:00.414050   286 net.cpp:260] Setting up res4a_branch2b/relu
I0511 10:38:00.414196   286 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 10:38:00.414384   286 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 10:38:00.414489   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.414613   286 net.cpp:200] Created Layer pool4 (31)
I0511 10:38:00.414752   286 net.cpp:572] pool4 <- res4a_branch2b
I0511 10:38:00.414855   286 net.cpp:542] pool4 -> pool4
I0511 10:38:00.415100   286 net.cpp:260] Setting up pool4
I0511 10:38:00.415486   286 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 10:38:00.415629   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 10:38:00.415730   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.415884   286 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 10:38:00.416031   286 net.cpp:572] res5a_branch2a <- pool4
I0511 10:38:00.416136   286 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 10:38:00.563432   286 net.cpp:260] Setting up res5a_branch2a
I0511 10:38:00.570969   286 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 10:38:00.571703   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 10:38:00.571997   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.572183   286 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 10:38:00.572372   286 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 10:38:00.572612   286 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 10:38:00.573835   286 net.cpp:260] Setting up res5a_branch2a/bn
I0511 10:38:00.576297   286 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 10:38:00.576719   286 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 10:38:00.577056   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.577350   286 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 10:38:00.577684   286 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 10:38:00.577986   286 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 10:38:00.578306   286 net.cpp:260] Setting up res5a_branch2a/relu
I0511 10:38:00.578636   286 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 10:38:00.585314   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 10:38:00.585705   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.586031   286 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 10:38:00.586378   286 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 10:38:00.586674   286 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 10:38:00.703346   286 net.cpp:260] Setting up res5a_branch2b
I0511 10:38:00.716594   286 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 10:38:00.717247   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 10:38:00.717890   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.718536   286 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 10:38:00.719210   286 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 10:38:00.719818   286 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 10:38:00.720806   286 net.cpp:260] Setting up res5a_branch2b/bn
I0511 10:38:00.723659   286 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 10:38:00.724120   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 10:38:00.724508   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.724864   286 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 10:38:00.725219   286 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 10:38:00.725749   286 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 10:38:00.726313   286 net.cpp:260] Setting up res5a_branch2b/relu
I0511 10:38:00.726874   286 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 10:38:00.727447   286 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 10:38:00.727969   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.728497   286 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 10:38:00.729038   286 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 10:38:00.729553   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:38:00.730093   286 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:38:00.730782   286 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 10:38:00.732031   286 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 10:38:00.732620   286 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 10:38:00.733178   286 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 10:38:00.733645   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.733929   286 net.cpp:200] Created Layer pool6 (39)
I0511 10:38:00.734226   286 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 10:38:00.734499   286 net.cpp:542] pool6 -> pool6
I0511 10:38:00.734931   286 net.cpp:260] Setting up pool6
I0511 10:38:00.736011   286 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 10:38:00.736325   286 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 10:38:00.736589   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.736857   286 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 10:38:00.737136   286 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 10:38:00.737391   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 10:38:00.737692   286 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 10:38:00.738068   286 net.cpp:260] Setting up pool6_pool6_0_split
I0511 10:38:00.738816   286 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 10:38:00.739123   286 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 10:38:00.739426   286 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 10:38:00.739682   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.739948   286 net.cpp:200] Created Layer pool7 (41)
I0511 10:38:00.740243   286 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 10:38:00.740463   286 net.cpp:542] pool7 -> pool7
I0511 10:38:00.740705   286 net.cpp:260] Setting up pool7
I0511 10:38:00.741253   286 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 10:38:00.741466   286 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 10:38:00.741647   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.741816   286 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 10:38:00.742002   286 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 10:38:00.742166   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 10:38:00.742347   286 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 10:38:00.742597   286 net.cpp:260] Setting up pool7_pool7_0_split
I0511 10:38:00.743060   286 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 10:38:00.743328   286 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 10:38:00.743525   286 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 10:38:00.743698   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.743867   286 net.cpp:200] Created Layer pool8 (43)
I0511 10:38:00.744055   286 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 10:38:00.744220   286 net.cpp:542] pool8 -> pool8
I0511 10:38:00.744480   286 net.cpp:260] Setting up pool8
I0511 10:38:00.745043   286 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 10:38:00.745244   286 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 10:38:00.745425   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.745620   286 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 10:38:00.745817   286 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 10:38:00.745999   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 10:38:00.746189   286 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 10:38:00.746438   286 net.cpp:260] Setting up pool8_pool8_0_split
I0511 10:38:00.746917   286 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 10:38:00.747118   286 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 10:38:00.747320   286 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 10:38:00.747503   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.747689   286 net.cpp:200] Created Layer pool9 (45)
I0511 10:38:00.747886   286 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 10:38:00.748059   286 net.cpp:542] pool9 -> pool9
I0511 10:38:00.748311   286 net.cpp:260] Setting up pool9
I0511 10:38:00.748843   286 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 10:38:00.749049   286 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 10:38:00.749225   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.749434   286 net.cpp:200] Created Layer ctx_output1 (46)
I0511 10:38:00.749675   286 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 10:38:00.749852   286 net.cpp:542] ctx_output1 -> ctx_output1
I0511 10:38:00.751111   286 net.cpp:260] Setting up ctx_output1
I0511 10:38:00.756677   286 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 10:38:00.757061   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 10:38:00.757411   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.757840   286 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 10:38:00.758211   286 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 10:38:00.758512   286 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 10:38:00.758862   286 net.cpp:260] Setting up ctx_output1/relu
I0511 10:38:00.759196   286 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 10:38:00.759539   286 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 10:38:00.759840   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.760138   286 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 10:38:00.760450   286 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 10:38:00.760742   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 10:38:00.761060   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 10:38:00.761397   286 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 10:38:00.761997   286 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 10:38:00.763012   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.763362   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.763697   286 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 10:38:00.764024   286 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 10:38:00.764314   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.764631   286 net.cpp:200] Created Layer ctx_output2 (49)
I0511 10:38:00.765009   286 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 10:38:00.765303   286 net.cpp:542] ctx_output2 -> ctx_output2
I0511 10:38:00.785568   286 net.cpp:260] Setting up ctx_output2
I0511 10:38:00.796783   286 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 10:38:00.797606   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 10:38:00.798049   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.798391   286 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 10:38:00.798707   286 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 10:38:00.799026   286 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 10:38:00.799343   286 net.cpp:260] Setting up ctx_output2/relu
I0511 10:38:00.799687   286 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 10:38:00.799973   286 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 10:38:00.800173   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.800361   286 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 10:38:00.800559   286 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 10:38:00.800806   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 10:38:00.801097   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 10:38:00.801417   286 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 10:38:00.801928   286 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 10:38:00.803295   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803515   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803692   286 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 10:38:00.803872   286 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 10:38:00.803959   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.804122   286 net.cpp:200] Created Layer ctx_output3 (52)
I0511 10:38:00.804332   286 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 10:38:00.804495   286 net.cpp:542] ctx_output3 -> ctx_output3
I0511 10:38:00.807998   286 net.cpp:260] Setting up ctx_output3
I0511 10:38:00.853966   286 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 10:38:00.854054   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 10:38:00.854094   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.854138   286 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 10:38:00.854307   286 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 10:38:00.854369   286 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 10:38:00.854499   286 net.cpp:260] Setting up ctx_output3/relu
I0511 10:38:00.854560   286 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 10:38:00.854614   286 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 10:38:00.856178   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.856326   286 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 10:38:00.856459   286 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 10:38:00.856592   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 10:38:00.856725   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 10:38:00.856856   286 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 10:38:00.857107   286 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 10:38:00.857239   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857378   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857513   286 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 10:38:00.857647   286 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 10:38:00.857782   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.857921   286 net.cpp:200] Created Layer ctx_output4 (55)
I0511 10:38:00.858053   286 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 10:38:00.858180   286 net.cpp:542] ctx_output4 -> ctx_output4
I0511 10:38:00.861441   286 net.cpp:260] Setting up ctx_output4
I0511 10:38:00.861624   286 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 10:38:00.861768   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 10:38:00.861899   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.862032   286 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 10:38:00.862161   286 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 10:38:00.862291   286 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 10:38:00.862423   286 net.cpp:260] Setting up ctx_output4/relu
I0511 10:38:00.862551   286 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 10:38:00.862684   286 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 10:38:00.862818   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.862957   286 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 10:38:00.863085   286 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 10:38:00.863217   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 10:38:00.863346   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 10:38:00.863478   286 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 10:38:00.863682   286 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 10:38:00.863811   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.863945   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.864074   286 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 10:38:00.864207   286 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 10:38:00.864341   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.864490   286 net.cpp:200] Created Layer ctx_output5 (58)
I0511 10:38:00.864634   286 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 10:38:00.864852   286 net.cpp:542] ctx_output5 -> ctx_output5
I0511 10:38:00.868456   286 net.cpp:260] Setting up ctx_output5
I0511 10:38:00.892488   286 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 10:38:00.893256   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 10:38:00.893589   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.893831   286 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 10:38:00.894083   286 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 10:38:00.894318   286 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 10:38:00.894584   286 net.cpp:260] Setting up ctx_output5/relu
I0511 10:38:00.894876   286 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 10:38:00.895146   286 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 10:38:00.895357   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.895589   286 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 10:38:00.895853   286 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 10:38:00.896073   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 10:38:00.896307   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:00.896549   286 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:00.896899   286 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 10:38:00.897919   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898227   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898476   286 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 10:38:00.898728   286 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 10:38:00.898949   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.899190   286 net.cpp:200] Created Layer ctx_output6 (61)
I0511 10:38:00.899886   286 net.cpp:572] ctx_output6 <- pool9
I0511 10:38:00.900143   286 net.cpp:542] ctx_output6 -> ctx_output6
I0511 10:38:00.904469   286 net.cpp:260] Setting up ctx_output6
I0511 10:38:00.914566   286 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 10:38:00.921326   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 10:38:00.921558   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.921710   286 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 10:38:00.921867   286 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 10:38:00.922020   286 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 10:38:00.922179   286 net.cpp:260] Setting up ctx_output6/relu
I0511 10:38:00.922334   286 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 10:38:00.922490   286 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 10:38:00.922634   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.922761   286 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 10:38:00.922881   286 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 10:38:00.923010   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:00.923146   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:00.923323   286 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:00.923676   286 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 10:38:00.923897   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924033   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924175   286 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 10:38:00.924319   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 10:38:00.924453   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.924603   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 10:38:00.924772   286 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 10:38:00.924907   286 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 10:38:00.925599   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 10:38:00.926632   286 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 10:38:00.926802   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:00.926968   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.927125   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 10:38:00.927274   286 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 10:38:00.927417   286 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.927764   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.928125   286 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 10:38:00.928275   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:00.928418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.928560   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 10:38:00.928707   286 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 10:38:00.928859   286 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.931531   286 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 10:38:00.995368   286 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 10:38:00.995499   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 10:38:00.995635   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.996258   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 10:38:00.996297   286 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 10:38:00.996336   286 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 10:38:00.996858   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 10:38:00.996901   286 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 10:38:00.996950   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:00.996986   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:00.997026   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 10:38:00.997061   286 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 10:38:01.001762   286 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003091   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003160   286 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 10:38:01.003233   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.003342   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.003415   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 10:38:01.003512   286 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 10:38:01.003571   286 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.007719   286 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.007805   286 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 10:38:01.007864   286 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.007908   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.007977   286 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 10:38:01.008116   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 10:38:01.008242   286 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 10:38:01.008378   286 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 10:38:01.008595   286 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 10:38:01.008671   286 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 10:38:01.008769   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.008944   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.009074   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 10:38:01.009162   286 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 10:38:01.009327   286 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 10:38:01.010057   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 10:38:01.010139   286 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 10:38:01.010243   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.010421   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.010550   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 10:38:01.010668   286 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 10:38:01.010792   286 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011106   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011178   286 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 10:38:01.011273   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.011447   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.011570   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 10:38:01.011689   286 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 10:38:01.011811   286 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.012079   286 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.012151   286 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 10:38:01.012308   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.012419   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.012580   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 10:38:01.012702   286 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 10:38:01.012825   286 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 10:38:01.013501   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 10:38:01.013582   286 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 10:38:01.013746   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.013869   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.013996   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 10:38:01.014117   286 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 10:38:01.014237   286 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.014554   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.014626   286 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 10:38:01.014721   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.014904   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.015027   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 10:38:01.015146   286 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 10:38:01.015269   286 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.015532   286 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.015604   286 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 10:38:01.015697   286 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.015875   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.016002   286 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 10:38:01.016121   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 10:38:01.016243   286 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 10:38:01.016364   286 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 10:38:01.016566   286 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 10:38:01.016638   286 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 10:38:01.016786   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.016906   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.017035   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 10:38:01.017155   286 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 10:38:01.017277   286 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 10:38:01.017979   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 10:38:01.018059   286 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 10:38:01.018162   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.018340   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.018467   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 10:38:01.018632   286 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 10:38:01.018764   286 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019129   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019202   286 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 10:38:01.019336   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.019464   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.019587   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 10:38:01.019748   286 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 10:38:01.019822   286 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.020148   286 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.020220   286 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 10:38:01.020474   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.020546   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.020682   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 10:38:01.020843   286 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 10:38:01.020972   286 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 10:38:01.021735   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 10:38:01.021816   286 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 10:38:01.022091   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.022186   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.022320   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 10:38:01.022481   286 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 10:38:01.022608   286 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023000   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023072   286 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 10:38:01.023289   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.023397   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.023525   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 10:38:01.023663   286 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 10:38:01.023833   286 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.024147   286 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.024217   286 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 10:38:01.024353   286 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.024487   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.024657   286 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 10:38:01.024780   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 10:38:01.024917   286 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 10:38:01.025084   286 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 10:38:01.025353   286 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 10:38:01.025436   286 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 10:38:01.033692   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.033830   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.034090   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 10:38:01.034365   286 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 10:38:01.034440   286 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 10:38:01.035321   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 10:38:01.036125   286 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 10:38:01.036221   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.036479   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.036602   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 10:38:01.036777   286 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 10:38:01.041563   286 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.041996   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.042387   286 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 10:38:01.042600   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.042681   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.042845   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 10:38:01.042994   286 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 10:38:01.043151   286 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.043577   286 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.043860   286 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 10:38:01.043954   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.044111   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.044279   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 10:38:01.044360   286 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 10:38:01.044531   286 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 10:38:01.045389   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 10:38:01.046135   286 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 10:38:01.046314   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.046407   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.046576   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 10:38:01.046659   286 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 10:38:01.046815   286 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.047209   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.047474   286 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 10:38:01.047552   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.047694   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.047881   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 10:38:01.048049   286 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 10:38:01.048224   286 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.048606   286 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.048838   286 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 10:38:01.048918   286 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.049057   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.049238   286 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 10:38:01.049414   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 10:38:01.049567   286 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 10:38:01.049634   286 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 10:38:01.049844   286 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 10:38:01.050012   286 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 10:38:01.050119   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.050230   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.050340   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 10:38:01.050597   286 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 10:38:01.050678   286 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 10:38:01.051476   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 10:38:01.052158   286 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 10:38:01.052243   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.052417   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.052587   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 10:38:01.052740   286 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 10:38:01.052808   286 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.053140   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.053472   286 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 10:38:01.053550   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.053763   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.053886   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 10:38:01.054059   286 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 10:38:01.054227   286 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.054481   286 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.054741   286 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 10:38:01.054818   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.054955   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.055100   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 10:38:01.055243   286 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 10:38:01.055456   286 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 10:38:01.056166   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 10:38:01.056797   286 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 10:38:01.056887   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.057072   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.057251   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 10:38:01.057420   286 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 10:38:01.057567   286 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.057929   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.058221   286 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 10:38:01.058292   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.058440   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.058503   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 10:38:01.058616   286 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 10:38:01.058717   286 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.058956   286 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.059193   286 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 10:38:01.059270   286 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.059418   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.059568   286 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 10:38:01.059666   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 10:38:01.059819   286 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 10:38:01.059964   286 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 10:38:01.060174   286 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 10:38:01.065316   286 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 10:38:01.065474   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 10:38:01.065591   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.065871   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 10:38:01.065935   286 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 10:38:01.066057   286 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 10:38:01.066812   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 10:38:01.067467   286 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 10:38:01.067553   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 10:38:01.067764   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.067901   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 10:38:01.068085   286 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 10:38:01.068341   286 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.068719   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.069054   286 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 10:38:01.069150   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 10:38:01.069320   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.069591   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 10:38:01.069686   286 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 10:38:01.069890   286 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.070278   286 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.070530   286 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 10:38:01.070632   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 10:38:01.070816   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.071074   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 10:38:01.071166   286 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 10:38:01.071343   286 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 10:38:01.072058   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 10:38:01.072712   286 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 10:38:01.072793   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 10:38:01.072934   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.073125   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 10:38:01.073278   286 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 10:38:01.073446   286 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.073894   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.074204   286 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 10:38:01.074295   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 10:38:01.074442   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.074676   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 10:38:01.074789   286 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 10:38:01.074926   286 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.075320   286 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.075564   286 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 10:38:01.075661   286 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 10:38:01.075822   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.075947   286 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 10:38:01.076059   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 10:38:01.076195   286 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 10:38:01.076347   286 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 10:38:01.076622   286 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 10:38:01.076746   286 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 10:38:01.076815   286 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 10:38:01.076876   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.076925   286 net.cpp:200] Created Layer mbox_loc (106)
I0511 10:38:01.076975   286 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 10:38:01.077023   286 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 10:38:01.077072   286 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 10:38:01.077121   286 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 10:38:01.077173   286 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 10:38:01.077239   286 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 10:38:01.077283   286 net.cpp:542] mbox_loc -> mbox_loc
I0511 10:38:01.077378   286 net.cpp:260] Setting up mbox_loc
I0511 10:38:01.077456   286 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 10:38:01.077512   286 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 10:38:01.077556   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.077603   286 net.cpp:200] Created Layer mbox_conf (107)
I0511 10:38:01.077649   286 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 10:38:01.077697   286 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 10:38:01.077746   286 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 10:38:01.077795   286 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 10:38:01.077842   286 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 10:38:01.077889   286 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 10:38:01.077935   286 net.cpp:542] mbox_conf -> mbox_conf
I0511 10:38:01.078016   286 net.cpp:260] Setting up mbox_conf
I0511 10:38:01.078094   286 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 10:38:01.078148   286 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 10:38:01.078197   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.078245   286 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 10:38:01.078292   286 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 10:38:01.078339   286 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 10:38:01.078388   286 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 10:38:01.078436   286 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 10:38:01.078483   286 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 10:38:01.078531   286 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 10:38:01.078577   286 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 10:38:01.078649   286 net.cpp:260] Setting up mbox_priorbox
I0511 10:38:01.078718   286 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 10:38:01.078769   286 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 10:38:01.078814   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.078869   286 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 10:38:01.078922   286 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 10:38:01.078969   286 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 10:38:01.079041   286 net.cpp:260] Setting up mbox_conf_reshape
I0511 10:38:01.079111   286 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 10:38:01.079159   286 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 10:38:01.079200   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.079250   286 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 10:38:01.079300   286 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 10:38:01.079342   286 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 10:38:01.079469   286 net.cpp:260] Setting up mbox_conf_softmax
I0511 10:38:01.079599   286 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 10:38:01.079653   286 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 10:38:01.079697   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.079743   286 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 10:38:01.079794   286 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 10:38:01.079839   286 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 10:38:01.082798   286 net.cpp:260] Setting up mbox_conf_flatten
I0511 10:38:01.086413   286 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 10:38:01.086676   286 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 10:38:01.086910   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.087154   286 net.cpp:200] Created Layer detection_out (112)
I0511 10:38:01.089323   286 net.cpp:572] detection_out <- mbox_loc
I0511 10:38:01.089581   286 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 10:38:01.089674   286 net.cpp:572] detection_out <- mbox_priorbox
I0511 10:38:01.090101   286 net.cpp:542] detection_out -> detection_out
I0511 10:38:01.090975   286 net.cpp:260] Setting up detection_out
I0511 10:38:01.091850   286 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 10:38:01.091948   286 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 10:38:01.092154   286 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 10:38:01.092577   286 net.cpp:200] Created Layer detection_eval (113)
I0511 10:38:01.092790   286 net.cpp:572] detection_eval <- detection_out
I0511 10:38:01.092880   286 net.cpp:572] detection_eval <- label
I0511 10:38:01.093076   286 net.cpp:542] detection_eval -> detection_eval
I0511 10:38:01.093842   286 net.cpp:260] Setting up detection_eval
I0511 10:38:01.094563   286 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 10:38:01.094794   286 net.cpp:338] detection_eval does not need backward computation.
I0511 10:38:01.094879   286 net.cpp:338] detection_out does not need backward computation.
I0511 10:38:01.095168   286 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 10:38:01.095257   286 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 10:38:01.095538   286 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 10:38:01.095626   286 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 10:38:01.095911   286 net.cpp:338] mbox_conf does not need backward computation.
I0511 10:38:01.096004   286 net.cpp:338] mbox_loc does not need backward computation.
I0511 10:38:01.096196   286 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.096505   286 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.096588   286 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.096873   286 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 10:38:01.096959   286 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.097246   286 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.097338   286 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 10:38:01.097625   286 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.097714   286 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.097999   286 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.098083   286 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 10:38:01.098351   286 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.098585   286 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.098680   286 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 10:38:01.098964   286 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.099064   286 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.099285   286 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.099573   286 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 10:38:01.099660   286 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.099933   286 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.100028   286 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 10:38:01.100324   286 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.100414   286 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.100698   286 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.100778   286 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 10:38:01.101047   286 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.101265   286 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.101362   286 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 10:38:01.101797   286 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.102041   286 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.102277   286 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.103260   286 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 10:38:01.103477   286 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.103550   286 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.103826   286 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 10:38:01.104034   286 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 10:38:01.104112   286 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 10:38:01.104434   286 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 10:38:01.104504   286 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 10:38:01.104782   286 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 10:38:01.104984   286 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 10:38:01.105187   286 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 10:38:01.105263   286 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 10:38:01.105618   286 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 10:38:01.105690   286 net.cpp:338] ctx_output6 does not need backward computation.
I0511 10:38:01.105962   286 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 10:38:01.106161   286 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 10:38:01.106359   286 net.cpp:338] ctx_output5 does not need backward computation.
I0511 10:38:01.106432   286 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 10:38:01.106757   286 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 10:38:01.106827   286 net.cpp:338] ctx_output4 does not need backward computation.
I0511 10:38:01.107106   286 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 10:38:01.107311   286 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 10:38:01.107515   286 net.cpp:338] ctx_output3 does not need backward computation.
I0511 10:38:01.107592   286 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 10:38:01.107831   286 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 10:38:01.108152   286 net.cpp:338] ctx_output2 does not need backward computation.
I0511 10:38:01.108224   286 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 10:38:01.108547   286 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 10:38:01.108616   286 net.cpp:338] ctx_output1 does not need backward computation.
I0511 10:38:01.108891   286 net.cpp:338] pool9 does not need backward computation.
I0511 10:38:01.109091   286 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 10:38:01.109165   286 net.cpp:338] pool8 does not need backward computation.
I0511 10:38:01.109483   286 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 10:38:01.109683   286 net.cpp:338] pool7 does not need backward computation.
I0511 10:38:01.109756   286 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 10:38:01.110021   286 net.cpp:338] pool6 does not need backward computation.
I0511 10:38:01.110220   286 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 10:38:01.110422   286 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 10:38:01.110493   286 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 10:38:01.110752   286 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 10:38:01.110944   286 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 10:38:01.111141   286 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 10:38:01.111212   286 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 10:38:01.111485   286 net.cpp:338] pool4 does not need backward computation.
I0511 10:38:01.111692   286 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 10:38:01.111779   286 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 10:38:01.112089   286 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 10:38:01.112166   286 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 10:38:01.112476   286 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 10:38:01.112568   286 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 10:38:01.112799   286 net.cpp:338] pool3 does not need backward computation.
I0511 10:38:01.113103   286 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 10:38:01.113327   286 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 10:38:01.113412   286 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 10:38:01.121309   286 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 10:38:01.121418   286 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 10:38:01.121649   286 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 10:38:01.121954   286 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 10:38:01.122041   286 net.cpp:338] pool2 does not need backward computation.
I0511 10:38:01.122339   286 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 10:38:01.122416   286 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 10:38:01.122604   286 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 10:38:01.122911   286 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 10:38:01.123003   286 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 10:38:01.123298   286 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 10:38:01.123389   286 net.cpp:338] pool1 does not need backward computation.
I0511 10:38:01.123682   286 net.cpp:338] conv1b/relu does not need backward computation.
I0511 10:38:01.123781   286 net.cpp:338] conv1b/bn does not need backward computation.
I0511 10:38:01.124011   286 net.cpp:338] conv1b does not need backward computation.
I0511 10:38:01.124308   286 net.cpp:338] conv1a/relu does not need backward computation.
I0511 10:38:01.124399   286 net.cpp:338] conv1a/bn does not need backward computation.
I0511 10:38:01.124693   286 net.cpp:338] conv1a does not need backward computation.
I0511 10:38:01.124775   286 net.cpp:338] data/bias does not need backward computation.
I0511 10:38:01.125072   286 net.cpp:338] data_data_0_split does not need backward computation.
I0511 10:38:01.125170   286 net.cpp:338] data does not need backward computation.
I0511 10:38:01.125361   286 net.cpp:380] This network produces output detection_eval
I0511 10:38:01.125874   286 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 10:38:01.126257   286 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 10:38:01.126338   286 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 10:38:01.126636   286 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 10:38:01.126725   286 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 10:38:01.127008   286 net.cpp:421] Network initialization done.
I0511 10:38:01.127588   286 solver.cpp:55] Solver scaffolding done.
I0511 10:38:01.134549   286 caffe.cpp:158] Finetuning from /workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
I0511 10:38:01.166608   286 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 10:38:01.168434   286 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 10:38:01.168687   286 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 10:38:01.169045   286 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 10:38:01.169394   286 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.169845   286 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 10:38:01.169935   286 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 10:38:01.170405   286 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.170811   286 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 10:38:01.170994   286 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 10:38:01.171176   286 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.171464   286 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.171808   286 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.171887   286 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.172248   286 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.172611   286 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.172691   286 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 10:38:01.172857   286 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.173296   286 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.173667   286 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.173751   286 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.174124   286 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.174484   286 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.174561   286 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.174814   286 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 10:38:01.175009   286 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.175540   286 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.175928   286 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.176091   286 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.176482   286 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.176846   286 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.177006   286 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 10:38:01.177176   286 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.178481   286 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.178851   286 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.179016   286 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.179925   286 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.180318   286 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.180486   286 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.180663   286 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 10:38:01.180835   286 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 10:38:01.180999   286 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 10:38:01.181160   286 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 10:38:01.181329   286 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 10:38:01.181493   286 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 10:38:01.181664   286 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 10:38:01.181828   286 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 10:38:01.182121   286 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 10:38:01.182288   286 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 10:38:01.182462   286 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 10:38:01.182631   286 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 10:38:01.183017   286 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 10:38:01.183194   286 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 10:38:01.183369   286 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 10:38:01.183539   286 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 10:38:01.183950   286 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 10:38:01.184123   286 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 10:38:01.184290   286 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 10:38:01.184465   286 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 10:38:01.184850   286 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 10:38:01.185022   286 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 10:38:01.185194   286 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 10:38:01.185366   286 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 10:38:01.185762   286 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 10:38:01.185933   286 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 10:38:01.186108   286 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 10:38:01.186296   286 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 10:38:01.186681   286 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 10:38:01.186849   286 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 10:38:01.187016   286 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 10:38:01.187181   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.187356   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 10:38:01.188225   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.188688   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.188854   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.189029   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.189196   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 10:38:01.189911   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.190171   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 10:38:01.190552   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.190805   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.191025   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.191185   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.191355   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.191519   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 10:38:01.191956   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.192281   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.192445   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.192625   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.192786   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 10:38:01.195355   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.196084   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 10:38:01.196985   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.197255   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.197477   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.197638   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.197811   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.198031   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 10:38:01.198453   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.198796   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.198962   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.199127   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.199317   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 10:38:01.201678   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.202448   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 10:38:01.203383   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.203635   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.203831   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.203982   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.204149   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.204309   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 10:38:01.204739   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.205052   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.205209   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.205415   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.205565   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 10:38:01.208089   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.208870   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 10:38:01.209805   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.210052   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.210258   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.210422   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.210597   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.210768   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 10:38:01.211201   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.211534   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.211699   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.211879   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.212046   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 10:38:01.212749   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.213001   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 10:38:01.213376   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.213618   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.213819   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.213979   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.214154   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.214368   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 10:38:01.214773   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.215095   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.215256   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.215468   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.215624   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 10:38:01.216327   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.216609   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 10:38:01.216974   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.217208   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.217423   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.217581   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.217789   286 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 10:38:01.217947   286 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 10:38:01.218113   286 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 10:38:01.218281   286 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 10:38:01.226122   286 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 10:38:01.259639   286 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 10:38:01.259920   286 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 10:38:01.260269   286 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 10:38:01.260607   286 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.261065   286 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 10:38:01.261265   286 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 10:38:01.261592   286 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.262035   286 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 10:38:01.262204   286 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 10:38:01.262380   286 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.262717   286 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.263142   286 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.263317   286 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.263643   286 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.273527   286 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.273838   286 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 10:38:01.274055   286 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.274770   286 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.275532   286 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.275728   286 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.276331   286 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.277051   286 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.277247   286 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.277442   286 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 10:38:01.277660   286 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.278848   286 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.279575   286 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.279785   286 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.280678   286 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.281399   286 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.281605   286 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 10:38:01.281803   286 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 10:38:01.285168   286 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 10:38:01.285991   286 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 10:38:01.286219   286 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 10:38:01.288219   286 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 10:38:01.289085   286 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 10:38:01.289299   286 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 10:38:01.289515   286 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 10:38:01.289700   286 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 10:38:01.289882   286 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 10:38:01.290081   286 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 10:38:01.290271   286 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 10:38:01.290452   286 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 10:38:01.290635   286 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 10:38:01.290822   286 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 10:38:01.291348   286 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 10:38:01.291553   286 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 10:38:01.291736   286 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 10:38:01.291929   286 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 10:38:01.292723   286 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 10:38:01.292903   286 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 10:38:01.293149   286 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 10:38:01.293285   286 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 10:38:01.294036   286 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 10:38:01.294201   286 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 10:38:01.294376   286 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 10:38:01.294589   286 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 10:38:01.295296   286 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 10:38:01.295464   286 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 10:38:01.295600   286 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 10:38:01.295737   286 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 10:38:01.296484   286 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 10:38:01.296715   286 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 10:38:01.296912   286 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 10:38:01.297065   286 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 10:38:01.297871   286 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 10:38:01.298100   286 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 10:38:01.298367   286 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 10:38:01.298622   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.298801   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 10:38:01.305400   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.306962   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.307109   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.307308   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.307507   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 10:38:01.309159   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.309569   286 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 10:38:01.310184   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.310328   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.310472   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.310600   286 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.310724   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.310847   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 10:38:01.311291   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.311669   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.311801   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.311996   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.312127   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 10:38:01.324643   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.326035   286 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 10:38:01.329679   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.330022   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.330471   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.330860   286 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.331136   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.331310   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 10:38:01.332109   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.332574   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.332746   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.332899   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.333058   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 10:38:01.337882   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.338722   286 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 10:38:01.340797   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.341045   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.341317   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.341480   286 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.341631   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.341784   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 10:38:01.342450   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.342924   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.343081   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.343235   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.343379   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 10:38:01.354245   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.355154   286 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 10:38:01.358006   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.358327   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.358742   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.358916   286 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.359082   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.359253   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 10:38:01.360002   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.360553   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.360744   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.360913   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.361079   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 10:38:01.362339   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.362622   286 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 10:38:01.363209   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.363447   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.363703   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.363850   286 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.363993   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 10:38:01.364146   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 10:38:01.364729   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.365173   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 10:38:01.365340   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 10:38:01.365602   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 10:38:01.365751   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 10:38:01.366809   286 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 10:38:01.367069   286 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 10:38:01.367630   286 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 10:38:01.367913   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 10:38:01.368168   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 10:38:01.368326   286 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 10:38:01.368482   286 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 10:38:01.368618   286 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 10:38:01.368772   286 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 10:38:01.368923   286 net.cpp:1137] Ignoring source layer mbox_loss
I0511 10:38:01.369447   286 caffe.cpp:260] Starting Optimization
I0511 10:38:01.370682   286 solver.cpp:455] Solving ssdJacintoNetV2
I0511 10:38:01.370862   286 solver.cpp:456] Learning Rate Policy: poly
I0511 10:38:01.371042   286 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 10:38:01.375810   286 solver.cpp:269] Initial Test started...
I0511 10:38:01.395205   286 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 10:38:01.399256   286 net.cpp:1071] Ignoring source layer mbox_loss
I0511 10:38:01.394722   331 common.cpp:528] NVML initialized, thread 331
I0511 10:38:01.595722   331 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 331
I0511 10:38:28.091308   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:38:28.412811   286 solver.cpp:749] class AP 1: 0
I0511 10:38:28.419812   286 solver.cpp:749] class AP 2: 0
I0511 10:38:28.434264   286 solver.cpp:749] class AP 3: 0
I0511 10:38:28.434299   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0
I0511 10:38:28.434365   286 solver.cpp:274] Initial Test completed in 27.0391s
I0511 10:38:29.393462   286 solver.cpp:360] Iteration 0 (0.959058 s), loss = 17.6562
I0511 10:38:29.393839   286 solver.cpp:378]     Train net output #0: mbox_loss = 17.8346 (* 1 = 17.8346 loss)
I0511 10:38:29.393970   286 sgd_solver.cpp:172] Iteration 0, lr = 0.01, m = 0.9, wd = 0.0005, gs = 1
I0511 10:38:29.541718   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 7.07G, req 0.74M)	t: 0 0 1.91
I0511 10:38:29.812618   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.92 1.99
I0511 10:38:30.201333   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.96 2.71
I0511 10:38:30.466063   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 7.07G, req 0.74M)	t: 0 0.33 0.78
I0511 10:38:30.853801   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.11G 64/1 6 4 5 	(avail 6.96G, req 0.11G)	t: 0 0.65 1.1
I0511 10:38:31.122493   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.11G 128/4 6 4 0 	(avail 6.96G, req 0.11G)	t: 0 0.2 0.43
I0511 10:38:31.424767   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.11G 128/1 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.54 0.58
I0511 10:38:31.657657   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.11G 256/4 6 4 5 	(avail 6.96G, req 0.11G)	t: 0 0.18 0.23
I0511 10:38:31.942916   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.11G 256/1 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.55 0.5
I0511 10:38:32.143879   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.11G 512/4 7 5 5 	(avail 6.96G, req 0.11G)	t: 0 0.11 0.11
I0511 10:38:32.630931   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.11G 128/1 1 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.41 0.82
I0511 10:38:32.894946   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.11G 512/1 1 1 0 	(avail 6.95G, req 0.11G)	t: 0 0.15 0.2
I0511 10:38:33.206898   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.11G 512/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.07 0.09
I0511 10:38:33.467043   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.11G 512/1 0 0 1 	(avail 6.96G, req 0.11G)	t: 0 0.05 0.06
I0511 10:38:33.677325   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.11G 512/1 0 0 3 	(avail 6.96G, req 0.11G)	t: 0 0.05 0.05
I0511 10:38:33.866238   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.11G 512/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.06 0.06
I0511 10:38:34.167804   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.26 0.72
I0511 10:38:34.478067   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.11G 256/1 1 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.26 0.71
I0511 10:38:34.679678   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.14 0.08
I0511 10:38:34.853935   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.09 0.11
I0511 10:38:35.038169   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 6.96G, req 0.11G)	t: 0 0.01 0.02
I0511 10:38:35.225252   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.07 0.03
I0511 10:38:35.401845   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.584975   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.95G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.765408   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:35.952461   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.130211   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 6.96G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.308430   286 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 6.95G, req 0.11G)	t: 0 0.02 0.02
I0511 10:38:36.956701   286 solver.cpp:360] Iteration 1 (7.56243 s), loss = 15.8703
I0511 10:38:36.956981   286 solver.cpp:378]     Train net output #0: mbox_loss = 13.5519 (* 1 = 13.5519 loss)
I0511 10:38:38.168962   286 solver.cpp:360] Iteration 2 (1.21224 s), loss = 14.956
I0511 10:38:38.169925   286 solver.cpp:378]     Train net output #0: mbox_loss = 13.8961 (* 1 = 13.8961 loss)
I0511 10:38:57.125018   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:40:03.165524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:41:14.132916   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:42:28.609467   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:42:48.614398   286 solver.cpp:354] Iteration 100 (0.391304 iter/s, 250.445s/98 iter), 3.8/376.5ep, loss = 4.91637
I0511 10:42:48.615317   286 solver.cpp:378]     Train net output #0: mbox_loss = 4.19354 (* 1 = 4.19354 loss)
I0511 10:42:48.615847   286 sgd_solver.cpp:172] Iteration 100, lr = 0.00960596, m = 0.9, wd = 0.0005, gs = 1
I0511 10:43:35.426385   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:44:40.301086   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:45:52.520774   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:47:07.786118   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:47:10.175945   286 solver.cpp:354] Iteration 200 (0.382319 iter/s, 261.562s/100 iter), 7.5/376.5ep, loss = 4.07285
I0511 10:47:10.176024   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.58295 (* 1 = 3.58295 loss)
I0511 10:47:10.176046   286 sgd_solver.cpp:172] Iteration 200, lr = 0.00922368, m = 0.9, wd = 0.0005, gs = 1
I0511 10:48:14.055537   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:49:22.252074   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:50:40.964177   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:51:42.024085   286 solver.cpp:354] Iteration 300 (0.367852 iter/s, 271.848s/100 iter), 11.3/376.5ep, loss = 4.21286
I0511 10:51:42.024226   286 solver.cpp:378]     Train net output #0: mbox_loss = 4.17682 (* 1 = 4.17682 loss)
I0511 10:51:42.024268   286 sgd_solver.cpp:172] Iteration 300, lr = 0.00885293, m = 0.9, wd = 0.0005, gs = 1
I0511 10:51:48.562729   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:52:54.873471   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:54:12.802685   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:55:23.551412   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:56:05.629467   286 solver.cpp:354] Iteration 400 (0.379355 iter/s, 263.605s/100 iter), 15.1/376.5ep, loss = 3.72909
I0511 10:56:05.629534   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.93742 (* 1 = 3.93742 loss)
I0511 10:56:05.629544   286 sgd_solver.cpp:172] Iteration 400, lr = 0.00849346, m = 0.9, wd = 0.0005, gs = 1
I0511 10:56:29.952297   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:57:43.743602   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:58:51.682006   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 10:59:57.250562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:00:23.382565   286 solver.cpp:354] Iteration 500 (0.387968 iter/s, 257.753s/100 iter), 18.8/376.5ep, loss = 3.60061
I0511 11:00:23.382637   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.24536 (* 1 = 3.24536 loss)
I0511 11:00:23.382660   286 sgd_solver.cpp:172] Iteration 500, lr = 0.00814506, m = 0.9, wd = 0.0005, gs = 1
I0511 11:01:11.306205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:02:20.201948   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:03:25.341446   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:04:37.361199   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:04:48.727902   286 solver.cpp:354] Iteration 600 (0.376868 iter/s, 265.345s/100 iter), 22.6/376.5ep, loss = 3.71362
I0511 11:04:48.727995   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.55587 (* 1 = 3.55587 loss)
I0511 11:04:48.728022   286 sgd_solver.cpp:172] Iteration 600, lr = 0.00780749, m = 0.9, wd = 0.0005, gs = 1
I0511 11:05:48.521507   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:07:02.944953   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:08:09.233359   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:09:16.780077   286 solver.cpp:354] Iteration 700 (0.373062 iter/s, 268.052s/100 iter), 26.4/376.5ep, loss = 3.65001
I0511 11:09:16.780711   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.73428 (* 1 = 3.73428 loss)
I0511 11:09:16.780903   286 sgd_solver.cpp:172] Iteration 700, lr = 0.00748052, m = 0.9, wd = 0.0005, gs = 1
I0511 11:09:26.408531   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:10:34.786350   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:11:41.689963   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:12:56.029769   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:13:43.569540   286 solver.cpp:354] Iteration 800 (0.374828 iter/s, 266.789s/100 iter), 30.1/376.5ep, loss = 3.43742
I0511 11:13:43.570178   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.4464 (* 1 = 3.4464 loss)
I0511 11:13:43.570397   286 sgd_solver.cpp:172] Iteration 800, lr = 0.00716393, m = 0.9, wd = 0.0005, gs = 1
I0511 11:14:06.653177   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:15:16.172699   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:16:30.039755   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:17:44.701989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:18:09.307843   286 solver.cpp:354] Iteration 900 (0.37631 iter/s, 265.738s/100 iter), 33.9/376.5ep, loss = 3.52763
I0511 11:18:09.308257   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.65744 (* 1 = 3.65744 loss)
I0511 11:18:09.308470   286 sgd_solver.cpp:172] Iteration 900, lr = 0.0068575, m = 0.9, wd = 0.0005, gs = 1
I0511 11:18:48.855034   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:20:03.712283   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:21:15.829341   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:22:26.632393   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:22:37.554395   286 solver.cpp:354] Iteration 1000 (0.372791 iter/s, 268.247s/100 iter), 37.6/376.5ep, loss = 3.44066
I0511 11:22:37.554469   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.49448 (* 1 = 3.49448 loss)
I0511 11:22:37.555598   286 sgd_solver.cpp:172] Iteration 1000, lr = 0.006561, m = 0.9, wd = 0.0005, gs = 1
I0511 11:23:37.579844   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:24:54.242961   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:26:09.785871   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:27:11.479239   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:27:12.089705   286 solver.cpp:354] Iteration 1100 (0.364251 iter/s, 274.536s/100 iter), 41.4/376.5ep, loss = 3.35575
I0511 11:27:12.089983   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.20541 (* 1 = 3.20541 loss)
I0511 11:27:12.090056   286 sgd_solver.cpp:172] Iteration 1100, lr = 0.00627422, m = 0.9, wd = 0.0005, gs = 1
I0511 11:28:22.553395   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:29:36.227958   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:30:40.405364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:31:31.089360   286 solver.cpp:354] Iteration 1200 (0.386101 iter/s, 259s/100 iter), 45.2/376.5ep, loss = 3.31291
I0511 11:31:31.089628   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.32026 (* 1 = 3.32026 loss)
I0511 11:31:31.089767   286 sgd_solver.cpp:172] Iteration 1200, lr = 0.00599695, m = 0.9, wd = 0.0005, gs = 1
I0511 11:31:49.211495   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:33:00.732100   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:34:11.613571   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:35:23.095744   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:35:52.320159   286 solver.cpp:354] Iteration 1300 (0.382803 iter/s, 261.231s/100 iter), 48.9/376.5ep, loss = 3.1759
I0511 11:35:52.320235   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.83433 (* 1 = 3.83433 loss)
I0511 11:35:52.320259   286 sgd_solver.cpp:172] Iteration 1300, lr = 0.00572898, m = 0.9, wd = 0.0005, gs = 1
I0511 11:36:29.350693   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:37:43.388226   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:38:55.138619   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:40:05.133401   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:40:23.446975   286 solver.cpp:354] Iteration 1400 (0.368831 iter/s, 271.127s/100 iter), 52.7/376.5ep, loss = 3.24382
I0511 11:40:23.447376   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.0333 (* 1 = 3.0333 loss)
I0511 11:40:23.447553   286 sgd_solver.cpp:172] Iteration 1400, lr = 0.00547008, m = 0.9, wd = 0.0005, gs = 1
I0511 11:41:20.986205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:42:29.725340   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:43:33.881989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:44:45.873347   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:44:47.280021   286 solver.cpp:354] Iteration 1500 (0.379028 iter/s, 263.833s/100 iter), 56.5/376.5ep, loss = 3.09131
I0511 11:44:47.280622   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.15936 (* 1 = 3.15936 loss)
I0511 11:44:47.280884   286 sgd_solver.cpp:172] Iteration 1500, lr = 0.00522006, m = 0.9, wd = 0.0005, gs = 1
I0511 11:46:05.818781   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:47:07.989359   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:48:17.968626   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:49:22.973845   286 solver.cpp:354] Iteration 1600 (0.362721 iter/s, 275.694s/100 iter), 60.2/376.5ep, loss = 3.17959
I0511 11:49:22.973912   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.86817 (* 1 = 2.86817 loss)
I0511 11:49:22.973922   286 sgd_solver.cpp:172] Iteration 1600, lr = 0.00497871, m = 0.9, wd = 0.0005, gs = 1
I0511 11:49:42.087678   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:50:47.660218   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:51:56.369282   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:53:06.144986   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:53:48.199604   286 solver.cpp:354] Iteration 1700 (0.377038 iter/s, 265.226s/100 iter), 64/376.5ep, loss = 3.00944
I0511 11:53:48.200031   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.78802 (* 1 = 2.78802 loss)
I0511 11:53:48.200186   286 sgd_solver.cpp:172] Iteration 1700, lr = 0.00474583, m = 0.9, wd = 0.0005, gs = 1
I0511 11:54:25.107169   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:55:33.825628   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:56:51.222885   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:58:07.273593   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 11:58:28.588940   286 solver.cpp:354] Iteration 1800 (0.356649 iter/s, 280.388s/100 iter), 67.8/376.5ep, loss = 3.05312
I0511 11:58:28.589234   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.0944 (* 1 = 3.0944 loss)
I0511 11:58:28.589326   286 sgd_solver.cpp:172] Iteration 1800, lr = 0.00452122, m = 0.9, wd = 0.0005, gs = 1
I0511 11:59:16.565518   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:00:21.824263   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:01:41.926645   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:02:47.576617   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:02:52.693686   286 solver.cpp:354] Iteration 1900 (0.37864 iter/s, 264.103s/100 iter), 71.5/376.5ep, loss = 3.15033
I0511 12:02:52.693955   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.22117 (* 1 = 3.22117 loss)
I0511 12:02:52.693980   286 sgd_solver.cpp:172] Iteration 1900, lr = 0.00430467, m = 0.9, wd = 0.0005, gs = 1
I0511 12:03:53.067049   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:05:04.221717   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:06:15.752265   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:07:15.737447   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 12:07:16.003279   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 12:07:16.088572   286 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 12:07:39.658536   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:07:40.059329   286 solver.cpp:749] class AP 1: 0.879249
I0511 12:07:40.110666   286 solver.cpp:749] class AP 2: 0.840918
I0511 12:07:40.112437   286 solver.cpp:749] class AP 3: 0.906466
I0511 12:07:40.112879   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.875544
I0511 12:07:40.112967   286 solver.cpp:284] Tests completed in 287.418s
I0511 12:07:40.950157   286 solver.cpp:354] Iteration 2000 (0.347925 iter/s, 287.418s/100 iter), 75.3/376.5ep, loss = 3.07947
I0511 12:07:40.950234   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.79876 (* 1 = 2.79876 loss)
I0511 12:07:40.950276   286 sgd_solver.cpp:172] Iteration 2000, lr = 0.004096, m = 0.9, wd = 0.0005, gs = 1
I0511 12:07:44.587966   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:08:55.205845   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:10:08.024942   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:11:23.318500   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:12:05.289585   286 solver.cpp:354] Iteration 2100 (0.378303 iter/s, 264.339s/100 iter), 79.1/376.5ep, loss = 3.1227
I0511 12:12:05.289705   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.05706 (* 1 = 3.05706 loss)
I0511 12:12:05.289732   286 sgd_solver.cpp:172] Iteration 2100, lr = 0.00389501, m = 0.9, wd = 0.0005, gs = 1
I0511 12:12:27.215154   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:13:38.954730   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:14:46.994252   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:15:59.583885   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:16:27.190752   286 solver.cpp:354] Iteration 2200 (0.381825 iter/s, 261.9s/100 iter), 82.8/376.5ep, loss = 3.05287
I0511 12:16:27.190817   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.07876 (* 1 = 3.07876 loss)
I0511 12:16:27.190838   286 sgd_solver.cpp:172] Iteration 2200, lr = 0.00370151, m = 0.9, wd = 0.0005, gs = 1
I0511 12:17:07.749131   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:18:12.230006   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:19:31.211865   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:20:39.364607   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:20:48.393659   286 solver.cpp:354] Iteration 2300 (0.382845 iter/s, 261.202s/100 iter), 86.6/376.5ep, loss = 2.79292
I0511 12:20:48.394317   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.48514 (* 1 = 2.48514 loss)
I0511 12:20:48.394619   286 sgd_solver.cpp:172] Iteration 2300, lr = 0.0035153, m = 0.9, wd = 0.0005, gs = 1
I0511 12:21:44.641352   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:23:00.002321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:24:05.931144   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:25:09.035069   286 solver.cpp:354] Iteration 2400 (0.38367 iter/s, 260.641s/100 iter), 90.4/376.5ep, loss = 2.7932
I0511 12:25:09.035487   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.40063 (* 1 = 2.40063 loss)
I0511 12:25:09.035624   286 sgd_solver.cpp:172] Iteration 2400, lr = 0.00333622, m = 0.9, wd = 0.0005, gs = 1
I0511 12:25:14.365535   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:26:21.527201   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:27:34.652740   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:28:51.651880   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:29:33.386966   286 solver.cpp:354] Iteration 2500 (0.378284 iter/s, 264.352s/100 iter), 94.1/376.5ep, loss = 2.81492
I0511 12:29:33.387517   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.60854 (* 1 = 2.60854 loss)
I0511 12:29:33.387727   286 sgd_solver.cpp:172] Iteration 2500, lr = 0.00316406, m = 0.9, wd = 0.0005, gs = 1
I0511 12:29:54.993562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:31:01.488196   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:32:17.283821   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:33:21.858769   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:33:49.713783   286 solver.cpp:354] Iteration 2600 (0.390125 iter/s, 256.328s/100 iter), 97.9/376.5ep, loss = 2.73197
I0511 12:33:49.713941   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.54406 (* 1 = 2.54406 loss)
I0511 12:33:49.713989   286 sgd_solver.cpp:172] Iteration 2600, lr = 0.00299866, m = 0.9, wd = 0.0005, gs = 1
I0511 12:34:29.731086   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:35:45.453665   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:36:53.442992   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:37:58.900590   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:38:16.697453   286 solver.cpp:354] Iteration 2700 (0.374554 iter/s, 266.984s/100 iter), 101.6/376.5ep, loss = 2.93363
I0511 12:38:16.697540   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.18382 (* 1 = 3.18382 loss)
I0511 12:38:16.697566   286 sgd_solver.cpp:172] Iteration 2700, lr = 0.00283982, m = 0.9, wd = 0.0005, gs = 1
I0511 12:39:13.403147   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:40:26.884312   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:41:37.105988   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:42:36.481858   286 solver.cpp:354] Iteration 2800 (0.384934 iter/s, 259.785s/100 iter), 105.4/376.5ep, loss = 2.71231
I0511 12:42:36.482138   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.96847 (* 1 = 2.96847 loss)
I0511 12:42:36.482244   286 sgd_solver.cpp:172] Iteration 2800, lr = 0.00268739, m = 0.9, wd = 0.0005, gs = 1
I0511 12:42:37.723563   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:43:46.591173   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:45:01.515321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:46:09.190704   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:47:06.386873   286 solver.cpp:354] Iteration 2900 (0.370501 iter/s, 269.905s/100 iter), 109.2/376.5ep, loss = 2.74402
I0511 12:47:06.386988   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.80988 (* 1 = 2.80988 loss)
I0511 12:47:06.387012   286 sgd_solver.cpp:172] Iteration 2900, lr = 0.00254117, m = 0.9, wd = 0.0005, gs = 1
I0511 12:47:26.112315   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:48:37.512287   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:49:43.472292   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:50:57.740178   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:51:35.410475   286 solver.cpp:354] Iteration 3000 (0.371715 iter/s, 269.023s/100 iter), 112.9/376.5ep, loss = 2.8412
I0511 12:51:35.410542   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.64109 (* 1 = 2.64109 loss)
I0511 12:51:35.410553   286 sgd_solver.cpp:172] Iteration 3000, lr = 0.002401, m = 0.9, wd = 0.0005, gs = 1
I0511 12:52:14.139542   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:53:29.024706   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:54:41.149220   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:55:49.354791   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:56:07.841018   286 solver.cpp:354] Iteration 3100 (0.367066 iter/s, 272.43s/100 iter), 116.7/376.5ep, loss = 2.75702
I0511 12:56:07.841323   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39492 (* 1 = 2.39492 loss)
I0511 12:56:07.841435   286 sgd_solver.cpp:172] Iteration 3100, lr = 0.00226671, m = 0.9, wd = 0.0005, gs = 1
I0511 12:56:54.763296   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:58:11.073791   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 12:59:17.272364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:00:26.318444   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:00:29.234131   286 solver.cpp:354] Iteration 3200 (0.382566 iter/s, 261.393s/100 iter), 120.5/376.5ep, loss = 2.63678
I0511 13:00:29.234170   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.46977 (* 1 = 2.46977 loss)
I0511 13:00:29.234181   286 sgd_solver.cpp:172] Iteration 3200, lr = 0.00213814, m = 0.9, wd = 0.0005, gs = 1
I0511 13:01:40.847404   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:02:44.592392   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:03:56.883589   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:04:53.933248   286 solver.cpp:354] Iteration 3300 (0.377789 iter/s, 264.698s/100 iter), 124.2/376.5ep, loss = 2.74173
I0511 13:04:53.937497   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.10632 (* 1 = 2.10632 loss)
I0511 13:04:53.937575   286 sgd_solver.cpp:172] Iteration 3300, lr = 0.00201511, m = 0.9, wd = 0.0005, gs = 1
I0511 13:05:09.040370   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:06:14.338225   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:07:25.205103   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:08:39.808761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:09:17.926232   286 solver.cpp:354] Iteration 3400 (0.378801 iter/s, 263.991s/100 iter), 128/376.5ep, loss = 2.649
I0511 13:09:17.927139   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.8199 (* 1 = 2.8199 loss)
I0511 13:09:17.927505   286 sgd_solver.cpp:172] Iteration 3400, lr = 0.00189747, m = 0.9, wd = 0.0005, gs = 1
I0511 13:09:53.793624   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:10:58.517418   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:12:16.417186   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:13:27.841019   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:13:50.945454   286 solver.cpp:354] Iteration 3500 (0.366277 iter/s, 273.018s/100 iter), 131.8/376.5ep, loss = 2.67151
I0511 13:13:50.945530   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.23974 (* 1 = 2.23974 loss)
I0511 13:13:50.945556   286 sgd_solver.cpp:172] Iteration 3500, lr = 0.00178506, m = 0.9, wd = 0.0005, gs = 1
I0511 13:14:35.788682   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:15:48.467964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:16:53.009790   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:18:04.037165   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:18:06.838940   286 solver.cpp:354] Iteration 3600 (0.390789 iter/s, 255.892s/100 iter), 135.5/376.5ep, loss = 2.57451
I0511 13:18:06.839455   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.47927 (* 1 = 2.47927 loss)
I0511 13:18:06.839705   286 sgd_solver.cpp:172] Iteration 3600, lr = 0.00167772, m = 0.9, wd = 0.0005, gs = 1
I0511 13:19:10.187364   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:20:17.900319   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:21:29.689242   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:22:31.841965   286 solver.cpp:354] Iteration 3700 (0.377356 iter/s, 265.002s/100 iter), 139.3/376.5ep, loss = 2.60107
I0511 13:22:31.842711   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.14181 (* 1 = 2.14181 loss)
I0511 13:22:31.842969   286 sgd_solver.cpp:172] Iteration 3700, lr = 0.0015753, m = 0.9, wd = 0.0005, gs = 1
I0511 13:22:47.849494   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:23:49.027128   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:24:56.902205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:26:08.242411   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:26:52.545734   286 solver.cpp:354] Iteration 3800 (0.383579 iter/s, 260.702s/100 iter), 143.1/376.5ep, loss = 2.76805
I0511 13:26:52.545801   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.64585 (* 1 = 2.64585 loss)
I0511 13:26:52.545812   286 sgd_solver.cpp:172] Iteration 3800, lr = 0.00147763, m = 0.9, wd = 0.0005, gs = 1
I0511 13:27:20.809334   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:28:26.125339   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:29:41.801555   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:30:51.501336   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:31:15.801491   286 solver.cpp:354] Iteration 3900 (0.37986 iter/s, 263.255s/100 iter), 146.8/376.5ep, loss = 2.58987
I0511 13:31:15.801527   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.53969 (* 1 = 2.53969 loss)
I0511 13:31:15.801539   286 sgd_solver.cpp:172] Iteration 3900, lr = 0.00138458, m = 0.9, wd = 0.0005, gs = 1
I0511 13:31:57.397296   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:33:08.280045   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:34:19.864984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:24.890027   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:30.135825   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0511 13:35:30.173681   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0511 13:35:30.217507   286 solver.cpp:637] Iteration 4000, Testing net (#0)
I0511 13:35:57.209758   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:35:57.714069   286 solver.cpp:749] class AP 1: 0.895331
I0511 13:35:57.718536   286 solver.cpp:749] class AP 2: 0.884352
I0511 13:35:57.719480   286 solver.cpp:749] class AP 3: 0.903
I0511 13:35:57.719511   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894228
I0511 13:35:57.719575   286 solver.cpp:284] Tests completed in 281.917s
I0511 13:35:58.392904   286 solver.cpp:354] Iteration 4000 (0.354714 iter/s, 281.917s/100 iter), 150.6/376.5ep, loss = 2.55356
I0511 13:35:58.393193   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.09378 (* 1 = 2.09378 loss)
I0511 13:35:58.393275   286 sgd_solver.cpp:172] Iteration 4000, lr = 0.001296, m = 0.9, wd = 0.0005, gs = 1
I0511 13:36:50.674672   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:37:53.737460   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:39:16.183795   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:40:17.428336   286 solver.cpp:354] Iteration 4100 (0.386046 iter/s, 259.036s/100 iter), 154.4/376.5ep, loss = 2.51746
I0511 13:40:17.429203   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.36696 (* 1 = 2.36696 loss)
I0511 13:40:17.429647   286 sgd_solver.cpp:172] Iteration 4100, lr = 0.00121174, m = 0.9, wd = 0.0005, gs = 1
I0511 13:40:22.139886   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:41:26.006645   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:42:44.228413   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:43:50.919600   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:44:40.386477   286 solver.cpp:354] Iteration 4200 (0.380288 iter/s, 262.959s/100 iter), 158.1/376.5ep, loss = 2.48454
I0511 13:44:40.387066   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.95867 (* 1 = 1.95867 loss)
I0511 13:44:40.387343   286 sgd_solver.cpp:172] Iteration 4200, lr = 0.00113165, m = 0.9, wd = 0.0005, gs = 1
I0511 13:45:00.134527   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:46:11.440686   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:47:33.638960   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:48:40.566258   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:49:10.108214   286 solver.cpp:354] Iteration 4300 (0.370752 iter/s, 269.722s/100 iter), 161.9/376.5ep, loss = 2.66537
I0511 13:49:10.108485   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.17375 (* 1 = 2.17375 loss)
I0511 13:49:10.108569   286 sgd_solver.cpp:172] Iteration 4300, lr = 0.0010556, m = 0.9, wd = 0.0005, gs = 1
I0511 13:49:54.331159   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:50:59.843852   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:52:08.072981   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:53:22.533658   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:53:32.761421   286 solver.cpp:354] Iteration 4400 (0.38073 iter/s, 262.653s/100 iter), 165.6/376.5ep, loss = 2.55626
I0511 13:53:32.762244   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.09898 (* 1 = 3.09898 loss)
I0511 13:53:32.762508   286 sgd_solver.cpp:172] Iteration 4400, lr = 0.00098345, m = 0.9, wd = 0.0005, gs = 1
I0511 13:54:32.260608   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:55:37.447028   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:56:43.742321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:57:51.340132   286 solver.cpp:354] Iteration 4500 (0.38673 iter/s, 258.579s/100 iter), 169.4/376.5ep, loss = 2.53083
I0511 13:57:51.340680   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.38264 (* 1 = 2.38264 loss)
I0511 13:57:51.340816   286 sgd_solver.cpp:172] Iteration 4500, lr = 0.000915063, m = 0.9, wd = 0.0005, gs = 1
I0511 13:57:54.133273   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 13:59:02.404615   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:00:11.371378   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:01:19.137918   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:02:08.944617   286 solver.cpp:354] Iteration 4600 (0.388192 iter/s, 257.604s/100 iter), 173.2/376.5ep, loss = 2.55919
I0511 14:02:08.945158   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.2639 (* 1 = 2.2639 loss)
I0511 14:02:08.945295   286 sgd_solver.cpp:172] Iteration 4600, lr = 0.000850305, m = 0.9, wd = 0.0005, gs = 1
I0511 14:02:25.426777   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:03:36.698139   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:04:52.299738   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:05:57.266844   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:06:32.370049   286 solver.cpp:354] Iteration 4700 (0.379615 iter/s, 263.425s/100 iter), 176.9/376.5ep, loss = 2.26435
I0511 14:06:32.370859   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.24497 (* 1 = 2.24497 loss)
I0511 14:06:32.371233   286 sgd_solver.cpp:172] Iteration 4700, lr = 0.000789048, m = 0.9, wd = 0.0005, gs = 1
I0511 14:07:12.517341   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:08:16.817073   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:09:27.969321   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:10:31.794914   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:10:54.785575   286 solver.cpp:354] Iteration 4800 (0.381076 iter/s, 262.415s/100 iter), 180.7/376.5ep, loss = 2.51465
I0511 14:10:54.785679   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.63854 (* 1 = 2.63854 loss)
I0511 14:10:54.785713   286 sgd_solver.cpp:172] Iteration 4800, lr = 0.000731161, m = 0.9, wd = 0.0005, gs = 1
I0511 14:11:47.349313   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:12:51.029984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:14:02.539785   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:15:11.173334   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:15:12.481969   286 solver.cpp:354] Iteration 4900 (0.388054 iter/s, 257.696s/100 iter), 184.5/376.5ep, loss = 2.46885
I0511 14:15:12.482333   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.61849 (* 1 = 2.61849 loss)
I0511 14:15:12.482445   286 sgd_solver.cpp:172] Iteration 4900, lr = 0.00067652, m = 0.9, wd = 0.0005, gs = 1
I0511 14:16:16.027968   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:17:19.601358   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:18:28.823508   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:19:19.600805   286 solver.cpp:354] Iteration 5000 (0.404664 iter/s, 247.118s/100 iter), 188.2/376.5ep, loss = 2.35858
I0511 14:19:19.600908   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.06529 (* 1 = 2.06529 loss)
I0511 14:19:19.600934   286 sgd_solver.cpp:172] Iteration 5000, lr = 0.000625, m = 0.9, wd = 0.0005, gs = 1
I0511 14:19:37.514231   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:20:42.713737   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:21:57.599524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:23:11.385015   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:23:48.169528   286 solver.cpp:354] Iteration 5100 (0.372345 iter/s, 268.568s/100 iter), 192/376.5ep, loss = 2.45979
I0511 14:23:48.170099   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.56464 (* 1 = 2.56464 loss)
I0511 14:23:48.170296   286 sgd_solver.cpp:172] Iteration 5100, lr = 0.00057648, m = 0.9, wd = 0.0005, gs = 1
I0511 14:24:14.310308   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:25:27.188686   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:26:38.342267   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:27:43.074115   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:28:08.558845   286 solver.cpp:354] Iteration 5200 (0.384041 iter/s, 260.389s/100 iter), 195.8/376.5ep, loss = 2.47466
I0511 14:28:08.558895   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.50955 (* 1 = 2.50955 loss)
I0511 14:28:08.558910   286 sgd_solver.cpp:172] Iteration 5200, lr = 0.000530842, m = 0.9, wd = 0.0005, gs = 1
I0511 14:28:52.875831   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:30:09.653620   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:31:12.377820   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:32:23.693809   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:32:30.000942   286 solver.cpp:354] Iteration 5300 (0.382494 iter/s, 261.442s/100 iter), 199.5/376.5ep, loss = 2.40503
I0511 14:32:30.001298   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.30751 (* 1 = 2.30751 loss)
I0511 14:32:30.001462   286 sgd_solver.cpp:172] Iteration 5300, lr = 0.000487968, m = 0.9, wd = 0.0005, gs = 1
I0511 14:33:29.752796   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:34:38.402200   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:35:49.980350   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:36:55.671003   286 solver.cpp:354] Iteration 5400 (0.376407 iter/s, 265.67s/100 iter), 203.3/376.5ep, loss = 2.47235
I0511 14:36:55.671073   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.21606 (* 1 = 2.21606 loss)
I0511 14:36:55.671084   286 sgd_solver.cpp:172] Iteration 5400, lr = 0.000447745, m = 0.9, wd = 0.0005, gs = 1
I0511 14:37:06.020602   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:38:10.276371   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:39:15.951314   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:40:32.351894   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:41:13.609359   286 solver.cpp:354] Iteration 5500 (0.38769 iter/s, 257.938s/100 iter), 207.1/376.5ep, loss = 2.31745
I0511 14:41:13.609762   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.76545 (* 1 = 2.76545 loss)
I0511 14:41:13.609951   286 sgd_solver.cpp:172] Iteration 5500, lr = 0.000410062, m = 0.9, wd = 0.0005, gs = 1
I0511 14:41:38.267742   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:42:51.352392   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:44:05.462697   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:45:15.655130   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:45:38.377336   286 solver.cpp:354] Iteration 5600 (0.37769 iter/s, 264.768s/100 iter), 210.8/376.5ep, loss = 2.37671
I0511 14:45:38.377447   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.80275 (* 1 = 2.80275 loss)
I0511 14:45:38.377499   286 sgd_solver.cpp:172] Iteration 5600, lr = 0.00037481, m = 0.9, wd = 0.0005, gs = 1
I0511 14:46:20.286691   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:47:37.883761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:48:46.160190   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:49:55.102929   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:50:08.614735   286 solver.cpp:354] Iteration 5700 (0.370047 iter/s, 270.236s/100 iter), 214.6/376.5ep, loss = 2.14596
I0511 14:50:08.614796   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.96386 (* 1 = 1.96386 loss)
I0511 14:50:08.614816   286 sgd_solver.cpp:172] Iteration 5700, lr = 0.00034188, m = 0.9, wd = 0.0005, gs = 1
I0511 14:51:06.059387   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:52:22.318501   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:53:28.155045   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:54:29.501780   286 solver.cpp:354] Iteration 5800 (0.38331 iter/s, 260.886s/100 iter), 218.4/376.5ep, loss = 2.20686
I0511 14:54:29.501899   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.12483 (* 1 = 2.12483 loss)
I0511 14:54:29.501921   286 sgd_solver.cpp:172] Iteration 5800, lr = 0.00031117, m = 0.9, wd = 0.0005, gs = 1
I0511 14:54:41.173198   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:55:52.591112   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:56:57.947305   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:58:11.827960   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 14:58:56.322983   286 solver.cpp:354] Iteration 5900 (0.374784 iter/s, 266.82s/100 iter), 222.1/376.5ep, loss = 2.23025
I0511 14:58:56.323608   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.0907 (* 1 = 2.0907 loss)
I0511 14:58:56.323763   286 sgd_solver.cpp:172] Iteration 5900, lr = 0.000282576, m = 0.9, wd = 0.0005, gs = 1
I0511 14:59:21.182163   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:00:23.018683   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:01:40.175355   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:02:50.411329   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:03:17.114616   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0511 15:03:17.172052   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0511 15:03:17.193516   286 solver.cpp:637] Iteration 6000, Testing net (#0)
I0511 15:03:41.279762   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:03:41.845378   286 solver.cpp:749] class AP 1: 0.902398
I0511 15:03:41.846396   286 solver.cpp:749] class AP 2: 0.892614
I0511 15:03:41.846835   286 solver.cpp:749] class AP 3: 0.902081
I0511 15:03:41.846863   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899031
I0511 15:03:41.846915   286 solver.cpp:284] Tests completed in 285.523s
I0511 15:03:42.425446   286 solver.cpp:354] Iteration 6000 (0.350235 iter/s, 285.523s/100 iter), 225.9/376.5ep, loss = 2.27116
I0511 15:03:42.425484   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39021 (* 1 = 2.39021 loss)
I0511 15:03:42.425494   286 sgd_solver.cpp:172] Iteration 6000, lr = 0.000256, m = 0.9, wd = 0.0005, gs = 1
I0511 15:04:09.587678   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:05:21.828974   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:06:27.871925   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:07:34.535059   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:07:50.633746   286 solver.cpp:354] Iteration 6100 (0.402889 iter/s, 248.208s/100 iter), 229.6/376.5ep, loss = 2.33219
I0511 15:07:50.634197   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.59976 (* 1 = 2.59976 loss)
I0511 15:07:50.634369   286 sgd_solver.cpp:172] Iteration 6100, lr = 0.000231344, m = 0.9, wd = 0.0005, gs = 1
I0511 15:08:42.834233   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:09:56.460775   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:10:59.864198   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:12:10.109671   286 solver.cpp:354] Iteration 6200 (0.385393 iter/s, 259.475s/100 iter), 233.4/376.5ep, loss = 2.30922
I0511 15:12:10.109740   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.06719 (* 1 = 2.06719 loss)
I0511 15:12:10.109751   286 sgd_solver.cpp:172] Iteration 6200, lr = 0.000208514, m = 0.9, wd = 0.0005, gs = 1
I0511 15:12:12.339005   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:13:24.473868   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:14:31.629825   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:15:43.173020   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:16:32.450592   286 solver.cpp:354] Iteration 6300 (0.381184 iter/s, 262.34s/100 iter), 237.2/376.5ep, loss = 2.28051
I0511 15:16:32.450721   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.97659 (* 1 = 1.97659 loss)
I0511 15:16:32.450765   286 sgd_solver.cpp:172] Iteration 6300, lr = 0.000187416, m = 0.9, wd = 0.0005, gs = 1
I0511 15:16:50.844229   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:18:02.489755   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:19:11.244990   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:20:25.167714   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:20:58.709304   286 solver.cpp:354] Iteration 6400 (0.375575 iter/s, 266.258s/100 iter), 240.9/376.5ep, loss = 2.245
I0511 15:20:58.709363   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.39626 (* 1 = 2.39626 loss)
I0511 15:20:58.709373   286 sgd_solver.cpp:172] Iteration 6400, lr = 0.000167962, m = 0.9, wd = 0.0005, gs = 1
I0511 15:21:35.598964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:22:57.373780   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:24:06.000193   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:25:10.659116   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:25:31.005918   286 solver.cpp:354] Iteration 6500 (0.367247 iter/s, 272.296s/100 iter), 244.7/376.5ep, loss = 2.49009
I0511 15:25:31.006278   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.52787 (* 1 = 2.52787 loss)
I0511 15:25:31.006395   286 sgd_solver.cpp:172] Iteration 6500, lr = 0.000150063, m = 0.9, wd = 0.0005, gs = 1
I0511 15:26:26.402189   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:27:33.935775   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:28:43.717872   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:29:52.436280   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:29:55.120820   286 solver.cpp:354] Iteration 6600 (0.378624 iter/s, 264.115s/100 iter), 248.5/376.5ep, loss = 2.33857
I0511 15:29:55.121644   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.55622 (* 1 = 2.55622 loss)
I0511 15:29:55.121953   286 sgd_solver.cpp:172] Iteration 6600, lr = 0.000133634, m = 0.9, wd = 0.0005, gs = 1
I0511 15:31:04.691828   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:32:11.757402   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:33:15.932106   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:34:10.471763   286 solver.cpp:354] Iteration 6700 (0.391619 iter/s, 255.351s/100 iter), 252.2/376.5ep, loss = 2.36313
I0511 15:34:10.471865   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.8176 (* 1 = 1.8176 loss)
I0511 15:34:10.471891   286 sgd_solver.cpp:172] Iteration 6700, lr = 0.000118592, m = 0.9, wd = 0.0005, gs = 1
I0511 15:34:28.992126   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:35:32.187165   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:36:38.552619   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:37:45.253664   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:38:24.437515   286 solver.cpp:354] Iteration 6800 (0.393755 iter/s, 253.965s/100 iter), 256/376.5ep, loss = 2.11028
I0511 15:38:24.437613   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.22305 (* 1 = 2.22305 loss)
I0511 15:38:24.437639   286 sgd_solver.cpp:172] Iteration 6800, lr = 0.000104858, m = 0.9, wd = 0.0005, gs = 1
I0511 15:39:03.429338   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:40:07.609115   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:41:13.936862   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:42:25.769069   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:42:48.835839   286 solver.cpp:354] Iteration 6900 (0.378218 iter/s, 264.398s/100 iter), 259.8/376.5ep, loss = 2.27571
I0511 15:42:48.836088   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.91882 (* 1 = 1.91882 loss)
I0511 15:42:48.836158   286 sgd_solver.cpp:172] Iteration 6900, lr = 9.23521e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:43:36.997524   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:44:40.528015   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:45:55.457073   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:47:08.861306   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:47:10.037616   286 solver.cpp:354] Iteration 7000 (0.382847 iter/s, 261.201s/100 iter), 263.5/376.5ep, loss = 2.21986
I0511 15:47:10.037907   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.45303 (* 1 = 2.45303 loss)
I0511 15:47:10.038018   286 sgd_solver.cpp:172] Iteration 7000, lr = 8.1e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:48:13.642457   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:49:31.888128   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:50:44.515708   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:51:43.701668   286 solver.cpp:354] Iteration 7100 (0.365413 iter/s, 273.663s/100 iter), 267.3/376.5ep, loss = 2.16657
I0511 15:51:43.701923   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.15395 (* 1 = 2.15395 loss)
I0511 15:51:43.702021   286 sgd_solver.cpp:172] Iteration 7100, lr = 7.07281e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:51:55.925788   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:53:05.393335   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:54:10.114141   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:55:29.308032   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:56:10.454062   286 solver.cpp:354] Iteration 7200 (0.37488 iter/s, 266.752s/100 iter), 271.1/376.5ep, loss = 2.25104
I0511 15:56:10.455046   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.45581 (* 1 = 2.45581 loss)
I0511 15:56:10.455458   286 sgd_solver.cpp:172] Iteration 7200, lr = 6.14656e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 15:56:42.564884   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:57:47.640079   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 15:59:03.445338   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:00:14.926611   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:00:39.265496   286 solver.cpp:354] Iteration 7300 (0.372007 iter/s, 268.812s/100 iter), 274.8/376.5ep, loss = 2.27763
I0511 16:00:39.265856   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.21352 (* 1 = 2.21352 loss)
I0511 16:00:39.266031   286 sgd_solver.cpp:172] Iteration 7300, lr = 5.31441e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:01:21.630378   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:02:30.629869   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:03:49.762984   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:04:56.008975   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:05:03.062042   286 solver.cpp:354] Iteration 7400 (0.379079 iter/s, 263.797s/100 iter), 278.6/376.5ep, loss = 2.1906
I0511 16:05:03.062125   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.68163 (* 1 = 1.68163 loss)
I0511 16:05:03.062153   286 sgd_solver.cpp:172] Iteration 7400, lr = 4.56976e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:05:59.355573   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:07:15.925882   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:08:21.498852   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:09:24.217327   286 solver.cpp:354] Iteration 7500 (0.382914 iter/s, 261.155s/100 iter), 282.4/376.5ep, loss = 2.13837
I0511 16:09:24.218309   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.2895 (* 1 = 2.2895 loss)
I0511 16:09:24.218569   286 sgd_solver.cpp:172] Iteration 7500, lr = 3.90625e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:09:27.092594   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:10:30.863638   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:11:49.645085   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:13:00.644201   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:13:29.997458   286 solver.cpp:354] Iteration 7600 (0.406868 iter/s, 245.78s/100 iter), 286.1/376.5ep, loss = 2.27851
I0511 16:13:29.997714   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.3643 (* 1 = 2.3643 loss)
I0511 16:13:29.997809   286 sgd_solver.cpp:172] Iteration 7600, lr = 3.31776e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:13:42.776939   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:14:28.675060   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:15:12.805907   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:15:51.212491   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:16:10.499958   286 solver.cpp:354] Iteration 7700 (0.623044 iter/s, 160.502s/100 iter), 289.9/376.5ep, loss = 2.2783
I0511 16:16:10.499995   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.98052 (* 1 = 1.98052 loss)
I0511 16:16:10.500005   286 sgd_solver.cpp:172] Iteration 7700, lr = 2.79841e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:16:33.372638   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:17:14.421397   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:03.723328   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:47.234839   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:18:51.817420   286 solver.cpp:354] Iteration 7800 (0.619896 iter/s, 161.317s/100 iter), 293.6/376.5ep, loss = 2.10944
I0511 16:18:51.817495   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.15876 (* 1 = 2.15876 loss)
I0511 16:18:51.817521   286 sgd_solver.cpp:172] Iteration 7800, lr = 2.34256e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:19:27.776983   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:20:11.649794   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:20:56.553231   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:21:35.426523   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:21:36.680392   286 solver.cpp:354] Iteration 7900 (0.606565 iter/s, 164.863s/100 iter), 297.4/376.5ep, loss = 2.17191
I0511 16:21:36.680541   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.021 (* 1 = 2.021 loss)
I0511 16:21:36.680604   286 sgd_solver.cpp:172] Iteration 7900, lr = 1.94481e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:22:17.673382   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:23:00.965333   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:23:42.828294   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:24:09.244841   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0511 16:24:09.284216   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0511 16:24:09.359680   286 solver.cpp:637] Iteration 8000, Testing net (#0)
I0511 16:24:25.265329   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:24:25.702939   286 solver.cpp:749] class AP 1: 0.902042
I0511 16:24:25.703631   286 solver.cpp:749] class AP 2: 0.887863
I0511 16:24:25.703860   286 solver.cpp:749] class AP 3: 0.90274
I0511 16:24:25.703871   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897548
I0511 16:24:25.703908   286 solver.cpp:284] Tests completed in 169.023s
I0511 16:24:26.115075   286 solver.cpp:354] Iteration 8000 (0.591634 iter/s, 169.023s/100 iter), 301.2/376.5ep, loss = 2.23565
I0511 16:24:26.115190   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.26163 (* 1 = 2.26163 loss)
I0511 16:24:26.115236   286 sgd_solver.cpp:172] Iteration 8000, lr = 1.6e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:24:30.135054   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:25:14.574992   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:25:56.786598   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:26:41.331065   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:27:00.310946   286 solver.cpp:354] Iteration 8100 (0.648526 iter/s, 154.196s/100 iter), 304.9/376.5ep, loss = 2.14424
I0511 16:27:00.311252   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.96885 (* 1 = 1.96885 loss)
I0511 16:27:00.311309   286 sgd_solver.cpp:172] Iteration 8100, lr = 1.30321e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:27:22.615207   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:28:09.342959   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:28:48.916963   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:29:27.535276   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:29:40.441097   286 solver.cpp:354] Iteration 8200 (0.624493 iter/s, 160.13s/100 iter), 308.7/376.5ep, loss = 2.25428
I0511 16:29:40.441215   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.04646 (* 1 = 2.04646 loss)
I0511 16:29:40.441258   286 sgd_solver.cpp:172] Iteration 8200, lr = 1.04976e-05, m = 0.9, wd = 0.0005, gs = 1
I0511 16:30:13.592989   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:30:55.455257   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:31:36.515295   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:32:20.302894   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:32:20.665565   286 solver.cpp:354] Iteration 8300 (0.62413 iter/s, 160.223s/100 iter), 312.5/376.5ep, loss = 2.29563
I0511 16:32:20.665751   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.40904 (* 1 = 2.40904 loss)
I0511 16:32:20.665818   286 sgd_solver.cpp:172] Iteration 8300, lr = 8.3521e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:33:02.484475   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:33:43.240209   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:34:24.105861   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:35:02.028978   286 solver.cpp:354] Iteration 8400 (0.619724 iter/s, 161.362s/100 iter), 316.2/376.5ep, loss = 2.28401
I0511 16:35:02.029556   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.86963 (* 1 = 1.86963 loss)
I0511 16:35:02.029690   286 sgd_solver.cpp:172] Iteration 8400, lr = 6.5536e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:35:11.009938   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:35:52.894030   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:36:35.760008   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:37:17.757217   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:37:42.382684   286 solver.cpp:354] Iteration 8500 (0.623625 iter/s, 160.353s/100 iter), 320/376.5ep, loss = 2.27398
I0511 16:37:42.382864   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.18477 (* 1 = 2.18477 loss)
I0511 16:37:42.382912   286 sgd_solver.cpp:172] Iteration 8500, lr = 5.0625e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:38:03.450585   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:38:43.457687   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:39:22.327143   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:40:07.794312   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:40:20.838984   286 solver.cpp:354] Iteration 8600 (0.631092 iter/s, 158.456s/100 iter), 323.8/376.5ep, loss = 2.39464
I0511 16:40:20.839390   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.02272 (* 1 = 2.02272 loss)
I0511 16:40:20.839588   286 sgd_solver.cpp:172] Iteration 8600, lr = 3.8416e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:40:46.781332   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:41:30.368785   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:10.501636   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:57.561727   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:42:58.886313   286 solver.cpp:354] Iteration 8700 (0.632725 iter/s, 158.047s/100 iter), 327.5/376.5ep, loss = 2.13683
I0511 16:42:58.886471   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.16945 (* 1 = 2.16945 loss)
I0511 16:42:58.886520   286 sgd_solver.cpp:172] Iteration 8700, lr = 2.8561e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:43:39.298666   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:44:19.123512   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:45:01.745896   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:45:34.876058   286 solver.cpp:354] Iteration 8800 (0.64107 iter/s, 155.989s/100 iter), 331.3/376.5ep, loss = 2.40312
I0511 16:45:34.876492   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.04759 (* 1 = 2.04759 loss)
I0511 16:45:34.876618   286 sgd_solver.cpp:172] Iteration 8800, lr = 2.0736e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:45:43.850401   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:46:22.865329   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:47:11.569540   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:47:57.133761   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:48:22.820315   286 solver.cpp:354] Iteration 8900 (0.595438 iter/s, 167.944s/100 iter), 335.1/376.5ep, loss = 2.17821
I0511 16:48:22.820353   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.20312 (* 1 = 2.20312 loss)
I0511 16:48:22.820361   286 sgd_solver.cpp:172] Iteration 8900, lr = 1.4641e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:48:36.715961   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:49:20.383297   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:00.097582   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:42.608451   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:50:58.479560   286 solver.cpp:354] Iteration 9000 (0.642431 iter/s, 155.659s/100 iter), 338.8/376.5ep, loss = 2.18224
I0511 16:50:58.479645   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.73082 (* 1 = 1.73082 loss)
I0511 16:50:58.479669   286 sgd_solver.cpp:172] Iteration 9000, lr = 1e-06, m = 0.9, wd = 0.0005, gs = 1
I0511 16:51:21.580065   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:52:03.864205   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:52:43.513609   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:53:28.449131   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:53:34.075006   286 solver.cpp:354] Iteration 9100 (0.642694 iter/s, 155.595s/100 iter), 342.6/376.5ep, loss = 2.19036
I0511 16:53:34.075166   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.82579 (* 1 = 1.82579 loss)
I0511 16:53:34.075212   286 sgd_solver.cpp:172] Iteration 9100, lr = 6.56099e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:54:11.565376   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:54:53.837738   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:55:34.529760   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:56:15.418911   286 solver.cpp:354] Iteration 9200 (0.619796 iter/s, 161.343s/100 iter), 346.4/376.5ep, loss = 2.23599
I0511 16:56:15.419200   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.90666 (* 1 = 1.90666 loss)
I0511 16:56:15.419309   286 sgd_solver.cpp:172] Iteration 9200, lr = 4.096e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:56:21.277537   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:57:02.657733   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:57:44.023488   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:58:25.042374   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:58:59.089303   286 solver.cpp:354] Iteration 9300 (0.610986 iter/s, 163.67s/100 iter), 350.1/376.5ep, loss = 2.07393
I0511 16:58:59.089525   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.6072 (* 1 = 1.6072 loss)
I0511 16:58:59.089589   286 sgd_solver.cpp:172] Iteration 9300, lr = 2.401e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 16:59:13.115172   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 16:59:52.247352   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:00:35.480310   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:01:18.659698   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:01:37.360014   286 solver.cpp:354] Iteration 9400 (0.631831 iter/s, 158.27s/100 iter), 353.9/376.5ep, loss = 2.28215
I0511 17:01:37.360424   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.17892 (* 1 = 2.17892 loss)
I0511 17:01:37.360540   286 sgd_solver.cpp:172] Iteration 9400, lr = 1.296e-07, m = 0.9, wd = 0.0005, gs = 1
I0511 17:02:00.776641   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:02:43.891194   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:03:26.324103   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:04:05.488236   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:04:14.910394   286 solver.cpp:354] Iteration 9500 (0.634718 iter/s, 157.55s/100 iter), 357.6/376.5ep, loss = 2.12974
I0511 17:04:14.910800   286 solver.cpp:378]     Train net output #0: mbox_loss = 3.07233 (* 1 = 3.07233 loss)
I0511 17:04:14.910974   286 sgd_solver.cpp:172] Iteration 9500, lr = 6.25001e-08, m = 0.9, wd = 0.0005, gs = 1
I0511 17:04:47.470029   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:05:35.667562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:06:13.618160   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:06:52.747225   286 solver.cpp:354] Iteration 9600 (0.633562 iter/s, 157.838s/100 iter), 361.4/376.5ep, loss = 2.39994
I0511 17:06:52.747494   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.44933 (* 1 = 2.44933 loss)
I0511 17:06:52.747506   286 sgd_solver.cpp:172] Iteration 9600, lr = 2.56001e-08, m = 0.9, wd = 0.0005, gs = 1
I0511 17:06:53.927964   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:07:40.285562   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:08:23.153424   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:09:04.857872   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:09:38.679633   286 solver.cpp:354] Iteration 9700 (0.602652 iter/s, 165.933s/100 iter), 365.2/376.5ep, loss = 2.32112
I0511 17:09:38.680035   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.88492 (* 1 = 1.88492 loss)
I0511 17:09:38.680127   286 sgd_solver.cpp:172] Iteration 9700, lr = 8.09997e-09, m = 0.9, wd = 0.0005, gs = 1
I0511 17:09:45.193013   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:10:33.109450   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:11:11.886011   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:11:54.116119   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:12:16.107566   286 solver.cpp:354] Iteration 9800 (0.63521 iter/s, 157.428s/100 iter), 368.9/376.5ep, loss = 2.38863
I0511 17:12:16.107741   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.29323 (* 1 = 2.29323 loss)
I0511 17:12:16.107795   286 sgd_solver.cpp:172] Iteration 9800, lr = 1.59999e-09, m = 0.9, wd = 0.0005, gs = 1
I0511 17:12:40.061962   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:13:19.355566   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:13:57.506289   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:14:42.720100   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:14:53.041469   286 solver.cpp:354] Iteration 9900 (0.63721 iter/s, 156.934s/100 iter), 372.7/376.5ep, loss = 2.44318
I0511 17:14:53.041887   286 solver.cpp:378]     Train net output #0: mbox_loss = 2.37752 (* 1 = 2.37752 loss)
I0511 17:14:53.042093   286 sgd_solver.cpp:172] Iteration 9900, lr = 9.99996e-11, m = 0.9, wd = 0.0005, gs = 1
I0511 17:15:22.521019   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:16:04.028110   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:16:46.902962   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:26.762310   286 solver.cpp:354] Iteration 9999 (0.644024 iter/s, 153.721s/99 iter), 376.4/376.5ep, loss = 2.18223
I0511 17:17:26.762776   286 solver.cpp:378]     Train net output #0: mbox_loss = 1.80593 (* 1 = 1.80593 loss)
I0511 17:17:26.762920   286 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 17:17:26.830179   286 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0511 17:17:27.047933   286 solver.cpp:503] Iteration 10000, loss = 2.2178
I0511 17:17:27.048238   286 solver.cpp:637] Iteration 10000, Testing net (#0)
I0511 17:17:27.323174   292 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:41.535437   329 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:17:42.039341   286 solver.cpp:749] class AP 1: 0.902412
I0511 17:17:42.040043   286 solver.cpp:749] class AP 2: 0.8876
I0511 17:17:42.040272   286 solver.cpp:749] class AP 3: 0.902822
I0511 17:17:42.040278   286 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897611
I0511 17:17:42.040299   286 caffe.cpp:268] Solver performance on device 0: 0.4179 * 16 = 13.37 img/sec (10000 itr in 2.393e+04 sec)
I0511 17:17:42.040307   286 caffe.cpp:271] Optimization Done in 6h 39m 50s
caffe.bin: ../nptl/pthread_mutex_lock.c:115: __pthread_mutex_lock: Assertion `mutex->__data.__owner == 0' failed.
*** Aborted at 1589217462 (unix time) try "date -d @1589217462" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x11e) received by PID 286 (TID 0x7fef47fff700) from PID 286; stack trace: ***
    @     0x7ff19793bf20 (unknown)
    @     0x7ff19793be97 gsignal
    @     0x7ff19793d801 abort
    @     0x7ff19792d39a (unknown)
    @     0x7ff19792d412 __assert_fail
    @     0x7ff1976e8208 __GI___pthread_mutex_lock
    @     0x7ff1994ec128 boost::mutex::lock()
    @     0x7ff1994ed020 boost::unique_lock<>::lock()
    @     0x7ff19998489f caffe::BlockingQueue<>::push()
    @     0x7ff19956b49e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7ff1995a5476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7ff1995197ee caffe::InternalThread::entry()
    @     0x7ff19951b54b boost::detail::thread_data<>::run()
    @     0x7ff198b537ee thread_proxy
    @     0x7ff1976e56db start_thread
    @     0x7ff197a1e88f clone
    @                0x0 (unknown)
I0511 17:17:42.483932   333 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 17:17:42 2020
I0511 17:17:42.736526   333 caffe.cpp:904] CuDNN version: 7605
I0511 17:17:42.736531   333 caffe.cpp:905] CuBLAS version: 10202
I0511 17:17:42.736534   333 caffe.cpp:906] CUDA version: 10020
I0511 17:17:42.736536   333 caffe.cpp:907] CUDA driver version: 10020
I0511 17:17:42.736539   333 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/solver.prototxt
[3]: --weights=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
[4]: --gpu
[5]: 0
I0511 17:17:42.757985   333 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 17:17:42.758013   333 gpu_memory.cpp:107] Total memory: 16900227072, Free: 12089163776, dev_info[0]: total=16900227072 free=12089163776
I0511 17:17:42.758211   333 caffe.cpp:226] Using GPUs 0
I0511 17:17:42.758337   333 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 17:17:42.758400   333 solver.cpp:41] Solver data type: FLOAT
I0511 17:17:42.766357   333 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/test.prototxt"
test_iter: 107
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 10000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 1e-05
snapshot: 1000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 2
type: "Adam"
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 17:17:42.766574   333 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/train.prototxt
I0511 17:17:42.768115   333 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 16
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 17:17:42.769078   333 net.cpp:110] Using FLOAT as default forward math type
I0511 17:17:42.769096   333 net.cpp:116] Using FLOAT as default backward math type
I0511 17:17:42.769106   333 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 17:17:42.769114   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.769229   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:42.769637   338 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 17:17:42.769660   333 net.cpp:200] Created Layer data (0)
I0511 17:17:42.769673   333 net.cpp:542] data -> data
I0511 17:17:42.769706   333 net.cpp:542] data -> label
I0511 17:17:42.769731   333 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 16
I0511 17:17:42.769804   333 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 17:17:42.770242   339 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.770406   340 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.770601   341 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.772722   333 annotated_data_layer.cpp:105] output data size: 16,3,320,768
I0511 17:17:42.773005   333 annotated_data_layer.cpp:150] [0] Output data size: 16, 3, 320, 768
I0511 17:17:42.773077   333 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 17:17:42.773510   343 data_layer.cpp:105] [0] Parser threads: 4
I0511 17:17:42.773526   343 data_layer.cpp:107] [0] Transformer threads: 4
I0511 17:17:42.773751   342 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.774191   333 net.cpp:260] Setting up data
I0511 17:17:42.774716   333 net.cpp:267] TRAIN Top shape for layer 0 'data' 16 3 320 768 (11796480)
I0511 17:17:42.774771   333 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 17:17:42.774788   333 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 17:17:42.774813   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.774847   333 net.cpp:200] Created Layer data_data_0_split (1)
I0511 17:17:42.774859   333 net.cpp:572] data_data_0_split <- data
I0511 17:17:42.774879   333 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 17:17:42.774889   333 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 17:17:42.774895   333 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 17:17:42.774907   333 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 17:17:42.774911   333 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 17:17:42.774916   333 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 17:17:42.774920   333 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 17:17:42.775038   333 net.cpp:260] Setting up data_data_0_split
I0511 17:17:42.775045   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775054   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775066   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775074   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775085   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775091   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775097   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775127   333 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 17:17:42.775148   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.775172   333 net.cpp:200] Created Layer data/bias (2)
I0511 17:17:42.775185   333 net.cpp:572] data/bias <- data_data_0_split_0
I0511 17:17:42.775193   333 net.cpp:542] data/bias -> data/bias
I0511 17:17:42.775354   333 net.cpp:260] Setting up data/bias
I0511 17:17:42.775384   333 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 16 3 320 768 (11796480)
I0511 17:17:42.775413   333 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 17:17:42.775437   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.775507   333 net.cpp:200] Created Layer conv1a (3)
I0511 17:17:42.775518   333 net.cpp:572] conv1a <- data/bias
I0511 17:17:42.775527   333 net.cpp:542] conv1a -> conv1a
I0511 17:17:46.122023   333 net.cpp:260] Setting up conv1a
I0511 17:17:46.122062   333 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 16 32 160 384 (31457280)
I0511 17:17:46.122100   333 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 17:17:46.122148   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122175   333 net.cpp:200] Created Layer conv1a/bn (4)
I0511 17:17:46.122185   333 net.cpp:572] conv1a/bn <- conv1a
I0511 17:17:46.122195   333 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 17:17:46.122743   333 net.cpp:260] Setting up conv1a/bn
I0511 17:17:46.122750   333 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 16 32 160 384 (31457280)
I0511 17:17:46.122772   333 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 17:17:46.122779   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122790   333 net.cpp:200] Created Layer conv1a/relu (5)
I0511 17:17:46.122797   333 net.cpp:572] conv1a/relu <- conv1a
I0511 17:17:46.122803   333 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 17:17:46.122834   333 net.cpp:260] Setting up conv1a/relu
I0511 17:17:46.122840   333 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 16 32 160 384 (31457280)
I0511 17:17:46.122849   333 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 17:17:46.122856   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122875   333 net.cpp:200] Created Layer conv1b (6)
I0511 17:17:46.122880   333 net.cpp:572] conv1b <- conv1a
I0511 17:17:46.122887   333 net.cpp:542] conv1b -> conv1b
I0511 17:17:46.123528   333 net.cpp:260] Setting up conv1b
I0511 17:17:46.123538   333 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 16 32 160 384 (31457280)
I0511 17:17:46.123555   333 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 17:17:46.123562   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.123574   333 net.cpp:200] Created Layer conv1b/bn (7)
I0511 17:17:46.123580   333 net.cpp:572] conv1b/bn <- conv1b
I0511 17:17:46.123587   333 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 17:17:46.124027   333 net.cpp:260] Setting up conv1b/bn
I0511 17:17:46.124032   333 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 16 32 160 384 (31457280)
I0511 17:17:46.124048   333 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 17:17:46.124055   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124064   333 net.cpp:200] Created Layer conv1b/relu (8)
I0511 17:17:46.124070   333 net.cpp:572] conv1b/relu <- conv1b
I0511 17:17:46.124078   333 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 17:17:46.124085   333 net.cpp:260] Setting up conv1b/relu
I0511 17:17:46.124090   333 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 16 32 160 384 (31457280)
I0511 17:17:46.124100   333 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 17:17:46.124106   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124122   333 net.cpp:200] Created Layer pool1 (9)
I0511 17:17:46.124127   333 net.cpp:572] pool1 <- conv1b
I0511 17:17:46.124135   333 net.cpp:542] pool1 -> pool1
I0511 17:17:46.124222   333 net.cpp:260] Setting up pool1
I0511 17:17:46.124228   333 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 16 32 80 192 (7864320)
I0511 17:17:46.124238   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 17:17:46.124244   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124260   333 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 17:17:46.124266   333 net.cpp:572] res2a_branch2a <- pool1
I0511 17:17:46.124274   333 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 17:17:46.125957   333 net.cpp:260] Setting up res2a_branch2a
I0511 17:17:46.125972   333 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 16 64 80 192 (15728640)
I0511 17:17:46.125991   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.126017   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126030   333 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 17:17:46.126037   333 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 17:17:46.126044   333 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 17:17:46.126466   333 net.cpp:260] Setting up res2a_branch2a/bn
I0511 17:17:46.126472   333 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 16 64 80 192 (15728640)
I0511 17:17:46.126488   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.126497   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126505   333 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 17:17:46.126511   333 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 17:17:46.126518   333 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 17:17:46.126526   333 net.cpp:260] Setting up res2a_branch2a/relu
I0511 17:17:46.126531   333 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 16 64 80 192 (15728640)
I0511 17:17:46.126541   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 17:17:46.126547   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126562   333 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 17:17:46.126569   333 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 17:17:46.126574   333 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 17:17:46.127110   333 net.cpp:260] Setting up res2a_branch2b
I0511 17:17:46.127117   333 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 16 64 80 192 (15728640)
I0511 17:17:46.127130   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.127136   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127146   333 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 17:17:46.127152   333 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 17:17:46.127159   333 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 17:17:46.127585   333 net.cpp:260] Setting up res2a_branch2b/bn
I0511 17:17:46.127591   333 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 16 64 80 192 (15728640)
I0511 17:17:46.127606   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.127612   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127620   333 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 17:17:46.127627   333 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 17:17:46.127633   333 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 17:17:46.127641   333 net.cpp:260] Setting up res2a_branch2b/relu
I0511 17:17:46.127647   333 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 16 64 80 192 (15728640)
I0511 17:17:46.127656   333 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 17:17:46.127662   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127673   333 net.cpp:200] Created Layer pool2 (16)
I0511 17:17:46.127679   333 net.cpp:572] pool2 <- res2a_branch2b
I0511 17:17:46.127686   333 net.cpp:542] pool2 -> pool2
I0511 17:17:46.127739   333 net.cpp:260] Setting up pool2
I0511 17:17:46.127745   333 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 16 64 40 96 (3932160)
I0511 17:17:46.127755   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 17:17:46.127761   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127777   333 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 17:17:46.127784   333 net.cpp:572] res3a_branch2a <- pool2
I0511 17:17:46.127802   333 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 17:17:46.129876   333 net.cpp:260] Setting up res3a_branch2a
I0511 17:17:46.129926   333 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 16 128 40 96 (7864320)
I0511 17:17:46.129973   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.130007   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.130048   333 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 17:17:46.130082   333 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 17:17:46.130118   333 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 17:17:46.130630   333 net.cpp:260] Setting up res3a_branch2a/bn
I0511 17:17:46.130673   333 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 16 128 40 96 (7864320)
I0511 17:17:46.130725   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.130760   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.130800   333 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 17:17:46.130837   333 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 17:17:46.130874   333 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 17:17:46.130911   333 net.cpp:260] Setting up res3a_branch2a/relu
I0511 17:17:46.130947   333 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 16 128 40 96 (7864320)
I0511 17:17:46.130987   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 17:17:46.131027   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.131072   333 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 17:17:46.131108   333 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 17:17:46.131144   333 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 17:17:46.132377   333 net.cpp:260] Setting up res3a_branch2b
I0511 17:17:46.132427   333 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 16 128 40 96 (7864320)
I0511 17:17:46.132472   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.132508   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.132548   333 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 17:17:46.132583   333 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 17:17:46.132620   333 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 17:17:46.133131   333 net.cpp:260] Setting up res3a_branch2b/bn
I0511 17:17:46.133175   333 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 16 128 40 96 (7864320)
I0511 17:17:46.133224   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.133258   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133301   333 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 17:17:46.133338   333 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 17:17:46.133375   333 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 17:17:46.133415   333 net.cpp:260] Setting up res3a_branch2b/relu
I0511 17:17:46.133450   333 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 16 128 40 96 (7864320)
I0511 17:17:46.133488   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.133524   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133563   333 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 17:17:46.133597   333 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 17:17:46.133633   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.133675   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.133765   333 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 17:17:46.133800   333 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 17:17:46.133842   333 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 17:17:46.133882   333 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 17:17:46.133918   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133956   333 net.cpp:200] Created Layer pool3 (24)
I0511 17:17:46.133992   333 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.134028   333 net.cpp:542] pool3 -> pool3
I0511 17:17:46.134122   333 net.cpp:260] Setting up pool3
I0511 17:17:46.134166   333 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 16 128 20 48 (1966080)
I0511 17:17:46.134207   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 17:17:46.134240   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.134284   333 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 17:17:46.134320   333 net.cpp:572] res4a_branch2a <- pool3
I0511 17:17:46.134356   333 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 17:17:46.175707   333 net.cpp:260] Setting up res4a_branch2a
I0511 17:17:46.175745   333 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 16 256 20 48 (3932160)
I0511 17:17:46.175776   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.175786   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.175804   333 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 17:17:46.175814   333 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 17:17:46.175825   333 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 17:17:46.176281   333 net.cpp:260] Setting up res4a_branch2a/bn
I0511 17:17:46.176290   333 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 16 256 20 48 (3932160)
I0511 17:17:46.176306   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.176314   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.176324   333 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 17:17:46.176331   333 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 17:17:46.176337   333 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 17:17:46.176347   333 net.cpp:260] Setting up res4a_branch2a/relu
I0511 17:17:46.176353   333 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 16 256 20 48 (3932160)
I0511 17:17:46.176362   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 17:17:46.176368   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.176388   333 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 17:17:46.176394   333 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 17:17:46.176401   333 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 17:17:46.180006   333 net.cpp:260] Setting up res4a_branch2b
I0511 17:17:46.180019   333 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 16 256 20 48 (3932160)
I0511 17:17:46.180035   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.180042   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180054   333 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 17:17:46.180061   333 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 17:17:46.180068   333 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 17:17:46.180480   333 net.cpp:260] Setting up res4a_branch2b/bn
I0511 17:17:46.180521   333 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 16 256 20 48 (3932160)
I0511 17:17:46.180537   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.180544   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180553   333 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 17:17:46.180559   333 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 17:17:46.180565   333 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 17:17:46.180574   333 net.cpp:260] Setting up res4a_branch2b/relu
I0511 17:17:46.180580   333 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 16 256 20 48 (3932160)
I0511 17:17:46.180589   333 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 17:17:46.180596   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180608   333 net.cpp:200] Created Layer pool4 (31)
I0511 17:17:46.180613   333 net.cpp:572] pool4 <- res4a_branch2b
I0511 17:17:46.180619   333 net.cpp:542] pool4 -> pool4
I0511 17:17:46.180680   333 net.cpp:260] Setting up pool4
I0511 17:17:46.180685   333 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 16 256 10 24 (983040)
I0511 17:17:46.180696   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 17:17:46.180701   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180721   333 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 17:17:46.180727   333 net.cpp:572] res5a_branch2a <- pool4
I0511 17:17:46.180733   333 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 17:17:46.256343   333 net.cpp:260] Setting up res5a_branch2a
I0511 17:17:46.256378   333 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 16 512 10 24 (1966080)
I0511 17:17:46.256412   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.256420   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.256439   333 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 17:17:46.256449   333 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 17:17:46.256461   333 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 17:17:46.256944   333 net.cpp:260] Setting up res5a_branch2a/bn
I0511 17:17:46.256951   333 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 16 512 10 24 (1966080)
I0511 17:17:46.256968   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.256976   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.256986   333 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 17:17:46.256992   333 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 17:17:46.256999   333 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 17:17:46.257009   333 net.cpp:260] Setting up res5a_branch2a/relu
I0511 17:17:46.257015   333 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 16 512 10 24 (1966080)
I0511 17:17:46.257025   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 17:17:46.257030   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.257050   333 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 17:17:46.257056   333 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 17:17:46.257063   333 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 17:17:46.273281   333 net.cpp:260] Setting up res5a_branch2b
I0511 17:17:46.273499   333 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 16 512 10 24 (1966080)
I0511 17:17:46.273685   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.273844   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.274015   333 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 17:17:46.274168   333 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 17:17:46.274322   333 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 17:17:46.274919   333 net.cpp:260] Setting up res5a_branch2b/bn
I0511 17:17:46.275095   333 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 16 512 10 24 (1966080)
I0511 17:17:46.275266   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.275418   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.275573   333 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 17:17:46.275727   333 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 17:17:46.275877   333 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 17:17:46.276032   333 net.cpp:260] Setting up res5a_branch2b/relu
I0511 17:17:46.276187   333 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 16 512 10 24 (1966080)
I0511 17:17:46.276351   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.276502   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.276659   333 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 17:17:46.276810   333 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 17:17:46.276963   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.277122   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.277326   333 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 17:17:46.277483   333 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 17:17:46.277642   333 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 17:17:46.277799   333 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 17:17:46.277951   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.278107   333 net.cpp:200] Created Layer pool6 (39)
I0511 17:17:46.278259   333 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.278410   333 net.cpp:542] pool6 -> pool6
I0511 17:17:46.278626   333 net.cpp:260] Setting up pool6
I0511 17:17:46.278777   333 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 16 512 5 12 (491520)
I0511 17:17:46.278935   333 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 17:17:46.279088   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.279242   333 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 17:17:46.279392   333 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 17:17:46.279544   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 17:17:46.279698   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 17:17:46.279898   333 net.cpp:260] Setting up pool6_pool6_0_split
I0511 17:17:46.280051   333 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 17:17:46.280210   333 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 17:17:46.280365   333 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 17:17:46.280516   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.280671   333 net.cpp:200] Created Layer pool7 (41)
I0511 17:17:46.280822   333 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 17:17:46.280973   333 net.cpp:542] pool7 -> pool7
I0511 17:17:46.281193   333 net.cpp:260] Setting up pool7
I0511 17:17:46.281378   333 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 16 512 3 6 (147456)
I0511 17:17:46.281563   333 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 17:17:46.281735   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.281909   333 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 17:17:46.282081   333 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 17:17:46.282254   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 17:17:46.282430   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 17:17:46.282661   333 net.cpp:260] Setting up pool7_pool7_0_split
I0511 17:17:46.282829   333 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 17:17:46.283004   333 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 17:17:46.283179   333 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 17:17:46.283347   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.283520   333 net.cpp:200] Created Layer pool8 (43)
I0511 17:17:46.283691   333 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 17:17:46.283859   333 net.cpp:542] pool8 -> pool8
I0511 17:17:46.284116   333 net.cpp:260] Setting up pool8
I0511 17:17:46.284284   333 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 16 512 2 3 (49152)
I0511 17:17:46.284466   333 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 17:17:46.284636   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.284812   333 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 17:17:46.284979   333 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 17:17:46.285153   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 17:17:46.285328   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 17:17:46.285548   333 net.cpp:260] Setting up pool8_pool8_0_split
I0511 17:17:46.285725   333 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 17:17:46.285898   333 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 17:17:46.286070   333 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 17:17:46.286240   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.286414   333 net.cpp:200] Created Layer pool9 (45)
I0511 17:17:46.286584   333 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 17:17:46.286756   333 net.cpp:542] pool9 -> pool9
I0511 17:17:46.287001   333 net.cpp:260] Setting up pool9
I0511 17:17:46.287168   333 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 16 512 1 2 (16384)
I0511 17:17:46.287345   333 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 17:17:46.287513   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.287696   333 net.cpp:200] Created Layer ctx_output1 (46)
I0511 17:17:46.287868   333 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.288038   333 net.cpp:542] ctx_output1 -> ctx_output1
I0511 17:17:46.289332   333 net.cpp:260] Setting up ctx_output1
I0511 17:17:46.289530   333 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 16 256 40 96 (15728640)
I0511 17:17:46.289716   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 17:17:46.289887   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.290069   333 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 17:17:46.290246   333 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 17:17:46.290421   333 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 17:17:46.290597   333 net.cpp:260] Setting up ctx_output1/relu
I0511 17:17:46.290773   333 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 16 256 40 96 (15728640)
I0511 17:17:46.290959   333 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 17:17:46.291131   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.291304   333 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 17:17:46.291476   333 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 17:17:46.291646   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.291819   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.291990   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.292234   333 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 17:17:46.292402   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292578   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292752   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292927   333 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 17:17:46.293088   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.293251   333 net.cpp:200] Created Layer ctx_output2 (49)
I0511 17:17:46.293371   333 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.293419   333 net.cpp:542] ctx_output2 -> ctx_output2
I0511 17:17:46.296434   333 net.cpp:260] Setting up ctx_output2
I0511 17:17:46.296499   333 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 16 256 10 24 (983040)
I0511 17:17:46.296617   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 17:17:46.296658   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.296733   333 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 17:17:46.296802   333 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 17:17:46.296875   333 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 17:17:46.296945   333 net.cpp:260] Setting up ctx_output2/relu
I0511 17:17:46.297011   333 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 16 256 10 24 (983040)
I0511 17:17:46.297088   333 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 17:17:46.297153   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.297245   333 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 17:17:46.297387   333 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 17:17:46.297453   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.297593   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.297670   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.297823   333 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 17:17:46.297894   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.297979   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.298053   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.298120   333 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 17:17:46.298198   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.298290   333 net.cpp:200] Created Layer ctx_output3 (52)
I0511 17:17:46.298365   333 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 17:17:46.298434   333 net.cpp:542] ctx_output3 -> ctx_output3
I0511 17:17:46.302407   333 net.cpp:260] Setting up ctx_output3
I0511 17:17:46.302513   333 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 16 256 5 12 (245760)
I0511 17:17:46.302609   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 17:17:46.302685   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.302762   333 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 17:17:46.302839   333 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 17:17:46.302919   333 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 17:17:46.302999   333 net.cpp:260] Setting up ctx_output3/relu
I0511 17:17:46.303073   333 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 16 256 5 12 (245760)
I0511 17:17:46.303153   333 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 17:17:46.303232   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.303316   333 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 17:17:46.303390   333 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 17:17:46.303472   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.303550   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.303634   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.303786   333 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 17:17:46.303864   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.303943   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.304023   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.304101   333 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 17:17:46.304180   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.304265   333 net.cpp:200] Created Layer ctx_output4 (55)
I0511 17:17:46.304340   333 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 17:17:46.304420   333 net.cpp:542] ctx_output4 -> ctx_output4
I0511 17:17:46.307709   333 net.cpp:260] Setting up ctx_output4
I0511 17:17:46.307812   333 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 16 256 3 6 (73728)
I0511 17:17:46.307904   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 17:17:46.307978   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.308061   333 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 17:17:46.308137   333 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 17:17:46.308216   333 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 17:17:46.308295   333 net.cpp:260] Setting up ctx_output4/relu
I0511 17:17:46.308369   333 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 16 256 3 6 (73728)
I0511 17:17:46.308449   333 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 17:17:46.308526   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.308609   333 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 17:17:46.308683   333 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 17:17:46.308764   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.308847   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.308948   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.309115   333 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 17:17:46.309195   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309274   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309371   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309451   333 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 17:17:46.309526   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.309612   333 net.cpp:200] Created Layer ctx_output5 (58)
I0511 17:17:46.309682   333 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 17:17:46.309767   333 net.cpp:542] ctx_output5 -> ctx_output5
I0511 17:17:46.312775   333 net.cpp:260] Setting up ctx_output5
I0511 17:17:46.312862   333 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 16 256 2 3 (24576)
I0511 17:17:46.312953   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 17:17:46.313024   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.313097   333 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 17:17:46.313169   333 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 17:17:46.313246   333 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 17:17:46.313331   333 net.cpp:260] Setting up ctx_output5/relu
I0511 17:17:46.313400   333 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 16 256 2 3 (24576)
I0511 17:17:46.313479   333 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 17:17:46.313540   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.313621   333 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 17:17:46.313694   333 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 17:17:46.313762   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.313841   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.313927   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.314071   333 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 17:17:46.314150   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314225   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314298   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314370   333 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 17:17:46.314440   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.314528   333 net.cpp:200] Created Layer ctx_output6 (61)
I0511 17:17:46.314599   333 net.cpp:572] ctx_output6 <- pool9
I0511 17:17:46.314674   333 net.cpp:542] ctx_output6 -> ctx_output6
I0511 17:17:46.317685   333 net.cpp:260] Setting up ctx_output6
I0511 17:17:46.317775   333 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 16 256 1 2 (8192)
I0511 17:17:46.317857   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 17:17:46.317929   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.318014   333 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 17:17:46.318091   333 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 17:17:46.318174   333 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 17:17:46.318248   333 net.cpp:260] Setting up ctx_output6/relu
I0511 17:17:46.318317   333 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 16 256 1 2 (8192)
I0511 17:17:46.318403   333 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 17:17:46.318475   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.318548   333 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 17:17:46.318629   333 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 17:17:46.318722   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.318806   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.318883   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.319042   333 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 17:17:46.319120   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319200   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319278   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319357   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.319449   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.319545   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 17:17:46.319627   333 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.319703   333 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 17:17:46.320255   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 17:17:46.320339   333 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 16 16 40 96 (983040)
I0511 17:17:46.320430   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.320505   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.320593   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 17:17:46.320681   333 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 17:17:46.320758   333 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.320979   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.321058   333 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 16 40 96 16 (983040)
I0511 17:17:46.321138   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.321214   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.321302   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 17:17:46.321380   333 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.321457   333 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.325834   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.325943   333 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 16 61440 (983040)
I0511 17:17:46.326046   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.326138   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.326246   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 17:17:46.326328   333 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.326429   333 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 17:17:46.326999   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 17:17:46.327086   333 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 16 16 40 96 (983040)
I0511 17:17:46.327173   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.327250   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.327335   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 17:17:46.327410   333 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 17:17:46.327492   333 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.335727   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.335990   333 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 16 40 96 16 (983040)
I0511 17:17:46.336088   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.336182   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.336268   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 17:17:46.336354   333 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.336447   333 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.343909   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.344871   333 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 16 61440 (983040)
I0511 17:17:46.344986   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.345072   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.345161   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 17:17:46.345257   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.345360   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 17:17:46.345443   333 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 17:17:46.345633   333 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 17:17:46.345713   333 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 17:17:46.345801   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.345878   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.345978   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 17:17:46.346076   333 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.346156   333 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 17:17:46.346805   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 17:17:46.347357   333 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 16 24 10 24 (92160)
I0511 17:17:46.347455   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.347548   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.347630   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 17:17:46.347713   333 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 17:17:46.347791   333 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348054   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348222   333 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 16 10 24 24 (92160)
I0511 17:17:46.348330   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.348410   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.348500   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 17:17:46.348587   333 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348672   333 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.350100   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.350575   333 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 16 5760 (92160)
I0511 17:17:46.350688   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.350772   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.350877   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 17:17:46.350975   333 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.351064   333 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 17:17:46.351673   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 17:17:46.352223   333 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 16 24 10 24 (92160)
I0511 17:17:46.352342   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.352463   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.352581   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 17:17:46.352697   333 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 17:17:46.352807   333 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353041   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353266   333 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 16 10 24 24 (92160)
I0511 17:17:46.353411   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.353531   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.353646   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 17:17:46.353762   333 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353873   333 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.354074   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.354274   333 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 16 5760 (92160)
I0511 17:17:46.354390   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.354501   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.354614   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 17:17:46.354727   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.354837   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 17:17:46.354949   333 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 17:17:46.355087   333 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 17:17:46.355221   333 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 17:17:46.355337   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.355453   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.355588   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 17:17:46.355705   333 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.355818   333 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 17:17:46.356400   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 17:17:46.356990   333 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 16 24 5 12 (23040)
I0511 17:17:46.357127   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.357242   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.357360   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 17:17:46.357477   333 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 17:17:46.357589   333 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.357828   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.358063   333 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 16 5 12 24 (23040)
I0511 17:17:46.358180   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.358289   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.358400   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 17:17:46.358510   333 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.358626   333 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.358808   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.358988   333 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 16 1440 (23040)
I0511 17:17:46.359100   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.359208   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.359326   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 17:17:46.359443   333 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.359552   333 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 17:17:46.360110   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 17:17:46.360672   333 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 16 24 5 12 (23040)
I0511 17:17:46.360793   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.360899   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.361006   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 17:17:46.361110   333 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 17:17:46.361210   333 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.361441   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.361666   333 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 16 5 12 24 (23040)
I0511 17:17:46.361775   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.361877   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.361981   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 17:17:46.362084   333 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.362185   333 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.362362   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.362540   333 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 16 1440 (23040)
I0511 17:17:46.362663   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.362769   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.362874   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 17:17:46.362978   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.363080   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 17:17:46.363184   333 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 17:17:46.363307   333 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 17:17:46.363427   333 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 17:17:46.363533   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.363633   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.363744   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 17:17:46.363853   333 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.363955   333 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 17:17:46.364495   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 17:17:46.365056   333 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 16 24 3 6 (6912)
I0511 17:17:46.365180   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.365284   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.365411   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 17:17:46.365527   333 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 17:17:46.365638   333 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.365862   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.366091   333 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 16 3 6 24 (6912)
I0511 17:17:46.366204   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.366312   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.366422   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 17:17:46.366533   333 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.366642   333 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.366822   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.367003   333 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 16 432 (6912)
I0511 17:17:46.367116   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.367224   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.367344   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 17:17:46.367460   333 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.367571   333 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 17:17:46.368165   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 17:17:46.368764   333 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 16 24 3 6 (6912)
I0511 17:17:46.368886   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.368999   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.369117   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 17:17:46.369246   333 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 17:17:46.369361   333 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.369580   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.369792   333 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 16 3 6 24 (6912)
I0511 17:17:46.369884   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.369961   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.370043   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 17:17:46.370124   333 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.370205   333 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.370362   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.370514   333 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 16 432 (6912)
I0511 17:17:46.370597   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.370679   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.370802   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 17:17:46.370884   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.370970   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 17:17:46.371054   333 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 17:17:46.371157   333 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 17:17:46.371258   333 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 17:17:46.371348   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.371428   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.371515   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 17:17:46.371603   333 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.371681   333 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 17:17:46.372193   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 17:17:46.372710   333 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 16 16 2 3 (1536)
I0511 17:17:46.372804   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.372886   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.372970   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 17:17:46.373052   333 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 17:17:46.373131   333 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373344   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373564   333 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 16 2 3 16 (1536)
I0511 17:17:46.373651   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.373728   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.373808   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 17:17:46.373889   333 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373966   333 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.374115   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.374279   333 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 16 96 (1536)
I0511 17:17:46.374363   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.374440   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.374529   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 17:17:46.374616   333 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.374694   333 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 17:17:46.375190   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 17:17:46.375690   333 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 16 16 2 3 (1536)
I0511 17:17:46.375783   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.375865   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.375946   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 17:17:46.376027   333 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 17:17:46.376106   333 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376303   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376507   333 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 16 2 3 16 (1536)
I0511 17:17:46.376598   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.376667   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.376756   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 17:17:46.376835   333 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376914   333 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.377065   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.377203   333 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 16 96 (1536)
I0511 17:17:46.377295   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.377382   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.377460   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 17:17:46.377537   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.377617   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 17:17:46.377692   333 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 17:17:46.377794   333 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 17:17:46.377893   333 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 17:17:46.377979   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.378053   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.378139   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 17:17:46.378226   333 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.378305   333 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 17:17:46.378782   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 17:17:46.379269   333 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 16 16 1 2 (512)
I0511 17:17:46.379365   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.379457   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.379561   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 17:17:46.379647   333 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 17:17:46.379724   333 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.379936   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.380133   333 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 16 1 2 16 (512)
I0511 17:17:46.380224   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.380296   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.380384   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 17:17:46.380460   333 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.380538   333 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.380684   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.380833   333 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 16 32 (512)
I0511 17:17:46.380919   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.380996   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.381093   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 17:17:46.381182   333 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.381261   333 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 17:17:46.381745   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 17:17:46.382225   333 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 16 16 1 2 (512)
I0511 17:17:46.382320   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.382407   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.382488   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 17:17:46.382577   333 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 17:17:46.382658   333 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.382863   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.383052   333 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 16 1 2 16 (512)
I0511 17:17:46.383143   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.383216   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.383301   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 17:17:46.383378   333 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.383451   333 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.383596   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.383738   333 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 16 32 (512)
I0511 17:17:46.383828   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.383901   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.383985   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 17:17:46.384066   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.384145   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 17:17:46.384236   333 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 17:17:46.384351   333 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 17:17:46.384446   333 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 17:17:46.384529   333 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 17:17:46.384611   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.384703   333 net.cpp:200] Created Layer mbox_loc (106)
I0511 17:17:46.384776   333 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.384865   333 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.384944   333 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.385020   333 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.385109   333 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.385180   333 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.385257   333 net.cpp:542] mbox_loc -> mbox_loc
I0511 17:17:46.385378   333 net.cpp:260] Setting up mbox_loc
I0511 17:17:46.385488   333 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 16 69200 (1107200)
I0511 17:17:46.385578   333 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 17:17:46.385649   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.385732   333 net.cpp:200] Created Layer mbox_conf (107)
I0511 17:17:46.385814   333 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.385903   333 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.385974   333 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.386061   333 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.386135   333 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.386215   333 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.386292   333 net.cpp:542] mbox_conf -> mbox_conf
I0511 17:17:46.386404   333 net.cpp:260] Setting up mbox_conf
I0511 17:17:46.386500   333 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 16 69200 (1107200)
I0511 17:17:46.386590   333 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 17:17:46.386662   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.386754   333 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 17:17:46.386826   333 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 17:17:46.386901   333 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 17:17:46.386983   333 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 17:17:46.387054   333 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 17:17:46.387135   333 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 17:17:46.387217   333 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 17:17:46.387295   333 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 17:17:46.387400   333 net.cpp:260] Setting up mbox_priorbox
I0511 17:17:46.387508   333 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 17:17:46.387595   333 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 17:17:46.387672   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.387769   333 net.cpp:200] Created Layer mbox_loss (109)
I0511 17:17:46.387856   333 net.cpp:572] mbox_loss <- mbox_loc
I0511 17:17:46.387936   333 net.cpp:572] mbox_loss <- mbox_conf
I0511 17:17:46.388007   333 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 17:17:46.388093   333 net.cpp:572] mbox_loss <- label
I0511 17:17:46.388170   333 net.cpp:542] mbox_loss -> mbox_loss
I0511 17:17:46.388324   333 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 17:17:46.388486   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.388697   333 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 17:17:46.388896   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.389112   333 net.cpp:260] Setting up mbox_loss
I0511 17:17:46.389319   333 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 17:17:46.389408   333 net.cpp:271]     with loss weight 1
I0511 17:17:46.389535   333 net.cpp:336] mbox_loss needs backward computation.
I0511 17:17:46.389621   333 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 17:17:46.389698   333 net.cpp:336] mbox_conf needs backward computation.
I0511 17:17:46.389788   333 net.cpp:336] mbox_loc needs backward computation.
I0511 17:17:46.389865   333 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.389953   333 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.390030   333 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.390105   333 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 17:17:46.390187   333 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.390262   333 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.390336   333 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 17:17:46.390411   333 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.390496   333 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.390565   333 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.390650   333 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 17:17:46.390725   333 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.390805   333 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.390918   333 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 17:17:46.391003   333 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.391074   333 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.391162   333 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.391235   333 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 17:17:46.391309   333 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.391383   333 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.391458   333 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 17:17:46.391535   333 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.391609   333 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.391690   333 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.391764   333 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 17:17:46.391837   333 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.391918   333 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.391997   333 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 17:17:46.392077   333 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.392154   333 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.392241   333 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.392315   333 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 17:17:46.392407   333 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.392488   333 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.392565   333 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 17:17:46.392642   333 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.392719   333 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.392796   333 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.392872   333 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 17:17:46.392947   333 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.393023   333 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.393100   333 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 17:17:46.393172   333 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 17:17:46.393260   333 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 17:17:46.393344   333 net.cpp:336] ctx_output6 needs backward computation.
I0511 17:17:46.393420   333 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 17:17:46.393499   333 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 17:17:46.393584   333 net.cpp:336] ctx_output5 needs backward computation.
I0511 17:17:46.393656   333 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 17:17:46.393736   333 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 17:17:46.393808   333 net.cpp:336] ctx_output4 needs backward computation.
I0511 17:17:46.393893   333 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 17:17:46.393968   333 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 17:17:46.394044   333 net.cpp:336] ctx_output3 needs backward computation.
I0511 17:17:46.394120   333 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 17:17:46.394201   333 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 17:17:46.394271   333 net.cpp:336] ctx_output2 needs backward computation.
I0511 17:17:46.394356   333 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 17:17:46.394433   333 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 17:17:46.394510   333 net.cpp:336] ctx_output1 needs backward computation.
I0511 17:17:46.394585   333 net.cpp:336] pool9 needs backward computation.
I0511 17:17:46.394663   333 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 17:17:46.394742   333 net.cpp:336] pool8 needs backward computation.
I0511 17:17:46.394829   333 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 17:17:46.394899   333 net.cpp:336] pool7 needs backward computation.
I0511 17:17:46.394975   333 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 17:17:46.395051   333 net.cpp:336] pool6 needs backward computation.
I0511 17:17:46.395128   333 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 17:17:46.395202   333 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 17:17:46.395274   333 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 17:17:46.395359   333 net.cpp:336] res5a_branch2b needs backward computation.
I0511 17:17:46.395434   333 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 17:17:46.395506   333 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 17:17:46.395591   333 net.cpp:336] res5a_branch2a needs backward computation.
I0511 17:17:46.395665   333 net.cpp:336] pool4 needs backward computation.
I0511 17:17:46.395741   333 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 17:17:46.395828   333 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 17:17:46.395921   333 net.cpp:336] res4a_branch2b needs backward computation.
I0511 17:17:46.395993   333 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 17:17:46.396066   333 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 17:17:46.396140   333 net.cpp:336] res4a_branch2a needs backward computation.
I0511 17:17:46.396216   333 net.cpp:336] pool3 needs backward computation.
I0511 17:17:46.396301   333 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 17:17:46.396371   333 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 17:17:46.396445   333 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 17:17:46.396520   333 net.cpp:336] res3a_branch2b needs backward computation.
I0511 17:17:46.396595   333 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 17:17:46.396669   333 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 17:17:46.396742   333 net.cpp:336] res3a_branch2a needs backward computation.
I0511 17:17:46.396816   333 net.cpp:336] pool2 needs backward computation.
I0511 17:17:46.396893   333 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 17:17:46.396967   333 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 17:17:46.397042   333 net.cpp:336] res2a_branch2b needs backward computation.
I0511 17:17:46.397117   333 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 17:17:46.397193   333 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 17:17:46.397269   333 net.cpp:336] res2a_branch2a needs backward computation.
I0511 17:17:46.397357   333 net.cpp:336] pool1 needs backward computation.
I0511 17:17:46.397428   333 net.cpp:336] conv1b/relu needs backward computation.
I0511 17:17:46.397512   333 net.cpp:336] conv1b/bn needs backward computation.
I0511 17:17:46.397586   333 net.cpp:336] conv1b needs backward computation.
I0511 17:17:46.397661   333 net.cpp:336] conv1a/relu needs backward computation.
I0511 17:17:46.397737   333 net.cpp:336] conv1a/bn needs backward computation.
I0511 17:17:46.397811   333 net.cpp:336] conv1a needs backward computation.
I0511 17:17:46.397888   333 net.cpp:338] data/bias does not need backward computation.
I0511 17:17:46.397974   333 net.cpp:338] data_data_0_split does not need backward computation.
I0511 17:17:46.398048   333 net.cpp:338] data does not need backward computation.
I0511 17:17:46.398123   333 net.cpp:380] This network produces output mbox_loss
I0511 17:17:46.398334   333 net.cpp:403] Top memory (TRAIN) required for data: 2411201928 diff: 2411201928
I0511 17:17:46.398552   333 net.cpp:406] Bottom memory (TRAIN) required for data: 2411201920 diff: 2411201920
I0511 17:17:46.398635   333 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 1043431424 diff: 1043431424
I0511 17:17:46.398706   333 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 17:17:46.398787   333 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 17:17:46.398862   333 net.cpp:421] Network initialization done.
I0511 17:17:46.400710   333 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/test.prototxt
I0511 17:17:46.402889   333 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 17:17:46.411530   333 net.cpp:110] Using FLOAT as default forward math type
I0511 17:17:46.412184   333 net.cpp:116] Using FLOAT as default backward math type
I0511 17:17:46.412271   333 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 17:17:46.412355   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.412463   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.412787   333 net.cpp:200] Created Layer data (0)
I0511 17:17:46.413089   333 net.cpp:542] data -> data
I0511 17:17:46.413174   333 net.cpp:542] data -> label
I0511 17:17:46.413264   333 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 17:17:46.413419   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.422325   376 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 17:17:46.424015   333 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 17:17:46.424108   333 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 17:17:46.424162   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.424273   333 net.cpp:260] Setting up data
I0511 17:17:46.424283   333 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 17:17:46.424301   333 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 17:17:46.424311   333 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 17:17:46.424319   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.424331   333 net.cpp:200] Created Layer data_data_0_split (1)
I0511 17:17:46.424337   333 net.cpp:572] data_data_0_split <- data
I0511 17:17:46.424345   333 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 17:17:46.424355   333 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 17:17:46.424366   333 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 17:17:46.424373   333 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 17:17:46.424381   333 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 17:17:46.424387   333 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 17:17:46.424394   333 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 17:17:46.424515   333 net.cpp:260] Setting up data_data_0_split
I0511 17:17:46.424547   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424574   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424603   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424628   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424654   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424679   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424703   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424729   333 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 17:17:46.424751   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.424779   333 net.cpp:200] Created Layer data/bias (2)
I0511 17:17:46.424801   333 net.cpp:572] data/bias <- data_data_0_split_0
I0511 17:17:46.424825   333 net.cpp:542] data/bias -> data/bias
I0511 17:17:46.425046   333 net.cpp:260] Setting up data/bias
I0511 17:17:46.425076   333 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 17:17:46.425113   333 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 17:17:46.425135   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.425170   333 net.cpp:200] Created Layer conv1a (3)
I0511 17:17:46.425192   333 net.cpp:572] conv1a <- data/bias
I0511 17:17:46.425216   333 net.cpp:542] conv1a -> conv1a
I0511 17:17:46.426573   333 net.cpp:260] Setting up conv1a
I0511 17:17:46.426612   333 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 17:17:46.426648   333 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 17:17:46.426671   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.426700   333 net.cpp:200] Created Layer conv1a/bn (4)
I0511 17:17:46.426723   333 net.cpp:572] conv1a/bn <- conv1a
I0511 17:17:46.426746   333 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 17:17:46.427309   333 net.cpp:260] Setting up conv1a/bn
I0511 17:17:46.427335   333 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 17:17:46.427371   333 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 17:17:46.427394   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.427426   333 net.cpp:200] Created Layer conv1a/relu (5)
I0511 17:17:46.427448   333 net.cpp:572] conv1a/relu <- conv1a
I0511 17:17:46.427481   333 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 17:17:46.427508   333 net.cpp:260] Setting up conv1a/relu
I0511 17:17:46.427531   333 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 17:17:46.427557   333 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 17:17:46.427580   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.427614   333 net.cpp:200] Created Layer conv1b (6)
I0511 17:17:46.427637   333 net.cpp:572] conv1b <- conv1a
I0511 17:17:46.427662   333 net.cpp:542] conv1b -> conv1b
I0511 17:17:46.428103   377 data_layer.cpp:105] (0) Parser threads: 1
I0511 17:17:46.428439   377 data_layer.cpp:107] (0) Transformer threads: 1
I0511 17:17:46.428977   333 net.cpp:260] Setting up conv1b
I0511 17:17:46.428994   333 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 17:17:46.429044   333 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 17:17:46.429069   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429097   333 net.cpp:200] Created Layer conv1b/bn (7)
I0511 17:17:46.429121   333 net.cpp:572] conv1b/bn <- conv1b
I0511 17:17:46.429147   333 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 17:17:46.429702   333 net.cpp:260] Setting up conv1b/bn
I0511 17:17:46.429731   333 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 17:17:46.429764   333 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 17:17:46.429785   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429811   333 net.cpp:200] Created Layer conv1b/relu (8)
I0511 17:17:46.429831   333 net.cpp:572] conv1b/relu <- conv1b
I0511 17:17:46.429853   333 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 17:17:46.429878   333 net.cpp:260] Setting up conv1b/relu
I0511 17:17:46.429898   333 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 17:17:46.429924   333 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 17:17:46.429944   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429970   333 net.cpp:200] Created Layer pool1 (9)
I0511 17:17:46.429991   333 net.cpp:572] pool1 <- conv1b
I0511 17:17:46.430012   333 net.cpp:542] pool1 -> pool1
I0511 17:17:46.430100   333 net.cpp:260] Setting up pool1
I0511 17:17:46.430124   333 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 17:17:46.430150   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 17:17:46.430171   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.430199   333 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 17:17:46.430220   333 net.cpp:572] res2a_branch2a <- pool1
I0511 17:17:46.430243   333 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 17:17:46.430990   333 net.cpp:260] Setting up res2a_branch2a
I0511 17:17:46.431023   333 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 17:17:46.431061   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.431080   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431107   333 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 17:17:46.431128   333 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 17:17:46.431150   333 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 17:17:46.431653   333 net.cpp:260] Setting up res2a_branch2a/bn
I0511 17:17:46.431684   333 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 17:17:46.431718   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.431744   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431777   333 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 17:17:46.431795   333 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 17:17:46.431820   333 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 17:17:46.431844   333 net.cpp:260] Setting up res2a_branch2a/relu
I0511 17:17:46.431864   333 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 17:17:46.431890   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 17:17:46.431910   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431941   333 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 17:17:46.431962   333 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 17:17:46.431983   333 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 17:17:46.432514   333 net.cpp:260] Setting up res2a_branch2b
I0511 17:17:46.432546   333 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 17:17:46.432576   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.432597   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.432623   333 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 17:17:46.432644   333 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 17:17:46.432667   333 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 17:17:46.433121   333 net.cpp:260] Setting up res2a_branch2b/bn
I0511 17:17:46.433151   333 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 17:17:46.433184   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.433205   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433229   333 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 17:17:46.433250   333 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 17:17:46.433272   333 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 17:17:46.433301   333 net.cpp:260] Setting up res2a_branch2b/relu
I0511 17:17:46.433336   333 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 17:17:46.433369   333 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 17:17:46.433400   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433436   333 net.cpp:200] Created Layer pool2 (16)
I0511 17:17:46.433471   333 net.cpp:572] pool2 <- res2a_branch2b
I0511 17:17:46.433501   333 net.cpp:542] pool2 -> pool2
I0511 17:17:46.433593   333 net.cpp:260] Setting up pool2
I0511 17:17:46.433687   333 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 17:17:46.433727   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 17:17:46.433754   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433791   333 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 17:17:46.433828   333 net.cpp:572] res3a_branch2a <- pool2
I0511 17:17:46.433857   333 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 17:17:46.450163   333 net.cpp:260] Setting up res3a_branch2a
I0511 17:17:46.450193   333 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 17:17:46.450220   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.450229   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450244   333 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 17:17:46.450254   333 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 17:17:46.450263   333 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 17:17:46.450675   333 net.cpp:260] Setting up res3a_branch2a/bn
I0511 17:17:46.450712   333 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 17:17:46.450734   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.450742   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450752   333 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 17:17:46.450758   333 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 17:17:46.450764   333 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 17:17:46.450774   333 net.cpp:260] Setting up res3a_branch2a/relu
I0511 17:17:46.450779   333 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 17:17:46.450788   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 17:17:46.450794   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450811   333 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 17:17:46.450817   333 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 17:17:46.450824   333 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 17:17:46.451901   333 net.cpp:260] Setting up res3a_branch2b
I0511 17:17:46.451910   333 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 17:17:46.451922   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.451928   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.451938   333 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 17:17:46.451944   333 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 17:17:46.451951   333 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 17:17:46.452301   333 net.cpp:260] Setting up res3a_branch2b/bn
I0511 17:17:46.452306   333 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 17:17:46.452322   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.452327   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452334   333 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 17:17:46.452340   333 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 17:17:46.452347   333 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 17:17:46.452353   333 net.cpp:260] Setting up res3a_branch2b/relu
I0511 17:17:46.452359   333 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 17:17:46.452368   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.452373   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452383   333 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 17:17:46.452389   333 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 17:17:46.452394   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.452401   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.452440   333 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 17:17:46.452445   333 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 17:17:46.452455   333 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 17:17:46.452462   333 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 17:17:46.452468   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452478   333 net.cpp:200] Created Layer pool3 (24)
I0511 17:17:46.452484   333 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.452502   333 net.cpp:542] pool3 -> pool3
I0511 17:17:46.452555   333 net.cpp:260] Setting up pool3
I0511 17:17:46.452561   333 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 17:17:46.452570   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 17:17:46.452576   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452590   333 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 17:17:46.452596   333 net.cpp:572] res4a_branch2a <- pool3
I0511 17:17:46.452602   333 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 17:17:46.460067   333 net.cpp:260] Setting up res4a_branch2a
I0511 17:17:46.460103   333 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 17:17:46.460140   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.460163   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460191   333 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 17:17:46.460214   333 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 17:17:46.460239   333 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 17:17:46.460703   333 net.cpp:260] Setting up res4a_branch2a/bn
I0511 17:17:46.460728   333 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 17:17:46.460762   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.460784   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460809   333 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 17:17:46.460832   333 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 17:17:46.460855   333 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 17:17:46.460880   333 net.cpp:260] Setting up res4a_branch2a/relu
I0511 17:17:46.460901   333 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 17:17:46.460927   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 17:17:46.460949   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460979   333 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 17:17:46.461002   333 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 17:17:46.461025   333 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 17:17:46.464432   333 net.cpp:260] Setting up res4a_branch2b
I0511 17:17:46.466212   333 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 17:17:46.466311   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.466377   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.466437   333 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 17:17:46.466496   333 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 17:17:46.466547   333 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 17:17:46.467195   333 net.cpp:260] Setting up res4a_branch2b/bn
I0511 17:17:46.467849   333 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 17:17:46.467901   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.467938   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.467978   333 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 17:17:46.468014   333 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 17:17:46.468042   333 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 17:17:46.468075   333 net.cpp:260] Setting up res4a_branch2b/relu
I0511 17:17:46.468111   333 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 17:17:46.468163   333 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 17:17:46.468191   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.468225   333 net.cpp:200] Created Layer pool4 (31)
I0511 17:17:46.468256   333 net.cpp:572] pool4 <- res4a_branch2b
I0511 17:17:46.468286   333 net.cpp:542] pool4 -> pool4
I0511 17:17:46.468376   333 net.cpp:260] Setting up pool4
I0511 17:17:46.468468   333 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 17:17:46.468505   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 17:17:46.468538   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.468576   333 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 17:17:46.468616   333 net.cpp:572] res5a_branch2a <- pool4
I0511 17:17:46.468645   333 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 17:17:46.507541   333 net.cpp:260] Setting up res5a_branch2a
I0511 17:17:46.507601   333 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 17:17:46.507639   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.507658   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.507671   333 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 17:17:46.507707   333 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 17:17:46.507730   333 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 17:17:46.508177   333 net.cpp:260] Setting up res5a_branch2a/bn
I0511 17:17:46.508201   333 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 17:17:46.508234   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.508255   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.508280   333 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 17:17:46.508301   333 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 17:17:46.508322   333 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 17:17:46.508347   333 net.cpp:260] Setting up res5a_branch2a/relu
I0511 17:17:46.508366   333 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 17:17:46.508390   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 17:17:46.508411   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.508440   333 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 17:17:46.508461   333 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 17:17:46.508482   333 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 17:17:46.522357   333 net.cpp:260] Setting up res5a_branch2b
I0511 17:17:46.522419   333 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 17:17:46.522467   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.522488   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.522518   333 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 17:17:46.522541   333 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 17:17:46.522565   333 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 17:17:46.522992   333 net.cpp:260] Setting up res5a_branch2b/bn
I0511 17:17:46.523020   333 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 17:17:46.523051   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.523072   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523097   333 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 17:17:46.523128   333 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 17:17:46.523161   333 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 17:17:46.523186   333 net.cpp:260] Setting up res5a_branch2b/relu
I0511 17:17:46.523206   333 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 17:17:46.523231   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.523252   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523277   333 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 17:17:46.523296   333 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 17:17:46.523319   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.523342   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.523406   333 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 17:17:46.523427   333 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 17:17:46.523450   333 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 17:17:46.523474   333 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 17:17:46.523495   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523521   333 net.cpp:200] Created Layer pool6 (39)
I0511 17:17:46.523542   333 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.523563   333 net.cpp:542] pool6 -> pool6
I0511 17:17:46.523641   333 net.cpp:260] Setting up pool6
I0511 17:17:46.523661   333 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 17:17:46.523685   333 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 17:17:46.523705   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523728   333 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 17:17:46.523749   333 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 17:17:46.523770   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 17:17:46.523792   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 17:17:46.523849   333 net.cpp:260] Setting up pool6_pool6_0_split
I0511 17:17:46.523869   333 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 17:17:46.523891   333 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 17:17:46.523916   333 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 17:17:46.523936   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523959   333 net.cpp:200] Created Layer pool7 (41)
I0511 17:17:46.523980   333 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 17:17:46.524001   333 net.cpp:542] pool7 -> pool7
I0511 17:17:46.524076   333 net.cpp:260] Setting up pool7
I0511 17:17:46.524096   333 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 17:17:46.524121   333 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 17:17:46.524140   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524163   333 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 17:17:46.524184   333 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 17:17:46.524206   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 17:17:46.524231   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 17:17:46.524287   333 net.cpp:260] Setting up pool7_pool7_0_split
I0511 17:17:46.524307   333 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 17:17:46.524343   333 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 17:17:46.524366   333 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 17:17:46.524387   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524411   333 net.cpp:200] Created Layer pool8 (43)
I0511 17:17:46.524432   333 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 17:17:46.524453   333 net.cpp:542] pool8 -> pool8
I0511 17:17:46.524526   333 net.cpp:260] Setting up pool8
I0511 17:17:46.524547   333 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 17:17:46.524571   333 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 17:17:46.524591   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524614   333 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 17:17:46.524634   333 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 17:17:46.524657   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 17:17:46.524678   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 17:17:46.524734   333 net.cpp:260] Setting up pool8_pool8_0_split
I0511 17:17:46.524753   333 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 17:17:46.524776   333 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 17:17:46.524799   333 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 17:17:46.524821   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524843   333 net.cpp:200] Created Layer pool9 (45)
I0511 17:17:46.524864   333 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 17:17:46.524885   333 net.cpp:542] pool9 -> pool9
I0511 17:17:46.524956   333 net.cpp:260] Setting up pool9
I0511 17:17:46.524976   333 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 17:17:46.525000   333 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 17:17:46.525020   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.525051   333 net.cpp:200] Created Layer ctx_output1 (46)
I0511 17:17:46.525072   333 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.525094   333 net.cpp:542] ctx_output1 -> ctx_output1
I0511 17:17:46.526096   333 net.cpp:260] Setting up ctx_output1
I0511 17:17:46.526130   333 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 17:17:46.526160   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 17:17:46.526181   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526206   333 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 17:17:46.526226   333 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 17:17:46.526248   333 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 17:17:46.526271   333 net.cpp:260] Setting up ctx_output1/relu
I0511 17:17:46.526291   333 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 17:17:46.526315   333 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 17:17:46.526336   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526360   333 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 17:17:46.526379   333 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 17:17:46.526401   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.526423   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.526446   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.526532   333 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 17:17:46.526553   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526577   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526599   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526623   333 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 17:17:46.526644   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526674   333 net.cpp:200] Created Layer ctx_output2 (49)
I0511 17:17:46.526695   333 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.526716   333 net.cpp:542] ctx_output2 -> ctx_output2
I0511 17:17:46.530591   333 net.cpp:260] Setting up ctx_output2
I0511 17:17:46.530633   333 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 17:17:46.530665   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 17:17:46.530686   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.530710   333 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 17:17:46.530732   333 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 17:17:46.530755   333 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 17:17:46.530778   333 net.cpp:260] Setting up ctx_output2/relu
I0511 17:17:46.530798   333 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 17:17:46.530822   333 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 17:17:46.530843   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.530866   333 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 17:17:46.530887   333 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 17:17:46.530908   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.530931   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.530953   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.531031   333 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 17:17:46.531051   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531075   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531097   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531121   333 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 17:17:46.531141   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.531172   333 net.cpp:200] Created Layer ctx_output3 (52)
I0511 17:17:46.531193   333 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 17:17:46.531214   333 net.cpp:542] ctx_output3 -> ctx_output3
I0511 17:17:46.534360   333 net.cpp:260] Setting up ctx_output3
I0511 17:17:46.534405   333 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 17:17:46.534446   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 17:17:46.534471   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.534497   333 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 17:17:46.534521   333 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 17:17:46.534546   333 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 17:17:46.534579   333 net.cpp:260] Setting up ctx_output3/relu
I0511 17:17:46.534610   333 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 17:17:46.534637   333 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 17:17:46.534660   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.534687   333 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 17:17:46.534710   333 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 17:17:46.534735   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.534761   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.534787   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.534886   333 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 17:17:46.534909   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534935   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534962   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534989   333 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 17:17:46.535012   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.535044   333 net.cpp:200] Created Layer ctx_output4 (55)
I0511 17:17:46.535068   333 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 17:17:46.535094   333 net.cpp:542] ctx_output4 -> ctx_output4
I0511 17:17:46.538358   333 net.cpp:260] Setting up ctx_output4
I0511 17:17:46.538396   333 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 17:17:46.538431   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 17:17:46.538455   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.538481   333 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 17:17:46.538506   333 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 17:17:46.538532   333 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 17:17:46.538558   333 net.cpp:260] Setting up ctx_output4/relu
I0511 17:17:46.538580   333 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 17:17:46.538607   333 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 17:17:46.538631   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.538657   333 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 17:17:46.538681   333 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 17:17:46.538705   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.538730   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.538758   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.538846   333 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 17:17:46.538869   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538895   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538923   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538949   333 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 17:17:46.538972   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.539019   333 net.cpp:200] Created Layer ctx_output5 (58)
I0511 17:17:46.539043   333 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 17:17:46.539068   333 net.cpp:542] ctx_output5 -> ctx_output5
I0511 17:17:46.542299   333 net.cpp:260] Setting up ctx_output5
I0511 17:17:46.542335   333 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 17:17:46.542369   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 17:17:46.542393   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542420   333 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 17:17:46.542444   333 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 17:17:46.542469   333 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 17:17:46.542495   333 net.cpp:260] Setting up ctx_output5/relu
I0511 17:17:46.542518   333 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 17:17:46.542546   333 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 17:17:46.542569   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542596   333 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 17:17:46.542620   333 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 17:17:46.542644   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.542670   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.542696   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.542780   333 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 17:17:46.542804   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542829   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542856   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542884   333 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 17:17:46.542907   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542940   333 net.cpp:200] Created Layer ctx_output6 (61)
I0511 17:17:46.542964   333 net.cpp:572] ctx_output6 <- pool9
I0511 17:17:46.542989   333 net.cpp:542] ctx_output6 -> ctx_output6
I0511 17:17:46.547009   333 net.cpp:260] Setting up ctx_output6
I0511 17:17:46.547047   333 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 17:17:46.547078   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 17:17:46.547101   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547124   333 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 17:17:46.547147   333 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 17:17:46.547168   333 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 17:17:46.547192   333 net.cpp:260] Setting up ctx_output6/relu
I0511 17:17:46.547211   333 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 17:17:46.547236   333 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 17:17:46.547257   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547281   333 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 17:17:46.547302   333 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 17:17:46.547322   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.547350   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.547381   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.547459   333 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 17:17:46.547480   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547503   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547526   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547550   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.547570   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547602   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 17:17:46.547623   333 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.547646   333 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 17:17:46.548056   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 17:17:46.548084   333 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 17:17:46.548115   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.548135   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.548161   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 17:17:46.548182   333 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 17:17:46.548205   333 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548341   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548363   333 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 17:17:46.548388   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.548408   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.548432   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 17:17:46.548454   333 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548475   333 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.550631   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.550668   333 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 17:17:46.550696   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.550717   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.550750   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 17:17:46.550771   333 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.550794   333 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 17:17:46.551251   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 17:17:46.551276   333 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 17:17:46.551306   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.551327   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.551352   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 17:17:46.551373   333 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 17:17:46.551395   333 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551535   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551558   333 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 17:17:46.551582   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.551602   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.551626   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 17:17:46.551646   333 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551668   333 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.553830   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.553871   333 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 17:17:46.553903   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.553927   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.553958   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 17:17:46.553983   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.554011   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 17:17:46.554039   333 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 17:17:46.554108   333 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 17:17:46.554131   333 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 17:17:46.554159   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.554183   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.554217   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 17:17:46.554241   333 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.554266   333 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 17:17:46.554800   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 17:17:46.554829   333 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 17:17:46.554862   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.554885   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.554914   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 17:17:46.554939   333 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 17:17:46.554963   333 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555125   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555152   333 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 17:17:46.555181   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.555203   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.555230   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 17:17:46.555254   333 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555279   333 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.555392   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.555418   333 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 17:17:46.555444   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.555486   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.555519   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 17:17:46.555543   333 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.555568   333 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 17:17:46.556067   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 17:17:46.556095   333 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 17:17:46.556128   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.556151   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556180   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 17:17:46.556202   333 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 17:17:46.556228   333 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556390   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556414   333 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 17:17:46.556442   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.556465   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556491   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 17:17:46.556515   333 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556540   333 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.556643   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.556668   333 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 17:17:46.556695   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.556718   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556747   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 17:17:46.556771   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.556797   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 17:17:46.556823   333 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 17:17:46.556877   333 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 17:17:46.556900   333 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 17:17:46.556927   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.556951   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556982   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 17:17:46.557006   333 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.557031   333 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 17:17:46.557551   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 17:17:46.557581   333 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 17:17:46.557613   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.557636   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.557664   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 17:17:46.557688   333 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 17:17:46.557716   333 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.557881   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.557907   333 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 17:17:46.557935   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.557958   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.557984   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 17:17:46.558007   333 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.558032   333 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.558126   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.558156   333 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 17:17:46.558182   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.558205   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.558238   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 17:17:46.558261   333 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.558286   333 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 17:17:46.558775   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 17:17:46.558804   333 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 17:17:46.558835   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.558858   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.558887   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 17:17:46.558910   333 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 17:17:46.558934   333 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559088   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559113   333 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 17:17:46.559141   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.559165   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559190   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 17:17:46.559214   333 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559238   333 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.559329   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.559352   333 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 17:17:46.559378   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.559401   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559429   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 17:17:46.559453   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.559478   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 17:17:46.559504   333 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 17:17:46.559553   333 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 17:17:46.559576   333 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 17:17:46.559617   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.559639   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559671   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 17:17:46.559695   333 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.559720   333 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 17:17:46.560217   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 17:17:46.560245   333 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 17:17:46.560276   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.560299   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560328   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 17:17:46.560351   333 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 17:17:46.560375   333 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560540   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560565   333 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 17:17:46.560592   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.560616   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560642   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 17:17:46.560665   333 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560690   333 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.560781   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.560811   333 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 17:17:46.560837   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.560859   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560892   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 17:17:46.560916   333 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.560941   333 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 17:17:46.561448   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 17:17:46.561478   333 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 17:17:46.561509   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.561533   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.561560   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 17:17:46.561584   333 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 17:17:46.561609   333 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561762   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561789   333 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 17:17:46.561815   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.561838   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.561861   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 17:17:46.561882   333 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561908   333 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.561991   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.562011   333 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 17:17:46.562034   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.562055   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562080   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 17:17:46.562101   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.562124   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 17:17:46.562145   333 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 17:17:46.562189   333 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 17:17:46.562211   333 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 17:17:46.562233   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.562254   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562283   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 17:17:46.562304   333 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.562325   333 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 17:17:46.562747   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 17:17:46.562772   333 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 17:17:46.562801   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.562821   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562846   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 17:17:46.562867   333 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 17:17:46.562889   333 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563027   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563050   333 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 17:17:46.563073   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.563093   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563117   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 17:17:46.563138   333 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563159   333 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.563238   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.563259   333 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 17:17:46.563283   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.563304   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563333   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 17:17:46.563354   333 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.563376   333 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 17:17:46.563789   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 17:17:46.563815   333 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 17:17:46.563848   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.563879   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563905   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 17:17:46.563925   333 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 17:17:46.563948   333 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564083   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564106   333 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 17:17:46.564129   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.564150   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564173   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 17:17:46.564194   333 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564216   333 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.564292   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.564312   333 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 17:17:46.564335   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.564357   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564380   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 17:17:46.564401   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.564424   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 17:17:46.564445   333 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 17:17:46.564482   333 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 17:17:46.564502   333 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 17:17:46.564525   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.564545   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564574   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 17:17:46.564596   333 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.564617   333 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 17:17:46.565035   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 17:17:46.565060   333 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 17:17:46.565090   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.565110   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565136   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 17:17:46.565157   333 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 17:17:46.565179   333 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565322   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565346   333 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 17:17:46.565369   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.565390   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565413   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 17:17:46.565439   333 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565469   333 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.565544   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.565565   333 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 17:17:46.565588   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.565609   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565639   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 17:17:46.565659   333 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.565680   333 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 17:17:46.566083   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 17:17:46.566110   333 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 17:17:46.566138   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.566159   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566184   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 17:17:46.566205   333 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 17:17:46.566227   333 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566365   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566388   333 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 17:17:46.566411   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.566431   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566455   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 17:17:46.566475   333 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566498   333 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.566574   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.566596   333 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 17:17:46.566618   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.566638   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566663   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 17:17:46.566684   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.566706   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 17:17:46.566728   333 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 17:17:46.566764   333 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 17:17:46.566784   333 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 17:17:46.566807   333 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 17:17:46.566828   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566851   333 net.cpp:200] Created Layer mbox_loc (106)
I0511 17:17:46.566874   333 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.566895   333 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.566917   333 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.566938   333 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.566965   333 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.566987   333 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.567016   333 net.cpp:542] mbox_loc -> mbox_loc
I0511 17:17:46.567059   333 net.cpp:260] Setting up mbox_loc
I0511 17:17:46.567078   333 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 17:17:46.567102   333 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 17:17:46.567123   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567147   333 net.cpp:200] Created Layer mbox_conf (107)
I0511 17:17:46.567167   333 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.567189   333 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.567210   333 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.567232   333 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.567255   333 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.567276   333 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.567296   333 net.cpp:542] mbox_conf -> mbox_conf
I0511 17:17:46.567337   333 net.cpp:260] Setting up mbox_conf
I0511 17:17:46.567358   333 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 17:17:46.567382   333 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 17:17:46.567402   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567425   333 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 17:17:46.567446   333 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 17:17:46.567468   333 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 17:17:46.567489   333 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 17:17:46.567510   333 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 17:17:46.567531   333 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 17:17:46.567553   333 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 17:17:46.567574   333 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 17:17:46.567615   333 net.cpp:260] Setting up mbox_priorbox
I0511 17:17:46.567634   333 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 17:17:46.567658   333 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 17:17:46.567678   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567706   333 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 17:17:46.567728   333 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 17:17:46.567749   333 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 17:17:46.567791   333 net.cpp:260] Setting up mbox_conf_reshape
I0511 17:17:46.567811   333 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 17:17:46.567836   333 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 17:17:46.567857   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567885   333 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 17:17:46.567906   333 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 17:17:46.567927   333 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 17:17:46.568020   333 net.cpp:260] Setting up mbox_conf_softmax
I0511 17:17:46.568040   333 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 17:17:46.568064   333 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 17:17:46.568085   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.568109   333 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 17:17:46.568128   333 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 17:17:46.568154   333 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 17:17:46.570369   333 net.cpp:260] Setting up mbox_conf_flatten
I0511 17:17:46.570405   333 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 17:17:46.570435   333 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 17:17:46.570456   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.570502   333 net.cpp:200] Created Layer detection_out (112)
I0511 17:17:46.570523   333 net.cpp:572] detection_out <- mbox_loc
I0511 17:17:46.570546   333 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 17:17:46.570569   333 net.cpp:572] detection_out <- mbox_priorbox
I0511 17:17:46.570590   333 net.cpp:542] detection_out -> detection_out
I0511 17:17:46.571322   333 net.cpp:260] Setting up detection_out
I0511 17:17:46.571349   333 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 17:17:46.571374   333 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 17:17:46.571395   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.571424   333 net.cpp:200] Created Layer detection_eval (113)
I0511 17:17:46.571444   333 net.cpp:572] detection_eval <- detection_out
I0511 17:17:46.571466   333 net.cpp:572] detection_eval <- label
I0511 17:17:46.571489   333 net.cpp:542] detection_eval -> detection_eval
I0511 17:17:46.572026   333 net.cpp:260] Setting up detection_eval
I0511 17:17:46.572050   333 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 17:17:46.572074   333 net.cpp:338] detection_eval does not need backward computation.
I0511 17:17:46.572096   333 net.cpp:338] detection_out does not need backward computation.
I0511 17:17:46.572118   333 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 17:17:46.572139   333 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 17:17:46.572161   333 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 17:17:46.572182   333 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 17:17:46.572206   333 net.cpp:338] mbox_conf does not need backward computation.
I0511 17:17:46.572229   333 net.cpp:338] mbox_loc does not need backward computation.
I0511 17:17:46.572252   333 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572274   333 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572295   333 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572316   333 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572337   333 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572358   333 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572379   333 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572401   333 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572422   333 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572443   333 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572464   333 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572486   333 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572507   333 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572528   333 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572549   333 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572571   333 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572597   333 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572626   333 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572647   333 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572669   333 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572690   333 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572711   333 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572732   333 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572753   333 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572774   333 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572795   333 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572816   333 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572839   333 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572860   333 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572881   333 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572902   333 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572923   333 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572944   333 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572965   333 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572986   333 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 17:17:46.573007   333 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.573029   333 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.573050   333 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.573071   333 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 17:17:46.573092   333 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.573113   333 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.573134   333 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 17:17:46.573156   333 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 17:17:46.573177   333 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 17:17:46.573199   333 net.cpp:338] ctx_output6 does not need backward computation.
I0511 17:17:46.573220   333 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 17:17:46.573240   333 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 17:17:46.573262   333 net.cpp:338] ctx_output5 does not need backward computation.
I0511 17:17:46.573283   333 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 17:17:46.573318   333 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 17:17:46.573343   333 net.cpp:338] ctx_output4 does not need backward computation.
I0511 17:17:46.573367   333 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 17:17:46.573390   333 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 17:17:46.573415   333 net.cpp:338] ctx_output3 does not need backward computation.
I0511 17:17:46.573438   333 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 17:17:46.573467   333 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 17:17:46.573499   333 net.cpp:338] ctx_output2 does not need backward computation.
I0511 17:17:46.573524   333 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 17:17:46.573549   333 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 17:17:46.573572   333 net.cpp:338] ctx_output1 does not need backward computation.
I0511 17:17:46.573596   333 net.cpp:338] pool9 does not need backward computation.
I0511 17:17:46.573621   333 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 17:17:46.573645   333 net.cpp:338] pool8 does not need backward computation.
I0511 17:17:46.573670   333 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 17:17:46.573694   333 net.cpp:338] pool7 does not need backward computation.
I0511 17:17:46.573719   333 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 17:17:46.573742   333 net.cpp:338] pool6 does not need backward computation.
I0511 17:17:46.573767   333 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 17:17:46.573791   333 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 17:17:46.573814   333 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 17:17:46.573838   333 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 17:17:46.573863   333 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 17:17:46.573885   333 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 17:17:46.573909   333 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 17:17:46.573932   333 net.cpp:338] pool4 does not need backward computation.
I0511 17:17:46.573956   333 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 17:17:46.573979   333 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 17:17:46.574002   333 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 17:17:46.574026   333 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 17:17:46.574049   333 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 17:17:46.574072   333 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 17:17:46.574097   333 net.cpp:338] pool3 does not need backward computation.
I0511 17:17:46.574121   333 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 17:17:46.574146   333 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 17:17:46.574169   333 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 17:17:46.574193   333 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 17:17:46.574216   333 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 17:17:46.574240   333 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 17:17:46.574263   333 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 17:17:46.574286   333 net.cpp:338] pool2 does not need backward computation.
I0511 17:17:46.574311   333 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 17:17:46.574334   333 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 17:17:46.574357   333 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 17:17:46.574381   333 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 17:17:46.574404   333 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 17:17:46.574427   333 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 17:17:46.574451   333 net.cpp:338] pool1 does not need backward computation.
I0511 17:17:46.574476   333 net.cpp:338] conv1b/relu does not need backward computation.
I0511 17:17:46.574501   333 net.cpp:338] conv1b/bn does not need backward computation.
I0511 17:17:46.574532   333 net.cpp:338] conv1b does not need backward computation.
I0511 17:17:46.574555   333 net.cpp:338] conv1a/relu does not need backward computation.
I0511 17:17:46.574579   333 net.cpp:338] conv1a/bn does not need backward computation.
I0511 17:17:46.574602   333 net.cpp:338] conv1a does not need backward computation.
I0511 17:17:46.574625   333 net.cpp:338] data/bias does not need backward computation.
I0511 17:17:46.574651   333 net.cpp:338] data_data_0_split does not need backward computation.
I0511 17:17:46.574676   333 net.cpp:338] data does not need backward computation.
I0511 17:17:46.574698   333 net.cpp:380] This network produces output detection_eval
I0511 17:17:46.574878   333 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 17:17:46.574903   333 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 17:17:46.574925   333 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 17:17:46.574949   333 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 17:17:46.574972   333 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 17:17:46.574995   333 net.cpp:421] Network initialization done.
I0511 17:17:46.575351   333 solver.cpp:55] Solver scaffolding done.
I0511 17:17:46.582146   333 caffe.cpp:158] Finetuning from training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 17:17:46.586655   333 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 17:17:46.586690   333 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 17:17:46.586710   333 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 17:17:46.586786   333 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 17:17:46.586843   333 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.586952   333 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 17:17:46.586973   333 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 17:17:46.587031   333 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587134   333 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 17:17:46.587157   333 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 17:17:46.587177   333 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.587241   333 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587343   333 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.587364   333 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.587424   333 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587530   333 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.587551   333 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 17:17:46.587570   333 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.587664   333 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587771   333 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.587792   333 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.587867   333 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587971   333 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.587992   333 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.588012   333 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 17:17:46.588037   333 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.588253   333 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.588366   333 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.588387   333 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.588510   333 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.588618   333 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.588639   333 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 17:17:46.588658   333 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.589247   333 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.589375   333 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.589397   333 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.589725   333 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.589839   333 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.589860   333 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.589880   333 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 17:17:46.589900   333 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 17:17:46.589920   333 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 17:17:46.589938   333 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 17:17:46.589957   333 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 17:17:46.589977   333 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 17:17:46.589996   333 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 17:17:46.590015   333 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 17:17:46.590088   333 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 17:17:46.590106   333 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590126   333 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 17:17:46.590248   333 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 17:17:46.590272   333 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590291   333 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 17:17:46.590415   333 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 17:17:46.590438   333 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590457   333 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 17:17:46.590577   333 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 17:17:46.590600   333 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590620   333 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 17:17:46.590736   333 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 17:17:46.590759   333 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590777   333 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 17:17:46.590893   333 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 17:17:46.590914   333 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590939   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591006   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591027   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591045   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591102   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591125   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591143   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591162   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591228   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591245   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591265   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591323   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591342   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591361   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591380   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591439   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591457   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591477   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591536   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591554   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591574   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591593   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591652   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591670   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591691   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591749   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591768   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591789   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591807   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591866   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591884   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591903   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591961   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591979   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.592000   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.592023   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.592089   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.592108   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.592128   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.592185   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.592203   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.592223   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.592242   333 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 17:17:46.592262   333 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 17:17:46.592281   333 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 17:17:46.592300   333 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 17:17:46.597434   333 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 17:17:46.597472   333 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 17:17:46.597494   333 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 17:17:46.597560   333 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 17:17:46.597630   333 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.597755   333 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 17:17:46.597779   333 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 17:17:46.597846   333 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.597968   333 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 17:17:46.597991   333 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 17:17:46.598013   333 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.598088   333 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598212   333 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.598237   333 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.598307   333 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598424   333 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.598448   333 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 17:17:46.598469   333 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.598572   333 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598695   333 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.598717   333 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.598803   333 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598924   333 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.598948   333 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.598970   333 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 17:17:46.598992   333 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.599218   333 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.599347   333 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.599371   333 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.599520   333 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.599659   333 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.599684   333 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 17:17:46.599706   333 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.600389   333 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.600522   333 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.600545   333 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.600929   333 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.601060   333 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.601085   333 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601107   333 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 17:17:46.601130   333 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 17:17:46.601151   333 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 17:17:46.601173   333 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 17:17:46.601195   333 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 17:17:46.601217   333 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 17:17:46.601239   333 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 17:17:46.601261   333 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 17:17:46.601361   333 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 17:17:46.601383   333 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601405   333 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 17:17:46.601542   333 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 17:17:46.601569   333 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601591   333 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 17:17:46.601728   333 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 17:17:46.601754   333 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601776   333 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 17:17:46.601904   333 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 17:17:46.601927   333 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601945   333 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 17:17:46.602063   333 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 17:17:46.602085   333 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 17:17:46.602104   333 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 17:17:46.602218   333 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 17:17:46.602241   333 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 17:17:46.602259   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602319   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602336   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602355   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602417   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602444   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602464   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602483   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602542   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602561   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602581   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602639   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602658   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602677   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602696   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602754   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602773   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602792   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602852   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602870   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602890   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602910   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602968   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602988   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603006   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603065   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603083   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603102   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603122   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.603180   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.603199   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603219   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603276   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603293   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603313   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603332   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.603390   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.603408   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603428   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603490   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603515   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603535   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603554   333 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 17:17:46.603574   333 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 17:17:46.603592   333 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 17:17:46.603612   333 net.cpp:1137] Ignoring source layer mbox_loss
I0511 17:17:46.603828   333 caffe.cpp:260] Starting Optimization
I0511 17:17:46.603854   333 solver.cpp:455] Solving ssdJacintoNetV2
I0511 17:17:46.603873   333 solver.cpp:456] Learning Rate Policy: poly
I0511 17:17:46.603924   333 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 17:17:46.608160   333 solver.cpp:269] Initial Test started...
I0511 17:17:46.608201   333 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 17:17:46.611214   333 net.cpp:1071] Ignoring source layer mbox_loss
I0511 17:17:46.612951   378 common.cpp:528] NVML initialized, thread 378
I0511 17:17:46.658514   378 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 378
I0511 17:18:03.767459   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:18:03.946961   333 solver.cpp:749] class AP 1: 0.902301
I0511 17:18:03.947893   333 solver.cpp:749] class AP 2: 0.887808
I0511 17:18:03.948199   333 solver.cpp:749] class AP 3: 0.902711
I0511 17:18:03.948207   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897607
I0511 17:18:03.948246   333 solver.cpp:274] Initial Test completed in 17.34s
I0511 17:18:04.659658   333 solver.cpp:360] Iteration 0 (0.711375 s), loss = 2.1365
I0511 17:18:04.659819   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.13313 (* 1 = 2.13313 loss)
I0511 17:18:04.659910   333 sgd_solver.cpp:172] Iteration 0, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I0511 17:18:04.775753   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 7.43G, req 0.74M)	t: 0 0 1.88
I0511 17:18:05.045018   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.85 1.93
I0511 17:18:05.389921   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.83 2.66
I0511 17:18:05.642237   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.31 0.79
I0511 17:18:05.961027   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.11G 64/1 6 4 5 	(avail 7.32G, req 0.11G)	t: 0 0.72 1.16
I0511 17:18:06.174163   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.11G 128/4 6 4 0 	(avail 7.32G, req 0.11G)	t: 0 0.15 0.39
I0511 17:18:06.457942   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.11G 128/1 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.48 0.51
I0511 17:18:06.669474   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.11G 256/4 6 4 5 	(avail 7.32G, req 0.11G)	t: 0 0.12 0.18
I0511 17:18:06.938400   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.11G 256/1 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.53 0.48
I0511 17:18:07.157773   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.11G 512/4 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.11 0.11
I0511 17:18:07.610962   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.11G 128/1 1 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.43 0.82
I0511 17:18:07.830991   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.11G 512/1 1 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.13 0.2
I0511 17:18:08.033882   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.11G 512/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.06 0.08
I0511 17:18:08.230051   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.05
I0511 17:18:08.425616   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.04
I0511 17:18:08.621701   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.03
I0511 17:18:08.918817   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.27 0.78
I0511 17:18:09.175052   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.29 0.78
I0511 17:18:09.359982   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.03 0.05
I0511 17:18:09.544384   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.03 0.05
I0511 17:18:09.725730   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.02 0.02
I0511 17:18:09.897591   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.02 0.02
I0511 17:18:10.077782   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.257812   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.437355   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.609409   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.782119   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.953788   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:11.384562   333 solver.cpp:360] Iteration 1 (6.7249 s), loss = 2.09856
I0511 17:18:11.384606   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.699 (* 1 = 1.699 loss)
I0511 17:18:11.850543   333 solver.cpp:360] Iteration 2 (0.465974 s), loss = 2.07411
I0511 17:18:11.850651   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.18515 (* 1 = 2.18515 loss)
I0511 17:18:23.001194   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:19:03.266186   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:19:45.682925   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:20:31.706429   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:20:44.765702   333 solver.cpp:354] Iteration 100 (0.640878 iter/s, 152.915s/98 iter), 3.8/376.5ep, loss = 2.45554
I0511 17:20:44.765810   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.09453 (* 1 = 2.09453 loss)
I0511 17:20:44.765849   333 sgd_solver.cpp:172] Iteration 100, lr = 0.000960596, m = 0.9, wd = 1e-05, gs = 1
I0511 17:21:13.261363   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:21:52.522627   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:22:37.643249   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:23:20.549937   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:23:20.614359   333 solver.cpp:354] Iteration 200 (0.641648 iter/s, 155.849s/100 iter), 7.5/376.5ep, loss = 2.22128
I0511 17:23:20.614567   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.81194 (* 1 = 1.81194 loss)
I0511 17:23:20.614632   333 sgd_solver.cpp:172] Iteration 200, lr = 0.000922368, m = 0.9, wd = 1e-05, gs = 1
I0511 17:24:01.467376   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:24:41.761487   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:25:31.080615   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:26:07.161725   333 solver.cpp:354] Iteration 300 (0.60043 iter/s, 166.547s/100 iter), 11.3/376.5ep, loss = 2.42609
I0511 17:26:07.161931   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.40756 (* 1 = 2.40756 loss)
I0511 17:26:07.161981   333 sgd_solver.cpp:172] Iteration 300, lr = 0.000885293, m = 0.9, wd = 1e-05, gs = 1
I0511 17:26:09.534718   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:26:52.304467   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:27:38.235322   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:28:19.790803   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:28:44.452936   333 solver.cpp:354] Iteration 400 (0.635764 iter/s, 157.291s/100 iter), 15.1/376.5ep, loss = 2.23426
I0511 17:28:44.453182   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.54844 (* 1 = 2.54844 loss)
I0511 17:28:44.453264   333 sgd_solver.cpp:172] Iteration 400, lr = 0.000849347, m = 0.9, wd = 1e-05, gs = 1
I0511 17:28:58.654088   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:29:42.934747   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:30:24.552893   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:05.045603   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:21.363632   333 solver.cpp:637] Iteration 500, Testing net (#0)
I0511 17:31:35.686610   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:35.930398   333 solver.cpp:749] class AP 1: 0.904178
I0511 17:31:35.930835   333 solver.cpp:749] class AP 2: 0.87761
I0511 17:31:35.931067   333 solver.cpp:749] class AP 3: 0.904251
I0511 17:31:35.931075   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895346
I0511 17:31:35.931102   333 solver.cpp:284] Tests completed in 171.478s
I0511 17:31:36.385363   333 solver.cpp:354] Iteration 500 (0.583165 iter/s, 171.478s/100 iter), 18.8/376.5ep, loss = 2.21601
I0511 17:31:36.385434   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.3453 (* 1 = 2.3453 loss)
I0511 17:31:36.385462   333 sgd_solver.cpp:172] Iteration 500, lr = 0.000814506, m = 0.9, wd = 1e-05, gs = 1
I0511 17:31:56.219297   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:32:37.279547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:33:19.111665   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:34:00.243744   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:34:07.294708   333 solver.cpp:354] Iteration 600 (0.66265 iter/s, 150.909s/100 iter), 22.6/376.5ep, loss = 2.43671
I0511 17:34:07.294849   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.1361 (* 1 = 2.1361 loss)
I0511 17:34:07.294893   333 sgd_solver.cpp:172] Iteration 600, lr = 0.000780749, m = 0.9, wd = 1e-05, gs = 1
I0511 17:34:43.553920   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:35:28.338800   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:36:10.504659   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:36:50.824117   333 solver.cpp:354] Iteration 700 (0.611512 iter/s, 163.529s/100 iter), 26.4/376.5ep, loss = 2.34746
I0511 17:36:50.824357   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.18696 (* 1 = 2.18696 loss)
I0511 17:36:50.824430   333 sgd_solver.cpp:172] Iteration 700, lr = 0.000748052, m = 0.9, wd = 1e-05, gs = 1
I0511 17:36:53.742157   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:37:35.216517   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:38:20.611024   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:39:04.562110   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:39:31.154881   333 solver.cpp:354] Iteration 800 (0.623712 iter/s, 160.331s/100 iter), 30.1/376.5ep, loss = 2.22285
I0511 17:39:31.155395   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.57144 (* 1 = 1.57144 loss)
I0511 17:39:31.155616   333 sgd_solver.cpp:172] Iteration 800, lr = 0.000716393, m = 0.9, wd = 1e-05, gs = 1
I0511 17:39:43.769470   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:40:27.195207   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:41:11.945358   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:41:50.792615   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:42:08.869518   333 solver.cpp:354] Iteration 900 (0.634058 iter/s, 157.714s/100 iter), 33.9/376.5ep, loss = 2.24554
I0511 17:42:08.869776   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33181 (* 1 = 2.33181 loss)
I0511 17:42:08.869868   333 sgd_solver.cpp:172] Iteration 900, lr = 0.00068575, m = 0.9, wd = 1e-05, gs = 1
I0511 17:42:34.187516   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:43:18.382637   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:02.407367   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:44.571188   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:53.026440   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_1000.caffemodel
I0511 17:44:53.122733   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_1000.solverstate
I0511 17:44:53.192353   333 solver.cpp:637] Iteration 1000, Testing net (#0)
I0511 17:45:07.854907   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:45:08.164901   333 solver.cpp:749] class AP 1: 0.900153
I0511 17:45:08.165505   333 solver.cpp:749] class AP 2: 0.895976
I0511 17:45:08.165786   333 solver.cpp:749] class AP 3: 0.902156
I0511 17:45:08.165792   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899428
I0511 17:45:08.165817   333 solver.cpp:284] Tests completed in 179.296s
I0511 17:45:08.773604   333 solver.cpp:354] Iteration 1000 (0.557737 iter/s, 179.296s/100 iter), 37.6/376.5ep, loss = 2.29679
I0511 17:45:08.773689   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33482 (* 1 = 2.33482 loss)
I0511 17:45:08.773717   333 sgd_solver.cpp:172] Iteration 1000, lr = 0.0006561, m = 0.9, wd = 1e-05, gs = 1
I0511 17:45:36.144349   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:46:22.847509   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:04.607033   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:44.797724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:46.461377   333 solver.cpp:354] Iteration 1100 (0.634166 iter/s, 157.688s/100 iter), 41.4/376.5ep, loss = 2.19785
I0511 17:47:46.461544   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12194 (* 1 = 2.12194 loss)
I0511 17:47:46.461596   333 sgd_solver.cpp:172] Iteration 1100, lr = 0.000627422, m = 0.9, wd = 1e-05, gs = 1
I0511 17:48:28.288537   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:49:12.091766   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:49:51.407722   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:50:22.809286   333 solver.cpp:354] Iteration 1200 (0.6396 iter/s, 156.348s/100 iter), 45.2/376.5ep, loss = 2.26381
I0511 17:50:22.809757   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.14008 (* 1 = 2.14008 loss)
I0511 17:50:22.809865   333 sgd_solver.cpp:172] Iteration 1200, lr = 0.000599695, m = 0.9, wd = 1e-05, gs = 1
I0511 17:50:32.939734   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:51:20.879318   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:52:01.095721   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:52:40.156750   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:53:03.206368   333 solver.cpp:354] Iteration 1300 (0.623454 iter/s, 160.397s/100 iter), 48.9/376.5ep, loss = 2.35582
I0511 17:53:03.206826   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.73048 (* 1 = 2.73048 loss)
I0511 17:53:03.207052   333 sgd_solver.cpp:172] Iteration 1300, lr = 0.000572898, m = 0.9, wd = 1e-05, gs = 1
I0511 17:53:24.905720   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:54:05.658897   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:54:46.881399   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:55:28.374125   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:55:40.950692   333 solver.cpp:354] Iteration 1400 (0.633938 iter/s, 157.744s/100 iter), 52.7/376.5ep, loss = 2.1636
I0511 17:55:40.951102   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.95651 (* 1 = 1.95651 loss)
I0511 17:55:40.951282   333 sgd_solver.cpp:172] Iteration 1400, lr = 0.000547008, m = 0.9, wd = 1e-05, gs = 1
I0511 17:56:12.735571   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:56:53.579159   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:57:30.539316   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:15.557353   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:16.141642   333 solver.cpp:637] Iteration 1500, Testing net (#0)
I0511 17:58:30.913405   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:31.289669   333 solver.cpp:749] class AP 1: 0.901587
I0511 17:58:31.290128   333 solver.cpp:749] class AP 2: 0.885566
I0511 17:58:31.290326   333 solver.cpp:749] class AP 3: 0.903264
I0511 17:58:31.290338   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896806
I0511 17:58:31.290370   333 solver.cpp:284] Tests completed in 170.339s
I0511 17:58:31.733687   333 solver.cpp:354] Iteration 1500 (0.587063 iter/s, 170.339s/100 iter), 56.5/376.5ep, loss = 2.16692
I0511 17:58:31.733727   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.21362 (* 1 = 2.21362 loss)
I0511 17:58:31.733738   333 sgd_solver.cpp:172] Iteration 1500, lr = 0.000522006, m = 0.9, wd = 1e-05, gs = 1
I0511 17:59:09.987963   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:59:49.683116   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:00:31.653239   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:01:07.423645   333 solver.cpp:354] Iteration 1600 (0.642303 iter/s, 155.69s/100 iter), 60.2/376.5ep, loss = 2.27686
I0511 18:01:07.423733   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.01189 (* 1 = 2.01189 loss)
I0511 18:01:07.423758   333 sgd_solver.cpp:172] Iteration 1600, lr = 0.000497871, m = 0.9, wd = 1e-05, gs = 1
I0511 18:01:15.863561   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:01:57.228724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:02:35.436547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:03:18.770547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:03:42.631866   333 solver.cpp:354] Iteration 1700 (0.644297 iter/s, 155.208s/100 iter), 64/376.5ep, loss = 2.20515
I0511 18:03:42.631904   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.08376 (* 1 = 2.08376 loss)
I0511 18:03:42.631916   333 sgd_solver.cpp:172] Iteration 1700, lr = 0.000474583, m = 0.9, wd = 1e-05, gs = 1
I0511 18:04:03.960618   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:04:50.468394   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:05:31.360563   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:06:17.035137   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:06:30.699640   333 solver.cpp:354] Iteration 1800 (0.594999 iter/s, 168.068s/100 iter), 67.8/376.5ep, loss = 2.22852
I0511 18:06:30.699990   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.42132 (* 1 = 2.42132 loss)
I0511 18:06:30.700186   333 sgd_solver.cpp:172] Iteration 1800, lr = 0.000452122, m = 0.9, wd = 1e-05, gs = 1
I0511 18:07:00.961182   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:07:37.246225   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:08:20.879458   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:09:04.433501   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:09:07.426964   333 solver.cpp:354] Iteration 1900 (0.638052 iter/s, 156.727s/100 iter), 71.5/376.5ep, loss = 2.27271
I0511 18:09:07.427042   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.48626 (* 1 = 2.48626 loss)
I0511 18:09:07.427069   333 sgd_solver.cpp:172] Iteration 1900, lr = 0.000430467, m = 0.9, wd = 1e-05, gs = 1
I0511 18:09:42.385061   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:10:20.854490   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:09.156301   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:43.601752   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 18:11:43.663370   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 18:11:43.728310   333 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 18:11:55.097368   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:55.535323   333 solver.cpp:749] class AP 1: 0.903854
I0511 18:11:55.535676   333 solver.cpp:749] class AP 2: 0.893114
I0511 18:11:55.535877   333 solver.cpp:749] class AP 3: 0.902501
I0511 18:11:55.535888   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899823
I0511 18:11:55.535926   333 solver.cpp:284] Tests completed in 168.109s
I0511 18:11:55.965535   333 solver.cpp:354] Iteration 2000 (0.594853 iter/s, 168.109s/100 iter), 75.3/376.5ep, loss = 2.22376
I0511 18:11:55.965626   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.22239 (* 1 = 2.22239 loss)
I0511 18:11:55.965658   333 sgd_solver.cpp:172] Iteration 2000, lr = 0.0004096, m = 0.9, wd = 1e-05, gs = 1
I0511 18:11:57.897276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:12:39.867288   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:13:24.696393   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:14:10.261602   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:14:34.669576   333 solver.cpp:354] Iteration 2100 (0.630105 iter/s, 158.704s/100 iter), 79.1/376.5ep, loss = 2.2403
I0511 18:14:34.669703   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27341 (* 1 = 2.27341 loss)
I0511 18:14:34.669752   333 sgd_solver.cpp:172] Iteration 2100, lr = 0.000389501, m = 0.9, wd = 1e-05, gs = 1
I0511 18:14:48.245709   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:15:30.749893   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:16:13.501139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:16:55.315295   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:17:10.949285   333 solver.cpp:354] Iteration 2200 (0.63988 iter/s, 156.279s/100 iter), 82.8/376.5ep, loss = 2.27257
I0511 18:17:10.949703   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.39891 (* 1 = 2.39891 loss)
I0511 18:17:10.949903   333 sgd_solver.cpp:172] Iteration 2200, lr = 0.000370151, m = 0.9, wd = 1e-05, gs = 1
I0511 18:17:34.167419   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:18:14.629576   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:02.477895   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:42.704844   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:48.350114   333 solver.cpp:354] Iteration 2300 (0.635322 iter/s, 157.4s/100 iter), 86.6/376.5ep, loss = 2.0549
I0511 18:19:48.350543   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82411 (* 1 = 1.82411 loss)
I0511 18:19:48.350757   333 sgd_solver.cpp:172] Iteration 2300, lr = 0.00035153, m = 0.9, wd = 1e-05, gs = 1
I0511 18:20:23.300657   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:21:05.117789   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:21:43.527817   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:22:20.229408   333 solver.cpp:354] Iteration 2400 (0.658419 iter/s, 151.879s/100 iter), 90.4/376.5ep, loss = 2.04018
I0511 18:22:20.229952   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.86707 (* 1 = 1.86707 loss)
I0511 18:22:20.230144   333 sgd_solver.cpp:172] Iteration 2400, lr = 0.000333622, m = 0.9, wd = 1e-05, gs = 1
I0511 18:22:27.298089   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:23:06.215690   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:23:46.155441   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:24:32.111308   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:24:57.386494   333 solver.cpp:637] Iteration 2500, Testing net (#0)
I0511 18:25:13.420470   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:25:13.923166   333 solver.cpp:749] class AP 1: 0.902947
I0511 18:25:13.923579   333 solver.cpp:749] class AP 2: 0.891763
I0511 18:25:13.923779   333 solver.cpp:749] class AP 3: 0.903372
I0511 18:25:13.923785   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89936
I0511 18:25:13.923812   333 solver.cpp:284] Tests completed in 173.694s
I0511 18:25:14.353237   333 solver.cpp:354] Iteration 2500 (0.575725 iter/s, 173.694s/100 iter), 94.1/376.5ep, loss = 2.03225
I0511 18:25:14.353325   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.86228 (* 1 = 1.86228 loss)
I0511 18:25:14.353358   333 sgd_solver.cpp:172] Iteration 2500, lr = 0.000316406, m = 0.9, wd = 1e-05, gs = 1
I0511 18:25:19.602969   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:26:00.657719   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:26:42.633878   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:27:22.068154   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:27:42.064335   333 solver.cpp:354] Iteration 2600 (0.676998 iter/s, 147.711s/100 iter), 97.9/376.5ep, loss = 2.1111
I0511 18:27:42.064678   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.63182 (* 1 = 1.63182 loss)
I0511 18:27:42.064786   333 sgd_solver.cpp:172] Iteration 2600, lr = 0.000299866, m = 0.9, wd = 1e-05, gs = 1
I0511 18:28:03.605492   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:28:49.804783   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:29:30.085525   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:30:08.500242   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:30:19.853060   333 solver.cpp:354] Iteration 2700 (0.63376 iter/s, 157.788s/100 iter), 101.6/376.5ep, loss = 2.31247
I0511 18:30:19.853147   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33624 (* 1 = 2.33624 loss)
I0511 18:30:19.853176   333 sgd_solver.cpp:172] Iteration 2700, lr = 0.000283982, m = 0.9, wd = 1e-05, gs = 1
I0511 18:30:55.610414   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:31:39.330112   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:32:16.456498   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:32:57.606693   333 solver.cpp:354] Iteration 2800 (0.633901 iter/s, 157.753s/100 iter), 105.4/376.5ep, loss = 2.06867
I0511 18:32:57.607113   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.23668 (* 1 = 2.23668 loss)
I0511 18:32:57.607297   333 sgd_solver.cpp:172] Iteration 2800, lr = 0.000268739, m = 0.9, wd = 1e-05, gs = 1
I0511 18:32:57.950987   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:33:37.820617   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:34:22.074807   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:35:04.936424   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:35:33.652398   333 solver.cpp:354] Iteration 2900 (0.640839 iter/s, 156.045s/100 iter), 109.2/376.5ep, loss = 2.14422
I0511 18:35:33.652766   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.20666 (* 1 = 2.20666 loss)
I0511 18:35:33.652966   333 sgd_solver.cpp:172] Iteration 2900, lr = 0.000254117, m = 0.9, wd = 1e-05, gs = 1
I0511 18:35:48.573292   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:36:29.234525   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:37:13.252988   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:37:56.083349   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:38:16.775661   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_3000.caffemodel
I0511 18:38:16.818892   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_3000.solverstate
I0511 18:38:16.858485   333 solver.cpp:637] Iteration 3000, Testing net (#0)
I0511 18:38:31.198621   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:38:31.766930   333 solver.cpp:749] class AP 1: 0.899426
I0511 18:38:31.767313   333 solver.cpp:749] class AP 2: 0.897494
I0511 18:38:31.767520   333 solver.cpp:749] class AP 3: 0.903234
I0511 18:38:31.767526   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900051
I0511 18:38:31.767554   333 solver.cpp:284] Tests completed in 178.115s
I0511 18:38:32.225045   333 solver.cpp:354] Iteration 3000 (0.561435 iter/s, 178.115s/100 iter), 112.9/376.5ep, loss = 2.20762
I0511 18:38:32.225133   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12977 (* 1 = 2.12977 loss)
I0511 18:38:32.225163   333 sgd_solver.cpp:172] Iteration 3000, lr = 0.0002401, m = 0.9, wd = 1e-05, gs = 1
I0511 18:38:47.630647   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:39:28.963418   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:40:10.237226   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:40:52.660075   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:41:03.604025   333 solver.cpp:354] Iteration 3100 (0.660595 iter/s, 151.379s/100 iter), 116.7/376.5ep, loss = 2.16867
I0511 18:41:03.604110   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82897 (* 1 = 1.82897 loss)
I0511 18:41:03.604135   333 sgd_solver.cpp:172] Iteration 3100, lr = 0.000226671, m = 0.9, wd = 1e-05, gs = 1
I0511 18:41:36.891857   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:42:17.578469   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:42:58.889701   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:43:39.818974   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:43:41.200614   333 solver.cpp:354] Iteration 3200 (0.634533 iter/s, 157.596s/100 iter), 120.5/376.5ep, loss = 2.06938
I0511 18:43:41.200793   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.00634 (* 1 = 2.00634 loss)
I0511 18:43:41.200858   333 sgd_solver.cpp:172] Iteration 3200, lr = 0.000213814, m = 0.9, wd = 1e-05, gs = 1
I0511 18:44:24.222885   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:45:06.174834   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:45:47.801210   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:46:25.766161   333 solver.cpp:354] Iteration 3300 (0.607662 iter/s, 164.565s/100 iter), 124.2/376.5ep, loss = 2.2489
I0511 18:46:25.766561   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74222 (* 1 = 1.74222 loss)
I0511 18:46:25.766767   333 sgd_solver.cpp:172] Iteration 3300, lr = 0.000201511, m = 0.9, wd = 1e-05, gs = 1
I0511 18:46:32.646620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:47:15.757292   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:47:55.306365   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:48:39.434648   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:49:04.382586   333 solver.cpp:354] Iteration 3400 (0.630456 iter/s, 158.615s/100 iter), 128/376.5ep, loss = 2.13052
I0511 18:49:04.382772   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.28424 (* 1 = 2.28424 loss)
I0511 18:49:04.382841   333 sgd_solver.cpp:172] Iteration 3400, lr = 0.000189747, m = 0.9, wd = 1e-05, gs = 1
I0511 18:49:28.654067   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:50:05.578708   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:50:48.657356   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:51:36.528040   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:51:48.473649   333 solver.cpp:637] Iteration 3500, Testing net (#0)
I0511 18:52:01.724843   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:52:02.359068   333 solver.cpp:749] class AP 1: 0.901739
I0511 18:52:02.359395   333 solver.cpp:749] class AP 2: 0.894491
I0511 18:52:02.359593   333 solver.cpp:749] class AP 3: 0.903358
I0511 18:52:02.359602   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899863
I0511 18:52:02.359632   333 solver.cpp:284] Tests completed in 177.976s
I0511 18:52:02.776721   333 solver.cpp:354] Iteration 3500 (0.561873 iter/s, 177.976s/100 iter), 131.8/376.5ep, loss = 2.08863
I0511 18:52:02.776816   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.79966 (* 1 = 1.79966 loss)
I0511 18:52:02.776844   333 sgd_solver.cpp:172] Iteration 3500, lr = 0.000178506, m = 0.9, wd = 1e-05, gs = 1
I0511 18:52:23.673970   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:53:08.379017   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:53:47.937162   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:54:28.750699   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:54:31.163288   333 solver.cpp:354] Iteration 3600 (0.673919 iter/s, 148.386s/100 iter), 135.5/376.5ep, loss = 2.07714
I0511 18:54:31.163471   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.83491 (* 1 = 1.83491 loss)
I0511 18:54:31.163538   333 sgd_solver.cpp:172] Iteration 3600, lr = 0.000167772, m = 0.9, wd = 1e-05, gs = 1
I0511 18:55:11.932245   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:55:49.697877   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:56:35.533568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:57:14.244565   333 solver.cpp:354] Iteration 3700 (0.613194 iter/s, 163.081s/100 iter), 139.3/376.5ep, loss = 2.07412
I0511 18:57:14.244861   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.93443 (* 1 = 1.93443 loss)
I0511 18:57:14.244930   333 sgd_solver.cpp:172] Iteration 3700, lr = 0.00015753, m = 0.9, wd = 1e-05, gs = 1
I0511 18:57:17.763350   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:57:56.776213   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:58:43.296139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:59:22.801322   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:59:49.615442   333 solver.cpp:354] Iteration 3800 (0.643624 iter/s, 155.37s/100 iter), 143.1/376.5ep, loss = 2.26262
I0511 18:59:49.615830   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.11317 (* 1 = 2.11317 loss)
I0511 18:59:49.616019   333 sgd_solver.cpp:172] Iteration 3800, lr = 0.000147763, m = 0.9, wd = 1e-05, gs = 1
I0511 19:00:04.168287   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:00:49.451126   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:01:30.498494   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:02:10.791354   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:02:29.300794   333 solver.cpp:354] Iteration 3900 (0.626234 iter/s, 159.685s/100 iter), 146.8/376.5ep, loss = 2.15146
I0511 19:02:29.301172   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.87135 (* 1 = 1.87135 loss)
I0511 19:02:29.301383   333 sgd_solver.cpp:172] Iteration 3900, lr = 0.000138458, m = 0.9, wd = 1e-05, gs = 1
I0511 19:02:54.985561   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:03:35.003332   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:04:16.469045   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:01.983453   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:02.674352   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0511 19:05:02.729351   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0511 19:05:02.817267   333 solver.cpp:637] Iteration 4000, Testing net (#0)
I0511 19:05:17.901135   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:18.597537   333 solver.cpp:749] class AP 1: 0.901215
I0511 19:05:18.597880   333 solver.cpp:749] class AP 2: 0.894794
I0511 19:05:18.598089   333 solver.cpp:749] class AP 3: 0.9034
I0511 19:05:18.598098   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899803
I0511 19:05:18.598122   333 solver.cpp:284] Tests completed in 169.297s
I0511 19:05:19.025316   333 solver.cpp:354] Iteration 4000 (0.590679 iter/s, 169.297s/100 iter), 150.6/376.5ep, loss = 2.0302
I0511 19:05:19.025396   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.70781 (* 1 = 1.70781 loss)
I0511 19:05:19.025425   333 sgd_solver.cpp:172] Iteration 4000, lr = 0.0001296, m = 0.9, wd = 1e-05, gs = 1
I0511 19:05:49.007318   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:06:29.712373   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:07:17.207667   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:07:54.277428   333 solver.cpp:354] Iteration 4100 (0.644116 iter/s, 155.252s/100 iter), 154.4/376.5ep, loss = 2.03315
I0511 19:07:54.277530   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74185 (* 1 = 1.74185 loss)
I0511 19:07:54.277559   333 sgd_solver.cpp:172] Iteration 4100, lr = 0.000121174, m = 0.9, wd = 1e-05, gs = 1
I0511 19:07:57.258788   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:08:36.801816   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:09:20.991451   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:10:03.795454   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:10:32.067924   333 solver.cpp:354] Iteration 4200 (0.633754 iter/s, 157.79s/100 iter), 158.1/376.5ep, loss = 2.00974
I0511 19:10:32.067960   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.5257 (* 1 = 1.5257 loss)
I0511 19:10:32.067970   333 sgd_solver.cpp:172] Iteration 4200, lr = 0.000113165, m = 0.9, wd = 1e-05, gs = 1
I0511 19:10:46.294337   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:11:25.306128   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:12:14.035629   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:12:51.068722   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:13:12.803186   333 solver.cpp:354] Iteration 4300 (0.622143 iter/s, 160.735s/100 iter), 161.9/376.5ep, loss = 2.18249
I0511 19:13:12.803400   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.72748 (* 1 = 1.72748 loss)
I0511 19:13:12.803458   333 sgd_solver.cpp:172] Iteration 4300, lr = 0.00010556, m = 0.9, wd = 1e-05, gs = 1
I0511 19:13:38.470691   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:14:17.873144   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:14:58.854243   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:15:45.234310   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:15:50.840242   333 solver.cpp:354] Iteration 4400 (0.632765 iter/s, 158.037s/100 iter), 165.6/376.5ep, loss = 2.08601
I0511 19:15:50.840674   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.68042 (* 1 = 2.68042 loss)
I0511 19:15:50.840831   333 sgd_solver.cpp:172] Iteration 4400, lr = 9.8345e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:16:23.401515   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:17:04.504091   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:17:44.851847   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:24.403488   333 solver.cpp:637] Iteration 4500, Testing net (#0)
I0511 19:18:26.190488   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:38.964437   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:39.724462   333 solver.cpp:749] class AP 1: 0.900225
I0511 19:18:39.724790   333 solver.cpp:749] class AP 2: 0.894932
I0511 19:18:39.724978   333 solver.cpp:749] class AP 3: 0.90345
I0511 19:18:39.724985   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899535
I0511 19:18:39.725013   333 solver.cpp:284] Tests completed in 168.884s
I0511 19:18:40.230610   333 solver.cpp:354] Iteration 4500 (0.592121 iter/s, 168.884s/100 iter), 169.4/376.5ep, loss = 2.10947
I0511 19:18:40.230695   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.88875 (* 1 = 1.88875 loss)
I0511 19:18:40.230726   333 sgd_solver.cpp:172] Iteration 4500, lr = 9.15063e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:19:19.545835   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:19:57.820793   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:20:36.446559   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:21:08.981542   333 solver.cpp:354] Iteration 4600 (0.672266 iter/s, 148.751s/100 iter), 173.2/376.5ep, loss = 2.22177
I0511 19:21:08.982142   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.95665 (* 1 = 1.95665 loss)
I0511 19:21:08.982403   333 sgd_solver.cpp:172] Iteration 4600, lr = 8.50305e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:21:19.327275   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:22:05.401099   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:22:42.507550   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:23:23.840378   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:23:45.820061   333 solver.cpp:354] Iteration 4700 (0.637594 iter/s, 156.84s/100 iter), 176.9/376.5ep, loss = 1.84253
I0511 19:23:45.820096   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.85887 (* 1 = 1.85887 loss)
I0511 19:23:45.820106   333 sgd_solver.cpp:172] Iteration 4700, lr = 7.89048e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:24:07.387917   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:24:48.432276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:25:30.404213   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:26:13.927551   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:26:25.360671   333 solver.cpp:354] Iteration 4800 (0.626796 iter/s, 159.541s/100 iter), 180.7/376.5ep, loss = 2.15876
I0511 19:26:25.360754   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.14312 (* 1 = 2.14312 loss)
I0511 19:26:25.360782   333 sgd_solver.cpp:172] Iteration 4800, lr = 7.31161e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:26:59.899405   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:27:38.053601   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:28:19.582867   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:29:04.041452   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:29:05.519140   333 solver.cpp:354] Iteration 4900 (0.624379 iter/s, 160.159s/100 iter), 184.5/376.5ep, loss = 2.14678
I0511 19:29:05.519594   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27382 (* 1 = 2.27382 loss)
I0511 19:29:05.519819   333 sgd_solver.cpp:172] Iteration 4900, lr = 6.7652e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:29:43.849869   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:30:23.743458   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:01.877153   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:35.561861   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_5000.caffemodel
I0511 19:31:35.613117   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_5000.solverstate
I0511 19:31:35.648772   333 solver.cpp:637] Iteration 5000, Testing net (#0)
I0511 19:31:49.057711   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:49.885879   333 solver.cpp:749] class AP 1: 0.901071
I0511 19:31:49.886230   333 solver.cpp:749] class AP 2: 0.894496
I0511 19:31:49.886426   333 solver.cpp:749] class AP 3: 0.903653
I0511 19:31:49.886435   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89974
I0511 19:31:49.886466   333 solver.cpp:284] Tests completed in 164.368s
I0511 19:31:50.387248   333 solver.cpp:354] Iteration 5000 (0.608392 iter/s, 164.368s/100 iter), 188.2/376.5ep, loss = 2.06687
I0511 19:31:50.387336   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.8139 (* 1 = 1.8139 loss)
I0511 19:31:50.387370   333 sgd_solver.cpp:172] Iteration 5000, lr = 6.25e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:31:53.166568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:32:34.955423   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:33:14.951212   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:33:57.944811   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:34:20.971208   333 solver.cpp:354] Iteration 5100 (0.66408 iter/s, 150.584s/100 iter), 192/376.5ep, loss = 2.14621
I0511 19:34:20.971696   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27968 (* 1 = 2.27968 loss)
I0511 19:34:20.971891   333 sgd_solver.cpp:172] Iteration 5100, lr = 5.7648e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:34:41.494321   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:35:20.537277   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:36:03.843325   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:36:46.659878   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:37:01.749354   333 solver.cpp:354] Iteration 5200 (0.621975 iter/s, 160.778s/100 iter), 195.8/376.5ep, loss = 2.20325
I0511 19:37:01.749445   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.17569 (* 1 = 2.17569 loss)
I0511 19:37:01.749478   333 sgd_solver.cpp:172] Iteration 5200, lr = 5.30842e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:37:28.218608   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:38:08.579845   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:38:48.253741   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:39:32.126600   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:39:33.429520   333 solver.cpp:354] Iteration 5300 (0.659282 iter/s, 151.68s/100 iter), 199.5/376.5ep, loss = 2.08597
I0511 19:39:33.429601   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.93086 (* 1 = 1.93086 loss)
I0511 19:39:33.429630   333 sgd_solver.cpp:172] Iteration 5300, lr = 4.87968e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:40:09.510433   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:40:56.328109   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:41:37.030686   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:42:13.194873   333 solver.cpp:354] Iteration 5400 (0.625918 iter/s, 159.765s/100 iter), 203.3/376.5ep, loss = 2.18293
I0511 19:42:13.195312   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12727 (* 1 = 2.12727 loss)
I0511 19:42:13.195466   333 sgd_solver.cpp:172] Iteration 5400, lr = 4.47746e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:42:18.449568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:43:03.337172   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:43:42.374411   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:44:25.810364   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:44:48.626117   333 solver.cpp:637] Iteration 5500, Testing net (#0)
I0511 19:45:03.002197   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:45:03.899068   333 solver.cpp:749] class AP 1: 0.9017
I0511 19:45:03.899384   333 solver.cpp:749] class AP 2: 0.894776
I0511 19:45:03.899564   333 solver.cpp:749] class AP 3: 0.903797
I0511 19:45:03.899569   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900091
I0511 19:45:03.899595   333 solver.cpp:284] Tests completed in 170.705s
I0511 19:45:04.347746   333 solver.cpp:354] Iteration 5500 (0.585807 iter/s, 170.705s/100 iter), 207.1/376.5ep, loss = 2.01783
I0511 19:45:04.347832   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.36785 (* 1 = 2.36785 loss)
I0511 19:45:04.347865   333 sgd_solver.cpp:172] Iteration 5500, lr = 4.10062e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:45:14.127439   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:45:59.024170   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:46:38.602417   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:47:26.992494   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:47:39.051219   333 solver.cpp:354] Iteration 5600 (0.646398 iter/s, 154.703s/100 iter), 210.8/376.5ep, loss = 2.09941
I0511 19:47:39.051719   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.42673 (* 1 = 2.42673 loss)
I0511 19:47:39.051920   333 sgd_solver.cpp:172] Iteration 5600, lr = 3.7481e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:48:03.793782   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:48:50.710464   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:49:30.663128   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:50:12.062755   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:50:20.091789   333 solver.cpp:354] Iteration 5700 (0.620962 iter/s, 161.04s/100 iter), 214.6/376.5ep, loss = 1.9082
I0511 19:50:20.092170   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74167 (* 1 = 1.74167 loss)
I0511 19:50:20.092308   333 sgd_solver.cpp:172] Iteration 5700, lr = 3.4188e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:50:57.793723   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:51:40.231519   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:52:22.650105   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:52:57.699731   333 solver.cpp:354] Iteration 5800 (0.634486 iter/s, 157.608s/100 iter), 218.4/376.5ep, loss = 1.93827
I0511 19:52:57.700093   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.91764 (* 1 = 1.91764 loss)
I0511 19:52:57.700191   333 sgd_solver.cpp:172] Iteration 5800, lr = 3.1117e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:53:04.791491   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:53:46.456118   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:54:28.475489   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:55:09.266777   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:55:40.967952   333 solver.cpp:354] Iteration 5900 (0.612489 iter/s, 163.268s/100 iter), 222.1/376.5ep, loss = 1.99071
I0511 19:55:40.968020   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.87105 (* 1 = 1.87105 loss)
I0511 19:55:40.968030   333 sgd_solver.cpp:172] Iteration 5900, lr = 2.82576e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:55:54.954825   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:56:30.833425   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:57:13.155719   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:57:57.059520   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:58:14.870103   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0511 19:58:14.893638   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0511 19:58:14.916656   333 solver.cpp:637] Iteration 6000, Testing net (#0)
I0511 19:58:27.519702   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:58:28.474046   333 solver.cpp:749] class AP 1: 0.903364
I0511 19:58:28.474370   333 solver.cpp:749] class AP 2: 0.894245
I0511 19:58:28.474565   333 solver.cpp:749] class AP 3: 0.903755
I0511 19:58:28.474572   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900454
I0511 19:58:28.474601   333 solver.cpp:284] Tests completed in 167.507s
I0511 19:58:28.981010   333 solver.cpp:354] Iteration 6000 (0.596992 iter/s, 167.507s/100 iter), 225.9/376.5ep, loss = 2.06508
I0511 19:58:28.981094   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.26495 (* 1 = 2.26495 loss)
I0511 19:58:28.981125   333 sgd_solver.cpp:172] Iteration 6000, lr = 2.56e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:58:45.964639   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:59:30.016384   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:09.044770   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:50.886214   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:59.848747   333 solver.cpp:354] Iteration 6100 (0.662833 iter/s, 150.868s/100 iter), 229.6/376.5ep, loss = 2.11894
I0511 20:00:59.848866   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.36641 (* 1 = 2.36641 loss)
I0511 20:00:59.848908   333 sgd_solver.cpp:172] Iteration 6100, lr = 2.31344e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:01:31.579408   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:02:16.673519   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:02:58.749572   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:03:38.422562   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:03:39.183970   333 solver.cpp:354] Iteration 6200 (0.627608 iter/s, 159.335s/100 iter), 233.4/376.5ep, loss = 2.07418
I0511 20:03:39.184159   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.84768 (* 1 = 1.84768 loss)
I0511 20:03:39.184222   333 sgd_solver.cpp:172] Iteration 6200, lr = 2.08514e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:04:20.552605   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:05:06.445207   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:05:43.039703   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:06:13.098628   333 solver.cpp:354] Iteration 6300 (0.649711 iter/s, 153.915s/100 iter), 237.2/376.5ep, loss = 2.09865
I0511 20:06:13.098876   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.79895 (* 1 = 1.79895 loss)
I0511 20:06:13.098950   333 sgd_solver.cpp:172] Iteration 6300, lr = 1.87416e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:06:24.556910   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:07:08.267098   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:07:47.172108   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:08:27.381362   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:08:52.831589   333 solver.cpp:354] Iteration 6400 (0.626045 iter/s, 159.733s/100 iter), 240.9/376.5ep, loss = 2.05062
I0511 20:08:52.831668   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.30984 (* 1 = 2.30984 loss)
I0511 20:08:52.831697   333 sgd_solver.cpp:172] Iteration 6400, lr = 1.67962e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:09:17.226994   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:09:59.683701   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:10:40.216989   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:18.505347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:29.927526   333 solver.cpp:637] Iteration 6500, Testing net (#0)
I0511 20:11:45.537655   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:46.558358   333 solver.cpp:749] class AP 1: 0.901561
I0511 20:11:46.558682   333 solver.cpp:749] class AP 2: 0.892937
I0511 20:11:46.558883   333 solver.cpp:749] class AP 3: 0.903572
I0511 20:11:46.558894   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899357
I0511 20:11:46.558931   333 solver.cpp:284] Tests completed in 173.727s
I0511 20:11:46.999064   333 solver.cpp:354] Iteration 6500 (0.575615 iter/s, 173.727s/100 iter), 244.7/376.5ep, loss = 2.34782
I0511 20:11:46.999156   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.38831 (* 1 = 2.38831 loss)
I0511 20:11:46.999188   333 sgd_solver.cpp:172] Iteration 6500, lr = 1.50063e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:12:12.384449   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:12:54.578227   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:13:32.474273   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:14:18.048465   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:14:19.799342   333 solver.cpp:354] Iteration 6600 (0.65445 iter/s, 152.8s/100 iter), 248.5/376.5ep, loss = 2.18753
I0511 20:14:19.799383   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.54035 (* 1 = 2.54035 loss)
I0511 20:14:19.799392   333 sgd_solver.cpp:172] Iteration 6600, lr = 1.33634e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:14:59.739039   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:15:37.114758   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:16:17.561978   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:16:50.229394   333 solver.cpp:354] Iteration 6700 (0.664762 iter/s, 150.43s/100 iter), 252.2/376.5ep, loss = 2.16264
I0511 20:16:50.229940   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.62793 (* 1 = 1.62793 loss)
I0511 20:16:50.230186   333 sgd_solver.cpp:172] Iteration 6700, lr = 1.18592e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:17:01.219527   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:17:39.833256   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:18:19.635516   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:19:00.760093   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:19:24.686924   333 solver.cpp:354] Iteration 6800 (0.647427 iter/s, 154.458s/100 iter), 256/376.5ep, loss = 1.91357
I0511 20:19:24.687258   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.0049 (* 1 = 2.0049 loss)
I0511 20:19:24.687430   333 sgd_solver.cpp:172] Iteration 6800, lr = 1.04858e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:19:45.261571   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:20:24.420707   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:21:03.373297   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:21:46.095031   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:22:00.828568   333 solver.cpp:354] Iteration 6900 (0.640445 iter/s, 156.141s/100 iter), 259.8/376.5ep, loss = 2.08973
I0511 20:22:00.828958   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82734 (* 1 = 1.82734 loss)
I0511 20:22:00.829071   333 sgd_solver.cpp:172] Iteration 6900, lr = 9.23521e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:22:29.671348   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:23:09.704118   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:23:56.205437   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:37.849931   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:39.806205   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_7000.caffemodel
I0511 20:24:39.828034   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_7000.solverstate
I0511 20:24:39.849496   333 solver.cpp:637] Iteration 7000, Testing net (#0)
I0511 20:24:53.843796   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:54.929301   333 solver.cpp:749] class AP 1: 0.901893
I0511 20:24:54.929613   333 solver.cpp:749] class AP 2: 0.892473
I0511 20:24:54.929797   333 solver.cpp:749] class AP 3: 0.903604
I0511 20:24:54.929803   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899323
I0511 20:24:54.929831   333 solver.cpp:284] Tests completed in 174.101s
I0511 20:24:55.447754   333 solver.cpp:354] Iteration 7000 (0.574379 iter/s, 174.101s/100 iter), 263.5/376.5ep, loss = 2.03306
I0511 20:24:55.447873   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27706 (* 1 = 2.27706 loss)
I0511 20:24:55.447921   333 sgd_solver.cpp:172] Iteration 7000, lr = 8.1e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:25:27.499387   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:26:13.159543   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:26:56.920620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:27:30.162551   333 solver.cpp:354] Iteration 7100 (0.646351 iter/s, 154.715s/100 iter), 267.3/376.5ep, loss = 1.99298
I0511 20:27:30.162861   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.98301 (* 1 = 1.98301 loss)
I0511 20:27:30.162935   333 sgd_solver.cpp:172] Iteration 7100, lr = 7.07281e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:27:36.013757   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:28:18.960119   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:29:02.724834   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:29:47.536797   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:30:11.925447   333 solver.cpp:354] Iteration 7200 (0.618189 iter/s, 161.763s/100 iter), 271.1/376.5ep, loss = 2.10133
I0511 20:30:11.925707   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.44128 (* 1 = 2.44128 loss)
I0511 20:30:11.925827   333 sgd_solver.cpp:172] Iteration 7200, lr = 6.14656e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:30:32.539546   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:31:09.955901   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:31:56.346940   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:32:36.103513   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:32:50.682014   333 solver.cpp:354] Iteration 7300 (0.629888 iter/s, 158.758s/100 iter), 274.8/376.5ep, loss = 2.13205
I0511 20:32:50.682054   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.0813 (* 1 = 2.0813 loss)
I0511 20:32:50.682063   333 sgd_solver.cpp:172] Iteration 7300, lr = 5.31441e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:33:18.065133   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:34:04.092648   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:34:47.243006   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:35:27.977479   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:35:36.891813   333 solver.cpp:354] Iteration 7400 (0.601638 iter/s, 166.213s/100 iter), 278.6/376.5ep, loss = 2.04621
I0511 20:35:36.891988   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.53125 (* 1 = 1.53125 loss)
I0511 20:35:36.892036   333 sgd_solver.cpp:172] Iteration 7400, lr = 4.56976e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:36:06.833580   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:36:50.665303   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:37:31.463693   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:09.387392   333 solver.cpp:637] Iteration 7500, Testing net (#0)
I0511 20:38:12.455142   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:22.955489   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:24.106088   333 solver.cpp:749] class AP 1: 0.901765
I0511 20:38:24.106434   333 solver.cpp:749] class AP 2: 0.891668
I0511 20:38:24.106623   333 solver.cpp:749] class AP 3: 0.90364
I0511 20:38:24.106631   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899024
I0511 20:38:24.106657   333 solver.cpp:284] Tests completed in 167.217s
I0511 20:38:24.613920   333 solver.cpp:354] Iteration 7500 (0.598025 iter/s, 167.217s/100 iter), 282.4/376.5ep, loss = 1.98951
I0511 20:38:24.614004   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.20505 (* 1 = 2.20505 loss)
I0511 20:38:24.614032   333 sgd_solver.cpp:172] Iteration 7500, lr = 3.90625e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:39:02.539664   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:39:48.095142   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:40:33.083389   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:41:02.386482   333 solver.cpp:354] Iteration 7600 (0.633817 iter/s, 157.774s/100 iter), 286.1/376.5ep, loss = 2.1258
I0511 20:41:02.386865   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.19024 (* 1 = 2.19024 loss)
I0511 20:41:02.387060   333 sgd_solver.cpp:172] Iteration 7600, lr = 3.31776e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:41:14.679160   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:42:01.336994   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:42:45.205490   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:43:24.274483   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:43:46.210186   333 solver.cpp:354] Iteration 7700 (0.610407 iter/s, 163.825s/100 iter), 289.9/376.5ep, loss = 2.14316
I0511 20:43:46.210389   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.67414 (* 1 = 1.67414 loss)
I0511 20:43:46.210458   333 sgd_solver.cpp:172] Iteration 7700, lr = 2.79841e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:44:06.188493   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:44:51.800328   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:45:36.621703   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:46:18.036373   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:46:26.962218   333 solver.cpp:354] Iteration 7800 (0.622072 iter/s, 160.753s/100 iter), 293.6/376.5ep, loss = 1.92477
I0511 20:46:26.962455   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.9255 (* 1 = 1.9255 loss)
I0511 20:46:26.962520   333 sgd_solver.cpp:172] Iteration 7800, lr = 2.34256e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:47:03.351625   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:47:44.307417   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:48:26.429347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:49:08.857781   333 solver.cpp:354] Iteration 7900 (0.617679 iter/s, 161.896s/100 iter), 297.4/376.5ep, loss = 2.01367
I0511 20:49:08.858206   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.78698 (* 1 = 1.78698 loss)
I0511 20:49:08.858331   333 sgd_solver.cpp:172] Iteration 7900, lr = 1.94481e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:49:09.658891   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:49:51.460252   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:50:31.253098   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:14.431957   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:41.935463   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0511 20:51:41.955485   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0511 20:51:41.974681   333 solver.cpp:637] Iteration 8000, Testing net (#0)
I0511 20:51:56.240662   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:57.453459   333 solver.cpp:749] class AP 1: 0.90197
I0511 20:51:57.453791   333 solver.cpp:749] class AP 2: 0.891493
I0511 20:51:57.453979   333 solver.cpp:749] class AP 3: 0.903665
I0511 20:51:57.453984   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899043
I0511 20:51:57.454010   333 solver.cpp:284] Tests completed in 168.597s
I0511 20:51:57.874972   333 solver.cpp:354] Iteration 8000 (0.59313 iter/s, 168.597s/100 iter), 301.2/376.5ep, loss = 2.07895
I0511 20:51:57.875056   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.09299 (* 1 = 2.09299 loss)
I0511 20:51:57.875087   333 sgd_solver.cpp:172] Iteration 8000, lr = 1.6e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:52:01.756975   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:52:45.760192   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:53:28.238932   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:54:10.453548   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:54:33.062490   333 solver.cpp:354] Iteration 8100 (0.644379 iter/s, 155.188s/100 iter), 304.9/376.5ep, loss = 1.99671
I0511 20:54:33.062669   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.85317 (* 1 = 1.85317 loss)
I0511 20:54:33.062731   333 sgd_solver.cpp:172] Iteration 8100, lr = 1.30321e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:54:54.600567   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:55:42.416186   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:56:19.824044   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:57:00.943271   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:57:13.574784   333 solver.cpp:354] Iteration 8200 (0.623003 iter/s, 160.513s/100 iter), 308.7/376.5ep, loss = 2.13371
I0511 20:57:13.575054   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.81334 (* 1 = 1.81334 loss)
I0511 20:57:13.575150   333 sgd_solver.cpp:172] Iteration 8200, lr = 1.04976e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:57:47.227385   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:58:29.210474   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:07.568279   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:55.976347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:56.957222   333 solver.cpp:354] Iteration 8300 (0.612059 iter/s, 163.383s/100 iter), 312.5/376.5ep, loss = 2.18343
I0511 20:59:56.957448   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.32801 (* 1 = 2.32801 loss)
I0511 20:59:56.957516   333 sgd_solver.cpp:172] Iteration 8300, lr = 8.3521e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:00:35.984805   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:01:15.671908   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:01:59.514173   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:02:36.771632   333 solver.cpp:354] Iteration 8400 (0.625724 iter/s, 159.815s/100 iter), 316.2/376.5ep, loss = 2.17791
I0511 21:02:36.771929   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.84049 (* 1 = 1.84049 loss)
I0511 21:02:36.772028   333 sgd_solver.cpp:172] Iteration 8400, lr = 6.5536e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:02:44.454139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:03:26.891999   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:04:09.673383   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:04:53.429077   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:05:18.184871   333 solver.cpp:637] Iteration 8500, Testing net (#0)
I0511 21:05:30.283288   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:05:31.559095   333 solver.cpp:749] class AP 1: 0.901647
I0511 21:05:31.559430   333 solver.cpp:749] class AP 2: 0.891616
I0511 21:05:31.559624   333 solver.cpp:749] class AP 3: 0.903371
I0511 21:05:31.559633   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.898878
I0511 21:05:31.559662   333 solver.cpp:284] Tests completed in 174.789s
I0511 21:05:31.990664   333 solver.cpp:354] Iteration 8500 (0.57212 iter/s, 174.789s/100 iter), 320/376.5ep, loss = 2.13822
I0511 21:05:31.990744   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.97944 (* 1 = 1.97944 loss)
I0511 21:05:31.990775   333 sgd_solver.cpp:172] Iteration 8500, lr = 5.0625e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:05:46.790441   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:06:27.364746   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:07:04.395090   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:07:48.378629   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:08:03.116268   333 solver.cpp:354] Iteration 8600 (0.661699 iter/s, 151.126s/100 iter), 323.8/376.5ep, loss = 2.27183
I0511 21:08:03.116860   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.91977 (* 1 = 1.91977 loss)
I0511 21:08:03.117131   333 sgd_solver.cpp:172] Iteration 8600, lr = 3.8416e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:08:31.343760   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:09:10.999379   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:09:53.228312   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:10:36.317342   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:10:38.914086   333 solver.cpp:354] Iteration 8700 (0.641856 iter/s, 155.798s/100 iter), 327.5/376.5ep, loss = 1.98083
I0511 21:10:38.914260   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.07024 (* 1 = 2.07024 loss)
I0511 21:10:38.914325   333 sgd_solver.cpp:172] Iteration 8700, lr = 2.8561e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:11:19.195433   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:11:58.426579   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:12:39.896157   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:13:12.742578   333 solver.cpp:354] Iteration 8800 (0.650073 iter/s, 153.829s/100 iter), 331.3/376.5ep, loss = 2.25732
I0511 21:13:12.742779   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.9723 (* 1 = 1.9723 loss)
I0511 21:13:12.742825   333 sgd_solver.cpp:172] Iteration 8800, lr = 2.0736e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:13:21.572659   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:14:03.181809   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:14:47.657747   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:15:36.479804   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:16:00.106556   333 solver.cpp:354] Iteration 8900 (0.597499 iter/s, 167.364s/100 iter), 335.1/376.5ep, loss = 2.04262
I0511 21:16:00.106813   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.00751 (* 1 = 2.00751 loss)
I0511 21:16:00.106899   333 sgd_solver.cpp:172] Iteration 8900, lr = 1.4641e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:16:18.145308   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:16:57.771366   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:17:39.250295   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:26.392151   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:38.295130   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_9000.caffemodel
I0511 21:18:38.365571   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_9000.solverstate
I0511 21:18:38.425760   333 solver.cpp:637] Iteration 9000, Testing net (#0)
I0511 21:18:50.515949   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:51.852352   333 solver.cpp:749] class AP 1: 0.90119
I0511 21:18:51.852696   333 solver.cpp:749] class AP 2: 0.892908
I0511 21:18:51.852883   333 solver.cpp:749] class AP 3: 0.902728
I0511 21:18:51.852890   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.898942
I0511 21:18:51.852916   333 solver.cpp:284] Tests completed in 171.747s
I0511 21:18:52.278501   333 solver.cpp:354] Iteration 9000 (0.582252 iter/s, 171.747s/100 iter), 338.8/376.5ep, loss = 2.06684
I0511 21:18:52.278523   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.521 (* 1 = 1.521 loss)
I0511 21:18:52.278529   333 sgd_solver.cpp:172] Iteration 9000, lr = 1e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:19:10.352946   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:19:53.415158   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:20:35.455065   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:21:18.951011   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:21:26.108482   333 solver.cpp:354] Iteration 9100 (0.650067 iter/s, 153.83s/100 iter), 342.6/376.5ep, loss = 2.03956
I0511 21:21:26.108692   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.62536 (* 1 = 1.62536 loss)
I0511 21:21:26.108763   333 sgd_solver.cpp:172] Iteration 9100, lr = 6.56099e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:21:59.845499   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:22:47.502254   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:23:28.403669   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:24:09.125432   333 solver.cpp:354] Iteration 9200 (0.613432 iter/s, 163.017s/100 iter), 346.4/376.5ep, loss = 2.09217
I0511 21:24:09.125531   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.73983 (* 1 = 1.73983 loss)
I0511 21:24:09.125557   333 sgd_solver.cpp:172] Iteration 9200, lr = 4.096e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:24:09.617555   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:24:53.837898   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:25:36.629724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:26:19.207026   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:26:51.372265   333 solver.cpp:354] Iteration 9300 (0.616343 iter/s, 162.247s/100 iter), 350.1/376.5ep, loss = 1.91648
I0511 21:26:51.372471   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.41199 (* 1 = 1.41199 loss)
I0511 21:26:51.372524   333 sgd_solver.cpp:172] Iteration 9300, lr = 2.401e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:27:05.626217   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:27:45.806048   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:28:30.350258   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:29:12.593127   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:29:30.813346   333 solver.cpp:354] Iteration 9400 (0.627189 iter/s, 159.441s/100 iter), 353.9/376.5ep, loss = 2.15166
I0511 21:29:30.813431   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.05091 (* 1 = 2.05091 loss)
I0511 21:29:30.813457   333 sgd_solver.cpp:172] Iteration 9400, lr = 1.296e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:29:52.967552   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:30:36.460341   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:31:18.386620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:31:55.391315   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:32:06.779017   333 solver.cpp:637] Iteration 9500, Testing net (#0)
I0511 21:32:17.900935   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:32:19.306239   333 solver.cpp:749] class AP 1: 0.902223
I0511 21:32:19.306591   333 solver.cpp:749] class AP 2: 0.892146
I0511 21:32:19.306776   333 solver.cpp:749] class AP 3: 0.903544
I0511 21:32:19.306782   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899304
I0511 21:32:19.306813   333 solver.cpp:284] Tests completed in 168.494s
I0511 21:32:19.757396   333 solver.cpp:354] Iteration 9500 (0.593493 iter/s, 168.494s/100 iter), 357.6/376.5ep, loss = 1.98063
I0511 21:32:19.757484   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.99207 (* 1 = 2.99207 loss)
I0511 21:32:19.757529   333 sgd_solver.cpp:172] Iteration 9500, lr = 6.25001e-09, m = 0.9, wd = 1e-05, gs = 1
I0511 21:32:47.832080   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:33:33.259634   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:34:11.186547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:34:53.445513   333 solver.cpp:354] Iteration 9600 (0.650667 iter/s, 153.689s/100 iter), 361.4/376.5ep, loss = 2.25232
I0511 21:34:53.445617   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.29087 (* 1 = 2.29087 loss)
I0511 21:34:53.445645   333 sgd_solver.cpp:172] Iteration 9600, lr = 2.56001e-09, m = 0.9, wd = 1e-05, gs = 1
I0511 21:34:54.428040   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:35:37.219324   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:36:20.195230   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:37:01.839841   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:37:34.491852   333 solver.cpp:354] Iteration 9700 (0.620938 iter/s, 161.047s/100 iter), 365.2/376.5ep, loss = 2.17996
I0511 21:37:34.492084   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.71302 (* 1 = 1.71302 loss)
I0511 21:37:34.492149   333 sgd_solver.cpp:172] Iteration 9700, lr = 8.09997e-10, m = 0.9, wd = 1e-05, gs = 1
I0511 21:37:44.847134   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:38:25.889276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:39:05.092862   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:39:53.679926   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:40:13.841048   333 solver.cpp:354] Iteration 9800 (0.627556 iter/s, 159.348s/100 iter), 368.9/376.5ep, loss = 2.27157
I0511 21:40:13.841493   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27908 (* 1 = 2.27908 loss)
I0511 21:40:13.841683   333 sgd_solver.cpp:172] Iteration 9800, lr = 1.59999e-10, m = 0.9, wd = 1e-05, gs = 1
I0511 21:40:35.639956   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:41:15.715489   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:41:58.198559   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:42:40.277652   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:42:50.433265   333 solver.cpp:354] Iteration 9900 (0.63862 iter/s, 156.588s/100 iter), 372.7/376.5ep, loss = 2.2722
I0511 21:42:50.433460   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12744 (* 1 = 2.12744 loss)
I0511 21:42:50.433524   333 sgd_solver.cpp:172] Iteration 9900, lr = 9.99996e-12, m = 0.9, wd = 1e-05, gs = 1
I0511 21:43:20.209372   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:44:05.563848   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:44:47.815733   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:25.252507   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:26.258118   333 solver.cpp:354] Iteration 9999 (0.635343 iter/s, 155.821s/99 iter), 376.4/376.5ep, loss = 2.03161
I0511 21:45:26.258263   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.71914 (* 1 = 1.71914 loss)
I0511 21:45:26.258314   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 21:45:26.302434   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0511 21:45:26.508044   333 solver.cpp:503] Iteration 10000, loss = 2.07326
I0511 21:45:26.508451   333 solver.cpp:637] Iteration 10000, Testing net (#0)
I0511 21:45:40.300990   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:41.771572   333 solver.cpp:749] class AP 1: 0.90185
I0511 21:45:41.771916   333 solver.cpp:749] class AP 2: 0.892207
I0511 21:45:41.772109   333 solver.cpp:749] class AP 3: 0.903409
I0511 21:45:41.772116   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899155
I0511 21:45:41.772138   333 caffe.cpp:268] Solver performance on device 0: 0.6239 * 16 = 19.96 img/sec (10000 itr in 1.603e+04 sec)
I0511 21:45:41.772145   333 caffe.cpp:271] Optimization Done in 4h 27m 59s
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument
*** Aborted at 1589233541 (unix time) try "date -d @1589233541" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x14d) received by PID 333 (TID 0x7f124bfff700) from PID 333; stack trace: ***
    @     0x7f148a6b3f20 (unknown)
    @     0x7f148a6b3e97 gsignal
    @     0x7f148a6b5801 abort
    @     0x7f148e7bf84a __gnu_cxx::__verbose_terminate_handler()
    @     0x7f148e7bdf47 __cxxabiv1::__terminate()
    @     0x7f148e7bdf7d std::terminate()
    @     0x7f148e7be15a __cxa_throw
    @     0x7f148c2640ac boost::throw_exception<>()
    @     0x7f148c26419d boost::mutex::lock()
    @     0x7f148c265020 boost::unique_lock<>::lock()
    @     0x7f148c6fc89f caffe::BlockingQueue<>::push()
    @     0x7f148c2e349e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7f148c31d476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7f148c2917ee caffe::InternalThread::entry()
    @     0x7f148c29354b boost::detail::thread_data<>::run()
    @     0x7f148b8cb7ee thread_proxy
    @     0x7f148a45d6db start_thread
    @     0x7f148a79688f clone
    @                0x0 (unknown)
I0511 21:45:42.178694   380 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 21:45:42 2020
I0511 21:45:42.430066   380 caffe.cpp:904] CuDNN version: 7605
I0511 21:45:42.430071   380 caffe.cpp:905] CuBLAS version: 10202
I0511 21:45:42.430074   380 caffe.cpp:906] CUDA version: 10020
I0511 21:45:42.430076   380 caffe.cpp:907] CUDA driver version: 10020
I0511 21:45:42.430079   380 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/solver.prototxt
[3]: --weights=/workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
[4]: --gpu
[5]: 0
I0511 21:45:42.451269   380 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 21:45:42.451297   380 gpu_memory.cpp:107] Total memory: 16900227072, Free: 12089163776, dev_info[0]: total=16900227072 free=12089163776
I0511 21:45:42.451499   380 caffe.cpp:226] Using GPUs 0
I0511 21:45:42.451627   380 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 21:45:42.451696   380 solver.cpp:41] Solver data type: FLOAT
I0511 21:45:42.459705   380 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/test.prototxt"
test_iter: 107
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 20000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 1e-05
snapshot: 1000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 1
type: "Adam"
display_sparsity: 2000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.75
sparsity_step_factor: 0.05
sparsity_step_iter: 2000
sparsity_start_iter: 0
sparsity_start_factor: 0.5
sparsity_threshold_maxratio: 0.2
sparsity_itr_increment_bfr_applying: true
sparsity_threshold_value_max: 0.2
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 21:45:42.459931   380 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/train.prototxt
I0511 21:45:42.461521   380 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 48
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 21:45:42.462453   380 net.cpp:110] Using FLOAT as default forward math type
I0511 21:45:42.462471   380 net.cpp:116] Using FLOAT as default backward math type
I0511 21:45:42.462481   380 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 21:45:42.462487   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.462604   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:42.463037   385 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 21:45:42.463055   380 net.cpp:200] Created Layer data (0)
I0511 21:45:42.463073   380 net.cpp:542] data -> data
I0511 21:45:42.463105   380 net.cpp:542] data -> label
I0511 21:45:42.463129   380 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 48
I0511 21:45:42.463271   380 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 21:45:42.463645   386 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.463793   387 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.464022   388 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.464269   389 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 21:45:42.466291   380 annotated_data_layer.cpp:105] output data size: 48,3,320,768
I0511 21:45:42.466593   380 annotated_data_layer.cpp:150] [0] Output data size: 48, 3, 320, 768
I0511 21:45:42.466675   380 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 21:45:42.467247   390 data_layer.cpp:105] [0] Parser threads: 4
I0511 21:45:42.467260   390 data_layer.cpp:107] [0] Transformer threads: 4
I0511 21:45:42.467676   380 net.cpp:260] Setting up data
I0511 21:45:42.468137   380 net.cpp:267] TRAIN Top shape for layer 0 'data' 48 3 320 768 (35389440)
I0511 21:45:42.468184   380 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 21:45:42.468197   380 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 21:45:42.468204   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468215   380 net.cpp:200] Created Layer data_data_0_split (1)
I0511 21:45:42.468223   380 net.cpp:572] data_data_0_split <- data
I0511 21:45:42.468240   380 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 21:45:42.468250   380 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 21:45:42.468257   380 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 21:45:42.468266   380 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 21:45:42.468274   380 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 21:45:42.468282   380 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 21:45:42.468291   380 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 21:45:42.468395   380 net.cpp:260] Setting up data_data_0_split
I0511 21:45:42.468401   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468410   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468417   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468423   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468431   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468438   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468446   380 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 48 3 320 768 (35389440)
I0511 21:45:42.468456   380 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 21:45:42.468461   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468477   380 net.cpp:200] Created Layer data/bias (2)
I0511 21:45:42.468482   380 net.cpp:572] data/bias <- data_data_0_split_0
I0511 21:45:42.468487   380 net.cpp:542] data/bias -> data/bias
I0511 21:45:42.468626   380 net.cpp:260] Setting up data/bias
I0511 21:45:42.468631   380 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 48 3 320 768 (35389440)
I0511 21:45:42.468650   380 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 21:45:42.468655   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:42.468713   380 net.cpp:200] Created Layer conv1a (3)
I0511 21:45:42.468737   380 net.cpp:572] conv1a <- data/bias
I0511 21:45:42.468744   380 net.cpp:542] conv1a -> conv1a
I0511 21:45:45.975857   380 net.cpp:260] Setting up conv1a
I0511 21:45:45.975996   380 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 48 32 160 384 (94371840)
I0511 21:45:45.976083   380 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 21:45:45.976140   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.976202   380 net.cpp:200] Created Layer conv1a/bn (4)
I0511 21:45:45.976253   380 net.cpp:572] conv1a/bn <- conv1a
I0511 21:45:45.976305   380 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 21:45:45.977051   380 net.cpp:260] Setting up conv1a/bn
I0511 21:45:45.977120   380 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 48 32 160 384 (94371840)
I0511 21:45:45.977187   380 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 21:45:45.977236   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.977293   380 net.cpp:200] Created Layer conv1a/relu (5)
I0511 21:45:45.977344   380 net.cpp:572] conv1a/relu <- conv1a
I0511 21:45:45.977393   380 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 21:45:45.977460   380 net.cpp:260] Setting up conv1a/relu
I0511 21:45:45.977507   380 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 48 32 160 384 (94371840)
I0511 21:45:45.977560   380 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 21:45:45.977607   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.977668   380 net.cpp:200] Created Layer conv1b (6)
I0511 21:45:45.977715   380 net.cpp:572] conv1b <- conv1a
I0511 21:45:45.977762   380 net.cpp:542] conv1b -> conv1b
I0511 21:45:45.978826   380 net.cpp:260] Setting up conv1b
I0511 21:45:45.978894   380 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 48 32 160 384 (94371840)
I0511 21:45:45.978955   380 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 21:45:45.979003   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.979058   380 net.cpp:200] Created Layer conv1b/bn (7)
I0511 21:45:45.979104   380 net.cpp:572] conv1b/bn <- conv1b
I0511 21:45:45.979152   380 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 21:45:45.979830   380 net.cpp:260] Setting up conv1b/bn
I0511 21:45:45.979894   380 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 48 32 160 384 (94371840)
I0511 21:45:45.979957   380 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 21:45:45.980005   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980057   380 net.cpp:200] Created Layer conv1b/relu (8)
I0511 21:45:45.980104   380 net.cpp:572] conv1b/relu <- conv1b
I0511 21:45:45.980154   380 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 21:45:45.980206   380 net.cpp:260] Setting up conv1b/relu
I0511 21:45:45.980252   380 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 48 32 160 384 (94371840)
I0511 21:45:45.980304   380 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 21:45:45.980351   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980408   380 net.cpp:200] Created Layer pool1 (9)
I0511 21:45:45.980456   380 net.cpp:572] pool1 <- conv1b
I0511 21:45:45.980505   380 net.cpp:542] pool1 -> pool1
I0511 21:45:45.980669   380 net.cpp:260] Setting up pool1
I0511 21:45:45.980722   380 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 48 32 80 192 (23592960)
I0511 21:45:45.980798   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 21:45:45.980866   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.980964   380 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 21:45:45.981036   380 net.cpp:572] res2a_branch2a <- pool1
I0511 21:45:45.981120   380 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 21:45:45.983037   380 net.cpp:260] Setting up res2a_branch2a
I0511 21:45:45.983139   380 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 48 64 80 192 (47185920)
I0511 21:45:45.983222   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.983291   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.983369   380 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 21:45:45.983438   380 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 21:45:45.983511   380 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 21:45:45.984113   380 net.cpp:260] Setting up res2a_branch2a/bn
I0511 21:45:45.984203   380 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 48 64 80 192 (47185920)
I0511 21:45:45.984287   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.984355   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.984432   380 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 21:45:45.984504   380 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 21:45:45.984578   380 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 21:45:45.984658   380 net.cpp:260] Setting up res2a_branch2a/relu
I0511 21:45:45.984731   380 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 48 64 80 192 (47185920)
I0511 21:45:45.984807   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 21:45:45.984879   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.984964   380 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 21:45:45.985038   380 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 21:45:45.985110   380 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 21:45:45.985870   380 net.cpp:260] Setting up res2a_branch2b
I0511 21:45:45.985962   380 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 48 64 80 192 (47185920)
I0511 21:45:45.986039   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:45.986106   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.986179   380 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 21:45:45.986251   380 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 21:45:45.986320   380 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 21:45:45.986932   380 net.cpp:260] Setting up res2a_branch2b/bn
I0511 21:45:45.987011   380 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 48 64 80 192 (47185920)
I0511 21:45:45.987125   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 21:45:45.987234   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.987335   380 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 21:45:45.987393   380 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 21:45:45.987434   380 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 21:45:45.987500   380 net.cpp:260] Setting up res2a_branch2b/relu
I0511 21:45:45.987567   380 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 48 64 80 192 (47185920)
I0511 21:45:45.987640   380 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 21:45:45.987706   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.987782   380 net.cpp:200] Created Layer pool2 (16)
I0511 21:45:45.987823   380 net.cpp:572] pool2 <- res2a_branch2b
I0511 21:45:45.987891   380 net.cpp:542] pool2 -> pool2
I0511 21:45:45.988015   380 net.cpp:260] Setting up pool2
I0511 21:45:45.988024   380 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 48 64 40 96 (11796480)
I0511 21:45:45.988039   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 21:45:45.988078   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.988111   380 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 21:45:45.988134   380 net.cpp:572] res3a_branch2a <- pool2
I0511 21:45:45.988162   380 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 21:45:45.989650   380 net.cpp:260] Setting up res3a_branch2a
I0511 21:45:45.989665   380 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 48 128 40 96 (23592960)
I0511 21:45:45.989681   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.989709   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.989737   380 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 21:45:45.989759   380 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 21:45:45.989784   380 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 21:45:45.990322   380 net.cpp:260] Setting up res3a_branch2a/bn
I0511 21:45:45.990348   380 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 48 128 40 96 (23592960)
I0511 21:45:45.990391   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.990414   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.990445   380 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 21:45:45.990468   380 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 21:45:45.990492   380 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 21:45:45.990523   380 net.cpp:260] Setting up res3a_branch2a/relu
I0511 21:45:45.990545   380 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 48 128 40 96 (23592960)
I0511 21:45:45.990574   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 21:45:45.990598   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.990633   380 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 21:45:45.990658   380 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 21:45:45.990684   380 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 21:45:45.991925   380 net.cpp:260] Setting up res3a_branch2b
I0511 21:45:45.991955   380 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 48 128 40 96 (23592960)
I0511 21:45:45.991991   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:45.992015   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992045   380 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 21:45:45.992069   380 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 21:45:45.992095   380 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 21:45:45.992636   380 net.cpp:260] Setting up res3a_branch2b/bn
I0511 21:45:45.992664   380 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 48 128 40 96 (23592960)
I0511 21:45:45.992700   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 21:45:45.992725   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992753   380 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 21:45:45.992779   380 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 21:45:45.992808   380 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 21:45:45.992835   380 net.cpp:260] Setting up res3a_branch2b/relu
I0511 21:45:45.992857   380 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 48 128 40 96 (23592960)
I0511 21:45:45.992885   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:45.992910   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.992942   380 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 21:45:45.992978   380 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 21:45:45.993003   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:45.993028   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:45.993108   380 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 21:45:45.993130   380 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 48 128 40 96 (23592960)
I0511 21:45:45.993157   380 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 48 128 40 96 (23592960)
I0511 21:45:45.993188   380 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 21:45:45.993211   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.993239   380 net.cpp:200] Created Layer pool3 (24)
I0511 21:45:45.993264   380 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:45.993295   380 net.cpp:542] pool3 -> pool3
I0511 21:45:45.993388   380 net.cpp:260] Setting up pool3
I0511 21:45:45.993412   380 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 48 128 20 48 (5898240)
I0511 21:45:45.993443   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 21:45:45.993466   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.993505   380 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 21:45:45.993530   380 net.cpp:572] res4a_branch2a <- pool3
I0511 21:45:45.993552   380 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 21:45:45.999234   380 net.cpp:260] Setting up res4a_branch2a
I0511 21:45:45.999248   380 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 48 256 20 48 (11796480)
I0511 21:45:45.999266   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:45.999291   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.999317   380 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 21:45:45.999338   380 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 21:45:45.999361   380 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 21:45:45.999864   380 net.cpp:260] Setting up res4a_branch2a/bn
I0511 21:45:45.999891   380 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 48 256 20 48 (11796480)
I0511 21:45:45.999927   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 21:45:45.999950   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:45.999975   380 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 21:45:45.999996   380 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 21:45:46.000020   380 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 21:45:46.000046   380 net.cpp:260] Setting up res4a_branch2a/relu
I0511 21:45:46.000066   380 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 48 256 20 48 (11796480)
I0511 21:45:46.000095   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 21:45:46.000116   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.000149   380 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 21:45:46.000171   380 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 21:45:46.000192   380 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 21:45:46.003631   380 net.cpp:260] Setting up res4a_branch2b
I0511 21:45:46.003715   380 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 48 256 20 48 (11796480)
I0511 21:45:46.003814   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.003958   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.004076   380 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 21:45:46.004227   380 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 21:45:46.004328   380 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 21:45:46.004923   380 net.cpp:260] Setting up res4a_branch2b/bn
I0511 21:45:46.005030   380 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 48 256 20 48 (11796480)
I0511 21:45:46.005167   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.005256   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.005412   380 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 21:45:46.005511   380 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 21:45:46.005609   380 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 21:45:46.005707   380 net.cpp:260] Setting up res4a_branch2b/relu
I0511 21:45:46.005795   380 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 48 256 20 48 (11796480)
I0511 21:45:46.005895   380 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 21:45:46.005991   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.006093   380 net.cpp:200] Created Layer pool4 (31)
I0511 21:45:46.006182   380 net.cpp:572] pool4 <- res4a_branch2b
I0511 21:45:46.006276   380 net.cpp:542] pool4 -> pool4
I0511 21:45:46.006440   380 net.cpp:260] Setting up pool4
I0511 21:45:46.006531   380 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 48 256 10 24 (2949120)
I0511 21:45:46.006630   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 21:45:46.006723   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.006829   380 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 21:45:46.006923   380 net.cpp:572] res5a_branch2a <- pool4
I0511 21:45:46.007017   380 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 21:45:46.033661   380 net.cpp:260] Setting up res5a_branch2a
I0511 21:45:46.033818   380 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 48 512 10 24 (5898240)
I0511 21:45:46.033932   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.034029   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.034132   380 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 21:45:46.034229   380 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 21:45:46.034322   380 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 21:45:46.034942   380 net.cpp:260] Setting up res5a_branch2a/bn
I0511 21:45:46.035054   380 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 48 512 10 24 (5898240)
I0511 21:45:46.035174   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.035272   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.035370   380 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 21:45:46.035463   380 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 21:45:46.035558   380 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 21:45:46.035663   380 net.cpp:260] Setting up res5a_branch2a/relu
I0511 21:45:46.035751   380 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 48 512 10 24 (5898240)
I0511 21:45:46.035849   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 21:45:46.035946   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.036059   380 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 21:45:46.036154   380 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 21:45:46.036262   380 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 21:45:46.050024   380 net.cpp:260] Setting up res5a_branch2b
I0511 21:45:46.050174   380 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 48 512 10 24 (5898240)
I0511 21:45:46.050323   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.050472   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.050627   380 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 21:45:46.050777   380 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 21:45:46.050921   380 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 21:45:46.051609   380 net.cpp:260] Setting up res5a_branch2b/bn
I0511 21:45:46.051766   380 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 48 512 10 24 (5898240)
I0511 21:45:46.051926   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.052069   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.052215   380 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 21:45:46.052353   380 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 21:45:46.052496   380 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 21:45:46.052642   380 net.cpp:260] Setting up res5a_branch2b/relu
I0511 21:45:46.052781   380 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 48 512 10 24 (5898240)
I0511 21:45:46.052929   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.053071   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.053216   380 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 21:45:46.053313   380 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 21:45:46.053467   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.053563   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.053714   380 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 21:45:46.053807   380 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 48 512 10 24 (5898240)
I0511 21:45:46.053907   380 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 48 512 10 24 (5898240)
I0511 21:45:46.054003   380 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 21:45:46.054095   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.054193   380 net.cpp:200] Created Layer pool6 (39)
I0511 21:45:46.054284   380 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.054378   380 net.cpp:542] pool6 -> pool6
I0511 21:45:46.054543   380 net.cpp:260] Setting up pool6
I0511 21:45:46.054633   380 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 48 512 5 12 (1474560)
I0511 21:45:46.054731   380 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 21:45:46.054769   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.054919   380 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 21:45:46.054956   380 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 21:45:46.055104   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 21:45:46.055197   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 21:45:46.055338   380 net.cpp:260] Setting up pool6_pool6_0_split
I0511 21:45:46.055377   380 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 48 512 5 12 (1474560)
I0511 21:45:46.055523   380 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 48 512 5 12 (1474560)
I0511 21:45:46.055624   380 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 21:45:46.055719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.055830   380 net.cpp:200] Created Layer pool7 (41)
I0511 21:45:46.055920   380 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 21:45:46.056013   380 net.cpp:542] pool7 -> pool7
I0511 21:45:46.056177   380 net.cpp:260] Setting up pool7
I0511 21:45:46.056267   380 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 48 512 3 6 (442368)
I0511 21:45:46.056363   380 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 21:45:46.056455   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.056550   380 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 21:45:46.056641   380 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 21:45:46.056733   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 21:45:46.056830   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 21:45:46.056968   380 net.cpp:260] Setting up pool7_pool7_0_split
I0511 21:45:46.057008   380 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 48 512 3 6 (442368)
I0511 21:45:46.057157   380 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 48 512 3 6 (442368)
I0511 21:45:46.057253   380 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 21:45:46.057349   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.057448   380 net.cpp:200] Created Layer pool8 (43)
I0511 21:45:46.057538   380 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 21:45:46.057631   380 net.cpp:542] pool8 -> pool8
I0511 21:45:46.057792   380 net.cpp:260] Setting up pool8
I0511 21:45:46.057829   380 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 48 512 2 3 (147456)
I0511 21:45:46.057977   380 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 21:45:46.058069   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.058164   380 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 21:45:46.058254   380 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 21:45:46.058348   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 21:45:46.058449   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 21:45:46.058588   380 net.cpp:260] Setting up pool8_pool8_0_split
I0511 21:45:46.058676   380 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 48 512 2 3 (147456)
I0511 21:45:46.058717   380 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 48 512 2 3 (147456)
I0511 21:45:46.058866   380 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 21:45:46.058957   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.059054   380 net.cpp:200] Created Layer pool9 (45)
I0511 21:45:46.059146   380 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 21:45:46.059239   380 net.cpp:542] pool9 -> pool9
I0511 21:45:46.059397   380 net.cpp:260] Setting up pool9
I0511 21:45:46.059434   380 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 48 512 1 2 (49152)
I0511 21:45:46.059582   380 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 21:45:46.059674   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.059778   380 net.cpp:200] Created Layer ctx_output1 (46)
I0511 21:45:46.059870   380 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.059963   380 net.cpp:542] ctx_output1 -> ctx_output1
I0511 21:45:46.061087   380 net.cpp:260] Setting up ctx_output1
I0511 21:45:46.061134   380 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 48 256 40 96 (47185920)
I0511 21:45:46.061308   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 21:45:46.061408   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.061507   380 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 21:45:46.061607   380 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 21:45:46.061699   380 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 21:45:46.061795   380 net.cpp:260] Setting up ctx_output1/relu
I0511 21:45:46.061831   380 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 48 256 40 96 (47185920)
I0511 21:45:46.061981   380 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 21:45:46.062077   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.062173   380 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 21:45:46.062265   380 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 21:45:46.062358   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.062453   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.062547   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.062705   380 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 21:45:46.062793   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.062888   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.062983   380 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 48 256 40 96 (47185920)
I0511 21:45:46.063079   380 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 21:45:46.063117   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.063273   380 net.cpp:200] Created Layer ctx_output2 (49)
I0511 21:45:46.063366   380 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.063460   380 net.cpp:542] ctx_output2 -> ctx_output2
I0511 21:45:46.066623   380 net.cpp:260] Setting up ctx_output2
I0511 21:45:46.066678   380 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 48 256 10 24 (2949120)
I0511 21:45:46.066854   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 21:45:46.066946   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.067040   380 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 21:45:46.067133   380 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 21:45:46.067226   380 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 21:45:46.067322   380 net.cpp:260] Setting up ctx_output2/relu
I0511 21:45:46.067359   380 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 48 256 10 24 (2949120)
I0511 21:45:46.067519   380 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 21:45:46.067556   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.067706   380 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 21:45:46.067797   380 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 21:45:46.067891   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.067986   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.068080   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.068248   380 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 21:45:46.068338   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068434   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068538   380 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 48 256 10 24 (2949120)
I0511 21:45:46.068645   380 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 21:45:46.068735   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.068838   380 net.cpp:200] Created Layer ctx_output3 (52)
I0511 21:45:46.068930   380 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 21:45:46.069022   380 net.cpp:542] ctx_output3 -> ctx_output3
I0511 21:45:46.073102   380 net.cpp:260] Setting up ctx_output3
I0511 21:45:46.073158   380 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 48 256 5 12 (737280)
I0511 21:45:46.073339   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 21:45:46.073432   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.073529   380 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 21:45:46.073619   380 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 21:45:46.073719   380 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 21:45:46.073817   380 net.cpp:260] Setting up ctx_output3/relu
I0511 21:45:46.073853   380 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 48 256 5 12 (737280)
I0511 21:45:46.074003   380 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 21:45:46.074095   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.074190   380 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 21:45:46.074281   380 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 21:45:46.074374   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.074470   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.074564   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.074734   380 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 21:45:46.074823   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.074867   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.075016   380 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 48 256 5 12 (737280)
I0511 21:45:46.075111   380 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 21:45:46.075202   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.075306   380 net.cpp:200] Created Layer ctx_output4 (55)
I0511 21:45:46.075398   380 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 21:45:46.075491   380 net.cpp:542] ctx_output4 -> ctx_output4
I0511 21:45:46.078663   380 net.cpp:260] Setting up ctx_output4
I0511 21:45:46.078716   380 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 48 256 3 6 (221184)
I0511 21:45:46.078891   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 21:45:46.078982   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.079085   380 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 21:45:46.079190   380 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 21:45:46.079282   380 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 21:45:46.079377   380 net.cpp:260] Setting up ctx_output4/relu
I0511 21:45:46.079414   380 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 48 256 3 6 (221184)
I0511 21:45:46.079563   380 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 21:45:46.079660   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.079771   380 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 21:45:46.079861   380 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 21:45:46.079955   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.080050   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.080145   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.080315   380 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 21:45:46.080353   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080502   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080596   380 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 48 256 3 6 (221184)
I0511 21:45:46.080693   380 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 21:45:46.080785   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.080886   380 net.cpp:200] Created Layer ctx_output5 (58)
I0511 21:45:46.080977   380 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 21:45:46.081073   380 net.cpp:542] ctx_output5 -> ctx_output5
I0511 21:45:46.084260   380 net.cpp:260] Setting up ctx_output5
I0511 21:45:46.084313   380 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 48 256 2 3 (73728)
I0511 21:45:46.084503   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 21:45:46.084615   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.084722   380 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 21:45:46.084827   380 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 21:45:46.084933   380 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 21:45:46.085047   380 net.cpp:260] Setting up ctx_output5/relu
I0511 21:45:46.085150   380 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 48 256 2 3 (73728)
I0511 21:45:46.085258   380 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 21:45:46.085372   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.085481   380 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 21:45:46.085584   380 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 21:45:46.085697   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.085804   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.085912   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.086104   380 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 21:45:46.086196   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086262   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086362   380 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 48 256 2 3 (73728)
I0511 21:45:46.086472   380 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 21:45:46.086575   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.086688   380 net.cpp:200] Created Layer ctx_output6 (61)
I0511 21:45:46.086789   380 net.cpp:572] ctx_output6 <- pool9
I0511 21:45:46.086896   380 net.cpp:542] ctx_output6 -> ctx_output6
I0511 21:45:46.090193   380 net.cpp:260] Setting up ctx_output6
I0511 21:45:46.090324   380 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 48 256 1 2 (24576)
I0511 21:45:46.090458   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 21:45:46.090564   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.090674   380 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 21:45:46.090790   380 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 21:45:46.090888   380 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 21:45:46.090988   380 net.cpp:260] Setting up ctx_output6/relu
I0511 21:45:46.091085   380 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 48 256 1 2 (24576)
I0511 21:45:46.091187   380 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 21:45:46.091233   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.091320   380 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 21:45:46.091395   380 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 21:45:46.091470   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.091586   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.091665   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.091835   380 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 21:45:46.091935   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092027   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092083   380 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 48 256 1 2 (24576)
I0511 21:45:46.092175   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.092252   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.092373   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 21:45:46.092412   380 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.092486   380 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 21:45:46.093057   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 21:45:46.093102   380 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 48 16 40 96 (2949120)
I0511 21:45:46.093197   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.093273   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.093400   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 21:45:46.093439   380 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 21:45:46.093513   380 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.093767   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.093806   380 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 48 40 96 16 (2949120)
I0511 21:45:46.093888   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.093960   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.094038   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 21:45:46.094113   380 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.094185   380 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.105696   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.105799   380 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 48 61440 (2949120)
I0511 21:45:46.105975   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.106060   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.106199   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 21:45:46.106241   380 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.106333   380 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 21:45:46.107059   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 21:45:46.107113   380 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 48 16 40 96 (2949120)
I0511 21:45:46.107220   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.107307   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.107393   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 21:45:46.107478   380 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 21:45:46.107556   380 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.107822   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.107859   380 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 48 40 96 16 (2949120)
I0511 21:45:46.107944   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.108016   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.108135   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 21:45:46.108176   380 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.108255   380 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.119660   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.119756   380 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 48 61440 (2949120)
I0511 21:45:46.119905   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.119998   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.120129   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 21:45:46.120170   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.120251   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 21:45:46.120337   380 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 21:45:46.120528   380 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 21:45:46.120566   380 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 21:45:46.120648   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.120726   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.120862   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 21:45:46.120901   380 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.120995   380 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 21:45:46.121706   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 21:45:46.121758   380 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 48 24 10 24 (276480)
I0511 21:45:46.121932   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.122030   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.122141   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 21:45:46.122201   380 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 21:45:46.122285   380 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.122604   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.122642   380 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 48 10 24 24 (276480)
I0511 21:45:46.122735   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.122822   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.122898   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 21:45:46.122974   380 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.123047   380 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.125253   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.125352   380 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 48 5760 (276480)
I0511 21:45:46.125524   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.125562   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.125718   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 21:45:46.125816   380 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.125914   380 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 21:45:46.126530   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 21:45:46.126663   380 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 48 24 10 24 (276480)
I0511 21:45:46.126796   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.126924   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.127043   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 21:45:46.127157   380 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 21:45:46.127271   380 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.127538   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.127648   380 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 48 10 24 24 (276480)
I0511 21:45:46.127768   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.127882   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.127997   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 21:45:46.128110   380 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.128227   380 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.130420   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.130568   380 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 48 5760 (276480)
I0511 21:45:46.130698   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.130818   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.130939   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 21:45:46.131054   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.131170   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 21:45:46.131295   380 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 21:45:46.131480   380 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 21:45:46.131603   380 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 21:45:46.131722   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.131834   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.131961   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 21:45:46.132081   380 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.132196   380 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 21:45:46.132845   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 21:45:46.132970   380 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 48 24 5 12 (69120)
I0511 21:45:46.133101   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.133219   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.133354   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 21:45:46.133474   380 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 21:45:46.133594   380 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.133863   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.133973   380 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 48 5 12 24 (69120)
I0511 21:45:46.134090   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.134205   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.134323   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 21:45:46.134441   380 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.134557   380 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.134778   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.134896   380 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 48 1440 (69120)
I0511 21:45:46.135020   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.135133   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.135258   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 21:45:46.135378   380 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.135502   380 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 21:45:46.136158   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 21:45:46.136289   380 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 48 24 5 12 (69120)
I0511 21:45:46.136430   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.136559   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.136689   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 21:45:46.136819   380 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 21:45:46.136945   380 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.137234   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.137357   380 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 48 5 12 24 (69120)
I0511 21:45:46.137495   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.137621   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.137766   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 21:45:46.137907   380 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.138031   380 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.138275   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.138401   380 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 48 1440 (69120)
I0511 21:45:46.138540   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.138731   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.138880   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 21:45:46.139012   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.139144   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 21:45:46.139277   380 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 21:45:46.139461   380 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 21:45:46.139581   380 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 21:45:46.139719   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.139848   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.139997   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 21:45:46.140130   380 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.140261   380 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 21:45:46.140954   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 21:45:46.141096   380 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 48 24 3 6 (20736)
I0511 21:45:46.141242   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.141424   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.141563   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 21:45:46.141693   380 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 21:45:46.141816   380 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142097   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142216   380 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 48 3 6 24 (20736)
I0511 21:45:46.142343   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.142464   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.142596   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 21:45:46.142724   380 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.142848   380 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.143070   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.143190   380 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 48 432 (20736)
I0511 21:45:46.143317   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.143437   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.143571   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 21:45:46.143699   380 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.143822   380 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 21:45:46.144482   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 21:45:46.144627   380 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 48 24 3 6 (20736)
I0511 21:45:46.144768   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.144896   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.145025   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 21:45:46.145150   380 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 21:45:46.145275   380 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.145601   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.145720   380 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 48 3 6 24 (20736)
I0511 21:45:46.145853   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.145977   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.146102   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 21:45:46.146226   380 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.146358   380 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.146584   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.146703   380 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 48 432 (20736)
I0511 21:45:46.146828   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.146950   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.147076   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 21:45:46.147200   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.147326   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 21:45:46.147450   380 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 21:45:46.147594   380 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 21:45:46.147718   380 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 21:45:46.147843   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.147964   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.148097   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 21:45:46.148226   380 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.148351   380 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 21:45:46.148959   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 21:45:46.149087   380 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 48 16 2 3 (4608)
I0511 21:45:46.149227   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.149360   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.149490   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 21:45:46.149613   380 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 21:45:46.149737   380 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150003   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150120   380 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 48 2 3 16 (4608)
I0511 21:45:46.150246   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.150380   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.150561   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 21:45:46.150683   380 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.150806   380 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.151016   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.151134   380 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 48 96 (4608)
I0511 21:45:46.151259   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.151379   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.151515   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 21:45:46.151640   380 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.151764   380 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 21:45:46.152308   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 21:45:46.152436   380 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 48 16 2 3 (4608)
I0511 21:45:46.152575   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.152704   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.152832   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 21:45:46.152954   380 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 21:45:46.153077   380 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.153362   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.153486   380 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 48 2 3 16 (4608)
I0511 21:45:46.153626   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.153754   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.153880   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 21:45:46.154002   380 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.154127   380 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.154338   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.154458   380 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 48 96 (4608)
I0511 21:45:46.154584   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.154707   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.154834   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 21:45:46.154956   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.155079   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 21:45:46.155205   380 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 21:45:46.155349   380 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 21:45:46.155473   380 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 21:45:46.155601   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.155721   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.155853   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 21:45:46.155978   380 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.156123   380 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 21:45:46.156688   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 21:45:46.156814   380 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 48 16 1 2 (1536)
I0511 21:45:46.156895   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.157047   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.157157   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 21:45:46.157285   380 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 21:45:46.157423   380 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.157701   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.157819   380 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 48 1 2 16 (1536)
I0511 21:45:46.157944   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.158064   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.158187   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 21:45:46.158306   380 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.158428   380 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.158632   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.158746   380 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 48 32 (1536)
I0511 21:45:46.158871   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.158989   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.159121   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 21:45:46.159245   380 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.159366   380 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 21:45:46.159935   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 21:45:46.160064   380 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 48 16 1 2 (1536)
I0511 21:45:46.160202   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.160320   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.160451   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 21:45:46.160579   380 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 21:45:46.160701   380 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.160975   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.161094   380 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 48 1 2 16 (1536)
I0511 21:45:46.161221   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.161346   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.161469   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 21:45:46.161590   380 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.161712   380 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.161918   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.162036   380 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 48 32 (1536)
I0511 21:45:46.162160   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.162290   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.162439   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 21:45:46.162559   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.162680   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 21:45:46.162801   380 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 21:45:46.162945   380 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 21:45:46.163069   380 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 21:45:46.163192   380 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 21:45:46.163311   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.163439   380 net.cpp:200] Created Layer mbox_loc (106)
I0511 21:45:46.163566   380 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.163697   380 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.163825   380 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.163942   380 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.164057   380 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.164175   380 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.164288   380 net.cpp:542] mbox_loc -> mbox_loc
I0511 21:45:46.164431   380 net.cpp:260] Setting up mbox_loc
I0511 21:45:46.164548   380 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 48 69200 (3321600)
I0511 21:45:46.164669   380 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 21:45:46.164783   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.164899   380 net.cpp:200] Created Layer mbox_conf (107)
I0511 21:45:46.165016   380 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.165133   380 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.165249   380 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.165369   380 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.165485   380 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.165601   380 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.165715   380 net.cpp:542] mbox_conf -> mbox_conf
I0511 21:45:46.165858   380 net.cpp:260] Setting up mbox_conf
I0511 21:45:46.165977   380 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 48 69200 (3321600)
I0511 21:45:46.166096   380 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 21:45:46.166211   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.166330   380 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 21:45:46.166442   380 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 21:45:46.166559   380 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 21:45:46.166682   380 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 21:45:46.166795   380 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 21:45:46.166910   380 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 21:45:46.167024   380 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 21:45:46.167140   380 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 21:45:46.167284   380 net.cpp:260] Setting up mbox_priorbox
I0511 21:45:46.167402   380 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 21:45:46.167520   380 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 21:45:46.167635   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.167769   380 net.cpp:200] Created Layer mbox_loss (109)
I0511 21:45:46.167887   380 net.cpp:572] mbox_loss <- mbox_loc
I0511 21:45:46.168011   380 net.cpp:572] mbox_loss <- mbox_conf
I0511 21:45:46.168125   380 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 21:45:46.168241   380 net.cpp:572] mbox_loss <- label
I0511 21:45:46.168356   380 net.cpp:542] mbox_loss -> mbox_loss
I0511 21:45:46.168560   380 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 21:45:46.168670   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.168933   380 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 21:45:46.169044   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.169318   380 net.cpp:260] Setting up mbox_loss
I0511 21:45:46.169435   380 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 21:45:46.169550   380 net.cpp:271]     with loss weight 1
I0511 21:45:46.169692   380 net.cpp:336] mbox_loss needs backward computation.
I0511 21:45:46.169809   380 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 21:45:46.169929   380 net.cpp:336] mbox_conf needs backward computation.
I0511 21:45:46.170044   380 net.cpp:336] mbox_loc needs backward computation.
I0511 21:45:46.170168   380 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.170290   380 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.170406   380 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.170519   380 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 21:45:46.170637   380 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.170750   380 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.170866   380 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 21:45:46.170981   380 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.171095   380 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.171211   380 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.171324   380 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 21:45:46.171439   380 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.171553   380 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.171669   380 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 21:45:46.171783   380 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.171900   380 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.172013   380 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.172127   380 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 21:45:46.172241   380 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.172358   380 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.172472   380 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 21:45:46.172588   380 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.172704   380 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.172818   380 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.172940   380 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 21:45:46.173054   380 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.173168   380 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.173306   380 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 21:45:46.173427   380 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.173544   380 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.173657   380 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.173771   380 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 21:45:46.173887   380 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.174002   380 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.174115   380 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 21:45:46.174237   380 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.174353   380 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 21:45:46.174468   380 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 21:45:46.174583   380 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 21:45:46.174697   380 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 21:45:46.174813   380 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 21:45:46.174927   380 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 21:45:46.175045   380 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 21:45:46.175160   380 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 21:45:46.175274   380 net.cpp:336] ctx_output6 needs backward computation.
I0511 21:45:46.175390   380 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 21:45:46.175505   380 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 21:45:46.175618   380 net.cpp:336] ctx_output5 needs backward computation.
I0511 21:45:46.175736   380 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 21:45:46.175849   380 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 21:45:46.175963   380 net.cpp:336] ctx_output4 needs backward computation.
I0511 21:45:46.176079   380 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 21:45:46.176194   380 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 21:45:46.176308   380 net.cpp:336] ctx_output3 needs backward computation.
I0511 21:45:46.176422   380 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 21:45:46.176537   380 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 21:45:46.176653   380 net.cpp:336] ctx_output2 needs backward computation.
I0511 21:45:46.176767   380 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 21:45:46.176882   380 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 21:45:46.176997   380 net.cpp:336] ctx_output1 needs backward computation.
I0511 21:45:46.177114   380 net.cpp:336] pool9 needs backward computation.
I0511 21:45:46.177227   380 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 21:45:46.177368   380 net.cpp:336] pool8 needs backward computation.
I0511 21:45:46.177484   380 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 21:45:46.177623   380 net.cpp:336] pool7 needs backward computation.
I0511 21:45:46.177727   380 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 21:45:46.177829   380 net.cpp:336] pool6 needs backward computation.
I0511 21:45:46.177930   380 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 21:45:46.178032   380 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 21:45:46.178128   380 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 21:45:46.178228   380 net.cpp:336] res5a_branch2b needs backward computation.
I0511 21:45:46.178328   380 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 21:45:46.178433   380 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 21:45:46.178547   380 net.cpp:336] res5a_branch2a needs backward computation.
I0511 21:45:46.178644   380 net.cpp:336] pool4 needs backward computation.
I0511 21:45:46.178746   380 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 21:45:46.178843   380 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 21:45:46.178938   380 net.cpp:336] res4a_branch2b needs backward computation.
I0511 21:45:46.179033   380 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 21:45:46.179129   380 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 21:45:46.179224   380 net.cpp:336] res4a_branch2a needs backward computation.
I0511 21:45:46.179322   380 net.cpp:336] pool3 needs backward computation.
I0511 21:45:46.179422   380 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 21:45:46.179523   380 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 21:45:46.179625   380 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 21:45:46.179726   380 net.cpp:336] res3a_branch2b needs backward computation.
I0511 21:45:46.179828   380 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 21:45:46.179924   380 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 21:45:46.180019   380 net.cpp:336] res3a_branch2a needs backward computation.
I0511 21:45:46.180114   380 net.cpp:336] pool2 needs backward computation.
I0511 21:45:46.180210   380 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 21:45:46.180305   380 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 21:45:46.180402   380 net.cpp:336] res2a_branch2b needs backward computation.
I0511 21:45:46.180498   380 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 21:45:46.180598   380 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 21:45:46.180703   380 net.cpp:336] res2a_branch2a needs backward computation.
I0511 21:45:46.180805   380 net.cpp:336] pool1 needs backward computation.
I0511 21:45:46.180902   380 net.cpp:336] conv1b/relu needs backward computation.
I0511 21:45:46.180997   380 net.cpp:336] conv1b/bn needs backward computation.
I0511 21:45:46.181095   380 net.cpp:336] conv1b needs backward computation.
I0511 21:45:46.181190   380 net.cpp:336] conv1a/relu needs backward computation.
I0511 21:45:46.181285   380 net.cpp:336] conv1a/bn needs backward computation.
I0511 21:45:46.181437   380 net.cpp:336] conv1a needs backward computation.
I0511 21:45:46.181555   380 net.cpp:338] data/bias does not need backward computation.
I0511 21:45:46.181674   380 net.cpp:338] data_data_0_split does not need backward computation.
I0511 21:45:46.181789   380 net.cpp:338] data does not need backward computation.
I0511 21:45:46.181905   380 net.cpp:380] This network produces output mbox_loss
I0511 21:45:46.182181   380 net.cpp:403] Top memory (TRAIN) required for data: 7231391112 diff: 7231391112
I0511 21:45:46.182294   380 net.cpp:406] Bottom memory (TRAIN) required for data: 7231391104 diff: 7231391104
I0511 21:45:46.182407   380 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 3130294272 diff: 3130294272
I0511 21:45:46.182523   380 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 21:45:46.182636   380 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 21:45:46.182757   380 net.cpp:421] Network initialization done.
I0511 21:45:46.184695   380 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/test.prototxt
I0511 21:45:46.185166   380 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 21:45:46.190017   380 net.cpp:110] Using FLOAT as default forward math type
I0511 21:45:46.190050   380 net.cpp:116] Using FLOAT as default backward math type
I0511 21:45:46.190078   380 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 21:45:46.190105   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.190148   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.190302   380 net.cpp:200] Created Layer data (0)
I0511 21:45:46.190325   380 net.cpp:542] data -> data
I0511 21:45:46.190349   380 net.cpp:542] data -> label
I0511 21:45:46.190379   380 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 21:45:46.190418   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.201643   423 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 21:45:46.203445   380 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 21:45:46.203574   380 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 21:45:46.203630   380 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 21:45:46.203825   380 net.cpp:260] Setting up data
I0511 21:45:46.203841   380 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 21:45:46.203862   380 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 21:45:46.203877   380 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 21:45:46.203887   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.203908   380 net.cpp:200] Created Layer data_data_0_split (1)
I0511 21:45:46.203920   380 net.cpp:572] data_data_0_split <- data
I0511 21:45:46.203929   380 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 21:45:46.203941   380 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 21:45:46.203949   380 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 21:45:46.203964   380 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 21:45:46.203980   380 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 21:45:46.203995   380 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 21:45:46.204003   380 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 21:45:46.204187   380 net.cpp:260] Setting up data_data_0_split
I0511 21:45:46.204202   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204221   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204237   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204252   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204259   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204272   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204288   380 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 21:45:46.204308   380 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 21:45:46.204313   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.204330   380 net.cpp:200] Created Layer data/bias (2)
I0511 21:45:46.204344   380 net.cpp:572] data/bias <- data_data_0_split_0
I0511 21:45:46.204355   380 net.cpp:542] data/bias -> data/bias
I0511 21:45:46.204576   380 net.cpp:260] Setting up data/bias
I0511 21:45:46.204589   380 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 21:45:46.204613   380 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 21:45:46.204627   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.204653   380 net.cpp:200] Created Layer conv1a (3)
I0511 21:45:46.204665   380 net.cpp:572] conv1a <- data/bias
I0511 21:45:46.204682   380 net.cpp:542] conv1a -> conv1a
I0511 21:45:46.206666   424 data_layer.cpp:105] (0) Parser threads: 1
I0511 21:45:46.206688   380 net.cpp:260] Setting up conv1a
I0511 21:45:46.207042   380 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 21:45:46.207024   424 data_layer.cpp:107] (0) Transformer threads: 1
I0511 21:45:46.207072   380 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 21:45:46.207108   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.207129   380 net.cpp:200] Created Layer conv1a/bn (4)
I0511 21:45:46.207142   380 net.cpp:572] conv1a/bn <- conv1a
I0511 21:45:46.207170   380 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 21:45:46.207883   380 net.cpp:260] Setting up conv1a/bn
I0511 21:45:46.207899   380 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 21:45:46.207928   380 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 21:45:46.207938   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.207947   380 net.cpp:200] Created Layer conv1a/relu (5)
I0511 21:45:46.207957   380 net.cpp:572] conv1a/relu <- conv1a
I0511 21:45:46.207967   380 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 21:45:46.207984   380 net.cpp:260] Setting up conv1a/relu
I0511 21:45:46.207996   380 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 21:45:46.208011   380 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 21:45:46.208019   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.208043   380 net.cpp:200] Created Layer conv1b (6)
I0511 21:45:46.208055   380 net.cpp:572] conv1b <- conv1a
I0511 21:45:46.208063   380 net.cpp:542] conv1b -> conv1b
I0511 21:45:46.208582   380 net.cpp:260] Setting up conv1b
I0511 21:45:46.208597   380 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 21:45:46.208617   380 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 21:45:46.208628   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.208652   380 net.cpp:200] Created Layer conv1b/bn (7)
I0511 21:45:46.208662   380 net.cpp:572] conv1b/bn <- conv1b
I0511 21:45:46.208668   380 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 21:45:46.209357   380 net.cpp:260] Setting up conv1b/bn
I0511 21:45:46.209374   380 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 21:45:46.209400   380 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 21:45:46.209410   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209420   380 net.cpp:200] Created Layer conv1b/relu (8)
I0511 21:45:46.209434   380 net.cpp:572] conv1b/relu <- conv1b
I0511 21:45:46.209451   380 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 21:45:46.209465   380 net.cpp:260] Setting up conv1b/relu
I0511 21:45:46.209476   380 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 21:45:46.209494   380 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 21:45:46.209509   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209527   380 net.cpp:200] Created Layer pool1 (9)
I0511 21:45:46.209537   380 net.cpp:572] pool1 <- conv1b
I0511 21:45:46.209543   380 net.cpp:542] pool1 -> pool1
I0511 21:45:46.209640   380 net.cpp:260] Setting up pool1
I0511 21:45:46.209650   380 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 21:45:46.209668   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 21:45:46.209681   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.209703   380 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 21:45:46.209712   380 net.cpp:572] res2a_branch2a <- pool1
I0511 21:45:46.209720   380 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 21:45:46.210548   380 net.cpp:260] Setting up res2a_branch2a
I0511 21:45:46.210561   380 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 21:45:46.210582   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.210593   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.210613   380 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 21:45:46.210626   380 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 21:45:46.210634   380 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 21:45:46.211292   380 net.cpp:260] Setting up res2a_branch2a/bn
I0511 21:45:46.211303   380 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 21:45:46.211334   380 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.211347   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.211364   380 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 21:45:46.211370   380 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 21:45:46.211381   380 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 21:45:46.211401   380 net.cpp:260] Setting up res2a_branch2a/relu
I0511 21:45:46.211413   380 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 21:45:46.211428   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 21:45:46.211434   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.211458   380 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 21:45:46.211470   380 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 21:45:46.211478   380 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 21:45:46.227989   380 net.cpp:260] Setting up res2a_branch2b
I0511 21:45:46.228031   380 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 21:45:46.228070   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.228085   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.228121   380 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 21:45:46.228145   380 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 21:45:46.228171   380 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 21:45:46.229591   380 net.cpp:260] Setting up res2a_branch2b/bn
I0511 21:45:46.229984   380 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 21:45:46.230072   380 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.230096   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230127   380 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 21:45:46.230162   380 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 21:45:46.230197   380 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 21:45:46.230238   380 net.cpp:260] Setting up res2a_branch2b/relu
I0511 21:45:46.230262   380 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 21:45:46.230300   380 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 21:45:46.230325   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230386   380 net.cpp:200] Created Layer pool2 (16)
I0511 21:45:46.230409   380 net.cpp:572] pool2 <- res2a_branch2b
I0511 21:45:46.230446   380 net.cpp:542] pool2 -> pool2
I0511 21:45:46.230635   380 net.cpp:260] Setting up pool2
I0511 21:45:46.230660   380 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 21:45:46.230698   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 21:45:46.230721   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.230783   380 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 21:45:46.230795   380 net.cpp:572] res3a_branch2a <- pool2
I0511 21:45:46.230823   380 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 21:45:46.235287   380 net.cpp:260] Setting up res3a_branch2a
I0511 21:45:46.235301   380 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 21:45:46.235317   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.235348   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.235366   380 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 21:45:46.235378   380 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 21:45:46.235384   380 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 21:45:46.235874   380 net.cpp:260] Setting up res3a_branch2a/bn
I0511 21:45:46.235885   380 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 21:45:46.235910   380 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.235919   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.235929   380 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 21:45:46.235942   380 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 21:45:46.235956   380 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 21:45:46.235965   380 net.cpp:260] Setting up res3a_branch2a/relu
I0511 21:45:46.235973   380 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 21:45:46.235987   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 21:45:46.235993   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.236018   380 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 21:45:46.236028   380 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 21:45:46.236035   380 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 21:45:46.237156   380 net.cpp:260] Setting up res3a_branch2b
I0511 21:45:46.237172   380 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 21:45:46.237191   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.237201   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.237211   380 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 21:45:46.237221   380 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 21:45:46.237227   380 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 21:45:46.246433   380 net.cpp:260] Setting up res3a_branch2b/bn
I0511 21:45:46.246552   380 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 21:45:46.246776   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.246965   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.247051   380 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 21:45:46.247100   380 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 21:45:46.247114   380 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 21:45:46.247614   380 net.cpp:260] Setting up res3a_branch2b/relu
I0511 21:45:46.247885   380 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 21:45:46.248051   380 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.248260   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.248576   380 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 21:45:46.248718   380 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 21:45:46.248903   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:46.249301   380 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.250378   380 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 21:45:46.250589   380 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 21:45:46.250723   380 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 21:45:46.251240   380 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 21:45:46.251564   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.251822   380 net.cpp:200] Created Layer pool3 (24)
I0511 21:45:46.251963   380 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 21:45:46.252144   380 net.cpp:542] pool3 -> pool3
I0511 21:45:46.253098   380 net.cpp:260] Setting up pool3
I0511 21:45:46.253113   380 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 21:45:46.253127   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 21:45:46.253136   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.253295   380 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 21:45:46.253309   380 net.cpp:572] res4a_branch2a <- pool3
I0511 21:45:46.253324   380 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 21:45:46.280545   380 net.cpp:260] Setting up res4a_branch2a
I0511 21:45:46.280565   380 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 21:45:46.280586   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.280592   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.280612   380 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 21:45:46.280623   380 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 21:45:46.289446   380 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 21:45:46.290066   380 net.cpp:260] Setting up res4a_branch2a/bn
I0511 21:45:46.290174   380 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 21:45:46.290297   380 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.290405   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.290518   380 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 21:45:46.290627   380 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 21:45:46.290735   380 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 21:45:46.290845   380 net.cpp:260] Setting up res4a_branch2a/relu
I0511 21:45:46.290949   380 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 21:45:46.291060   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 21:45:46.291167   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.291285   380 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 21:45:46.291391   380 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 21:45:46.291496   380 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 21:45:46.295001   380 net.cpp:260] Setting up res4a_branch2b
I0511 21:45:46.309317   380 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 21:45:46.309464   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.309574   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.309691   380 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 21:45:46.309803   380 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 21:45:46.309914   380 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 21:45:46.310549   380 net.cpp:260] Setting up res4a_branch2b/bn
I0511 21:45:46.310658   380 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 21:45:46.310782   380 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.310899   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.311014   380 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 21:45:46.311136   380 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 21:45:46.311244   380 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 21:45:46.311352   380 net.cpp:260] Setting up res4a_branch2b/relu
I0511 21:45:46.311461   380 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 21:45:46.311573   380 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 21:45:46.311681   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.311796   380 net.cpp:200] Created Layer pool4 (31)
I0511 21:45:46.311904   380 net.cpp:572] pool4 <- res4a_branch2b
I0511 21:45:46.312014   380 net.cpp:542] pool4 -> pool4
I0511 21:45:46.312203   380 net.cpp:260] Setting up pool4
I0511 21:45:46.312309   380 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 21:45:46.312422   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 21:45:46.312531   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.312652   380 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 21:45:46.312760   380 net.cpp:572] res5a_branch2a <- pool4
I0511 21:45:46.312870   380 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 21:45:46.362547   380 net.cpp:260] Setting up res5a_branch2a
I0511 21:45:46.362637   380 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 21:45:46.362727   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 21:45:46.362792   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.362861   380 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 21:45:46.362923   380 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 21:45:46.362987   380 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 21:45:46.363510   380 net.cpp:260] Setting up res5a_branch2a/bn
I0511 21:45:46.363582   380 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 21:45:46.363656   380 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 21:45:46.363718   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.363782   380 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 21:45:46.363843   380 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 21:45:46.363906   380 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 21:45:46.363970   380 net.cpp:260] Setting up res5a_branch2a/relu
I0511 21:45:46.364030   380 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 21:45:46.364094   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 21:45:46.364156   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.364226   380 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 21:45:46.364289   380 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 21:45:46.364351   380 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 21:45:46.377768   380 net.cpp:260] Setting up res5a_branch2b
I0511 21:45:46.377907   380 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 21:45:46.378005   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 21:45:46.378073   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.378150   380 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 21:45:46.378221   380 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 21:45:46.378293   380 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 21:45:46.378808   380 net.cpp:260] Setting up res5a_branch2b/bn
I0511 21:45:46.378886   380 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 21:45:46.378983   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 21:45:46.379052   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.379124   380 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 21:45:46.379196   380 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 21:45:46.379271   380 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 21:45:46.379344   380 net.cpp:260] Setting up res5a_branch2b/relu
I0511 21:45:46.379412   380 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 21:45:46.379489   380 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 21:45:46.379559   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.379632   380 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 21:45:46.379701   380 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 21:45:46.379776   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.379850   380 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.379967   380 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 21:45:46.380038   380 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 21:45:46.380115   380 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 21:45:46.380187   380 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 21:45:46.380256   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.380331   380 net.cpp:200] Created Layer pool6 (39)
I0511 21:45:46.380400   380 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 21:45:46.380470   380 net.cpp:542] pool6 -> pool6
I0511 21:45:46.380605   380 net.cpp:260] Setting up pool6
I0511 21:45:46.380674   380 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 21:45:46.380751   380 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 21:45:46.380825   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.380897   380 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 21:45:46.380965   380 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 21:45:46.381037   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 21:45:46.381110   380 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 21:45:46.381222   380 net.cpp:260] Setting up pool6_pool6_0_split
I0511 21:45:46.381295   380 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 21:45:46.381371   380 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 21:45:46.381444   380 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 21:45:46.381513   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.381588   380 net.cpp:200] Created Layer pool7 (41)
I0511 21:45:46.381657   380 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 21:45:46.381726   380 net.cpp:542] pool7 -> pool7
I0511 21:45:46.381860   380 net.cpp:260] Setting up pool7
I0511 21:45:46.381929   380 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 21:45:46.382002   380 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 21:45:46.382071   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.382148   380 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 21:45:46.382220   380 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 21:45:46.382299   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 21:45:46.382373   380 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 21:45:46.382485   380 net.cpp:260] Setting up pool7_pool7_0_split
I0511 21:45:46.382553   380 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 21:45:46.382624   380 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 21:45:46.382699   380 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 21:45:46.382766   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.382839   380 net.cpp:200] Created Layer pool8 (43)
I0511 21:45:46.382908   380 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 21:45:46.382978   380 net.cpp:542] pool8 -> pool8
I0511 21:45:46.383110   380 net.cpp:260] Setting up pool8
I0511 21:45:46.383183   380 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 21:45:46.383256   380 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 21:45:46.383324   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.383396   380 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 21:45:46.383466   380 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 21:45:46.383535   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 21:45:46.383606   380 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 21:45:46.383713   380 net.cpp:260] Setting up pool8_pool8_0_split
I0511 21:45:46.383780   380 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 21:45:46.383852   380 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 21:45:46.383924   380 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 21:45:46.383992   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.384066   380 net.cpp:200] Created Layer pool9 (45)
I0511 21:45:46.384135   380 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 21:45:46.384209   380 net.cpp:542] pool9 -> pool9
I0511 21:45:46.384338   380 net.cpp:260] Setting up pool9
I0511 21:45:46.384407   380 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 21:45:46.384479   380 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 21:45:46.384550   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.384631   380 net.cpp:200] Created Layer ctx_output1 (46)
I0511 21:45:46.384701   380 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 21:45:46.384773   380 net.cpp:542] ctx_output1 -> ctx_output1
I0511 21:45:46.385855   380 net.cpp:260] Setting up ctx_output1
I0511 21:45:46.385944   380 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 21:45:46.386016   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 21:45:46.386077   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.386142   380 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 21:45:46.386204   380 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 21:45:46.386265   380 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 21:45:46.386330   380 net.cpp:260] Setting up ctx_output1/relu
I0511 21:45:46.386390   380 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 21:45:46.386454   380 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 21:45:46.386518   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.386590   380 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 21:45:46.386669   380 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 21:45:46.386750   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.386822   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.386893   380 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.387023   380 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 21:45:46.387091   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387164   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387236   380 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 21:45:46.387310   380 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 21:45:46.387382   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.387460   380 net.cpp:200] Created Layer ctx_output2 (49)
I0511 21:45:46.387531   380 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 21:45:46.387601   380 net.cpp:542] ctx_output2 -> ctx_output2
I0511 21:45:46.391471   380 net.cpp:260] Setting up ctx_output2
I0511 21:45:46.391567   380 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 21:45:46.391642   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 21:45:46.391705   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.391768   380 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 21:45:46.391830   380 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 21:45:46.391893   380 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 21:45:46.391958   380 net.cpp:260] Setting up ctx_output2/relu
I0511 21:45:46.392019   380 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 21:45:46.392084   380 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 21:45:46.392145   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.392210   380 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 21:45:46.392271   380 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 21:45:46.392333   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.392397   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.392462   380 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.392585   380 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 21:45:46.392647   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392711   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392774   380 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 21:45:46.392839   380 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 21:45:46.392899   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.392969   380 net.cpp:200] Created Layer ctx_output3 (52)
I0511 21:45:46.393033   380 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 21:45:46.393095   380 net.cpp:542] ctx_output3 -> ctx_output3
I0511 21:45:46.396152   380 net.cpp:260] Setting up ctx_output3
I0511 21:45:46.396239   380 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 21:45:46.396322   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 21:45:46.396394   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.396458   380 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 21:45:46.396520   380 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 21:45:46.396582   380 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 21:45:46.396646   380 net.cpp:260] Setting up ctx_output3/relu
I0511 21:45:46.396706   380 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 21:45:46.404789   380 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 21:45:46.404901   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.404991   380 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 21:45:46.405125   380 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 21:45:46.405211   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.405300   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.405388   380 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.405566   380 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 21:45:46.405740   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.405831   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.405916   380 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 21:45:46.406003   380 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 21:45:46.406085   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.406177   380 net.cpp:200] Created Layer ctx_output4 (55)
I0511 21:45:46.406268   380 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 21:45:46.406350   380 net.cpp:542] ctx_output4 -> ctx_output4
I0511 21:45:46.409499   380 net.cpp:260] Setting up ctx_output4
I0511 21:45:46.412668   380 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 21:45:46.412770   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 21:45:46.412847   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.412923   380 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 21:45:46.413003   380 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 21:45:46.413077   380 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 21:45:46.413152   380 net.cpp:260] Setting up ctx_output4/relu
I0511 21:45:46.413226   380 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 21:45:46.413303   380 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 21:45:46.413377   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.413450   380 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 21:45:46.413522   380 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 21:45:46.413596   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.413669   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.413745   380 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.413892   380 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 21:45:46.414038   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414124   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414219   380 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 21:45:46.414297   380 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 21:45:46.414368   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.414449   380 net.cpp:200] Created Layer ctx_output5 (58)
I0511 21:45:46.414530   380 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 21:45:46.414602   380 net.cpp:542] ctx_output5 -> ctx_output5
I0511 21:45:46.417708   380 net.cpp:260] Setting up ctx_output5
I0511 21:45:46.420851   380 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 21:45:46.420959   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 21:45:46.421047   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.421136   380 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 21:45:46.421221   380 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 21:45:46.421301   380 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 21:45:46.421387   380 net.cpp:260] Setting up ctx_output5/relu
I0511 21:45:46.421470   380 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 21:45:46.421561   380 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 21:45:46.421645   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.421730   380 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 21:45:46.421815   380 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 21:45:46.421896   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.421986   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.422076   380 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.422235   380 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 21:45:46.422394   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422482   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422571   380 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 21:45:46.422660   380 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 21:45:46.422740   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.422830   380 net.cpp:200] Created Layer ctx_output6 (61)
I0511 21:45:46.422920   380 net.cpp:572] ctx_output6 <- pool9
I0511 21:45:46.423007   380 net.cpp:542] ctx_output6 -> ctx_output6
I0511 21:45:46.427263   380 net.cpp:260] Setting up ctx_output6
I0511 21:45:46.431545   380 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 21:45:46.431653   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 21:45:46.431731   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.431807   380 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 21:45:46.431881   380 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 21:45:46.431954   380 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 21:45:46.432031   380 net.cpp:260] Setting up ctx_output6/relu
I0511 21:45:46.432104   380 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 21:45:46.432181   380 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 21:45:46.432261   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.432353   380 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 21:45:46.432427   380 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 21:45:46.432497   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.432571   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.432646   380 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.432790   380 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 21:45:46.432934   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433012   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433086   380 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 21:45:46.433162   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.433234   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.433321   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 21:45:46.433405   380 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 21:45:46.433480   380 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 21:45:46.433954   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 21:45:46.434432   380 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 21:45:46.434518   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.434594   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.434672   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 21:45:46.434748   380 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 21:45:46.434821   380 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435019   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435214   380 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 21:45:46.435290   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.435359   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.435434   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 21:45:46.435506   380 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 21:45:46.435580   380 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.439497   380 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.440151   380 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 21:45:46.440250   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.440335   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.440433   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 21:45:46.440531   380 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 21:45:46.440614   380 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 21:45:46.441155   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 21:45:46.441709   380 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 21:45:46.441807   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.441905   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.442016   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 21:45:46.442101   380 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 21:45:46.442183   380 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.442391   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.442602   380 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 21:45:46.442690   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.442773   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.442858   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 21:45:46.442943   380 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 21:45:46.443024   380 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.445452   380 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.447906   380 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 21:45:46.447994   380 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.448065   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.448143   380 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 21:45:46.448220   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 21:45:46.448293   380 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 21:45:46.448367   380 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 21:45:46.448479   380 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 21:45:46.448587   380 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 21:45:46.448662   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.448734   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.448812   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 21:45:46.448892   380 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 21:45:46.448964   380 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 21:45:46.449771   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 21:45:46.450063   380 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 21:45:46.450150   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.450225   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.450304   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 21:45:46.450381   380 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 21:45:46.450453   380 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.450654   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.450851   380 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 21:45:46.450928   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.451002   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.451074   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 21:45:46.451156   380 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 21:45:46.451232   380 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.451414   380 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.451576   380 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 21:45:46.451651   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.451722   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.451802   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 21:45:46.451884   380 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 21:45:46.451954   380 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 21:45:46.452471   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 21:45:46.452996   380 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 21:45:46.453079   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.453153   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.453228   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 21:45:46.453307   380 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 21:45:46.453382   380 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.453584   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.453783   380 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 21:45:46.453862   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.453933   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.454005   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 21:45:46.454082   380 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 21:45:46.454157   380 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.454301   380 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.454447   380 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 21:45:46.454524   380 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.454596   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.454669   380 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 21:45:46.454748   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 21:45:46.454824   380 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 21:45:46.454898   380 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 21:45:46.455003   380 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 21:45:46.455102   380 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 21:45:46.455179   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.455250   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.455328   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 21:45:46.455406   380 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 21:45:46.455482   380 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 21:45:46.456003   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 21:45:46.456535   380 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 21:45:46.456629   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.456719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.456799   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 21:45:46.456872   380 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 21:45:46.456944   380 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457147   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457350   380 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 21:45:46.457448   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.457530   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.457618   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 21:45:46.457708   380 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 21:45:46.457789   380 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.457948   380 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.458106   380 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 21:45:46.458191   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.458273   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.458364   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 21:45:46.458456   380 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 21:45:46.458536   380 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 21:45:46.459113   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 21:45:46.459692   380 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 21:45:46.459789   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.459877   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.459962   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 21:45:46.460048   380 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 21:45:46.460134   380 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460350   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460566   380 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 21:45:46.460656   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.460737   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.460824   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 21:45:46.460907   380 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 21:45:46.460992   380 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.461140   380 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.461285   380 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 21:45:46.461385   380 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.461467   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.461553   380 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 21:45:46.461644   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 21:45:46.461731   380 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 21:45:46.461838   380 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 21:45:46.461951   380 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 21:45:46.462060   380 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 21:45:46.462146   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.462226   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.462316   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 21:45:46.462404   380 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 21:45:46.462500   380 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 21:45:46.463073   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 21:45:46.463649   380 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 21:45:46.463747   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.463832   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.463920   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 21:45:46.464005   380 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 21:45:46.464087   380 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464315   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464537   380 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 21:45:46.464624   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.464704   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.464789   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 21:45:46.464874   380 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 21:45:46.464958   380 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.465103   380 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.465245   380 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 21:45:46.465339   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.465421   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.465513   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 21:45:46.465602   380 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 21:45:46.465690   380 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 21:45:46.466259   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 21:45:46.466832   380 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 21:45:46.466926   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.467011   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.467101   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 21:45:46.467187   380 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 21:45:46.467270   380 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.467486   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.467700   380 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 21:45:46.467792   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.467875   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.467947   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 21:45:46.468020   380 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 21:45:46.468091   380 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.468228   380 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.468359   380 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 21:45:46.468444   380 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.468516   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.468591   380 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 21:45:46.468667   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 21:45:46.468739   380 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 21:45:46.468812   380 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 21:45:46.468909   380 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 21:45:46.469012   380 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 21:45:46.469100   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.469182   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.469272   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 21:45:46.469368   380 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 21:45:46.469451   380 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 21:45:46.469986   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 21:45:46.470520   380 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 21:45:46.470602   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.470674   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.470753   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 21:45:46.470830   380 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 21:45:46.470904   380 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471099   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471292   380 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 21:45:46.471370   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.471439   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.471513   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 21:45:46.471583   380 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 21:45:46.471654   380 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.471788   380 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.471920   380 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 21:45:46.471994   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.472064   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.472142   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 21:45:46.472230   380 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 21:45:46.472318   380 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 21:45:46.472765   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 21:45:46.473215   380 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 21:45:46.473309   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.473377   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.473453   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 21:45:46.473531   380 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 21:45:46.473603   380 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.473795   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.473985   380 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 21:45:46.474062   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.474133   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.474203   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 21:45:46.474277   380 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 21:45:46.474347   380 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.474483   380 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.474617   380 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 21:45:46.474692   380 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.474761   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.474841   380 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 21:45:46.474916   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 21:45:46.474989   380 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 21:45:46.475062   380 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 21:45:46.475153   380 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 21:45:46.475246   380 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 21:45:46.475322   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 21:45:46.475391   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.475471   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 21:45:46.475553   380 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 21:45:46.475625   380 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 21:45:46.476089   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 21:45:46.476559   380 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 21:45:46.476644   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 21:45:46.476719   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.476794   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 21:45:46.476868   380 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 21:45:46.476940   380 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477128   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477321   380 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 21:45:46.477439   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 21:45:46.477522   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.477610   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 21:45:46.477694   380 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 21:45:46.477774   380 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.477926   380 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.478076   380 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 21:45:46.478161   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 21:45:46.478241   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.478338   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 21:45:46.478428   380 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 21:45:46.478510   380 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 21:45:46.479022   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 21:45:46.479907   380 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 21:45:46.479992   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 21:45:46.480057   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.480134   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 21:45:46.480199   380 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 21:45:46.480270   380 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480475   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480545   380 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 21:45:46.480613   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 21:45:46.480684   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.480751   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 21:45:46.480821   380 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 21:45:46.480887   380 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.481024   380 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.481168   380 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 21:45:46.481252   380 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 21:45:46.481359   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.481446   380 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 21:45:46.481534   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 21:45:46.481616   380 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 21:45:46.481703   380 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 21:45:46.481806   380 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 21:45:46.481905   380 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 21:45:46.481990   380 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 21:45:46.482070   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.482169   380 net.cpp:200] Created Layer mbox_loc (106)
I0511 21:45:46.482254   380 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 21:45:46.482771   380 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 21:45:46.482846   380 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 21:45:46.482913   380 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 21:45:46.482985   380 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 21:45:46.483052   380 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 21:45:46.483121   380 net.cpp:542] mbox_loc -> mbox_loc
I0511 21:45:46.483220   380 net.cpp:260] Setting up mbox_loc
I0511 21:45:46.483289   380 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 21:45:46.483359   380 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 21:45:46.483428   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.483502   380 net.cpp:200] Created Layer mbox_conf (107)
I0511 21:45:46.483567   380 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 21:45:46.483639   380 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 21:45:46.483705   380 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 21:45:46.483775   380 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 21:45:46.483842   380 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 21:45:46.483916   380 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 21:45:46.483990   380 net.cpp:542] mbox_conf -> mbox_conf
I0511 21:45:46.484086   380 net.cpp:260] Setting up mbox_conf
I0511 21:45:46.484158   380 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 21:45:46.484230   380 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 21:45:46.484308   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.485137   380 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 21:45:46.485206   380 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 21:45:46.485270   380 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 21:45:46.485337   380 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 21:45:46.485410   380 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 21:45:46.485477   380 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 21:45:46.485549   380 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 21:45:46.485615   380 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 21:45:46.485723   380 net.cpp:260] Setting up mbox_priorbox
I0511 21:45:46.485792   380 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 21:45:46.485867   380 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 21:45:46.485934   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.486013   380 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 21:45:46.486090   380 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 21:45:46.486160   380 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 21:45:46.486246   380 net.cpp:260] Setting up mbox_conf_reshape
I0511 21:45:46.486306   380 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 21:45:46.486371   380 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 21:45:46.486433   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.486502   380 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 21:45:46.486562   380 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 21:45:46.486625   380 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 21:45:46.486780   380 net.cpp:260] Setting up mbox_conf_softmax
I0511 21:45:46.486853   380 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 21:45:46.486937   380 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 21:45:46.487015   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.487083   380 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 21:45:46.487144   380 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 21:45:46.487207   380 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 21:45:46.489516   380 net.cpp:260] Setting up mbox_conf_flatten
I0511 21:45:46.491750   380 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 21:45:46.491843   380 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 21:45:46.491915   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.492012   380 net.cpp:200] Created Layer detection_out (112)
I0511 21:45:46.492110   380 net.cpp:572] detection_out <- mbox_loc
I0511 21:45:46.492182   380 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 21:45:46.492255   380 net.cpp:572] detection_out <- mbox_priorbox
I0511 21:45:46.492327   380 net.cpp:542] detection_out -> detection_out
I0511 21:45:46.493100   380 net.cpp:260] Setting up detection_out
I0511 21:45:46.493896   380 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 21:45:46.493995   380 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 21:45:46.494489   380 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 21:45:46.494583   380 net.cpp:200] Created Layer detection_eval (113)
I0511 21:45:46.494654   380 net.cpp:572] detection_eval <- detection_out
I0511 21:45:46.494729   380 net.cpp:572] detection_eval <- label
I0511 21:45:46.494801   380 net.cpp:542] detection_eval -> detection_eval
I0511 21:45:46.495478   380 net.cpp:260] Setting up detection_eval
I0511 21:45:46.495854   380 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 21:45:46.495945   380 net.cpp:338] detection_eval does not need backward computation.
I0511 21:45:46.496026   380 net.cpp:338] detection_out does not need backward computation.
I0511 21:45:46.496107   380 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 21:45:46.496193   380 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 21:45:46.496277   380 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 21:45:46.496361   380 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 21:45:46.496450   380 net.cpp:338] mbox_conf does not need backward computation.
I0511 21:45:46.496537   380 net.cpp:338] mbox_loc does not need backward computation.
I0511 21:45:46.496623   380 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.496711   380 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.496793   380 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.496876   380 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 21:45:46.496958   380 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.497040   380 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.497125   380 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 21:45:46.497206   380 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.497298   380 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.497386   380 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.497470   380 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 21:45:46.497556   380 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.497648   380 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.497746   380 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 21:45:46.497828   380 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.497911   380 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.497992   380 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.498075   380 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 21:45:46.498162   380 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.498245   380 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.498327   380 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 21:45:46.498409   380 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.498494   380 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.498580   380 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.498663   380 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 21:45:46.498745   380 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.498831   380 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.498914   380 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 21:45:46.498994   380 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.499079   380 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.499161   380 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.499243   380 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 21:45:46.499325   380 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.499408   380 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.499491   380 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 21:45:46.499572   380 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 21:45:46.499644   380 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 21:45:46.499716   380 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 21:45:46.499790   380 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 21:45:46.499861   380 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 21:45:46.499936   380 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 21:45:46.500006   380 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 21:45:46.500082   380 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 21:45:46.500157   380 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 21:45:46.500228   380 net.cpp:338] ctx_output6 does not need backward computation.
I0511 21:45:46.500303   380 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 21:45:46.500373   380 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 21:45:46.500447   380 net.cpp:338] ctx_output5 does not need backward computation.
I0511 21:45:46.500517   380 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 21:45:46.500589   380 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 21:45:46.500667   380 net.cpp:338] ctx_output4 does not need backward computation.
I0511 21:45:46.500741   380 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 21:45:46.500828   380 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 21:45:46.500898   380 net.cpp:338] ctx_output3 does not need backward computation.
I0511 21:45:46.500969   380 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 21:45:46.501044   380 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 21:45:46.501114   380 net.cpp:338] ctx_output2 does not need backward computation.
I0511 21:45:46.501188   380 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 21:45:46.501260   380 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 21:45:46.501341   380 net.cpp:338] ctx_output1 does not need backward computation.
I0511 21:45:46.501426   380 net.cpp:338] pool9 does not need backward computation.
I0511 21:45:46.501509   380 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 21:45:46.501593   380 net.cpp:338] pool8 does not need backward computation.
I0511 21:45:46.501680   380 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 21:45:46.501763   380 net.cpp:338] pool7 does not need backward computation.
I0511 21:45:46.501844   380 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 21:45:46.501925   380 net.cpp:338] pool6 does not need backward computation.
I0511 21:45:46.502009   380 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 21:45:46.502091   380 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 21:45:46.502174   380 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 21:45:46.502256   380 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 21:45:46.502337   380 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 21:45:46.502418   380 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 21:45:46.502499   380 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 21:45:46.502578   380 net.cpp:338] pool4 does not need backward computation.
I0511 21:45:46.502660   380 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 21:45:46.502746   380 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 21:45:46.502825   380 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 21:45:46.502907   380 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 21:45:46.502991   380 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 21:45:46.503070   380 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 21:45:46.503139   380 net.cpp:338] pool3 does not need backward computation.
I0511 21:45:46.503211   380 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 21:45:46.503281   380 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 21:45:46.503356   380 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 21:45:46.503429   380 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 21:45:46.503504   380 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 21:45:46.503573   380 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 21:45:46.503643   380 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 21:45:46.503716   380 net.cpp:338] pool2 does not need backward computation.
I0511 21:45:46.503787   380 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 21:45:46.503859   380 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 21:45:46.503932   380 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 21:45:46.504002   380 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 21:45:46.504077   380 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 21:45:46.504164   380 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 21:45:46.504235   380 net.cpp:338] pool1 does not need backward computation.
I0511 21:45:46.504304   380 net.cpp:338] conv1b/relu does not need backward computation.
I0511 21:45:46.504379   380 net.cpp:338] conv1b/bn does not need backward computation.
I0511 21:45:46.504448   380 net.cpp:338] conv1b does not need backward computation.
I0511 21:45:46.504523   380 net.cpp:338] conv1a/relu does not need backward computation.
I0511 21:45:46.504591   380 net.cpp:338] conv1a/bn does not need backward computation.
I0511 21:45:46.504660   380 net.cpp:338] conv1a does not need backward computation.
I0511 21:45:46.504734   380 net.cpp:338] data/bias does not need backward computation.
I0511 21:45:46.504808   380 net.cpp:338] data_data_0_split does not need backward computation.
I0511 21:45:46.504880   380 net.cpp:338] data does not need backward computation.
I0511 21:45:46.504953   380 net.cpp:380] This network produces output detection_eval
I0511 21:45:46.505162   380 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 21:45:46.505416   380 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 21:45:46.505486   380 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 21:45:46.505555   380 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 21:45:46.505623   380 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 21:45:46.505692   380 net.cpp:421] Network initialization done.
I0511 21:45:46.506050   380 solver.cpp:55] Solver scaffolding done.
I0511 21:45:46.513011   380 caffe.cpp:158] Finetuning from /workspace/caffe-jacinto-models/trained/object_detection/voc0712/JDetNet/ssd512x512_ds_PSP_dsFac_32_fc_0_hdDS8_1_kerMbox_3_1stHdSameOpCh_1/sparse/voc0712_ssdJacintoNetV2_iter_104000.caffemodel
I0511 21:45:46.527025   380 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 21:45:46.534432   380 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 21:45:46.534554   380 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 21:45:46.534783   380 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 21:45:46.534942   380 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.535185   380 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 21:45:46.535269   380 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 21:45:46.535419   380 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.535650   380 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 21:45:46.535728   380 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 21:45:46.535809   380 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.535976   380 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.536211   380 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.536289   380 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.536443   380 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.536674   380 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.536751   380 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 21:45:46.536826   380 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.537063   380 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.537307   380 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.537396   380 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.537587   380 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.537837   380 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.537915   380 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.537992   380 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 21:45:46.538069   380 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.538545   380 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.538815   380 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.538892   380 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.539188   380 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.539433   380 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.539515   380 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 21:45:46.539593   380 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.540920   380 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.541182   380 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.541261   380 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.542013   380 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.542279   380 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.542358   380 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.542434   380 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 21:45:46.542512   380 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 21:45:46.542590   380 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 21:45:46.542667   380 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 21:45:46.542744   380 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 21:45:46.542821   380 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 21:45:46.542898   380 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 21:45:46.542975   380 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 21:45:46.543145   380 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 21:45:46.543216   380 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 21:45:46.543285   380 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 21:45:46.543354   380 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 21:45:46.543648   380 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 21:45:46.543731   380 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 21:45:46.543803   380 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 21:45:46.543871   380 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 21:45:46.544159   380 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 21:45:46.544242   380 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 21:45:46.544312   380 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 21:45:46.544381   380 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 21:45:46.544672   380 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 21:45:46.544754   380 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 21:45:46.544839   380 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 21:45:46.544929   380 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 21:45:46.545212   380 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 21:45:46.545298   380 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 21:45:46.545372   380 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 21:45:46.545442   380 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 21:45:46.545727   380 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 21:45:46.545809   380 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 21:45:46.545881   380 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 21:45:46.545949   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.546025   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 21:45:46.546768   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.547029   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.547103   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.547173   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.547241   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 21:45:46.549500   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.550314   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 21:45:46.551229   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.551368   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.551482   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.551561   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.551640   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.551717   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 21:45:46.552065   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.552281   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.552363   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.552440   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.552518   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 21:45:46.559976   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.560619   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 21:45:46.561362   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.561511   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.561622   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.561691   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.561758   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.561825   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 21:45:46.562155   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.562381   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.562458   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.562525   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.562593   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 21:45:46.564708   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.565276   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 21:45:46.566826   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.567023   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.567152   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.567235   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.567313   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.567390   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 21:45:46.567761   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.567975   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.568055   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.568136   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.568222   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 21:45:46.594996   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.598479   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 21:45:46.599906   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.600064   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.600219   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.600311   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.600399   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.600487   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 21:45:46.600895   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.601161   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.601245   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.601333   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.601418   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 21:45:46.604529   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.605144   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 21:45:46.606446   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.606596   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.606735   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.606819   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.606909   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.606995   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 21:45:46.607367   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.607615   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.607702   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.607790   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.607924   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 21:45:46.610924   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.611598   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 21:45:46.612929   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.613085   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.613231   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.613323   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.613415   380 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 21:45:46.613502   380 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 21:45:46.613587   380 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 21:45:46.613677   380 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 21:45:46.630904   380 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 21:45:46.643815   380 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 21:45:46.643945   380 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 21:45:46.644137   380 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 21:45:46.644341   380 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.644647   380 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 21:45:46.644735   380 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 21:45:46.644918   380 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.645222   380 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 21:45:46.645305   380 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 21:45:46.645390   380 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.645604   380 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.645902   380 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.645987   380 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.646188   380 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.646478   380 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.646566   380 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 21:45:46.646651   380 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.646950   380 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.647256   380 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.647346   380 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.647585   380 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.647878   380 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.647965   380 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.648064   380 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 21:45:46.648183   380 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.648825   380 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.649161   380 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.649250   380 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.649665   380 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.649991   380 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.650080   380 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 21:45:46.650172   380 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 21:45:46.658277   380 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 21:45:46.658674   380 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 21:45:46.658762   380 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 21:45:46.659781   380 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 21:45:46.660140   380 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 21:45:46.660233   380 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 21:45:46.660318   380 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 21:45:46.660400   380 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 21:45:46.660488   380 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 21:45:46.660567   380 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 21:45:46.660645   380 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 21:45:46.660727   380 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 21:45:46.660807   380 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 21:45:46.660894   380 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 21:45:46.661178   380 net.cpp:1137] Ignoring source layer ctx_output1/bn
I0511 21:45:46.661275   380 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 21:45:46.661379   380 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 21:45:46.661473   380 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 21:45:46.661918   380 net.cpp:1137] Ignoring source layer ctx_output2/bn
I0511 21:45:46.662030   380 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 21:45:46.662133   380 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 21:45:46.662228   380 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 21:45:46.662672   380 net.cpp:1137] Ignoring source layer ctx_output3/bn
I0511 21:45:46.662775   380 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 21:45:46.662859   380 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 21:45:46.662938   380 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 21:45:46.663318   380 net.cpp:1137] Ignoring source layer ctx_output4/bn
I0511 21:45:46.663414   380 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 21:45:46.663503   380 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 21:45:46.663583   380 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 21:45:46.663957   380 net.cpp:1137] Ignoring source layer ctx_output5/bn
I0511 21:45:46.664050   380 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 21:45:46.664145   380 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 21:45:46.664270   380 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 21:45:46.664722   380 net.cpp:1137] Ignoring source layer ctx_output6/bn
I0511 21:45:46.664824   380 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 21:45:46.664909   380 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 21:45:46.664991   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.665079   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_loc to ctx_output1/relu_mbox_loc target blob 0
W0511 21:45:46.665654   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.666043   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.666131   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.666224   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.666311   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 0
W0511 21:45:46.669927   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.670670   380 net.cpp:1194] Copying from ctx_output1/relu_mbox_conf to ctx_output1/relu_mbox_conf target blob 1
W0511 21:45:46.672155   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output1/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.672310   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.672458   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.672547   380 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.672642   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.672730   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_loc to ctx_output2/relu_mbox_loc target blob 0
W0511 21:45:46.673192   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.673476   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.673569   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.673660   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.673746   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 0
W0511 21:45:46.677266   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.677953   380 net.cpp:1194] Copying from ctx_output2/relu_mbox_conf to ctx_output2/relu_mbox_conf target blob 1
W0511 21:45:46.679345   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output2/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.679528   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.679678   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.679766   380 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.679850   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.679940   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_loc to ctx_output3/relu_mbox_loc target blob 0
W0511 21:45:46.680398   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.680662   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.680747   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.680824   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.680905   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 0
W0511 21:45:46.684139   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.684760   380 net.cpp:1194] Copying from ctx_output3/relu_mbox_conf to ctx_output3/relu_mbox_conf target blob 1
W0511 21:45:46.686022   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output3/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.686182   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.686327   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.686410   380 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.686488   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.686571   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_loc to ctx_output4/relu_mbox_loc target blob 0
W0511 21:45:46.686995   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_loc'; shape mismatch.  Source param shape is 24 256 3 3 (55296); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.687269   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.687357   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.687438   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.687517   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 0
W0511 21:45:46.690886   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 256 3 3 (290304); target param shape is 24 256 1 1 (6144). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.691555   380 net.cpp:1194] Copying from ctx_output4/relu_mbox_conf to ctx_output4/relu_mbox_conf target blob 1
W0511 21:45:46.692900   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output4/relu_mbox_conf'; shape mismatch.  Source param shape is 126 (126); target param shape is 24 (24). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.693017   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.693118   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.693197   380 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.693270   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.693367   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_loc to ctx_output5/relu_mbox_loc target blob 0
W0511 21:45:46.693657   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.693861   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.693940   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.694015   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.694093   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 0
W0511 21:45:46.696245   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.696812   380 net.cpp:1194] Copying from ctx_output5/relu_mbox_conf to ctx_output5/relu_mbox_conf target blob 1
W0511 21:45:46.697512   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output5/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.697643   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.697746   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.697820   380 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.697890   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
W0511 21:45:46.697962   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_loc to ctx_output6/relu_mbox_loc target blob 0
W0511 21:45:46.698228   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_loc'; shape mismatch.  Source param shape is 16 256 3 3 (36864); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.698418   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 21:45:46.698490   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 21:45:46.698559   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
W0511 21:45:46.698630   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 0
W0511 21:45:46.700592   380 net.cpp:1210] Cannot copy param 0 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 256 3 3 (193536); target param shape is 16 256 1 1 (4096). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
W0511 21:45:46.701179   380 net.cpp:1194] Copying from ctx_output6/relu_mbox_conf to ctx_output6/relu_mbox_conf target blob 1
W0511 21:45:46.701872   380 net.cpp:1210] Cannot copy param 1 weights from layer 'ctx_output6/relu_mbox_conf'; shape mismatch.  Source param shape is 84 (84); target param shape is 16 (16). To learn this layer's parameters from scratch rather than copying from a saved net, rename the layer.
I0511 21:45:46.702011   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 21:45:46.702121   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 21:45:46.702199   380 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 21:45:46.702273   380 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 21:45:46.702347   380 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 21:45:46.702420   380 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 21:45:46.702495   380 net.cpp:1137] Ignoring source layer mbox_loss
I0511 21:45:46.702904   380 caffe.cpp:260] Starting Optimization
I0511 21:45:46.703333   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 21:45:46.719657   380 solver.cpp:455] Solving ssdJacintoNetV2
I0511 21:45:46.720780   380 solver.cpp:456] Learning Rate Policy: poly
I0511 21:45:46.720927   380 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 21:45:46.725401   380 solver.cpp:269] Initial Test started...
I0511 21:45:46.729791   380 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 21:45:46.733114   380 net.cpp:1071] Ignoring source layer mbox_loss
I0511 21:45:46.750828   425 common.cpp:528] NVML initialized, thread 425
I0511 21:45:46.801403   425 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 425
I0511 21:45:52.641427   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:46:21.546025   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:46:22.080286   380 solver.cpp:749] class AP 1: 0
I0511 21:46:22.096787   380 solver.cpp:749] class AP 2: 0
I0511 21:46:22.122843   380 solver.cpp:749] class AP 3: 0
I0511 21:46:22.122872   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0
I0511 21:46:22.122942   380 solver.cpp:274] Initial Test completed in 35.3925s
I0511 21:46:22.998708   380 solver.cpp:360] Iteration 0 (0.875677 s), 0/1129.4ep, loss = 17.8434
I0511 21:46:22.998790   380 solver.cpp:378]     Train net output #0: mbox_loss = 17.8434 (* 1 = 17.8434 loss)
I0511 21:46:22.998824   380 sgd_solver.cpp:172] Iteration 0, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I0511 21:46:23.004179   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.5 sparsity_achieved=0.584013 iter=0
W0511 21:46:23.004243   380 net.cpp:2654] conv1a ni=3 no=32
W0511 21:46:23.933059   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.229583
W0511 21:46:23.933159   380 net.cpp:2654] conv1b ni=32 no=32
W0511 21:46:24.291654   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.497396
W0511 21:46:24.291754   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0511 21:46:24.548342   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.5
W0511 21:46:24.548440   380 net.cpp:2654] res2a_branch2b ni=64 no=64
I0511 21:46:25.367794   386 data_reader.cpp:320] Restarting data pre-fetching
W0511 21:46:26.278247   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.489041
W0511 21:46:26.278638   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0511 21:46:26.911547   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.499932
W0511 21:46:26.911657   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0511 21:46:27.630895   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.499891
W0511 21:46:27.631026   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0511 21:46:28.542709   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.499736
W0511 21:46:28.542829   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0511 21:46:29.339414   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.498535
W0511 21:46:29.339459   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0511 21:46:30.473320   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.496552
W0511 21:46:30.473347   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0511 21:46:31.159669   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.564619
W0511 21:46:31.159693   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0511 21:46:31.159698   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0511 21:46:31.159704   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0511 21:46:31.159709   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0511 21:46:31.159714   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0511 21:46:31.159720   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0511 21:46:31.159729   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159735   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0511 21:46:31.159742   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159765   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159772   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159781   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159790   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0511 21:46:31.159798   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0511 21:46:31.159804   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159811   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0511 21:46:31.159817   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0511 21:46:31.159826   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0511 21:46:31.159837   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 21:46:31.162729   380 solver.cpp:391] Sparsity after update:
I0511 21:46:31.163635   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0511 21:46:31.163641   380 net.cpp:2780] conv1a_param_0(0.23) 
I0511 21:46:31.163666   380 net.cpp:2780] conv1b_param_0(0.497) 
I0511 21:46:31.163671   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163681   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163688   380 net.cpp:2780] ctx_output1_param_0(0) 
I0511 21:46:31.163693   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163697   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163702   380 net.cpp:2780] ctx_output2_param_0(0) 
I0511 21:46:31.163707   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163712   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163718   380 net.cpp:2780] ctx_output3_param_0(0) 
I0511 21:46:31.163724   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163731   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163736   380 net.cpp:2780] ctx_output4_param_0(0) 
I0511 21:46:31.163741   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163746   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163753   380 net.cpp:2780] ctx_output5_param_0(0) 
I0511 21:46:31.163758   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0511 21:46:31.163763   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0511 21:46:31.163769   380 net.cpp:2780] ctx_output6_param_0(0) 
I0511 21:46:31.163775   380 net.cpp:2780] res2a_branch2a_param_0(0.5) 
I0511 21:46:31.163781   380 net.cpp:2780] res2a_branch2b_param_0(0.489) 
I0511 21:46:31.163786   380 net.cpp:2780] res3a_branch2a_param_0(0.5) 
I0511 21:46:31.163794   380 net.cpp:2780] res3a_branch2b_param_0(0.5) 
I0511 21:46:31.163800   380 net.cpp:2780] res4a_branch2a_param_0(0.5) 
I0511 21:46:31.163807   380 net.cpp:2780] res4a_branch2b_param_0(0.499) 
I0511 21:46:31.163812   380 net.cpp:2780] res5a_branch2a_param_0(0.497) 
I0511 21:46:31.163817   380 net.cpp:2780] res5a_branch2b_param_0(0.565) 
I0511 21:46:31.163822   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.21038e+06/3.10435e+06) 0.39
I0511 21:46:31.901993   380 solver.cpp:360] Iteration 1 (8.90313 s), 0.1/1129.4ep, loss = 16.3147
I0511 21:46:31.902081   380 solver.cpp:378]     Train net output #0: mbox_loss = 14.786 (* 1 = 14.786 loss)
I0511 21:46:32.064534   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 1.01G, req 0.74M)	t: 0 0 5.56
I0511 21:46:32.289000   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 1.01G, req 0.74M)	t: 0 2.5 5.65
I0511 21:46:32.534826   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 1 	(avail 1.01G, req 0.74M)	t: 0 2.52 7.91
I0511 21:46:32.650166   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 1.01G, req 0.74M)	t: 0 0.87 2.28
I0511 21:46:32.823280   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.32G 64/1 6 4 5 	(avail 0.69G, req 0.32G)	t: 0 1.9 3.22
I0511 21:46:32.931272   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.32G 128/4 6 4 0 	(avail 0.69G, req 0.32G)	t: 0 0.38 1.03
I0511 21:46:33.075250   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.32G 128/1 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 1.42 1.49
I0511 21:46:33.155385   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.32G 256/4 6 4 5 	(avail 0.69G, req 0.32G)	t: 0 0.32 0.49
I0511 21:46:33.314446   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.32G 256/1 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 1.28 1.28
I0511 21:46:33.387312   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.32G 512/4 7 5 5 	(avail 0.69G, req 0.32G)	t: 0 0.26 0.29
I0511 21:46:33.495271   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.32G 128/1 1 1 0 	(avail 0.69G, req 0.32G)	t: 0 1.45 2.37
I0511 21:46:33.551244   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.32G 512/1 1 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.38 0.47
I0511 21:46:33.607254   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.32G 512/1 1 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.11 0.18
I0511 21:46:33.663237   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.32G 512/1 0 1 1 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.07
I0511 21:46:33.715263   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.32G 512/1 0 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.05 0.05
I0511 21:46:33.763232   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.32G 512/1 0 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.05 0.04
I0511 21:46:33.855271   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.32G 256/1 1 1 1 	(avail 0.69G, req 0.32G)	t: 0 0.73 2.05
I0511 21:46:33.951265   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.32G 256/1 1 1 3 	(avail 0.69G, req 0.32G)	t: 0 0.74 2.07
I0511 21:46:33.999212   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.11
I0511 21:46:34.047281   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.07 0.11
I0511 21:46:34.087225   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.04
I0511 21:46:34.131233   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.32G 256/1 0 1 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.03
I0511 21:46:34.171270   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.03 0.02
I0511 21:46:34.219235   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.267227   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.307215   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.355239   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:34.395215   380 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.32G 256/1 0 0 0 	(avail 0.69G, req 0.32G)	t: 0 0.02 0.02
I0511 21:46:35.128628   380 solver.cpp:360] Iteration 2 (3.22657 s), 0.1/1129.4ep, loss = 15.6573
I0511 21:46:35.128664   380 solver.cpp:378]     Train net output #0: mbox_loss = 14.3426 (* 1 = 14.3426 loss)
I0511 21:47:15.048736   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:47:53.209851   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:48:31.739748   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:49:10.025204   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:49:48.905861   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:50:07.818722   380 solver.cpp:354] Iteration 100 (0.460771 iter/s, 212.687s/98 iter), 5.6/1129.4ep, loss = 5.08009
I0511 21:50:07.819167   380 solver.cpp:378]     Train net output #0: mbox_loss = 5.69373 (* 1 = 5.69373 loss)
I0511 21:50:07.819371   380 sgd_solver.cpp:172] Iteration 100, lr = 0.00098015, m = 0.9, wd = 1e-05, gs = 1
I0511 21:50:43.786126   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:51:20.762719   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:52:04.629755   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:52:46.639966   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:53:30.403527   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:54:07.693262   380 solver.cpp:354] Iteration 200 (0.416889 iter/s, 239.872s/100 iter), 11.3/1129.4ep, loss = 4.06675
I0511 21:54:07.693459   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.85671 (* 1 = 3.85671 loss)
I0511 21:54:07.693507   380 sgd_solver.cpp:172] Iteration 200, lr = 0.000960596, m = 0.9, wd = 1e-05, gs = 1
I0511 21:54:13.241771   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:54:52.132915   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:55:37.447532   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:56:16.683847   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:57:02.713503   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:57:41.708932   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:58:01.006506   380 solver.cpp:354] Iteration 300 (0.428612 iter/s, 233.311s/100 iter), 16.9/1129.4ep, loss = 3.94508
I0511 21:58:01.006546   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.88616 (* 1 = 3.88616 loss)
I0511 21:58:01.006554   380 sgd_solver.cpp:172] Iteration 300, lr = 0.000941337, m = 0.9, wd = 1e-05, gs = 1
I0511 21:58:26.044611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:59:08.337473   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:59:45.354251   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:00:32.321157   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:05.956228   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:52.184914   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:01:57.573454   380 solver.cpp:354] Iteration 400 (0.422716 iter/s, 236.566s/100 iter), 22.6/1129.4ep, loss = 3.69972
I0511 22:01:57.573590   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.5853 (* 1 = 3.5853 loss)
I0511 22:01:57.573602   380 sgd_solver.cpp:172] Iteration 400, lr = 0.000922368, m = 0.9, wd = 1e-05, gs = 1
I0511 22:02:32.082953   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:03:14.627357   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:03:58.124053   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:04:38.828341   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:05:31.923715   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:02.066817   380 solver.cpp:637] Iteration 500, Testing net (#0)
I0511 22:06:08.480397   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:25.735750   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:06:25.989441   380 solver.cpp:749] class AP 1: 0.851463
I0511 22:06:26.010355   380 solver.cpp:749] class AP 2: 0.844034
I0511 22:06:26.013630   380 solver.cpp:749] class AP 3: 0.902044
I0511 22:06:26.013648   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.865847
I0511 22:06:26.013690   380 solver.cpp:284] Tests completed in 268.439s
I0511 22:06:26.698017   380 solver.cpp:354] Iteration 500 (0.372524 iter/s, 268.439s/100 iter), 28.2/1129.4ep, loss = 3.50596
I0511 22:06:26.698105   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.11099 (* 1 = 3.11099 loss)
I0511 22:06:26.698137   380 sgd_solver.cpp:172] Iteration 500, lr = 0.000903688, m = 0.9, wd = 1e-05, gs = 1
I0511 22:06:58.158004   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:07:35.389096   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:08:26.342031   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:07.359182   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:46.397260   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:09:59.481369   380 solver.cpp:354] Iteration 600 (0.469964 iter/s, 212.782s/100 iter), 33.9/1129.4ep, loss = 3.43911
I0511 22:09:59.481544   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.24994 (* 1 = 3.24994 loss)
I0511 22:09:59.481590   380 sgd_solver.cpp:172] Iteration 600, lr = 0.000885293, m = 0.9, wd = 1e-05, gs = 1
I0511 22:10:29.885285   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:11:11.987272   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:11:57.356365   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:12:37.101845   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:13:17.148389   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:14:02.032563   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:14:14.627190   380 solver.cpp:354] Iteration 700 (0.391934 iter/s, 255.145s/100 iter), 39.5/1129.4ep, loss = 3.36716
I0511 22:14:14.627347   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.26968 (* 1 = 3.26968 loss)
I0511 22:14:14.627391   380 sgd_solver.cpp:172] Iteration 700, lr = 0.00086718, m = 0.9, wd = 1e-05, gs = 1
I0511 22:14:37.594638   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:15:34.038246   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:16:11.919057   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:16:57.985312   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:17:38.787438   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:18:12.441474   380 solver.cpp:354] Iteration 800 (0.420496 iter/s, 237.814s/100 iter), 45.2/1129.4ep, loss = 3.20753
I0511 22:18:12.441648   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.62432 (* 1 = 3.62432 loss)
I0511 22:18:12.441720   380 sgd_solver.cpp:172] Iteration 800, lr = 0.000849347, m = 0.9, wd = 1e-05, gs = 1
I0511 22:18:19.131451   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:18:59.130355   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:19:38.248065   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:20:22.736457   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:20:59.781296   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:21:45.405323   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:22:04.609818   380 solver.cpp:354] Iteration 900 (0.430722 iter/s, 232.168s/100 iter), 50.8/1129.4ep, loss = 3.20883
I0511 22:22:04.610272   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.09849 (* 1 = 3.09849 loss)
I0511 22:22:04.610445   380 sgd_solver.cpp:172] Iteration 900, lr = 0.00083179, m = 0.9, wd = 1e-05, gs = 1
I0511 22:22:24.244216   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:23:14.587445   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:23:59.035549   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:24:37.702049   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:25:18.616324   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:25:59.149740   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:26:00.239434   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_1000.caffemodel
I0511 22:26:00.318750   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_1000.solverstate
I0511 22:26:00.352358   380 solver.cpp:637] Iteration 1000, Testing net (#0)
I0511 22:26:26.882102   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:26:27.479652   380 solver.cpp:749] class AP 1: 0.881073
I0511 22:26:27.533767   380 solver.cpp:749] class AP 2: 0.842335
I0511 22:26:27.539366   380 solver.cpp:749] class AP 3: 0.90211
I0511 22:26:27.539384   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.875172
I0511 22:26:27.539441   380 solver.cpp:284] Tests completed in 262.929s
I0511 22:26:28.131887   380 solver.cpp:354] Iteration 1000 (0.38033 iter/s, 262.929s/100 iter), 56.5/1129.4ep, loss = 3.18494
I0511 22:26:28.132262   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.06059 (* 1 = 3.06059 loss)
I0511 22:26:28.132462   380 sgd_solver.cpp:172] Iteration 1000, lr = 0.000814506, m = 0.9, wd = 1e-05, gs = 1
I0511 22:26:45.552937   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:27:26.108860   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:28:07.312378   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:28:57.275434   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:29:31.142695   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:30:00.924348   380 solver.cpp:354] Iteration 1100 (0.469942 iter/s, 212.792s/100 iter), 62.1/1129.4ep, loss = 3.14656
I0511 22:30:00.924497   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.01286 (* 1 = 3.01286 loss)
I0511 22:30:00.924542   380 sgd_solver.cpp:172] Iteration 1100, lr = 0.000797494, m = 0.9, wd = 1e-05, gs = 1
I0511 22:30:26.362645   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:31:05.960283   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:31:46.105351   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:32:30.511308   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:33:09.913506   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:33:55.848616   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:34:19.987020   380 solver.cpp:354] Iteration 1200 (0.386008 iter/s, 259.062s/100 iter), 67.8/1129.4ep, loss = 3.10649
I0511 22:34:19.987246   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.09124 (* 1 = 3.09124 loss)
I0511 22:34:19.987310   380 sgd_solver.cpp:172] Iteration 1200, lr = 0.000780749, m = 0.9, wd = 1e-05, gs = 1
I0511 22:34:32.029151   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:35:16.130746   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:35:57.764910   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:36:53.717509   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:37:33.048996   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:38:07.328758   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:38:09.071995   380 solver.cpp:354] Iteration 1300 (0.43652 iter/s, 229.085s/100 iter), 73.4/1129.4ep, loss = 3.04207
I0511 22:38:09.072032   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53374 (* 1 = 2.53374 loss)
I0511 22:38:09.072043   380 sgd_solver.cpp:172] Iteration 1300, lr = 0.000764269, m = 0.9, wd = 1e-05, gs = 1
I0511 22:38:51.081827   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:39:27.380702   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:40:15.551990   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:40:56.110198   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:41:44.151913   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:42:04.743002   380 solver.cpp:354] Iteration 1400 (0.424321 iter/s, 235.671s/100 iter), 79.1/1129.4ep, loss = 2.9028
I0511 22:42:04.743041   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.16925 (* 1 = 3.16925 loss)
I0511 22:42:04.743050   380 sgd_solver.cpp:172] Iteration 1400, lr = 0.000748052, m = 0.9, wd = 1e-05, gs = 1
I0511 22:42:23.253594   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:43:03.677249   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:43:48.909335   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:44:30.940543   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:11.500739   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:49.746834   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:45:57.984078   380 solver.cpp:637] Iteration 1500, Testing net (#0)
I0511 22:46:27.310767   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:46:27.983259   380 solver.cpp:749] class AP 1: 0.88929
I0511 22:46:27.994613   380 solver.cpp:749] class AP 2: 0.868738
I0511 22:46:27.998314   380 solver.cpp:749] class AP 3: 0.901085
I0511 22:46:27.998335   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.886371
I0511 22:46:27.998386   380 solver.cpp:284] Tests completed in 263.255s
I0511 22:46:28.861436   380 solver.cpp:354] Iteration 1500 (0.37986 iter/s, 263.255s/100 iter), 84.7/1129.4ep, loss = 2.88776
I0511 22:46:28.861608   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.7638 (* 1 = 2.7638 loss)
I0511 22:46:28.861662   380 sgd_solver.cpp:172] Iteration 1500, lr = 0.000732094, m = 0.9, wd = 1e-05, gs = 1
I0511 22:46:38.012153   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:47:21.871023   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:47:56.417906   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:48:41.931571   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:49:19.163460   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:49:59.534721   380 solver.cpp:354] Iteration 1600 (0.474671 iter/s, 210.672s/100 iter), 90.4/1129.4ep, loss = 2.79613
I0511 22:49:59.535276   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.83263 (* 1 = 2.83263 loss)
I0511 22:49:59.535456   380 sgd_solver.cpp:172] Iteration 1600, lr = 0.000716393, m = 0.9, wd = 1e-05, gs = 1
I0511 22:50:06.111376   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:50:40.529433   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:51:31.187630   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:52:13.043351   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:52:52.419611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:53:34.819511   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:53:57.594475   380 solver.cpp:354] Iteration 1700 (0.420065 iter/s, 238.058s/100 iter), 96/1129.4ep, loss = 2.84702
I0511 22:53:57.594514   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.999 (* 1 = 2.999 loss)
I0511 22:53:57.594524   380 sgd_solver.cpp:172] Iteration 1700, lr = 0.000700946, m = 0.9, wd = 1e-05, gs = 1
I0511 22:54:11.547266   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:54:53.910516   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:55:32.299279   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:56:15.066823   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:56:56.904923   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:57:34.897579   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:57:52.261819   380 solver.cpp:354] Iteration 1800 (0.426137 iter/s, 234.666s/100 iter), 101.6/1129.4ep, loss = 2.88403
I0511 22:57:52.262017   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.06196 (* 1 = 3.06196 loss)
I0511 22:57:52.262084   380 sgd_solver.cpp:172] Iteration 1800, lr = 0.00068575, m = 0.9, wd = 1e-05, gs = 1
I0511 22:58:31.483157   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:59:11.234608   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 22:59:47.568879   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:00:21.941752   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:01:00.834185   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:01:40.379721   380 solver.cpp:354] Iteration 1900 (0.438372 iter/s, 228.117s/100 iter), 107.3/1129.4ep, loss = 2.81459
I0511 23:01:40.379788   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60571 (* 1 = 2.60571 loss)
I0511 23:01:40.379798   380 sgd_solver.cpp:172] Iteration 1900, lr = 0.000670802, m = 0.9, wd = 1e-05, gs = 1
I0511 23:01:45.764611   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:02:23.773350   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:03:15.836768   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:03:52.831061   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:04:49.744917   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:05:25.983264   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:05:41.228611   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 23:05:41.286837   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 23:05:41.326206   380 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 23:06:10.085602   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:06:10.563458   380 solver.cpp:749] class AP 1: 0.897847
I0511 23:06:10.566098   380 solver.cpp:749] class AP 2: 0.880573
I0511 23:06:10.567783   380 solver.cpp:749] class AP 3: 0.901148
I0511 23:06:10.567796   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893189
I0511 23:06:10.567833   380 solver.cpp:284] Tests completed in 270.187s
I0511 23:06:11.180995   380 solver.cpp:354] Iteration 2000 (0.370114 iter/s, 270.187s/100 iter), 112.9/1129.4ep, loss = 2.90048
I0511 23:06:11.181115   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61702 (* 1 = 2.61702 loss)
I0511 23:06:11.181162   380 sgd_solver.cpp:172] Iteration 2000, lr = 0.0006561, m = 0.9, wd = 1e-05, gs = 1
I0511 23:06:11.182655   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.55 sparsity_achieved=0.389898 iter=2000
W0511 23:06:11.182711   380 net.cpp:2654] conv1a ni=3 no=32
W0511 23:06:12.070410   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.25
W0511 23:06:12.070803   380 net.cpp:2654] conv1b ni=32 no=32
W0511 23:06:12.661587   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.538628
W0511 23:06:12.661717   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0511 23:06:13.434237   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.548611
W0511 23:06:13.434377   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0511 23:06:15.373550   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.54069
W0511 23:06:15.373591   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0511 23:06:16.388242   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.548611
W0511 23:06:16.388270   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0511 23:06:17.769129   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.548611
W0511 23:06:17.769151   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0511 23:06:18.676594   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.549479
W0511 23:06:18.676614   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0511 23:06:19.744892   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.547092
W0511 23:06:19.744912   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0511 23:06:21.542587   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.546757
W0511 23:06:21.542609   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0511 23:06:22.659111   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.549274
W0511 23:06:22.659132   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0511 23:06:22.659137   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0511 23:06:22.659143   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0511 23:06:22.659149   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0511 23:06:22.659154   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0511 23:06:22.659160   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0511 23:06:22.659173   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659179   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0511 23:06:22.659201   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659209   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659222   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659229   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659236   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0511 23:06:22.659245   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0511 23:06:22.659250   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659257   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0511 23:06:22.659272   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0511 23:06:22.659278   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0511 23:06:22.659289   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0511 23:06:22.662211   380 solver.cpp:391] Sparsity after update:
I0511 23:06:22.663183   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0511 23:06:22.663192   380 net.cpp:2780] conv1a_param_0(0.25) 
I0511 23:06:22.663200   380 net.cpp:2780] conv1b_param_0(0.539) 
I0511 23:06:22.663218   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663223   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663242   380 net.cpp:2780] ctx_output1_param_0(0) 
I0511 23:06:22.663246   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663249   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663254   380 net.cpp:2780] ctx_output2_param_0(0) 
I0511 23:06:22.663259   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663262   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663269   380 net.cpp:2780] ctx_output3_param_0(0) 
I0511 23:06:22.663275   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663278   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663285   380 net.cpp:2780] ctx_output4_param_0(0) 
I0511 23:06:22.663290   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663295   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663300   380 net.cpp:2780] ctx_output5_param_0(0) 
I0511 23:06:22.663305   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0511 23:06:22.663309   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0511 23:06:22.663314   380 net.cpp:2780] ctx_output6_param_0(0) 
I0511 23:06:22.663319   380 net.cpp:2780] res2a_branch2a_param_0(0.549) 
I0511 23:06:22.663324   380 net.cpp:2780] res2a_branch2b_param_0(0.541) 
I0511 23:06:22.663329   380 net.cpp:2780] res3a_branch2a_param_0(0.549) 
I0511 23:06:22.663332   380 net.cpp:2780] res3a_branch2b_param_0(0.549) 
I0511 23:06:22.663336   380 net.cpp:2780] res4a_branch2a_param_0(0.549) 
I0511 23:06:22.663339   380 net.cpp:2780] res4a_branch2b_param_0(0.547) 
I0511 23:06:22.663347   380 net.cpp:2780] res5a_branch2a_param_0(0.547) 
I0511 23:06:22.663352   380 net.cpp:2780] res5a_branch2b_param_0(0.549) 
I0511 23:06:22.663355   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.28928e+06/3.10435e+06) 0.415
I0511 23:06:26.968063   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:07:08.982769   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:07:48.924650   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:08:32.735319   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:08.995749   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:48.425508   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:09:54.001981   380 solver.cpp:354] Iteration 2100 (0.448792 iter/s, 222.82s/100 iter), 118.6/1129.4ep, loss = 2.7656
I0511 23:09:54.002017   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61097 (* 1 = 2.61097 loss)
I0511 23:09:54.002027   380 sgd_solver.cpp:172] Iteration 2100, lr = 0.000641641, m = 0.9, wd = 1e-05, gs = 1
I0511 23:10:25.882692   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:11:07.183223   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:11:49.188369   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:12:26.681581   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:13:26.537501   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:13:56.643184   380 solver.cpp:354] Iteration 2200 (0.412132 iter/s, 242.64s/100 iter), 124.2/1129.4ep, loss = 2.86298
I0511 23:13:56.643621   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.95602 (* 1 = 2.95602 loss)
I0511 23:13:56.643819   380 sgd_solver.cpp:172] Iteration 2200, lr = 0.000627422, m = 0.9, wd = 1e-05, gs = 1
I0511 23:14:04.083290   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:14:51.854902   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:15:29.150980   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:16:12.861479   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:16:49.339495   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:17:27.793597   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:17:58.672216   380 solver.cpp:354] Iteration 2300 (0.413175 iter/s, 242.028s/100 iter), 129.9/1129.4ep, loss = 2.77348
I0511 23:17:58.672421   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.77719 (* 1 = 2.77719 loss)
I0511 23:17:58.672482   380 sgd_solver.cpp:172] Iteration 2300, lr = 0.000613441, m = 0.9, wd = 1e-05, gs = 1
I0511 23:18:13.328398   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:18:52.190073   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:19:45.094656   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:20:23.562669   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:21:10.713533   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:21:49.973322   380 solver.cpp:354] Iteration 2400 (0.432338 iter/s, 231.3s/100 iter), 135.5/1129.4ep, loss = 2.72122
I0511 23:21:49.973412   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47747 (* 1 = 2.47747 loss)
I0511 23:21:49.973438   380 sgd_solver.cpp:172] Iteration 2400, lr = 0.000599695, m = 0.9, wd = 1e-05, gs = 1
I0511 23:21:52.867935   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:22:28.632994   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:23:10.311792   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:23:49.549851   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:24:32.578976   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:25:12.752198   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:25:45.686859   380 solver.cpp:637] Iteration 2500, Testing net (#0)
I0511 23:25:53.278416   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:26:10.875247   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:26:11.730819   380 solver.cpp:749] class AP 1: 0.899247
I0511 23:26:11.732950   380 solver.cpp:749] class AP 2: 0.881324
I0511 23:26:11.733897   380 solver.cpp:749] class AP 3: 0.902666
I0511 23:26:11.733909   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894412
I0511 23:26:11.733942   380 solver.cpp:284] Tests completed in 261.762s
I0511 23:26:12.395884   380 solver.cpp:354] Iteration 2500 (0.382026 iter/s, 261.762s/100 iter), 141.2/1129.4ep, loss = 2.84025
I0511 23:26:12.395969   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.9878 (* 1 = 2.9878 loss)
I0511 23:26:12.396001   380 sgd_solver.cpp:172] Iteration 2500, lr = 0.000586182, m = 0.9, wd = 1e-05, gs = 1
I0511 23:26:45.407070   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:27:20.784235   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:28:10.744758   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:28:51.855731   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:29:27.997948   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:29:43.873634   380 solver.cpp:354] Iteration 2600 (0.472861 iter/s, 211.479s/100 iter), 146.8/1129.4ep, loss = 2.74006
I0511 23:29:43.873716   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.6806 (* 1 = 2.6806 loss)
I0511 23:29:43.873742   380 sgd_solver.cpp:172] Iteration 2600, lr = 0.000572898, m = 0.9, wd = 1e-05, gs = 1
I0511 23:30:06.048714   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:30:46.617630   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:31:28.477247   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:32:10.740487   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:02.929312   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:37.388928   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:33:45.903798   380 solver.cpp:354] Iteration 2700 (0.413171 iter/s, 242.031s/100 iter), 152.5/1129.4ep, loss = 2.64169
I0511 23:33:45.904040   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.68634 (* 1 = 2.68634 loss)
I0511 23:33:45.904109   380 sgd_solver.cpp:172] Iteration 2700, lr = 0.000559841, m = 0.9, wd = 1e-05, gs = 1
I0511 23:34:29.028403   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:35:08.894178   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:35:47.070574   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:36:26.629539   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:37:04.359081   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:37:42.269634   380 solver.cpp:354] Iteration 2800 (0.423072 iter/s, 236.366s/100 iter), 158.1/1129.4ep, loss = 2.65224
I0511 23:37:42.270206   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.36787 (* 1 = 2.36787 loss)
I0511 23:37:42.270313   380 sgd_solver.cpp:172] Iteration 2800, lr = 0.000547008, m = 0.9, wd = 1e-05, gs = 1
I0511 23:37:51.326196   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:38:33.341886   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:39:20.142961   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:39:59.192663   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:40:42.338551   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:41:28.958895   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:41:33.686410   380 solver.cpp:354] Iteration 2900 (0.432121 iter/s, 231.417s/100 iter), 163.8/1129.4ep, loss = 2.63049
I0511 23:41:33.686769   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.90685 (* 1 = 2.90685 loss)
I0511 23:41:33.686784   380 sgd_solver.cpp:172] Iteration 2900, lr = 0.000534398, m = 0.9, wd = 1e-05, gs = 1
I0511 23:42:05.466763   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:42:45.522639   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:43:22.156185   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:44:07.320704   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:44:44.338208   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:34.165438   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:34.394577   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_3000.caffemodel
I0511 23:45:34.418658   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_3000.solverstate
I0511 23:45:34.442899   380 solver.cpp:637] Iteration 3000, Testing net (#0)
I0511 23:45:58.098038   423 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:45:58.667392   380 solver.cpp:749] class AP 1: 0.888234
I0511 23:45:58.669116   380 solver.cpp:749] class AP 2: 0.881476
I0511 23:45:58.669879   380 solver.cpp:749] class AP 3: 0.903607
I0511 23:45:58.669893   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891106
I0511 23:45:58.669929   380 solver.cpp:284] Tests completed in 264.984s
I0511 23:45:59.380784   380 solver.cpp:354] Iteration 3000 (0.377382 iter/s, 264.984s/100 iter), 169.4/1129.4ep, loss = 2.7778
I0511 23:45:59.380868   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.73399 (* 1 = 2.73399 loss)
I0511 23:45:59.380894   380 sgd_solver.cpp:172] Iteration 3000, lr = 0.000522006, m = 0.9, wd = 1e-05, gs = 1
I0511 23:46:19.517340   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:46:53.087132   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:47:35.475322   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:48:12.615372   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:49:04.852743   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:49:24.120959   380 solver.cpp:354] Iteration 3100 (0.488424 iter/s, 204.74s/100 iter), 175.1/1129.4ep, loss = 2.64164
I0511 23:49:24.121253   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53339 (* 1 = 2.53339 loss)
I0511 23:49:24.121384   380 sgd_solver.cpp:172] Iteration 3100, lr = 0.000509832, m = 0.9, wd = 1e-05, gs = 1
I0511 23:49:43.378356   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:50:23.451146   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:51:11.076329   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:51:48.454010   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:52:34.068723   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:53:14.722208   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:53:23.689450   380 solver.cpp:354] Iteration 3200 (0.417417 iter/s, 239.568s/100 iter), 180.7/1129.4ep, loss = 2.67375
I0511 23:53:23.689833   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.36061 (* 1 = 2.36061 loss)
I0511 23:53:23.690014   380 sgd_solver.cpp:172] Iteration 3200, lr = 0.000497871, m = 0.9, wd = 1e-05, gs = 1
I0511 23:53:51.517578   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:54:31.763139   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:55:11.999732   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:55:57.817662   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:56:33.243101   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:57:17.805027   380 solver.cpp:354] Iteration 3300 (0.42714 iter/s, 234.115s/100 iter), 186.4/1129.4ep, loss = 2.66382
I0511 23:57:17.805342   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.14024 (* 1 = 3.14024 loss)
I0511 23:57:17.805407   380 sgd_solver.cpp:172] Iteration 3300, lr = 0.000486123, m = 0.9, wd = 1e-05, gs = 1
I0511 23:57:23.981336   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:58:01.414300   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:58:37.225128   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:59:20.610926   386 data_reader.cpp:320] Restarting data pre-fetching
I0511 23:59:58.795346   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:00:44.572506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:01:06.861644   380 solver.cpp:354] Iteration 3400 (0.436575 iter/s, 229.056s/100 iter), 192/1129.4ep, loss = 2.56589
I0512 00:01:06.862188   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46831 (* 1 = 2.46831 loss)
I0512 00:01:06.862443   380 sgd_solver.cpp:172] Iteration 3400, lr = 0.000474583, m = 0.9, wd = 1e-05, gs = 1
I0512 00:01:21.509789   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:02:09.187813   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:02:45.658895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:03:30.086712   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:04:10.998262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:04:50.490295   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:05:02.011380   380 solver.cpp:637] Iteration 3500, Testing net (#0)
I0512 00:05:28.300626   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:05:28.933239   380 solver.cpp:749] class AP 1: 0.88368
I0512 00:05:28.934778   380 solver.cpp:749] class AP 2: 0.872317
I0512 00:05:28.935155   380 solver.cpp:749] class AP 3: 0.903052
I0512 00:05:28.935163   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.886349
I0512 00:05:28.935191   380 solver.cpp:284] Tests completed in 262.073s
I0512 00:05:29.544144   380 solver.cpp:354] Iteration 3500 (0.381573 iter/s, 262.073s/100 iter), 197.6/1129.4ep, loss = 2.6441
I0512 00:05:29.544229   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.82891 (* 1 = 2.82891 loss)
I0512 00:05:29.544260   380 sgd_solver.cpp:172] Iteration 3500, lr = 0.00046325, m = 0.9, wd = 1e-05, gs = 1
I0512 00:05:41.379496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:06:19.428290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:06:57.669991   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:07:40.457116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:08:26.791290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:09:03.554411   380 solver.cpp:354] Iteration 3600 (0.467268 iter/s, 214.01s/100 iter), 203.3/1129.4ep, loss = 2.56633
I0512 00:09:03.554484   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.93594 (* 1 = 2.93594 loss)
I0512 00:09:03.554495   380 sgd_solver.cpp:172] Iteration 3600, lr = 0.000452122, m = 0.9, wd = 1e-05, gs = 1
I0512 00:09:09.457468   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:09:43.959610   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:10:44.147754   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:11:20.719260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:12:01.509384   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:12:42.186445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:13:10.385424   380 solver.cpp:354] Iteration 3700 (0.405136 iter/s, 246.831s/100 iter), 208.9/1129.4ep, loss = 2.52452
I0512 00:13:10.385459   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.65433 (* 1 = 2.65433 loss)
I0512 00:13:10.385468   380 sgd_solver.cpp:172] Iteration 3700, lr = 0.000441195, m = 0.9, wd = 1e-05, gs = 1
I0512 00:13:18.944260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:14:03.477363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:14:40.823678   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:15:26.753348   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:16:02.798437   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:17:04.629142   380 solver.cpp:354] Iteration 3800 (0.426907 iter/s, 234.243s/100 iter), 214.6/1129.4ep, loss = 2.51966
I0512 00:17:04.629246   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.03686 (* 1 = 3.03686 loss)
I0512 00:17:04.629273   380 sgd_solver.cpp:172] Iteration 3800, lr = 0.000430467, m = 0.9, wd = 1e-05, gs = 1
I0512 00:17:05.309358   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:17:47.298697   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:18:24.579331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:19:01.347045   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:19:41.815564   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:20:22.512291   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:20:58.006129   380 solver.cpp:354] Iteration 3900 (0.428492 iter/s, 233.377s/100 iter), 220.2/1129.4ep, loss = 2.57888
I0512 00:20:58.006188   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58013 (* 1 = 2.58013 loss)
I0512 00:20:58.006197   380 sgd_solver.cpp:172] Iteration 3900, lr = 0.000419936, m = 0.9, wd = 1e-05, gs = 1
I0512 00:20:59.617907   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:21:44.414579   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:22:25.717340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:23:13.284507   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:23:56.977728   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:24:36.236737   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:24:47.099014   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0512 00:24:47.154350   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0512 00:24:47.220252   380 solver.cpp:637] Iteration 4000, Testing net (#0)
I0512 00:25:17.687211   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:25:18.384301   380 solver.cpp:749] class AP 1: 0.897955
I0512 00:25:18.385728   380 solver.cpp:749] class AP 2: 0.882179
I0512 00:25:18.386346   380 solver.cpp:749] class AP 3: 0.900449
I0512 00:25:18.386356   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893528
I0512 00:25:18.386390   380 solver.cpp:284] Tests completed in 260.38s
I0512 00:25:18.999925   380 solver.cpp:354] Iteration 4000 (0.384054 iter/s, 260.38s/100 iter), 225.9/1129.4ep, loss = 2.54187
I0512 00:25:19.000003   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43002 (* 1 = 2.43002 loss)
I0512 00:25:19.000034   380 sgd_solver.cpp:172] Iteration 4000, lr = 0.0004096, m = 0.9, wd = 1e-05, gs = 1
I0512 00:25:19.001554   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.6 sparsity_achieved=0.415315 iter=4000
W0512 00:25:19.001595   380 net.cpp:2654] conv1a ni=3 no=32
W0512 00:25:19.999111   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.267917
W0512 00:25:19.999209   380 net.cpp:2654] conv1b ni=32 no=32
W0512 00:25:21.056505   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.590712
W0512 00:25:21.056607   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 00:25:22.421207   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.597222
W0512 00:25:22.421568   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 00:25:24.674854   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.580187
W0512 00:25:24.674901   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 00:25:26.327927   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.598958
W0512 00:25:26.327955   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 00:25:28.556594   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.597222
W0512 00:25:28.556618   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 00:25:29.489646   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.599826
W0512 00:25:29.489667   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 00:25:30.545923   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.597243
W0512 00:25:30.545943   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 00:25:32.338028   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.596279
W0512 00:25:32.338049   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 00:25:33.474155   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.599789
W0512 00:25:33.474177   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 00:25:33.474185   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 00:25:33.474211   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 00:25:33.474217   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 00:25:33.474229   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 00:25:33.474236   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 00:25:33.474251   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474263   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 00:25:33.474275   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474287   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474299   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474313   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474324   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 00:25:33.474335   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 00:25:33.474349   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474359   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 00:25:33.474365   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 00:25:33.474373   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 00:25:33.474387   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 00:25:33.477280   380 solver.cpp:391] Sparsity after update:
I0512 00:25:33.478197   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 00:25:33.478204   380 net.cpp:2780] conv1a_param_0(0.268) 
I0512 00:25:33.478214   380 net.cpp:2780] conv1b_param_0(0.591) 
I0512 00:25:33.478235   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478241   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478265   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 00:25:33.478269   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478274   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478281   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 00:25:33.478286   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478291   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478296   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 00:25:33.478302   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478307   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478312   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 00:25:33.478317   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478320   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478327   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 00:25:33.478330   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 00:25:33.478335   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 00:25:33.478343   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 00:25:33.478345   380 net.cpp:2780] res2a_branch2a_param_0(0.597) 
I0512 00:25:33.478349   380 net.cpp:2780] res2a_branch2b_param_0(0.58) 
I0512 00:25:33.478353   380 net.cpp:2780] res3a_branch2a_param_0(0.599) 
I0512 00:25:33.478355   380 net.cpp:2780] res3a_branch2b_param_0(0.597) 
I0512 00:25:33.478360   380 net.cpp:2780] res4a_branch2a_param_0(0.6) 
I0512 00:25:33.478365   380 net.cpp:2780] res4a_branch2b_param_0(0.597) 
I0512 00:25:33.478370   380 net.cpp:2780] res5a_branch2a_param_0(0.596) 
I0512 00:25:33.478377   380 net.cpp:2780] res5a_branch2b_param_0(0.6) 
I0512 00:25:33.478381   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.40667e+06/3.10435e+06) 0.453
I0512 00:25:41.890031   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:26:19.934027   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:27:00.383478   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:27:39.531344   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:15.345149   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:56.722363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:28:59.517132   380 solver.cpp:354] Iteration 4100 (0.45348 iter/s, 220.517s/100 iter), 231.5/1129.4ep, loss = 2.5599
I0512 00:28:59.517355   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.5621 (* 1 = 2.5621 loss)
I0512 00:28:59.517426   380 sgd_solver.cpp:172] Iteration 4100, lr = 0.000399456, m = 0.9, wd = 1e-05, gs = 1
I0512 00:29:35.432925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:30:22.431565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:30:59.449494   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:31:59.734853   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:32:36.922135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:33:08.756563   380 solver.cpp:354] Iteration 4200 (0.401221 iter/s, 249.239s/100 iter), 237.2/1129.4ep, loss = 2.58784
I0512 00:33:08.757025   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.50377 (* 1 = 2.50377 loss)
I0512 00:33:08.757138   380 sgd_solver.cpp:172] Iteration 4200, lr = 0.000389501, m = 0.9, wd = 1e-05, gs = 1
I0512 00:33:15.244516   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:33:53.878259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:34:29.457478   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:35:45.235251   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:36:51.147622   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:38:06.437019   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:38:49.161453   380 solver.cpp:354] Iteration 4300 (0.293768 iter/s, 340.405s/100 iter), 242.8/1129.4ep, loss = 2.64839
I0512 00:38:49.161515   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.20717 (* 1 = 3.20717 loss)
I0512 00:38:49.161525   380 sgd_solver.cpp:172] Iteration 4300, lr = 0.000379733, m = 0.9, wd = 1e-05, gs = 1
I0512 00:39:08.013336   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:40:46.569418   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:41:52.466670   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:42:58.383314   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:43:59.158247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:45:03.231246   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:45:06.409447   380 solver.cpp:354] Iteration 4400 (0.265078 iter/s, 377.248s/100 iter), 248.5/1129.4ep, loss = 2.42696
I0512 00:45:06.410135   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71346 (* 1 = 2.71346 loss)
I0512 00:45:06.410580   380 sgd_solver.cpp:172] Iteration 4400, lr = 0.000370151, m = 0.9, wd = 1e-05, gs = 1
I0512 00:46:11.114528   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:47:15.570771   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:48:13.562254   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:49:20.250169   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:50:28.279299   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:51:22.421309   380 solver.cpp:637] Iteration 4500, Testing net (#0)
I0512 00:52:09.145686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:52:11.678988   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:52:12.886446   380 solver.cpp:749] class AP 1: 0.901306
I0512 00:52:12.888061   380 solver.cpp:749] class AP 2: 0.887872
I0512 00:52:12.888891   380 solver.cpp:749] class AP 3: 0.900151
I0512 00:52:12.888979   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896443
I0512 00:52:12.889042   380 solver.cpp:284] Tests completed in 426.479s
I0512 00:52:13.905397   380 solver.cpp:354] Iteration 4500 (0.234478 iter/s, 426.479s/100 iter), 254.1/1129.4ep, loss = 2.47644
I0512 00:52:13.905475   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.28363 (* 1 = 2.28363 loss)
I0512 00:52:13.905989   380 sgd_solver.cpp:172] Iteration 4500, lr = 0.00036075, m = 0.9, wd = 1e-05, gs = 1
I0512 00:53:06.347507   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:54:28.477331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:55:34.395138   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:56:34.613667   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:57:43.642853   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:58:05.879189   380 solver.cpp:354] Iteration 4600 (0.284112 iter/s, 351.974s/100 iter), 259.8/1129.4ep, loss = 2.48582
I0512 00:58:05.880385   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53226 (* 1 = 2.53226 loss)
I0512 00:58:05.881044   380 sgd_solver.cpp:172] Iteration 4600, lr = 0.00035153, m = 0.9, wd = 1e-05, gs = 1
I0512 00:58:50.233384   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 00:59:58.286686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:01:04.093536   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:02:33.850375   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:03:43.130002   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:04:57.689337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:05:00.288084   380 solver.cpp:354] Iteration 4700 (0.241308 iter/s, 414.409s/100 iter), 265.4/1129.4ep, loss = 2.6142
I0512 01:05:00.288306   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.74339 (* 1 = 2.74339 loss)
I0512 01:05:00.288378   380 sgd_solver.cpp:172] Iteration 4700, lr = 0.000342488, m = 0.9, wd = 1e-05, gs = 1
I0512 01:06:17.668298   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:07:17.538868   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:08:32.459085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:09:35.858275   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:10:41.118965   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:11:32.169776   380 solver.cpp:354] Iteration 4800 (0.25518 iter/s, 391.881s/100 iter), 271.1/1129.4ep, loss = 2.37495
I0512 01:11:32.170899   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0665 (* 1 = 2.0665 loss)
I0512 01:11:32.171406   380 sgd_solver.cpp:172] Iteration 4800, lr = 0.000333622, m = 0.9, wd = 1e-05, gs = 1
I0512 01:11:48.141237   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:13:04.379415   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:14:17.595906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:15:19.940905   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:16:50.792737   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:17:57.526757   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:18:02.896693   380 solver.cpp:354] Iteration 4900 (0.255934 iter/s, 390.726s/100 iter), 276.7/1129.4ep, loss = 2.47929
I0512 01:18:02.897406   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71888 (* 1 = 2.71888 loss)
I0512 01:18:02.897801   380 sgd_solver.cpp:172] Iteration 4900, lr = 0.000324929, m = 0.9, wd = 1e-05, gs = 1
I0512 01:19:00.090277   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:20:11.640247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:21:12.851711   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:22:18.052014   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:23:17.748912   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:24:24.445372   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_5000.caffemodel
I0512 01:24:24.561967   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_5000.solverstate
I0512 01:24:24.635025   380 solver.cpp:637] Iteration 5000, Testing net (#0)
I0512 01:24:32.267335   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:25:13.569141   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:25:15.028043   380 solver.cpp:749] class AP 1: 0.897915
I0512 01:25:15.029959   380 solver.cpp:749] class AP 2: 0.877443
I0512 01:25:15.030586   380 solver.cpp:749] class AP 3: 0.900239
I0512 01:25:15.030632   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891866
I0512 01:25:15.030709   380 solver.cpp:284] Tests completed in 432.133s
I0512 01:25:15.869117   380 solver.cpp:354] Iteration 5000 (0.23141 iter/s, 432.133s/100 iter), 282.4/1129.4ep, loss = 2.42633
I0512 01:25:15.870133   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46512 (* 1 = 2.46512 loss)
I0512 01:25:15.870707   380 sgd_solver.cpp:172] Iteration 5000, lr = 0.000316406, m = 0.9, wd = 1e-05, gs = 1
I0512 01:25:46.670099   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:27:08.021355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:28:21.935257   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:29:23.425238   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:30:21.727969   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:30:46.149371   380 solver.cpp:354] Iteration 5100 (0.302774 iter/s, 330.28s/100 iter), 288/1129.4ep, loss = 2.59005
I0512 01:30:46.149914   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.67602 (* 1 = 2.67602 loss)
I0512 01:30:46.150158   380 sgd_solver.cpp:172] Iteration 5100, lr = 0.000308053, m = 0.9, wd = 1e-05, gs = 1
I0512 01:31:04.598172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:31:44.917948   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:32:24.953783   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:33:15.006157   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:00.359247   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:37.900871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:34:48.811710   380 solver.cpp:354] Iteration 5200 (0.412096 iter/s, 242.662s/100 iter), 293.6/1129.4ep, loss = 2.47679
I0512 01:34:48.811821   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72997 (* 1 = 2.72997 loss)
I0512 01:34:48.811863   380 sgd_solver.cpp:172] Iteration 5200, lr = 0.000299866, m = 0.9, wd = 1e-05, gs = 1
I0512 01:35:22.147125   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:35:59.396337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:36:52.705267   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:37:28.802376   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:38:06.915724   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:38:44.870774   380 solver.cpp:354] Iteration 5300 (0.423623 iter/s, 236.059s/100 iter), 299.3/1129.4ep, loss = 2.4971
I0512 01:38:44.871619   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47255 (* 1 = 2.47255 loss)
I0512 01:38:44.871984   380 sgd_solver.cpp:172] Iteration 5300, lr = 0.000291843, m = 0.9, wd = 1e-05, gs = 1
I0512 01:38:45.288408   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:39:23.111728   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:40:13.466006   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:40:54.115346   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:41:38.392027   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:42:18.859532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:42:49.373077   380 solver.cpp:354] Iteration 5400 (0.408996 iter/s, 244.501s/100 iter), 304.9/1129.4ep, loss = 2.40164
I0512 01:42:49.373296   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18969 (* 1 = 2.18969 loss)
I0512 01:42:49.373342   380 sgd_solver.cpp:172] Iteration 5400, lr = 0.000283982, m = 0.9, wd = 1e-05, gs = 1
I0512 01:43:12.293882   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:43:56.938295   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:44:34.046520   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:45:13.077294   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:45:50.584296   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:46:42.368672   380 solver.cpp:637] Iteration 5500, Testing net (#0)
I0512 01:46:45.264559   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:47:11.358120   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:47:12.487283   380 solver.cpp:749] class AP 1: 0.895555
I0512 01:47:12.487982   380 solver.cpp:749] class AP 2: 0.875854
I0512 01:47:12.488297   380 solver.cpp:749] class AP 3: 0.902538
I0512 01:47:12.488303   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891316
I0512 01:47:12.488332   380 solver.cpp:284] Tests completed in 263.114s
I0512 01:47:13.092278   380 solver.cpp:354] Iteration 5500 (0.380063 iter/s, 263.114s/100 iter), 310.6/1129.4ep, loss = 2.44502
I0512 01:47:13.093457   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.5611 (* 1 = 2.5611 loss)
I0512 01:47:13.093531   380 sgd_solver.cpp:172] Iteration 5500, lr = 0.000276282, m = 0.9, wd = 1e-05, gs = 1
I0512 01:47:26.430277   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:48:07.747608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:48:49.451440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:49:26.062801   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:50:23.291360   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:51:02.309056   380 solver.cpp:354] Iteration 5600 (0.43627 iter/s, 229.216s/100 iter), 316.2/1129.4ep, loss = 2.4636
I0512 01:51:02.309507   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47253 (* 1 = 2.47253 loss)
I0512 01:51:02.309653   380 sgd_solver.cpp:172] Iteration 5600, lr = 0.000268739, m = 0.9, wd = 1e-05, gs = 1
I0512 01:51:07.505848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:52:05.297349   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:52:40.907613   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:53:20.685048   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:53:58.284906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:54:39.758596   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:55:10.406332   380 solver.cpp:354] Iteration 5700 (0.403069 iter/s, 248.097s/100 iter), 321.9/1129.4ep, loss = 2.35871
I0512 01:55:10.406992   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46216 (* 1 = 2.46216 loss)
I0512 01:55:10.407331   380 sgd_solver.cpp:172] Iteration 5700, lr = 0.000261351, m = 0.9, wd = 1e-05, gs = 1
I0512 01:55:24.810925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:56:05.006271   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:56:49.403934   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:57:30.045353   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:58:35.759140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:59:14.579017   380 solver.cpp:354] Iteration 5800 (0.409547 iter/s, 244.172s/100 iter), 327.5/1129.4ep, loss = 2.36464
I0512 01:59:14.579270   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35028 (* 1 = 2.35028 loss)
I0512 01:59:14.579339   380 sgd_solver.cpp:172] Iteration 5800, lr = 0.000254117, m = 0.9, wd = 1e-05, gs = 1
I0512 01:59:17.267081   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 01:59:55.256178   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:00:31.821322   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:01:07.483500   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:01:53.408804   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:02:34.153854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:03:07.722275   380 solver.cpp:354] Iteration 5900 (0.428922 iter/s, 233.143s/100 iter), 333.2/1129.4ep, loss = 2.51474
I0512 02:03:07.722615   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46806 (* 1 = 2.46806 loss)
I0512 02:03:07.722698   380 sgd_solver.cpp:172] Iteration 5900, lr = 0.000247034, m = 0.9, wd = 1e-05, gs = 1
I0512 02:03:14.772049   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:04:01.791177   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:04:39.074381   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:05:34.150161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:12.605933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:53.141966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:06:59.611079   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0512 02:06:59.651274   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0512 02:06:59.691298   380 solver.cpp:637] Iteration 6000, Testing net (#0)
I0512 02:07:29.223470   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:07:30.474630   380 solver.cpp:749] class AP 1: 0.898383
I0512 02:07:30.475436   380 solver.cpp:749] class AP 2: 0.883382
I0512 02:07:30.475797   380 solver.cpp:749] class AP 3: 0.901935
I0512 02:07:30.475805   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894566
I0512 02:07:30.475832   380 solver.cpp:284] Tests completed in 262.753s
I0512 02:07:31.042992   380 solver.cpp:354] Iteration 6000 (0.380586 iter/s, 262.753s/100 iter), 338.8/1129.4ep, loss = 2.40074
I0512 02:07:31.043082   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25514 (* 1 = 2.25514 loss)
I0512 02:07:31.043117   380 sgd_solver.cpp:172] Iteration 6000, lr = 0.0002401, m = 0.9, wd = 1e-05, gs = 1
I0512 02:07:31.044647   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.65 sparsity_achieved=0.453127 iter=6000
W0512 02:07:31.044697   380 net.cpp:2654] conv1a ni=3 no=32
W0512 02:07:32.137281   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.285
W0512 02:07:32.137385   380 net.cpp:2654] conv1b ni=32 no=32
W0512 02:07:33.770866   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.624132
W0512 02:07:33.771011   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 02:07:35.376111   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.648763
W0512 02:07:35.376155   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 02:07:37.960913   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.610135
W0512 02:07:37.960958   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 02:07:40.682441   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.649306
W0512 02:07:40.682487   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 02:07:44.198565   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.648953
W0512 02:07:44.198611   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 02:07:45.186856   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.649306
W0512 02:07:45.186897   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 02:07:46.380535   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.647393
W0512 02:07:46.380578   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 02:07:48.179569   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.645804
W0512 02:07:48.179612   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 02:07:49.414454   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.649255
W0512 02:07:49.414499   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 02:07:49.414506   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 02:07:49.414511   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 02:07:49.414517   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 02:07:49.414523   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 02:07:49.414532   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 02:07:49.414539   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414549   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 02:07:49.414557   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414564   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414574   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414583   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414592   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 02:07:49.414598   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 02:07:49.414608   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414616   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 02:07:49.414624   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 02:07:49.414633   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 02:07:49.414645   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 02:07:49.417568   380 solver.cpp:391] Sparsity after update:
I0512 02:07:49.418488   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 02:07:49.418496   380 net.cpp:2780] conv1a_param_0(0.285) 
I0512 02:07:49.418504   380 net.cpp:2780] conv1b_param_0(0.624) 
I0512 02:07:49.418524   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418529   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418531   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 02:07:49.418535   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418542   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418547   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 02:07:49.418552   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418558   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418563   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 02:07:49.418568   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418579   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418584   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 02:07:49.418589   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418594   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418599   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 02:07:49.418629   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 02:07:49.418639   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 02:07:49.418645   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 02:07:49.418649   380 net.cpp:2780] res2a_branch2a_param_0(0.649) 
I0512 02:07:49.418655   380 net.cpp:2780] res2a_branch2b_param_0(0.61) 
I0512 02:07:49.418660   380 net.cpp:2780] res3a_branch2a_param_0(0.649) 
I0512 02:07:49.418664   380 net.cpp:2780] res3a_branch2b_param_0(0.649) 
I0512 02:07:49.418669   380 net.cpp:2780] res4a_branch2a_param_0(0.649) 
I0512 02:07:49.418674   380 net.cpp:2780] res4a_branch2b_param_0(0.647) 
I0512 02:07:49.418682   380 net.cpp:2780] res5a_branch2a_param_0(0.646) 
I0512 02:07:49.418687   380 net.cpp:2780] res5a_branch2b_param_0(0.649) 
I0512 02:07:49.418691   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.52322e+06/3.10435e+06) 0.491
I0512 02:07:57.337417   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:08:35.339231   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:09:16.781379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:09:57.599200   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:10:40.014683   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:11:18.504251   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:11:21.065613   380 solver.cpp:354] Iteration 6100 (0.434741 iter/s, 230.022s/100 iter), 344.5/1129.4ep, loss = 2.37823
I0512 02:11:21.065686   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29026 (* 1 = 2.29026 loss)
I0512 02:11:21.065716   380 sgd_solver.cpp:172] Iteration 6100, lr = 0.000233313, m = 0.9, wd = 1e-05, gs = 1
I0512 02:12:00.484292   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:12:41.341153   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:13:31.993492   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:14:20.075188   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:15:00.228516   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:15:31.409224   380 solver.cpp:354] Iteration 6200 (0.39945 iter/s, 250.344s/100 iter), 350.1/1129.4ep, loss = 2.38103
I0512 02:15:31.409317   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0734 (* 1 = 2.0734 loss)
I0512 02:15:31.409343   380 sgd_solver.cpp:172] Iteration 6200, lr = 0.000226671, m = 0.9, wd = 1e-05, gs = 1
I0512 02:15:40.641471   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:16:17.975491   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:17:06.512168   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:17:46.844496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:18:25.919242   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:19:03.070468   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:19:19.619683   380 solver.cpp:354] Iteration 6300 (0.43819 iter/s, 228.211s/100 iter), 355.8/1129.4ep, loss = 2.44095
I0512 02:19:19.620252   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47413 (* 1 = 2.47413 loss)
I0512 02:19:19.620535   380 sgd_solver.cpp:172] Iteration 6300, lr = 0.000220172, m = 0.9, wd = 1e-05, gs = 1
I0512 02:19:42.132400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:20:31.852154   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:21:12.622151   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:21:53.386695   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:22:32.827358   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:23:17.012738   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:23:19.670917   380 solver.cpp:354] Iteration 6400 (0.416577 iter/s, 240.052s/100 iter), 361.4/1129.4ep, loss = 2.54247
I0512 02:23:19.671205   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.53932 (* 1 = 2.53932 loss)
I0512 02:23:19.671219   380 sgd_solver.cpp:172] Iteration 6400, lr = 0.000213814, m = 0.9, wd = 1e-05, gs = 1
I0512 02:24:07.153723   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:24:46.166132   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:25:28.350947   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:26:04.832834   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:26:52.673930   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:16.427119   380 solver.cpp:637] Iteration 6500, Testing net (#0)
I0512 02:27:32.325964   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:41.824787   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:27:42.891902   380 solver.cpp:749] class AP 1: 0.899231
I0512 02:27:42.892781   380 solver.cpp:749] class AP 2: 0.876477
I0512 02:27:42.893106   380 solver.cpp:749] class AP 3: 0.900726
I0512 02:27:42.893113   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892145
I0512 02:27:42.893142   380 solver.cpp:284] Tests completed in 263.223s
I0512 02:27:43.521646   380 solver.cpp:354] Iteration 6500 (0.379907 iter/s, 263.223s/100 iter), 367.1/1129.4ep, loss = 2.38855
I0512 02:27:43.521730   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.23517 (* 1 = 2.23517 loss)
I0512 02:27:43.521760   380 sgd_solver.cpp:172] Iteration 6500, lr = 0.000207594, m = 0.9, wd = 1e-05, gs = 1
I0512 02:28:20.457262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:29:09.182498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:29:44.098693   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:30:26.720405   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:31:03.733336   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:31:13.881167   380 solver.cpp:354] Iteration 6600 (0.475376 iter/s, 210.36s/100 iter), 372.7/1129.4ep, loss = 2.47914
I0512 02:31:13.881321   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.06515 (* 1 = 2.06515 loss)
I0512 02:31:13.881364   380 sgd_solver.cpp:172] Iteration 6600, lr = 0.000201511, m = 0.9, wd = 1e-05, gs = 1
I0512 02:31:47.213681   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:32:26.804457   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:33:05.777323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:33:48.916040   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:34:25.905556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:35:08.179304   380 solver.cpp:354] Iteration 6700 (0.426807 iter/s, 234.298s/100 iter), 378.4/1129.4ep, loss = 2.54409
I0512 02:35:08.179548   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.89707 (* 1 = 2.89707 loss)
I0512 02:35:08.179615   380 sgd_solver.cpp:172] Iteration 6700, lr = 0.000195563, m = 0.9, wd = 1e-05, gs = 1
I0512 02:35:13.747311   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:35:56.936542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:36:38.242431   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:37:19.163228   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:37:55.278175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:38:40.550518   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:39:02.777071   380 solver.cpp:354] Iteration 6800 (0.426262 iter/s, 234.598s/100 iter), 384/1129.4ep, loss = 2.4659
I0512 02:39:02.777297   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55173 (* 1 = 2.55173 loss)
I0512 02:39:02.777372   380 sgd_solver.cpp:172] Iteration 6800, lr = 0.000189747, m = 0.9, wd = 1e-05, gs = 1
I0512 02:39:16.613379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:40:05.811794   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:40:43.814064   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:41:31.436228   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:42:14.553663   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:42:50.486626   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:43:02.620867   380 solver.cpp:354] Iteration 6900 (0.416938 iter/s, 239.844s/100 iter), 389.6/1129.4ep, loss = 2.30128
I0512 02:43:02.621024   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.21489 (* 1 = 2.21489 loss)
I0512 02:43:02.621069   380 sgd_solver.cpp:172] Iteration 6900, lr = 0.000184062, m = 0.9, wd = 1e-05, gs = 1
I0512 02:43:37.003266   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:44:13.428620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:45:02.111227   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:45:39.607476   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:46:19.494508   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:46:56.615309   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_7000.caffemodel
I0512 02:46:56.647401   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_7000.solverstate
I0512 02:46:56.711889   380 solver.cpp:637] Iteration 7000, Testing net (#0)
I0512 02:47:03.149209   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:47:26.042374   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:47:27.137480   380 solver.cpp:749] class AP 1: 0.897499
I0512 02:47:27.138204   380 solver.cpp:749] class AP 2: 0.884294
I0512 02:47:27.138494   380 solver.cpp:749] class AP 3: 0.901668
I0512 02:47:27.138504   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894487
I0512 02:47:27.138535   380 solver.cpp:284] Tests completed in 264.517s
I0512 02:47:27.695593   380 solver.cpp:354] Iteration 7000 (0.378047 iter/s, 264.517s/100 iter), 395.3/1129.4ep, loss = 2.31362
I0512 02:47:27.695677   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.21658 (* 1 = 2.21658 loss)
I0512 02:47:27.695708   380 sgd_solver.cpp:172] Iteration 7000, lr = 0.000178506, m = 0.9, wd = 1e-05, gs = 1
I0512 02:47:46.806356   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:48:34.450616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:49:09.251739   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:01.704924   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:40.268707   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:50:55.102490   380 solver.cpp:354] Iteration 7100 (0.482144 iter/s, 207.407s/100 iter), 400.9/1129.4ep, loss = 2.39177
I0512 02:50:55.102576   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.2713 (* 1 = 2.2713 loss)
I0512 02:50:55.102607   380 sgd_solver.cpp:172] Iteration 7100, lr = 0.000173077, m = 0.9, wd = 1e-05, gs = 1
I0512 02:51:24.675582   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:52:05.977023   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:52:47.077607   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:53:29.852808   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:54:07.301862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:54:51.198400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:55:01.143505   380 solver.cpp:354] Iteration 7200 (0.406437 iter/s, 246.041s/100 iter), 406.6/1129.4ep, loss = 2.45712
I0512 02:55:01.144268   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3552 (* 1 = 2.3552 loss)
I0512 02:55:01.144668   380 sgd_solver.cpp:172] Iteration 7200, lr = 0.000167772, m = 0.9, wd = 1e-05, gs = 1
I0512 02:55:27.655175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:56:15.547619   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:57:00.134402   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:57:37.289343   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:58:23.646422   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:58:56.136005   380 solver.cpp:354] Iteration 7300 (0.425546 iter/s, 234.992s/100 iter), 412.2/1129.4ep, loss = 2.34368
I0512 02:58:56.136322   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35763 (* 1 = 2.35763 loss)
I0512 02:58:56.136392   380 sgd_solver.cpp:172] Iteration 7300, lr = 0.00016259, m = 0.9, wd = 1e-05, gs = 1
I0512 02:59:03.511423   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 02:59:47.841972   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:00:28.883803   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:01:07.289131   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:01:52.231138   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:02:27.931105   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:02:55.459163   380 solver.cpp:354] Iteration 7400 (0.417845 iter/s, 239.323s/100 iter), 417.9/1129.4ep, loss = 2.28563
I0512 03:02:55.459203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31655 (* 1 = 2.31655 loss)
I0512 03:02:55.459213   380 sgd_solver.cpp:172] Iteration 7400, lr = 0.00015753, m = 0.9, wd = 1e-05, gs = 1
I0512 03:03:25.003631   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:04:01.221992   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:04:47.544104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:05:24.232769   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:06:01.468278   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:06:44.811024   380 solver.cpp:637] Iteration 7500, Testing net (#0)
I0512 03:06:47.519542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:07:13.366176   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:07:14.527379   380 solver.cpp:749] class AP 1: 0.895763
I0512 03:07:14.528084   380 solver.cpp:749] class AP 2: 0.884477
I0512 03:07:14.528374   380 solver.cpp:749] class AP 3: 0.900954
I0512 03:07:14.528383   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893731
I0512 03:07:14.528416   380 solver.cpp:284] Tests completed in 259.069s
I0512 03:07:15.087018   380 solver.cpp:354] Iteration 7500 (0.385997 iter/s, 259.069s/100 iter), 423.5/1129.4ep, loss = 2.39279
I0512 03:07:15.087107   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18968 (* 1 = 2.18968 loss)
I0512 03:07:15.087139   380 sgd_solver.cpp:172] Iteration 7500, lr = 0.000152588, m = 0.9, wd = 1e-05, gs = 1
I0512 03:07:26.689069   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:08:10.824625   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:08:45.930652   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:09:34.342669   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:10:09.833281   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:10:40.802503   380 solver.cpp:354] Iteration 7600 (0.486109 iter/s, 205.715s/100 iter), 429.2/1129.4ep, loss = 2.18991
I0512 03:10:40.802843   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19425 (* 1 = 2.19425 loss)
I0512 03:10:40.802923   380 sgd_solver.cpp:172] Iteration 7600, lr = 0.000147763, m = 0.9, wd = 1e-05, gs = 1
I0512 03:10:56.908025   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:11:37.434936   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:12:14.655290   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:12:59.465934   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:13:33.625195   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:14:12.815861   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:14:41.563577   380 solver.cpp:354] Iteration 7700 (0.41535 iter/s, 240.761s/100 iter), 434.8/1129.4ep, loss = 2.40904
I0512 03:14:41.563728   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31752 (* 1 = 2.31752 loss)
I0512 03:14:41.563771   380 sgd_solver.cpp:172] Iteration 7700, lr = 0.000143054, m = 0.9, wd = 1e-05, gs = 1
I0512 03:14:55.705982   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:15:45.825004   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:16:29.093302   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:17:04.945158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:17:48.602969   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:18:27.172235   380 solver.cpp:354] Iteration 7800 (0.443246 iter/s, 225.608s/100 iter), 440.5/1129.4ep, loss = 2.32555
I0512 03:18:27.172441   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.05796 (* 1 = 2.05796 loss)
I0512 03:18:27.172492   380 sgd_solver.cpp:172] Iteration 7800, lr = 0.000138458, m = 0.9, wd = 1e-05, gs = 1
I0512 03:18:27.567354   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:19:07.835121   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:19:48.661317   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:20:28.069588   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:21:08.481639   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:21:43.226600   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:22:19.123342   380 solver.cpp:354] Iteration 7900 (0.431126 iter/s, 231.951s/100 iter), 446.1/1129.4ep, loss = 2.40362
I0512 03:22:19.123687   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.54337 (* 1 = 2.54337 loss)
I0512 03:22:19.123773   380 sgd_solver.cpp:172] Iteration 7900, lr = 0.000133974, m = 0.9, wd = 1e-05, gs = 1
I0512 03:22:36.546411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:23:16.599068   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:24:05.626708   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:24:43.629340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:25:24.582979   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:09.663314   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:18.795955   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0512 03:26:18.824919   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0512 03:26:18.868686   380 solver.cpp:637] Iteration 8000, Testing net (#0)
I0512 03:26:44.520768   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:26:45.819016   380 solver.cpp:749] class AP 1: 0.897608
I0512 03:26:45.819759   380 solver.cpp:749] class AP 2: 0.885291
I0512 03:26:45.820050   380 solver.cpp:749] class AP 3: 0.901436
I0512 03:26:45.820057   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894778
I0512 03:26:45.820087   380 solver.cpp:284] Tests completed in 266.691s
I0512 03:26:46.517526   380 solver.cpp:354] Iteration 8000 (0.374965 iter/s, 266.691s/100 iter), 451.8/1129.4ep, loss = 2.36739
I0512 03:26:46.517626   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40254 (* 1 = 2.40254 loss)
I0512 03:26:46.517653   380 sgd_solver.cpp:172] Iteration 8000, lr = 0.0001296, m = 0.9, wd = 1e-05, gs = 1
I0512 03:26:46.519505   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.7 sparsity_achieved=0.490671 iter=8000
W0512 03:26:46.519564   380 net.cpp:2654] conv1a ni=3 no=32
W0512 03:26:47.884352   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.302083
W0512 03:26:47.884447   380 net.cpp:2654] conv1b ni=32 no=32
W0512 03:26:49.876003   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.652344
W0512 03:26:49.876109   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 03:26:51.816432   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.693251
W0512 03:26:51.816476   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 03:26:54.638629   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.632161
W0512 03:26:54.638676   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 03:26:58.599370   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.699653
W0512 03:26:58.599416   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 03:27:03.321456   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.693115
W0512 03:27:03.321501   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 03:27:04.541266   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.699653
W0512 03:27:04.541312   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 03:27:06.170826   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.697544
W0512 03:27:06.170872   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 03:27:08.111204   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.695323
W0512 03:27:08.111256   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 03:27:09.490093   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.699592
W0512 03:27:09.490136   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 03:27:09.490142   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 03:27:09.490149   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 03:27:09.490154   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 03:27:09.490159   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 03:27:09.490164   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 03:27:09.490170   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490177   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 03:27:09.490183   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490190   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490211   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490219   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490229   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 03:27:09.490236   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 03:27:09.490244   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490250   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 03:27:09.490257   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 03:27:09.490263   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 03:27:09.490272   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 03:27:09.493151   380 solver.cpp:391] Sparsity after update:
I0512 03:27:09.494045   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 03:27:09.494052   380 net.cpp:2780] conv1a_param_0(0.302) 
I0512 03:27:09.494060   380 net.cpp:2780] conv1b_param_0(0.652) 
I0512 03:27:09.494082   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494086   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494089   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 03:27:09.494092   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494096   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494099   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 03:27:09.494102   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494105   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494108   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 03:27:09.494113   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494117   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494128   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 03:27:09.494133   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494138   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494141   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 03:27:09.494146   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 03:27:09.494151   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 03:27:09.494156   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 03:27:09.494160   380 net.cpp:2780] res2a_branch2a_param_0(0.693) 
I0512 03:27:09.494168   380 net.cpp:2780] res2a_branch2b_param_0(0.632) 
I0512 03:27:09.494174   380 net.cpp:2780] res3a_branch2a_param_0(0.7) 
I0512 03:27:09.494179   380 net.cpp:2780] res3a_branch2b_param_0(0.693) 
I0512 03:27:09.494184   380 net.cpp:2780] res4a_branch2a_param_0(0.7) 
I0512 03:27:09.494189   380 net.cpp:2780] res4a_branch2b_param_0(0.698) 
I0512 03:27:09.494192   380 net.cpp:2780] res5a_branch2a_param_0(0.695) 
I0512 03:27:09.494197   380 net.cpp:2780] res5a_branch2b_param_0(0.7) 
I0512 03:27:09.494202   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.64003e+06/3.10435e+06) 0.528
I0512 03:27:17.393595   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:28:00.671474   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:28:34.868084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:29:22.881727   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:30:02.886854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:30:39.690645   380 solver.cpp:354] Iteration 8100 (0.428874 iter/s, 233.169s/100 iter), 457.4/1129.4ep, loss = 2.49695
I0512 03:30:39.690950   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.39197 (* 1 = 2.39197 loss)
I0512 03:30:39.691006   380 sgd_solver.cpp:172] Iteration 8100, lr = 0.000125334, m = 0.9, wd = 1e-05, gs = 1
I0512 03:30:41.673918   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:31:20.868306   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:32:00.966837   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:33:00.779994   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:33:37.856308   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:34:16.923619   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:34:49.851321   380 solver.cpp:354] Iteration 8200 (0.399749 iter/s, 250.157s/100 iter), 463.1/1129.4ep, loss = 2.53674
I0512 03:34:49.851662   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33144 (* 1 = 2.33144 loss)
I0512 03:34:49.851744   380 sgd_solver.cpp:172] Iteration 8200, lr = 0.000121174, m = 0.9, wd = 1e-05, gs = 1
I0512 03:35:00.245226   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:35:38.543448   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:36:26.458299   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:37:04.579043   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:37:40.072616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:38:17.089570   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:38:41.628803   380 solver.cpp:354] Iteration 8300 (0.431453 iter/s, 231.775s/100 iter), 468.7/1129.4ep, loss = 2.33841
I0512 03:38:41.629285   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.84869 (* 1 = 1.84869 loss)
I0512 03:38:41.629484   380 sgd_solver.cpp:172] Iteration 8300, lr = 0.000117118, m = 0.9, wd = 1e-05, gs = 1
I0512 03:39:14.493449   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:39:55.888578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:40:31.644776   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:41:19.291715   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:41:56.028820   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:42:38.358232   380 solver.cpp:354] Iteration 8400 (0.422427 iter/s, 236.727s/100 iter), 474.4/1129.4ep, loss = 2.3373
I0512 03:42:38.358451   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11929 (* 1 = 2.11929 loss)
I0512 03:42:38.358513   380 sgd_solver.cpp:172] Iteration 8400, lr = 0.000113165, m = 0.9, wd = 1e-05, gs = 1
I0512 03:42:43.653321   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:43:21.214802   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:43:58.160298   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:44:38.720536   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:45:18.189263   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:13.061955   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:29.102859   380 solver.cpp:637] Iteration 8500, Testing net (#0)
I0512 03:46:54.475260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:46:58.953804   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:47:00.598454   380 solver.cpp:749] class AP 1: 0.89757
I0512 03:47:00.599237   380 solver.cpp:749] class AP 2: 0.882906
I0512 03:47:00.599557   380 solver.cpp:749] class AP 3: 0.900166
I0512 03:47:00.599567   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893547
I0512 03:47:00.599599   380 solver.cpp:284] Tests completed in 262.239s
I0512 03:47:01.413116   380 solver.cpp:354] Iteration 8500 (0.381331 iter/s, 262.239s/100 iter), 480/1129.4ep, loss = 2.42511
I0512 03:47:01.413203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52729 (* 1 = 2.52729 loss)
I0512 03:47:01.413234   380 sgd_solver.cpp:172] Iteration 8500, lr = 0.000109313, m = 0.9, wd = 1e-05, gs = 1
I0512 03:47:46.690104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:48:23.432672   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:49:06.078362   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:49:47.503825   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:50:26.453498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:50:41.045670   380 solver.cpp:354] Iteration 8600 (0.455309 iter/s, 219.631s/100 iter), 485.6/1129.4ep, loss = 2.47804
I0512 03:50:41.045874   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55095 (* 1 = 2.55095 loss)
I0512 03:50:41.045920   380 sgd_solver.cpp:172] Iteration 8600, lr = 0.00010556, m = 0.9, wd = 1e-05, gs = 1
I0512 03:51:07.445318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:51:43.803124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:52:42.218966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:53:19.591488   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:53:53.739470   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:54:31.832521   380 solver.cpp:354] Iteration 8700 (0.433303 iter/s, 230.786s/100 iter), 491.3/1129.4ep, loss = 2.17636
I0512 03:54:31.832587   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.01066 (* 1 = 2.01066 loss)
I0512 03:54:31.832597   380 sgd_solver.cpp:172] Iteration 8700, lr = 0.000101905, m = 0.9, wd = 1e-05, gs = 1
I0512 03:54:33.863315   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:55:13.862351   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:56:06.395097   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:56:42.319245   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:57:23.943122   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:58:02.073696   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:58:25.842339   380 solver.cpp:354] Iteration 8800 (0.427333 iter/s, 234.009s/100 iter), 496.9/1129.4ep, loss = 2.26597
I0512 03:58:25.842375   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47366 (* 1 = 2.47366 loss)
I0512 03:58:25.842386   380 sgd_solver.cpp:172] Iteration 8800, lr = 9.8345e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 03:58:46.940827   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 03:59:31.486565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:00:10.049578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:00:52.036797   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:01:30.084602   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:02:19.907476   380 solver.cpp:354] Iteration 8900 (0.427228 iter/s, 234.067s/100 iter), 502.6/1129.4ep, loss = 2.32096
I0512 04:02:19.907789   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.63535 (* 1 = 2.63535 loss)
I0512 04:02:19.907861   380 sgd_solver.cpp:172] Iteration 8900, lr = 9.48794e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:02:21.475105   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:02:55.891996   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:03:37.904000   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:04:22.565506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:04:57.132908   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:05:45.763237   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:17.981604   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_9000.caffemodel
I0512 04:06:18.005462   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_9000.solverstate
I0512 04:06:18.027801   380 solver.cpp:637] Iteration 9000, Testing net (#0)
I0512 04:06:24.600473   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:41.067543   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:06:43.336433   380 solver.cpp:749] class AP 1: 0.897619
I0512 04:06:43.337236   380 solver.cpp:749] class AP 2: 0.886354
I0512 04:06:43.337545   380 solver.cpp:749] class AP 3: 0.900405
I0512 04:06:43.337553   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894793
I0512 04:06:43.337582   380 solver.cpp:284] Tests completed in 263.431s
I0512 04:06:43.981307   380 solver.cpp:354] Iteration 9000 (0.379606 iter/s, 263.431s/100 iter), 508.2/1129.4ep, loss = 2.34535
I0512 04:06:43.981343   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.12829 (* 1 = 2.12829 loss)
I0512 04:06:43.981351   380 sgd_solver.cpp:172] Iteration 9000, lr = 9.15063e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:07:15.315498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:07:51.825415   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:08:35.767640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:09:20.775972   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:09:56.172760   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:10:16.714072   380 solver.cpp:354] Iteration 9100 (0.470073 iter/s, 212.733s/100 iter), 513.9/1129.4ep, loss = 2.28868
I0512 04:10:16.714788   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.93488 (* 1 = 1.93488 loss)
I0512 04:10:16.715176   380 sgd_solver.cpp:172] Iteration 9100, lr = 8.82238e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:10:44.757681   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:11:24.644364   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:12:08.173871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:12:44.703150   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:13:31.190140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:14:17.660177   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:14:18.881845   380 solver.cpp:354] Iteration 9200 (0.412937 iter/s, 242.168s/100 iter), 519.5/1129.4ep, loss = 2.38187
I0512 04:14:18.882496   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46838 (* 1 = 2.46838 loss)
I0512 04:14:18.882824   380 sgd_solver.cpp:172] Iteration 9200, lr = 8.50305e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:14:58.226096   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:15:46.145756   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:16:23.331537   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:17:16.906608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:17:56.628084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:18:28.268833   380 solver.cpp:354] Iteration 9300 (0.400984 iter/s, 249.387s/100 iter), 525.2/1129.4ep, loss = 2.47833
I0512 04:18:28.269058   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.79426 (* 1 = 2.79426 loss)
I0512 04:18:28.269119   380 sgd_solver.cpp:172] Iteration 9300, lr = 8.19247e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:18:36.292224   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:19:15.597388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:19:54.318579   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:20:40.433630   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:21:18.065428   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:22:00.226513   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:22:23.438215   380 solver.cpp:354] Iteration 9400 (0.425226 iter/s, 235.169s/100 iter), 530.8/1129.4ep, loss = 2.34775
I0512 04:22:23.438251   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.57279 (* 1 = 2.57279 loss)
I0512 04:22:23.438261   380 sgd_solver.cpp:172] Iteration 9400, lr = 7.89048e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:22:39.044164   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:23:30.671339   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:24:15.297966   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:24:52.300066   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:25:33.650758   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:09.076153   380 solver.cpp:637] Iteration 9500, Testing net (#0)
I0512 04:26:10.033074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:36.053133   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:26:37.611305   380 solver.cpp:749] class AP 1: 0.897164
I0512 04:26:37.612017   380 solver.cpp:749] class AP 2: 0.887014
I0512 04:26:37.612320   380 solver.cpp:749] class AP 3: 0.902297
I0512 04:26:37.612327   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895492
I0512 04:26:37.612354   380 solver.cpp:284] Tests completed in 254.174s
I0512 04:26:38.291482   380 solver.cpp:354] Iteration 9500 (0.393432 iter/s, 254.174s/100 iter), 536.5/1129.4ep, loss = 2.32927
I0512 04:26:38.291617   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18902 (* 1 = 2.18902 loss)
I0512 04:26:38.291666   380 sgd_solver.cpp:172] Iteration 9500, lr = 7.59691e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:26:55.160392   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:27:34.971447   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:28:15.618667   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:29:01.073982   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:29:41.493643   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:30:10.876461   380 solver.cpp:354] Iteration 9600 (0.470401 iter/s, 212.584s/100 iter), 542.1/1129.4ep, loss = 2.38155
I0512 04:30:10.876710   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38257 (* 1 = 2.38257 loss)
I0512 04:30:10.876776   380 sgd_solver.cpp:172] Iteration 9600, lr = 7.31161e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:30:37.339706   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:31:16.071599   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:32:01.316895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:32:39.146456   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:33:17.844205   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:33:55.999173   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:34:18.285456   380 solver.cpp:354] Iteration 9700 (0.40419 iter/s, 247.408s/100 iter), 547.8/1129.4ep, loss = 2.36669
I0512 04:34:18.285892   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41858 (* 1 = 2.41858 loss)
I0512 04:34:18.286020   380 sgd_solver.cpp:172] Iteration 9700, lr = 7.03443e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:34:34.332509   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:35:22.874950   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:36:01.235574   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:36:55.050454   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:37:32.878162   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:38:10.755548   380 solver.cpp:354] Iteration 9800 (0.430164 iter/s, 232.469s/100 iter), 553.4/1129.4ep, loss = 2.38713
I0512 04:38:10.755614   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33087 (* 1 = 2.33087 loss)
I0512 04:38:10.755622   380 sgd_solver.cpp:172] Iteration 9800, lr = 6.7652e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:38:11.685073   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:38:58.482578   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:39:33.765489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:40:16.352944   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:40:53.765167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:41:39.234714   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:42:07.209664   380 solver.cpp:354] Iteration 9900 (0.422916 iter/s, 236.453s/100 iter), 559.1/1129.4ep, loss = 2.29045
I0512 04:42:07.209703   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22078 (* 1 = 2.22078 loss)
I0512 04:42:07.209713   380 sgd_solver.cpp:172] Iteration 9900, lr = 6.50378e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:42:18.775606   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:43:06.571552   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:43:50.888077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:44:27.324867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:05.549017   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:45.848542   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:45:56.049854   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0512 04:45:56.080005   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0512 04:45:56.104878   380 solver.cpp:637] Iteration 10000, Testing net (#0)
I0512 04:46:23.322026   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:46:25.711717   380 solver.cpp:749] class AP 1: 0.896814
I0512 04:46:25.712582   380 solver.cpp:749] class AP 2: 0.889558
I0512 04:46:25.712939   380 solver.cpp:749] class AP 3: 0.900951
I0512 04:46:25.712949   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895774
I0512 04:46:25.712985   380 solver.cpp:284] Tests completed in 258.503s
I0512 04:46:26.315979   380 solver.cpp:354] Iteration 10000 (0.386843 iter/s, 258.503s/100 iter), 564.7/1129.4ep, loss = 2.3947
I0512 04:46:26.316064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35071 (* 1 = 2.35071 loss)
I0512 04:46:26.316097   380 sgd_solver.cpp:172] Iteration 10000, lr = 6.25e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:46:26.317803   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.75 sparsity_achieved=0.528301 iter=10000
W0512 04:46:26.317849   380 net.cpp:2654] conv1a ni=3 no=32
W0512 04:46:27.700212   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.319583
W0512 04:46:27.700350   380 net.cpp:2654] conv1b ni=32 no=32
W0512 04:46:29.601946   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.667101
W0512 04:46:29.601994   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 04:46:31.954355   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.733778
W0512 04:46:31.954398   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 04:46:34.964366   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.645833
W0512 04:46:34.964411   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 04:46:40.177001   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.74608
W0512 04:46:40.177047   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 04:46:45.732903   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.724745
W0512 04:46:45.732949   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 04:46:47.585765   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.75
W0512 04:46:47.585808   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 04:46:50.289683   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.747694
W0512 04:46:50.289726   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 04:46:52.512779   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.74526
W0512 04:46:52.512823   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 04:46:54.126011   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.749939
W0512 04:46:54.126070   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 04:46:54.126075   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 04:46:54.126081   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 04:46:54.126087   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 04:46:54.126092   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 04:46:54.126106   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 04:46:54.126111   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126118   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 04:46:54.126140   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126147   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126158   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126166   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126173   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 04:46:54.126179   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 04:46:54.126190   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126196   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 04:46:54.126204   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 04:46:54.126209   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 04:46:54.126222   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 04:46:54.129120   380 solver.cpp:391] Sparsity after update:
I0512 04:46:54.130077   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 04:46:54.130086   380 net.cpp:2780] conv1a_param_0(0.32) 
I0512 04:46:54.130095   380 net.cpp:2780] conv1b_param_0(0.667) 
I0512 04:46:54.130097   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130101   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130105   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 04:46:54.130108   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130111   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130115   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 04:46:54.130118   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130121   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130125   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 04:46:54.130128   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130132   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130139   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 04:46:54.130144   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130152   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130156   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 04:46:54.130162   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 04:46:54.130167   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 04:46:54.130172   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 04:46:54.130179   380 net.cpp:2780] res2a_branch2a_param_0(0.734) 
I0512 04:46:54.130184   380 net.cpp:2780] res2a_branch2b_param_0(0.646) 
I0512 04:46:54.130189   380 net.cpp:2780] res3a_branch2a_param_0(0.746) 
I0512 04:46:54.130195   380 net.cpp:2780] res3a_branch2b_param_0(0.725) 
I0512 04:46:54.130198   380 net.cpp:2780] res4a_branch2a_param_0(0.75) 
I0512 04:46:54.130203   380 net.cpp:2780] res4a_branch2b_param_0(0.748) 
I0512 04:46:54.130205   380 net.cpp:2780] res5a_branch2a_param_0(0.745) 
I0512 04:46:54.130210   380 net.cpp:2780] res5a_branch2b_param_0(0.75) 
I0512 04:46:54.130214   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.75642e+06/3.10435e+06) 0.566
I0512 04:47:04.026461   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:47:42.293543   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:48:18.007519   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:49:02.739923   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:49:38.108216   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:50:17.342411   380 solver.cpp:354] Iteration 10100 (0.432852 iter/s, 231.026s/100 iter), 570.4/1129.4ep, loss = 2.32927
I0512 04:50:17.342597   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22941 (* 1 = 2.22941 loss)
I0512 04:50:17.342640   380 sgd_solver.cpp:172] Iteration 10100, lr = 6.00373e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:50:20.613549   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:50:58.246646   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:51:52.230434   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:52:35.205265   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:53:13.295640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:53:56.651862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:54:24.401233   380 solver.cpp:354] Iteration 10200 (0.404763 iter/s, 247.058s/100 iter), 576/1129.4ep, loss = 2.43863
I0512 04:54:24.401412   380 solver.cpp:378]     Train net output #0: mbox_loss = 3.0371 (* 1 = 3.0371 loss)
I0512 04:54:24.401465   380 sgd_solver.cpp:172] Iteration 10200, lr = 5.7648e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:54:35.641506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:55:13.492697   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:55:52.306445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:56:31.541483   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:57:17.623597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:57:54.230746   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:58:13.791709   380 solver.cpp:354] Iteration 10300 (0.435939 iter/s, 229.39s/100 iter), 581.6/1129.4ep, loss = 2.43791
I0512 04:58:13.791882   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.70717 (* 1 = 2.70717 loss)
I0512 04:58:13.791939   380 sgd_solver.cpp:172] Iteration 10300, lr = 5.53308e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 04:58:54.651705   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 04:59:29.620061   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:00:07.606716   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:00:44.238610   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:01:21.053959   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:01:59.582717   380 solver.cpp:354] Iteration 10400 (0.442889 iter/s, 225.79s/100 iter), 587.3/1129.4ep, loss = 2.21844
I0512 05:01:59.583503   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.08876 (* 1 = 2.08876 loss)
I0512 05:01:59.583925   380 sgd_solver.cpp:172] Iteration 10400, lr = 5.30842e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:02:01.327734   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:02:38.461591   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:03:30.491304   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:04:06.099306   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:01.590224   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:39.963915   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:05:53.941879   380 solver.cpp:637] Iteration 10500, Testing net (#0)
I0512 05:06:18.894109   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:06:21.693877   380 solver.cpp:749] class AP 1: 0.895421
I0512 05:06:21.708896   380 solver.cpp:749] class AP 2: 0.890704
I0512 05:06:21.711741   380 solver.cpp:749] class AP 3: 0.901217
I0512 05:06:21.712734   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895781
I0512 05:06:21.712834   380 solver.cpp:284] Tests completed in 262.129s
I0512 05:06:22.824540   380 solver.cpp:354] Iteration 10500 (0.381491 iter/s, 262.129s/100 iter), 592.9/1129.4ep, loss = 2.3693
I0512 05:06:22.824630   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.47519 (* 1 = 2.47519 loss)
I0512 05:06:22.824659   380 sgd_solver.cpp:172] Iteration 10500, lr = 5.09067e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:06:26.919333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:07:07.914069   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:07:43.175319   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:08:35.711709   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:09:14.252532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:09:55.657963   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:10:01.209481   380 solver.cpp:354] Iteration 10600 (0.457901 iter/s, 218.388s/100 iter), 598.6/1129.4ep, loss = 2.35791
I0512 05:10:01.209532   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.97221 (* 1 = 1.97221 loss)
I0512 05:10:01.209545   380 sgd_solver.cpp:172] Iteration 10600, lr = 4.87968e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:10:28.937144   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:11:16.933604   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:12:00.187927   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:12:35.605412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:13:22.758742   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:13:52.295043   380 solver.cpp:354] Iteration 10700 (0.432735 iter/s, 231.088s/100 iter), 604.2/1129.4ep, loss = 2.21944
I0512 05:13:52.295637   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.42109 (* 1 = 2.42109 loss)
I0512 05:13:52.295953   380 sgd_solver.cpp:172] Iteration 10700, lr = 4.67532e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:13:59.257160   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:14:45.193357   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:15:24.771397   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:16:04.701340   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:16:45.088380   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:17:27.546627   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:17:51.631371   380 solver.cpp:354] Iteration 10800 (0.417819 iter/s, 239.338s/100 iter), 609.9/1129.4ep, loss = 2.34322
I0512 05:17:51.631536   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22657 (* 1 = 2.22657 loss)
I0512 05:17:51.631585   380 sgd_solver.cpp:172] Iteration 10800, lr = 4.47746e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:18:15.397620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:18:51.053941   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:19:37.359869   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:20:18.237088   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:04.266841   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:45.601585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:21:47.690296   380 solver.cpp:354] Iteration 10900 (0.423621 iter/s, 236.06s/100 iter), 615.5/1129.4ep, loss = 2.26057
I0512 05:21:47.690331   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.28003 (* 1 = 2.28003 loss)
I0512 05:21:47.690341   380 sgd_solver.cpp:172] Iteration 10900, lr = 4.28593e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:22:22.958495   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:23:09.518605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:23:48.710981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:24:33.674439   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:25:06.079530   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:25:36.023643   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_11000.caffemodel
I0512 05:25:36.087622   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_11000.solverstate
I0512 05:25:36.153710   380 solver.cpp:637] Iteration 11000, Testing net (#0)
I0512 05:25:46.254866   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:26:02.649729   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:26:05.895247   380 solver.cpp:749] class AP 1: 0.896072
I0512 05:26:05.896275   380 solver.cpp:749] class AP 2: 0.890738
I0512 05:26:05.896700   380 solver.cpp:749] class AP 3: 0.900932
I0512 05:26:05.896709   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895914
I0512 05:26:05.896744   380 solver.cpp:284] Tests completed in 258.207s
I0512 05:26:06.641871   380 solver.cpp:354] Iteration 11000 (0.387286 iter/s, 258.207s/100 iter), 621.2/1129.4ep, loss = 2.2689
I0512 05:26:06.642454   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.56788 (* 1 = 2.56788 loss)
I0512 05:26:06.642767   380 sgd_solver.cpp:172] Iteration 11000, lr = 4.10062e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:26:41.267556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:27:17.095515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:28:08.419981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:28:48.638711   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:29:24.349315   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:29:43.101438   380 solver.cpp:354] Iteration 11100 (0.461979 iter/s, 216.46s/100 iter), 626.8/1129.4ep, loss = 2.43016
I0512 05:29:43.101524   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40675 (* 1 = 2.40675 loss)
I0512 05:29:43.101557   380 sgd_solver.cpp:172] Iteration 11100, lr = 3.92139e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:30:03.228891   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:30:42.014852   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:31:25.862293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:32:05.414085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:32:57.188159   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:33:33.602154   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:33:42.497421   380 solver.cpp:354] Iteration 11200 (0.417718 iter/s, 239.396s/100 iter), 632.5/1129.4ep, loss = 2.2224
I0512 05:33:42.497505   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72 (* 1 = 2.72 loss)
I0512 05:33:42.497535   380 sgd_solver.cpp:172] Iteration 11200, lr = 3.7481e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:34:30.818600   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:35:08.251355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:35:43.669617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:36:22.749773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:37:01.802333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:37:34.361750   380 solver.cpp:354] Iteration 11300 (0.431286 iter/s, 231.865s/100 iter), 638.1/1129.4ep, loss = 2.29065
I0512 05:37:34.361814   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.18509 (* 1 = 2.18509 loss)
I0512 05:37:34.361824   380 sgd_solver.cpp:172] Iteration 11300, lr = 3.58061e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:37:40.772616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:38:16.627167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:39:01.740345   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:39:41.658077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:40:26.172933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:41:11.747084   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:41:21.665264   380 solver.cpp:354] Iteration 11400 (0.43994 iter/s, 227.304s/100 iter), 643.8/1129.4ep, loss = 2.35193
I0512 05:41:21.665997   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58476 (* 1 = 2.58476 loss)
I0512 05:41:21.666395   380 sgd_solver.cpp:172] Iteration 11400, lr = 3.4188e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:41:49.021987   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:42:31.677724   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:43:09.831244   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:43:48.723028   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:44:28.003379   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:11.438060   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:11.715428   380 solver.cpp:637] Iteration 11500, Testing net (#0)
I0512 05:45:37.323577   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:45:40.198287   380 solver.cpp:749] class AP 1: 0.897706
I0512 05:45:40.198969   380 solver.cpp:749] class AP 2: 0.889887
I0512 05:45:40.199286   380 solver.cpp:749] class AP 3: 0.901156
I0512 05:45:40.199296   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89625
I0512 05:45:40.199331   380 solver.cpp:284] Tests completed in 258.537s
I0512 05:45:40.945160   380 solver.cpp:354] Iteration 11500 (0.386793 iter/s, 258.537s/100 iter), 649.4/1129.4ep, loss = 2.3099
I0512 05:45:40.945250   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.0581 (* 1 = 2.0581 loss)
I0512 05:45:40.945281   380 sgd_solver.cpp:172] Iteration 11500, lr = 3.26254e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:45:58.905357   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:46:36.544039   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:47:25.804399   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:48:04.763736   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:48:50.102066   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:49:15.950212   380 solver.cpp:354] Iteration 11600 (0.465102 iter/s, 215.006s/100 iter), 655.1/1129.4ep, loss = 2.36105
I0512 05:49:15.950249   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.20784 (* 1 = 2.20784 loss)
I0512 05:49:15.950259   380 sgd_solver.cpp:172] Iteration 11600, lr = 3.1117e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:49:32.186085   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:50:10.271440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:50:51.499954   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:51:28.598505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:52:19.084802   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:52:59.000892   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:53:16.934952   380 solver.cpp:354] Iteration 11700 (0.414962 iter/s, 240.986s/100 iter), 660.7/1129.4ep, loss = 2.39388
I0512 05:53:16.935112   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38527 (* 1 = 2.38527 loss)
I0512 05:53:16.935122   380 sgd_solver.cpp:172] Iteration 11700, lr = 2.96615e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:53:47.080124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:54:24.476706   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:55:10.370474   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:55:57.592562   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:56:36.655004   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:57:13.084990   380 solver.cpp:354] Iteration 11800 (0.423458 iter/s, 236.151s/100 iter), 666.4/1129.4ep, loss = 2.40543
I0512 05:57:13.085055   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41194 (* 1 = 2.41194 loss)
I0512 05:57:13.085065   380 sgd_solver.cpp:172] Iteration 11800, lr = 2.82576e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 05:57:16.590603   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:57:52.495090   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:58:33.725603   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:59:13.370117   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 05:59:51.943583   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:00:50.892933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:01:07.639163   380 solver.cpp:354] Iteration 11900 (0.426339 iter/s, 234.555s/100 iter), 672/1129.4ep, loss = 2.33039
I0512 06:01:07.639201   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3926 (* 1 = 2.3926 loss)
I0512 06:01:07.639211   380 sgd_solver.cpp:172] Iteration 11900, lr = 2.69042e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:01:31.972064   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:02:20.921489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:03:03.102632   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:03:39.798557   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:04:22.278528   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:04:58.035933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:05:17.578611   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_12000.caffemodel
I0512 06:05:17.667927   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_12000.solverstate
I0512 06:05:17.748509   380 solver.cpp:637] Iteration 12000, Testing net (#0)
I0512 06:05:37.238816   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:05:39.338567   380 solver.cpp:749] class AP 1: 0.897155
I0512 06:05:39.339229   380 solver.cpp:749] class AP 2: 0.890343
I0512 06:05:39.339509   380 solver.cpp:749] class AP 3: 0.901765
I0512 06:05:39.339514   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896421
I0512 06:05:39.339540   380 solver.cpp:284] Tests completed in 271.701s
I0512 06:05:39.938904   380 solver.cpp:354] Iteration 12000 (0.368051 iter/s, 271.701s/100 iter), 677.6/1129.4ep, loss = 2.26669
I0512 06:05:39.938992   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43762 (* 1 = 2.43762 loss)
I0512 06:05:39.939023   380 sgd_solver.cpp:172] Iteration 12000, lr = 2.56e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:05:39.940562   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.8 sparsity_achieved=0.565792 iter=12000
W0512 06:05:39.940603   380 net.cpp:2654] conv1a ni=3 no=32
W0512 06:05:41.443749   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.335
W0512 06:05:41.443835   380 net.cpp:2654] conv1b ni=32 no=32
W0512 06:05:43.122349   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.671007
W0512 06:05:43.122392   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 06:05:45.833979   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.762967
W0512 06:05:45.834020   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 06:05:48.915009   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.651476
W0512 06:05:48.915053   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 06:05:54.849905   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.773465
W0512 06:05:54.849951   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 06:06:00.813643   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.742459
W0512 06:06:00.813690   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 06:06:03.962257   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.799479
W0512 06:06:03.962302   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 06:06:08.405756   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.796061
W0512 06:06:08.405803   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 06:06:11.206773   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.794759
W0512 06:06:11.206817   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 06:06:13.264230   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.799391
W0512 06:06:13.264277   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 06:06:13.264283   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 06:06:13.264289   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 06:06:13.264295   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 06:06:13.264300   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 06:06:13.264307   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 06:06:13.264312   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264320   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 06:06:13.264326   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264333   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264355   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264361   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264369   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 06:06:13.264379   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 06:06:13.264384   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264391   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 06:06:13.264400   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 06:06:13.264406   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 06:06:13.264416   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 06:06:13.267336   380 solver.cpp:391] Sparsity after update:
I0512 06:06:13.268277   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 06:06:13.268286   380 net.cpp:2780] conv1a_param_0(0.335) 
I0512 06:06:13.268296   380 net.cpp:2780] conv1b_param_0(0.671) 
I0512 06:06:13.268298   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268306   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268308   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 06:06:13.268311   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268316   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268318   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 06:06:13.268322   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268326   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268334   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 06:06:13.268339   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268343   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268348   380 net.cpp:2780] ctx_output4_param_0(0) 
I0512 06:06:13.268355   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268359   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268364   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 06:06:13.268369   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 06:06:13.268373   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 06:06:13.268381   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 06:06:13.268386   380 net.cpp:2780] res2a_branch2a_param_0(0.763) 
I0512 06:06:13.268393   380 net.cpp:2780] res2a_branch2b_param_0(0.651) 
I0512 06:06:13.268398   380 net.cpp:2780] res3a_branch2a_param_0(0.773) 
I0512 06:06:13.268402   380 net.cpp:2780] res3a_branch2b_param_0(0.742) 
I0512 06:06:13.268409   380 net.cpp:2780] res4a_branch2a_param_0(0.799) 
I0512 06:06:13.268442   380 net.cpp:2780] res4a_branch2b_param_0(0.796) 
I0512 06:06:13.268452   380 net.cpp:2780] res5a_branch2a_param_0(0.795) 
I0512 06:06:13.268458   380 net.cpp:2780] res5a_branch2b_param_0(0.799) 
I0512 06:06:13.268463   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.86901e+06/3.10435e+06) 0.602
I0512 06:06:26.517848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:07:02.980505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:07:45.765640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:08:21.696214   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:09:01.983026   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:09:37.656339   380 solver.cpp:354] Iteration 12100 (0.420666 iter/s, 237.718s/100 iter), 683.3/1129.4ep, loss = 2.30271
I0512 06:09:37.657096   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43908 (* 1 = 2.43908 loss)
I0512 06:09:37.657335   380 sgd_solver.cpp:172] Iteration 12100, lr = 2.43438e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:09:43.331024   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:10:23.222741   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:11:21.117360   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:11:57.352170   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:12:36.948956   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:13:14.694136   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:13:42.489498   380 solver.cpp:354] Iteration 12200 (0.408441 iter/s, 244.834s/100 iter), 688.9/1129.4ep, loss = 2.34875
I0512 06:13:42.489533   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19593 (* 1 = 2.19593 loss)
I0512 06:13:42.489542   380 sgd_solver.cpp:172] Iteration 12200, lr = 2.31344e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:13:57.892750   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:14:41.506403   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:15:18.580929   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:16:06.041558   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:16:41.908457   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:17:35.333668   380 solver.cpp:354] Iteration 12300 (0.429475 iter/s, 232.842s/100 iter), 694.6/1129.4ep, loss = 2.39844
I0512 06:17:35.334223   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.26889 (* 1 = 2.26889 loss)
I0512 06:17:35.334553   380 sgd_solver.cpp:172] Iteration 12300, lr = 2.19706e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:17:36.900149   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:18:16.666059   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:18:52.242817   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:19:32.508054   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:20:11.345551   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:21:01.066597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:21:32.221345   380 solver.cpp:354] Iteration 12400 (0.422145 iter/s, 236.885s/100 iter), 700.2/1129.4ep, loss = 2.34175
I0512 06:21:32.221438   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.14213 (* 1 = 2.14213 loss)
I0512 06:21:32.221469   380 sgd_solver.cpp:172] Iteration 12400, lr = 2.08514e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:21:36.863664   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:22:30.779036   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:23:08.783572   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:23:50.718506   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:24:31.727022   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:09.870867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:31.636656   380 solver.cpp:637] Iteration 12500, Testing net (#0)
I0512 06:25:55.800508   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:25:58.212220   380 solver.cpp:749] class AP 1: 0.896631
I0512 06:25:58.213078   380 solver.cpp:749] class AP 2: 0.884702
I0512 06:25:58.213413   380 solver.cpp:749] class AP 3: 0.901848
I0512 06:25:58.213421   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894394
I0512 06:25:58.213450   380 solver.cpp:284] Tests completed in 265.991s
I0512 06:25:58.916282   380 solver.cpp:354] Iteration 12500 (0.375953 iter/s, 265.991s/100 iter), 705.9/1129.4ep, loss = 2.3806
I0512 06:25:58.916401   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33346 (* 1 = 2.33346 loss)
I0512 06:25:58.916447   380 sgd_solver.cpp:172] Iteration 12500, lr = 1.97754e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:26:05.879794   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:26:48.089718   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:27:23.452514   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:28:01.666285   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:28:40.225848   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:29:24.613023   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:29:28.099177   380 solver.cpp:354] Iteration 12600 (0.478052 iter/s, 209.182s/100 iter), 711.5/1129.4ep, loss = 2.28146
I0512 06:29:28.099916   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.1506 (* 1 = 2.1506 loss)
I0512 06:29:28.100299   380 sgd_solver.cpp:172] Iteration 12600, lr = 1.87416e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:30:03.520756   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:30:52.604082   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:31:31.773980   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:32:25.838932   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:33:05.975986   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:33:34.286411   380 solver.cpp:354] Iteration 12700 (0.406196 iter/s, 246.187s/100 iter), 717.2/1129.4ep, loss = 2.43517
I0512 06:33:34.286657   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48013 (* 1 = 2.48013 loss)
I0512 06:33:34.286748   380 sgd_solver.cpp:172] Iteration 12700, lr = 1.77489e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:33:40.941913   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:34:17.781616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:34:55.426512   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:35:48.387617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:36:26.879323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:37:04.952895   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:37:24.351830   380 solver.cpp:354] Iteration 12800 (0.43466 iter/s, 230.065s/100 iter), 722.8/1129.4ep, loss = 2.29932
I0512 06:37:24.352488   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11013 (* 1 = 2.11013 loss)
I0512 06:37:24.352850   380 sgd_solver.cpp:172] Iteration 12800, lr = 1.67962e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:37:44.789419   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:38:34.387388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:39:18.631997   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:39:58.429492   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:40:34.252135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:41:13.095304   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:41:15.594710   380 solver.cpp:354] Iteration 12900 (0.432446 iter/s, 231.243s/100 iter), 728.5/1129.4ep, loss = 2.33723
I0512 06:41:15.594748   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35788 (* 1 = 2.35788 loss)
I0512 06:41:15.594758   380 sgd_solver.cpp:172] Iteration 12900, lr = 1.58823e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:41:56.351114   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:42:34.818361   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:43:17.867835   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:44:04.965185   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:44:39.564811   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:08.764317   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_13000.caffemodel
I0512 06:45:08.822512   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_13000.solverstate
I0512 06:45:08.884050   380 solver.cpp:637] Iteration 13000, Testing net (#0)
I0512 06:45:33.703161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:35.911193   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:45:38.684885   380 solver.cpp:749] class AP 1: 0.897355
I0512 06:45:38.685725   380 solver.cpp:749] class AP 2: 0.88589
I0512 06:45:38.686061   380 solver.cpp:749] class AP 3: 0.902037
I0512 06:45:38.686069   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895094
I0512 06:45:38.686097   380 solver.cpp:284] Tests completed in 263.091s
I0512 06:45:39.281121   380 solver.cpp:354] Iteration 13000 (0.380096 iter/s, 263.091s/100 iter), 734.1/1129.4ep, loss = 2.44566
I0512 06:45:39.281203   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.42635 (* 1 = 2.42635 loss)
I0512 06:45:39.281234   380 sgd_solver.cpp:172] Iteration 13000, lr = 1.50063e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:46:08.765684   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:46:57.449867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:47:42.440268   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:48:20.579772   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:48:58.451648   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:49:06.276201   380 solver.cpp:354] Iteration 13100 (0.483104 iter/s, 206.995s/100 iter), 739.8/1129.4ep, loss = 2.44823
I0512 06:49:06.276286   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60307 (* 1 = 2.60307 loss)
I0512 06:49:06.276316   380 sgd_solver.cpp:172] Iteration 13100, lr = 1.4167e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:49:36.681020   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:50:20.691504   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:50:59.741583   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:51:43.453512   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:52:21.239766   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:53:19.637573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:53:20.796137   380 solver.cpp:354] Iteration 13200 (0.392894 iter/s, 254.522s/100 iter), 745.4/1129.4ep, loss = 2.46841
I0512 06:53:20.796175   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46276 (* 1 = 2.46276 loss)
I0512 06:53:20.796183   380 sgd_solver.cpp:172] Iteration 13200, lr = 1.33634e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:54:05.901422   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:54:43.672833   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:55:21.113030   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:55:56.085239   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:56:34.869011   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:57:06.451875   380 solver.cpp:354] Iteration 13300 (0.443151 iter/s, 225.657s/100 iter), 751.1/1129.4ep, loss = 2.34491
I0512 06:57:06.452064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17581 (* 1 = 2.17581 loss)
I0512 06:57:06.452111   380 sgd_solver.cpp:172] Iteration 13300, lr = 1.25944e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 06:57:17.110515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:57:54.811944   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:58:41.091114   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 06:59:16.049782   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:12.797559   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:50.825961   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:00:54.735842   380 solver.cpp:354] Iteration 13400 (0.438049 iter/s, 228.285s/100 iter), 756.7/1129.4ep, loss = 2.37319
I0512 07:00:54.735980   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3782 (* 1 = 2.3782 loss)
I0512 07:00:54.736027   380 sgd_solver.cpp:172] Iteration 13400, lr = 1.18592e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:01:30.692363   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:02:10.166597   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:02:46.698156   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:03:28.340123   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:04:04.508785   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:04:43.612576   380 solver.cpp:637] Iteration 13500, Testing net (#0)
I0512 07:04:48.126879   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:05:10.335208   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:05:12.436916   380 solver.cpp:749] class AP 1: 0.895944
I0512 07:05:12.437710   380 solver.cpp:749] class AP 2: 0.885567
I0512 07:05:12.438035   380 solver.cpp:749] class AP 3: 0.901646
I0512 07:05:12.438042   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894386
I0512 07:05:12.438071   380 solver.cpp:284] Tests completed in 257.703s
I0512 07:05:13.064388   380 solver.cpp:354] Iteration 13500 (0.388044 iter/s, 257.703s/100 iter), 762.4/1129.4ep, loss = 2.30305
I0512 07:05:13.064474   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55962 (* 1 = 2.55962 loss)
I0512 07:05:13.064507   380 sgd_solver.cpp:172] Iteration 13500, lr = 1.11566e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:05:31.713337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:06:15.545054   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:06:55.901111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:07:34.915421   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:08:31.364018   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:08:45.467353   380 solver.cpp:354] Iteration 13600 (0.470802 iter/s, 212.403s/100 iter), 768/1129.4ep, loss = 2.43224
I0512 07:08:45.467612   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.6149 (* 1 = 2.6149 loss)
I0512 07:08:45.467680   380 sgd_solver.cpp:172] Iteration 13600, lr = 1.04858e-05, m = 0.9, wd = 1e-05, gs = 1
I0512 07:09:09.037259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:09:48.372699   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:10:25.051229   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:11:13.652639   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:11:52.639192   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:12:30.236826   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:12:44.709638   380 solver.cpp:354] Iteration 13700 (0.417986 iter/s, 239.243s/100 iter), 773.6/1129.4ep, loss = 2.34681
I0512 07:12:44.709672   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.37087 (* 1 = 2.37087 loss)
I0512 07:12:44.709681   380 sgd_solver.cpp:172] Iteration 13700, lr = 9.8456e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:13:16.381981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:13:53.819025   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:14:47.908991   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:15:24.681938   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:16:04.766158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:16:39.083851   380 solver.cpp:354] Iteration 13800 (0.426668 iter/s, 234.375s/100 iter), 779.3/1129.4ep, loss = 2.36962
I0512 07:16:39.083916   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.51439 (* 1 = 2.51439 loss)
I0512 07:16:39.083927   380 sgd_solver.cpp:172] Iteration 13800, lr = 9.23521e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:16:43.347702   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:17:18.843503   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:18:06.169550   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:18:46.751183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:19:28.314239   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:20:07.549818   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:20:32.880179   380 solver.cpp:354] Iteration 13900 (0.427722 iter/s, 233.797s/100 iter), 784.9/1129.4ep, loss = 2.3397
I0512 07:20:32.880704   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.11196 (* 1 = 2.11196 loss)
I0512 07:20:32.880937   380 sgd_solver.cpp:172] Iteration 13900, lr = 8.65365e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:21:03.613611   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:21:44.093758   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:22:19.291666   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:23:03.619459   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:23:40.908710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:19.048044   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:28.633548   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_14000.caffemodel
I0512 07:24:28.688812   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_14000.solverstate
I0512 07:24:28.737344   380 solver.cpp:637] Iteration 14000, Testing net (#0)
I0512 07:24:50.037305   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:24:52.355204   380 solver.cpp:749] class AP 1: 0.89628
I0512 07:24:52.356040   380 solver.cpp:749] class AP 2: 0.88667
I0512 07:24:52.356384   380 solver.cpp:749] class AP 3: 0.901667
I0512 07:24:52.356395   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.894872
I0512 07:24:52.356429   380 solver.cpp:284] Tests completed in 259.474s
I0512 07:24:52.981204   380 solver.cpp:354] Iteration 14000 (0.385395 iter/s, 259.474s/100 iter), 790.6/1129.4ep, loss = 2.39895
I0512 07:24:52.981297   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.46555 (* 1 = 2.46555 loss)
I0512 07:24:52.981331   380 sgd_solver.cpp:172] Iteration 14000, lr = 8.1e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:24:52.983011   380 solver.cpp:981] Finding and applying sparsity: sparsity_target=0.75 sparsity_factor=0.85 sparsity_achieved=0.602061 iter=14000
W0512 07:24:52.983054   380 net.cpp:2654] conv1a ni=3 no=32
W0512 07:24:54.762243   380 net.cpp:2716] conv1a ZeroWeightsFraction=0.3425
W0512 07:24:54.762342   380 net.cpp:2654] conv1b ni=32 no=32
W0512 07:24:56.442049   380 net.cpp:2716] conv1b ZeroWeightsFraction=0.674479
W0512 07:24:56.442095   380 net.cpp:2654] res2a_branch2a ni=32 no=64
W0512 07:24:59.412950   380 net.cpp:2716] res2a_branch2a ZeroWeightsFraction=0.780599
W0512 07:24:59.412993   380 net.cpp:2654] res2a_branch2b ni=64 no=64
W0512 07:25:02.505936   380 net.cpp:2716] res2a_branch2b ZeroWeightsFraction=0.65332
W0512 07:25:02.505980   380 net.cpp:2654] res3a_branch2a ni=64 no=128
W0512 07:25:08.671644   380 net.cpp:2716] res3a_branch2a ZeroWeightsFraction=0.783596
W0512 07:25:08.671689   380 net.cpp:2654] res3a_branch2b ni=128 no=128
W0512 07:25:14.824723   380 net.cpp:2716] res3a_branch2b ZeroWeightsFraction=0.750217
W0512 07:25:14.824767   380 net.cpp:2654] res4a_branch2a ni=128 no=256
W0512 07:25:20.207489   380 net.cpp:2716] res4a_branch2a ZeroWeightsFraction=0.84947
W0512 07:25:20.207548   380 net.cpp:2654] res4a_branch2b ni=256 no=256
W0512 07:25:27.279160   380 net.cpp:2716] res4a_branch2b ZeroWeightsFraction=0.843174
W0512 07:25:27.279202   380 net.cpp:2654] res5a_branch2a ni=256 no=512
W0512 07:25:31.233297   380 net.cpp:2716] res5a_branch2a ZeroWeightsFraction=0.844131
W0512 07:25:31.233340   380 net.cpp:2654] res5a_branch2b ni=512 no=512
W0512 07:25:34.229763   380 net.cpp:2716] res5a_branch2b ZeroWeightsFraction=0.849526
W0512 07:25:34.229809   380 net.cpp:2654] ctx_output1 ni=128 no=256
W0512 07:25:34.229815   380 net.cpp:2654] ctx_output2 ni=512 no=256
W0512 07:25:34.229821   380 net.cpp:2654] ctx_output3 ni=512 no=256
W0512 07:25:34.229826   380 net.cpp:2654] ctx_output4 ni=512 no=256
W0512 07:25:34.229832   380 net.cpp:2654] ctx_output5 ni=512 no=256
W0512 07:25:34.229838   380 net.cpp:2654] ctx_output6 ni=512 no=256
W0512 07:25:34.229845   380 net.cpp:2654] ctx_output1/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229851   380 net.cpp:2654] ctx_output1/relu_mbox_conf ni=256 no=16
W0512 07:25:34.229861   380 net.cpp:2654] ctx_output2/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229871   380 net.cpp:2654] ctx_output2/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229880   380 net.cpp:2654] ctx_output3/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229889   380 net.cpp:2654] ctx_output3/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229916   380 net.cpp:2654] ctx_output4/relu_mbox_loc ni=256 no=24
W0512 07:25:34.229930   380 net.cpp:2654] ctx_output4/relu_mbox_conf ni=256 no=24
W0512 07:25:34.229941   380 net.cpp:2654] ctx_output5/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229957   380 net.cpp:2654] ctx_output5/relu_mbox_conf ni=256 no=16
W0512 07:25:34.229970   380 net.cpp:2654] ctx_output6/relu_mbox_loc ni=256 no=16
W0512 07:25:34.229982   380 net.cpp:2654] ctx_output6/relu_mbox_conf ni=256 no=16
I0512 07:25:34.229993   380 net.cpp:2749] All zero weights of convolution layers are frozen
I0512 07:25:34.232905   380 solver.cpp:391] Sparsity after update:
I0512 07:25:34.233840   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 07:25:34.233850   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 07:25:34.233858   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 07:25:34.233861   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233865   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233868   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 07:25:34.233872   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233876   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233883   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 07:25:34.233888   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233893   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233901   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 07:25:34.233904   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233909   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233913   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 07:25:34.233922   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233927   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233932   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 07:25:34.233935   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 07:25:34.233938   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 07:25:34.233943   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 07:25:34.233947   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 07:25:34.233952   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 07:25:34.233958   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 07:25:34.233963   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 07:25:34.233968   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 07:25:34.233973   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 07:25:34.233978   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 07:25:34.234004   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 07:25:34.234009   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 07:25:46.099409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:26:24.533906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:27:08.471392   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:27:43.094532   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:28:29.307065   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:29:00.848176   380 solver.cpp:354] Iteration 14100 (0.403451 iter/s, 247.862s/100 iter), 796.2/1129.4ep, loss = 2.5574
I0512 07:29:00.848471   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29777 (* 1 = 2.29777 loss)
I0512 07:29:00.848538   380 sgd_solver.cpp:172] Iteration 14100, lr = 7.57335e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:29:05.882730   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:30:04.028123   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:30:44.361773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:31:22.282876   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:32:03.865661   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:32:39.793854   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:33:10.702345   380 solver.cpp:354] Iteration 14200 (0.400239 iter/s, 249.85s/100 iter), 801.9/1129.4ep, loss = 2.47844
I0512 07:33:10.703030   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.58026 (* 1 = 2.58026 loss)
I0512 07:33:10.703384   380 sgd_solver.cpp:172] Iteration 14200, lr = 7.07281e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:33:24.347329   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:34:02.782104   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:34:47.743248   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:35:24.586400   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:36:22.143801   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:37:02.439826   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:37:03.478307   380 solver.cpp:354] Iteration 14300 (0.429602 iter/s, 232.773s/100 iter), 807.5/1129.4ep, loss = 2.39447
I0512 07:37:03.478458   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52669 (* 1 = 2.52669 loss)
I0512 07:37:03.478509   380 sgd_solver.cpp:172] Iteration 14300, lr = 6.5975e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:37:38.083612   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:38:15.703184   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:38:52.380204   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:39:40.429337   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:40:19.911423   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:40:50.818274   380 solver.cpp:354] Iteration 14400 (0.439874 iter/s, 227.338s/100 iter), 813.2/1129.4ep, loss = 2.50824
I0512 07:40:50.818390   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.49054 (* 1 = 2.49054 loss)
I0512 07:40:50.818434   380 sgd_solver.cpp:172] Iteration 14400, lr = 6.14656e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:40:58.933917   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:41:37.699744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:42:16.515658   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:43:16.743243   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:43:52.033319   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:44:32.242260   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:44:45.479857   380 solver.cpp:637] Iteration 14500, Testing net (#0)
I0512 07:45:08.862388   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:45:12.184242   380 solver.cpp:749] class AP 1: 0.894689
I0512 07:45:12.185427   380 solver.cpp:749] class AP 2: 0.884244
I0512 07:45:12.185783   380 solver.cpp:749] class AP 3: 0.901387
I0512 07:45:12.185791   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89344
I0512 07:45:12.185820   380 solver.cpp:284] Tests completed in 261.366s
I0512 07:45:12.900179   380 solver.cpp:354] Iteration 14500 (0.382605 iter/s, 261.366s/100 iter), 818.8/1129.4ep, loss = 2.38669
I0512 07:45:12.900267   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.4358 (* 1 = 2.4358 loss)
I0512 07:45:12.900295   380 sgd_solver.cpp:172] Iteration 14500, lr = 5.71914e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:45:22.118444   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:45:59.421556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:46:46.104092   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:47:24.289927   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:04.250116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:39.939463   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:48:49.415818   380 solver.cpp:354] Iteration 14600 (0.461863 iter/s, 216.514s/100 iter), 824.5/1129.4ep, loss = 2.30774
I0512 07:48:49.416157   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.43388 (* 1 = 2.43388 loss)
I0512 07:48:49.416244   380 sgd_solver.cpp:172] Iteration 14600, lr = 5.31441e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:49:33.194670   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:50:11.710112   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:50:51.186725   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:51:34.303781   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:52:09.476140   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:52:40.223341   380 solver.cpp:354] Iteration 14700 (0.433264 iter/s, 230.806s/100 iter), 830.1/1129.4ep, loss = 2.47492
I0512 07:52:40.223424   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.22872 (* 1 = 2.22872 loss)
I0512 07:52:40.223451   380 sgd_solver.cpp:172] Iteration 14700, lr = 4.93155e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:52:56.503324   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:53:40.316382   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:54:23.106197   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:55:04.873273   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:55:49.470324   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:56:32.330438   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:56:44.101650   380 solver.cpp:354] Iteration 14800 (0.410043 iter/s, 243.877s/100 iter), 835.8/1129.4ep, loss = 2.38121
I0512 07:56:44.101953   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.40385 (* 1 = 2.40385 loss)
I0512 07:56:44.102036   380 sgd_solver.cpp:172] Iteration 14800, lr = 4.56976e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 07:57:13.446024   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:58:00.623440   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:58:41.222637   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 07:59:21.740432   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:01.214862   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:39.803169   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:00:40.230898   380 solver.cpp:354] Iteration 14900 (0.423495 iter/s, 236.13s/100 iter), 841.4/1129.4ep, loss = 2.32013
I0512 08:00:40.231585   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.04751 (* 1 = 2.04751 loss)
I0512 08:00:40.231972   380 sgd_solver.cpp:172] Iteration 14900, lr = 4.22825e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:01:24.912009   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:02:03.143669   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:02:54.398586   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:03:34.460433   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:04:21.644702   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:04:43.264290   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_15000.caffemodel
I0512 08:04:43.337749   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_15000.solverstate
I0512 08:04:43.399899   380 solver.cpp:637] Iteration 15000, Testing net (#0)
I0512 08:05:02.137131   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:05:05.637182   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:05:09.070629   380 solver.cpp:749] class AP 1: 0.893947
I0512 08:05:09.071787   380 solver.cpp:749] class AP 2: 0.882938
I0512 08:05:09.072137   380 solver.cpp:749] class AP 3: 0.901647
I0512 08:05:09.072144   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892844
I0512 08:05:09.072175   380 solver.cpp:284] Tests completed in 268.842s
I0512 08:05:09.737620   380 solver.cpp:354] Iteration 15000 (0.371965 iter/s, 268.842s/100 iter), 847.1/1129.4ep, loss = 2.59361
I0512 08:05:09.737712   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.71177 (* 1 = 2.71177 loss)
I0512 08:05:09.737746   380 sgd_solver.cpp:172] Iteration 15000, lr = 3.90625e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:05:43.523984   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:06:26.522555   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:07:02.259291   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:07:51.357620   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:08:30.935556   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:08:49.985224   380 solver.cpp:354] Iteration 15100 (0.454034 iter/s, 220.248s/100 iter), 852.7/1129.4ep, loss = 2.38181
I0512 08:08:49.985416   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.2876 (* 1 = 2.2876 loss)
I0512 08:08:49.985479   380 sgd_solver.cpp:172] Iteration 15100, lr = 3.603e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:09:12.115409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:09:54.221241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:10:39.264596   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:11:26.965469   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:12:04.211906   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:12:46.289477   380 solver.cpp:354] Iteration 15200 (0.423183 iter/s, 236.304s/100 iter), 858.4/1129.4ep, loss = 2.4371
I0512 08:12:46.290117   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.80906 (* 1 = 2.80906 loss)
I0512 08:12:46.290447   380 sgd_solver.cpp:172] Iteration 15200, lr = 3.31776e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:12:55.527381   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:13:32.113687   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:14:05.849428   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:14:45.373320   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:15:21.321030   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:16:15.836139   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:16:33.305686   380 solver.cpp:354] Iteration 15300 (0.440497 iter/s, 227.016s/100 iter), 864/1129.4ep, loss = 2.35677
I0512 08:16:33.305727   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.65949 (* 1 = 2.65949 loss)
I0512 08:16:33.305737   380 sgd_solver.cpp:172] Iteration 15300, lr = 3.0498e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:16:51.347573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:17:42.169284   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:18:17.864292   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:19:01.127099   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:19:47.540293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:20:22.977674   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:20:27.777370   380 solver.cpp:354] Iteration 15400 (0.426491 iter/s, 234.471s/100 iter), 869.6/1129.4ep, loss = 2.44611
I0512 08:20:27.777546   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.61646 (* 1 = 2.61646 loss)
I0512 08:20:27.777606   380 sgd_solver.cpp:172] Iteration 15400, lr = 2.79841e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:21:02.634498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:21:39.585217   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:22:24.819844   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:23:01.597182   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:23:54.346920   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:29.915454   380 solver.cpp:637] Iteration 15500, Testing net (#0)
I0512 08:24:35.203483   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:50.184361   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:24:52.788005   380 solver.cpp:749] class AP 1: 0.894075
I0512 08:24:52.789157   380 solver.cpp:749] class AP 2: 0.883818
I0512 08:24:52.789511   380 solver.cpp:749] class AP 3: 0.901737
I0512 08:24:52.789520   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89321
I0512 08:24:52.789551   380 solver.cpp:284] Tests completed in 265.012s
I0512 08:24:53.500522   380 solver.cpp:354] Iteration 15500 (0.377342 iter/s, 265.012s/100 iter), 875.3/1129.4ep, loss = 2.42094
I0512 08:24:53.500592   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38818 (* 1 = 2.38818 loss)
I0512 08:24:53.500622   380 sgd_solver.cpp:172] Iteration 15500, lr = 2.56289e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:25:12.217156   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:25:58.856359   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:26:34.289676   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:27:32.933523   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:28:09.246408   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:28:23.760172   380 solver.cpp:354] Iteration 15600 (0.475603 iter/s, 210.259s/100 iter), 880.9/1129.4ep, loss = 2.39625
I0512 08:28:23.760283   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.4466 (* 1 = 2.4466 loss)
I0512 08:28:23.760327   380 sgd_solver.cpp:172] Iteration 15600, lr = 2.34256e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:28:50.703773   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:29:33.323573   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:30:11.193820   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:31:02.298489   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:31:41.053411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:32:16.280588   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:32:24.060652   380 solver.cpp:354] Iteration 15700 (0.416147 iter/s, 240.3s/100 iter), 886.6/1129.4ep, loss = 2.40741
I0512 08:32:24.061125   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48293 (* 1 = 2.48293 loss)
I0512 08:32:24.061213   380 sgd_solver.cpp:172] Iteration 15700, lr = 2.13675e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:32:52.615475   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:33:40.227411   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:34:25.732493   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:35:06.001135   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:35:56.376116   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:36:29.090582   380 solver.cpp:354] Iteration 15800 (0.408112 iter/s, 245.031s/100 iter), 892.2/1129.4ep, loss = 2.35173
I0512 08:36:29.091447   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.10538 (* 1 = 2.10538 loss)
I0512 08:36:29.091814   380 sgd_solver.cpp:172] Iteration 15800, lr = 1.94481e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:36:36.692628   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:37:21.156144   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:37:59.355221   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:38:39.619859   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:39:24.415330   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:40:05.484552   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:40:23.683187   380 solver.cpp:354] Iteration 15900 (0.42627 iter/s, 234.593s/100 iter), 897.9/1129.4ep, loss = 2.45922
I0512 08:40:23.683353   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.32499 (* 1 = 2.32499 loss)
I0512 08:40:23.683400   380 sgd_solver.cpp:172] Iteration 15900, lr = 1.7661e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:40:46.294355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:41:26.560863   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:42:12.992933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:42:48.878226   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:43:31.637800   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:16.927175   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:26.361054   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_16000.caffemodel
I0512 08:44:26.382162   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_16000.solverstate
I0512 08:44:26.404278   380 solver.cpp:637] Iteration 16000, Testing net (#0)
I0512 08:44:45.449192   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:44:48.496340   380 solver.cpp:749] class AP 1: 0.893932
I0512 08:44:48.497509   380 solver.cpp:749] class AP 2: 0.883147
I0512 08:44:48.497857   380 solver.cpp:749] class AP 3: 0.901828
I0512 08:44:48.497864   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892969
I0512 08:44:48.497893   380 solver.cpp:284] Tests completed in 264.815s
I0512 08:44:49.228394   380 solver.cpp:354] Iteration 16000 (0.377622 iter/s, 264.815s/100 iter), 903.5/1129.4ep, loss = 2.38821
I0512 08:44:49.228482   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.60551 (* 1 = 2.60551 loss)
I0512 08:44:49.228514   380 sgd_solver.cpp:172] Iteration 16000, lr = 1.6e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:44:49.230512   380 solver.cpp:391] Sparsity after update:
I0512 08:44:49.232025   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 08:44:49.232057   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 08:44:49.232089   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 08:44:49.232117   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232146   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232175   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 08:44:49.232203   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232232   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232260   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 08:44:49.232290   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232318   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232347   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 08:44:49.232375   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232404   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232434   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 08:44:49.232462   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232491   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232520   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 08:44:49.232548   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 08:44:49.232578   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 08:44:49.232605   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 08:44:49.232635   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 08:44:49.232663   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 08:44:49.232692   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 08:44:49.232720   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 08:44:49.232749   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 08:44:49.232777   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 08:44:49.232807   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 08:44:49.232836   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 08:44:49.232864   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 08:45:01.285671   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:45:39.896585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:46:18.314790   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:47:10.651660   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:47:52.706176   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:48:24.228271   380 solver.cpp:354] Iteration 16100 (0.465116 iter/s, 215s/100 iter), 909.2/1129.4ep, loss = 2.5473
I0512 08:48:24.228338   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.98722 (* 1 = 2.98722 loss)
I0512 08:48:24.228348   380 sgd_solver.cpp:172] Iteration 16100, lr = 1.4459e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:48:37.516037   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:49:19.842875   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:50:00.832288   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:50:37.076860   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:51:14.554430   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:51:58.141252   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:52:24.244592   380 solver.cpp:354] Iteration 16200 (0.416638 iter/s, 240.016s/100 iter), 914.8/1129.4ep, loss = 2.35982
I0512 08:52:24.244797   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.38771 (* 1 = 2.38771 loss)
I0512 08:52:24.244860   380 sgd_solver.cpp:172] Iteration 16200, lr = 1.30321e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:52:37.518167   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:53:24.339283   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:54:09.353605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:54:48.211962   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:55:41.459074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:56:17.799016   380 solver.cpp:354] Iteration 16300 (0.428166 iter/s, 233.554s/100 iter), 920.5/1129.4ep, loss = 2.42821
I0512 08:56:17.799552   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.52797 (* 1 = 2.52797 loss)
I0512 08:56:17.799749   380 sgd_solver.cpp:172] Iteration 16300, lr = 1.17135e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 08:56:17.957899   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:56:55.372046   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:57:33.934849   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:58:09.779541   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:58:50.629307   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 08:59:30.157141   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:00:09.487685   380 solver.cpp:354] Iteration 16400 (0.431614 iter/s, 231.689s/100 iter), 926.1/1129.4ep, loss = 2.34306
I0512 09:00:09.488273   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.1992 (* 1 = 2.1992 loss)
I0512 09:00:09.488590   380 sgd_solver.cpp:172] Iteration 16400, lr = 1.04976e-06, m = 0.9, wd = 1e-05, gs = 1
I0512 09:00:27.970585   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:01:08.162022   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:01:56.319124   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:02:36.146018   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:03:12.089181   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:03:52.189498   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:04:08.930269   380 solver.cpp:637] Iteration 16500, Testing net (#0)
I0512 09:04:31.769407   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:04:35.871034   380 solver.cpp:749] class AP 1: 0.894414
I0512 09:04:35.872736   380 solver.cpp:749] class AP 2: 0.881759
I0512 09:04:35.873374   380 solver.cpp:749] class AP 3: 0.901836
I0512 09:04:35.873437   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.892669
I0512 09:04:35.873618   380 solver.cpp:284] Tests completed in 266.386s
I0512 09:04:36.761648   380 solver.cpp:354] Iteration 16500 (0.375396 iter/s, 266.386s/100 iter), 931.8/1129.4ep, loss = 2.44951
I0512 09:04:36.761726   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29817 (* 1 = 2.29817 loss)
I0512 09:04:36.761777   380 sgd_solver.cpp:172] Iteration 16500, lr = 9.37891e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:04:47.350136   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:05:31.553633   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:06:06.055147   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:06:49.699376   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:07:30.675356   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:08:09.775640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:08:11.336812   380 solver.cpp:354] Iteration 16600 (0.466038 iter/s, 214.575s/100 iter), 937.4/1129.4ep, loss = 2.29924
I0512 09:08:11.337090   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25949 (* 1 = 2.25949 loss)
I0512 09:08:11.337174   380 sgd_solver.cpp:172] Iteration 16600, lr = 8.3521e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:08:58.117007   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:09:31.857744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:10:24.973925   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:11:02.719388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:11:46.422344   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:12:08.214195   380 solver.cpp:354] Iteration 16700 (0.422159 iter/s, 236.877s/100 iter), 943.1/1129.4ep, loss = 2.34044
I0512 09:12:08.214231   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.25168 (* 1 = 2.25168 loss)
I0512 09:12:08.214241   380 sgd_solver.cpp:172] Iteration 16700, lr = 7.41201e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:12:25.287760   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:12:59.633983   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:13:44.129186   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:14:23.907331   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:15:17.726172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:15:56.456158   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:16:07.331291   380 solver.cpp:354] Iteration 16800 (0.418205 iter/s, 239.117s/100 iter), 948.7/1129.4ep, loss = 2.46709
I0512 09:16:07.331331   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.51219 (* 1 = 2.51219 loss)
I0512 09:16:07.331341   380 sgd_solver.cpp:172] Iteration 16800, lr = 6.5536e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:16:38.833515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:17:20.532500   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:17:56.674775   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:18:43.436904   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:19:24.144241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:20:00.027390   380 solver.cpp:354] Iteration 16900 (0.429745 iter/s, 232.696s/100 iter), 954.4/1129.4ep, loss = 2.38701
I0512 09:20:00.028015   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.55418 (* 1 = 2.55418 loss)
I0512 09:20:00.028208   380 sgd_solver.cpp:172] Iteration 16900, lr = 5.772e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:20:04.087844   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:20:42.352347   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:21:36.486074   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:22:16.480111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:22:49.602617   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:23:33.124745   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:23:57.040663   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_17000.caffemodel
I0512 09:23:57.119530   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_17000.solverstate
I0512 09:23:57.178508   380 solver.cpp:637] Iteration 17000, Testing net (#0)
I0512 09:24:09.690515   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:24:18.671679   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:24:22.396333   380 solver.cpp:749] class AP 1: 0.893847
I0512 09:24:22.397408   380 solver.cpp:749] class AP 2: 0.883152
I0512 09:24:22.397742   380 solver.cpp:749] class AP 3: 0.902081
I0512 09:24:22.397748   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893027
I0512 09:24:22.397776   380 solver.cpp:284] Tests completed in 262.37s
I0512 09:24:22.997289   380 solver.cpp:354] Iteration 17000 (0.381141 iter/s, 262.37s/100 iter), 960/1129.4ep, loss = 2.37627
I0512 09:24:22.997375   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.27167 (* 1 = 2.27167 loss)
I0512 09:24:22.997406   380 sgd_solver.cpp:172] Iteration 17000, lr = 5.0625e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:25:11.451094   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:25:48.409762   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:26:32.423051   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:15.355129   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:52.065673   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:27:58.965366   380 solver.cpp:354] Iteration 17100 (0.463032 iter/s, 215.968s/100 iter), 965.6/1129.4ep, loss = 2.53495
I0512 09:27:58.965456   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.94788 (* 1 = 2.94788 loss)
I0512 09:27:58.965487   380 sgd_solver.cpp:172] Iteration 17100, lr = 4.4205e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:28:31.583958   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:29:08.700513   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:29:57.060436   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:30:36.301640   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:31:22.720608   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:32:01.204000   380 solver.cpp:354] Iteration 17200 (0.412816 iter/s, 242.238s/100 iter), 971.3/1129.4ep, loss = 2.50831
I0512 09:32:01.204064   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19645 (* 1 = 2.19645 loss)
I0512 09:32:01.204077   380 sgd_solver.cpp:172] Iteration 17200, lr = 3.8416e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:32:05.907874   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:32:43.711859   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:33:35.113812   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:34:11.718183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:34:54.357329   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:35:31.596019   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:35:51.828601   380 solver.cpp:354] Iteration 17300 (0.433605 iter/s, 230.624s/100 iter), 976.9/1129.4ep, loss = 2.41484
I0512 09:35:51.828850   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.32872 (* 1 = 2.32872 loss)
I0512 09:35:51.828946   380 sgd_solver.cpp:172] Iteration 17300, lr = 3.32151e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:36:04.833793   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:36:43.360210   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:37:19.799355   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:38:11.991829   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:38:48.852129   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:39:44.566401   380 solver.cpp:354] Iteration 17400 (0.429668 iter/s, 232.738s/100 iter), 982.6/1129.4ep, loss = 2.41857
I0512 09:39:44.566473   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.68868 (* 1 = 2.68868 loss)
I0512 09:39:44.566483   380 sgd_solver.cpp:172] Iteration 17400, lr = 2.8561e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:39:45.958361   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:40:19.623339   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:40:58.618078   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:41:38.603543   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:42:15.619128   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:42:55.659443   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:30.294371   380 solver.cpp:637] Iteration 17500, Testing net (#0)
I0512 09:43:34.333338   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:55.230094   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:43:59.798846   380 solver.cpp:749] class AP 1: 0.894015
I0512 09:43:59.799955   380 solver.cpp:749] class AP 2: 0.883818
I0512 09:43:59.800318   380 solver.cpp:749] class AP 3: 0.902187
I0512 09:43:59.800331   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89334
I0512 09:43:59.800367   380 solver.cpp:284] Tests completed in 255.233s
I0512 09:44:00.401837   380 solver.cpp:354] Iteration 17500 (0.391799 iter/s, 255.233s/100 iter), 988.2/1129.4ep, loss = 2.41114
I0512 09:44:00.401960   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.34293 (* 1 = 2.34293 loss)
I0512 09:44:00.401973   380 sgd_solver.cpp:172] Iteration 17500, lr = 2.44141e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:44:31.086401   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:45:10.028558   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:45:58.613703   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:46:46.429435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:47:20.131259   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:47:36.915020   380 solver.cpp:354] Iteration 17600 (0.461868 iter/s, 216.512s/100 iter), 993.9/1129.4ep, loss = 2.34302
I0512 09:47:36.915184   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19788 (* 1 = 2.19788 loss)
I0512 09:47:36.915230   380 sgd_solver.cpp:172] Iteration 17600, lr = 2.0736e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:48:01.992120   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:48:37.623883   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:49:13.399333   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:49:50.513161   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:50:32.550293   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:51:18.771561   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:51:32.709905   380 solver.cpp:354] Iteration 17700 (0.424099 iter/s, 235.794s/100 iter), 999.5/1129.4ep, loss = 2.38956
I0512 09:51:32.710103   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.41128 (* 1 = 2.41128 loss)
I0512 09:51:32.710168   380 sgd_solver.cpp:172] Iteration 17700, lr = 1.74901e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:51:54.833321   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:52:55.531157   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:53:35.126605   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:54:12.918452   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:54:51.484804   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:55:26.470865   380 solver.cpp:354] Iteration 17800 (0.427789 iter/s, 233.76s/100 iter), 1005.2/1129.4ep, loss = 2.5572
I0512 09:55:26.471468   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.48266 (* 1 = 2.48266 loss)
I0512 09:55:26.471725   380 sgd_solver.cpp:172] Iteration 17800, lr = 1.4641e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:55:31.733242   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:56:12.667940   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:56:52.708580   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:57:40.627449   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:58:15.290505   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:59:12.145421   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 09:59:20.464395   380 solver.cpp:354] Iteration 17900 (0.427363 iter/s, 233.993s/100 iter), 1010.8/1129.4ep, loss = 2.30859
I0512 09:59:20.464648   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.45775 (* 1 = 2.45775 loss)
I0512 09:59:20.464715   380 sgd_solver.cpp:172] Iteration 17900, lr = 1.21551e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 09:59:49.537147   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:00:23.347481   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:01:06.467250   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:01:46.742223   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:02:27.568100   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:03.545318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:04.961740   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_18000.caffemodel
I0512 10:03:04.999786   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_18000.solverstate
I0512 10:03:05.038211   380 solver.cpp:637] Iteration 18000, Testing net (#0)
I0512 10:03:31.070262   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:03:35.863893   380 solver.cpp:749] class AP 1: 0.894205
I0512 10:03:35.864966   380 solver.cpp:749] class AP 2: 0.883221
I0512 10:03:35.865311   380 solver.cpp:749] class AP 3: 0.902125
I0512 10:03:35.865319   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893184
I0512 10:03:35.865352   380 solver.cpp:284] Tests completed in 255.4s
I0512 10:03:36.538617   380 solver.cpp:354] Iteration 18000 (0.391542 iter/s, 255.4s/100 iter), 1016.5/1129.4ep, loss = 2.364
I0512 10:03:36.538779   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72345 (* 1 = 2.72345 loss)
I0512 10:03:36.538792   380 sgd_solver.cpp:172] Iteration 18000, lr = 1e-07, m = 0.9, wd = 1e-05, gs = 1
I0512 10:03:36.541278   380 solver.cpp:391] Sparsity after update:
I0512 10:03:36.543077   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 10:03:36.543113   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 10:03:36.543145   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 10:03:36.543171   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543198   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543226   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 10:03:36.543251   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543277   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543303   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 10:03:36.543329   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543355   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543380   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 10:03:36.543407   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543433   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543459   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 10:03:36.543485   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543511   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543537   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 10:03:36.543563   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 10:03:36.543591   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 10:03:36.543617   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 10:03:36.543642   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 10:03:36.543668   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 10:03:36.543695   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 10:03:36.543731   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 10:03:36.543757   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 10:03:36.543782   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 10:03:36.543808   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 10:03:36.543833   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 10:03:36.543859   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 10:03:53.986996   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:04:34.130010   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:05:15.419371   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:05:55.690867   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:06:35.354212   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:07:06.036717   380 solver.cpp:354] Iteration 18100 (0.477332 iter/s, 209.498s/100 iter), 1022.1/1129.4ep, loss = 2.37254
I0512 10:07:06.037008   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.33276 (* 1 = 2.33276 loss)
I0512 10:07:06.037099   380 sgd_solver.cpp:172] Iteration 18100, lr = 8.14507e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:07:30.109714   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:08:10.033643   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:08:56.287663   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:09:36.800096   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:10:15.656172   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:11:00.226097   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:11:18.488560   380 solver.cpp:354] Iteration 18200 (0.396116 iter/s, 252.451s/100 iter), 1027.8/1129.4ep, loss = 2.41399
I0512 10:11:18.488835   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.35256 (* 1 = 2.35256 loss)
I0512 10:11:18.488921   380 sgd_solver.cpp:172] Iteration 18200, lr = 6.56099e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:11:37.381693   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:12:27.789412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:13:07.988039   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:13:57.189648   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:14:37.208496   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:15:16.227200   380 solver.cpp:354] Iteration 18300 (0.420631 iter/s, 237.738s/100 iter), 1033.4/1129.4ep, loss = 2.35758
I0512 10:15:16.271337   380 solver.cpp:378]     Train net output #0: mbox_loss = 1.95332 (* 1 = 1.95332 loss)
I0512 10:15:16.271955   380 sgd_solver.cpp:172] Iteration 18300, lr = 5.22006e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:15:16.977077   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:15:58.747262   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:16:38.960530   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:17:18.091311   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:17:56.294893   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:18:41.618063   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:19:08.334026   380 solver.cpp:354] Iteration 18400 (0.430834 iter/s, 232.108s/100 iter), 1039.1/1129.4ep, loss = 2.42412
I0512 10:19:08.334183   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.63416 (* 1 = 2.63416 loss)
I0512 10:19:08.334226   380 sgd_solver.cpp:172] Iteration 18400, lr = 4.096e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:19:23.928412   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:20:11.579933   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:20:55.875021   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:21:32.531742   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:22:21.297595   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:22:58.824362   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:23:02.914670   380 solver.cpp:637] Iteration 18500, Testing net (#0)
I0512 10:23:27.618016   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:23:32.076793   380 solver.cpp:749] class AP 1: 0.894244
I0512 10:23:32.078614   380 solver.cpp:749] class AP 2: 0.883184
I0512 10:23:32.079195   380 solver.cpp:749] class AP 3: 0.902082
I0512 10:23:32.079210   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89317
I0512 10:23:32.079277   380 solver.cpp:284] Tests completed in 263.747s
I0512 10:23:32.924772   380 solver.cpp:354] Iteration 18500 (0.379152 iter/s, 263.747s/100 iter), 1044.7/1129.4ep, loss = 2.42639
I0512 10:23:32.924883   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.13619 (* 1 = 2.13619 loss)
I0512 10:23:32.924927   380 sgd_solver.cpp:172] Iteration 18500, lr = 3.16406e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:23:41.474529   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:24:24.268353   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:25:02.933275   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:25:50.584750   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:26:25.249871   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:27:14.985824   380 solver.cpp:354] Iteration 18600 (0.450325 iter/s, 222.062s/100 iter), 1050.4/1129.4ep, loss = 2.50778
I0512 10:27:14.986133   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.72977 (* 1 = 2.72977 loss)
I0512 10:27:14.986200   380 sgd_solver.cpp:172] Iteration 18600, lr = 2.401e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:27:19.936694   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:27:57.307183   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:28:42.810482   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:29:24.149341   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:30:02.309686   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:30:39.263618   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:31:14.248764   380 solver.cpp:354] Iteration 18700 (0.417949 iter/s, 239.263s/100 iter), 1056/1129.4ep, loss = 2.36643
I0512 10:31:14.249168   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19184 (* 1 = 2.19184 loss)
I0512 10:31:14.249269   380 sgd_solver.cpp:172] Iteration 18700, lr = 1.78506e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:31:20.953889   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:32:01.320109   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:32:39.821368   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:33:27.452232   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:34:10.262470   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:34:48.508263   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:35:04.294538   380 solver.cpp:354] Iteration 18800 (0.434696 iter/s, 230.046s/100 iter), 1061.6/1129.4ep, loss = 2.50304
I0512 10:35:04.295138   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.9241 (* 1 = 2.9241 loss)
I0512 10:35:04.295331   380 sgd_solver.cpp:172] Iteration 18800, lr = 1.296e-08, m = 0.9, wd = 1e-05, gs = 1
I0512 10:35:40.580126   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:36:18.823345   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:36:58.233330   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:37:38.596127   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:38:24.585521   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:39:01.600636   380 solver.cpp:354] Iteration 18900 (0.421396 iter/s, 237.306s/100 iter), 1067.3/1129.4ep, loss = 2.45993
I0512 10:39:01.600919   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17704 (* 1 = 2.17704 loss)
I0512 10:39:01.601016   380 sgd_solver.cpp:172] Iteration 18900, lr = 9.15063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:39:06.669787   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:39:43.263815   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:40:35.636435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:41:15.616729   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:42:08.310307   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:42:44.338111   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:43:00.499802   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_19000.caffemodel
I0512 10:43:00.522817   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_19000.solverstate
I0512 10:43:00.546180   380 solver.cpp:637] Iteration 19000, Testing net (#0)
I0512 10:43:24.714201   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:43:28.755618   380 solver.cpp:749] class AP 1: 0.893837
I0512 10:43:28.756716   380 solver.cpp:749] class AP 2: 0.882615
I0512 10:43:28.757050   380 solver.cpp:749] class AP 3: 0.901859
I0512 10:43:28.757056   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89277
I0512 10:43:28.757083   380 solver.cpp:284] Tests completed in 267.157s
I0512 10:43:29.377202   380 solver.cpp:354] Iteration 19000 (0.374312 iter/s, 267.157s/100 iter), 1072.9/1129.4ep, loss = 2.3911
I0512 10:43:29.377315   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.45187 (* 1 = 2.45187 loss)
I0512 10:43:29.377348   380 sgd_solver.cpp:172] Iteration 19000, lr = 6.25001e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:43:32.705907   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:44:16.508692   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:44:51.424710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:45:31.700912   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:46:08.219813   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:46:53.901805   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:47:03.135761   380 solver.cpp:354] Iteration 19100 (0.46782 iter/s, 213.757s/100 iter), 1078.6/1129.4ep, loss = 2.47819
I0512 10:47:03.135973   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.73547 (* 1 = 2.73547 loss)
I0512 10:47:03.136044   380 sgd_solver.cpp:172] Iteration 19100, lr = 4.10063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:47:33.655812   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:48:22.380635   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:49:04.559005   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:49:43.341435   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:50:35.243722   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:51:07.066197   380 solver.cpp:354] Iteration 19200 (0.409953 iter/s, 243.93s/100 iter), 1084.2/1129.4ep, loss = 2.46651
I0512 10:51:07.066262   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.66086 (* 1 = 2.66086 loss)
I0512 10:51:07.066273   380 sgd_solver.cpp:172] Iteration 19200, lr = 2.56001e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:51:15.088189   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:51:55.549378   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:52:34.900723   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:53:11.360589   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:53:53.550539   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:54:33.451581   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:54:59.043887   380 solver.cpp:354] Iteration 19300 (0.431076 iter/s, 231.978s/100 iter), 1089.9/1129.4ep, loss = 2.5041
I0512 10:54:59.044142   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.31234 (* 1 = 2.31234 loss)
I0512 10:54:59.044237   380 sgd_solver.cpp:172] Iteration 19300, lr = 1.50063e-09, m = 0.9, wd = 1e-05, gs = 1
I0512 10:55:16.332653   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:55:52.994753   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:56:46.064904   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:57:23.968318   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:58:07.646241   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:58:53.409446   380 solver.cpp:354] Iteration 19400 (0.426684 iter/s, 234.366s/100 iter), 1095.5/1129.4ep, loss = 2.36279
I0512 10:58:53.409543   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.44068 (* 1 = 2.44068 loss)
I0512 10:58:53.409570   380 sgd_solver.cpp:172] Iteration 19400, lr = 8.09997e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 10:58:53.814441   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 10:59:31.142905   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:00:11.775565   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:00:51.498445   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:01:34.773409   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:02:15.129990   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:02:46.427273   380 solver.cpp:637] Iteration 19500, Testing net (#0)
I0512 11:02:57.369012   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:03:12.871645   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:03:18.023680   380 solver.cpp:749] class AP 1: 0.893981
I0512 11:03:18.024809   380 solver.cpp:749] class AP 2: 0.878386
I0512 11:03:18.025156   380 solver.cpp:749] class AP 3: 0.901904
I0512 11:03:18.025166   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.891424
I0512 11:03:18.025200   380 solver.cpp:284] Tests completed in 264.616s
I0512 11:03:18.631800   380 solver.cpp:354] Iteration 19500 (0.377906 iter/s, 264.616s/100 iter), 1101.2/1129.4ep, loss = 2.40557
I0512 11:03:18.631891   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.19637 (* 1 = 2.19637 loss)
I0512 11:03:18.631924   380 sgd_solver.cpp:172] Iteration 19500, lr = 3.90624e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 11:03:50.350059   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:04:31.066613   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:05:22.685827   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:05:59.506305   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:06:46.420635   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:07:00.657367   380 solver.cpp:354] Iteration 19600 (0.450399 iter/s, 222.026s/100 iter), 1106.8/1129.4ep, loss = 2.5909
I0512 11:07:00.657501   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.86336 (* 1 = 2.86336 loss)
I0512 11:07:00.657531   380 sgd_solver.cpp:172] Iteration 19600, lr = 1.59999e-10, m = 0.9, wd = 1e-05, gs = 1
I0512 11:07:26.213730   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:08:05.588981   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:08:48.882877   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:09:28.033928   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:15.373303   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:52.054744   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:10:54.259781   380 solver.cpp:354] Iteration 19700 (0.428078 iter/s, 233.602s/100 iter), 1112.5/1129.4ep, loss = 2.26548
I0512 11:10:54.259948   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.14124 (* 1 = 2.14124 loss)
I0512 11:10:54.259999   380 sgd_solver.cpp:172] Iteration 19700, lr = 5.06248e-11, m = 0.9, wd = 1e-05, gs = 1
I0512 11:11:34.855651   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:12:19.291388   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:12:57.851460   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:13:41.991153   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:14:18.974550   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:14:50.000511   380 solver.cpp:354] Iteration 19800 (0.424195 iter/s, 235.741s/100 iter), 1118.1/1129.4ep, loss = 2.31879
I0512 11:14:50.000788   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.17575 (* 1 = 2.17575 loss)
I0512 11:14:50.000876   380 sgd_solver.cpp:172] Iteration 19800, lr = 9.99996e-12, m = 0.9, wd = 1e-05, gs = 1
I0512 11:15:05.379460   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:15:41.667323   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:16:32.575320   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:17:11.074219   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:17:49.139986   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:18:33.495342   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:18:45.050544   380 solver.cpp:354] Iteration 19900 (0.425441 iter/s, 235.05s/100 iter), 1123.8/1129.4ep, loss = 2.4196
I0512 11:18:45.051076   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.29033 (* 1 = 2.29033 loss)
I0512 11:18:45.051331   380 sgd_solver.cpp:172] Iteration 19900, lr = 6.24998e-13, m = 0.9, wd = 1e-05, gs = 1
I0512 11:19:08.698869   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:19:57.257997   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:20:36.218704   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:21:14.165791   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:21:55.740710   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:22:40.995616   386 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:22:41.113509   380 solver.cpp:354] Iteration 19999 (0.41938 iter/s, 236.063s/99 iter), 1129.4/1129.4ep, loss = 2.47274
I0512 11:22:41.114009   380 solver.cpp:378]     Train net output #0: mbox_loss = 2.3165 (* 1 = 2.3165 loss)
I0512 11:22:41.115195   380 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.caffemodel
I0512 11:22:41.173271   380 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.solverstate
I0512 11:22:41.241598   380 solver.cpp:421] Sparsity after training:
I0512 11:22:41.251746   380 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 11:22:41.252666   380 net.cpp:2780] conv1a_param_0(0.343) 
I0512 11:22:41.252876   380 net.cpp:2780] conv1b_param_0(0.674) 
I0512 11:22:41.253069   380 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 11:22:41.253253   380 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 11:22:41.253446   380 net.cpp:2780] ctx_output1_param_0(0) 
I0512 11:22:41.253631   380 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 11:22:41.253814   380 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 11:22:41.253998   380 net.cpp:2780] ctx_output2_param_0(0) 
I0512 11:22:41.254182   380 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 11:22:41.254364   380 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 11:22:41.254549   380 net.cpp:2780] ctx_output3_param_0(0) 
I0512 11:22:41.254734   380 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 11:22:41.254920   380 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 11:22:41.255105   380 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 11:22:41.255288   380 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 11:22:41.255466   380 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 11:22:41.255650   380 net.cpp:2780] ctx_output5_param_0(0) 
I0512 11:22:41.255831   380 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 11:22:41.256014   380 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 11:22:41.256202   380 net.cpp:2780] ctx_output6_param_0(0) 
I0512 11:22:41.256395   380 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 11:22:41.256578   380 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 11:22:41.256772   380 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 11:22:41.256953   380 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 11:22:41.257115   380 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 11:22:41.257279   380 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 11:22:41.257448   380 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 11:22:41.257609   380 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 11:22:41.257769   380 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 11:22:42.144740   380 solver.cpp:503] Iteration 20000, loss = 2.44613
I0512 11:22:42.157303   380 solver.cpp:637] Iteration 20000, Testing net (#0)
I0512 11:23:04.479737   423 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:23:05.758611   422 blocking_queue.cpp:40] Data layer prefetch queue empty
I0512 11:23:08.859124   380 solver.cpp:749] class AP 1: 0.894072
I0512 11:23:08.860196   380 solver.cpp:749] class AP 2: 0.883149
I0512 11:23:08.860532   380 solver.cpp:749] class AP 3: 0.901858
I0512 11:23:08.860538   380 solver.cpp:755] Test net output mAP #0: detection_eval = 0.893027
I0512 11:23:08.860563   380 caffe.cpp:268] Solver performance on device 0: 0.4086 * 48 = 19.61 img/sec (20000 itr in 4.894e+04 sec)
I0512 11:23:08.860569   380 caffe.cpp:271] Optimization Done in 13h 37m 26s
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument
*** Aborted at 1589282588 (unix time) try "date -d @1589282588" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x17c) received by PID 380 (TID 0x7f2fc7fff700) from PID 380; stack trace: ***
    @     0x7f32025c1f20 (unknown)
    @     0x7f32025c1e97 gsignal
    @     0x7f32025c3801 abort
    @     0x7f32066cd84a __gnu_cxx::__verbose_terminate_handler()
    @     0x7f32066cbf47 __cxxabiv1::__terminate()
    @     0x7f32066cbf7d std::terminate()
    @     0x7f32066cc15a __cxa_throw
    @     0x7f32041720ac boost::throw_exception<>()
    @     0x7f320417219d boost::mutex::lock()
    @     0x7f3204173020 boost::unique_lock<>::lock()
    @     0x7f320460a89f caffe::BlockingQueue<>::push()
    @     0x7f32041f149e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7f320422b476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7f320419f7ee caffe::InternalThread::entry()
    @     0x7f32041a154b boost::detail::thread_data<>::run()
    @     0x7f32037d97ee thread_proxy
    @     0x7f320236b6db start_thread
    @     0x7f32026a488f clone
    @                0x0 (unknown)
I0512 11:23:09.887946   498 caffe.cpp:902] This is NVCaffe 0.17.0 started at Tue May 12 11:23:09 2020
I0512 11:23:10.311076   498 caffe.cpp:904] CuDNN version: 7605
I0512 11:23:10.311081   498 caffe.cpp:905] CuBLAS version: 10202
I0512 11:23:10.311084   498 caffe.cpp:906] CUDA version: 10020
I0512 11:23:10.311087   498 caffe.cpp:907] CUDA driver version: 10020
I0512 11:23:10.311089   498 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: test_detection
[2]: --model=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test/test.prototxt
[3]: --iterations=85
[4]: --display_sparsity=1
[5]: --weights=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/sparse/EYES_ssdJacintoNetV2_iter_20000.caffemodel
[6]: --gpu
[7]: 0
I0512 11:23:10.332355   498 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0512 11:23:10.332383   498 gpu_memory.cpp:107] Total memory: 16900227072, Free: 16697655296, dev_info[0]: total=16900227072 free=16697655296
I0512 11:23:10.332581   498 caffe.cpp:406] Use GPU with device ID 0
I0512 11:23:10.332700   498 caffe.cpp:409] GPU device name: Quadro RTX 5000
I0512 11:23:10.365586   498 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 10
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0512 11:23:10.366556   498 net.cpp:110] Using FLOAT as default forward math type
I0512 11:23:10.366577   498 net.cpp:116] Using FLOAT as default backward math type
I0512 11:23:10.366587   498 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0512 11:23:10.366595   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:10.366739   498 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:10.366888   498 net.cpp:200] Created Layer data (0)
I0512 11:23:10.367110   498 net.cpp:542] data -> data
I0512 11:23:10.367125   503 blocking_queue.cpp:40] Data layer prefetch queue empty
I0512 11:23:10.367146   498 net.cpp:542] data -> label
I0512 11:23:10.367167   498 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 10
I0512 11:23:10.367187   498 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:10.367527   504 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0512 11:23:10.370656   498 annotated_data_layer.cpp:105] output data size: 10,3,320,768
I0512 11:23:10.370736   498 annotated_data_layer.cpp:150] (0) Output data size: 10, 3, 320, 768
I0512 11:23:10.370787   498 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:10.370883   498 net.cpp:260] Setting up data
I0512 11:23:10.370893   498 net.cpp:267] TEST Top shape for layer 0 'data' 10 3 320 768 (7372800)
I0512 11:23:10.371083   498 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0512 11:23:10.371083   505 data_layer.cpp:105] (0) Parser threads: 1
I0512 11:23:10.371100   498 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0512 11:23:10.371101   505 data_layer.cpp:107] (0) Transformer threads: 1
I0512 11:23:10.371110   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:10.371126   498 net.cpp:200] Created Layer data_data_0_split (1)
I0512 11:23:10.371136   498 net.cpp:572] data_data_0_split <- data
I0512 11:23:10.371158   498 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0512 11:23:10.371170   498 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0512 11:23:10.371179   498 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0512 11:23:10.371187   498 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0512 11:23:10.371197   498 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0512 11:23:10.371206   498 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0512 11:23:10.371212   498 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0512 11:23:10.371356   498 net.cpp:260] Setting up data_data_0_split
I0512 11:23:10.371366   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371376   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371385   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371393   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371402   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371412   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371421   498 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:10.371443   498 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0512 11:23:10.371450   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:10.371466   498 net.cpp:200] Created Layer data/bias (2)
I0512 11:23:10.371474   498 net.cpp:572] data/bias <- data_data_0_split_0
I0512 11:23:10.371481   498 net.cpp:542] data/bias -> data/bias
I0512 11:23:10.371647   498 net.cpp:260] Setting up data/bias
I0512 11:23:10.371655   498 net.cpp:267] TEST Top shape for layer 2 'data/bias' 10 3 320 768 (7372800)
I0512 11:23:10.371682   498 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0512 11:23:10.371687   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:10.371711   498 net.cpp:200] Created Layer conv1a (3)
I0512 11:23:10.371718   498 net.cpp:572] conv1a <- data/bias
I0512 11:23:10.371726   498 net.cpp:542] conv1a -> conv1a
I0512 11:23:11.283793   498 net.cpp:260] Setting up conv1a
I0512 11:23:11.283818   498 net.cpp:267] TEST Top shape for layer 3 'conv1a' 10 32 160 384 (19660800)
I0512 11:23:11.283843   498 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0512 11:23:11.283848   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.283864   498 net.cpp:200] Created Layer conv1a/bn (4)
I0512 11:23:11.283869   498 net.cpp:572] conv1a/bn <- conv1a
I0512 11:23:11.283875   498 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0512 11:23:11.284266   498 net.cpp:260] Setting up conv1a/bn
I0512 11:23:11.284276   498 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 10 32 160 384 (19660800)
I0512 11:23:11.284291   498 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0512 11:23:11.284299   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.284308   498 net.cpp:200] Created Layer conv1a/relu (5)
I0512 11:23:11.284317   498 net.cpp:572] conv1a/relu <- conv1a
I0512 11:23:11.284322   498 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0512 11:23:11.284344   498 net.cpp:260] Setting up conv1a/relu
I0512 11:23:11.284369   498 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 10 32 160 384 (19660800)
I0512 11:23:11.284377   498 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0512 11:23:11.284384   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.284400   498 net.cpp:200] Created Layer conv1b (6)
I0512 11:23:11.284407   498 net.cpp:572] conv1b <- conv1a
I0512 11:23:11.284413   498 net.cpp:542] conv1b -> conv1b
I0512 11:23:11.284821   498 net.cpp:260] Setting up conv1b
I0512 11:23:11.284831   498 net.cpp:267] TEST Top shape for layer 6 'conv1b' 10 32 160 384 (19660800)
I0512 11:23:11.284842   498 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0512 11:23:11.284847   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.284857   498 net.cpp:200] Created Layer conv1b/bn (7)
I0512 11:23:11.284863   498 net.cpp:572] conv1b/bn <- conv1b
I0512 11:23:11.284869   498 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0512 11:23:11.285177   498 net.cpp:260] Setting up conv1b/bn
I0512 11:23:11.285184   498 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 10 32 160 384 (19660800)
I0512 11:23:11.285194   498 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0512 11:23:11.285200   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.285207   498 net.cpp:200] Created Layer conv1b/relu (8)
I0512 11:23:11.285212   498 net.cpp:572] conv1b/relu <- conv1b
I0512 11:23:11.285218   498 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0512 11:23:11.285225   498 net.cpp:260] Setting up conv1b/relu
I0512 11:23:11.285231   498 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 10 32 160 384 (19660800)
I0512 11:23:11.285257   498 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0512 11:23:11.285264   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.285274   498 net.cpp:200] Created Layer pool1 (9)
I0512 11:23:11.285279   498 net.cpp:572] pool1 <- conv1b
I0512 11:23:11.285285   498 net.cpp:542] pool1 -> pool1
I0512 11:23:11.285362   498 net.cpp:260] Setting up pool1
I0512 11:23:11.285367   498 net.cpp:267] TEST Top shape for layer 9 'pool1' 10 32 80 192 (4915200)
I0512 11:23:11.285374   498 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0512 11:23:11.285378   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.285390   498 net.cpp:200] Created Layer res2a_branch2a (10)
I0512 11:23:11.285398   498 net.cpp:572] res2a_branch2a <- pool1
I0512 11:23:11.285403   498 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0512 11:23:11.286469   498 net.cpp:260] Setting up res2a_branch2a
I0512 11:23:11.286479   498 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 10 64 80 192 (9830400)
I0512 11:23:11.286491   498 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:11.286496   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.286506   498 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0512 11:23:11.286514   498 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0512 11:23:11.286520   498 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0512 11:23:11.286797   498 net.cpp:260] Setting up res2a_branch2a/bn
I0512 11:23:11.286803   498 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 10 64 80 192 (9830400)
I0512 11:23:11.286813   498 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0512 11:23:11.286818   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.286825   498 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0512 11:23:11.286832   498 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0512 11:23:11.286839   498 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0512 11:23:11.286845   498 net.cpp:260] Setting up res2a_branch2a/relu
I0512 11:23:11.286849   498 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 10 64 80 192 (9830400)
I0512 11:23:11.286856   498 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0512 11:23:11.286864   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.286877   498 net.cpp:200] Created Layer res2a_branch2b (13)
I0512 11:23:11.286886   498 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0512 11:23:11.286895   498 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0512 11:23:11.287192   498 net.cpp:260] Setting up res2a_branch2b
I0512 11:23:11.287202   498 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 10 64 80 192 (9830400)
I0512 11:23:11.287217   498 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:11.287225   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.287235   498 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0512 11:23:11.287243   498 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0512 11:23:11.287253   498 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0512 11:23:11.287555   498 net.cpp:260] Setting up res2a_branch2b/bn
I0512 11:23:11.287565   498 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 10 64 80 192 (9830400)
I0512 11:23:11.287585   498 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0512 11:23:11.287592   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.287600   498 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0512 11:23:11.287607   498 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0512 11:23:11.287631   498 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0512 11:23:11.287642   498 net.cpp:260] Setting up res2a_branch2b/relu
I0512 11:23:11.287649   498 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 10 64 80 192 (9830400)
I0512 11:23:11.287660   498 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0512 11:23:11.287667   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.287678   498 net.cpp:200] Created Layer pool2 (16)
I0512 11:23:11.287689   498 net.cpp:572] pool2 <- res2a_branch2b
I0512 11:23:11.287698   498 net.cpp:542] pool2 -> pool2
I0512 11:23:11.287748   498 net.cpp:260] Setting up pool2
I0512 11:23:11.287755   498 net.cpp:267] TEST Top shape for layer 16 'pool2' 10 64 40 96 (2457600)
I0512 11:23:11.287761   498 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0512 11:23:11.287765   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.287777   498 net.cpp:200] Created Layer res3a_branch2a (17)
I0512 11:23:11.287783   498 net.cpp:572] res3a_branch2a <- pool2
I0512 11:23:11.287788   498 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0512 11:23:11.288780   498 net.cpp:260] Setting up res3a_branch2a
I0512 11:23:11.288789   498 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 10 128 40 96 (4915200)
I0512 11:23:11.288800   498 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:11.288806   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.288816   498 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0512 11:23:11.288823   498 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0512 11:23:11.288830   498 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0512 11:23:11.289059   498 net.cpp:260] Setting up res3a_branch2a/bn
I0512 11:23:11.289064   498 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 10 128 40 96 (4915200)
I0512 11:23:11.289077   498 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0512 11:23:11.289085   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.289091   498 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0512 11:23:11.289098   498 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0512 11:23:11.289104   498 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0512 11:23:11.289113   498 net.cpp:260] Setting up res3a_branch2a/relu
I0512 11:23:11.289115   498 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 10 128 40 96 (4915200)
I0512 11:23:11.289122   498 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0512 11:23:11.289130   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.289139   498 net.cpp:200] Created Layer res3a_branch2b (20)
I0512 11:23:11.289145   498 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0512 11:23:11.289150   498 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0512 11:23:11.289723   498 net.cpp:260] Setting up res3a_branch2b
I0512 11:23:11.289731   498 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 10 128 40 96 (4915200)
I0512 11:23:11.289739   498 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:11.289743   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.289753   498 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0512 11:23:11.289760   498 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0512 11:23:11.289767   498 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0512 11:23:11.289994   498 net.cpp:260] Setting up res3a_branch2b/bn
I0512 11:23:11.290000   498 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 10 128 40 96 (4915200)
I0512 11:23:11.290009   498 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0512 11:23:11.290026   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.290032   498 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0512 11:23:11.290038   498 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0512 11:23:11.290045   498 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0512 11:23:11.290051   498 net.cpp:260] Setting up res3a_branch2b/relu
I0512 11:23:11.290057   498 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 10 128 40 96 (4915200)
I0512 11:23:11.290067   498 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0512 11:23:11.290076   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.290082   498 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0512 11:23:11.290086   498 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0512 11:23:11.290091   498 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0512 11:23:11.290099   498 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0512 11:23:11.290133   498 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0512 11:23:11.290138   498 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 10 128 40 96 (4915200)
I0512 11:23:11.290143   498 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 10 128 40 96 (4915200)
I0512 11:23:11.290149   498 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0512 11:23:11.290154   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.290160   498 net.cpp:200] Created Layer pool3 (24)
I0512 11:23:11.290189   498 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0512 11:23:11.290199   498 net.cpp:542] pool3 -> pool3
I0512 11:23:11.290241   498 net.cpp:260] Setting up pool3
I0512 11:23:11.290246   498 net.cpp:267] TEST Top shape for layer 24 'pool3' 10 128 20 48 (1228800)
I0512 11:23:11.290253   498 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0512 11:23:11.290258   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.290271   498 net.cpp:200] Created Layer res4a_branch2a (25)
I0512 11:23:11.290279   498 net.cpp:572] res4a_branch2a <- pool3
I0512 11:23:11.290285   498 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0512 11:23:11.294215   498 net.cpp:260] Setting up res4a_branch2a
I0512 11:23:11.294226   498 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 10 256 20 48 (2457600)
I0512 11:23:11.294239   498 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:11.294250   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.294258   498 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0512 11:23:11.294266   498 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0512 11:23:11.294272   498 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0512 11:23:11.294528   498 net.cpp:260] Setting up res4a_branch2a/bn
I0512 11:23:11.294534   498 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 10 256 20 48 (2457600)
I0512 11:23:11.294544   498 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0512 11:23:11.294553   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.294560   498 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0512 11:23:11.294569   498 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0512 11:23:11.294574   498 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0512 11:23:11.294584   498 net.cpp:260] Setting up res4a_branch2a/relu
I0512 11:23:11.294589   498 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 10 256 20 48 (2457600)
I0512 11:23:11.294611   498 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0512 11:23:11.294620   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.294634   498 net.cpp:200] Created Layer res4a_branch2b (28)
I0512 11:23:11.294641   498 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0512 11:23:11.294647   498 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0512 11:23:11.296396   498 net.cpp:260] Setting up res4a_branch2b
I0512 11:23:11.296406   498 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 10 256 20 48 (2457600)
I0512 11:23:11.296416   498 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:11.296422   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.296428   498 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0512 11:23:11.296433   498 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0512 11:23:11.296437   498 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0512 11:23:11.296696   498 net.cpp:260] Setting up res4a_branch2b/bn
I0512 11:23:11.296703   498 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 10 256 20 48 (2457600)
I0512 11:23:11.296713   498 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0512 11:23:11.296721   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.296730   498 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0512 11:23:11.296736   498 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0512 11:23:11.296742   498 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0512 11:23:11.296751   498 net.cpp:260] Setting up res4a_branch2b/relu
I0512 11:23:11.296757   498 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 10 256 20 48 (2457600)
I0512 11:23:11.296766   498 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0512 11:23:11.296772   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.296782   498 net.cpp:200] Created Layer pool4 (31)
I0512 11:23:11.296788   498 net.cpp:572] pool4 <- res4a_branch2b
I0512 11:23:11.296793   498 net.cpp:542] pool4 -> pool4
I0512 11:23:11.296838   498 net.cpp:260] Setting up pool4
I0512 11:23:11.296844   498 net.cpp:267] TEST Top shape for layer 31 'pool4' 10 256 10 24 (614400)
I0512 11:23:11.296849   498 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0512 11:23:11.296854   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.296866   498 net.cpp:200] Created Layer res5a_branch2a (32)
I0512 11:23:11.296874   498 net.cpp:572] res5a_branch2a <- pool4
I0512 11:23:11.296880   498 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0512 11:23:11.310767   498 net.cpp:260] Setting up res5a_branch2a
I0512 11:23:11.310781   498 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 10 512 10 24 (1228800)
I0512 11:23:11.310792   498 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:11.310796   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.310804   498 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0512 11:23:11.310811   498 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0512 11:23:11.310818   498 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0512 11:23:11.311094   498 net.cpp:260] Setting up res5a_branch2a/bn
I0512 11:23:11.311100   498 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 10 512 10 24 (1228800)
I0512 11:23:11.311113   498 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0512 11:23:11.311120   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.311125   498 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0512 11:23:11.311148   498 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0512 11:23:11.311154   498 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0512 11:23:11.311164   498 net.cpp:260] Setting up res5a_branch2a/relu
I0512 11:23:11.311169   498 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 10 512 10 24 (1228800)
I0512 11:23:11.311177   498 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0512 11:23:11.311182   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.311195   498 net.cpp:200] Created Layer res5a_branch2b (35)
I0512 11:23:11.311203   498 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0512 11:23:11.311206   498 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0512 11:23:11.318327   498 net.cpp:260] Setting up res5a_branch2b
I0512 11:23:11.318339   498 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 10 512 10 24 (1228800)
I0512 11:23:11.318356   498 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:11.318361   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318369   498 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0512 11:23:11.318377   498 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0512 11:23:11.318382   498 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0512 11:23:11.318626   498 net.cpp:260] Setting up res5a_branch2b/bn
I0512 11:23:11.318631   498 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 10 512 10 24 (1228800)
I0512 11:23:11.318641   498 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0512 11:23:11.318646   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318652   498 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0512 11:23:11.318656   498 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0512 11:23:11.318660   498 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0512 11:23:11.318676   498 net.cpp:260] Setting up res5a_branch2b/relu
I0512 11:23:11.318681   498 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 10 512 10 24 (1228800)
I0512 11:23:11.318691   498 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0512 11:23:11.318698   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318706   498 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0512 11:23:11.318712   498 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0512 11:23:11.318717   498 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0512 11:23:11.318727   498 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0512 11:23:11.318763   498 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0512 11:23:11.318768   498 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 10 512 10 24 (1228800)
I0512 11:23:11.318773   498 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 10 512 10 24 (1228800)
I0512 11:23:11.318781   498 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0512 11:23:11.318789   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318796   498 net.cpp:200] Created Layer pool6 (39)
I0512 11:23:11.318804   498 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0512 11:23:11.318809   498 net.cpp:542] pool6 -> pool6
I0512 11:23:11.318856   498 net.cpp:260] Setting up pool6
I0512 11:23:11.318862   498 net.cpp:267] TEST Top shape for layer 39 'pool6' 10 512 5 12 (307200)
I0512 11:23:11.318868   498 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0512 11:23:11.318874   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318895   498 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0512 11:23:11.318899   498 net.cpp:572] pool6_pool6_0_split <- pool6
I0512 11:23:11.318904   498 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0512 11:23:11.318912   498 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0512 11:23:11.318941   498 net.cpp:260] Setting up pool6_pool6_0_split
I0512 11:23:11.318946   498 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 10 512 5 12 (307200)
I0512 11:23:11.318953   498 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 10 512 5 12 (307200)
I0512 11:23:11.318958   498 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0512 11:23:11.318964   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.318972   498 net.cpp:200] Created Layer pool7 (41)
I0512 11:23:11.318980   498 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0512 11:23:11.318986   498 net.cpp:542] pool7 -> pool7
I0512 11:23:11.319023   498 net.cpp:260] Setting up pool7
I0512 11:23:11.319028   498 net.cpp:267] TEST Top shape for layer 41 'pool7' 10 512 3 6 (92160)
I0512 11:23:11.319034   498 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0512 11:23:11.319038   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319046   498 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0512 11:23:11.319051   498 net.cpp:572] pool7_pool7_0_split <- pool7
I0512 11:23:11.319057   498 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0512 11:23:11.319068   498 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0512 11:23:11.319101   498 net.cpp:260] Setting up pool7_pool7_0_split
I0512 11:23:11.319108   498 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 10 512 3 6 (92160)
I0512 11:23:11.319118   498 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 10 512 3 6 (92160)
I0512 11:23:11.319124   498 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0512 11:23:11.319128   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319134   498 net.cpp:200] Created Layer pool8 (43)
I0512 11:23:11.319141   498 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0512 11:23:11.319146   498 net.cpp:542] pool8 -> pool8
I0512 11:23:11.319190   498 net.cpp:260] Setting up pool8
I0512 11:23:11.319196   498 net.cpp:267] TEST Top shape for layer 43 'pool8' 10 512 2 3 (30720)
I0512 11:23:11.319202   498 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0512 11:23:11.319206   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319211   498 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0512 11:23:11.319217   498 net.cpp:572] pool8_pool8_0_split <- pool8
I0512 11:23:11.319223   498 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0512 11:23:11.319232   498 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0512 11:23:11.319258   498 net.cpp:260] Setting up pool8_pool8_0_split
I0512 11:23:11.319265   498 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 10 512 2 3 (30720)
I0512 11:23:11.319272   498 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 10 512 2 3 (30720)
I0512 11:23:11.319280   498 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0512 11:23:11.319284   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319293   498 net.cpp:200] Created Layer pool9 (45)
I0512 11:23:11.319299   498 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0512 11:23:11.319305   498 net.cpp:542] pool9 -> pool9
I0512 11:23:11.319353   498 net.cpp:260] Setting up pool9
I0512 11:23:11.319361   498 net.cpp:267] TEST Top shape for layer 45 'pool9' 10 512 1 2 (10240)
I0512 11:23:11.319371   498 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0512 11:23:11.319387   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319402   498 net.cpp:200] Created Layer ctx_output1 (46)
I0512 11:23:11.319408   498 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0512 11:23:11.319413   498 net.cpp:542] ctx_output1 -> ctx_output1
I0512 11:23:11.319965   498 net.cpp:260] Setting up ctx_output1
I0512 11:23:11.319972   498 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 10 256 40 96 (9830400)
I0512 11:23:11.319981   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0512 11:23:11.319985   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.319990   498 net.cpp:200] Created Layer ctx_output1/relu (47)
I0512 11:23:11.319995   498 net.cpp:572] ctx_output1/relu <- ctx_output1
I0512 11:23:11.319999   498 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0512 11:23:11.320008   498 net.cpp:260] Setting up ctx_output1/relu
I0512 11:23:11.320011   498 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 10 256 40 96 (9830400)
I0512 11:23:11.320016   498 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0512 11:23:11.320020   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.320026   498 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0512 11:23:11.320031   498 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0512 11:23:11.320035   498 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0512 11:23:11.320040   498 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0512 11:23:11.320045   498 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0512 11:23:11.320076   498 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0512 11:23:11.320080   498 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:11.320086   498 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:11.320091   498 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:11.320098   498 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0512 11:23:11.320102   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.320111   498 net.cpp:200] Created Layer ctx_output2 (49)
I0512 11:23:11.320116   498 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0512 11:23:11.320120   498 net.cpp:542] ctx_output2 -> ctx_output2
I0512 11:23:11.321692   498 net.cpp:260] Setting up ctx_output2
I0512 11:23:11.321702   498 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 10 256 10 24 (614400)
I0512 11:23:11.321710   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0512 11:23:11.321715   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.321722   498 net.cpp:200] Created Layer ctx_output2/relu (50)
I0512 11:23:11.321727   498 net.cpp:572] ctx_output2/relu <- ctx_output2
I0512 11:23:11.321734   498 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0512 11:23:11.321743   498 net.cpp:260] Setting up ctx_output2/relu
I0512 11:23:11.321771   498 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 10 256 10 24 (614400)
I0512 11:23:11.321784   498 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0512 11:23:11.321789   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.321797   498 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0512 11:23:11.321806   498 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0512 11:23:11.321825   498 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0512 11:23:11.321835   498 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0512 11:23:11.321842   498 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0512 11:23:11.321894   498 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0512 11:23:11.321902   498 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:11.321908   498 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:11.321913   498 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:11.321920   498 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0512 11:23:11.321924   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.321936   498 net.cpp:200] Created Layer ctx_output3 (52)
I0512 11:23:11.321944   498 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0512 11:23:11.321954   498 net.cpp:542] ctx_output3 -> ctx_output3
I0512 11:23:11.324205   498 net.cpp:260] Setting up ctx_output3
I0512 11:23:11.324216   498 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 10 256 5 12 (153600)
I0512 11:23:11.324229   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0512 11:23:11.324234   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.324239   498 net.cpp:200] Created Layer ctx_output3/relu (53)
I0512 11:23:11.324244   498 net.cpp:572] ctx_output3/relu <- ctx_output3
I0512 11:23:11.324251   498 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0512 11:23:11.324259   498 net.cpp:260] Setting up ctx_output3/relu
I0512 11:23:11.324270   498 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 10 256 5 12 (153600)
I0512 11:23:11.324280   498 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0512 11:23:11.324286   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.324297   498 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0512 11:23:11.324322   498 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0512 11:23:11.324331   498 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0512 11:23:11.324337   498 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0512 11:23:11.324342   498 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0512 11:23:11.324396   498 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0512 11:23:11.324404   498 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:11.324411   498 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:11.324415   498 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:11.324421   498 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0512 11:23:11.324425   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.324438   498 net.cpp:200] Created Layer ctx_output4 (55)
I0512 11:23:11.324462   498 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0512 11:23:11.324470   498 net.cpp:542] ctx_output4 -> ctx_output4
I0512 11:23:11.326054   498 net.cpp:260] Setting up ctx_output4
I0512 11:23:11.326063   498 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 10 256 3 6 (46080)
I0512 11:23:11.326073   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0512 11:23:11.326088   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.326097   498 net.cpp:200] Created Layer ctx_output4/relu (56)
I0512 11:23:11.326104   498 net.cpp:572] ctx_output4/relu <- ctx_output4
I0512 11:23:11.326109   498 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0512 11:23:11.326115   498 net.cpp:260] Setting up ctx_output4/relu
I0512 11:23:11.326118   498 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 10 256 3 6 (46080)
I0512 11:23:11.326125   498 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0512 11:23:11.326133   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.326141   498 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0512 11:23:11.326148   498 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0512 11:23:11.326153   498 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0512 11:23:11.326159   498 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0512 11:23:11.326165   498 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0512 11:23:11.326206   498 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0512 11:23:11.326211   498 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:11.326216   498 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:11.326221   498 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:11.326226   498 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0512 11:23:11.326231   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.326239   498 net.cpp:200] Created Layer ctx_output5 (58)
I0512 11:23:11.326246   498 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0512 11:23:11.326251   498 net.cpp:542] ctx_output5 -> ctx_output5
I0512 11:23:11.327823   498 net.cpp:260] Setting up ctx_output5
I0512 11:23:11.327832   498 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 10 256 2 3 (15360)
I0512 11:23:11.327842   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0512 11:23:11.327847   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.327853   498 net.cpp:200] Created Layer ctx_output5/relu (59)
I0512 11:23:11.327858   498 net.cpp:572] ctx_output5/relu <- ctx_output5
I0512 11:23:11.327867   498 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0512 11:23:11.327877   498 net.cpp:260] Setting up ctx_output5/relu
I0512 11:23:11.327880   498 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 10 256 2 3 (15360)
I0512 11:23:11.327890   498 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0512 11:23:11.327895   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.327905   498 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0512 11:23:11.327911   498 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0512 11:23:11.327917   498 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0512 11:23:11.327925   498 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0512 11:23:11.327932   498 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0512 11:23:11.327976   498 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0512 11:23:11.327982   498 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:11.327987   498 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:11.328006   498 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:11.328014   498 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0512 11:23:11.328018   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.328028   498 net.cpp:200] Created Layer ctx_output6 (61)
I0512 11:23:11.328035   498 net.cpp:572] ctx_output6 <- pool9
I0512 11:23:11.328042   498 net.cpp:542] ctx_output6 -> ctx_output6
I0512 11:23:11.329627   498 net.cpp:260] Setting up ctx_output6
I0512 11:23:11.329635   498 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 10 256 1 2 (5120)
I0512 11:23:11.329644   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0512 11:23:11.329651   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.329658   498 net.cpp:200] Created Layer ctx_output6/relu (62)
I0512 11:23:11.329664   498 net.cpp:572] ctx_output6/relu <- ctx_output6
I0512 11:23:11.329672   498 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0512 11:23:11.329680   498 net.cpp:260] Setting up ctx_output6/relu
I0512 11:23:11.329686   498 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 10 256 1 2 (5120)
I0512 11:23:11.329697   498 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0512 11:23:11.329704   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.329711   498 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0512 11:23:11.329716   498 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0512 11:23:11.329721   498 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0512 11:23:11.329726   498 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0512 11:23:11.329732   498 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0512 11:23:11.329773   498 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0512 11:23:11.329777   498 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:11.329783   498 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:11.329788   498 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:11.329795   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.329799   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.329818   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0512 11:23:11.329823   498 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0512 11:23:11.329828   498 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0512 11:23:11.330063   498 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0512 11:23:11.330070   498 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 10 16 40 96 (614400)
I0512 11:23:11.330078   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.330083   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.330096   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0512 11:23:11.330121   498 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0512 11:23:11.330132   498 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0512 11:23:11.330225   498 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0512 11:23:11.330231   498 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 10 40 96 16 (614400)
I0512 11:23:11.330250   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.330256   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.330268   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0512 11:23:11.330276   498 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0512 11:23:11.330281   498 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0512 11:23:11.332309   498 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0512 11:23:11.332321   498 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 10 61440 (614400)
I0512 11:23:11.332331   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.332337   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.332350   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0512 11:23:11.332356   498 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0512 11:23:11.332363   498 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0512 11:23:11.332644   498 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0512 11:23:11.332650   498 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 10 16 40 96 (614400)
I0512 11:23:11.332661   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.332667   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.332674   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0512 11:23:11.332681   498 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0512 11:23:11.332689   498 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0512 11:23:11.332770   498 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0512 11:23:11.332777   498 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 10 40 96 16 (614400)
I0512 11:23:11.332782   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.332787   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.332793   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0512 11:23:11.332798   498 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0512 11:23:11.332803   498 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0512 11:23:11.334738   498 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0512 11:23:11.334749   498 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 10 61440 (614400)
I0512 11:23:11.334759   498 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.334765   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.334780   498 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0512 11:23:11.334786   498 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0512 11:23:11.334794   498 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0512 11:23:11.334803   498 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0512 11:23:11.334847   498 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0512 11:23:11.334853   498 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0512 11:23:11.334861   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.334866   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.334877   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0512 11:23:11.334895   498 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0512 11:23:11.334900   498 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0512 11:23:11.335187   498 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0512 11:23:11.335193   498 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 10 24 10 24 (57600)
I0512 11:23:11.335202   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.335207   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.335214   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0512 11:23:11.335219   498 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0512 11:23:11.335224   498 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0512 11:23:11.335299   498 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0512 11:23:11.335304   498 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 10 10 24 24 (57600)
I0512 11:23:11.335312   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.335317   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.335325   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0512 11:23:11.335330   498 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0512 11:23:11.335335   498 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0512 11:23:11.336282   498 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0512 11:23:11.336292   498 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 10 5760 (57600)
I0512 11:23:11.336302   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.336308   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.336321   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0512 11:23:11.336328   498 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0512 11:23:11.336334   498 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0512 11:23:11.336621   498 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0512 11:23:11.336628   498 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 10 24 10 24 (57600)
I0512 11:23:11.336639   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.336644   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.336652   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0512 11:23:11.336658   498 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0512 11:23:11.336664   498 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0512 11:23:11.336740   498 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0512 11:23:11.336745   498 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 10 10 24 24 (57600)
I0512 11:23:11.336750   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.336755   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.336760   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0512 11:23:11.336764   498 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0512 11:23:11.336768   498 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0512 11:23:11.337396   498 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0512 11:23:11.337405   498 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 10 5760 (57600)
I0512 11:23:11.337424   498 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.337427   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.337440   498 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0512 11:23:11.337468   498 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0512 11:23:11.337478   498 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0512 11:23:11.337486   498 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0512 11:23:11.337517   498 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0512 11:23:11.337524   498 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0512 11:23:11.337532   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.337536   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.337548   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0512 11:23:11.337554   498 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0512 11:23:11.337559   498 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0512 11:23:11.337834   498 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0512 11:23:11.337841   498 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 10 24 5 12 (14400)
I0512 11:23:11.337849   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.337855   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.337867   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0512 11:23:11.337873   498 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0512 11:23:11.337879   498 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0512 11:23:11.337965   498 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0512 11:23:11.337970   498 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 10 5 12 24 (14400)
I0512 11:23:11.337976   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.337981   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.337987   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0512 11:23:11.337991   498 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0512 11:23:11.337996   498 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0512 11:23:11.338057   498 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0512 11:23:11.338061   498 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 10 1440 (14400)
I0512 11:23:11.338068   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.338071   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.338081   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0512 11:23:11.338088   498 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0512 11:23:11.338094   498 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0512 11:23:11.338362   498 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0512 11:23:11.338368   498 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 10 24 5 12 (14400)
I0512 11:23:11.338377   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.338382   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.338389   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0512 11:23:11.338407   498 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0512 11:23:11.338413   498 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0512 11:23:11.338500   498 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0512 11:23:11.338505   498 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 10 5 12 24 (14400)
I0512 11:23:11.338513   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.338517   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.338522   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0512 11:23:11.338528   498 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0512 11:23:11.338536   498 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0512 11:23:11.338598   498 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0512 11:23:11.338603   498 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 10 1440 (14400)
I0512 11:23:11.338609   498 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.338614   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.338620   498 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0512 11:23:11.338626   498 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0512 11:23:11.338631   498 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0512 11:23:11.338639   498 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0512 11:23:11.338665   498 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0512 11:23:11.338673   498 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0512 11:23:11.338682   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.338687   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.338703   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0512 11:23:11.338711   498 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0512 11:23:11.338721   498 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0512 11:23:11.338994   498 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0512 11:23:11.339000   498 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 10 24 3 6 (4320)
I0512 11:23:11.339010   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.339015   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339022   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0512 11:23:11.339030   498 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0512 11:23:11.339036   498 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0512 11:23:11.339133   498 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0512 11:23:11.339138   498 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 10 3 6 24 (4320)
I0512 11:23:11.339143   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.339148   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339154   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0512 11:23:11.339160   498 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0512 11:23:11.339169   498 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0512 11:23:11.339229   498 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0512 11:23:11.339243   498 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 10 432 (4320)
I0512 11:23:11.339249   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.339254   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339267   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0512 11:23:11.339275   498 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0512 11:23:11.339282   498 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0512 11:23:11.339540   498 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0512 11:23:11.339547   498 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 10 24 3 6 (4320)
I0512 11:23:11.339555   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.339561   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339570   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0512 11:23:11.339576   498 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0512 11:23:11.339583   498 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0512 11:23:11.339658   498 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0512 11:23:11.339664   498 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 10 3 6 24 (4320)
I0512 11:23:11.339670   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.339675   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339681   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0512 11:23:11.339687   498 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0512 11:23:11.339694   498 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0512 11:23:11.339748   498 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0512 11:23:11.339754   498 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 10 432 (4320)
I0512 11:23:11.339761   498 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.339764   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339772   498 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0512 11:23:11.339779   498 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0512 11:23:11.339787   498 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0512 11:23:11.339794   498 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0512 11:23:11.339815   498 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0512 11:23:11.339821   498 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0512 11:23:11.339828   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.339831   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.339843   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0512 11:23:11.339849   498 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0512 11:23:11.339856   498 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0512 11:23:11.340086   498 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0512 11:23:11.340092   498 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 10 16 2 3 (960)
I0512 11:23:11.340101   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.340106   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340126   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0512 11:23:11.340132   498 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0512 11:23:11.340138   498 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0512 11:23:11.340209   498 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0512 11:23:11.340214   498 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 10 2 3 16 (960)
I0512 11:23:11.340220   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.340225   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340231   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0512 11:23:11.340238   498 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0512 11:23:11.340245   498 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0512 11:23:11.340291   498 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0512 11:23:11.340296   498 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 10 96 (960)
I0512 11:23:11.340302   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.340307   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340317   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0512 11:23:11.340322   498 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0512 11:23:11.340327   498 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0512 11:23:11.340550   498 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0512 11:23:11.340556   498 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 10 16 2 3 (960)
I0512 11:23:11.340564   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.340570   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340580   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0512 11:23:11.340586   498 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0512 11:23:11.340595   498 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0512 11:23:11.340674   498 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0512 11:23:11.340680   498 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 10 2 3 16 (960)
I0512 11:23:11.340687   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.340696   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340704   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0512 11:23:11.340711   498 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0512 11:23:11.340718   498 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0512 11:23:11.340767   498 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0512 11:23:11.340773   498 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 10 96 (960)
I0512 11:23:11.340780   498 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.340787   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340800   498 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0512 11:23:11.340806   498 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0512 11:23:11.340814   498 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0512 11:23:11.340822   498 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0512 11:23:11.340854   498 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0512 11:23:11.340860   498 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0512 11:23:11.340869   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0512 11:23:11.340878   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.340891   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0512 11:23:11.340900   498 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0512 11:23:11.340905   498 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0512 11:23:11.341152   498 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0512 11:23:11.341159   498 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 10 16 1 2 (320)
I0512 11:23:11.341176   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:11.341184   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341198   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0512 11:23:11.341205   498 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0512 11:23:11.341213   498 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0512 11:23:11.341295   498 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0512 11:23:11.341301   498 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 10 1 2 16 (320)
I0512 11:23:11.341308   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:11.341312   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341317   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0512 11:23:11.341322   498 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0512 11:23:11.341327   498 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0512 11:23:11.341378   498 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0512 11:23:11.341383   498 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 10 32 (320)
I0512 11:23:11.341389   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0512 11:23:11.341392   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341403   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0512 11:23:11.341410   498 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0512 11:23:11.341418   498 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0512 11:23:11.341655   498 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0512 11:23:11.341662   498 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 10 16 1 2 (320)
I0512 11:23:11.341670   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:11.341675   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341683   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0512 11:23:11.341711   498 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0512 11:23:11.341722   498 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0512 11:23:11.341800   498 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0512 11:23:11.341805   498 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 10 1 2 16 (320)
I0512 11:23:11.341812   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:11.341815   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341836   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0512 11:23:11.341842   498 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0512 11:23:11.341846   498 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0512 11:23:11.341897   498 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0512 11:23:11.341903   498 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 10 32 (320)
I0512 11:23:11.341909   498 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:11.341913   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341919   498 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0512 11:23:11.341925   498 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0512 11:23:11.341933   498 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0512 11:23:11.341939   498 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0512 11:23:11.341958   498 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0512 11:23:11.341964   498 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0512 11:23:11.341969   498 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0512 11:23:11.341972   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.341984   498 net.cpp:200] Created Layer mbox_loc (106)
I0512 11:23:11.341991   498 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0512 11:23:11.341998   498 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0512 11:23:11.342005   498 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0512 11:23:11.342010   498 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0512 11:23:11.342013   498 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0512 11:23:11.342018   498 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0512 11:23:11.342022   498 net.cpp:542] mbox_loc -> mbox_loc
I0512 11:23:11.342044   498 net.cpp:260] Setting up mbox_loc
I0512 11:23:11.342049   498 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 10 69200 (692000)
I0512 11:23:11.342056   498 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0512 11:23:11.342059   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.342065   498 net.cpp:200] Created Layer mbox_conf (107)
I0512 11:23:11.342072   498 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0512 11:23:11.342078   498 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0512 11:23:11.342085   498 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0512 11:23:11.342090   498 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0512 11:23:11.342098   498 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0512 11:23:11.342104   498 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0512 11:23:11.342111   498 net.cpp:542] mbox_conf -> mbox_conf
I0512 11:23:11.342133   498 net.cpp:260] Setting up mbox_conf
I0512 11:23:11.342144   498 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 10 69200 (692000)
I0512 11:23:11.342154   498 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0512 11:23:11.342162   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.342172   498 net.cpp:200] Created Layer mbox_priorbox (108)
I0512 11:23:11.342180   498 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0512 11:23:11.342186   498 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0512 11:23:11.342195   498 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0512 11:23:11.342202   498 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0512 11:23:11.342209   498 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0512 11:23:11.342226   498 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0512 11:23:11.342232   498 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0512 11:23:11.342259   498 net.cpp:260] Setting up mbox_priorbox
I0512 11:23:11.342268   498 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0512 11:23:11.342281   498 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0512 11:23:11.342290   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.342304   498 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0512 11:23:11.342310   498 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0512 11:23:11.342316   498 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0512 11:23:11.342340   498 net.cpp:260] Setting up mbox_conf_reshape
I0512 11:23:11.342346   498 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 10 17300 4 (692000)
I0512 11:23:11.342356   498 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0512 11:23:11.342365   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.342384   498 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0512 11:23:11.342391   498 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0512 11:23:11.342396   498 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0512 11:23:11.342449   498 net.cpp:260] Setting up mbox_conf_softmax
I0512 11:23:11.342455   498 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 10 17300 4 (692000)
I0512 11:23:11.342465   498 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0512 11:23:11.342471   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.342479   498 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0512 11:23:11.342485   498 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0512 11:23:11.342492   498 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0512 11:23:11.344638   498 net.cpp:260] Setting up mbox_conf_flatten
I0512 11:23:11.344651   498 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 10 69200 (692000)
I0512 11:23:11.344663   498 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0512 11:23:11.344669   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.344702   498 net.cpp:200] Created Layer detection_out (112)
I0512 11:23:11.344713   498 net.cpp:572] detection_out <- mbox_loc
I0512 11:23:11.344725   498 net.cpp:572] detection_out <- mbox_conf_flatten
I0512 11:23:11.344733   498 net.cpp:572] detection_out <- mbox_priorbox
I0512 11:23:11.344743   498 net.cpp:542] detection_out -> detection_out
I0512 11:23:11.345219   498 net.cpp:260] Setting up detection_out
I0512 11:23:11.345229   498 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0512 11:23:11.345242   498 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0512 11:23:11.345249   498 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:11.345263   498 net.cpp:200] Created Layer detection_eval (113)
I0512 11:23:11.345273   498 net.cpp:572] detection_eval <- detection_out
I0512 11:23:11.345279   498 net.cpp:572] detection_eval <- label
I0512 11:23:11.345284   498 net.cpp:542] detection_eval -> detection_eval
I0512 11:23:11.345602   498 net.cpp:260] Setting up detection_eval
I0512 11:23:11.345607   498 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0512 11:23:11.345614   498 net.cpp:338] detection_eval does not need backward computation.
I0512 11:23:11.345618   498 net.cpp:338] detection_out does not need backward computation.
I0512 11:23:11.345624   498 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0512 11:23:11.345633   498 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0512 11:23:11.345649   498 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0512 11:23:11.345655   498 net.cpp:338] mbox_priorbox does not need backward computation.
I0512 11:23:11.345664   498 net.cpp:338] mbox_conf does not need backward computation.
I0512 11:23:11.345674   498 net.cpp:338] mbox_loc does not need backward computation.
I0512 11:23:11.345681   498 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345688   498 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345695   498 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345702   498 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345711   498 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345717   498 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345723   498 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345729   498 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345734   498 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345738   498 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345746   498 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345752   498 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345758   498 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345764   498 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345772   498 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345778   498 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345784   498 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345788   498 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345793   498 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345796   498 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345800   498 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345804   498 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345809   498 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345816   498 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345822   498 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345829   498 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345835   498 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345842   498 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345847   498 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345854   498 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345861   498 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345865   498 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345870   498 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345875   498 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345881   498 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345893   498 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0512 11:23:11.345898   498 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:11.345903   498 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:11.345909   498 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0512 11:23:11.345917   498 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:11.345922   498 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:11.345927   498 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0512 11:23:11.345933   498 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0512 11:23:11.345940   498 net.cpp:338] ctx_output6/relu does not need backward computation.
I0512 11:23:11.345945   498 net.cpp:338] ctx_output6 does not need backward computation.
I0512 11:23:11.345953   498 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0512 11:23:11.345958   498 net.cpp:338] ctx_output5/relu does not need backward computation.
I0512 11:23:11.345965   498 net.cpp:338] ctx_output5 does not need backward computation.
I0512 11:23:11.345971   498 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0512 11:23:11.345978   498 net.cpp:338] ctx_output4/relu does not need backward computation.
I0512 11:23:11.345984   498 net.cpp:338] ctx_output4 does not need backward computation.
I0512 11:23:11.345990   498 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0512 11:23:11.345995   498 net.cpp:338] ctx_output3/relu does not need backward computation.
I0512 11:23:11.346004   498 net.cpp:338] ctx_output3 does not need backward computation.
I0512 11:23:11.346010   498 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0512 11:23:11.346019   498 net.cpp:338] ctx_output2/relu does not need backward computation.
I0512 11:23:11.346024   498 net.cpp:338] ctx_output2 does not need backward computation.
I0512 11:23:11.346031   498 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0512 11:23:11.346038   498 net.cpp:338] ctx_output1/relu does not need backward computation.
I0512 11:23:11.346045   498 net.cpp:338] ctx_output1 does not need backward computation.
I0512 11:23:11.346053   498 net.cpp:338] pool9 does not need backward computation.
I0512 11:23:11.346060   498 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0512 11:23:11.346067   498 net.cpp:338] pool8 does not need backward computation.
I0512 11:23:11.346076   498 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0512 11:23:11.346082   498 net.cpp:338] pool7 does not need backward computation.
I0512 11:23:11.346091   498 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0512 11:23:11.346097   498 net.cpp:338] pool6 does not need backward computation.
I0512 11:23:11.346105   498 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0512 11:23:11.346112   498 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0512 11:23:11.346119   498 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0512 11:23:11.346124   498 net.cpp:338] res5a_branch2b does not need backward computation.
I0512 11:23:11.346130   498 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0512 11:23:11.346137   498 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0512 11:23:11.346141   498 net.cpp:338] res5a_branch2a does not need backward computation.
I0512 11:23:11.346146   498 net.cpp:338] pool4 does not need backward computation.
I0512 11:23:11.346151   498 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0512 11:23:11.346155   498 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0512 11:23:11.346164   498 net.cpp:338] res4a_branch2b does not need backward computation.
I0512 11:23:11.346169   498 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0512 11:23:11.346176   498 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0512 11:23:11.346181   498 net.cpp:338] res4a_branch2a does not need backward computation.
I0512 11:23:11.346189   498 net.cpp:338] pool3 does not need backward computation.
I0512 11:23:11.346195   498 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0512 11:23:11.346202   498 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0512 11:23:11.346208   498 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0512 11:23:11.346213   498 net.cpp:338] res3a_branch2b does not need backward computation.
I0512 11:23:11.346221   498 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0512 11:23:11.346226   498 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0512 11:23:11.346231   498 net.cpp:338] res3a_branch2a does not need backward computation.
I0512 11:23:11.346240   498 net.cpp:338] pool2 does not need backward computation.
I0512 11:23:11.346246   498 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0512 11:23:11.346252   498 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0512 11:23:11.346257   498 net.cpp:338] res2a_branch2b does not need backward computation.
I0512 11:23:11.346266   498 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0512 11:23:11.346271   498 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0512 11:23:11.346276   498 net.cpp:338] res2a_branch2a does not need backward computation.
I0512 11:23:11.346284   498 net.cpp:338] pool1 does not need backward computation.
I0512 11:23:11.346290   498 net.cpp:338] conv1b/relu does not need backward computation.
I0512 11:23:11.346297   498 net.cpp:338] conv1b/bn does not need backward computation.
I0512 11:23:11.346302   498 net.cpp:338] conv1b does not need backward computation.
I0512 11:23:11.346312   498 net.cpp:338] conv1a/relu does not need backward computation.
I0512 11:23:11.346316   498 net.cpp:338] conv1a/bn does not need backward computation.
I0512 11:23:11.346324   498 net.cpp:338] conv1a does not need backward computation.
I0512 11:23:11.346330   498 net.cpp:338] data/bias does not need backward computation.
I0512 11:23:11.346338   498 net.cpp:338] data_data_0_split does not need backward computation.
I0512 11:23:11.346343   498 net.cpp:338] data does not need backward computation.
I0512 11:23:11.346346   498 net.cpp:380] This network produces output detection_eval
I0512 11:23:11.346444   498 net.cpp:403] Top memory (TEST) required for data: 1515720496 diff: 1515720496
I0512 11:23:11.346451   498 net.cpp:406] Bottom memory (TEST) required for data: 1515720416 diff: 1515720416
I0512 11:23:11.346454   498 net.cpp:409] Shared (in-place) memory (TEST) by data: 652144640 diff: 652144640
I0512 11:23:11.346458   498 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0512 11:23:11.346462   498 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0512 11:23:11.346467   498 net.cpp:421] Network initialization done.
I0512 11:23:11.352720   498 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0512 11:23:11.352735   498 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0512 11:23:11.352738   498 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0512 11:23:11.352772   498 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0512 11:23:11.352792   498 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0512 11:23:11.352839   498 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0512 11:23:11.352846   498 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0512 11:23:11.352869   498 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0512 11:23:11.352918   498 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0512 11:23:11.352926   498 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0512 11:23:11.352929   498 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0512 11:23:11.352960   498 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353003   498 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0512 11:23:11.353008   498 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0512 11:23:11.353026   498 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353065   498 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0512 11:23:11.353071   498 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0512 11:23:11.353075   498 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0512 11:23:11.353123   498 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353163   498 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0512 11:23:11.353168   498 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0512 11:23:11.353199   498 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353232   498 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0512 11:23:11.353237   498 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0512 11:23:11.353240   498 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0512 11:23:11.353243   498 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0512 11:23:11.353391   498 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353427   498 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0512 11:23:11.353433   498 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0512 11:23:11.353511   498 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0512 11:23:11.353543   498 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0512 11:23:11.353550   498 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0512 11:23:11.353555   498 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0512 11:23:11.354022   498 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0512 11:23:11.354063   498 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0512 11:23:11.354068   498 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0512 11:23:11.354329   498 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0512 11:23:11.354367   498 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0512 11:23:11.354374   498 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354378   498 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0512 11:23:11.354382   498 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0512 11:23:11.354385   498 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0512 11:23:11.354393   498 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0512 11:23:11.354398   498 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0512 11:23:11.354404   498 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0512 11:23:11.354409   498 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0512 11:23:11.354414   498 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0512 11:23:11.354456   498 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0512 11:23:11.354477   498 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354481   498 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0512 11:23:11.354566   498 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0512 11:23:11.354575   498 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354579   498 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0512 11:23:11.354656   498 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0512 11:23:11.354665   498 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354668   498 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0512 11:23:11.354748   498 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0512 11:23:11.354756   498 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354760   498 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0512 11:23:11.354838   498 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0512 11:23:11.354847   498 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354851   498 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0512 11:23:11.354929   498 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0512 11:23:11.354938   498 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0512 11:23:11.354940   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.354961   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.354967   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.354971   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.354991   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.354998   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355002   498 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355005   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.355026   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.355033   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.355036   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.355063   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.355072   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355079   498 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355084   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.355108   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.355113   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.355118   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.355140   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.355146   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355151   498 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355165   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.355197   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.355206   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.355209   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.355232   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.355237   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355240   498 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355243   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.355262   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.355266   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.355271   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.355290   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.355296   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355301   498 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355307   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
I0512 11:23:11.355327   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0512 11:23:11.355334   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0512 11:23:11.355337   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
I0512 11:23:11.355356   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0512 11:23:11.355363   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0512 11:23:11.355366   498 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0512 11:23:11.355370   498 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0512 11:23:11.355373   498 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0512 11:23:11.355376   498 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0512 11:23:11.355381   498 net.cpp:1137] Ignoring source layer mbox_loss
I0512 11:23:11.355583   498 caffe.cpp:419] Running for 85 iterations.
I0512 11:23:11.396422   498 caffe.cpp:449] Batch 0
I0512 11:23:11.419170   498 caffe.cpp:449] Batch 1
I0512 11:23:11.495020   498 caffe.cpp:449] Batch 2
I0512 11:23:11.602697   498 caffe.cpp:449] Batch 3
I0512 11:23:11.710274   498 caffe.cpp:449] Batch 4
I0512 11:23:11.817873   498 caffe.cpp:449] Batch 5
I0512 11:23:11.925905   498 caffe.cpp:449] Batch 6
I0512 11:23:12.033788   498 caffe.cpp:449] Batch 7
I0512 11:23:12.140556   498 caffe.cpp:449] Batch 8
I0512 11:23:12.248531   498 caffe.cpp:449] Batch 9
I0512 11:23:12.355599   498 caffe.cpp:449] Batch 10
I0512 11:23:12.461784   498 caffe.cpp:449] Batch 11
I0512 11:23:12.570276   498 caffe.cpp:449] Batch 12
I0512 11:23:12.678382   498 caffe.cpp:449] Batch 13
I0512 11:23:12.785702   498 caffe.cpp:449] Batch 14
I0512 11:23:12.892874   498 caffe.cpp:449] Batch 15
I0512 11:23:13.000392   498 caffe.cpp:449] Batch 16
I0512 11:23:13.106232   498 caffe.cpp:449] Batch 17
I0512 11:23:13.213353   498 caffe.cpp:449] Batch 18
I0512 11:23:13.318935   498 caffe.cpp:449] Batch 19
I0512 11:23:13.426606   498 caffe.cpp:449] Batch 20
I0512 11:23:13.533864   498 caffe.cpp:449] Batch 21
I0512 11:23:13.641486   498 caffe.cpp:449] Batch 22
I0512 11:23:13.748509   498 caffe.cpp:449] Batch 23
I0512 11:23:13.856581   498 caffe.cpp:449] Batch 24
I0512 11:23:13.963358   498 caffe.cpp:449] Batch 25
I0512 11:23:14.072607   498 caffe.cpp:449] Batch 26
I0512 11:23:14.179491   498 caffe.cpp:449] Batch 27
I0512 11:23:14.286077   498 caffe.cpp:449] Batch 28
I0512 11:23:14.393278   498 caffe.cpp:449] Batch 29
I0512 11:23:14.500517   498 caffe.cpp:449] Batch 30
I0512 11:23:14.607683   498 caffe.cpp:449] Batch 31
I0512 11:23:14.714352   498 caffe.cpp:449] Batch 32
I0512 11:23:14.822268   498 caffe.cpp:449] Batch 33
I0512 11:23:14.928849   498 caffe.cpp:449] Batch 34
I0512 11:23:15.037114   498 caffe.cpp:449] Batch 35
I0512 11:23:15.144627   498 caffe.cpp:449] Batch 36
I0512 11:23:15.253422   498 caffe.cpp:449] Batch 37
I0512 11:23:15.361246   498 caffe.cpp:449] Batch 38
I0512 11:23:15.468729   498 caffe.cpp:449] Batch 39
I0512 11:23:15.575903   498 caffe.cpp:449] Batch 40
I0512 11:23:15.683580   498 caffe.cpp:449] Batch 41
I0512 11:23:15.791067   498 caffe.cpp:449] Batch 42
I0512 11:23:15.897071   498 caffe.cpp:449] Batch 43
I0512 11:23:16.004714   498 caffe.cpp:449] Batch 44
I0512 11:23:16.111428   498 caffe.cpp:449] Batch 45
I0512 11:23:16.218567   498 caffe.cpp:449] Batch 46
I0512 11:23:16.325775   498 caffe.cpp:449] Batch 47
I0512 11:23:16.433482   498 caffe.cpp:449] Batch 48
I0512 11:23:16.539978   498 caffe.cpp:449] Batch 49
I0512 11:23:16.646920   498 caffe.cpp:449] Batch 50
I0512 11:23:16.754892   498 caffe.cpp:449] Batch 51
I0512 11:23:16.861797   498 caffe.cpp:449] Batch 52
I0512 11:23:16.968052   498 caffe.cpp:449] Batch 53
I0512 11:23:17.072311   498 caffe.cpp:449] Batch 54
I0512 11:23:17.179204   498 caffe.cpp:449] Batch 55
I0512 11:23:17.288398   498 caffe.cpp:449] Batch 56
I0512 11:23:17.395721   498 caffe.cpp:449] Batch 57
I0512 11:23:17.502921   498 caffe.cpp:449] Batch 58
I0512 11:23:17.609423   498 caffe.cpp:449] Batch 59
I0512 11:23:17.715229   498 caffe.cpp:449] Batch 60
I0512 11:23:17.822778   498 caffe.cpp:449] Batch 61
I0512 11:23:17.930460   498 caffe.cpp:449] Batch 62
I0512 11:23:18.038231   498 caffe.cpp:449] Batch 63
I0512 11:23:18.146185   498 caffe.cpp:449] Batch 64
I0512 11:23:18.252990   498 caffe.cpp:449] Batch 65
I0512 11:23:18.359697   498 caffe.cpp:449] Batch 66
I0512 11:23:18.468299   498 caffe.cpp:449] Batch 67
I0512 11:23:18.575109   498 caffe.cpp:449] Batch 68
I0512 11:23:18.682137   498 caffe.cpp:449] Batch 69
I0512 11:23:18.790078   498 caffe.cpp:449] Batch 70
I0512 11:23:18.896759   498 caffe.cpp:449] Batch 71
I0512 11:23:19.005017   498 caffe.cpp:449] Batch 72
I0512 11:23:19.113394   498 caffe.cpp:449] Batch 73
I0512 11:23:19.220588   498 caffe.cpp:449] Batch 74
I0512 11:23:19.328512   498 caffe.cpp:449] Batch 75
I0512 11:23:19.436257   498 caffe.cpp:449] Batch 76
I0512 11:23:19.543438   498 caffe.cpp:449] Batch 77
I0512 11:23:19.650223   498 caffe.cpp:449] Batch 78
I0512 11:23:19.757442   498 caffe.cpp:449] Batch 79
I0512 11:23:19.864732   498 caffe.cpp:449] Batch 80
I0512 11:23:19.973284   498 caffe.cpp:449] Batch 81
I0512 11:23:20.080358   498 caffe.cpp:449] Batch 82
I0512 11:23:20.149740   504 data_reader.cpp:320] Restarting data pre-fetching
I0512 11:23:20.187016   498 caffe.cpp:449] Batch 83
I0512 11:23:20.293607   498 caffe.cpp:449] Batch 84
I0512 11:23:20.293632   498 caffe.cpp:483] Loss: 0
I0512 11:23:20.293656   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293670   498 caffe.cpp:501] detection_eval = 1
I0512 11:23:20.293673   498 caffe.cpp:501] detection_eval = 11
I0512 11:23:20.293678   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293684   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293694   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293702   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.293707   498 caffe.cpp:501] detection_eval = 8.22353
I0512 11:23:20.293715   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293721   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293728   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293753   498 caffe.cpp:501] detection_eval = 3
I0512 11:23:20.293759   498 caffe.cpp:501] detection_eval = 10.8941
I0512 11:23:20.293778   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293784   498 caffe.cpp:501] detection_eval = -1
I0512 11:23:20.293788   498 caffe.cpp:501] detection_eval = 0
I0512 11:23:20.293792   498 caffe.cpp:501] detection_eval = 1.14118
I0512 11:23:20.293797   498 caffe.cpp:501] detection_eval = 0.574656
I0512 11:23:20.293802   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.293814   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.293820   498 caffe.cpp:501] detection_eval = 0
I0512 11:23:20.293828   498 caffe.cpp:501] detection_eval = 1.25882
I0512 11:23:20.293833   498 caffe.cpp:501] detection_eval = 0.43447
I0512 11:23:20.293840   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.293846   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.293851   498 caffe.cpp:501] detection_eval = 0
I0512 11:23:20.293857   498 caffe.cpp:501] detection_eval = 1.56471
I0512 11:23:20.293864   498 caffe.cpp:501] detection_eval = 0.144966
I0512 11:23:20.293872   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.293877   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.293880   498 caffe.cpp:501] detection_eval = 0
I0512 11:23:20.293887   498 caffe.cpp:501] detection_eval = 1.75294
I0512 11:23:20.293898   498 caffe.cpp:501] detection_eval = 0.0647134
I0512 11:23:20.293905   498 caffe.cpp:501] detection_eval = 0.0470588
I0512 11:23:20.293910   498 caffe.cpp:501] detection_eval = 0.952941
I0512 11:23:20.293916   498 caffe.cpp:501] detection_eval = 0
I0512 11:23:20.293922   498 caffe.cpp:501] detection_eval = 1.88235
I0512 11:23:20.293927   498 caffe.cpp:501] detection_eval = 0.0588118
I0512 11:23:20.293933   498 caffe.cpp:501] detection_eval = 0.0470588
I0512 11:23:20.293941   498 caffe.cpp:501] detection_eval = 0.952941
I0512 11:23:20.293947   498 caffe.cpp:501] detection_eval = 0.0117647
I0512 11:23:20.293954   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.293960   498 caffe.cpp:501] detection_eval = 0.0817746
I0512 11:23:20.293967   498 caffe.cpp:501] detection_eval = 0.0941176
I0512 11:23:20.293973   498 caffe.cpp:501] detection_eval = 0.905882
I0512 11:23:20.293982   498 caffe.cpp:501] detection_eval = 0.0352941
I0512 11:23:20.293987   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.293994   498 caffe.cpp:501] detection_eval = 0.148763
I0512 11:23:20.294000   498 caffe.cpp:501] detection_eval = 0.164706
I0512 11:23:20.294006   498 caffe.cpp:501] detection_eval = 0.835294
I0512 11:23:20.294013   498 caffe.cpp:501] detection_eval = 0.0823529
I0512 11:23:20.294019   498 caffe.cpp:501] detection_eval = 2.11765
I0512 11:23:20.294025   498 caffe.cpp:501] detection_eval = 0.181357
I0512 11:23:20.294034   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.294039   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.294046   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.294052   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.294060   498 caffe.cpp:501] detection_eval = 0.213898
I0512 11:23:20.294066   498 caffe.cpp:501] detection_eval = 0.282353
I0512 11:23:20.294073   498 caffe.cpp:501] detection_eval = 0.717647
I0512 11:23:20.294082   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.294088   498 caffe.cpp:501] detection_eval = 2.16471
I0512 11:23:20.294096   498 caffe.cpp:501] detection_eval = 0.258027
I0512 11:23:20.294101   498 caffe.cpp:501] detection_eval = 0.352941
I0512 11:23:20.294109   498 caffe.cpp:501] detection_eval = 0.647059
I0512 11:23:20.294117   498 caffe.cpp:501] detection_eval = 0.352941
I0512 11:23:20.294123   498 caffe.cpp:501] detection_eval = 2.15294
I0512 11:23:20.294131   498 caffe.cpp:501] detection_eval = 0.228451
I0512 11:23:20.294137   498 caffe.cpp:501] detection_eval = 0.317647
I0512 11:23:20.294144   498 caffe.cpp:501] detection_eval = 0.682353
I0512 11:23:20.294152   498 caffe.cpp:501] detection_eval = 0.458824
I0512 11:23:20.294158   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.294173   498 caffe.cpp:501] detection_eval = 0.184585
I0512 11:23:20.294180   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.294186   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.294193   498 caffe.cpp:501] detection_eval = 0.564706
I0512 11:23:20.294198   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.294209   498 caffe.cpp:501] detection_eval = 0.184007
I0512 11:23:20.294219   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.294229   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.294235   498 caffe.cpp:501] detection_eval = 0.670588
I0512 11:23:20.294242   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.294256   498 caffe.cpp:501] detection_eval = 0.204253
I0512 11:23:20.294263   498 caffe.cpp:501] detection_eval = 0.282353
I0512 11:23:20.294270   498 caffe.cpp:501] detection_eval = 0.717647
I0512 11:23:20.294277   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.294286   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.294292   498 caffe.cpp:501] detection_eval = 0.166235
I0512 11:23:20.294301   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.294307   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.294320   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.294327   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294333   498 caffe.cpp:501] detection_eval = 0.0971925
I0512 11:23:20.294339   498 caffe.cpp:501] detection_eval = 0.141176
I0512 11:23:20.294349   498 caffe.cpp:501] detection_eval = 0.858824
I0512 11:23:20.294361   498 caffe.cpp:501] detection_eval = 0.847059
I0512 11:23:20.294373   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294384   498 caffe.cpp:501] detection_eval = 0.147233
I0512 11:23:20.294394   498 caffe.cpp:501] detection_eval = 0.152941
I0512 11:23:20.294400   498 caffe.cpp:501] detection_eval = 0.847059
I0512 11:23:20.294404   498 caffe.cpp:501] detection_eval = 0.929412
I0512 11:23:20.294410   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294420   498 caffe.cpp:501] detection_eval = 0.158309
I0512 11:23:20.294427   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.294433   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.294441   498 caffe.cpp:501] detection_eval = 1.04706
I0512 11:23:20.294446   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.294453   498 caffe.cpp:501] detection_eval = 0.216051
I0512 11:23:20.294459   498 caffe.cpp:501] detection_eval = 0.294118
I0512 11:23:20.294484   498 caffe.cpp:501] detection_eval = 0.705882
I0512 11:23:20.294497   498 caffe.cpp:501] detection_eval = 1.08235
I0512 11:23:20.294502   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.294508   498 caffe.cpp:501] detection_eval = 0.181983
I0512 11:23:20.294513   498 caffe.cpp:501] detection_eval = 0.305882
I0512 11:23:20.294518   498 caffe.cpp:501] detection_eval = 0.694118
I0512 11:23:20.294526   498 caffe.cpp:501] detection_eval = 1.14118
I0512 11:23:20.294533   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294539   498 caffe.cpp:501] detection_eval = 0.105867
I0512 11:23:20.294544   498 caffe.cpp:501] detection_eval = 0.152941
I0512 11:23:20.294550   498 caffe.cpp:501] detection_eval = 0.847059
I0512 11:23:20.294555   498 caffe.cpp:501] detection_eval = 1.22353
I0512 11:23:20.294562   498 caffe.cpp:501] detection_eval = 2.04706
I0512 11:23:20.294569   498 caffe.cpp:501] detection_eval = 0.19379
I0512 11:23:20.294576   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.294582   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.294589   498 caffe.cpp:501] detection_eval = 1.29412
I0512 11:23:20.294595   498 caffe.cpp:501] detection_eval = 2.05882
I0512 11:23:20.294603   498 caffe.cpp:501] detection_eval = 0.201463
I0512 11:23:20.294612   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.294618   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.294625   498 caffe.cpp:501] detection_eval = 1.36471
I0512 11:23:20.294632   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.294647   498 caffe.cpp:501] detection_eval = 0.18475
I0512 11:23:20.294654   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.294661   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.294667   498 caffe.cpp:501] detection_eval = 1.43529
I0512 11:23:20.294675   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.294683   498 caffe.cpp:501] detection_eval = 0.102636
I0512 11:23:20.294689   498 caffe.cpp:501] detection_eval = 0.152941
I0512 11:23:20.294697   498 caffe.cpp:501] detection_eval = 0.847059
I0512 11:23:20.294703   498 caffe.cpp:501] detection_eval = 1.54118
I0512 11:23:20.294710   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.294715   498 caffe.cpp:501] detection_eval = 0.172987
I0512 11:23:20.294723   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.294732   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.294737   498 caffe.cpp:501] detection_eval = 1.63529
I0512 11:23:20.294744   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294750   498 caffe.cpp:501] detection_eval = 0.228308
I0512 11:23:20.294755   498 caffe.cpp:501] detection_eval = 0.305882
I0512 11:23:20.294762   498 caffe.cpp:501] detection_eval = 0.694118
I0512 11:23:20.294768   498 caffe.cpp:501] detection_eval = 1.72941
I0512 11:23:20.294776   498 caffe.cpp:501] detection_eval = 1.89412
I0512 11:23:20.294782   498 caffe.cpp:501] detection_eval = 0.144654
I0512 11:23:20.294788   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.294795   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.294800   498 caffe.cpp:501] detection_eval = 1.78824
I0512 11:23:20.294806   498 caffe.cpp:501] detection_eval = 1.91765
I0512 11:23:20.294811   498 caffe.cpp:501] detection_eval = 0.116691
I0512 11:23:20.294819   498 caffe.cpp:501] detection_eval = 0.129412
I0512 11:23:20.294826   498 caffe.cpp:501] detection_eval = 0.870588
I0512 11:23:20.294832   498 caffe.cpp:501] detection_eval = 1.83529
I0512 11:23:20.294837   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.294844   498 caffe.cpp:501] detection_eval = 0.172978
I0512 11:23:20.294849   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.294857   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.294862   498 caffe.cpp:501] detection_eval = 1.89412
I0512 11:23:20.294868   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.294878   498 caffe.cpp:501] detection_eval = 0.163564
I0512 11:23:20.294883   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.294889   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.294895   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.294901   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.294905   498 caffe.cpp:501] detection_eval = 0.146315
I0512 11:23:20.294912   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.294919   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.294925   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.294930   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.294937   498 caffe.cpp:501] detection_eval = 0.131982
I0512 11:23:20.294943   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.294950   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.294955   498 caffe.cpp:501] detection_eval = 2.05882
I0512 11:23:20.294961   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.294967   498 caffe.cpp:501] detection_eval = 0.125215
I0512 11:23:20.294975   498 caffe.cpp:501] detection_eval = 0.141176
I0512 11:23:20.294981   498 caffe.cpp:501] detection_eval = 0.858824
I0512 11:23:20.294987   498 caffe.cpp:501] detection_eval = 2.12941
I0512 11:23:20.294993   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.294999   498 caffe.cpp:501] detection_eval = 0.180842
I0512 11:23:20.295004   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.295011   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.295017   498 caffe.cpp:501] detection_eval = 2.17647
I0512 11:23:20.295022   498 caffe.cpp:501] detection_eval = 2.10588
I0512 11:23:20.295035   498 caffe.cpp:501] detection_eval = 0.167863
I0512 11:23:20.295043   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.295048   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.295055   498 caffe.cpp:501] detection_eval = 2.24706
I0512 11:23:20.295061   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.295068   498 caffe.cpp:501] detection_eval = 0.135387
I0512 11:23:20.295074   498 caffe.cpp:501] detection_eval = 0.164706
I0512 11:23:20.295081   498 caffe.cpp:501] detection_eval = 0.835294
I0512 11:23:20.295086   498 caffe.cpp:501] detection_eval = 2.34118
I0512 11:23:20.295090   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.295095   498 caffe.cpp:501] detection_eval = 0.125435
I0512 11:23:20.295102   498 caffe.cpp:501] detection_eval = 0.176471
I0512 11:23:20.295107   498 caffe.cpp:501] detection_eval = 0.823529
I0512 11:23:20.295112   498 caffe.cpp:501] detection_eval = 2.38824
I0512 11:23:20.295120   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.295125   498 caffe.cpp:501] detection_eval = 0.157741
I0512 11:23:20.295131   498 caffe.cpp:501] detection_eval = 0.176471
I0512 11:23:20.295137   498 caffe.cpp:501] detection_eval = 0.823529
I0512 11:23:20.295145   498 caffe.cpp:501] detection_eval = 2.48235
I0512 11:23:20.295150   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.295156   498 caffe.cpp:501] detection_eval = 0.172442
I0512 11:23:20.295162   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.295169   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.295177   498 caffe.cpp:501] detection_eval = 2.55294
I0512 11:23:20.295183   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.295188   498 caffe.cpp:501] detection_eval = 0.159328
I0512 11:23:20.295195   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.295204   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.295209   498 caffe.cpp:501] detection_eval = 2.64706
I0512 11:23:20.295212   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.295217   498 caffe.cpp:501] detection_eval = 0.166562
I0512 11:23:20.295220   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.295223   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.295228   498 caffe.cpp:501] detection_eval = 2.70588
I0512 11:23:20.295231   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.295234   498 caffe.cpp:501] detection_eval = 0.17705
I0512 11:23:20.295238   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.295243   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.295245   498 caffe.cpp:501] detection_eval = 2.72941
I0512 11:23:20.295249   498 caffe.cpp:501] detection_eval = 2.11765
I0512 11:23:20.295253   498 caffe.cpp:501] detection_eval = 0.180486
I0512 11:23:20.295261   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.295266   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.295275   498 caffe.cpp:501] detection_eval = 2.82353
I0512 11:23:20.295280   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.295287   498 caffe.cpp:501] detection_eval = 0.153229
I0512 11:23:20.295294   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.295301   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.295307   498 caffe.cpp:501] detection_eval = 2.90588
I0512 11:23:20.295315   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.295320   498 caffe.cpp:501] detection_eval = 0.151423
I0512 11:23:20.295327   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.295333   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.295339   498 caffe.cpp:501] detection_eval = 2.97647
I0512 11:23:20.295343   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.295347   498 caffe.cpp:501] detection_eval = 0.120919
I0512 11:23:20.295351   498 caffe.cpp:501] detection_eval = 0.164706
I0512 11:23:20.295354   498 caffe.cpp:501] detection_eval = 0.835294
I0512 11:23:20.295358   498 caffe.cpp:501] detection_eval = 3.02353
I0512 11:23:20.295361   498 caffe.cpp:501] detection_eval = 2.10588
I0512 11:23:20.295370   498 caffe.cpp:501] detection_eval = 0.1429
I0512 11:23:20.295374   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.295378   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.295382   498 caffe.cpp:501] detection_eval = 3.10588
I0512 11:23:20.295385   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.295389   498 caffe.cpp:501] detection_eval = 0.152962
I0512 11:23:20.295393   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.295398   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.295405   498 caffe.cpp:501] detection_eval = 3.24706
I0512 11:23:20.295411   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.295416   498 caffe.cpp:501] detection_eval = 0.198546
I0512 11:23:20.295423   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.295428   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.295433   498 caffe.cpp:501] detection_eval = 3.28235
I0512 11:23:20.295440   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.295445   498 caffe.cpp:501] detection_eval = 0.192733
I0512 11:23:20.295454   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.295459   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.295466   498 caffe.cpp:501] detection_eval = 3.38824
I0512 11:23:20.295472   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.295480   498 caffe.cpp:501] detection_eval = 0.199864
I0512 11:23:20.295485   498 caffe.cpp:501] detection_eval = 0.282353
I0512 11:23:20.295491   498 caffe.cpp:501] detection_eval = 0.717647
I0512 11:23:20.295497   498 caffe.cpp:501] detection_eval = 3.41176
I0512 11:23:20.295503   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.295507   498 caffe.cpp:501] detection_eval = 0.148228
I0512 11:23:20.295511   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.295517   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.295524   498 caffe.cpp:501] detection_eval = 3.50588
I0512 11:23:20.295531   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.295537   498 caffe.cpp:501] detection_eval = 0.0952197
I0512 11:23:20.295547   498 caffe.cpp:501] detection_eval = 0.152941
I0512 11:23:20.295552   498 caffe.cpp:501] detection_eval = 0.847059
I0512 11:23:20.295557   498 caffe.cpp:501] detection_eval = 3.6
I0512 11:23:20.295562   498 caffe.cpp:501] detection_eval = 1.90588
I0512 11:23:20.295568   498 caffe.cpp:501] detection_eval = 0.165517
I0512 11:23:20.295573   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.295580   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.295586   498 caffe.cpp:501] detection_eval = 3.64706
I0512 11:23:20.295593   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.295599   498 caffe.cpp:501] detection_eval = 0.191164
I0512 11:23:20.295605   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.295612   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.295619   498 caffe.cpp:501] detection_eval = 3.70588
I0512 11:23:20.295626   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.295632   498 caffe.cpp:501] detection_eval = 0.16917
I0512 11:23:20.295640   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.295646   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.295655   498 caffe.cpp:501] detection_eval = 3.75294
I0512 11:23:20.295660   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.295666   498 caffe.cpp:501] detection_eval = 0.146294
I0512 11:23:20.295672   498 caffe.cpp:501] detection_eval = 0.176471
I0512 11:23:20.295680   498 caffe.cpp:501] detection_eval = 0.823529
I0512 11:23:20.295686   498 caffe.cpp:501] detection_eval = 3.78824
I0512 11:23:20.295693   498 caffe.cpp:501] detection_eval = 2.14118
I0512 11:23:20.295701   498 caffe.cpp:501] detection_eval = 0.13518
I0512 11:23:20.295707   498 caffe.cpp:501] detection_eval = 0.129412
I0512 11:23:20.295716   498 caffe.cpp:501] detection_eval = 0.870588
I0512 11:23:20.295723   498 caffe.cpp:501] detection_eval = 3.91765
I0512 11:23:20.295729   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.295742   498 caffe.cpp:501] detection_eval = 0.179725
I0512 11:23:20.295747   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.295751   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.295758   498 caffe.cpp:501] detection_eval = 4
I0512 11:23:20.295764   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.295771   498 caffe.cpp:501] detection_eval = 0.196782
I0512 11:23:20.295778   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.295784   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.295792   498 caffe.cpp:501] detection_eval = 4.05882
I0512 11:23:20.295796   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.295799   498 caffe.cpp:501] detection_eval = 0.196125
I0512 11:23:20.295804   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.295811   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.295817   498 caffe.cpp:501] detection_eval = 4.10588
I0512 11:23:20.295822   498 caffe.cpp:501] detection_eval = 2.10588
I0512 11:23:20.295827   498 caffe.cpp:501] detection_eval = 0.192171
I0512 11:23:20.295835   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.295840   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.295847   498 caffe.cpp:501] detection_eval = 4.21176
I0512 11:23:20.295855   498 caffe.cpp:501] detection_eval = 2.04706
I0512 11:23:20.295861   498 caffe.cpp:501] detection_eval = 0.175469
I0512 11:23:20.295867   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.295872   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.295879   498 caffe.cpp:501] detection_eval = 4.29412
I0512 11:23:20.295886   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.295892   498 caffe.cpp:501] detection_eval = 0.179329
I0512 11:23:20.295900   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.295907   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.295913   498 caffe.cpp:501] detection_eval = 4.41176
I0512 11:23:20.295918   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.295925   498 caffe.cpp:501] detection_eval = 0.223788
I0512 11:23:20.295933   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.295941   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.295946   498 caffe.cpp:501] detection_eval = 4.45882
I0512 11:23:20.295954   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.295959   498 caffe.cpp:501] detection_eval = 0.19348
I0512 11:23:20.295967   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.295974   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.295981   498 caffe.cpp:501] detection_eval = 4.56471
I0512 11:23:20.295987   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.295994   498 caffe.cpp:501] detection_eval = 0.161602
I0512 11:23:20.296005   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.296011   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.296017   498 caffe.cpp:501] detection_eval = 4.6
I0512 11:23:20.296022   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.296028   498 caffe.cpp:501] detection_eval = 0.159581
I0512 11:23:20.296053   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.296062   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.296068   498 caffe.cpp:501] detection_eval = 4.69412
I0512 11:23:20.296077   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.296084   498 caffe.cpp:501] detection_eval = 0.168702
I0512 11:23:20.296092   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.296097   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.296104   498 caffe.cpp:501] detection_eval = 4.77647
I0512 11:23:20.296111   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.296135   498 caffe.cpp:501] detection_eval = 0.130514
I0512 11:23:20.296146   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.296152   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.296159   498 caffe.cpp:501] detection_eval = 4.88235
I0512 11:23:20.296166   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.296180   498 caffe.cpp:501] detection_eval = 0.208998
I0512 11:23:20.296185   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296195   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296200   498 caffe.cpp:501] detection_eval = 4.94118
I0512 11:23:20.296206   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.296212   498 caffe.cpp:501] detection_eval = 0.214204
I0512 11:23:20.296219   498 caffe.cpp:501] detection_eval = 0.329412
I0512 11:23:20.296226   498 caffe.cpp:501] detection_eval = 0.670588
I0512 11:23:20.296232   498 caffe.cpp:501] detection_eval = 5.02353
I0512 11:23:20.296238   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.296244   498 caffe.cpp:501] detection_eval = 0.175881
I0512 11:23:20.296252   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296260   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296267   498 caffe.cpp:501] detection_eval = 5.07059
I0512 11:23:20.296274   498 caffe.cpp:501] detection_eval = 2.05882
I0512 11:23:20.296283   498 caffe.cpp:501] detection_eval = 0.15614
I0512 11:23:20.296290   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.296299   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.296305   498 caffe.cpp:501] detection_eval = 5.15294
I0512 11:23:20.296311   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.296319   498 caffe.cpp:501] detection_eval = 0.168125
I0512 11:23:20.296325   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.296332   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.296339   498 caffe.cpp:501] detection_eval = 5.25882
I0512 11:23:20.296344   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.296350   498 caffe.cpp:501] detection_eval = 0.24437
I0512 11:23:20.296360   498 caffe.cpp:501] detection_eval = 0.294118
I0512 11:23:20.296368   498 caffe.cpp:501] detection_eval = 0.705882
I0512 11:23:20.296375   498 caffe.cpp:501] detection_eval = 5.30588
I0512 11:23:20.296380   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.296386   498 caffe.cpp:501] detection_eval = 0.165026
I0512 11:23:20.296391   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.296398   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.296404   498 caffe.cpp:501] detection_eval = 5.41176
I0512 11:23:20.296413   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.296418   498 caffe.cpp:501] detection_eval = 0.145657
I0512 11:23:20.296424   498 caffe.cpp:501] detection_eval = 0.176471
I0512 11:23:20.296428   498 caffe.cpp:501] detection_eval = 0.823529
I0512 11:23:20.296432   498 caffe.cpp:501] detection_eval = 5.50588
I0512 11:23:20.296437   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.296442   498 caffe.cpp:501] detection_eval = 0.183287
I0512 11:23:20.296448   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.296455   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.296461   498 caffe.cpp:501] detection_eval = 5.56471
I0512 11:23:20.296483   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.296494   498 caffe.cpp:501] detection_eval = 0.191452
I0512 11:23:20.296500   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296506   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296514   498 caffe.cpp:501] detection_eval = 5.58824
I0512 11:23:20.296520   498 caffe.cpp:501] detection_eval = 2.12941
I0512 11:23:20.296530   498 caffe.cpp:501] detection_eval = 0.189463
I0512 11:23:20.296535   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296540   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296546   498 caffe.cpp:501] detection_eval = 5.69412
I0512 11:23:20.296555   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.296561   498 caffe.cpp:501] detection_eval = 0.182659
I0512 11:23:20.296567   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.296573   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.296581   498 caffe.cpp:501] detection_eval = 5.78824
I0512 11:23:20.296587   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.296603   498 caffe.cpp:501] detection_eval = 0.205268
I0512 11:23:20.296612   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296622   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296629   498 caffe.cpp:501] detection_eval = 5.87059
I0512 11:23:20.296635   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.296643   498 caffe.cpp:501] detection_eval = 0.181426
I0512 11:23:20.296648   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.296656   498 caffe.cpp:501] detection_eval = 0.811765
I0512 11:23:20.296663   498 caffe.cpp:501] detection_eval = 5.94118
I0512 11:23:20.296667   498 caffe.cpp:501] detection_eval = 2.09412
I0512 11:23:20.296676   498 caffe.cpp:501] detection_eval = 0.153337
I0512 11:23:20.296684   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.296690   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.296697   498 caffe.cpp:501] detection_eval = 5.98824
I0512 11:23:20.296705   498 caffe.cpp:501] detection_eval = 2.12941
I0512 11:23:20.296711   498 caffe.cpp:501] detection_eval = 0.188909
I0512 11:23:20.296718   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.296725   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.296733   498 caffe.cpp:501] detection_eval = 6.08235
I0512 11:23:20.296741   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.296749   498 caffe.cpp:501] detection_eval = 0.17706
I0512 11:23:20.296754   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.296761   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.296767   498 caffe.cpp:501] detection_eval = 6.18824
I0512 11:23:20.296774   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.296782   498 caffe.cpp:501] detection_eval = 0.160543
I0512 11:23:20.296790   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296797   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296804   498 caffe.cpp:501] detection_eval = 6.23529
I0512 11:23:20.296809   498 caffe.cpp:501] detection_eval = 2.03529
I0512 11:23:20.296816   498 caffe.cpp:501] detection_eval = 0.158492
I0512 11:23:20.296821   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.296829   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.296836   498 caffe.cpp:501] detection_eval = 6.30588
I0512 11:23:20.296844   498 caffe.cpp:501] detection_eval = 2.08235
I0512 11:23:20.296849   498 caffe.cpp:501] detection_eval = 0.168707
I0512 11:23:20.296854   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.296861   498 caffe.cpp:501] detection_eval = 0.788235
I0512 11:23:20.296865   498 caffe.cpp:501] detection_eval = 6.37647
I0512 11:23:20.296869   498 caffe.cpp:501] detection_eval = 2.07059
I0512 11:23:20.296874   498 caffe.cpp:501] detection_eval = 0.172355
I0512 11:23:20.296881   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.296890   498 caffe.cpp:501] detection_eval = 0.741176
I0512 11:23:20.296900   498 caffe.cpp:501] detection_eval = 6.47059
I0512 11:23:20.296908   498 caffe.cpp:501] detection_eval = 2.04706
I0512 11:23:20.296914   498 caffe.cpp:501] detection_eval = 0.189963
I0512 11:23:20.296921   498 caffe.cpp:501] detection_eval = 0.223529
I0512 11:23:20.296927   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.296933   498 caffe.cpp:501] detection_eval = 6.42353
I0512 11:23:20.296942   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.296954   498 caffe.cpp:501] detection_eval = 0.180246
I0512 11:23:20.296968   498 caffe.cpp:501] detection_eval = 0.235294
I0512 11:23:20.296978   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.296985   498 caffe.cpp:501] detection_eval = 6.55294
I0512 11:23:20.296991   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.297000   498 caffe.cpp:501] detection_eval = 0.237308
I0512 11:23:20.297005   498 caffe.cpp:501] detection_eval = 0.341176
I0512 11:23:20.297009   498 caffe.cpp:501] detection_eval = 0.647059
I0512 11:23:20.297019   498 caffe.cpp:501] detection_eval = 6.61176
I0512 11:23:20.297029   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.297044   498 caffe.cpp:501] detection_eval = 0.230327
I0512 11:23:20.297052   498 caffe.cpp:501] detection_eval = 0.305882
I0512 11:23:20.297062   498 caffe.cpp:501] detection_eval = 0.682353
I0512 11:23:20.297066   498 caffe.cpp:501] detection_eval = 6.72941
I0512 11:23:20.297070   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.297073   498 caffe.cpp:501] detection_eval = 0.202374
I0512 11:23:20.297077   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.297083   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.297087   498 caffe.cpp:501] detection_eval = 6.8
I0512 11:23:20.297091   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.297096   498 caffe.cpp:501] detection_eval = 0.167974
I0512 11:23:20.297101   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.297106   498 caffe.cpp:501] detection_eval = 0.729412
I0512 11:23:20.297113   498 caffe.cpp:501] detection_eval = 6.87059
I0512 11:23:20.297118   498 caffe.cpp:501] detection_eval = 1.94118
I0512 11:23:20.297123   498 caffe.cpp:501] detection_eval = 0.115826
I0512 11:23:20.297130   498 caffe.cpp:501] detection_eval = 0.164706
I0512 11:23:20.297137   498 caffe.cpp:501] detection_eval = 0.823529
I0512 11:23:20.297144   498 caffe.cpp:501] detection_eval = 6.82353
I0512 11:23:20.297150   498 caffe.cpp:501] detection_eval = 1.96471
I0512 11:23:20.297156   498 caffe.cpp:501] detection_eval = 0.167547
I0512 11:23:20.297163   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.297169   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.297178   498 caffe.cpp:501] detection_eval = 6.91765
I0512 11:23:20.297183   498 caffe.cpp:501] detection_eval = 1.98824
I0512 11:23:20.297189   498 caffe.cpp:501] detection_eval = 0.195566
I0512 11:23:20.297195   498 caffe.cpp:501] detection_eval = 0.258824
I0512 11:23:20.297200   498 caffe.cpp:501] detection_eval = 0.717647
I0512 11:23:20.297204   498 caffe.cpp:501] detection_eval = 6.98824
I0512 11:23:20.297212   498 caffe.cpp:501] detection_eval = 2
I0512 11:23:20.297219   498 caffe.cpp:501] detection_eval = 0.140525
I0512 11:23:20.297224   498 caffe.cpp:501] detection_eval = 0.176471
I0512 11:23:20.297232   498 caffe.cpp:501] detection_eval = 0.8
I0512 11:23:20.297237   498 caffe.cpp:501] detection_eval = 6.98824
I0512 11:23:20.297245   498 caffe.cpp:501] detection_eval = 1.90588
I0512 11:23:20.297251   498 caffe.cpp:501] detection_eval = 0.144517
I0512 11:23:20.297257   498 caffe.cpp:501] detection_eval = 0.188235
I0512 11:23:20.297263   498 caffe.cpp:501] detection_eval = 0.776471
I0512 11:23:20.297269   498 caffe.cpp:501] detection_eval = 7.07059
I0512 11:23:20.297274   498 caffe.cpp:501] detection_eval = 1.88235
I0512 11:23:20.297282   498 caffe.cpp:501] detection_eval = 0.187301
I0512 11:23:20.297302   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.297307   498 caffe.cpp:501] detection_eval = 0.717647
I0512 11:23:20.297317   498 caffe.cpp:501] detection_eval = 7.10588
I0512 11:23:20.297322   498 caffe.cpp:501] detection_eval = 1.97647
I0512 11:23:20.297338   498 caffe.cpp:501] detection_eval = 0.160406
I0512 11:23:20.297344   498 caffe.cpp:501] detection_eval = 0.211765
I0512 11:23:20.297348   498 caffe.cpp:501] detection_eval = 0.752941
I0512 11:23:20.297353   498 caffe.cpp:501] detection_eval = 7.15294
I0512 11:23:20.297358   498 caffe.cpp:501] detection_eval = 2.02353
I0512 11:23:20.297363   498 caffe.cpp:501] detection_eval = 0.156009
I0512 11:23:20.297370   498 caffe.cpp:501] detection_eval = 0.2
I0512 11:23:20.297380   498 caffe.cpp:501] detection_eval = 0.764706
I0512 11:23:20.297389   498 caffe.cpp:501] detection_eval = 7.11765
I0512 11:23:20.297395   498 caffe.cpp:501] detection_eval = 2.04706
I0512 11:23:20.297402   498 caffe.cpp:501] detection_eval = 0.195592
I0512 11:23:20.297408   498 caffe.cpp:501] detection_eval = 0.247059
I0512 11:23:20.297415   498 caffe.cpp:501] detection_eval = 0.705882
I0512 11:23:20.297421   498 caffe.cpp:501] detection_eval = 7.07059
I0512 11:23:20.297427   498 caffe.cpp:501] detection_eval = 2.01176
I0512 11:23:20.297449   498 caffe.cpp:501] detection_eval = 0.206687
I0512 11:23:20.297461   498 caffe.cpp:501] detection_eval = 0.270588
I0512 11:23:20.297471   498 caffe.cpp:501] detection_eval = 0.670588
I0512 11:23:20.297796   498 caffe.cpp:546] class AP 1: 0.894039
I0512 11:23:20.298820   498 caffe.cpp:546] class AP 2: 0.882588
I0512 11:23:20.299151   498 caffe.cpp:546] class AP 3: 0.902024
I0512 11:23:20.299157   498 caffe.cpp:552] Test net output mAP #0: detection_eval = 0.892884
I0512 11:23:20.299161   498 caffe.cpp:556] =========================
I0512 11:23:20.299165   498 caffe.cpp:557] Sparsity of the test net:
I0512 11:23:20.300318   498 net.cpp:2769] Num Params(28), Sparsity (zero_weights/count): 
I0512 11:23:20.300326   498 net.cpp:2780] conv1a_param_0(0.343) 
I0512 11:23:20.300338   498 net.cpp:2780] conv1b_param_0(0.674) 
I0512 11:23:20.300345   498 net.cpp:2780] ctx_output1/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300351   498 net.cpp:2780] ctx_output1/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300359   498 net.cpp:2780] ctx_output1_param_0(0) 
I0512 11:23:20.300364   498 net.cpp:2780] ctx_output2/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300371   498 net.cpp:2780] ctx_output2/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300377   498 net.cpp:2780] ctx_output2_param_0(0) 
I0512 11:23:20.300381   498 net.cpp:2780] ctx_output3/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300387   498 net.cpp:2780] ctx_output3/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300390   498 net.cpp:2780] ctx_output3_param_0(0) 
I0512 11:23:20.300395   498 net.cpp:2780] ctx_output4/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300398   498 net.cpp:2780] ctx_output4/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300403   498 net.cpp:2780] ctx_output4_param_0(7.63e-06) 
I0512 11:23:20.300410   498 net.cpp:2780] ctx_output5/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300415   498 net.cpp:2780] ctx_output5/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300420   498 net.cpp:2780] ctx_output5_param_0(0) 
I0512 11:23:20.300428   498 net.cpp:2780] ctx_output6/relu_mbox_conf_param_0(0) 
I0512 11:23:20.300434   498 net.cpp:2780] ctx_output6/relu_mbox_loc_param_0(0) 
I0512 11:23:20.300438   498 net.cpp:2780] ctx_output6_param_0(0) 
I0512 11:23:20.300442   498 net.cpp:2780] res2a_branch2a_param_0(0.781) 
I0512 11:23:20.300446   498 net.cpp:2780] res2a_branch2b_param_0(0.653) 
I0512 11:23:20.300448   498 net.cpp:2780] res3a_branch2a_param_0(0.784) 
I0512 11:23:20.300452   498 net.cpp:2780] res3a_branch2b_param_0(0.75) 
I0512 11:23:20.300456   498 net.cpp:2780] res4a_branch2a_param_0(0.849) 
I0512 11:23:20.300458   498 net.cpp:2780] res4a_branch2b_param_0(0.843) 
I0512 11:23:20.300462   498 net.cpp:2780] res5a_branch2a_param_0(0.844) 
I0512 11:23:20.300464   498 net.cpp:2780] res5a_branch2b_param_0(0.85) 
I0512 11:23:20.300468   498 net.cpp:2784] Total Sparsity (zero_weights/count) =  (1.97991e+06/3.10435e+06) 0.638
I0512 11:23:20.300474   498 caffe.cpp:559] =========================
caffe.bin: ../nptl/pthread_mutex_lock.c:425: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != ESRCH || !robust' failed.
*** Aborted at 1589282600 (unix time) try "date -d @1589282600" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x1f2) received by PID 498 (TID 0x7f455bfff700) from PID 498; stack trace: ***
    @     0x7f45fa1cef20 (unknown)
    @     0x7f45fa1cee97 gsignal
    @     0x7f45fa1d0801 abort
    @     0x7f45fa1c039a (unknown)
    @     0x7f45fa1c0412 __assert_fail
    @     0x7f45f9f7af3c __pthread_mutex_lock_full
    @     0x7f45fbd7f128 boost::mutex::lock()
    @     0x7f45fbd80020 boost::unique_lock<>::lock()
    @     0x7f45fc21789f caffe::BlockingQueue<>::push()
    @     0x7f45fbdfe49e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7f45fbe38476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7f45fbdac7ee caffe::InternalThread::entry()
    @     0x7f45fbdae54b boost::detail::thread_data<>::run()
    @     0x7f45fb3e67ee thread_proxy
    @     0x7f45f9f786db start_thread
    @     0x7f45fa2b188f clone
    @                0x0 (unknown)
I0512 11:23:20.595904   507 caffe.cpp:902] This is NVCaffe 0.17.0 started at Tue May 12 11:23:20 2020
I0512 11:23:20.857865   507 caffe.cpp:904] CuDNN version: 7605
I0512 11:23:20.857874   507 caffe.cpp:905] CuBLAS version: 10202
I0512 11:23:20.857879   507 caffe.cpp:906] CUDA version: 10020
I0512 11:23:20.857882   507 caffe.cpp:907] CUDA driver version: 10020
I0512 11:23:20.857887   507 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: test_detection
[2]: --model=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test_quantize/test.prototxt
[3]: --iterations=85
[4]: --weights=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test/EYES_ssdJacintoNetV2_iter_20000.caffemodel
[5]: --gpu
[6]: 0
I0512 11:23:20.877696   507 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0512 11:23:20.877727   507 gpu_memory.cpp:107] Total memory: 16900227072, Free: 16697655296, dev_info[0]: total=16900227072 free=16697655296
I0512 11:23:20.877934   507 caffe.cpp:406] Use GPU with device ID 0
I0512 11:23:20.878047   507 caffe.cpp:409] GPU device name: Quadro RTX 5000
I0512 11:23:20.914692   507 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 10
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
quantize: true
I0512 11:23:20.915649   507 net.cpp:110] Using FLOAT as default forward math type
I0512 11:23:20.915670   507 net.cpp:116] Using FLOAT as default backward math type
I0512 11:23:20.915681   507 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0512 11:23:20.915688   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:20.915869   507 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:20.916013   507 net.cpp:200] Created Layer data (0)
I0512 11:23:20.916021   507 net.cpp:542] data -> data
I0512 11:23:20.916044   507 net.cpp:542] data -> label
I0512 11:23:20.916064   507 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 10
I0512 11:23:20.916081   507 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:20.916429   512 blocking_queue.cpp:40] Data layer prefetch queue empty
I0512 11:23:20.916649   513 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0512 11:23:20.918941   507 annotated_data_layer.cpp:105] output data size: 10,3,320,768
I0512 11:23:20.919015   507 annotated_data_layer.cpp:150] (0) Output data size: 10, 3, 320, 768
I0512 11:23:20.919073   507 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0512 11:23:20.919169   507 net.cpp:260] Setting up data
I0512 11:23:20.919178   507 net.cpp:267] TEST Top shape for layer 0 'data' 10 3 320 768 (7372800)
I0512 11:23:20.919399   507 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0512 11:23:20.919402   514 data_layer.cpp:105] (0) Parser threads: 1
I0512 11:23:20.919414   507 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0512 11:23:20.919414   514 data_layer.cpp:107] (0) Transformer threads: 1
I0512 11:23:20.919425   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:20.919443   507 net.cpp:200] Created Layer data_data_0_split (1)
I0512 11:23:20.919454   507 net.cpp:572] data_data_0_split <- data
I0512 11:23:20.919473   507 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0512 11:23:20.919486   507 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0512 11:23:20.919490   507 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0512 11:23:20.919495   507 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0512 11:23:20.919500   507 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0512 11:23:20.919504   507 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0512 11:23:20.919509   507 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0512 11:23:20.919590   507 net.cpp:260] Setting up data_data_0_split
I0512 11:23:20.919596   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919602   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919607   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919616   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919625   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919631   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919638   507 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 10 3 320 768 (7372800)
I0512 11:23:20.919657   507 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0512 11:23:20.919662   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:20.919677   507 net.cpp:200] Created Layer data/bias (2)
I0512 11:23:20.919684   507 net.cpp:572] data/bias <- data_data_0_split_0
I0512 11:23:20.919689   507 net.cpp:542] data/bias -> data/bias
I0512 11:23:20.919831   507 net.cpp:260] Setting up data/bias
I0512 11:23:20.919838   507 net.cpp:267] TEST Top shape for layer 2 'data/bias' 10 3 320 768 (7372800)
I0512 11:23:20.919868   507 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0512 11:23:20.919875   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:20.919896   507 net.cpp:200] Created Layer conv1a (3)
I0512 11:23:20.919903   507 net.cpp:572] conv1a <- data/bias
I0512 11:23:20.919909   507 net.cpp:542] conv1a -> conv1a
I0512 11:23:21.835239   507 net.cpp:260] Setting up conv1a
I0512 11:23:21.835263   507 net.cpp:267] TEST Top shape for layer 3 'conv1a' 10 32 160 384 (19660800)
I0512 11:23:21.835286   507 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0512 11:23:21.835292   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.835309   507 net.cpp:200] Created Layer conv1a/bn (4)
I0512 11:23:21.835314   507 net.cpp:572] conv1a/bn <- conv1a
I0512 11:23:21.835319   507 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0512 11:23:21.835701   507 net.cpp:260] Setting up conv1a/bn
I0512 11:23:21.835709   507 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 10 32 160 384 (19660800)
I0512 11:23:21.835726   507 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0512 11:23:21.835731   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.835741   507 net.cpp:200] Created Layer conv1a/relu (5)
I0512 11:23:21.835747   507 net.cpp:572] conv1a/relu <- conv1a
I0512 11:23:21.835752   507 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0512 11:23:21.835777   507 net.cpp:260] Setting up conv1a/relu
I0512 11:23:21.835784   507 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 10 32 160 384 (19660800)
I0512 11:23:21.835791   507 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0512 11:23:21.835796   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.835813   507 net.cpp:200] Created Layer conv1b (6)
I0512 11:23:21.835819   507 net.cpp:572] conv1b <- conv1a
I0512 11:23:21.835822   507 net.cpp:542] conv1b -> conv1b
I0512 11:23:21.836218   507 net.cpp:260] Setting up conv1b
I0512 11:23:21.836227   507 net.cpp:267] TEST Top shape for layer 6 'conv1b' 10 32 160 384 (19660800)
I0512 11:23:21.836237   507 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0512 11:23:21.836244   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.836256   507 net.cpp:200] Created Layer conv1b/bn (7)
I0512 11:23:21.836261   507 net.cpp:572] conv1b/bn <- conv1b
I0512 11:23:21.836266   507 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0512 11:23:21.836550   507 net.cpp:260] Setting up conv1b/bn
I0512 11:23:21.836555   507 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 10 32 160 384 (19660800)
I0512 11:23:21.836566   507 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0512 11:23:21.836571   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.836577   507 net.cpp:200] Created Layer conv1b/relu (8)
I0512 11:23:21.836581   507 net.cpp:572] conv1b/relu <- conv1b
I0512 11:23:21.836585   507 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0512 11:23:21.836594   507 net.cpp:260] Setting up conv1b/relu
I0512 11:23:21.836599   507 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 10 32 160 384 (19660800)
I0512 11:23:21.836624   507 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0512 11:23:21.836628   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.836640   507 net.cpp:200] Created Layer pool1 (9)
I0512 11:23:21.836647   507 net.cpp:572] pool1 <- conv1b
I0512 11:23:21.836653   507 net.cpp:542] pool1 -> pool1
I0512 11:23:21.836715   507 net.cpp:260] Setting up pool1
I0512 11:23:21.836720   507 net.cpp:267] TEST Top shape for layer 9 'pool1' 10 32 80 192 (4915200)
I0512 11:23:21.836727   507 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0512 11:23:21.836730   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.836740   507 net.cpp:200] Created Layer res2a_branch2a (10)
I0512 11:23:21.836745   507 net.cpp:572] res2a_branch2a <- pool1
I0512 11:23:21.836751   507 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0512 11:23:21.837821   507 net.cpp:260] Setting up res2a_branch2a
I0512 11:23:21.837831   507 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 10 64 80 192 (9830400)
I0512 11:23:21.837847   507 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:21.837854   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.837863   507 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0512 11:23:21.837872   507 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0512 11:23:21.837879   507 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0512 11:23:21.838140   507 net.cpp:260] Setting up res2a_branch2a/bn
I0512 11:23:21.838146   507 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 10 64 80 192 (9830400)
I0512 11:23:21.838157   507 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0512 11:23:21.838165   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838169   507 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0512 11:23:21.838174   507 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0512 11:23:21.838182   507 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0512 11:23:21.838189   507 net.cpp:260] Setting up res2a_branch2a/relu
I0512 11:23:21.838196   507 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 10 64 80 192 (9830400)
I0512 11:23:21.838207   507 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0512 11:23:21.838212   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838227   507 net.cpp:200] Created Layer res2a_branch2b (13)
I0512 11:23:21.838233   507 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0512 11:23:21.838238   507 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0512 11:23:21.838515   507 net.cpp:260] Setting up res2a_branch2b
I0512 11:23:21.838521   507 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 10 64 80 192 (9830400)
I0512 11:23:21.838531   507 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:21.838537   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838546   507 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0512 11:23:21.838551   507 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0512 11:23:21.838557   507 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0512 11:23:21.838802   507 net.cpp:260] Setting up res2a_branch2b/bn
I0512 11:23:21.838807   507 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 10 64 80 192 (9830400)
I0512 11:23:21.838816   507 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0512 11:23:21.838821   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838826   507 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0512 11:23:21.838830   507 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0512 11:23:21.838850   507 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0512 11:23:21.838855   507 net.cpp:260] Setting up res2a_branch2b/relu
I0512 11:23:21.838860   507 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 10 64 80 192 (9830400)
I0512 11:23:21.838865   507 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0512 11:23:21.838869   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838881   507 net.cpp:200] Created Layer pool2 (16)
I0512 11:23:21.838886   507 net.cpp:572] pool2 <- res2a_branch2b
I0512 11:23:21.838889   507 net.cpp:542] pool2 -> pool2
I0512 11:23:21.838935   507 net.cpp:260] Setting up pool2
I0512 11:23:21.838940   507 net.cpp:267] TEST Top shape for layer 16 'pool2' 10 64 40 96 (2457600)
I0512 11:23:21.838948   507 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0512 11:23:21.838953   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.838963   507 net.cpp:200] Created Layer res3a_branch2a (17)
I0512 11:23:21.838966   507 net.cpp:572] res3a_branch2a <- pool2
I0512 11:23:21.838971   507 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0512 11:23:21.839953   507 net.cpp:260] Setting up res3a_branch2a
I0512 11:23:21.839962   507 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 10 128 40 96 (4915200)
I0512 11:23:21.839970   507 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:21.839975   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.839982   507 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0512 11:23:21.839989   507 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0512 11:23:21.839993   507 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0512 11:23:21.840225   507 net.cpp:260] Setting up res3a_branch2a/bn
I0512 11:23:21.840230   507 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 10 128 40 96 (4915200)
I0512 11:23:21.840241   507 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0512 11:23:21.840247   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.840252   507 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0512 11:23:21.840256   507 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0512 11:23:21.840260   507 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0512 11:23:21.840268   507 net.cpp:260] Setting up res3a_branch2a/relu
I0512 11:23:21.840276   507 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 10 128 40 96 (4915200)
I0512 11:23:21.840281   507 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0512 11:23:21.840286   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.840298   507 net.cpp:200] Created Layer res3a_branch2b (20)
I0512 11:23:21.840306   507 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0512 11:23:21.840310   507 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0512 11:23:21.840873   507 net.cpp:260] Setting up res3a_branch2b
I0512 11:23:21.840879   507 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 10 128 40 96 (4915200)
I0512 11:23:21.840888   507 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:21.840893   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.840899   507 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0512 11:23:21.840904   507 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0512 11:23:21.840909   507 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0512 11:23:21.841145   507 net.cpp:260] Setting up res3a_branch2b/bn
I0512 11:23:21.841150   507 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 10 128 40 96 (4915200)
I0512 11:23:21.841159   507 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0512 11:23:21.841174   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.841181   507 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0512 11:23:21.841188   507 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0512 11:23:21.841194   507 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0512 11:23:21.841204   507 net.cpp:260] Setting up res3a_branch2b/relu
I0512 11:23:21.841210   507 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 10 128 40 96 (4915200)
I0512 11:23:21.841220   507 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0512 11:23:21.841228   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.841234   507 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0512 11:23:21.841241   507 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0512 11:23:21.841246   507 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0512 11:23:21.841255   507 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0512 11:23:21.841286   507 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0512 11:23:21.841298   507 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 10 128 40 96 (4915200)
I0512 11:23:21.841305   507 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 10 128 40 96 (4915200)
I0512 11:23:21.841315   507 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0512 11:23:21.841320   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.841331   507 net.cpp:200] Created Layer pool3 (24)
I0512 11:23:21.841337   507 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0512 11:23:21.841342   507 net.cpp:542] pool3 -> pool3
I0512 11:23:21.841380   507 net.cpp:260] Setting up pool3
I0512 11:23:21.841385   507 net.cpp:267] TEST Top shape for layer 24 'pool3' 10 128 20 48 (1228800)
I0512 11:23:21.841392   507 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0512 11:23:21.841398   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.841410   507 net.cpp:200] Created Layer res4a_branch2a (25)
I0512 11:23:21.841415   507 net.cpp:572] res4a_branch2a <- pool3
I0512 11:23:21.841421   507 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0512 11:23:21.845362   507 net.cpp:260] Setting up res4a_branch2a
I0512 11:23:21.845374   507 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 10 256 20 48 (2457600)
I0512 11:23:21.845386   507 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:21.845404   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.845414   507 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0512 11:23:21.845422   507 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0512 11:23:21.845428   507 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0512 11:23:21.845679   507 net.cpp:260] Setting up res4a_branch2a/bn
I0512 11:23:21.845685   507 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 10 256 20 48 (2457600)
I0512 11:23:21.845695   507 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0512 11:23:21.845701   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.845711   507 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0512 11:23:21.845716   507 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0512 11:23:21.845724   507 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0512 11:23:21.845733   507 net.cpp:260] Setting up res4a_branch2a/relu
I0512 11:23:21.845737   507 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 10 256 20 48 (2457600)
I0512 11:23:21.845758   507 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0512 11:23:21.845763   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.845773   507 net.cpp:200] Created Layer res4a_branch2b (28)
I0512 11:23:21.845779   507 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0512 11:23:21.845785   507 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0512 11:23:21.847524   507 net.cpp:260] Setting up res4a_branch2b
I0512 11:23:21.847532   507 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 10 256 20 48 (2457600)
I0512 11:23:21.847545   507 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:21.847553   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.847560   507 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0512 11:23:21.847568   507 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0512 11:23:21.847573   507 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0512 11:23:21.847820   507 net.cpp:260] Setting up res4a_branch2b/bn
I0512 11:23:21.847826   507 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 10 256 20 48 (2457600)
I0512 11:23:21.847836   507 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0512 11:23:21.847841   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.847847   507 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0512 11:23:21.847851   507 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0512 11:23:21.847856   507 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0512 11:23:21.847864   507 net.cpp:260] Setting up res4a_branch2b/relu
I0512 11:23:21.847868   507 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 10 256 20 48 (2457600)
I0512 11:23:21.847877   507 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0512 11:23:21.847883   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.847893   507 net.cpp:200] Created Layer pool4 (31)
I0512 11:23:21.847900   507 net.cpp:572] pool4 <- res4a_branch2b
I0512 11:23:21.847905   507 net.cpp:542] pool4 -> pool4
I0512 11:23:21.847944   507 net.cpp:260] Setting up pool4
I0512 11:23:21.847950   507 net.cpp:267] TEST Top shape for layer 31 'pool4' 10 256 10 24 (614400)
I0512 11:23:21.847956   507 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0512 11:23:21.847960   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.847976   507 net.cpp:200] Created Layer res5a_branch2a (32)
I0512 11:23:21.847983   507 net.cpp:572] res5a_branch2a <- pool4
I0512 11:23:21.847988   507 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0512 11:23:21.861873   507 net.cpp:260] Setting up res5a_branch2a
I0512 11:23:21.861887   507 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 10 512 10 24 (1228800)
I0512 11:23:21.861897   507 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0512 11:23:21.861902   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.861914   507 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0512 11:23:21.861922   507 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0512 11:23:21.861928   507 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0512 11:23:21.862185   507 net.cpp:260] Setting up res5a_branch2a/bn
I0512 11:23:21.862191   507 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 10 512 10 24 (1228800)
I0512 11:23:21.862201   507 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0512 11:23:21.862208   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.862215   507 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0512 11:23:21.862241   507 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0512 11:23:21.862247   507 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0512 11:23:21.862254   507 net.cpp:260] Setting up res5a_branch2a/relu
I0512 11:23:21.862259   507 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 10 512 10 24 (1228800)
I0512 11:23:21.862265   507 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0512 11:23:21.862272   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.862284   507 net.cpp:200] Created Layer res5a_branch2b (35)
I0512 11:23:21.862293   507 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0512 11:23:21.862298   507 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0512 11:23:21.869452   507 net.cpp:260] Setting up res5a_branch2b
I0512 11:23:21.869464   507 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 10 512 10 24 (1228800)
I0512 11:23:21.869480   507 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0512 11:23:21.869485   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.869493   507 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0512 11:23:21.869498   507 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0512 11:23:21.869503   507 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0512 11:23:21.869760   507 net.cpp:260] Setting up res5a_branch2b/bn
I0512 11:23:21.869765   507 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 10 512 10 24 (1228800)
I0512 11:23:21.869774   507 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0512 11:23:21.869779   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.869784   507 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0512 11:23:21.869791   507 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0512 11:23:21.869796   507 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0512 11:23:21.869803   507 net.cpp:260] Setting up res5a_branch2b/relu
I0512 11:23:21.869809   507 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 10 512 10 24 (1228800)
I0512 11:23:21.869819   507 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0512 11:23:21.869825   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.869833   507 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0512 11:23:21.869840   507 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0512 11:23:21.869848   507 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0512 11:23:21.869858   507 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0512 11:23:21.869895   507 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0512 11:23:21.869904   507 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 10 512 10 24 (1228800)
I0512 11:23:21.869910   507 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 10 512 10 24 (1228800)
I0512 11:23:21.869915   507 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0512 11:23:21.869918   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.869925   507 net.cpp:200] Created Layer pool6 (39)
I0512 11:23:21.869930   507 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0512 11:23:21.869936   507 net.cpp:542] pool6 -> pool6
I0512 11:23:21.869989   507 net.cpp:260] Setting up pool6
I0512 11:23:21.869995   507 net.cpp:267] TEST Top shape for layer 39 'pool6' 10 512 5 12 (307200)
I0512 11:23:21.870002   507 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0512 11:23:21.870005   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870023   507 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0512 11:23:21.870028   507 net.cpp:572] pool6_pool6_0_split <- pool6
I0512 11:23:21.870035   507 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0512 11:23:21.870040   507 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0512 11:23:21.870070   507 net.cpp:260] Setting up pool6_pool6_0_split
I0512 11:23:21.870079   507 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 10 512 5 12 (307200)
I0512 11:23:21.870088   507 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 10 512 5 12 (307200)
I0512 11:23:21.870098   507 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0512 11:23:21.870102   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870111   507 net.cpp:200] Created Layer pool7 (41)
I0512 11:23:21.870119   507 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0512 11:23:21.870126   507 net.cpp:542] pool7 -> pool7
I0512 11:23:21.870172   507 net.cpp:260] Setting up pool7
I0512 11:23:21.870177   507 net.cpp:267] TEST Top shape for layer 41 'pool7' 10 512 3 6 (92160)
I0512 11:23:21.870183   507 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0512 11:23:21.870187   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870193   507 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0512 11:23:21.870196   507 net.cpp:572] pool7_pool7_0_split <- pool7
I0512 11:23:21.870201   507 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0512 11:23:21.870213   507 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0512 11:23:21.870244   507 net.cpp:260] Setting up pool7_pool7_0_split
I0512 11:23:21.870250   507 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 10 512 3 6 (92160)
I0512 11:23:21.870257   507 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 10 512 3 6 (92160)
I0512 11:23:21.870265   507 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0512 11:23:21.870268   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870273   507 net.cpp:200] Created Layer pool8 (43)
I0512 11:23:21.870280   507 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0512 11:23:21.870286   507 net.cpp:542] pool8 -> pool8
I0512 11:23:21.870333   507 net.cpp:260] Setting up pool8
I0512 11:23:21.870339   507 net.cpp:267] TEST Top shape for layer 43 'pool8' 10 512 2 3 (30720)
I0512 11:23:21.870345   507 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0512 11:23:21.870349   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870354   507 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0512 11:23:21.870358   507 net.cpp:572] pool8_pool8_0_split <- pool8
I0512 11:23:21.870362   507 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0512 11:23:21.870366   507 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0512 11:23:21.870395   507 net.cpp:260] Setting up pool8_pool8_0_split
I0512 11:23:21.870400   507 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 10 512 2 3 (30720)
I0512 11:23:21.870405   507 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 10 512 2 3 (30720)
I0512 11:23:21.870412   507 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0512 11:23:21.870419   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870426   507 net.cpp:200] Created Layer pool9 (45)
I0512 11:23:21.870434   507 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0512 11:23:21.870440   507 net.cpp:542] pool9 -> pool9
I0512 11:23:21.870476   507 net.cpp:260] Setting up pool9
I0512 11:23:21.870481   507 net.cpp:267] TEST Top shape for layer 45 'pool9' 10 512 1 2 (10240)
I0512 11:23:21.870486   507 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0512 11:23:21.870496   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.870507   507 net.cpp:200] Created Layer ctx_output1 (46)
I0512 11:23:21.870510   507 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0512 11:23:21.870517   507 net.cpp:542] ctx_output1 -> ctx_output1
I0512 11:23:21.871067   507 net.cpp:260] Setting up ctx_output1
I0512 11:23:21.871071   507 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 10 256 40 96 (9830400)
I0512 11:23:21.871080   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0512 11:23:21.871084   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.871089   507 net.cpp:200] Created Layer ctx_output1/relu (47)
I0512 11:23:21.871093   507 net.cpp:572] ctx_output1/relu <- ctx_output1
I0512 11:23:21.871098   507 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0512 11:23:21.871102   507 net.cpp:260] Setting up ctx_output1/relu
I0512 11:23:21.871105   507 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 10 256 40 96 (9830400)
I0512 11:23:21.871111   507 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0512 11:23:21.871115   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.871120   507 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0512 11:23:21.871124   507 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0512 11:23:21.871127   507 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0512 11:23:21.871132   507 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0512 11:23:21.871137   507 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0512 11:23:21.871168   507 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0512 11:23:21.871171   507 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:21.871176   507 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:21.871181   507 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 10 256 40 96 (9830400)
I0512 11:23:21.871187   507 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0512 11:23:21.871191   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.871199   507 net.cpp:200] Created Layer ctx_output2 (49)
I0512 11:23:21.871203   507 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0512 11:23:21.871208   507 net.cpp:542] ctx_output2 -> ctx_output2
I0512 11:23:21.872763   507 net.cpp:260] Setting up ctx_output2
I0512 11:23:21.872768   507 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 10 256 10 24 (614400)
I0512 11:23:21.872777   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0512 11:23:21.872782   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.872787   507 net.cpp:200] Created Layer ctx_output2/relu (50)
I0512 11:23:21.872792   507 net.cpp:572] ctx_output2/relu <- ctx_output2
I0512 11:23:21.872795   507 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0512 11:23:21.872800   507 net.cpp:260] Setting up ctx_output2/relu
I0512 11:23:21.872803   507 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 10 256 10 24 (614400)
I0512 11:23:21.872810   507 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0512 11:23:21.872814   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.872820   507 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0512 11:23:21.872823   507 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0512 11:23:21.872835   507 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0512 11:23:21.872841   507 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0512 11:23:21.872845   507 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0512 11:23:21.872881   507 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0512 11:23:21.872885   507 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:21.872890   507 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:21.872895   507 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 10 256 10 24 (614400)
I0512 11:23:21.872900   507 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0512 11:23:21.872905   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.872912   507 net.cpp:200] Created Layer ctx_output3 (52)
I0512 11:23:21.872916   507 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0512 11:23:21.872921   507 net.cpp:542] ctx_output3 -> ctx_output3
I0512 11:23:21.875221   507 net.cpp:260] Setting up ctx_output3
I0512 11:23:21.875233   507 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 10 256 5 12 (153600)
I0512 11:23:21.875250   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0512 11:23:21.875258   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.875267   507 net.cpp:200] Created Layer ctx_output3/relu (53)
I0512 11:23:21.875278   507 net.cpp:572] ctx_output3/relu <- ctx_output3
I0512 11:23:21.875289   507 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0512 11:23:21.875303   507 net.cpp:260] Setting up ctx_output3/relu
I0512 11:23:21.875309   507 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 10 256 5 12 (153600)
I0512 11:23:21.875319   507 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0512 11:23:21.875326   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.875335   507 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0512 11:23:21.875347   507 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0512 11:23:21.875355   507 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0512 11:23:21.875362   507 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0512 11:23:21.875367   507 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0512 11:23:21.875407   507 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0512 11:23:21.875411   507 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:21.875417   507 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:21.875422   507 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 10 256 5 12 (153600)
I0512 11:23:21.875427   507 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0512 11:23:21.875432   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.875440   507 net.cpp:200] Created Layer ctx_output4 (55)
I0512 11:23:21.875444   507 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0512 11:23:21.875449   507 net.cpp:542] ctx_output4 -> ctx_output4
I0512 11:23:21.877023   507 net.cpp:260] Setting up ctx_output4
I0512 11:23:21.877028   507 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 10 256 3 6 (46080)
I0512 11:23:21.877038   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0512 11:23:21.877054   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.877060   507 net.cpp:200] Created Layer ctx_output4/relu (56)
I0512 11:23:21.877065   507 net.cpp:572] ctx_output4/relu <- ctx_output4
I0512 11:23:21.877069   507 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0512 11:23:21.877075   507 net.cpp:260] Setting up ctx_output4/relu
I0512 11:23:21.877079   507 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 10 256 3 6 (46080)
I0512 11:23:21.877084   507 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0512 11:23:21.877089   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.877094   507 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0512 11:23:21.877097   507 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0512 11:23:21.877100   507 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0512 11:23:21.877105   507 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0512 11:23:21.877110   507 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0512 11:23:21.877147   507 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0512 11:23:21.877151   507 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:21.877156   507 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:21.877161   507 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 10 256 3 6 (46080)
I0512 11:23:21.877166   507 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0512 11:23:21.877169   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.877178   507 net.cpp:200] Created Layer ctx_output5 (58)
I0512 11:23:21.877182   507 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0512 11:23:21.877187   507 net.cpp:542] ctx_output5 -> ctx_output5
I0512 11:23:21.878872   507 net.cpp:260] Setting up ctx_output5
I0512 11:23:21.878882   507 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 10 256 2 3 (15360)
I0512 11:23:21.878892   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0512 11:23:21.878898   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.878904   507 net.cpp:200] Created Layer ctx_output5/relu (59)
I0512 11:23:21.878909   507 net.cpp:572] ctx_output5/relu <- ctx_output5
I0512 11:23:21.878916   507 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0512 11:23:21.878921   507 net.cpp:260] Setting up ctx_output5/relu
I0512 11:23:21.878924   507 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 10 256 2 3 (15360)
I0512 11:23:21.878931   507 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0512 11:23:21.878934   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.878942   507 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0512 11:23:21.878947   507 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0512 11:23:21.878952   507 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0512 11:23:21.878957   507 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0512 11:23:21.878962   507 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0512 11:23:21.879001   507 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0512 11:23:21.879009   507 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:21.879017   507 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:21.879038   507 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 10 256 2 3 (15360)
I0512 11:23:21.879050   507 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0512 11:23:21.879056   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.879070   507 net.cpp:200] Created Layer ctx_output6 (61)
I0512 11:23:21.879076   507 net.cpp:572] ctx_output6 <- pool9
I0512 11:23:21.879082   507 net.cpp:542] ctx_output6 -> ctx_output6
I0512 11:23:21.880667   507 net.cpp:260] Setting up ctx_output6
I0512 11:23:21.880676   507 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 10 256 1 2 (5120)
I0512 11:23:21.880687   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0512 11:23:21.880693   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.880699   507 net.cpp:200] Created Layer ctx_output6/relu (62)
I0512 11:23:21.880703   507 net.cpp:572] ctx_output6/relu <- ctx_output6
I0512 11:23:21.880709   507 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0512 11:23:21.880717   507 net.cpp:260] Setting up ctx_output6/relu
I0512 11:23:21.880725   507 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 10 256 1 2 (5120)
I0512 11:23:21.880733   507 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0512 11:23:21.880738   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.880743   507 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0512 11:23:21.880750   507 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0512 11:23:21.880756   507 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0512 11:23:21.880761   507 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0512 11:23:21.880765   507 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0512 11:23:21.880808   507 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0512 11:23:21.880815   507 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:21.880820   507 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:21.880826   507 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 10 256 1 2 (5120)
I0512 11:23:21.880836   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.880839   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.880858   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0512 11:23:21.880867   507 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0512 11:23:21.880875   507 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0512 11:23:21.881116   507 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0512 11:23:21.881122   507 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 10 16 40 96 (614400)
I0512 11:23:21.881131   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.881139   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.881152   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0512 11:23:21.881160   507 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0512 11:23:21.881165   507 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0512 11:23:21.881264   507 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0512 11:23:21.881270   507 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 10 40 96 16 (614400)
I0512 11:23:21.881291   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.881296   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.881305   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0512 11:23:21.881309   507 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0512 11:23:21.881314   507 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0512 11:23:21.883324   507 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0512 11:23:21.883337   507 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 10 61440 (614400)
I0512 11:23:21.883347   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.883352   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.883365   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0512 11:23:21.883371   507 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0512 11:23:21.883378   507 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0512 11:23:21.883661   507 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0512 11:23:21.883667   507 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 10 16 40 96 (614400)
I0512 11:23:21.883678   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.883687   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.883700   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0512 11:23:21.883708   507 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0512 11:23:21.883716   507 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0512 11:23:21.883790   507 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0512 11:23:21.883795   507 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 10 40 96 16 (614400)
I0512 11:23:21.883802   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.883810   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.883818   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0512 11:23:21.883828   507 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0512 11:23:21.883834   507 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0512 11:23:21.885782   507 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0512 11:23:21.885794   507 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 10 61440 (614400)
I0512 11:23:21.885804   507 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.885810   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.885828   507 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0512 11:23:21.885838   507 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0512 11:23:21.885848   507 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0512 11:23:21.885855   507 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0512 11:23:21.885900   507 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0512 11:23:21.885905   507 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0512 11:23:21.885913   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.885921   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.885936   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0512 11:23:21.885953   507 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0512 11:23:21.885960   507 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0512 11:23:21.886247   507 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0512 11:23:21.886255   507 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 10 24 10 24 (57600)
I0512 11:23:21.886265   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.886272   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.886283   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0512 11:23:21.886292   507 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0512 11:23:21.886301   507 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0512 11:23:21.886392   507 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0512 11:23:21.886399   507 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 10 10 24 24 (57600)
I0512 11:23:21.886405   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.886409   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.886415   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0512 11:23:21.886420   507 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0512 11:23:21.886425   507 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0512 11:23:21.887377   507 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0512 11:23:21.887387   507 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 10 5760 (57600)
I0512 11:23:21.887395   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.887399   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.887414   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0512 11:23:21.887423   507 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0512 11:23:21.887432   507 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0512 11:23:21.887727   507 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0512 11:23:21.887734   507 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 10 24 10 24 (57600)
I0512 11:23:21.887745   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.887749   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.887756   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0512 11:23:21.887763   507 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0512 11:23:21.887769   507 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0512 11:23:21.887851   507 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0512 11:23:21.887856   507 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 10 10 24 24 (57600)
I0512 11:23:21.887863   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.887867   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.887874   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0512 11:23:21.887881   507 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0512 11:23:21.887887   507 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0512 11:23:21.888517   507 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0512 11:23:21.888526   507 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 10 5760 (57600)
I0512 11:23:21.888547   507 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.888551   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.888561   507 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0512 11:23:21.888567   507 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0512 11:23:21.888576   507 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0512 11:23:21.888583   507 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0512 11:23:21.888613   507 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0512 11:23:21.888620   507 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0512 11:23:21.888628   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.888631   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.888644   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0512 11:23:21.888651   507 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0512 11:23:21.888659   507 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0512 11:23:21.888947   507 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0512 11:23:21.888953   507 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 10 24 5 12 (14400)
I0512 11:23:21.888964   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.888969   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.888976   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0512 11:23:21.888981   507 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0512 11:23:21.888988   507 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0512 11:23:21.889068   507 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0512 11:23:21.889075   507 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 10 5 12 24 (14400)
I0512 11:23:21.889081   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.889086   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889096   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0512 11:23:21.889102   507 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0512 11:23:21.889108   507 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0512 11:23:21.889173   507 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0512 11:23:21.889178   507 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 10 1440 (14400)
I0512 11:23:21.889184   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.889189   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889201   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0512 11:23:21.889209   507 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0512 11:23:21.889216   507 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0512 11:23:21.889487   507 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0512 11:23:21.889494   507 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 10 24 5 12 (14400)
I0512 11:23:21.889503   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.889509   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889516   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0512 11:23:21.889535   507 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0512 11:23:21.889542   507 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0512 11:23:21.889619   507 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0512 11:23:21.889624   507 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 10 5 12 24 (14400)
I0512 11:23:21.889631   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.889634   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889641   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0512 11:23:21.889647   507 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0512 11:23:21.889652   507 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0512 11:23:21.889714   507 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0512 11:23:21.889720   507 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 10 1440 (14400)
I0512 11:23:21.889726   507 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.889731   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889737   507 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0512 11:23:21.889744   507 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0512 11:23:21.889751   507 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0512 11:23:21.889760   507 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0512 11:23:21.889778   507 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0512 11:23:21.889786   507 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0512 11:23:21.889794   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.889798   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.889811   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0512 11:23:21.889818   507 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0512 11:23:21.889825   507 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0512 11:23:21.890096   507 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0512 11:23:21.890102   507 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 10 24 3 6 (4320)
I0512 11:23:21.890111   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.890116   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890125   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0512 11:23:21.890131   507 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0512 11:23:21.890137   507 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0512 11:23:21.890236   507 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0512 11:23:21.890241   507 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 10 3 6 24 (4320)
I0512 11:23:21.890247   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.890252   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890259   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0512 11:23:21.890266   507 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0512 11:23:21.890272   507 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0512 11:23:21.890339   507 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0512 11:23:21.890353   507 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 10 432 (4320)
I0512 11:23:21.890359   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.890364   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890377   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0512 11:23:21.890384   507 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0512 11:23:21.890391   507 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0512 11:23:21.890656   507 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0512 11:23:21.890662   507 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 10 24 3 6 (4320)
I0512 11:23:21.890671   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.890676   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890686   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0512 11:23:21.890692   507 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0512 11:23:21.890699   507 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0512 11:23:21.890779   507 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0512 11:23:21.890784   507 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 10 3 6 24 (4320)
I0512 11:23:21.890790   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.890794   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890799   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0512 11:23:21.890806   507 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0512 11:23:21.890812   507 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0512 11:23:21.890864   507 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0512 11:23:21.890869   507 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 10 432 (4320)
I0512 11:23:21.890875   507 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.890882   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890890   507 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0512 11:23:21.890897   507 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0512 11:23:21.890904   507 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0512 11:23:21.890910   507 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0512 11:23:21.890931   507 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0512 11:23:21.890938   507 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0512 11:23:21.890945   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.890949   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.890964   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0512 11:23:21.890969   507 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0512 11:23:21.890975   507 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0512 11:23:21.891201   507 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0512 11:23:21.891206   507 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 10 16 2 3 (960)
I0512 11:23:21.891214   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.891221   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891242   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0512 11:23:21.891247   507 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0512 11:23:21.891254   507 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0512 11:23:21.891332   507 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0512 11:23:21.891338   507 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 10 2 3 16 (960)
I0512 11:23:21.891345   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.891348   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891353   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0512 11:23:21.891360   507 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0512 11:23:21.891364   507 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0512 11:23:21.891424   507 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0512 11:23:21.891432   507 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 10 96 (960)
I0512 11:23:21.891438   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.891441   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891453   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0512 11:23:21.891458   507 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0512 11:23:21.891465   507 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0512 11:23:21.891706   507 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0512 11:23:21.891713   507 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 10 16 2 3 (960)
I0512 11:23:21.891721   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.891726   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891733   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0512 11:23:21.891739   507 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0512 11:23:21.891744   507 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0512 11:23:21.891820   507 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0512 11:23:21.891825   507 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 10 2 3 16 (960)
I0512 11:23:21.891831   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.891835   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891841   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0512 11:23:21.891845   507 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0512 11:23:21.891852   507 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0512 11:23:21.891896   507 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0512 11:23:21.891901   507 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 10 96 (960)
I0512 11:23:21.891907   507 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.891911   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891917   507 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0512 11:23:21.891921   507 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0512 11:23:21.891927   507 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0512 11:23:21.891930   507 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0512 11:23:21.891960   507 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0512 11:23:21.891964   507 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0512 11:23:21.891969   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0512 11:23:21.891973   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.891983   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0512 11:23:21.891988   507 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0512 11:23:21.891993   507 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0512 11:23:21.892213   507 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0512 11:23:21.892220   507 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 10 16 1 2 (320)
I0512 11:23:21.892227   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0512 11:23:21.892231   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892238   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0512 11:23:21.892243   507 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0512 11:23:21.892247   507 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0512 11:23:21.892320   507 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0512 11:23:21.892325   507 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 10 1 2 16 (320)
I0512 11:23:21.892333   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0512 11:23:21.892338   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892343   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0512 11:23:21.892349   507 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0512 11:23:21.892354   507 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0512 11:23:21.892407   507 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0512 11:23:21.892413   507 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 10 32 (320)
I0512 11:23:21.892419   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0512 11:23:21.892423   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892432   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0512 11:23:21.892438   507 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0512 11:23:21.892444   507 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0512 11:23:21.892659   507 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0512 11:23:21.892665   507 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 10 16 1 2 (320)
I0512 11:23:21.892673   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0512 11:23:21.892678   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892684   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0512 11:23:21.892688   507 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0512 11:23:21.892693   507 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0512 11:23:21.892760   507 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0512 11:23:21.892765   507 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 10 1 2 16 (320)
I0512 11:23:21.892769   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0512 11:23:21.892773   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892789   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0512 11:23:21.892793   507 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0512 11:23:21.892797   507 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0512 11:23:21.892838   507 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0512 11:23:21.892843   507 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 10 32 (320)
I0512 11:23:21.892848   507 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0512 11:23:21.892854   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892861   507 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0512 11:23:21.892869   507 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0512 11:23:21.892874   507 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0512 11:23:21.892881   507 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0512 11:23:21.892897   507 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0512 11:23:21.892901   507 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0512 11:23:21.892906   507 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0512 11:23:21.892910   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.892920   507 net.cpp:200] Created Layer mbox_loc (106)
I0512 11:23:21.892925   507 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0512 11:23:21.892930   507 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0512 11:23:21.892936   507 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0512 11:23:21.892943   507 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0512 11:23:21.892948   507 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0512 11:23:21.892956   507 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0512 11:23:21.892961   507 net.cpp:542] mbox_loc -> mbox_loc
I0512 11:23:21.892982   507 net.cpp:260] Setting up mbox_loc
I0512 11:23:21.892992   507 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 10 69200 (692000)
I0512 11:23:21.893000   507 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0512 11:23:21.893007   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.893016   507 net.cpp:200] Created Layer mbox_conf (107)
I0512 11:23:21.893023   507 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0512 11:23:21.893030   507 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0512 11:23:21.893038   507 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0512 11:23:21.893044   507 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0512 11:23:21.893050   507 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0512 11:23:21.893057   507 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0512 11:23:21.893064   507 net.cpp:542] mbox_conf -> mbox_conf
I0512 11:23:21.893085   507 net.cpp:260] Setting up mbox_conf
I0512 11:23:21.893091   507 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 10 69200 (692000)
I0512 11:23:21.893102   507 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0512 11:23:21.893108   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.893117   507 net.cpp:200] Created Layer mbox_priorbox (108)
I0512 11:23:21.893123   507 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0512 11:23:21.893131   507 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0512 11:23:21.893138   507 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0512 11:23:21.893146   507 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0512 11:23:21.893152   507 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0512 11:23:21.893168   507 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0512 11:23:21.893177   507 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0512 11:23:21.893200   507 net.cpp:260] Setting up mbox_priorbox
I0512 11:23:21.893208   507 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0512 11:23:21.893214   507 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0512 11:23:21.893219   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.893229   507 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0512 11:23:21.893235   507 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0512 11:23:21.893241   507 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0512 11:23:21.893267   507 net.cpp:260] Setting up mbox_conf_reshape
I0512 11:23:21.893275   507 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 10 17300 4 (692000)
I0512 11:23:21.893281   507 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0512 11:23:21.893286   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.893311   507 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0512 11:23:21.893318   507 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0512 11:23:21.893324   507 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0512 11:23:21.893385   507 net.cpp:260] Setting up mbox_conf_softmax
I0512 11:23:21.893390   507 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 10 17300 4 (692000)
I0512 11:23:21.893396   507 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0512 11:23:21.893402   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.893407   507 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0512 11:23:21.893414   507 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0512 11:23:21.893420   507 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0512 11:23:21.895578   507 net.cpp:260] Setting up mbox_conf_flatten
I0512 11:23:21.895591   507 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 10 69200 (692000)
I0512 11:23:21.895601   507 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0512 11:23:21.895606   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.895633   507 net.cpp:200] Created Layer detection_out (112)
I0512 11:23:21.895642   507 net.cpp:572] detection_out <- mbox_loc
I0512 11:23:21.895648   507 net.cpp:572] detection_out <- mbox_conf_flatten
I0512 11:23:21.895655   507 net.cpp:572] detection_out <- mbox_priorbox
I0512 11:23:21.895659   507 net.cpp:542] detection_out -> detection_out
I0512 11:23:21.896131   507 net.cpp:260] Setting up detection_out
I0512 11:23:21.896137   507 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0512 11:23:21.896144   507 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0512 11:23:21.896150   507 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0512 11:23:21.896163   507 net.cpp:200] Created Layer detection_eval (113)
I0512 11:23:21.896169   507 net.cpp:572] detection_eval <- detection_out
I0512 11:23:21.896175   507 net.cpp:572] detection_eval <- label
I0512 11:23:21.896183   507 net.cpp:542] detection_eval -> detection_eval
I0512 11:23:21.896466   507 net.cpp:260] Setting up detection_eval
I0512 11:23:21.896471   507 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0512 11:23:21.896476   507 net.cpp:338] detection_eval does not need backward computation.
I0512 11:23:21.896481   507 net.cpp:338] detection_out does not need backward computation.
I0512 11:23:21.896486   507 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0512 11:23:21.896493   507 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0512 11:23:21.896499   507 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0512 11:23:21.896517   507 net.cpp:338] mbox_priorbox does not need backward computation.
I0512 11:23:21.896524   507 net.cpp:338] mbox_conf does not need backward computation.
I0512 11:23:21.896533   507 net.cpp:338] mbox_loc does not need backward computation.
I0512 11:23:21.896541   507 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896548   507 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896554   507 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896561   507 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896570   507 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896575   507 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896581   507 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896589   507 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896592   507 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896597   507 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896605   507 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896611   507 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896618   507 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896625   507 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896631   507 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896637   507 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896644   507 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896651   507 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896656   507 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896661   507 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896665   507 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896670   507 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896678   507 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896685   507 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896693   507 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896699   507 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896703   507 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896709   507 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896713   507 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896718   507 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896723   507 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896725   507 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896730   507 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896734   507 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896739   507 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896752   507 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0512 11:23:21.896759   507 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0512 11:23:21.896765   507 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0512 11:23:21.896773   507 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0512 11:23:21.896780   507 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0512 11:23:21.896786   507 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0512 11:23:21.896792   507 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0512 11:23:21.896801   507 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0512 11:23:21.896807   507 net.cpp:338] ctx_output6/relu does not need backward computation.
I0512 11:23:21.896812   507 net.cpp:338] ctx_output6 does not need backward computation.
I0512 11:23:21.896818   507 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0512 11:23:21.896826   507 net.cpp:338] ctx_output5/relu does not need backward computation.
I0512 11:23:21.896834   507 net.cpp:338] ctx_output5 does not need backward computation.
I0512 11:23:21.896840   507 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0512 11:23:21.896844   507 net.cpp:338] ctx_output4/relu does not need backward computation.
I0512 11:23:21.896848   507 net.cpp:338] ctx_output4 does not need backward computation.
I0512 11:23:21.896853   507 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0512 11:23:21.896857   507 net.cpp:338] ctx_output3/relu does not need backward computation.
I0512 11:23:21.896860   507 net.cpp:338] ctx_output3 does not need backward computation.
I0512 11:23:21.896867   507 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0512 11:23:21.896870   507 net.cpp:338] ctx_output2/relu does not need backward computation.
I0512 11:23:21.896874   507 net.cpp:338] ctx_output2 does not need backward computation.
I0512 11:23:21.896878   507 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0512 11:23:21.896883   507 net.cpp:338] ctx_output1/relu does not need backward computation.
I0512 11:23:21.896889   507 net.cpp:338] ctx_output1 does not need backward computation.
I0512 11:23:21.896895   507 net.cpp:338] pool9 does not need backward computation.
I0512 11:23:21.896901   507 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0512 11:23:21.896908   507 net.cpp:338] pool8 does not need backward computation.
I0512 11:23:21.896914   507 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0512 11:23:21.896920   507 net.cpp:338] pool7 does not need backward computation.
I0512 11:23:21.896926   507 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0512 11:23:21.896934   507 net.cpp:338] pool6 does not need backward computation.
I0512 11:23:21.896939   507 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0512 11:23:21.896947   507 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0512 11:23:21.896955   507 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0512 11:23:21.896961   507 net.cpp:338] res5a_branch2b does not need backward computation.
I0512 11:23:21.896968   507 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0512 11:23:21.896975   507 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0512 11:23:21.896981   507 net.cpp:338] res5a_branch2a does not need backward computation.
I0512 11:23:21.896986   507 net.cpp:338] pool4 does not need backward computation.
I0512 11:23:21.896992   507 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0512 11:23:21.896999   507 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0512 11:23:21.897012   507 net.cpp:338] res4a_branch2b does not need backward computation.
I0512 11:23:21.897018   507 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0512 11:23:21.897025   507 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0512 11:23:21.897032   507 net.cpp:338] res4a_branch2a does not need backward computation.
I0512 11:23:21.897038   507 net.cpp:338] pool3 does not need backward computation.
I0512 11:23:21.897044   507 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0512 11:23:21.897049   507 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0512 11:23:21.897056   507 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0512 11:23:21.897061   507 net.cpp:338] res3a_branch2b does not need backward computation.
I0512 11:23:21.897068   507 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0512 11:23:21.897071   507 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0512 11:23:21.897075   507 net.cpp:338] res3a_branch2a does not need backward computation.
I0512 11:23:21.897079   507 net.cpp:338] pool2 does not need backward computation.
I0512 11:23:21.897086   507 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0512 11:23:21.897090   507 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0512 11:23:21.897094   507 net.cpp:338] res2a_branch2b does not need backward computation.
I0512 11:23:21.897099   507 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0512 11:23:21.897106   507 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0512 11:23:21.897111   507 net.cpp:338] res2a_branch2a does not need backward computation.
I0512 11:23:21.897116   507 net.cpp:338] pool1 does not need backward computation.
I0512 11:23:21.897122   507 net.cpp:338] conv1b/relu does not need backward computation.
I0512 11:23:21.897128   507 net.cpp:338] conv1b/bn does not need backward computation.
I0512 11:23:21.897135   507 net.cpp:338] conv1b does not need backward computation.
I0512 11:23:21.897141   507 net.cpp:338] conv1a/relu does not need backward computation.
I0512 11:23:21.897150   507 net.cpp:338] conv1a/bn does not need backward computation.
I0512 11:23:21.897156   507 net.cpp:338] conv1a does not need backward computation.
I0512 11:23:21.897161   507 net.cpp:338] data/bias does not need backward computation.
I0512 11:23:21.897167   507 net.cpp:338] data_data_0_split does not need backward computation.
I0512 11:23:21.897173   507 net.cpp:338] data does not need backward computation.
I0512 11:23:21.897178   507 net.cpp:380] This network produces output detection_eval
I0512 11:23:21.897272   507 net.cpp:403] Top memory (TEST) required for data: 1515720496 diff: 1515720496
I0512 11:23:21.897277   507 net.cpp:406] Bottom memory (TEST) required for data: 1515720416 diff: 1515720416
I0512 11:23:21.897281   507 net.cpp:409] Shared (in-place) memory (TEST) by data: 652144640 diff: 652144640
I0512 11:23:21.897285   507 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0512 11:23:21.897294   507 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0512 11:23:21.897298   507 net.cpp:421] Network initialization done.
F0512 11:23:21.897552   507 io.cpp:55] Check failed: fd != -1 (-1 vs. -1) File not found: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/test/EYES_ssdJacintoNetV2_iter_20000.caffemodel
*** Check failure stack trace: ***
    @     0x7f57824514dd  google::LogMessage::Fail()
    @     0x7f5782459071  google::LogMessage::SendToLog()
    @     0x7f5782450ecd  google::LogMessage::Flush()
    @     0x7f578245276a  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f57802b1302  caffe::ReadProtoFromBinaryFile()
    @     0x7f57802dbc16  caffe::ReadNetParamsFromBinaryFileOrDie()
    @     0x7f5780212aea  caffe::Net::CopyTrainedLayersFromBinaryProto()
    @     0x7f5780212bad  caffe::Net::CopyTrainedLayersFrom()
    @     0x557983c9327f  test_detection()
    @     0x557983c8e6f9  main
    @     0x7f577e229b97  __libc_start_main
    @     0x557983c8f5da  _start
    @              (nil)  (unknown)
