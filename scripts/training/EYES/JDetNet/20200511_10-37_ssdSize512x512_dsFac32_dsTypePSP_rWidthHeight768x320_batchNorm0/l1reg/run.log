I0511 17:17:42.483932   333 caffe.cpp:902] This is NVCaffe 0.17.0 started at Mon May 11 17:17:42 2020
I0511 17:17:42.736526   333 caffe.cpp:904] CuDNN version: 7605
I0511 17:17:42.736531   333 caffe.cpp:905] CuBLAS version: 10202
I0511 17:17:42.736534   333 caffe.cpp:906] CUDA version: 10020
I0511 17:17:42.736536   333 caffe.cpp:907] CUDA driver version: 10020
I0511 17:17:42.736539   333 caffe.cpp:908] Arguments: 
[0]: /workspace/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/solver.prototxt
[3]: --weights=training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
[4]: --gpu
[5]: 0
I0511 17:17:42.757985   333 gpu_memory.cpp:105] GPUMemory::Manager initialized
I0511 17:17:42.758013   333 gpu_memory.cpp:107] Total memory: 16900227072, Free: 12089163776, dev_info[0]: total=16900227072 free=12089163776
I0511 17:17:42.758211   333 caffe.cpp:226] Using GPUs 0
I0511 17:17:42.758337   333 caffe.cpp:230] GPU 0: Quadro RTX 5000
I0511 17:17:42.758400   333 solver.cpp:41] Solver data type: FLOAT
I0511 17:17:42.766357   333 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/train.prototxt"
test_net: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/test.prototxt"
test_iter: 107
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 10000
lr_policy: "poly"
gamma: 0.1
power: 4
momentum: 0.9
weight_decay: 1e-05
snapshot: 1000
snapshot_prefix: "training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
average_loss: 10
stepvalue: 30000
stepvalue: 45000
stepvalue: 300000
iter_size: 2
type: "Adam"
eval_type: "detection"
ap_version: "11point"
show_per_class_result: true
I0511 17:17:42.766574   333 solver.cpp:76] Creating training net from train_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/train.prototxt
I0511 17:17:42.768115   333 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
      interp_mode: AREA
      interp_mode: NEAREST
      interp_mode: CUBIC
      interp_mode: LANCZOS4
    }
    emit_constraint {
      emit_type: CENTER
    }
    crop_h: 320
    crop_w: 768
    distort_param {
      brightness_prob: 0.5
      brightness_delta: 32
      contrast_prob: 0.5
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.5
      hue_delta: 18
      saturation_prob: 0.5
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    expand_param {
      prob: 0.5
      max_expand_ratio: 4
    }
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/EYES_trainval_lmdb"
    batch_size: 16
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
      max_sample: 1
      max_trials: 1
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.1
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.3
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.5
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.7
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        min_jaccard_overlap: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    batch_sampler {
      sampler {
        min_scale: 0.3
        max_scale: 1
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2
      }
      sample_constraint {
        max_jaccard_overlap: 1
      }
      max_sample: 1
      max_trials: 50
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_loss"
  type: "MultiBoxLoss"
  bottom: "mbox_loc"
  bottom: "mbox_conf"
  bottom: "mbox_priorbox"
  bottom: "label"
  top: "mbox_loss"
  include {
    phase: TRAIN
  }
  propagate_down: true
  propagate_down: true
  propagate_down: false
  propagate_down: false
  loss_param {
    normalization: VALID
  }
  multibox_loss_param {
    loc_loss_type: SMOOTH_L1
    conf_loss_type: SOFTMAX
    loc_weight: 1
    num_classes: 4
    share_location: true
    match_type: PER_PREDICTION
    overlap_threshold: 0.5
    use_prior_for_matching: true
    background_label_id: 0
    use_difficult_gt: false
    neg_pos_ratio: 3
    neg_overlap: 0.5
    code_type: CENTER_SIZE
    ignore_cross_boundary_bbox: false
    mining_type: MAX_NEGATIVE
    ignore_difficult_gt: false
  }
}
I0511 17:17:42.769078   333 net.cpp:110] Using FLOAT as default forward math type
I0511 17:17:42.769096   333 net.cpp:116] Using FLOAT as default backward math type
I0511 17:17:42.769106   333 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 17:17:42.769114   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.769229   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:42.769637   338 blocking_queue.cpp:40] Data layer prefetch queue empty
I0511 17:17:42.769660   333 net.cpp:200] Created Layer data (0)
I0511 17:17:42.769673   333 net.cpp:542] data -> data
I0511 17:17:42.769706   333 net.cpp:542] data -> label
I0511 17:17:42.769731   333 data_reader.cpp:58] Data Reader threads: 4, out queues: 16, depth: 16
I0511 17:17:42.769804   333 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 17:17:42.770242   339 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.770406   340 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.770601   341 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.772722   333 annotated_data_layer.cpp:105] output data size: 16,3,320,768
I0511 17:17:42.773005   333 annotated_data_layer.cpp:150] [0] Output data size: 16, 3, 320, 768
I0511 17:17:42.773077   333 internal_thread.cpp:19] Starting 4 internal thread(s) on device 0
I0511 17:17:42.773510   343 data_layer.cpp:105] [0] Parser threads: 4
I0511 17:17:42.773526   343 data_layer.cpp:107] [0] Transformer threads: 4
I0511 17:17:42.773751   342 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/EYES_trainval_lmdb
I0511 17:17:42.774191   333 net.cpp:260] Setting up data
I0511 17:17:42.774716   333 net.cpp:267] TRAIN Top shape for layer 0 'data' 16 3 320 768 (11796480)
I0511 17:17:42.774771   333 net.cpp:267] TRAIN Top shape for layer 0 'data' 1 1 4 8 (32)
I0511 17:17:42.774788   333 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 17:17:42.774813   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.774847   333 net.cpp:200] Created Layer data_data_0_split (1)
I0511 17:17:42.774859   333 net.cpp:572] data_data_0_split <- data
I0511 17:17:42.774879   333 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 17:17:42.774889   333 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 17:17:42.774895   333 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 17:17:42.774907   333 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 17:17:42.774911   333 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 17:17:42.774916   333 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 17:17:42.774920   333 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 17:17:42.775038   333 net.cpp:260] Setting up data_data_0_split
I0511 17:17:42.775045   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775054   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775066   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775074   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775085   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775091   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775097   333 net.cpp:267] TRAIN Top shape for layer 1 'data_data_0_split' 16 3 320 768 (11796480)
I0511 17:17:42.775127   333 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 17:17:42.775148   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.775172   333 net.cpp:200] Created Layer data/bias (2)
I0511 17:17:42.775185   333 net.cpp:572] data/bias <- data_data_0_split_0
I0511 17:17:42.775193   333 net.cpp:542] data/bias -> data/bias
I0511 17:17:42.775354   333 net.cpp:260] Setting up data/bias
I0511 17:17:42.775384   333 net.cpp:267] TRAIN Top shape for layer 2 'data/bias' 16 3 320 768 (11796480)
I0511 17:17:42.775413   333 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 17:17:42.775437   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:42.775507   333 net.cpp:200] Created Layer conv1a (3)
I0511 17:17:42.775518   333 net.cpp:572] conv1a <- data/bias
I0511 17:17:42.775527   333 net.cpp:542] conv1a -> conv1a
I0511 17:17:46.122023   333 net.cpp:260] Setting up conv1a
I0511 17:17:46.122062   333 net.cpp:267] TRAIN Top shape for layer 3 'conv1a' 16 32 160 384 (31457280)
I0511 17:17:46.122100   333 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 17:17:46.122148   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122175   333 net.cpp:200] Created Layer conv1a/bn (4)
I0511 17:17:46.122185   333 net.cpp:572] conv1a/bn <- conv1a
I0511 17:17:46.122195   333 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 17:17:46.122743   333 net.cpp:260] Setting up conv1a/bn
I0511 17:17:46.122750   333 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/bn' 16 32 160 384 (31457280)
I0511 17:17:46.122772   333 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 17:17:46.122779   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122790   333 net.cpp:200] Created Layer conv1a/relu (5)
I0511 17:17:46.122797   333 net.cpp:572] conv1a/relu <- conv1a
I0511 17:17:46.122803   333 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 17:17:46.122834   333 net.cpp:260] Setting up conv1a/relu
I0511 17:17:46.122840   333 net.cpp:267] TRAIN Top shape for layer 5 'conv1a/relu' 16 32 160 384 (31457280)
I0511 17:17:46.122849   333 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 17:17:46.122856   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.122875   333 net.cpp:200] Created Layer conv1b (6)
I0511 17:17:46.122880   333 net.cpp:572] conv1b <- conv1a
I0511 17:17:46.122887   333 net.cpp:542] conv1b -> conv1b
I0511 17:17:46.123528   333 net.cpp:260] Setting up conv1b
I0511 17:17:46.123538   333 net.cpp:267] TRAIN Top shape for layer 6 'conv1b' 16 32 160 384 (31457280)
I0511 17:17:46.123555   333 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 17:17:46.123562   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.123574   333 net.cpp:200] Created Layer conv1b/bn (7)
I0511 17:17:46.123580   333 net.cpp:572] conv1b/bn <- conv1b
I0511 17:17:46.123587   333 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 17:17:46.124027   333 net.cpp:260] Setting up conv1b/bn
I0511 17:17:46.124032   333 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/bn' 16 32 160 384 (31457280)
I0511 17:17:46.124048   333 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 17:17:46.124055   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124064   333 net.cpp:200] Created Layer conv1b/relu (8)
I0511 17:17:46.124070   333 net.cpp:572] conv1b/relu <- conv1b
I0511 17:17:46.124078   333 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 17:17:46.124085   333 net.cpp:260] Setting up conv1b/relu
I0511 17:17:46.124090   333 net.cpp:267] TRAIN Top shape for layer 8 'conv1b/relu' 16 32 160 384 (31457280)
I0511 17:17:46.124100   333 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 17:17:46.124106   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124122   333 net.cpp:200] Created Layer pool1 (9)
I0511 17:17:46.124127   333 net.cpp:572] pool1 <- conv1b
I0511 17:17:46.124135   333 net.cpp:542] pool1 -> pool1
I0511 17:17:46.124222   333 net.cpp:260] Setting up pool1
I0511 17:17:46.124228   333 net.cpp:267] TRAIN Top shape for layer 9 'pool1' 16 32 80 192 (7864320)
I0511 17:17:46.124238   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 17:17:46.124244   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.124260   333 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 17:17:46.124266   333 net.cpp:572] res2a_branch2a <- pool1
I0511 17:17:46.124274   333 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 17:17:46.125957   333 net.cpp:260] Setting up res2a_branch2a
I0511 17:17:46.125972   333 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a' 16 64 80 192 (15728640)
I0511 17:17:46.125991   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.126017   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126030   333 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 17:17:46.126037   333 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 17:17:46.126044   333 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 17:17:46.126466   333 net.cpp:260] Setting up res2a_branch2a/bn
I0511 17:17:46.126472   333 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/bn' 16 64 80 192 (15728640)
I0511 17:17:46.126488   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.126497   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126505   333 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 17:17:46.126511   333 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 17:17:46.126518   333 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 17:17:46.126526   333 net.cpp:260] Setting up res2a_branch2a/relu
I0511 17:17:46.126531   333 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2a/relu' 16 64 80 192 (15728640)
I0511 17:17:46.126541   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 17:17:46.126547   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.126562   333 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 17:17:46.126569   333 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 17:17:46.126574   333 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 17:17:46.127110   333 net.cpp:260] Setting up res2a_branch2b
I0511 17:17:46.127117   333 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b' 16 64 80 192 (15728640)
I0511 17:17:46.127130   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.127136   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127146   333 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 17:17:46.127152   333 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 17:17:46.127159   333 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 17:17:46.127585   333 net.cpp:260] Setting up res2a_branch2b/bn
I0511 17:17:46.127591   333 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/bn' 16 64 80 192 (15728640)
I0511 17:17:46.127606   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.127612   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127620   333 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 17:17:46.127627   333 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 17:17:46.127633   333 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 17:17:46.127641   333 net.cpp:260] Setting up res2a_branch2b/relu
I0511 17:17:46.127647   333 net.cpp:267] TRAIN Top shape for layer 15 'res2a_branch2b/relu' 16 64 80 192 (15728640)
I0511 17:17:46.127656   333 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 17:17:46.127662   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127673   333 net.cpp:200] Created Layer pool2 (16)
I0511 17:17:46.127679   333 net.cpp:572] pool2 <- res2a_branch2b
I0511 17:17:46.127686   333 net.cpp:542] pool2 -> pool2
I0511 17:17:46.127739   333 net.cpp:260] Setting up pool2
I0511 17:17:46.127745   333 net.cpp:267] TRAIN Top shape for layer 16 'pool2' 16 64 40 96 (3932160)
I0511 17:17:46.127755   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 17:17:46.127761   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.127777   333 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 17:17:46.127784   333 net.cpp:572] res3a_branch2a <- pool2
I0511 17:17:46.127802   333 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 17:17:46.129876   333 net.cpp:260] Setting up res3a_branch2a
I0511 17:17:46.129926   333 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a' 16 128 40 96 (7864320)
I0511 17:17:46.129973   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.130007   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.130048   333 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 17:17:46.130082   333 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 17:17:46.130118   333 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 17:17:46.130630   333 net.cpp:260] Setting up res3a_branch2a/bn
I0511 17:17:46.130673   333 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/bn' 16 128 40 96 (7864320)
I0511 17:17:46.130725   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.130760   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.130800   333 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 17:17:46.130837   333 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 17:17:46.130874   333 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 17:17:46.130911   333 net.cpp:260] Setting up res3a_branch2a/relu
I0511 17:17:46.130947   333 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2a/relu' 16 128 40 96 (7864320)
I0511 17:17:46.130987   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 17:17:46.131027   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.131072   333 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 17:17:46.131108   333 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 17:17:46.131144   333 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 17:17:46.132377   333 net.cpp:260] Setting up res3a_branch2b
I0511 17:17:46.132427   333 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b' 16 128 40 96 (7864320)
I0511 17:17:46.132472   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.132508   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.132548   333 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 17:17:46.132583   333 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 17:17:46.132620   333 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 17:17:46.133131   333 net.cpp:260] Setting up res3a_branch2b/bn
I0511 17:17:46.133175   333 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/bn' 16 128 40 96 (7864320)
I0511 17:17:46.133224   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.133258   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133301   333 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 17:17:46.133338   333 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 17:17:46.133375   333 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 17:17:46.133415   333 net.cpp:260] Setting up res3a_branch2b/relu
I0511 17:17:46.133450   333 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b/relu' 16 128 40 96 (7864320)
I0511 17:17:46.133488   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.133524   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133563   333 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 17:17:46.133597   333 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 17:17:46.133633   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.133675   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.133765   333 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 17:17:46.133800   333 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 17:17:46.133842   333 net.cpp:267] TRAIN Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 16 128 40 96 (7864320)
I0511 17:17:46.133882   333 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 17:17:46.133918   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.133956   333 net.cpp:200] Created Layer pool3 (24)
I0511 17:17:46.133992   333 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.134028   333 net.cpp:542] pool3 -> pool3
I0511 17:17:46.134122   333 net.cpp:260] Setting up pool3
I0511 17:17:46.134166   333 net.cpp:267] TRAIN Top shape for layer 24 'pool3' 16 128 20 48 (1966080)
I0511 17:17:46.134207   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 17:17:46.134240   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.134284   333 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 17:17:46.134320   333 net.cpp:572] res4a_branch2a <- pool3
I0511 17:17:46.134356   333 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 17:17:46.175707   333 net.cpp:260] Setting up res4a_branch2a
I0511 17:17:46.175745   333 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a' 16 256 20 48 (3932160)
I0511 17:17:46.175776   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.175786   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.175804   333 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 17:17:46.175814   333 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 17:17:46.175825   333 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 17:17:46.176281   333 net.cpp:260] Setting up res4a_branch2a/bn
I0511 17:17:46.176290   333 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/bn' 16 256 20 48 (3932160)
I0511 17:17:46.176306   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.176314   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.176324   333 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 17:17:46.176331   333 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 17:17:46.176337   333 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 17:17:46.176347   333 net.cpp:260] Setting up res4a_branch2a/relu
I0511 17:17:46.176353   333 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2a/relu' 16 256 20 48 (3932160)
I0511 17:17:46.176362   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 17:17:46.176368   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.176388   333 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 17:17:46.176394   333 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 17:17:46.176401   333 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 17:17:46.180006   333 net.cpp:260] Setting up res4a_branch2b
I0511 17:17:46.180019   333 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b' 16 256 20 48 (3932160)
I0511 17:17:46.180035   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.180042   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180054   333 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 17:17:46.180061   333 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 17:17:46.180068   333 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 17:17:46.180480   333 net.cpp:260] Setting up res4a_branch2b/bn
I0511 17:17:46.180521   333 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/bn' 16 256 20 48 (3932160)
I0511 17:17:46.180537   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.180544   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180553   333 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 17:17:46.180559   333 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 17:17:46.180565   333 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 17:17:46.180574   333 net.cpp:260] Setting up res4a_branch2b/relu
I0511 17:17:46.180580   333 net.cpp:267] TRAIN Top shape for layer 30 'res4a_branch2b/relu' 16 256 20 48 (3932160)
I0511 17:17:46.180589   333 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 17:17:46.180596   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180608   333 net.cpp:200] Created Layer pool4 (31)
I0511 17:17:46.180613   333 net.cpp:572] pool4 <- res4a_branch2b
I0511 17:17:46.180619   333 net.cpp:542] pool4 -> pool4
I0511 17:17:46.180680   333 net.cpp:260] Setting up pool4
I0511 17:17:46.180685   333 net.cpp:267] TRAIN Top shape for layer 31 'pool4' 16 256 10 24 (983040)
I0511 17:17:46.180696   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 17:17:46.180701   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.180721   333 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 17:17:46.180727   333 net.cpp:572] res5a_branch2a <- pool4
I0511 17:17:46.180733   333 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 17:17:46.256343   333 net.cpp:260] Setting up res5a_branch2a
I0511 17:17:46.256378   333 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a' 16 512 10 24 (1966080)
I0511 17:17:46.256412   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.256420   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.256439   333 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 17:17:46.256449   333 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 17:17:46.256461   333 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 17:17:46.256944   333 net.cpp:260] Setting up res5a_branch2a/bn
I0511 17:17:46.256951   333 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/bn' 16 512 10 24 (1966080)
I0511 17:17:46.256968   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.256976   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.256986   333 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 17:17:46.256992   333 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 17:17:46.256999   333 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 17:17:46.257009   333 net.cpp:260] Setting up res5a_branch2a/relu
I0511 17:17:46.257015   333 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2a/relu' 16 512 10 24 (1966080)
I0511 17:17:46.257025   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 17:17:46.257030   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.257050   333 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 17:17:46.257056   333 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 17:17:46.257063   333 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 17:17:46.273281   333 net.cpp:260] Setting up res5a_branch2b
I0511 17:17:46.273499   333 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b' 16 512 10 24 (1966080)
I0511 17:17:46.273685   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.273844   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.274015   333 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 17:17:46.274168   333 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 17:17:46.274322   333 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 17:17:46.274919   333 net.cpp:260] Setting up res5a_branch2b/bn
I0511 17:17:46.275095   333 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/bn' 16 512 10 24 (1966080)
I0511 17:17:46.275266   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.275418   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.275573   333 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 17:17:46.275727   333 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 17:17:46.275877   333 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 17:17:46.276032   333 net.cpp:260] Setting up res5a_branch2b/relu
I0511 17:17:46.276187   333 net.cpp:267] TRAIN Top shape for layer 37 'res5a_branch2b/relu' 16 512 10 24 (1966080)
I0511 17:17:46.276351   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.276502   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.276659   333 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 17:17:46.276810   333 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 17:17:46.276963   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.277122   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.277326   333 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 17:17:46.277483   333 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 17:17:46.277642   333 net.cpp:267] TRAIN Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 16 512 10 24 (1966080)
I0511 17:17:46.277799   333 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 17:17:46.277951   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.278107   333 net.cpp:200] Created Layer pool6 (39)
I0511 17:17:46.278259   333 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.278410   333 net.cpp:542] pool6 -> pool6
I0511 17:17:46.278626   333 net.cpp:260] Setting up pool6
I0511 17:17:46.278777   333 net.cpp:267] TRAIN Top shape for layer 39 'pool6' 16 512 5 12 (491520)
I0511 17:17:46.278935   333 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 17:17:46.279088   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.279242   333 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 17:17:46.279392   333 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 17:17:46.279544   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 17:17:46.279698   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 17:17:46.279898   333 net.cpp:260] Setting up pool6_pool6_0_split
I0511 17:17:46.280051   333 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 17:17:46.280210   333 net.cpp:267] TRAIN Top shape for layer 40 'pool6_pool6_0_split' 16 512 5 12 (491520)
I0511 17:17:46.280365   333 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 17:17:46.280516   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.280671   333 net.cpp:200] Created Layer pool7 (41)
I0511 17:17:46.280822   333 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 17:17:46.280973   333 net.cpp:542] pool7 -> pool7
I0511 17:17:46.281193   333 net.cpp:260] Setting up pool7
I0511 17:17:46.281378   333 net.cpp:267] TRAIN Top shape for layer 41 'pool7' 16 512 3 6 (147456)
I0511 17:17:46.281563   333 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 17:17:46.281735   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.281909   333 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 17:17:46.282081   333 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 17:17:46.282254   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 17:17:46.282430   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 17:17:46.282661   333 net.cpp:260] Setting up pool7_pool7_0_split
I0511 17:17:46.282829   333 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 17:17:46.283004   333 net.cpp:267] TRAIN Top shape for layer 42 'pool7_pool7_0_split' 16 512 3 6 (147456)
I0511 17:17:46.283179   333 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 17:17:46.283347   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.283520   333 net.cpp:200] Created Layer pool8 (43)
I0511 17:17:46.283691   333 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 17:17:46.283859   333 net.cpp:542] pool8 -> pool8
I0511 17:17:46.284116   333 net.cpp:260] Setting up pool8
I0511 17:17:46.284284   333 net.cpp:267] TRAIN Top shape for layer 43 'pool8' 16 512 2 3 (49152)
I0511 17:17:46.284466   333 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 17:17:46.284636   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.284812   333 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 17:17:46.284979   333 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 17:17:46.285153   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 17:17:46.285328   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 17:17:46.285548   333 net.cpp:260] Setting up pool8_pool8_0_split
I0511 17:17:46.285725   333 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 17:17:46.285898   333 net.cpp:267] TRAIN Top shape for layer 44 'pool8_pool8_0_split' 16 512 2 3 (49152)
I0511 17:17:46.286070   333 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 17:17:46.286240   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.286414   333 net.cpp:200] Created Layer pool9 (45)
I0511 17:17:46.286584   333 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 17:17:46.286756   333 net.cpp:542] pool9 -> pool9
I0511 17:17:46.287001   333 net.cpp:260] Setting up pool9
I0511 17:17:46.287168   333 net.cpp:267] TRAIN Top shape for layer 45 'pool9' 16 512 1 2 (16384)
I0511 17:17:46.287345   333 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 17:17:46.287513   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.287696   333 net.cpp:200] Created Layer ctx_output1 (46)
I0511 17:17:46.287868   333 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.288038   333 net.cpp:542] ctx_output1 -> ctx_output1
I0511 17:17:46.289332   333 net.cpp:260] Setting up ctx_output1
I0511 17:17:46.289530   333 net.cpp:267] TRAIN Top shape for layer 46 'ctx_output1' 16 256 40 96 (15728640)
I0511 17:17:46.289716   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 17:17:46.289887   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.290069   333 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 17:17:46.290246   333 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 17:17:46.290421   333 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 17:17:46.290597   333 net.cpp:260] Setting up ctx_output1/relu
I0511 17:17:46.290773   333 net.cpp:267] TRAIN Top shape for layer 47 'ctx_output1/relu' 16 256 40 96 (15728640)
I0511 17:17:46.290959   333 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 17:17:46.291131   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.291304   333 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 17:17:46.291476   333 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 17:17:46.291646   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.291819   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.291990   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.292234   333 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 17:17:46.292402   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292578   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292752   333 net.cpp:267] TRAIN Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 16 256 40 96 (15728640)
I0511 17:17:46.292927   333 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 17:17:46.293088   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.293251   333 net.cpp:200] Created Layer ctx_output2 (49)
I0511 17:17:46.293371   333 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.293419   333 net.cpp:542] ctx_output2 -> ctx_output2
I0511 17:17:46.296434   333 net.cpp:260] Setting up ctx_output2
I0511 17:17:46.296499   333 net.cpp:267] TRAIN Top shape for layer 49 'ctx_output2' 16 256 10 24 (983040)
I0511 17:17:46.296617   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 17:17:46.296658   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.296733   333 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 17:17:46.296802   333 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 17:17:46.296875   333 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 17:17:46.296945   333 net.cpp:260] Setting up ctx_output2/relu
I0511 17:17:46.297011   333 net.cpp:267] TRAIN Top shape for layer 50 'ctx_output2/relu' 16 256 10 24 (983040)
I0511 17:17:46.297088   333 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 17:17:46.297153   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.297245   333 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 17:17:46.297387   333 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 17:17:46.297453   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.297593   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.297670   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.297823   333 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 17:17:46.297894   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.297979   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.298053   333 net.cpp:267] TRAIN Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 16 256 10 24 (983040)
I0511 17:17:46.298120   333 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 17:17:46.298198   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.298290   333 net.cpp:200] Created Layer ctx_output3 (52)
I0511 17:17:46.298365   333 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 17:17:46.298434   333 net.cpp:542] ctx_output3 -> ctx_output3
I0511 17:17:46.302407   333 net.cpp:260] Setting up ctx_output3
I0511 17:17:46.302513   333 net.cpp:267] TRAIN Top shape for layer 52 'ctx_output3' 16 256 5 12 (245760)
I0511 17:17:46.302609   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 17:17:46.302685   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.302762   333 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 17:17:46.302839   333 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 17:17:46.302919   333 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 17:17:46.302999   333 net.cpp:260] Setting up ctx_output3/relu
I0511 17:17:46.303073   333 net.cpp:267] TRAIN Top shape for layer 53 'ctx_output3/relu' 16 256 5 12 (245760)
I0511 17:17:46.303153   333 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 17:17:46.303232   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.303316   333 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 17:17:46.303390   333 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 17:17:46.303472   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.303550   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.303634   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.303786   333 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 17:17:46.303864   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.303943   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.304023   333 net.cpp:267] TRAIN Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 16 256 5 12 (245760)
I0511 17:17:46.304101   333 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 17:17:46.304180   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.304265   333 net.cpp:200] Created Layer ctx_output4 (55)
I0511 17:17:46.304340   333 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 17:17:46.304420   333 net.cpp:542] ctx_output4 -> ctx_output4
I0511 17:17:46.307709   333 net.cpp:260] Setting up ctx_output4
I0511 17:17:46.307812   333 net.cpp:267] TRAIN Top shape for layer 55 'ctx_output4' 16 256 3 6 (73728)
I0511 17:17:46.307904   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 17:17:46.307978   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.308061   333 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 17:17:46.308137   333 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 17:17:46.308216   333 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 17:17:46.308295   333 net.cpp:260] Setting up ctx_output4/relu
I0511 17:17:46.308369   333 net.cpp:267] TRAIN Top shape for layer 56 'ctx_output4/relu' 16 256 3 6 (73728)
I0511 17:17:46.308449   333 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 17:17:46.308526   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.308609   333 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 17:17:46.308683   333 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 17:17:46.308764   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.308847   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.308948   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.309115   333 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 17:17:46.309195   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309274   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309371   333 net.cpp:267] TRAIN Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 16 256 3 6 (73728)
I0511 17:17:46.309451   333 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 17:17:46.309526   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.309612   333 net.cpp:200] Created Layer ctx_output5 (58)
I0511 17:17:46.309682   333 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 17:17:46.309767   333 net.cpp:542] ctx_output5 -> ctx_output5
I0511 17:17:46.312775   333 net.cpp:260] Setting up ctx_output5
I0511 17:17:46.312862   333 net.cpp:267] TRAIN Top shape for layer 58 'ctx_output5' 16 256 2 3 (24576)
I0511 17:17:46.312953   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 17:17:46.313024   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.313097   333 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 17:17:46.313169   333 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 17:17:46.313246   333 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 17:17:46.313331   333 net.cpp:260] Setting up ctx_output5/relu
I0511 17:17:46.313400   333 net.cpp:267] TRAIN Top shape for layer 59 'ctx_output5/relu' 16 256 2 3 (24576)
I0511 17:17:46.313479   333 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 17:17:46.313540   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.313621   333 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 17:17:46.313694   333 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 17:17:46.313762   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.313841   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.313927   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.314071   333 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 17:17:46.314150   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314225   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314298   333 net.cpp:267] TRAIN Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 16 256 2 3 (24576)
I0511 17:17:46.314370   333 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 17:17:46.314440   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.314528   333 net.cpp:200] Created Layer ctx_output6 (61)
I0511 17:17:46.314599   333 net.cpp:572] ctx_output6 <- pool9
I0511 17:17:46.314674   333 net.cpp:542] ctx_output6 -> ctx_output6
I0511 17:17:46.317685   333 net.cpp:260] Setting up ctx_output6
I0511 17:17:46.317775   333 net.cpp:267] TRAIN Top shape for layer 61 'ctx_output6' 16 256 1 2 (8192)
I0511 17:17:46.317857   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 17:17:46.317929   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.318014   333 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 17:17:46.318091   333 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 17:17:46.318174   333 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 17:17:46.318248   333 net.cpp:260] Setting up ctx_output6/relu
I0511 17:17:46.318317   333 net.cpp:267] TRAIN Top shape for layer 62 'ctx_output6/relu' 16 256 1 2 (8192)
I0511 17:17:46.318403   333 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 17:17:46.318475   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.318548   333 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 17:17:46.318629   333 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 17:17:46.318722   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.318806   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.318883   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.319042   333 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 17:17:46.319120   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319200   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319278   333 net.cpp:267] TRAIN Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 16 256 1 2 (8192)
I0511 17:17:46.319357   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.319449   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.319545   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 17:17:46.319627   333 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.319703   333 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 17:17:46.320255   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 17:17:46.320339   333 net.cpp:267] TRAIN Top shape for layer 64 'ctx_output1/relu_mbox_loc' 16 16 40 96 (983040)
I0511 17:17:46.320430   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.320505   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.320593   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 17:17:46.320681   333 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 17:17:46.320758   333 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.320979   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.321058   333 net.cpp:267] TRAIN Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 16 40 96 16 (983040)
I0511 17:17:46.321138   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.321214   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.321302   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 17:17:46.321380   333 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.321457   333 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.325834   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.325943   333 net.cpp:267] TRAIN Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 16 61440 (983040)
I0511 17:17:46.326046   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.326138   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.326246   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 17:17:46.326328   333 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.326429   333 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 17:17:46.326999   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 17:17:46.327086   333 net.cpp:267] TRAIN Top shape for layer 67 'ctx_output1/relu_mbox_conf' 16 16 40 96 (983040)
I0511 17:17:46.327173   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.327250   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.327335   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 17:17:46.327410   333 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 17:17:46.327492   333 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.335727   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.335990   333 net.cpp:267] TRAIN Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 16 40 96 16 (983040)
I0511 17:17:46.336088   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.336182   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.336268   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 17:17:46.336354   333 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.336447   333 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.343909   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.344871   333 net.cpp:267] TRAIN Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 16 61440 (983040)
I0511 17:17:46.344986   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.345072   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.345161   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 17:17:46.345257   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.345360   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 17:17:46.345443   333 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 17:17:46.345633   333 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 17:17:46.345713   333 net.cpp:267] TRAIN Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 17:17:46.345801   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.345878   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.345978   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 17:17:46.346076   333 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.346156   333 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 17:17:46.346805   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 17:17:46.347357   333 net.cpp:267] TRAIN Top shape for layer 71 'ctx_output2/relu_mbox_loc' 16 24 10 24 (92160)
I0511 17:17:46.347455   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.347548   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.347630   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 17:17:46.347713   333 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 17:17:46.347791   333 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348054   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348222   333 net.cpp:267] TRAIN Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 16 10 24 24 (92160)
I0511 17:17:46.348330   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.348410   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.348500   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 17:17:46.348587   333 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.348672   333 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.350100   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.350575   333 net.cpp:267] TRAIN Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 16 5760 (92160)
I0511 17:17:46.350688   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.350772   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.350877   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 17:17:46.350975   333 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.351064   333 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 17:17:46.351673   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 17:17:46.352223   333 net.cpp:267] TRAIN Top shape for layer 74 'ctx_output2/relu_mbox_conf' 16 24 10 24 (92160)
I0511 17:17:46.352342   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.352463   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.352581   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 17:17:46.352697   333 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 17:17:46.352807   333 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353041   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353266   333 net.cpp:267] TRAIN Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 16 10 24 24 (92160)
I0511 17:17:46.353411   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.353531   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.353646   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 17:17:46.353762   333 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.353873   333 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.354074   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.354274   333 net.cpp:267] TRAIN Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 16 5760 (92160)
I0511 17:17:46.354390   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.354501   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.354614   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 17:17:46.354727   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.354837   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 17:17:46.354949   333 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 17:17:46.355087   333 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 17:17:46.355221   333 net.cpp:267] TRAIN Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 17:17:46.355337   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.355453   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.355588   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 17:17:46.355705   333 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.355818   333 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 17:17:46.356400   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 17:17:46.356990   333 net.cpp:267] TRAIN Top shape for layer 78 'ctx_output3/relu_mbox_loc' 16 24 5 12 (23040)
I0511 17:17:46.357127   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.357242   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.357360   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 17:17:46.357477   333 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 17:17:46.357589   333 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.357828   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.358063   333 net.cpp:267] TRAIN Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 16 5 12 24 (23040)
I0511 17:17:46.358180   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.358289   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.358400   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 17:17:46.358510   333 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.358626   333 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.358808   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.358988   333 net.cpp:267] TRAIN Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 16 1440 (23040)
I0511 17:17:46.359100   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.359208   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.359326   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 17:17:46.359443   333 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.359552   333 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 17:17:46.360110   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 17:17:46.360672   333 net.cpp:267] TRAIN Top shape for layer 81 'ctx_output3/relu_mbox_conf' 16 24 5 12 (23040)
I0511 17:17:46.360793   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.360899   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.361006   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 17:17:46.361110   333 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 17:17:46.361210   333 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.361441   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.361666   333 net.cpp:267] TRAIN Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 16 5 12 24 (23040)
I0511 17:17:46.361775   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.361877   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.361981   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 17:17:46.362084   333 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.362185   333 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.362362   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.362540   333 net.cpp:267] TRAIN Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 16 1440 (23040)
I0511 17:17:46.362663   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.362769   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.362874   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 17:17:46.362978   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.363080   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 17:17:46.363184   333 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 17:17:46.363307   333 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 17:17:46.363427   333 net.cpp:267] TRAIN Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 17:17:46.363533   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.363633   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.363744   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 17:17:46.363853   333 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.363955   333 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 17:17:46.364495   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 17:17:46.365056   333 net.cpp:267] TRAIN Top shape for layer 85 'ctx_output4/relu_mbox_loc' 16 24 3 6 (6912)
I0511 17:17:46.365180   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.365284   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.365411   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 17:17:46.365527   333 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 17:17:46.365638   333 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.365862   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.366091   333 net.cpp:267] TRAIN Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 16 3 6 24 (6912)
I0511 17:17:46.366204   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.366312   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.366422   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 17:17:46.366533   333 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.366642   333 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.366822   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.367003   333 net.cpp:267] TRAIN Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 16 432 (6912)
I0511 17:17:46.367116   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.367224   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.367344   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 17:17:46.367460   333 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.367571   333 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 17:17:46.368165   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 17:17:46.368764   333 net.cpp:267] TRAIN Top shape for layer 88 'ctx_output4/relu_mbox_conf' 16 24 3 6 (6912)
I0511 17:17:46.368886   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.368999   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.369117   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 17:17:46.369246   333 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 17:17:46.369361   333 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.369580   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.369792   333 net.cpp:267] TRAIN Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 16 3 6 24 (6912)
I0511 17:17:46.369884   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.369961   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.370043   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 17:17:46.370124   333 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.370205   333 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.370362   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.370514   333 net.cpp:267] TRAIN Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 16 432 (6912)
I0511 17:17:46.370597   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.370679   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.370802   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 17:17:46.370884   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.370970   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 17:17:46.371054   333 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 17:17:46.371157   333 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 17:17:46.371258   333 net.cpp:267] TRAIN Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 17:17:46.371348   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.371428   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.371515   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 17:17:46.371603   333 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.371681   333 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 17:17:46.372193   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 17:17:46.372710   333 net.cpp:267] TRAIN Top shape for layer 92 'ctx_output5/relu_mbox_loc' 16 16 2 3 (1536)
I0511 17:17:46.372804   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.372886   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.372970   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 17:17:46.373052   333 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 17:17:46.373131   333 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373344   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373564   333 net.cpp:267] TRAIN Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 16 2 3 16 (1536)
I0511 17:17:46.373651   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.373728   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.373808   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 17:17:46.373889   333 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.373966   333 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.374115   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.374279   333 net.cpp:267] TRAIN Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 16 96 (1536)
I0511 17:17:46.374363   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.374440   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.374529   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 17:17:46.374616   333 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.374694   333 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 17:17:46.375190   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 17:17:46.375690   333 net.cpp:267] TRAIN Top shape for layer 95 'ctx_output5/relu_mbox_conf' 16 16 2 3 (1536)
I0511 17:17:46.375783   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.375865   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.375946   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 17:17:46.376027   333 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 17:17:46.376106   333 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376303   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376507   333 net.cpp:267] TRAIN Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 16 2 3 16 (1536)
I0511 17:17:46.376598   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.376667   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.376756   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 17:17:46.376835   333 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.376914   333 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.377065   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.377203   333 net.cpp:267] TRAIN Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 16 96 (1536)
I0511 17:17:46.377295   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.377382   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.377460   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 17:17:46.377537   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.377617   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 17:17:46.377692   333 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 17:17:46.377794   333 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 17:17:46.377893   333 net.cpp:267] TRAIN Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 17:17:46.377979   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.378053   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.378139   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 17:17:46.378226   333 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.378305   333 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 17:17:46.378782   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 17:17:46.379269   333 net.cpp:267] TRAIN Top shape for layer 99 'ctx_output6/relu_mbox_loc' 16 16 1 2 (512)
I0511 17:17:46.379365   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.379457   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.379561   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 17:17:46.379647   333 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 17:17:46.379724   333 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.379936   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.380133   333 net.cpp:267] TRAIN Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 16 1 2 16 (512)
I0511 17:17:46.380224   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.380296   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.380384   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 17:17:46.380460   333 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.380538   333 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.380684   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.380833   333 net.cpp:267] TRAIN Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 16 32 (512)
I0511 17:17:46.380919   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.380996   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.381093   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 17:17:46.381182   333 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.381261   333 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 17:17:46.381745   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 17:17:46.382225   333 net.cpp:267] TRAIN Top shape for layer 102 'ctx_output6/relu_mbox_conf' 16 16 1 2 (512)
I0511 17:17:46.382320   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.382407   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.382488   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 17:17:46.382577   333 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 17:17:46.382658   333 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.382863   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.383052   333 net.cpp:267] TRAIN Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 16 1 2 16 (512)
I0511 17:17:46.383143   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.383216   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.383301   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 17:17:46.383378   333 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.383451   333 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.383596   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.383738   333 net.cpp:267] TRAIN Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 16 32 (512)
I0511 17:17:46.383828   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.383901   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.383985   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 17:17:46.384066   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.384145   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 17:17:46.384236   333 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 17:17:46.384351   333 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 17:17:46.384446   333 net.cpp:267] TRAIN Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 17:17:46.384529   333 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 17:17:46.384611   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.384703   333 net.cpp:200] Created Layer mbox_loc (106)
I0511 17:17:46.384776   333 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.384865   333 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.384944   333 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.385020   333 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.385109   333 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.385180   333 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.385257   333 net.cpp:542] mbox_loc -> mbox_loc
I0511 17:17:46.385378   333 net.cpp:260] Setting up mbox_loc
I0511 17:17:46.385488   333 net.cpp:267] TRAIN Top shape for layer 106 'mbox_loc' 16 69200 (1107200)
I0511 17:17:46.385578   333 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 17:17:46.385649   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.385732   333 net.cpp:200] Created Layer mbox_conf (107)
I0511 17:17:46.385814   333 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.385903   333 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.385974   333 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.386061   333 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.386135   333 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.386215   333 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.386292   333 net.cpp:542] mbox_conf -> mbox_conf
I0511 17:17:46.386404   333 net.cpp:260] Setting up mbox_conf
I0511 17:17:46.386500   333 net.cpp:267] TRAIN Top shape for layer 107 'mbox_conf' 16 69200 (1107200)
I0511 17:17:46.386590   333 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 17:17:46.386662   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.386754   333 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 17:17:46.386826   333 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 17:17:46.386901   333 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 17:17:46.386983   333 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 17:17:46.387054   333 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 17:17:46.387135   333 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 17:17:46.387217   333 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 17:17:46.387295   333 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 17:17:46.387400   333 net.cpp:260] Setting up mbox_priorbox
I0511 17:17:46.387508   333 net.cpp:267] TRAIN Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 17:17:46.387595   333 layer_factory.hpp:172] Creating layer 'mbox_loss' of type 'MultiBoxLoss'
I0511 17:17:46.387672   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.387769   333 net.cpp:200] Created Layer mbox_loss (109)
I0511 17:17:46.387856   333 net.cpp:572] mbox_loss <- mbox_loc
I0511 17:17:46.387936   333 net.cpp:572] mbox_loss <- mbox_conf
I0511 17:17:46.388007   333 net.cpp:572] mbox_loss <- mbox_priorbox
I0511 17:17:46.388093   333 net.cpp:572] mbox_loss <- label
I0511 17:17:46.388170   333 net.cpp:542] mbox_loss -> mbox_loss
I0511 17:17:46.388324   333 layer_factory.hpp:172] Creating layer 'mbox_loss_smooth_L1_loc' of type 'SmoothL1Loss'
I0511 17:17:46.388486   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.388697   333 layer_factory.hpp:172] Creating layer 'mbox_loss_softmax_conf' of type 'SoftmaxWithLoss'
I0511 17:17:46.388896   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.389112   333 net.cpp:260] Setting up mbox_loss
I0511 17:17:46.389319   333 net.cpp:267] TRAIN Top shape for layer 109 'mbox_loss' (1)
I0511 17:17:46.389408   333 net.cpp:271]     with loss weight 1
I0511 17:17:46.389535   333 net.cpp:336] mbox_loss needs backward computation.
I0511 17:17:46.389621   333 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 17:17:46.389698   333 net.cpp:336] mbox_conf needs backward computation.
I0511 17:17:46.389788   333 net.cpp:336] mbox_loc needs backward computation.
I0511 17:17:46.389865   333 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.389953   333 net.cpp:336] ctx_output6/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.390030   333 net.cpp:336] ctx_output6/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.390105   333 net.cpp:336] ctx_output6/relu_mbox_conf needs backward computation.
I0511 17:17:46.390187   333 net.cpp:336] ctx_output6/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.390262   333 net.cpp:336] ctx_output6/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.390336   333 net.cpp:336] ctx_output6/relu_mbox_loc needs backward computation.
I0511 17:17:46.390411   333 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.390496   333 net.cpp:336] ctx_output5/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.390565   333 net.cpp:336] ctx_output5/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.390650   333 net.cpp:336] ctx_output5/relu_mbox_conf needs backward computation.
I0511 17:17:46.390725   333 net.cpp:336] ctx_output5/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.390805   333 net.cpp:336] ctx_output5/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.390918   333 net.cpp:336] ctx_output5/relu_mbox_loc needs backward computation.
I0511 17:17:46.391003   333 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.391074   333 net.cpp:336] ctx_output4/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.391162   333 net.cpp:336] ctx_output4/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.391235   333 net.cpp:336] ctx_output4/relu_mbox_conf needs backward computation.
I0511 17:17:46.391309   333 net.cpp:336] ctx_output4/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.391383   333 net.cpp:336] ctx_output4/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.391458   333 net.cpp:336] ctx_output4/relu_mbox_loc needs backward computation.
I0511 17:17:46.391535   333 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.391609   333 net.cpp:336] ctx_output3/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.391690   333 net.cpp:336] ctx_output3/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.391764   333 net.cpp:336] ctx_output3/relu_mbox_conf needs backward computation.
I0511 17:17:46.391837   333 net.cpp:336] ctx_output3/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.391918   333 net.cpp:336] ctx_output3/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.391997   333 net.cpp:336] ctx_output3/relu_mbox_loc needs backward computation.
I0511 17:17:46.392077   333 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.392154   333 net.cpp:336] ctx_output2/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.392241   333 net.cpp:336] ctx_output2/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.392315   333 net.cpp:336] ctx_output2/relu_mbox_conf needs backward computation.
I0511 17:17:46.392407   333 net.cpp:336] ctx_output2/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.392488   333 net.cpp:336] ctx_output2/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.392565   333 net.cpp:336] ctx_output2/relu_mbox_loc needs backward computation.
I0511 17:17:46.392642   333 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.392719   333 net.cpp:336] ctx_output1/relu_mbox_conf_flat needs backward computation.
I0511 17:17:46.392796   333 net.cpp:336] ctx_output1/relu_mbox_conf_perm needs backward computation.
I0511 17:17:46.392872   333 net.cpp:336] ctx_output1/relu_mbox_conf needs backward computation.
I0511 17:17:46.392947   333 net.cpp:336] ctx_output1/relu_mbox_loc_flat needs backward computation.
I0511 17:17:46.393023   333 net.cpp:336] ctx_output1/relu_mbox_loc_perm needs backward computation.
I0511 17:17:46.393100   333 net.cpp:336] ctx_output1/relu_mbox_loc needs backward computation.
I0511 17:17:46.393172   333 net.cpp:336] ctx_output6_ctx_output6/relu_0_split needs backward computation.
I0511 17:17:46.393260   333 net.cpp:336] ctx_output6/relu needs backward computation.
I0511 17:17:46.393344   333 net.cpp:336] ctx_output6 needs backward computation.
I0511 17:17:46.393420   333 net.cpp:336] ctx_output5_ctx_output5/relu_0_split needs backward computation.
I0511 17:17:46.393499   333 net.cpp:336] ctx_output5/relu needs backward computation.
I0511 17:17:46.393584   333 net.cpp:336] ctx_output5 needs backward computation.
I0511 17:17:46.393656   333 net.cpp:336] ctx_output4_ctx_output4/relu_0_split needs backward computation.
I0511 17:17:46.393736   333 net.cpp:336] ctx_output4/relu needs backward computation.
I0511 17:17:46.393808   333 net.cpp:336] ctx_output4 needs backward computation.
I0511 17:17:46.393893   333 net.cpp:336] ctx_output3_ctx_output3/relu_0_split needs backward computation.
I0511 17:17:46.393968   333 net.cpp:336] ctx_output3/relu needs backward computation.
I0511 17:17:46.394044   333 net.cpp:336] ctx_output3 needs backward computation.
I0511 17:17:46.394120   333 net.cpp:336] ctx_output2_ctx_output2/relu_0_split needs backward computation.
I0511 17:17:46.394201   333 net.cpp:336] ctx_output2/relu needs backward computation.
I0511 17:17:46.394271   333 net.cpp:336] ctx_output2 needs backward computation.
I0511 17:17:46.394356   333 net.cpp:336] ctx_output1_ctx_output1/relu_0_split needs backward computation.
I0511 17:17:46.394433   333 net.cpp:336] ctx_output1/relu needs backward computation.
I0511 17:17:46.394510   333 net.cpp:336] ctx_output1 needs backward computation.
I0511 17:17:46.394585   333 net.cpp:336] pool9 needs backward computation.
I0511 17:17:46.394663   333 net.cpp:336] pool8_pool8_0_split needs backward computation.
I0511 17:17:46.394742   333 net.cpp:336] pool8 needs backward computation.
I0511 17:17:46.394829   333 net.cpp:336] pool7_pool7_0_split needs backward computation.
I0511 17:17:46.394899   333 net.cpp:336] pool7 needs backward computation.
I0511 17:17:46.394975   333 net.cpp:336] pool6_pool6_0_split needs backward computation.
I0511 17:17:46.395051   333 net.cpp:336] pool6 needs backward computation.
I0511 17:17:46.395128   333 net.cpp:336] res5a_branch2b_res5a_branch2b/relu_0_split needs backward computation.
I0511 17:17:46.395202   333 net.cpp:336] res5a_branch2b/relu needs backward computation.
I0511 17:17:46.395274   333 net.cpp:336] res5a_branch2b/bn needs backward computation.
I0511 17:17:46.395359   333 net.cpp:336] res5a_branch2b needs backward computation.
I0511 17:17:46.395434   333 net.cpp:336] res5a_branch2a/relu needs backward computation.
I0511 17:17:46.395506   333 net.cpp:336] res5a_branch2a/bn needs backward computation.
I0511 17:17:46.395591   333 net.cpp:336] res5a_branch2a needs backward computation.
I0511 17:17:46.395665   333 net.cpp:336] pool4 needs backward computation.
I0511 17:17:46.395741   333 net.cpp:336] res4a_branch2b/relu needs backward computation.
I0511 17:17:46.395828   333 net.cpp:336] res4a_branch2b/bn needs backward computation.
I0511 17:17:46.395921   333 net.cpp:336] res4a_branch2b needs backward computation.
I0511 17:17:46.395993   333 net.cpp:336] res4a_branch2a/relu needs backward computation.
I0511 17:17:46.396066   333 net.cpp:336] res4a_branch2a/bn needs backward computation.
I0511 17:17:46.396140   333 net.cpp:336] res4a_branch2a needs backward computation.
I0511 17:17:46.396216   333 net.cpp:336] pool3 needs backward computation.
I0511 17:17:46.396301   333 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0511 17:17:46.396371   333 net.cpp:336] res3a_branch2b/relu needs backward computation.
I0511 17:17:46.396445   333 net.cpp:336] res3a_branch2b/bn needs backward computation.
I0511 17:17:46.396520   333 net.cpp:336] res3a_branch2b needs backward computation.
I0511 17:17:46.396595   333 net.cpp:336] res3a_branch2a/relu needs backward computation.
I0511 17:17:46.396669   333 net.cpp:336] res3a_branch2a/bn needs backward computation.
I0511 17:17:46.396742   333 net.cpp:336] res3a_branch2a needs backward computation.
I0511 17:17:46.396816   333 net.cpp:336] pool2 needs backward computation.
I0511 17:17:46.396893   333 net.cpp:336] res2a_branch2b/relu needs backward computation.
I0511 17:17:46.396967   333 net.cpp:336] res2a_branch2b/bn needs backward computation.
I0511 17:17:46.397042   333 net.cpp:336] res2a_branch2b needs backward computation.
I0511 17:17:46.397117   333 net.cpp:336] res2a_branch2a/relu needs backward computation.
I0511 17:17:46.397193   333 net.cpp:336] res2a_branch2a/bn needs backward computation.
I0511 17:17:46.397269   333 net.cpp:336] res2a_branch2a needs backward computation.
I0511 17:17:46.397357   333 net.cpp:336] pool1 needs backward computation.
I0511 17:17:46.397428   333 net.cpp:336] conv1b/relu needs backward computation.
I0511 17:17:46.397512   333 net.cpp:336] conv1b/bn needs backward computation.
I0511 17:17:46.397586   333 net.cpp:336] conv1b needs backward computation.
I0511 17:17:46.397661   333 net.cpp:336] conv1a/relu needs backward computation.
I0511 17:17:46.397737   333 net.cpp:336] conv1a/bn needs backward computation.
I0511 17:17:46.397811   333 net.cpp:336] conv1a needs backward computation.
I0511 17:17:46.397888   333 net.cpp:338] data/bias does not need backward computation.
I0511 17:17:46.397974   333 net.cpp:338] data_data_0_split does not need backward computation.
I0511 17:17:46.398048   333 net.cpp:338] data does not need backward computation.
I0511 17:17:46.398123   333 net.cpp:380] This network produces output mbox_loss
I0511 17:17:46.398334   333 net.cpp:403] Top memory (TRAIN) required for data: 2411201928 diff: 2411201928
I0511 17:17:46.398552   333 net.cpp:406] Bottom memory (TRAIN) required for data: 2411201920 diff: 2411201920
I0511 17:17:46.398635   333 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 1043431424 diff: 1043431424
I0511 17:17:46.398706   333 net.cpp:412] Parameters memory (TRAIN) required for data: 12464288 diff: 12464288
I0511 17:17:46.398787   333 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0511 17:17:46.398862   333 net.cpp:421] Network initialization done.
I0511 17:17:46.400710   333 solver.cpp:175] Creating test net (#0) specified by test_net file: training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/test.prototxt
I0511 17:17:46.402889   333 net.cpp:80] Initializing net from parameters: 
name: "ssdJacintoNetV2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "AnnotatedData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 0
    mean_value: 0
    mean_value: 0
    force_color: false
    resize_param {
      prob: 1
      resize_mode: WARP
      height: 320
      width: 768
      interp_mode: LINEAR
    }
    crop_h: 320
    crop_w: 768
  }
  data_param {
    source: "/workspace/data/EYES/lmdb/official_test_850images"
    batch_size: 8
    backend: LMDB
    threads: 4
    parser_threads: 4
  }
  annotated_data_param {
    batch_sampler {
    }
    label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool6"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool7"
  type: "Pooling"
  bottom: "pool6"
  top: "pool7"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool8"
  type: "Pooling"
  bottom: "pool7"
  top: "pool8"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "pool9"
  type: "Pooling"
  bottom: "pool8"
  top: "pool9"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "ctx_output1"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "ctx_output1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu"
  type: "ReLU"
  bottom: "ctx_output1"
  top: "ctx_output1"
}
layer {
  name: "ctx_output2"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "ctx_output2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu"
  type: "ReLU"
  bottom: "ctx_output2"
  top: "ctx_output2"
}
layer {
  name: "ctx_output3"
  type: "Convolution"
  bottom: "pool6"
  top: "ctx_output3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu"
  type: "ReLU"
  bottom: "ctx_output3"
  top: "ctx_output3"
}
layer {
  name: "ctx_output4"
  type: "Convolution"
  bottom: "pool7"
  top: "ctx_output4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu"
  type: "ReLU"
  bottom: "ctx_output4"
  top: "ctx_output4"
}
layer {
  name: "ctx_output5"
  type: "Convolution"
  bottom: "pool8"
  top: "ctx_output5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu"
  type: "ReLU"
  bottom: "ctx_output5"
  top: "ctx_output5"
}
layer {
  name: "ctx_output6"
  type: "Convolution"
  bottom: "pool9"
  top: "ctx_output6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu"
  type: "ReLU"
  bottom: "ctx_output6"
  top: "ctx_output6"
}
layer {
  name: "ctx_output1/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_loc"
  top: "ctx_output1/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_loc_perm"
  top: "ctx_output1/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output1"
  top: "ctx_output1/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output1/relu_mbox_conf"
  top: "ctx_output1/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output1/relu_mbox_conf_perm"
  top: "ctx_output1/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output1/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output1"
  bottom: "data"
  top: "ctx_output1/relu_mbox_priorbox"
  prior_box_param {
    min_size: 14.72
    max_size: 36.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_loc"
  top: "ctx_output2/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_loc_perm"
  top: "ctx_output2/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output2"
  top: "ctx_output2/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output2/relu_mbox_conf"
  top: "ctx_output2/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output2/relu_mbox_conf_perm"
  top: "ctx_output2/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output2/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output2"
  bottom: "data"
  top: "ctx_output2/relu_mbox_priorbox"
  prior_box_param {
    min_size: 36.8
    max_size: 110.4
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_loc"
  top: "ctx_output3/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_loc_perm"
  top: "ctx_output3/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output3"
  top: "ctx_output3/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output3/relu_mbox_conf"
  top: "ctx_output3/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output3/relu_mbox_conf_perm"
  top: "ctx_output3/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output3/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output3"
  bottom: "data"
  top: "ctx_output3/relu_mbox_priorbox"
  prior_box_param {
    min_size: 110.4
    max_size: 184
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_loc"
  top: "ctx_output4/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_loc_perm"
  top: "ctx_output4/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output4"
  top: "ctx_output4/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output4/relu_mbox_conf"
  top: "ctx_output4/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output4/relu_mbox_conf_perm"
  top: "ctx_output4/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output4/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output4"
  bottom: "data"
  top: "ctx_output4/relu_mbox_priorbox"
  prior_box_param {
    min_size: 184
    max_size: 257.6
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_loc"
  top: "ctx_output5/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_loc_perm"
  top: "ctx_output5/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output5"
  top: "ctx_output5/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output5/relu_mbox_conf"
  top: "ctx_output5/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output5/relu_mbox_conf_perm"
  top: "ctx_output5/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output5/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output5"
  bottom: "data"
  top: "ctx_output5/relu_mbox_priorbox"
  prior_box_param {
    min_size: 257.6
    max_size: 331.2
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_loc"
  top: "ctx_output6/relu_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_loc_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_loc_perm"
  top: "ctx_output6/relu_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf"
  type: "Convolution"
  bottom: "ctx_output6"
  top: "ctx_output6/relu_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_perm"
  type: "Permute"
  bottom: "ctx_output6/relu_mbox_conf"
  top: "ctx_output6/relu_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_conf_flat"
  type: "Flatten"
  bottom: "ctx_output6/relu_mbox_conf_perm"
  top: "ctx_output6/relu_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "ctx_output6/relu_mbox_priorbox"
  type: "PriorBox"
  bottom: "ctx_output6"
  bottom: "data"
  top: "ctx_output6/relu_mbox_priorbox"
  prior_box_param {
    min_size: 331.2
    max_size: 404.8
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_loc_flat"
  bottom: "ctx_output2/relu_mbox_loc_flat"
  bottom: "ctx_output3/relu_mbox_loc_flat"
  bottom: "ctx_output4/relu_mbox_loc_flat"
  bottom: "ctx_output5/relu_mbox_loc_flat"
  bottom: "ctx_output6/relu_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_conf_flat"
  bottom: "ctx_output2/relu_mbox_conf_flat"
  bottom: "ctx_output3/relu_mbox_conf_flat"
  bottom: "ctx_output4/relu_mbox_conf_flat"
  bottom: "ctx_output5/relu_mbox_conf_flat"
  bottom: "ctx_output6/relu_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "ctx_output1/relu_mbox_priorbox"
  bottom: "ctx_output2/relu_mbox_priorbox"
  bottom: "ctx_output3/relu_mbox_priorbox"
  bottom: "ctx_output4/relu_mbox_priorbox"
  bottom: "ctx_output5/relu_mbox_priorbox"
  bottom: "ctx_output6/relu_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 4
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 4
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    save_output_param {
      output_directory: ""
      output_name_prefix: "comp4_det_test_"
      output_format: "VOC"
      label_map_file: "/workspace/caffe-jacinto/data/EYES/labelmap_eye.prototxt"
      name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
      num_test_image: 850
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
  }
}
layer {
  name: "detection_eval"
  type: "DetectionEvaluate"
  bottom: "detection_out"
  bottom: "label"
  top: "detection_eval"
  include {
    phase: TEST
  }
  detection_evaluate_param {
    num_classes: 4
    background_label_id: 0
    overlap_threshold: 0.5
    evaluate_difficult_gt: false
    name_size_file: "/workspace/caffe-jacinto/data/EYES/test_name_size.txt"
  }
}
I0511 17:17:46.411530   333 net.cpp:110] Using FLOAT as default forward math type
I0511 17:17:46.412184   333 net.cpp:116] Using FLOAT as default backward math type
I0511 17:17:46.412271   333 layer_factory.hpp:172] Creating layer 'data' of type 'AnnotatedData'
I0511 17:17:46.412355   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.412463   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.412787   333 net.cpp:200] Created Layer data (0)
I0511 17:17:46.413089   333 net.cpp:542] data -> data
I0511 17:17:46.413174   333 net.cpp:542] data -> label
I0511 17:17:46.413264   333 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I0511 17:17:46.413419   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.422325   376 db_lmdb.cpp:36] Opened lmdb /workspace/data/EYES/lmdb/official_test_850images
I0511 17:17:46.424015   333 annotated_data_layer.cpp:105] output data size: 8,3,320,768
I0511 17:17:46.424108   333 annotated_data_layer.cpp:150] (0) Output data size: 8, 3, 320, 768
I0511 17:17:46.424162   333 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0511 17:17:46.424273   333 net.cpp:260] Setting up data
I0511 17:17:46.424283   333 net.cpp:267] TEST Top shape for layer 0 'data' 8 3 320 768 (5898240)
I0511 17:17:46.424301   333 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 2 8 (16)
I0511 17:17:46.424311   333 layer_factory.hpp:172] Creating layer 'data_data_0_split' of type 'Split'
I0511 17:17:46.424319   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.424331   333 net.cpp:200] Created Layer data_data_0_split (1)
I0511 17:17:46.424337   333 net.cpp:572] data_data_0_split <- data
I0511 17:17:46.424345   333 net.cpp:542] data_data_0_split -> data_data_0_split_0
I0511 17:17:46.424355   333 net.cpp:542] data_data_0_split -> data_data_0_split_1
I0511 17:17:46.424366   333 net.cpp:542] data_data_0_split -> data_data_0_split_2
I0511 17:17:46.424373   333 net.cpp:542] data_data_0_split -> data_data_0_split_3
I0511 17:17:46.424381   333 net.cpp:542] data_data_0_split -> data_data_0_split_4
I0511 17:17:46.424387   333 net.cpp:542] data_data_0_split -> data_data_0_split_5
I0511 17:17:46.424394   333 net.cpp:542] data_data_0_split -> data_data_0_split_6
I0511 17:17:46.424515   333 net.cpp:260] Setting up data_data_0_split
I0511 17:17:46.424547   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424574   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424603   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424628   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424654   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424679   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424703   333 net.cpp:267] TEST Top shape for layer 1 'data_data_0_split' 8 3 320 768 (5898240)
I0511 17:17:46.424729   333 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I0511 17:17:46.424751   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.424779   333 net.cpp:200] Created Layer data/bias (2)
I0511 17:17:46.424801   333 net.cpp:572] data/bias <- data_data_0_split_0
I0511 17:17:46.424825   333 net.cpp:542] data/bias -> data/bias
I0511 17:17:46.425046   333 net.cpp:260] Setting up data/bias
I0511 17:17:46.425076   333 net.cpp:267] TEST Top shape for layer 2 'data/bias' 8 3 320 768 (5898240)
I0511 17:17:46.425113   333 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I0511 17:17:46.425135   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.425170   333 net.cpp:200] Created Layer conv1a (3)
I0511 17:17:46.425192   333 net.cpp:572] conv1a <- data/bias
I0511 17:17:46.425216   333 net.cpp:542] conv1a -> conv1a
I0511 17:17:46.426573   333 net.cpp:260] Setting up conv1a
I0511 17:17:46.426612   333 net.cpp:267] TEST Top shape for layer 3 'conv1a' 8 32 160 384 (15728640)
I0511 17:17:46.426648   333 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0511 17:17:46.426671   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.426700   333 net.cpp:200] Created Layer conv1a/bn (4)
I0511 17:17:46.426723   333 net.cpp:572] conv1a/bn <- conv1a
I0511 17:17:46.426746   333 net.cpp:527] conv1a/bn -> conv1a (in-place)
I0511 17:17:46.427309   333 net.cpp:260] Setting up conv1a/bn
I0511 17:17:46.427335   333 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 8 32 160 384 (15728640)
I0511 17:17:46.427371   333 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I0511 17:17:46.427394   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.427426   333 net.cpp:200] Created Layer conv1a/relu (5)
I0511 17:17:46.427448   333 net.cpp:572] conv1a/relu <- conv1a
I0511 17:17:46.427481   333 net.cpp:527] conv1a/relu -> conv1a (in-place)
I0511 17:17:46.427508   333 net.cpp:260] Setting up conv1a/relu
I0511 17:17:46.427531   333 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 8 32 160 384 (15728640)
I0511 17:17:46.427557   333 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I0511 17:17:46.427580   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.427614   333 net.cpp:200] Created Layer conv1b (6)
I0511 17:17:46.427637   333 net.cpp:572] conv1b <- conv1a
I0511 17:17:46.427662   333 net.cpp:542] conv1b -> conv1b
I0511 17:17:46.428103   377 data_layer.cpp:105] (0) Parser threads: 1
I0511 17:17:46.428439   377 data_layer.cpp:107] (0) Transformer threads: 1
I0511 17:17:46.428977   333 net.cpp:260] Setting up conv1b
I0511 17:17:46.428994   333 net.cpp:267] TEST Top shape for layer 6 'conv1b' 8 32 160 384 (15728640)
I0511 17:17:46.429044   333 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0511 17:17:46.429069   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429097   333 net.cpp:200] Created Layer conv1b/bn (7)
I0511 17:17:46.429121   333 net.cpp:572] conv1b/bn <- conv1b
I0511 17:17:46.429147   333 net.cpp:527] conv1b/bn -> conv1b (in-place)
I0511 17:17:46.429702   333 net.cpp:260] Setting up conv1b/bn
I0511 17:17:46.429731   333 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 8 32 160 384 (15728640)
I0511 17:17:46.429764   333 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I0511 17:17:46.429785   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429811   333 net.cpp:200] Created Layer conv1b/relu (8)
I0511 17:17:46.429831   333 net.cpp:572] conv1b/relu <- conv1b
I0511 17:17:46.429853   333 net.cpp:527] conv1b/relu -> conv1b (in-place)
I0511 17:17:46.429878   333 net.cpp:260] Setting up conv1b/relu
I0511 17:17:46.429898   333 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 8 32 160 384 (15728640)
I0511 17:17:46.429924   333 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I0511 17:17:46.429944   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.429970   333 net.cpp:200] Created Layer pool1 (9)
I0511 17:17:46.429991   333 net.cpp:572] pool1 <- conv1b
I0511 17:17:46.430012   333 net.cpp:542] pool1 -> pool1
I0511 17:17:46.430100   333 net.cpp:260] Setting up pool1
I0511 17:17:46.430124   333 net.cpp:267] TEST Top shape for layer 9 'pool1' 8 32 80 192 (3932160)
I0511 17:17:46.430150   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I0511 17:17:46.430171   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.430199   333 net.cpp:200] Created Layer res2a_branch2a (10)
I0511 17:17:46.430220   333 net.cpp:572] res2a_branch2a <- pool1
I0511 17:17:46.430243   333 net.cpp:542] res2a_branch2a -> res2a_branch2a
I0511 17:17:46.430990   333 net.cpp:260] Setting up res2a_branch2a
I0511 17:17:46.431023   333 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 8 64 80 192 (7864320)
I0511 17:17:46.431061   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.431080   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431107   333 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I0511 17:17:46.431128   333 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I0511 17:17:46.431150   333 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0511 17:17:46.431653   333 net.cpp:260] Setting up res2a_branch2a/bn
I0511 17:17:46.431684   333 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 8 64 80 192 (7864320)
I0511 17:17:46.431718   333 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.431744   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431777   333 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I0511 17:17:46.431795   333 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I0511 17:17:46.431820   333 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0511 17:17:46.431844   333 net.cpp:260] Setting up res2a_branch2a/relu
I0511 17:17:46.431864   333 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 8 64 80 192 (7864320)
I0511 17:17:46.431890   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I0511 17:17:46.431910   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.431941   333 net.cpp:200] Created Layer res2a_branch2b (13)
I0511 17:17:46.431962   333 net.cpp:572] res2a_branch2b <- res2a_branch2a
I0511 17:17:46.431983   333 net.cpp:542] res2a_branch2b -> res2a_branch2b
I0511 17:17:46.432514   333 net.cpp:260] Setting up res2a_branch2b
I0511 17:17:46.432546   333 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 8 64 80 192 (7864320)
I0511 17:17:46.432576   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.432597   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.432623   333 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I0511 17:17:46.432644   333 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I0511 17:17:46.432667   333 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0511 17:17:46.433121   333 net.cpp:260] Setting up res2a_branch2b/bn
I0511 17:17:46.433151   333 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 8 64 80 192 (7864320)
I0511 17:17:46.433184   333 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.433205   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433229   333 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I0511 17:17:46.433250   333 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I0511 17:17:46.433272   333 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0511 17:17:46.433301   333 net.cpp:260] Setting up res2a_branch2b/relu
I0511 17:17:46.433336   333 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 8 64 80 192 (7864320)
I0511 17:17:46.433369   333 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I0511 17:17:46.433400   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433436   333 net.cpp:200] Created Layer pool2 (16)
I0511 17:17:46.433471   333 net.cpp:572] pool2 <- res2a_branch2b
I0511 17:17:46.433501   333 net.cpp:542] pool2 -> pool2
I0511 17:17:46.433593   333 net.cpp:260] Setting up pool2
I0511 17:17:46.433687   333 net.cpp:267] TEST Top shape for layer 16 'pool2' 8 64 40 96 (1966080)
I0511 17:17:46.433727   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I0511 17:17:46.433754   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.433791   333 net.cpp:200] Created Layer res3a_branch2a (17)
I0511 17:17:46.433828   333 net.cpp:572] res3a_branch2a <- pool2
I0511 17:17:46.433857   333 net.cpp:542] res3a_branch2a -> res3a_branch2a
I0511 17:17:46.450163   333 net.cpp:260] Setting up res3a_branch2a
I0511 17:17:46.450193   333 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 8 128 40 96 (3932160)
I0511 17:17:46.450220   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.450229   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450244   333 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I0511 17:17:46.450254   333 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I0511 17:17:46.450263   333 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0511 17:17:46.450675   333 net.cpp:260] Setting up res3a_branch2a/bn
I0511 17:17:46.450712   333 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 8 128 40 96 (3932160)
I0511 17:17:46.450734   333 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.450742   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450752   333 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I0511 17:17:46.450758   333 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I0511 17:17:46.450764   333 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0511 17:17:46.450774   333 net.cpp:260] Setting up res3a_branch2a/relu
I0511 17:17:46.450779   333 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 8 128 40 96 (3932160)
I0511 17:17:46.450788   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I0511 17:17:46.450794   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.450811   333 net.cpp:200] Created Layer res3a_branch2b (20)
I0511 17:17:46.450817   333 net.cpp:572] res3a_branch2b <- res3a_branch2a
I0511 17:17:46.450824   333 net.cpp:542] res3a_branch2b -> res3a_branch2b
I0511 17:17:46.451901   333 net.cpp:260] Setting up res3a_branch2b
I0511 17:17:46.451910   333 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 8 128 40 96 (3932160)
I0511 17:17:46.451922   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.451928   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.451938   333 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I0511 17:17:46.451944   333 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I0511 17:17:46.451951   333 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0511 17:17:46.452301   333 net.cpp:260] Setting up res3a_branch2b/bn
I0511 17:17:46.452306   333 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 8 128 40 96 (3932160)
I0511 17:17:46.452322   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.452327   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452334   333 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I0511 17:17:46.452340   333 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I0511 17:17:46.452347   333 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0511 17:17:46.452353   333 net.cpp:260] Setting up res3a_branch2b/relu
I0511 17:17:46.452359   333 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 8 128 40 96 (3932160)
I0511 17:17:46.452368   333 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.452373   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452383   333 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0511 17:17:46.452389   333 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0511 17:17:46.452394   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.452401   333 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.452440   333 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0511 17:17:46.452445   333 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 17:17:46.452455   333 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 40 96 (3932160)
I0511 17:17:46.452462   333 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I0511 17:17:46.452468   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452478   333 net.cpp:200] Created Layer pool3 (24)
I0511 17:17:46.452484   333 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0511 17:17:46.452502   333 net.cpp:542] pool3 -> pool3
I0511 17:17:46.452555   333 net.cpp:260] Setting up pool3
I0511 17:17:46.452561   333 net.cpp:267] TEST Top shape for layer 24 'pool3' 8 128 20 48 (983040)
I0511 17:17:46.452570   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I0511 17:17:46.452576   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.452590   333 net.cpp:200] Created Layer res4a_branch2a (25)
I0511 17:17:46.452596   333 net.cpp:572] res4a_branch2a <- pool3
I0511 17:17:46.452602   333 net.cpp:542] res4a_branch2a -> res4a_branch2a
I0511 17:17:46.460067   333 net.cpp:260] Setting up res4a_branch2a
I0511 17:17:46.460103   333 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 8 256 20 48 (1966080)
I0511 17:17:46.460140   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.460163   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460191   333 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I0511 17:17:46.460214   333 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I0511 17:17:46.460239   333 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0511 17:17:46.460703   333 net.cpp:260] Setting up res4a_branch2a/bn
I0511 17:17:46.460728   333 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 8 256 20 48 (1966080)
I0511 17:17:46.460762   333 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.460784   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460809   333 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I0511 17:17:46.460832   333 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I0511 17:17:46.460855   333 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0511 17:17:46.460880   333 net.cpp:260] Setting up res4a_branch2a/relu
I0511 17:17:46.460901   333 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 8 256 20 48 (1966080)
I0511 17:17:46.460927   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I0511 17:17:46.460949   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.460979   333 net.cpp:200] Created Layer res4a_branch2b (28)
I0511 17:17:46.461002   333 net.cpp:572] res4a_branch2b <- res4a_branch2a
I0511 17:17:46.461025   333 net.cpp:542] res4a_branch2b -> res4a_branch2b
I0511 17:17:46.464432   333 net.cpp:260] Setting up res4a_branch2b
I0511 17:17:46.466212   333 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 8 256 20 48 (1966080)
I0511 17:17:46.466311   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.466377   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.466437   333 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I0511 17:17:46.466496   333 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I0511 17:17:46.466547   333 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0511 17:17:46.467195   333 net.cpp:260] Setting up res4a_branch2b/bn
I0511 17:17:46.467849   333 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 8 256 20 48 (1966080)
I0511 17:17:46.467901   333 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.467938   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.467978   333 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I0511 17:17:46.468014   333 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I0511 17:17:46.468042   333 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0511 17:17:46.468075   333 net.cpp:260] Setting up res4a_branch2b/relu
I0511 17:17:46.468111   333 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 8 256 20 48 (1966080)
I0511 17:17:46.468163   333 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I0511 17:17:46.468191   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.468225   333 net.cpp:200] Created Layer pool4 (31)
I0511 17:17:46.468256   333 net.cpp:572] pool4 <- res4a_branch2b
I0511 17:17:46.468286   333 net.cpp:542] pool4 -> pool4
I0511 17:17:46.468376   333 net.cpp:260] Setting up pool4
I0511 17:17:46.468468   333 net.cpp:267] TEST Top shape for layer 31 'pool4' 8 256 10 24 (491520)
I0511 17:17:46.468505   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I0511 17:17:46.468538   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.468576   333 net.cpp:200] Created Layer res5a_branch2a (32)
I0511 17:17:46.468616   333 net.cpp:572] res5a_branch2a <- pool4
I0511 17:17:46.468645   333 net.cpp:542] res5a_branch2a -> res5a_branch2a
I0511 17:17:46.507541   333 net.cpp:260] Setting up res5a_branch2a
I0511 17:17:46.507601   333 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 8 512 10 24 (983040)
I0511 17:17:46.507639   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0511 17:17:46.507658   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.507671   333 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I0511 17:17:46.507707   333 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I0511 17:17:46.507730   333 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0511 17:17:46.508177   333 net.cpp:260] Setting up res5a_branch2a/bn
I0511 17:17:46.508201   333 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 8 512 10 24 (983040)
I0511 17:17:46.508234   333 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0511 17:17:46.508255   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.508280   333 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I0511 17:17:46.508301   333 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I0511 17:17:46.508322   333 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0511 17:17:46.508347   333 net.cpp:260] Setting up res5a_branch2a/relu
I0511 17:17:46.508366   333 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 8 512 10 24 (983040)
I0511 17:17:46.508390   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I0511 17:17:46.508411   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.508440   333 net.cpp:200] Created Layer res5a_branch2b (35)
I0511 17:17:46.508461   333 net.cpp:572] res5a_branch2b <- res5a_branch2a
I0511 17:17:46.508482   333 net.cpp:542] res5a_branch2b -> res5a_branch2b
I0511 17:17:46.522357   333 net.cpp:260] Setting up res5a_branch2b
I0511 17:17:46.522419   333 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 8 512 10 24 (983040)
I0511 17:17:46.522467   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0511 17:17:46.522488   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.522518   333 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I0511 17:17:46.522541   333 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I0511 17:17:46.522565   333 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0511 17:17:46.522992   333 net.cpp:260] Setting up res5a_branch2b/bn
I0511 17:17:46.523020   333 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 8 512 10 24 (983040)
I0511 17:17:46.523051   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0511 17:17:46.523072   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523097   333 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I0511 17:17:46.523128   333 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I0511 17:17:46.523161   333 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0511 17:17:46.523186   333 net.cpp:260] Setting up res5a_branch2b/relu
I0511 17:17:46.523206   333 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 8 512 10 24 (983040)
I0511 17:17:46.523231   333 layer_factory.hpp:172] Creating layer 'res5a_branch2b_res5a_branch2b/relu_0_split' of type 'Split'
I0511 17:17:46.523252   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523277   333 net.cpp:200] Created Layer res5a_branch2b_res5a_branch2b/relu_0_split (38)
I0511 17:17:46.523296   333 net.cpp:572] res5a_branch2b_res5a_branch2b/relu_0_split <- res5a_branch2b
I0511 17:17:46.523319   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.523342   333 net.cpp:542] res5a_branch2b_res5a_branch2b/relu_0_split -> res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.523406   333 net.cpp:260] Setting up res5a_branch2b_res5a_branch2b/relu_0_split
I0511 17:17:46.523427   333 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 17:17:46.523450   333 net.cpp:267] TEST Top shape for layer 38 'res5a_branch2b_res5a_branch2b/relu_0_split' 8 512 10 24 (983040)
I0511 17:17:46.523474   333 layer_factory.hpp:172] Creating layer 'pool6' of type 'Pooling'
I0511 17:17:46.523495   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523521   333 net.cpp:200] Created Layer pool6 (39)
I0511 17:17:46.523542   333 net.cpp:572] pool6 <- res5a_branch2b_res5a_branch2b/relu_0_split_0
I0511 17:17:46.523563   333 net.cpp:542] pool6 -> pool6
I0511 17:17:46.523641   333 net.cpp:260] Setting up pool6
I0511 17:17:46.523661   333 net.cpp:267] TEST Top shape for layer 39 'pool6' 8 512 5 12 (245760)
I0511 17:17:46.523685   333 layer_factory.hpp:172] Creating layer 'pool6_pool6_0_split' of type 'Split'
I0511 17:17:46.523705   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523728   333 net.cpp:200] Created Layer pool6_pool6_0_split (40)
I0511 17:17:46.523749   333 net.cpp:572] pool6_pool6_0_split <- pool6
I0511 17:17:46.523770   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_0
I0511 17:17:46.523792   333 net.cpp:542] pool6_pool6_0_split -> pool6_pool6_0_split_1
I0511 17:17:46.523849   333 net.cpp:260] Setting up pool6_pool6_0_split
I0511 17:17:46.523869   333 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 17:17:46.523891   333 net.cpp:267] TEST Top shape for layer 40 'pool6_pool6_0_split' 8 512 5 12 (245760)
I0511 17:17:46.523916   333 layer_factory.hpp:172] Creating layer 'pool7' of type 'Pooling'
I0511 17:17:46.523936   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.523959   333 net.cpp:200] Created Layer pool7 (41)
I0511 17:17:46.523980   333 net.cpp:572] pool7 <- pool6_pool6_0_split_0
I0511 17:17:46.524001   333 net.cpp:542] pool7 -> pool7
I0511 17:17:46.524076   333 net.cpp:260] Setting up pool7
I0511 17:17:46.524096   333 net.cpp:267] TEST Top shape for layer 41 'pool7' 8 512 3 6 (73728)
I0511 17:17:46.524121   333 layer_factory.hpp:172] Creating layer 'pool7_pool7_0_split' of type 'Split'
I0511 17:17:46.524140   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524163   333 net.cpp:200] Created Layer pool7_pool7_0_split (42)
I0511 17:17:46.524184   333 net.cpp:572] pool7_pool7_0_split <- pool7
I0511 17:17:46.524206   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_0
I0511 17:17:46.524231   333 net.cpp:542] pool7_pool7_0_split -> pool7_pool7_0_split_1
I0511 17:17:46.524287   333 net.cpp:260] Setting up pool7_pool7_0_split
I0511 17:17:46.524307   333 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 17:17:46.524343   333 net.cpp:267] TEST Top shape for layer 42 'pool7_pool7_0_split' 8 512 3 6 (73728)
I0511 17:17:46.524366   333 layer_factory.hpp:172] Creating layer 'pool8' of type 'Pooling'
I0511 17:17:46.524387   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524411   333 net.cpp:200] Created Layer pool8 (43)
I0511 17:17:46.524432   333 net.cpp:572] pool8 <- pool7_pool7_0_split_0
I0511 17:17:46.524453   333 net.cpp:542] pool8 -> pool8
I0511 17:17:46.524526   333 net.cpp:260] Setting up pool8
I0511 17:17:46.524547   333 net.cpp:267] TEST Top shape for layer 43 'pool8' 8 512 2 3 (24576)
I0511 17:17:46.524571   333 layer_factory.hpp:172] Creating layer 'pool8_pool8_0_split' of type 'Split'
I0511 17:17:46.524591   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524614   333 net.cpp:200] Created Layer pool8_pool8_0_split (44)
I0511 17:17:46.524634   333 net.cpp:572] pool8_pool8_0_split <- pool8
I0511 17:17:46.524657   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_0
I0511 17:17:46.524678   333 net.cpp:542] pool8_pool8_0_split -> pool8_pool8_0_split_1
I0511 17:17:46.524734   333 net.cpp:260] Setting up pool8_pool8_0_split
I0511 17:17:46.524753   333 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 17:17:46.524776   333 net.cpp:267] TEST Top shape for layer 44 'pool8_pool8_0_split' 8 512 2 3 (24576)
I0511 17:17:46.524799   333 layer_factory.hpp:172] Creating layer 'pool9' of type 'Pooling'
I0511 17:17:46.524821   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.524843   333 net.cpp:200] Created Layer pool9 (45)
I0511 17:17:46.524864   333 net.cpp:572] pool9 <- pool8_pool8_0_split_0
I0511 17:17:46.524885   333 net.cpp:542] pool9 -> pool9
I0511 17:17:46.524956   333 net.cpp:260] Setting up pool9
I0511 17:17:46.524976   333 net.cpp:267] TEST Top shape for layer 45 'pool9' 8 512 1 2 (8192)
I0511 17:17:46.525000   333 layer_factory.hpp:172] Creating layer 'ctx_output1' of type 'Convolution'
I0511 17:17:46.525020   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.525051   333 net.cpp:200] Created Layer ctx_output1 (46)
I0511 17:17:46.525072   333 net.cpp:572] ctx_output1 <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0511 17:17:46.525094   333 net.cpp:542] ctx_output1 -> ctx_output1
I0511 17:17:46.526096   333 net.cpp:260] Setting up ctx_output1
I0511 17:17:46.526130   333 net.cpp:267] TEST Top shape for layer 46 'ctx_output1' 8 256 40 96 (7864320)
I0511 17:17:46.526160   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu' of type 'ReLU'
I0511 17:17:46.526181   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526206   333 net.cpp:200] Created Layer ctx_output1/relu (47)
I0511 17:17:46.526226   333 net.cpp:572] ctx_output1/relu <- ctx_output1
I0511 17:17:46.526248   333 net.cpp:527] ctx_output1/relu -> ctx_output1 (in-place)
I0511 17:17:46.526271   333 net.cpp:260] Setting up ctx_output1/relu
I0511 17:17:46.526291   333 net.cpp:267] TEST Top shape for layer 47 'ctx_output1/relu' 8 256 40 96 (7864320)
I0511 17:17:46.526315   333 layer_factory.hpp:172] Creating layer 'ctx_output1_ctx_output1/relu_0_split' of type 'Split'
I0511 17:17:46.526336   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526360   333 net.cpp:200] Created Layer ctx_output1_ctx_output1/relu_0_split (48)
I0511 17:17:46.526379   333 net.cpp:572] ctx_output1_ctx_output1/relu_0_split <- ctx_output1
I0511 17:17:46.526401   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.526423   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.526446   333 net.cpp:542] ctx_output1_ctx_output1/relu_0_split -> ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.526532   333 net.cpp:260] Setting up ctx_output1_ctx_output1/relu_0_split
I0511 17:17:46.526553   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526577   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526599   333 net.cpp:267] TEST Top shape for layer 48 'ctx_output1_ctx_output1/relu_0_split' 8 256 40 96 (7864320)
I0511 17:17:46.526623   333 layer_factory.hpp:172] Creating layer 'ctx_output2' of type 'Convolution'
I0511 17:17:46.526644   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.526674   333 net.cpp:200] Created Layer ctx_output2 (49)
I0511 17:17:46.526695   333 net.cpp:572] ctx_output2 <- res5a_branch2b_res5a_branch2b/relu_0_split_1
I0511 17:17:46.526716   333 net.cpp:542] ctx_output2 -> ctx_output2
I0511 17:17:46.530591   333 net.cpp:260] Setting up ctx_output2
I0511 17:17:46.530633   333 net.cpp:267] TEST Top shape for layer 49 'ctx_output2' 8 256 10 24 (491520)
I0511 17:17:46.530665   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu' of type 'ReLU'
I0511 17:17:46.530686   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.530710   333 net.cpp:200] Created Layer ctx_output2/relu (50)
I0511 17:17:46.530732   333 net.cpp:572] ctx_output2/relu <- ctx_output2
I0511 17:17:46.530755   333 net.cpp:527] ctx_output2/relu -> ctx_output2 (in-place)
I0511 17:17:46.530778   333 net.cpp:260] Setting up ctx_output2/relu
I0511 17:17:46.530798   333 net.cpp:267] TEST Top shape for layer 50 'ctx_output2/relu' 8 256 10 24 (491520)
I0511 17:17:46.530822   333 layer_factory.hpp:172] Creating layer 'ctx_output2_ctx_output2/relu_0_split' of type 'Split'
I0511 17:17:46.530843   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.530866   333 net.cpp:200] Created Layer ctx_output2_ctx_output2/relu_0_split (51)
I0511 17:17:46.530887   333 net.cpp:572] ctx_output2_ctx_output2/relu_0_split <- ctx_output2
I0511 17:17:46.530908   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.530931   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.530953   333 net.cpp:542] ctx_output2_ctx_output2/relu_0_split -> ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.531031   333 net.cpp:260] Setting up ctx_output2_ctx_output2/relu_0_split
I0511 17:17:46.531051   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531075   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531097   333 net.cpp:267] TEST Top shape for layer 51 'ctx_output2_ctx_output2/relu_0_split' 8 256 10 24 (491520)
I0511 17:17:46.531121   333 layer_factory.hpp:172] Creating layer 'ctx_output3' of type 'Convolution'
I0511 17:17:46.531141   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.531172   333 net.cpp:200] Created Layer ctx_output3 (52)
I0511 17:17:46.531193   333 net.cpp:572] ctx_output3 <- pool6_pool6_0_split_1
I0511 17:17:46.531214   333 net.cpp:542] ctx_output3 -> ctx_output3
I0511 17:17:46.534360   333 net.cpp:260] Setting up ctx_output3
I0511 17:17:46.534405   333 net.cpp:267] TEST Top shape for layer 52 'ctx_output3' 8 256 5 12 (122880)
I0511 17:17:46.534446   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu' of type 'ReLU'
I0511 17:17:46.534471   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.534497   333 net.cpp:200] Created Layer ctx_output3/relu (53)
I0511 17:17:46.534521   333 net.cpp:572] ctx_output3/relu <- ctx_output3
I0511 17:17:46.534546   333 net.cpp:527] ctx_output3/relu -> ctx_output3 (in-place)
I0511 17:17:46.534579   333 net.cpp:260] Setting up ctx_output3/relu
I0511 17:17:46.534610   333 net.cpp:267] TEST Top shape for layer 53 'ctx_output3/relu' 8 256 5 12 (122880)
I0511 17:17:46.534637   333 layer_factory.hpp:172] Creating layer 'ctx_output3_ctx_output3/relu_0_split' of type 'Split'
I0511 17:17:46.534660   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.534687   333 net.cpp:200] Created Layer ctx_output3_ctx_output3/relu_0_split (54)
I0511 17:17:46.534710   333 net.cpp:572] ctx_output3_ctx_output3/relu_0_split <- ctx_output3
I0511 17:17:46.534735   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.534761   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.534787   333 net.cpp:542] ctx_output3_ctx_output3/relu_0_split -> ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.534886   333 net.cpp:260] Setting up ctx_output3_ctx_output3/relu_0_split
I0511 17:17:46.534909   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534935   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534962   333 net.cpp:267] TEST Top shape for layer 54 'ctx_output3_ctx_output3/relu_0_split' 8 256 5 12 (122880)
I0511 17:17:46.534989   333 layer_factory.hpp:172] Creating layer 'ctx_output4' of type 'Convolution'
I0511 17:17:46.535012   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.535044   333 net.cpp:200] Created Layer ctx_output4 (55)
I0511 17:17:46.535068   333 net.cpp:572] ctx_output4 <- pool7_pool7_0_split_1
I0511 17:17:46.535094   333 net.cpp:542] ctx_output4 -> ctx_output4
I0511 17:17:46.538358   333 net.cpp:260] Setting up ctx_output4
I0511 17:17:46.538396   333 net.cpp:267] TEST Top shape for layer 55 'ctx_output4' 8 256 3 6 (36864)
I0511 17:17:46.538431   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu' of type 'ReLU'
I0511 17:17:46.538455   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.538481   333 net.cpp:200] Created Layer ctx_output4/relu (56)
I0511 17:17:46.538506   333 net.cpp:572] ctx_output4/relu <- ctx_output4
I0511 17:17:46.538532   333 net.cpp:527] ctx_output4/relu -> ctx_output4 (in-place)
I0511 17:17:46.538558   333 net.cpp:260] Setting up ctx_output4/relu
I0511 17:17:46.538580   333 net.cpp:267] TEST Top shape for layer 56 'ctx_output4/relu' 8 256 3 6 (36864)
I0511 17:17:46.538607   333 layer_factory.hpp:172] Creating layer 'ctx_output4_ctx_output4/relu_0_split' of type 'Split'
I0511 17:17:46.538631   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.538657   333 net.cpp:200] Created Layer ctx_output4_ctx_output4/relu_0_split (57)
I0511 17:17:46.538681   333 net.cpp:572] ctx_output4_ctx_output4/relu_0_split <- ctx_output4
I0511 17:17:46.538705   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.538730   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.538758   333 net.cpp:542] ctx_output4_ctx_output4/relu_0_split -> ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.538846   333 net.cpp:260] Setting up ctx_output4_ctx_output4/relu_0_split
I0511 17:17:46.538869   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538895   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538923   333 net.cpp:267] TEST Top shape for layer 57 'ctx_output4_ctx_output4/relu_0_split' 8 256 3 6 (36864)
I0511 17:17:46.538949   333 layer_factory.hpp:172] Creating layer 'ctx_output5' of type 'Convolution'
I0511 17:17:46.538972   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.539019   333 net.cpp:200] Created Layer ctx_output5 (58)
I0511 17:17:46.539043   333 net.cpp:572] ctx_output5 <- pool8_pool8_0_split_1
I0511 17:17:46.539068   333 net.cpp:542] ctx_output5 -> ctx_output5
I0511 17:17:46.542299   333 net.cpp:260] Setting up ctx_output5
I0511 17:17:46.542335   333 net.cpp:267] TEST Top shape for layer 58 'ctx_output5' 8 256 2 3 (12288)
I0511 17:17:46.542369   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu' of type 'ReLU'
I0511 17:17:46.542393   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542420   333 net.cpp:200] Created Layer ctx_output5/relu (59)
I0511 17:17:46.542444   333 net.cpp:572] ctx_output5/relu <- ctx_output5
I0511 17:17:46.542469   333 net.cpp:527] ctx_output5/relu -> ctx_output5 (in-place)
I0511 17:17:46.542495   333 net.cpp:260] Setting up ctx_output5/relu
I0511 17:17:46.542518   333 net.cpp:267] TEST Top shape for layer 59 'ctx_output5/relu' 8 256 2 3 (12288)
I0511 17:17:46.542546   333 layer_factory.hpp:172] Creating layer 'ctx_output5_ctx_output5/relu_0_split' of type 'Split'
I0511 17:17:46.542569   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542596   333 net.cpp:200] Created Layer ctx_output5_ctx_output5/relu_0_split (60)
I0511 17:17:46.542620   333 net.cpp:572] ctx_output5_ctx_output5/relu_0_split <- ctx_output5
I0511 17:17:46.542644   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.542670   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.542696   333 net.cpp:542] ctx_output5_ctx_output5/relu_0_split -> ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.542780   333 net.cpp:260] Setting up ctx_output5_ctx_output5/relu_0_split
I0511 17:17:46.542804   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542829   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542856   333 net.cpp:267] TEST Top shape for layer 60 'ctx_output5_ctx_output5/relu_0_split' 8 256 2 3 (12288)
I0511 17:17:46.542884   333 layer_factory.hpp:172] Creating layer 'ctx_output6' of type 'Convolution'
I0511 17:17:46.542907   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.542940   333 net.cpp:200] Created Layer ctx_output6 (61)
I0511 17:17:46.542964   333 net.cpp:572] ctx_output6 <- pool9
I0511 17:17:46.542989   333 net.cpp:542] ctx_output6 -> ctx_output6
I0511 17:17:46.547009   333 net.cpp:260] Setting up ctx_output6
I0511 17:17:46.547047   333 net.cpp:267] TEST Top shape for layer 61 'ctx_output6' 8 256 1 2 (4096)
I0511 17:17:46.547078   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu' of type 'ReLU'
I0511 17:17:46.547101   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547124   333 net.cpp:200] Created Layer ctx_output6/relu (62)
I0511 17:17:46.547147   333 net.cpp:572] ctx_output6/relu <- ctx_output6
I0511 17:17:46.547168   333 net.cpp:527] ctx_output6/relu -> ctx_output6 (in-place)
I0511 17:17:46.547192   333 net.cpp:260] Setting up ctx_output6/relu
I0511 17:17:46.547211   333 net.cpp:267] TEST Top shape for layer 62 'ctx_output6/relu' 8 256 1 2 (4096)
I0511 17:17:46.547236   333 layer_factory.hpp:172] Creating layer 'ctx_output6_ctx_output6/relu_0_split' of type 'Split'
I0511 17:17:46.547257   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547281   333 net.cpp:200] Created Layer ctx_output6_ctx_output6/relu_0_split (63)
I0511 17:17:46.547302   333 net.cpp:572] ctx_output6_ctx_output6/relu_0_split <- ctx_output6
I0511 17:17:46.547322   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.547350   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.547381   333 net.cpp:542] ctx_output6_ctx_output6/relu_0_split -> ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.547459   333 net.cpp:260] Setting up ctx_output6_ctx_output6/relu_0_split
I0511 17:17:46.547480   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547503   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547526   333 net.cpp:267] TEST Top shape for layer 63 'ctx_output6_ctx_output6/relu_0_split' 8 256 1 2 (4096)
I0511 17:17:46.547550   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.547570   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.547602   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc (64)
I0511 17:17:46.547623   333 net.cpp:572] ctx_output1/relu_mbox_loc <- ctx_output1_ctx_output1/relu_0_split_0
I0511 17:17:46.547646   333 net.cpp:542] ctx_output1/relu_mbox_loc -> ctx_output1/relu_mbox_loc
I0511 17:17:46.548056   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc
I0511 17:17:46.548084   333 net.cpp:267] TEST Top shape for layer 64 'ctx_output1/relu_mbox_loc' 8 16 40 96 (491520)
I0511 17:17:46.548115   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.548135   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.548161   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_perm (65)
I0511 17:17:46.548182   333 net.cpp:572] ctx_output1/relu_mbox_loc_perm <- ctx_output1/relu_mbox_loc
I0511 17:17:46.548205   333 net.cpp:542] ctx_output1/relu_mbox_loc_perm -> ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548341   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548363   333 net.cpp:267] TEST Top shape for layer 65 'ctx_output1/relu_mbox_loc_perm' 8 40 96 16 (491520)
I0511 17:17:46.548388   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.548408   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.548432   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_loc_flat (66)
I0511 17:17:46.548454   333 net.cpp:572] ctx_output1/relu_mbox_loc_flat <- ctx_output1/relu_mbox_loc_perm
I0511 17:17:46.548475   333 net.cpp:542] ctx_output1/relu_mbox_loc_flat -> ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.550631   333 net.cpp:260] Setting up ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.550668   333 net.cpp:267] TEST Top shape for layer 66 'ctx_output1/relu_mbox_loc_flat' 8 61440 (491520)
I0511 17:17:46.550696   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.550717   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.550750   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf (67)
I0511 17:17:46.550771   333 net.cpp:572] ctx_output1/relu_mbox_conf <- ctx_output1_ctx_output1/relu_0_split_1
I0511 17:17:46.550794   333 net.cpp:542] ctx_output1/relu_mbox_conf -> ctx_output1/relu_mbox_conf
I0511 17:17:46.551251   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf
I0511 17:17:46.551276   333 net.cpp:267] TEST Top shape for layer 67 'ctx_output1/relu_mbox_conf' 8 16 40 96 (491520)
I0511 17:17:46.551306   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.551327   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.551352   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_perm (68)
I0511 17:17:46.551373   333 net.cpp:572] ctx_output1/relu_mbox_conf_perm <- ctx_output1/relu_mbox_conf
I0511 17:17:46.551395   333 net.cpp:542] ctx_output1/relu_mbox_conf_perm -> ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551535   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551558   333 net.cpp:267] TEST Top shape for layer 68 'ctx_output1/relu_mbox_conf_perm' 8 40 96 16 (491520)
I0511 17:17:46.551582   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.551602   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.551626   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_conf_flat (69)
I0511 17:17:46.551646   333 net.cpp:572] ctx_output1/relu_mbox_conf_flat <- ctx_output1/relu_mbox_conf_perm
I0511 17:17:46.551668   333 net.cpp:542] ctx_output1/relu_mbox_conf_flat -> ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.553830   333 net.cpp:260] Setting up ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.553871   333 net.cpp:267] TEST Top shape for layer 69 'ctx_output1/relu_mbox_conf_flat' 8 61440 (491520)
I0511 17:17:46.553903   333 layer_factory.hpp:172] Creating layer 'ctx_output1/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.553927   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.553958   333 net.cpp:200] Created Layer ctx_output1/relu_mbox_priorbox (70)
I0511 17:17:46.553983   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- ctx_output1_ctx_output1/relu_0_split_2
I0511 17:17:46.554011   333 net.cpp:572] ctx_output1/relu_mbox_priorbox <- data_data_0_split_1
I0511 17:17:46.554039   333 net.cpp:542] ctx_output1/relu_mbox_priorbox -> ctx_output1/relu_mbox_priorbox
I0511 17:17:46.554108   333 net.cpp:260] Setting up ctx_output1/relu_mbox_priorbox
I0511 17:17:46.554131   333 net.cpp:267] TEST Top shape for layer 70 'ctx_output1/relu_mbox_priorbox' 1 2 61440 (122880)
I0511 17:17:46.554159   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.554183   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.554217   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc (71)
I0511 17:17:46.554241   333 net.cpp:572] ctx_output2/relu_mbox_loc <- ctx_output2_ctx_output2/relu_0_split_0
I0511 17:17:46.554266   333 net.cpp:542] ctx_output2/relu_mbox_loc -> ctx_output2/relu_mbox_loc
I0511 17:17:46.554800   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc
I0511 17:17:46.554829   333 net.cpp:267] TEST Top shape for layer 71 'ctx_output2/relu_mbox_loc' 8 24 10 24 (46080)
I0511 17:17:46.554862   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.554885   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.554914   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_perm (72)
I0511 17:17:46.554939   333 net.cpp:572] ctx_output2/relu_mbox_loc_perm <- ctx_output2/relu_mbox_loc
I0511 17:17:46.554963   333 net.cpp:542] ctx_output2/relu_mbox_loc_perm -> ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555125   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555152   333 net.cpp:267] TEST Top shape for layer 72 'ctx_output2/relu_mbox_loc_perm' 8 10 24 24 (46080)
I0511 17:17:46.555181   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.555203   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.555230   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_loc_flat (73)
I0511 17:17:46.555254   333 net.cpp:572] ctx_output2/relu_mbox_loc_flat <- ctx_output2/relu_mbox_loc_perm
I0511 17:17:46.555279   333 net.cpp:542] ctx_output2/relu_mbox_loc_flat -> ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.555392   333 net.cpp:260] Setting up ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.555418   333 net.cpp:267] TEST Top shape for layer 73 'ctx_output2/relu_mbox_loc_flat' 8 5760 (46080)
I0511 17:17:46.555444   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.555486   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.555519   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf (74)
I0511 17:17:46.555543   333 net.cpp:572] ctx_output2/relu_mbox_conf <- ctx_output2_ctx_output2/relu_0_split_1
I0511 17:17:46.555568   333 net.cpp:542] ctx_output2/relu_mbox_conf -> ctx_output2/relu_mbox_conf
I0511 17:17:46.556067   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf
I0511 17:17:46.556095   333 net.cpp:267] TEST Top shape for layer 74 'ctx_output2/relu_mbox_conf' 8 24 10 24 (46080)
I0511 17:17:46.556128   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.556151   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556180   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_perm (75)
I0511 17:17:46.556202   333 net.cpp:572] ctx_output2/relu_mbox_conf_perm <- ctx_output2/relu_mbox_conf
I0511 17:17:46.556228   333 net.cpp:542] ctx_output2/relu_mbox_conf_perm -> ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556390   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556414   333 net.cpp:267] TEST Top shape for layer 75 'ctx_output2/relu_mbox_conf_perm' 8 10 24 24 (46080)
I0511 17:17:46.556442   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.556465   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556491   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_conf_flat (76)
I0511 17:17:46.556515   333 net.cpp:572] ctx_output2/relu_mbox_conf_flat <- ctx_output2/relu_mbox_conf_perm
I0511 17:17:46.556540   333 net.cpp:542] ctx_output2/relu_mbox_conf_flat -> ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.556643   333 net.cpp:260] Setting up ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.556668   333 net.cpp:267] TEST Top shape for layer 76 'ctx_output2/relu_mbox_conf_flat' 8 5760 (46080)
I0511 17:17:46.556695   333 layer_factory.hpp:172] Creating layer 'ctx_output2/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.556718   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556747   333 net.cpp:200] Created Layer ctx_output2/relu_mbox_priorbox (77)
I0511 17:17:46.556771   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- ctx_output2_ctx_output2/relu_0_split_2
I0511 17:17:46.556797   333 net.cpp:572] ctx_output2/relu_mbox_priorbox <- data_data_0_split_2
I0511 17:17:46.556823   333 net.cpp:542] ctx_output2/relu_mbox_priorbox -> ctx_output2/relu_mbox_priorbox
I0511 17:17:46.556877   333 net.cpp:260] Setting up ctx_output2/relu_mbox_priorbox
I0511 17:17:46.556900   333 net.cpp:267] TEST Top shape for layer 77 'ctx_output2/relu_mbox_priorbox' 1 2 5760 (11520)
I0511 17:17:46.556927   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.556951   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.556982   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc (78)
I0511 17:17:46.557006   333 net.cpp:572] ctx_output3/relu_mbox_loc <- ctx_output3_ctx_output3/relu_0_split_0
I0511 17:17:46.557031   333 net.cpp:542] ctx_output3/relu_mbox_loc -> ctx_output3/relu_mbox_loc
I0511 17:17:46.557551   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc
I0511 17:17:46.557581   333 net.cpp:267] TEST Top shape for layer 78 'ctx_output3/relu_mbox_loc' 8 24 5 12 (11520)
I0511 17:17:46.557613   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.557636   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.557664   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_perm (79)
I0511 17:17:46.557688   333 net.cpp:572] ctx_output3/relu_mbox_loc_perm <- ctx_output3/relu_mbox_loc
I0511 17:17:46.557716   333 net.cpp:542] ctx_output3/relu_mbox_loc_perm -> ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.557881   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.557907   333 net.cpp:267] TEST Top shape for layer 79 'ctx_output3/relu_mbox_loc_perm' 8 5 12 24 (11520)
I0511 17:17:46.557935   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.557958   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.557984   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_loc_flat (80)
I0511 17:17:46.558007   333 net.cpp:572] ctx_output3/relu_mbox_loc_flat <- ctx_output3/relu_mbox_loc_perm
I0511 17:17:46.558032   333 net.cpp:542] ctx_output3/relu_mbox_loc_flat -> ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.558126   333 net.cpp:260] Setting up ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.558156   333 net.cpp:267] TEST Top shape for layer 80 'ctx_output3/relu_mbox_loc_flat' 8 1440 (11520)
I0511 17:17:46.558182   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.558205   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.558238   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf (81)
I0511 17:17:46.558261   333 net.cpp:572] ctx_output3/relu_mbox_conf <- ctx_output3_ctx_output3/relu_0_split_1
I0511 17:17:46.558286   333 net.cpp:542] ctx_output3/relu_mbox_conf -> ctx_output3/relu_mbox_conf
I0511 17:17:46.558775   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf
I0511 17:17:46.558804   333 net.cpp:267] TEST Top shape for layer 81 'ctx_output3/relu_mbox_conf' 8 24 5 12 (11520)
I0511 17:17:46.558835   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.558858   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.558887   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_perm (82)
I0511 17:17:46.558910   333 net.cpp:572] ctx_output3/relu_mbox_conf_perm <- ctx_output3/relu_mbox_conf
I0511 17:17:46.558934   333 net.cpp:542] ctx_output3/relu_mbox_conf_perm -> ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559088   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559113   333 net.cpp:267] TEST Top shape for layer 82 'ctx_output3/relu_mbox_conf_perm' 8 5 12 24 (11520)
I0511 17:17:46.559141   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.559165   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559190   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_conf_flat (83)
I0511 17:17:46.559214   333 net.cpp:572] ctx_output3/relu_mbox_conf_flat <- ctx_output3/relu_mbox_conf_perm
I0511 17:17:46.559238   333 net.cpp:542] ctx_output3/relu_mbox_conf_flat -> ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.559329   333 net.cpp:260] Setting up ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.559352   333 net.cpp:267] TEST Top shape for layer 83 'ctx_output3/relu_mbox_conf_flat' 8 1440 (11520)
I0511 17:17:46.559378   333 layer_factory.hpp:172] Creating layer 'ctx_output3/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.559401   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559429   333 net.cpp:200] Created Layer ctx_output3/relu_mbox_priorbox (84)
I0511 17:17:46.559453   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- ctx_output3_ctx_output3/relu_0_split_2
I0511 17:17:46.559478   333 net.cpp:572] ctx_output3/relu_mbox_priorbox <- data_data_0_split_3
I0511 17:17:46.559504   333 net.cpp:542] ctx_output3/relu_mbox_priorbox -> ctx_output3/relu_mbox_priorbox
I0511 17:17:46.559553   333 net.cpp:260] Setting up ctx_output3/relu_mbox_priorbox
I0511 17:17:46.559576   333 net.cpp:267] TEST Top shape for layer 84 'ctx_output3/relu_mbox_priorbox' 1 2 1440 (2880)
I0511 17:17:46.559617   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.559639   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.559671   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc (85)
I0511 17:17:46.559695   333 net.cpp:572] ctx_output4/relu_mbox_loc <- ctx_output4_ctx_output4/relu_0_split_0
I0511 17:17:46.559720   333 net.cpp:542] ctx_output4/relu_mbox_loc -> ctx_output4/relu_mbox_loc
I0511 17:17:46.560217   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc
I0511 17:17:46.560245   333 net.cpp:267] TEST Top shape for layer 85 'ctx_output4/relu_mbox_loc' 8 24 3 6 (3456)
I0511 17:17:46.560276   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.560299   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560328   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_perm (86)
I0511 17:17:46.560351   333 net.cpp:572] ctx_output4/relu_mbox_loc_perm <- ctx_output4/relu_mbox_loc
I0511 17:17:46.560375   333 net.cpp:542] ctx_output4/relu_mbox_loc_perm -> ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560540   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560565   333 net.cpp:267] TEST Top shape for layer 86 'ctx_output4/relu_mbox_loc_perm' 8 3 6 24 (3456)
I0511 17:17:46.560592   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.560616   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560642   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_loc_flat (87)
I0511 17:17:46.560665   333 net.cpp:572] ctx_output4/relu_mbox_loc_flat <- ctx_output4/relu_mbox_loc_perm
I0511 17:17:46.560690   333 net.cpp:542] ctx_output4/relu_mbox_loc_flat -> ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.560781   333 net.cpp:260] Setting up ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.560811   333 net.cpp:267] TEST Top shape for layer 87 'ctx_output4/relu_mbox_loc_flat' 8 432 (3456)
I0511 17:17:46.560837   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.560859   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.560892   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf (88)
I0511 17:17:46.560916   333 net.cpp:572] ctx_output4/relu_mbox_conf <- ctx_output4_ctx_output4/relu_0_split_1
I0511 17:17:46.560941   333 net.cpp:542] ctx_output4/relu_mbox_conf -> ctx_output4/relu_mbox_conf
I0511 17:17:46.561448   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf
I0511 17:17:46.561478   333 net.cpp:267] TEST Top shape for layer 88 'ctx_output4/relu_mbox_conf' 8 24 3 6 (3456)
I0511 17:17:46.561509   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.561533   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.561560   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_perm (89)
I0511 17:17:46.561584   333 net.cpp:572] ctx_output4/relu_mbox_conf_perm <- ctx_output4/relu_mbox_conf
I0511 17:17:46.561609   333 net.cpp:542] ctx_output4/relu_mbox_conf_perm -> ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561762   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561789   333 net.cpp:267] TEST Top shape for layer 89 'ctx_output4/relu_mbox_conf_perm' 8 3 6 24 (3456)
I0511 17:17:46.561815   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.561838   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.561861   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_conf_flat (90)
I0511 17:17:46.561882   333 net.cpp:572] ctx_output4/relu_mbox_conf_flat <- ctx_output4/relu_mbox_conf_perm
I0511 17:17:46.561908   333 net.cpp:542] ctx_output4/relu_mbox_conf_flat -> ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.561991   333 net.cpp:260] Setting up ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.562011   333 net.cpp:267] TEST Top shape for layer 90 'ctx_output4/relu_mbox_conf_flat' 8 432 (3456)
I0511 17:17:46.562034   333 layer_factory.hpp:172] Creating layer 'ctx_output4/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.562055   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562080   333 net.cpp:200] Created Layer ctx_output4/relu_mbox_priorbox (91)
I0511 17:17:46.562101   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- ctx_output4_ctx_output4/relu_0_split_2
I0511 17:17:46.562124   333 net.cpp:572] ctx_output4/relu_mbox_priorbox <- data_data_0_split_4
I0511 17:17:46.562145   333 net.cpp:542] ctx_output4/relu_mbox_priorbox -> ctx_output4/relu_mbox_priorbox
I0511 17:17:46.562189   333 net.cpp:260] Setting up ctx_output4/relu_mbox_priorbox
I0511 17:17:46.562211   333 net.cpp:267] TEST Top shape for layer 91 'ctx_output4/relu_mbox_priorbox' 1 2 432 (864)
I0511 17:17:46.562233   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.562254   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562283   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc (92)
I0511 17:17:46.562304   333 net.cpp:572] ctx_output5/relu_mbox_loc <- ctx_output5_ctx_output5/relu_0_split_0
I0511 17:17:46.562325   333 net.cpp:542] ctx_output5/relu_mbox_loc -> ctx_output5/relu_mbox_loc
I0511 17:17:46.562747   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc
I0511 17:17:46.562772   333 net.cpp:267] TEST Top shape for layer 92 'ctx_output5/relu_mbox_loc' 8 16 2 3 (768)
I0511 17:17:46.562801   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.562821   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.562846   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_perm (93)
I0511 17:17:46.562867   333 net.cpp:572] ctx_output5/relu_mbox_loc_perm <- ctx_output5/relu_mbox_loc
I0511 17:17:46.562889   333 net.cpp:542] ctx_output5/relu_mbox_loc_perm -> ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563027   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563050   333 net.cpp:267] TEST Top shape for layer 93 'ctx_output5/relu_mbox_loc_perm' 8 2 3 16 (768)
I0511 17:17:46.563073   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.563093   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563117   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_loc_flat (94)
I0511 17:17:46.563138   333 net.cpp:572] ctx_output5/relu_mbox_loc_flat <- ctx_output5/relu_mbox_loc_perm
I0511 17:17:46.563159   333 net.cpp:542] ctx_output5/relu_mbox_loc_flat -> ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.563238   333 net.cpp:260] Setting up ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.563259   333 net.cpp:267] TEST Top shape for layer 94 'ctx_output5/relu_mbox_loc_flat' 8 96 (768)
I0511 17:17:46.563283   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.563304   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563333   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf (95)
I0511 17:17:46.563354   333 net.cpp:572] ctx_output5/relu_mbox_conf <- ctx_output5_ctx_output5/relu_0_split_1
I0511 17:17:46.563376   333 net.cpp:542] ctx_output5/relu_mbox_conf -> ctx_output5/relu_mbox_conf
I0511 17:17:46.563789   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf
I0511 17:17:46.563815   333 net.cpp:267] TEST Top shape for layer 95 'ctx_output5/relu_mbox_conf' 8 16 2 3 (768)
I0511 17:17:46.563848   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.563879   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.563905   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_perm (96)
I0511 17:17:46.563925   333 net.cpp:572] ctx_output5/relu_mbox_conf_perm <- ctx_output5/relu_mbox_conf
I0511 17:17:46.563948   333 net.cpp:542] ctx_output5/relu_mbox_conf_perm -> ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564083   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564106   333 net.cpp:267] TEST Top shape for layer 96 'ctx_output5/relu_mbox_conf_perm' 8 2 3 16 (768)
I0511 17:17:46.564129   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.564150   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564173   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_conf_flat (97)
I0511 17:17:46.564194   333 net.cpp:572] ctx_output5/relu_mbox_conf_flat <- ctx_output5/relu_mbox_conf_perm
I0511 17:17:46.564216   333 net.cpp:542] ctx_output5/relu_mbox_conf_flat -> ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.564292   333 net.cpp:260] Setting up ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.564312   333 net.cpp:267] TEST Top shape for layer 97 'ctx_output5/relu_mbox_conf_flat' 8 96 (768)
I0511 17:17:46.564335   333 layer_factory.hpp:172] Creating layer 'ctx_output5/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.564357   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564380   333 net.cpp:200] Created Layer ctx_output5/relu_mbox_priorbox (98)
I0511 17:17:46.564401   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- ctx_output5_ctx_output5/relu_0_split_2
I0511 17:17:46.564424   333 net.cpp:572] ctx_output5/relu_mbox_priorbox <- data_data_0_split_5
I0511 17:17:46.564445   333 net.cpp:542] ctx_output5/relu_mbox_priorbox -> ctx_output5/relu_mbox_priorbox
I0511 17:17:46.564482   333 net.cpp:260] Setting up ctx_output5/relu_mbox_priorbox
I0511 17:17:46.564502   333 net.cpp:267] TEST Top shape for layer 98 'ctx_output5/relu_mbox_priorbox' 1 2 96 (192)
I0511 17:17:46.564525   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc' of type 'Convolution'
I0511 17:17:46.564545   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.564574   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc (99)
I0511 17:17:46.564596   333 net.cpp:572] ctx_output6/relu_mbox_loc <- ctx_output6_ctx_output6/relu_0_split_0
I0511 17:17:46.564617   333 net.cpp:542] ctx_output6/relu_mbox_loc -> ctx_output6/relu_mbox_loc
I0511 17:17:46.565035   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc
I0511 17:17:46.565060   333 net.cpp:267] TEST Top shape for layer 99 'ctx_output6/relu_mbox_loc' 8 16 1 2 (256)
I0511 17:17:46.565090   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_perm' of type 'Permute'
I0511 17:17:46.565110   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565136   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_perm (100)
I0511 17:17:46.565157   333 net.cpp:572] ctx_output6/relu_mbox_loc_perm <- ctx_output6/relu_mbox_loc
I0511 17:17:46.565179   333 net.cpp:542] ctx_output6/relu_mbox_loc_perm -> ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565322   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565346   333 net.cpp:267] TEST Top shape for layer 100 'ctx_output6/relu_mbox_loc_perm' 8 1 2 16 (256)
I0511 17:17:46.565369   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_loc_flat' of type 'Flatten'
I0511 17:17:46.565390   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565413   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_loc_flat (101)
I0511 17:17:46.565439   333 net.cpp:572] ctx_output6/relu_mbox_loc_flat <- ctx_output6/relu_mbox_loc_perm
I0511 17:17:46.565469   333 net.cpp:542] ctx_output6/relu_mbox_loc_flat -> ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.565544   333 net.cpp:260] Setting up ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.565565   333 net.cpp:267] TEST Top shape for layer 101 'ctx_output6/relu_mbox_loc_flat' 8 32 (256)
I0511 17:17:46.565588   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf' of type 'Convolution'
I0511 17:17:46.565609   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.565639   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf (102)
I0511 17:17:46.565659   333 net.cpp:572] ctx_output6/relu_mbox_conf <- ctx_output6_ctx_output6/relu_0_split_1
I0511 17:17:46.565680   333 net.cpp:542] ctx_output6/relu_mbox_conf -> ctx_output6/relu_mbox_conf
I0511 17:17:46.566083   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf
I0511 17:17:46.566110   333 net.cpp:267] TEST Top shape for layer 102 'ctx_output6/relu_mbox_conf' 8 16 1 2 (256)
I0511 17:17:46.566138   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_perm' of type 'Permute'
I0511 17:17:46.566159   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566184   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_perm (103)
I0511 17:17:46.566205   333 net.cpp:572] ctx_output6/relu_mbox_conf_perm <- ctx_output6/relu_mbox_conf
I0511 17:17:46.566227   333 net.cpp:542] ctx_output6/relu_mbox_conf_perm -> ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566365   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566388   333 net.cpp:267] TEST Top shape for layer 103 'ctx_output6/relu_mbox_conf_perm' 8 1 2 16 (256)
I0511 17:17:46.566411   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_conf_flat' of type 'Flatten'
I0511 17:17:46.566431   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566455   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_conf_flat (104)
I0511 17:17:46.566475   333 net.cpp:572] ctx_output6/relu_mbox_conf_flat <- ctx_output6/relu_mbox_conf_perm
I0511 17:17:46.566498   333 net.cpp:542] ctx_output6/relu_mbox_conf_flat -> ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.566574   333 net.cpp:260] Setting up ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.566596   333 net.cpp:267] TEST Top shape for layer 104 'ctx_output6/relu_mbox_conf_flat' 8 32 (256)
I0511 17:17:46.566618   333 layer_factory.hpp:172] Creating layer 'ctx_output6/relu_mbox_priorbox' of type 'PriorBox'
I0511 17:17:46.566638   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566663   333 net.cpp:200] Created Layer ctx_output6/relu_mbox_priorbox (105)
I0511 17:17:46.566684   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- ctx_output6_ctx_output6/relu_0_split_2
I0511 17:17:46.566706   333 net.cpp:572] ctx_output6/relu_mbox_priorbox <- data_data_0_split_6
I0511 17:17:46.566728   333 net.cpp:542] ctx_output6/relu_mbox_priorbox -> ctx_output6/relu_mbox_priorbox
I0511 17:17:46.566764   333 net.cpp:260] Setting up ctx_output6/relu_mbox_priorbox
I0511 17:17:46.566784   333 net.cpp:267] TEST Top shape for layer 105 'ctx_output6/relu_mbox_priorbox' 1 2 32 (64)
I0511 17:17:46.566807   333 layer_factory.hpp:172] Creating layer 'mbox_loc' of type 'Concat'
I0511 17:17:46.566828   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.566851   333 net.cpp:200] Created Layer mbox_loc (106)
I0511 17:17:46.566874   333 net.cpp:572] mbox_loc <- ctx_output1/relu_mbox_loc_flat
I0511 17:17:46.566895   333 net.cpp:572] mbox_loc <- ctx_output2/relu_mbox_loc_flat
I0511 17:17:46.566917   333 net.cpp:572] mbox_loc <- ctx_output3/relu_mbox_loc_flat
I0511 17:17:46.566938   333 net.cpp:572] mbox_loc <- ctx_output4/relu_mbox_loc_flat
I0511 17:17:46.566965   333 net.cpp:572] mbox_loc <- ctx_output5/relu_mbox_loc_flat
I0511 17:17:46.566987   333 net.cpp:572] mbox_loc <- ctx_output6/relu_mbox_loc_flat
I0511 17:17:46.567016   333 net.cpp:542] mbox_loc -> mbox_loc
I0511 17:17:46.567059   333 net.cpp:260] Setting up mbox_loc
I0511 17:17:46.567078   333 net.cpp:267] TEST Top shape for layer 106 'mbox_loc' 8 69200 (553600)
I0511 17:17:46.567102   333 layer_factory.hpp:172] Creating layer 'mbox_conf' of type 'Concat'
I0511 17:17:46.567123   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567147   333 net.cpp:200] Created Layer mbox_conf (107)
I0511 17:17:46.567167   333 net.cpp:572] mbox_conf <- ctx_output1/relu_mbox_conf_flat
I0511 17:17:46.567189   333 net.cpp:572] mbox_conf <- ctx_output2/relu_mbox_conf_flat
I0511 17:17:46.567210   333 net.cpp:572] mbox_conf <- ctx_output3/relu_mbox_conf_flat
I0511 17:17:46.567232   333 net.cpp:572] mbox_conf <- ctx_output4/relu_mbox_conf_flat
I0511 17:17:46.567255   333 net.cpp:572] mbox_conf <- ctx_output5/relu_mbox_conf_flat
I0511 17:17:46.567276   333 net.cpp:572] mbox_conf <- ctx_output6/relu_mbox_conf_flat
I0511 17:17:46.567296   333 net.cpp:542] mbox_conf -> mbox_conf
I0511 17:17:46.567337   333 net.cpp:260] Setting up mbox_conf
I0511 17:17:46.567358   333 net.cpp:267] TEST Top shape for layer 107 'mbox_conf' 8 69200 (553600)
I0511 17:17:46.567382   333 layer_factory.hpp:172] Creating layer 'mbox_priorbox' of type 'Concat'
I0511 17:17:46.567402   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567425   333 net.cpp:200] Created Layer mbox_priorbox (108)
I0511 17:17:46.567446   333 net.cpp:572] mbox_priorbox <- ctx_output1/relu_mbox_priorbox
I0511 17:17:46.567468   333 net.cpp:572] mbox_priorbox <- ctx_output2/relu_mbox_priorbox
I0511 17:17:46.567489   333 net.cpp:572] mbox_priorbox <- ctx_output3/relu_mbox_priorbox
I0511 17:17:46.567510   333 net.cpp:572] mbox_priorbox <- ctx_output4/relu_mbox_priorbox
I0511 17:17:46.567531   333 net.cpp:572] mbox_priorbox <- ctx_output5/relu_mbox_priorbox
I0511 17:17:46.567553   333 net.cpp:572] mbox_priorbox <- ctx_output6/relu_mbox_priorbox
I0511 17:17:46.567574   333 net.cpp:542] mbox_priorbox -> mbox_priorbox
I0511 17:17:46.567615   333 net.cpp:260] Setting up mbox_priorbox
I0511 17:17:46.567634   333 net.cpp:267] TEST Top shape for layer 108 'mbox_priorbox' 1 2 69200 (138400)
I0511 17:17:46.567658   333 layer_factory.hpp:172] Creating layer 'mbox_conf_reshape' of type 'Reshape'
I0511 17:17:46.567678   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567706   333 net.cpp:200] Created Layer mbox_conf_reshape (109)
I0511 17:17:46.567728   333 net.cpp:572] mbox_conf_reshape <- mbox_conf
I0511 17:17:46.567749   333 net.cpp:542] mbox_conf_reshape -> mbox_conf_reshape
I0511 17:17:46.567791   333 net.cpp:260] Setting up mbox_conf_reshape
I0511 17:17:46.567811   333 net.cpp:267] TEST Top shape for layer 109 'mbox_conf_reshape' 8 17300 4 (553600)
I0511 17:17:46.567836   333 layer_factory.hpp:172] Creating layer 'mbox_conf_softmax' of type 'Softmax'
I0511 17:17:46.567857   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.567885   333 net.cpp:200] Created Layer mbox_conf_softmax (110)
I0511 17:17:46.567906   333 net.cpp:572] mbox_conf_softmax <- mbox_conf_reshape
I0511 17:17:46.567927   333 net.cpp:542] mbox_conf_softmax -> mbox_conf_softmax
I0511 17:17:46.568020   333 net.cpp:260] Setting up mbox_conf_softmax
I0511 17:17:46.568040   333 net.cpp:267] TEST Top shape for layer 110 'mbox_conf_softmax' 8 17300 4 (553600)
I0511 17:17:46.568064   333 layer_factory.hpp:172] Creating layer 'mbox_conf_flatten' of type 'Flatten'
I0511 17:17:46.568085   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.568109   333 net.cpp:200] Created Layer mbox_conf_flatten (111)
I0511 17:17:46.568128   333 net.cpp:572] mbox_conf_flatten <- mbox_conf_softmax
I0511 17:17:46.568154   333 net.cpp:542] mbox_conf_flatten -> mbox_conf_flatten
I0511 17:17:46.570369   333 net.cpp:260] Setting up mbox_conf_flatten
I0511 17:17:46.570405   333 net.cpp:267] TEST Top shape for layer 111 'mbox_conf_flatten' 8 69200 (553600)
I0511 17:17:46.570435   333 layer_factory.hpp:172] Creating layer 'detection_out' of type 'DetectionOutput'
I0511 17:17:46.570456   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.570502   333 net.cpp:200] Created Layer detection_out (112)
I0511 17:17:46.570523   333 net.cpp:572] detection_out <- mbox_loc
I0511 17:17:46.570546   333 net.cpp:572] detection_out <- mbox_conf_flatten
I0511 17:17:46.570569   333 net.cpp:572] detection_out <- mbox_priorbox
I0511 17:17:46.570590   333 net.cpp:542] detection_out -> detection_out
I0511 17:17:46.571322   333 net.cpp:260] Setting up detection_out
I0511 17:17:46.571349   333 net.cpp:267] TEST Top shape for layer 112 'detection_out' 1 1 1 7 (7)
I0511 17:17:46.571374   333 layer_factory.hpp:172] Creating layer 'detection_eval' of type 'DetectionEvaluate'
I0511 17:17:46.571395   333 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0511 17:17:46.571424   333 net.cpp:200] Created Layer detection_eval (113)
I0511 17:17:46.571444   333 net.cpp:572] detection_eval <- detection_out
I0511 17:17:46.571466   333 net.cpp:572] detection_eval <- label
I0511 17:17:46.571489   333 net.cpp:542] detection_eval -> detection_eval
I0511 17:17:46.572026   333 net.cpp:260] Setting up detection_eval
I0511 17:17:46.572050   333 net.cpp:267] TEST Top shape for layer 113 'detection_eval' 1 1 4 5 (20)
I0511 17:17:46.572074   333 net.cpp:338] detection_eval does not need backward computation.
I0511 17:17:46.572096   333 net.cpp:338] detection_out does not need backward computation.
I0511 17:17:46.572118   333 net.cpp:338] mbox_conf_flatten does not need backward computation.
I0511 17:17:46.572139   333 net.cpp:338] mbox_conf_softmax does not need backward computation.
I0511 17:17:46.572161   333 net.cpp:338] mbox_conf_reshape does not need backward computation.
I0511 17:17:46.572182   333 net.cpp:338] mbox_priorbox does not need backward computation.
I0511 17:17:46.572206   333 net.cpp:338] mbox_conf does not need backward computation.
I0511 17:17:46.572229   333 net.cpp:338] mbox_loc does not need backward computation.
I0511 17:17:46.572252   333 net.cpp:338] ctx_output6/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572274   333 net.cpp:338] ctx_output6/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572295   333 net.cpp:338] ctx_output6/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572316   333 net.cpp:338] ctx_output6/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572337   333 net.cpp:338] ctx_output6/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572358   333 net.cpp:338] ctx_output6/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572379   333 net.cpp:338] ctx_output6/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572401   333 net.cpp:338] ctx_output5/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572422   333 net.cpp:338] ctx_output5/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572443   333 net.cpp:338] ctx_output5/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572464   333 net.cpp:338] ctx_output5/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572486   333 net.cpp:338] ctx_output5/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572507   333 net.cpp:338] ctx_output5/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572528   333 net.cpp:338] ctx_output5/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572549   333 net.cpp:338] ctx_output4/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572571   333 net.cpp:338] ctx_output4/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572597   333 net.cpp:338] ctx_output4/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572626   333 net.cpp:338] ctx_output4/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572647   333 net.cpp:338] ctx_output4/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572669   333 net.cpp:338] ctx_output4/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572690   333 net.cpp:338] ctx_output4/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572711   333 net.cpp:338] ctx_output3/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572732   333 net.cpp:338] ctx_output3/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572753   333 net.cpp:338] ctx_output3/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572774   333 net.cpp:338] ctx_output3/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572795   333 net.cpp:338] ctx_output3/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572816   333 net.cpp:338] ctx_output3/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572839   333 net.cpp:338] ctx_output3/relu_mbox_loc does not need backward computation.
I0511 17:17:46.572860   333 net.cpp:338] ctx_output2/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.572881   333 net.cpp:338] ctx_output2/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.572902   333 net.cpp:338] ctx_output2/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.572923   333 net.cpp:338] ctx_output2/relu_mbox_conf does not need backward computation.
I0511 17:17:46.572944   333 net.cpp:338] ctx_output2/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.572965   333 net.cpp:338] ctx_output2/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.572986   333 net.cpp:338] ctx_output2/relu_mbox_loc does not need backward computation.
I0511 17:17:46.573007   333 net.cpp:338] ctx_output1/relu_mbox_priorbox does not need backward computation.
I0511 17:17:46.573029   333 net.cpp:338] ctx_output1/relu_mbox_conf_flat does not need backward computation.
I0511 17:17:46.573050   333 net.cpp:338] ctx_output1/relu_mbox_conf_perm does not need backward computation.
I0511 17:17:46.573071   333 net.cpp:338] ctx_output1/relu_mbox_conf does not need backward computation.
I0511 17:17:46.573092   333 net.cpp:338] ctx_output1/relu_mbox_loc_flat does not need backward computation.
I0511 17:17:46.573113   333 net.cpp:338] ctx_output1/relu_mbox_loc_perm does not need backward computation.
I0511 17:17:46.573134   333 net.cpp:338] ctx_output1/relu_mbox_loc does not need backward computation.
I0511 17:17:46.573156   333 net.cpp:338] ctx_output6_ctx_output6/relu_0_split does not need backward computation.
I0511 17:17:46.573177   333 net.cpp:338] ctx_output6/relu does not need backward computation.
I0511 17:17:46.573199   333 net.cpp:338] ctx_output6 does not need backward computation.
I0511 17:17:46.573220   333 net.cpp:338] ctx_output5_ctx_output5/relu_0_split does not need backward computation.
I0511 17:17:46.573240   333 net.cpp:338] ctx_output5/relu does not need backward computation.
I0511 17:17:46.573262   333 net.cpp:338] ctx_output5 does not need backward computation.
I0511 17:17:46.573283   333 net.cpp:338] ctx_output4_ctx_output4/relu_0_split does not need backward computation.
I0511 17:17:46.573318   333 net.cpp:338] ctx_output4/relu does not need backward computation.
I0511 17:17:46.573343   333 net.cpp:338] ctx_output4 does not need backward computation.
I0511 17:17:46.573367   333 net.cpp:338] ctx_output3_ctx_output3/relu_0_split does not need backward computation.
I0511 17:17:46.573390   333 net.cpp:338] ctx_output3/relu does not need backward computation.
I0511 17:17:46.573415   333 net.cpp:338] ctx_output3 does not need backward computation.
I0511 17:17:46.573438   333 net.cpp:338] ctx_output2_ctx_output2/relu_0_split does not need backward computation.
I0511 17:17:46.573467   333 net.cpp:338] ctx_output2/relu does not need backward computation.
I0511 17:17:46.573499   333 net.cpp:338] ctx_output2 does not need backward computation.
I0511 17:17:46.573524   333 net.cpp:338] ctx_output1_ctx_output1/relu_0_split does not need backward computation.
I0511 17:17:46.573549   333 net.cpp:338] ctx_output1/relu does not need backward computation.
I0511 17:17:46.573572   333 net.cpp:338] ctx_output1 does not need backward computation.
I0511 17:17:46.573596   333 net.cpp:338] pool9 does not need backward computation.
I0511 17:17:46.573621   333 net.cpp:338] pool8_pool8_0_split does not need backward computation.
I0511 17:17:46.573645   333 net.cpp:338] pool8 does not need backward computation.
I0511 17:17:46.573670   333 net.cpp:338] pool7_pool7_0_split does not need backward computation.
I0511 17:17:46.573694   333 net.cpp:338] pool7 does not need backward computation.
I0511 17:17:46.573719   333 net.cpp:338] pool6_pool6_0_split does not need backward computation.
I0511 17:17:46.573742   333 net.cpp:338] pool6 does not need backward computation.
I0511 17:17:46.573767   333 net.cpp:338] res5a_branch2b_res5a_branch2b/relu_0_split does not need backward computation.
I0511 17:17:46.573791   333 net.cpp:338] res5a_branch2b/relu does not need backward computation.
I0511 17:17:46.573814   333 net.cpp:338] res5a_branch2b/bn does not need backward computation.
I0511 17:17:46.573838   333 net.cpp:338] res5a_branch2b does not need backward computation.
I0511 17:17:46.573863   333 net.cpp:338] res5a_branch2a/relu does not need backward computation.
I0511 17:17:46.573885   333 net.cpp:338] res5a_branch2a/bn does not need backward computation.
I0511 17:17:46.573909   333 net.cpp:338] res5a_branch2a does not need backward computation.
I0511 17:17:46.573932   333 net.cpp:338] pool4 does not need backward computation.
I0511 17:17:46.573956   333 net.cpp:338] res4a_branch2b/relu does not need backward computation.
I0511 17:17:46.573979   333 net.cpp:338] res4a_branch2b/bn does not need backward computation.
I0511 17:17:46.574002   333 net.cpp:338] res4a_branch2b does not need backward computation.
I0511 17:17:46.574026   333 net.cpp:338] res4a_branch2a/relu does not need backward computation.
I0511 17:17:46.574049   333 net.cpp:338] res4a_branch2a/bn does not need backward computation.
I0511 17:17:46.574072   333 net.cpp:338] res4a_branch2a does not need backward computation.
I0511 17:17:46.574097   333 net.cpp:338] pool3 does not need backward computation.
I0511 17:17:46.574121   333 net.cpp:338] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0511 17:17:46.574146   333 net.cpp:338] res3a_branch2b/relu does not need backward computation.
I0511 17:17:46.574169   333 net.cpp:338] res3a_branch2b/bn does not need backward computation.
I0511 17:17:46.574193   333 net.cpp:338] res3a_branch2b does not need backward computation.
I0511 17:17:46.574216   333 net.cpp:338] res3a_branch2a/relu does not need backward computation.
I0511 17:17:46.574240   333 net.cpp:338] res3a_branch2a/bn does not need backward computation.
I0511 17:17:46.574263   333 net.cpp:338] res3a_branch2a does not need backward computation.
I0511 17:17:46.574286   333 net.cpp:338] pool2 does not need backward computation.
I0511 17:17:46.574311   333 net.cpp:338] res2a_branch2b/relu does not need backward computation.
I0511 17:17:46.574334   333 net.cpp:338] res2a_branch2b/bn does not need backward computation.
I0511 17:17:46.574357   333 net.cpp:338] res2a_branch2b does not need backward computation.
I0511 17:17:46.574381   333 net.cpp:338] res2a_branch2a/relu does not need backward computation.
I0511 17:17:46.574404   333 net.cpp:338] res2a_branch2a/bn does not need backward computation.
I0511 17:17:46.574427   333 net.cpp:338] res2a_branch2a does not need backward computation.
I0511 17:17:46.574451   333 net.cpp:338] pool1 does not need backward computation.
I0511 17:17:46.574476   333 net.cpp:338] conv1b/relu does not need backward computation.
I0511 17:17:46.574501   333 net.cpp:338] conv1b/bn does not need backward computation.
I0511 17:17:46.574532   333 net.cpp:338] conv1b does not need backward computation.
I0511 17:17:46.574555   333 net.cpp:338] conv1a/relu does not need backward computation.
I0511 17:17:46.574579   333 net.cpp:338] conv1a/bn does not need backward computation.
I0511 17:17:46.574602   333 net.cpp:338] conv1a does not need backward computation.
I0511 17:17:46.574625   333 net.cpp:338] data/bias does not need backward computation.
I0511 17:17:46.574651   333 net.cpp:338] data_data_0_split does not need backward computation.
I0511 17:17:46.574676   333 net.cpp:338] data does not need backward computation.
I0511 17:17:46.574698   333 net.cpp:380] This network produces output detection_eval
I0511 17:17:46.574878   333 net.cpp:403] Top memory (TEST) required for data: 1212797872 diff: 1212797872
I0511 17:17:46.574903   333 net.cpp:406] Bottom memory (TEST) required for data: 1212797792 diff: 1212797792
I0511 17:17:46.574925   333 net.cpp:409] Shared (in-place) memory (TEST) by data: 521715712 diff: 521715712
I0511 17:17:46.574949   333 net.cpp:412] Parameters memory (TEST) required for data: 12464288 diff: 12464288
I0511 17:17:46.574972   333 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I0511 17:17:46.574995   333 net.cpp:421] Network initialization done.
I0511 17:17:46.575351   333 solver.cpp:55] Solver scaffolding done.
I0511 17:17:46.582146   333 caffe.cpp:158] Finetuning from training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/initial/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 17:17:46.586655   333 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 17:17:46.586690   333 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 17:17:46.586710   333 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 17:17:46.586786   333 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 17:17:46.586843   333 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.586952   333 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 17:17:46.586973   333 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 17:17:46.587031   333 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587134   333 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 17:17:46.587157   333 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 17:17:46.587177   333 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.587241   333 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587343   333 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.587364   333 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.587424   333 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587530   333 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.587551   333 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 17:17:46.587570   333 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.587664   333 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587771   333 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.587792   333 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.587867   333 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.587971   333 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.587992   333 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.588012   333 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 17:17:46.588037   333 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.588253   333 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.588366   333 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.588387   333 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.588510   333 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.588618   333 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.588639   333 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 17:17:46.588658   333 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.589247   333 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.589375   333 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.589397   333 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.589725   333 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.589839   333 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.589860   333 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.589880   333 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 17:17:46.589900   333 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 17:17:46.589920   333 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 17:17:46.589938   333 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 17:17:46.589957   333 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 17:17:46.589977   333 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 17:17:46.589996   333 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 17:17:46.590015   333 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 17:17:46.590088   333 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 17:17:46.590106   333 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590126   333 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 17:17:46.590248   333 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 17:17:46.590272   333 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590291   333 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 17:17:46.590415   333 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 17:17:46.590438   333 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590457   333 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 17:17:46.590577   333 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 17:17:46.590600   333 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590620   333 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 17:17:46.590736   333 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 17:17:46.590759   333 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590777   333 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 17:17:46.590893   333 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 17:17:46.590914   333 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 17:17:46.590939   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591006   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591027   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591045   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591102   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591125   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591143   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591162   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591228   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591245   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591265   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591323   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591342   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591361   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591380   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591439   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591457   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591477   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591536   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591554   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591574   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591593   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591652   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591670   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591691   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591749   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591768   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.591789   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.591807   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.591866   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.591884   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.591903   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.591961   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.591979   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.592000   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.592023   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.592089   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.592108   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.592128   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.592185   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.592203   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.592223   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.592242   333 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 17:17:46.592262   333 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 17:17:46.592281   333 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 17:17:46.592300   333 net.cpp:1153] Copying source layer mbox_loss Type:MultiBoxLoss #blobs=0
I0511 17:17:46.597434   333 net.cpp:1153] Copying source layer data Type:AnnotatedData #blobs=0
I0511 17:17:46.597472   333 net.cpp:1153] Copying source layer data_data_0_split Type:Split #blobs=0
I0511 17:17:46.597494   333 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I0511 17:17:46.597560   333 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I0511 17:17:46.597630   333 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.597755   333 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0511 17:17:46.597779   333 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I0511 17:17:46.597846   333 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.597968   333 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0511 17:17:46.597991   333 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I0511 17:17:46.598013   333 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.598088   333 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598212   333 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.598237   333 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.598307   333 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598424   333 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.598448   333 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I0511 17:17:46.598469   333 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.598572   333 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598695   333 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.598717   333 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.598803   333 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.598924   333 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.598948   333 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.598970   333 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I0511 17:17:46.598992   333 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.599218   333 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.599347   333 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.599371   333 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.599520   333 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.599659   333 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.599684   333 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I0511 17:17:46.599706   333 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0511 17:17:46.600389   333 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0511 17:17:46.600522   333 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0511 17:17:46.600545   333 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0511 17:17:46.600929   333 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0511 17:17:46.601060   333 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0511 17:17:46.601085   333 net.cpp:1153] Copying source layer res5a_branch2b_res5a_branch2b/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601107   333 net.cpp:1153] Copying source layer pool6 Type:Pooling #blobs=0
I0511 17:17:46.601130   333 net.cpp:1153] Copying source layer pool6_pool6_0_split Type:Split #blobs=0
I0511 17:17:46.601151   333 net.cpp:1153] Copying source layer pool7 Type:Pooling #blobs=0
I0511 17:17:46.601173   333 net.cpp:1153] Copying source layer pool7_pool7_0_split Type:Split #blobs=0
I0511 17:17:46.601195   333 net.cpp:1153] Copying source layer pool8 Type:Pooling #blobs=0
I0511 17:17:46.601217   333 net.cpp:1153] Copying source layer pool8_pool8_0_split Type:Split #blobs=0
I0511 17:17:46.601239   333 net.cpp:1153] Copying source layer pool9 Type:Pooling #blobs=0
I0511 17:17:46.601261   333 net.cpp:1153] Copying source layer ctx_output1 Type:Convolution #blobs=2
I0511 17:17:46.601361   333 net.cpp:1153] Copying source layer ctx_output1/relu Type:ReLU #blobs=0
I0511 17:17:46.601383   333 net.cpp:1153] Copying source layer ctx_output1_ctx_output1/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601405   333 net.cpp:1153] Copying source layer ctx_output2 Type:Convolution #blobs=2
I0511 17:17:46.601542   333 net.cpp:1153] Copying source layer ctx_output2/relu Type:ReLU #blobs=0
I0511 17:17:46.601569   333 net.cpp:1153] Copying source layer ctx_output2_ctx_output2/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601591   333 net.cpp:1153] Copying source layer ctx_output3 Type:Convolution #blobs=2
I0511 17:17:46.601728   333 net.cpp:1153] Copying source layer ctx_output3/relu Type:ReLU #blobs=0
I0511 17:17:46.601754   333 net.cpp:1153] Copying source layer ctx_output3_ctx_output3/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601776   333 net.cpp:1153] Copying source layer ctx_output4 Type:Convolution #blobs=2
I0511 17:17:46.601904   333 net.cpp:1153] Copying source layer ctx_output4/relu Type:ReLU #blobs=0
I0511 17:17:46.601927   333 net.cpp:1153] Copying source layer ctx_output4_ctx_output4/relu_0_split Type:Split #blobs=0
I0511 17:17:46.601945   333 net.cpp:1153] Copying source layer ctx_output5 Type:Convolution #blobs=2
I0511 17:17:46.602063   333 net.cpp:1153] Copying source layer ctx_output5/relu Type:ReLU #blobs=0
I0511 17:17:46.602085   333 net.cpp:1153] Copying source layer ctx_output5_ctx_output5/relu_0_split Type:Split #blobs=0
I0511 17:17:46.602104   333 net.cpp:1153] Copying source layer ctx_output6 Type:Convolution #blobs=2
I0511 17:17:46.602218   333 net.cpp:1153] Copying source layer ctx_output6/relu Type:ReLU #blobs=0
I0511 17:17:46.602241   333 net.cpp:1153] Copying source layer ctx_output6_ctx_output6/relu_0_split Type:Split #blobs=0
I0511 17:17:46.602259   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602319   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602336   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602355   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602417   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602444   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602464   333 net.cpp:1153] Copying source layer ctx_output1/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602483   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602542   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602561   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602581   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602639   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602658   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602677   333 net.cpp:1153] Copying source layer ctx_output2/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602696   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602754   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602773   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.602792   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.602852   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.602870   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.602890   333 net.cpp:1153] Copying source layer ctx_output3/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.602910   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.602968   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.602988   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603006   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603065   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603083   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603102   333 net.cpp:1153] Copying source layer ctx_output4/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603122   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.603180   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.603199   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603219   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603276   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603293   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603313   333 net.cpp:1153] Copying source layer ctx_output5/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603332   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc Type:Convolution #blobs=2
I0511 17:17:46.603390   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_perm Type:Permute #blobs=0
I0511 17:17:46.603408   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_loc_flat Type:Flatten #blobs=0
I0511 17:17:46.603428   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf Type:Convolution #blobs=2
I0511 17:17:46.603490   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_perm Type:Permute #blobs=0
I0511 17:17:46.603515   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_conf_flat Type:Flatten #blobs=0
I0511 17:17:46.603535   333 net.cpp:1153] Copying source layer ctx_output6/relu_mbox_priorbox Type:PriorBox #blobs=0
I0511 17:17:46.603554   333 net.cpp:1153] Copying source layer mbox_loc Type:Concat #blobs=0
I0511 17:17:46.603574   333 net.cpp:1153] Copying source layer mbox_conf Type:Concat #blobs=0
I0511 17:17:46.603592   333 net.cpp:1153] Copying source layer mbox_priorbox Type:Concat #blobs=0
I0511 17:17:46.603612   333 net.cpp:1137] Ignoring source layer mbox_loss
I0511 17:17:46.603828   333 caffe.cpp:260] Starting Optimization
I0511 17:17:46.603854   333 solver.cpp:455] Solving ssdJacintoNetV2
I0511 17:17:46.603873   333 solver.cpp:456] Learning Rate Policy: poly
I0511 17:17:46.603924   333 net.cpp:1494] [0] Reserving 12451584 bytes of shared learnable space for type FLOAT
I0511 17:17:46.608160   333 solver.cpp:269] Initial Test started...
I0511 17:17:46.608201   333 solver.cpp:637] Iteration 0, Testing net (#0)
I0511 17:17:46.611214   333 net.cpp:1071] Ignoring source layer mbox_loss
I0511 17:17:46.612951   378 common.cpp:528] NVML initialized, thread 378
I0511 17:17:46.658514   378 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 378
I0511 17:18:03.767459   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:18:03.946961   333 solver.cpp:749] class AP 1: 0.902301
I0511 17:18:03.947893   333 solver.cpp:749] class AP 2: 0.887808
I0511 17:18:03.948199   333 solver.cpp:749] class AP 3: 0.902711
I0511 17:18:03.948207   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.897607
I0511 17:18:03.948246   333 solver.cpp:274] Initial Test completed in 17.34s
I0511 17:18:04.659658   333 solver.cpp:360] Iteration 0 (0.711375 s), loss = 2.1365
I0511 17:18:04.659819   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.13313 (* 1 = 2.13313 loss)
I0511 17:18:04.659910   333 sgd_solver.cpp:172] Iteration 0, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I0511 17:18:04.775753   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.74M 3/1 1 1 0 	(avail 7.43G, req 0.74M)	t: 0 0 1.88
I0511 17:18:05.045018   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.74M 32/4 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.85 1.93
I0511 17:18:05.389921   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.74M 32/1 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.83 2.66
I0511 17:18:05.642237   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.74M 64/4 1 4 0 	(avail 7.43G, req 0.74M)	t: 0 0.31 0.79
I0511 17:18:05.961027   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.11G 64/1 6 4 5 	(avail 7.32G, req 0.11G)	t: 0 0.72 1.16
I0511 17:18:06.174163   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.11G 128/4 6 4 0 	(avail 7.32G, req 0.11G)	t: 0 0.15 0.39
I0511 17:18:06.457942   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.11G 128/1 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.48 0.51
I0511 17:18:06.669474   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.11G 256/4 6 4 5 	(avail 7.32G, req 0.11G)	t: 0 0.12 0.18
I0511 17:18:06.938400   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.11G 256/1 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.53 0.48
I0511 17:18:07.157773   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.11G 512/4 7 5 5 	(avail 7.32G, req 0.11G)	t: 0 0.11 0.11
I0511 17:18:07.610962   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1' with space 0.11G 128/1 1 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.43 0.82
I0511 17:18:07.830991   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2' with space 0.11G 512/1 1 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.13 0.2
I0511 17:18:08.033882   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3' with space 0.11G 512/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.06 0.08
I0511 17:18:08.230051   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.05
I0511 17:18:08.425616   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.04
I0511 17:18:08.621701   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6' with space 0.11G 512/1 0 0 1 	(avail 7.32G, req 0.11G)	t: 0 0.04 0.03
I0511 17:18:08.918817   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.27 0.78
I0511 17:18:09.175052   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output1/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.29 0.78
I0511 17:18:09.359982   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_loc' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.03 0.05
I0511 17:18:09.544384   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output2/relu_mbox_conf' with space 0.11G 256/1 0 1 0 	(avail 7.32G, req 0.11G)	t: 0 0.03 0.05
I0511 17:18:09.725730   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.02 0.02
I0511 17:18:09.897591   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output3/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.02 0.02
I0511 17:18:10.077782   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.257812   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output4/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.437355   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.609409   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output5/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.782119   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_loc' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:10.953788   333 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_output6/relu_mbox_conf' with space 0.11G 256/1 0 0 0 	(avail 7.32G, req 0.11G)	t: 0 0.01 0.01
I0511 17:18:11.384562   333 solver.cpp:360] Iteration 1 (6.7249 s), loss = 2.09856
I0511 17:18:11.384606   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.699 (* 1 = 1.699 loss)
I0511 17:18:11.850543   333 solver.cpp:360] Iteration 2 (0.465974 s), loss = 2.07411
I0511 17:18:11.850651   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.18515 (* 1 = 2.18515 loss)
I0511 17:18:23.001194   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:19:03.266186   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:19:45.682925   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:20:31.706429   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:20:44.765702   333 solver.cpp:354] Iteration 100 (0.640878 iter/s, 152.915s/98 iter), 3.8/376.5ep, loss = 2.45554
I0511 17:20:44.765810   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.09453 (* 1 = 2.09453 loss)
I0511 17:20:44.765849   333 sgd_solver.cpp:172] Iteration 100, lr = 0.000960596, m = 0.9, wd = 1e-05, gs = 1
I0511 17:21:13.261363   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:21:52.522627   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:22:37.643249   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:23:20.549937   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:23:20.614359   333 solver.cpp:354] Iteration 200 (0.641648 iter/s, 155.849s/100 iter), 7.5/376.5ep, loss = 2.22128
I0511 17:23:20.614567   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.81194 (* 1 = 1.81194 loss)
I0511 17:23:20.614632   333 sgd_solver.cpp:172] Iteration 200, lr = 0.000922368, m = 0.9, wd = 1e-05, gs = 1
I0511 17:24:01.467376   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:24:41.761487   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:25:31.080615   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:26:07.161725   333 solver.cpp:354] Iteration 300 (0.60043 iter/s, 166.547s/100 iter), 11.3/376.5ep, loss = 2.42609
I0511 17:26:07.161931   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.40756 (* 1 = 2.40756 loss)
I0511 17:26:07.161981   333 sgd_solver.cpp:172] Iteration 300, lr = 0.000885293, m = 0.9, wd = 1e-05, gs = 1
I0511 17:26:09.534718   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:26:52.304467   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:27:38.235322   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:28:19.790803   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:28:44.452936   333 solver.cpp:354] Iteration 400 (0.635764 iter/s, 157.291s/100 iter), 15.1/376.5ep, loss = 2.23426
I0511 17:28:44.453182   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.54844 (* 1 = 2.54844 loss)
I0511 17:28:44.453264   333 sgd_solver.cpp:172] Iteration 400, lr = 0.000849347, m = 0.9, wd = 1e-05, gs = 1
I0511 17:28:58.654088   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:29:42.934747   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:30:24.552893   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:05.045603   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:21.363632   333 solver.cpp:637] Iteration 500, Testing net (#0)
I0511 17:31:35.686610   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:31:35.930398   333 solver.cpp:749] class AP 1: 0.904178
I0511 17:31:35.930835   333 solver.cpp:749] class AP 2: 0.87761
I0511 17:31:35.931067   333 solver.cpp:749] class AP 3: 0.904251
I0511 17:31:35.931075   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.895346
I0511 17:31:35.931102   333 solver.cpp:284] Tests completed in 171.478s
I0511 17:31:36.385363   333 solver.cpp:354] Iteration 500 (0.583165 iter/s, 171.478s/100 iter), 18.8/376.5ep, loss = 2.21601
I0511 17:31:36.385434   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.3453 (* 1 = 2.3453 loss)
I0511 17:31:36.385462   333 sgd_solver.cpp:172] Iteration 500, lr = 0.000814506, m = 0.9, wd = 1e-05, gs = 1
I0511 17:31:56.219297   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:32:37.279547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:33:19.111665   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:34:00.243744   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:34:07.294708   333 solver.cpp:354] Iteration 600 (0.66265 iter/s, 150.909s/100 iter), 22.6/376.5ep, loss = 2.43671
I0511 17:34:07.294849   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.1361 (* 1 = 2.1361 loss)
I0511 17:34:07.294893   333 sgd_solver.cpp:172] Iteration 600, lr = 0.000780749, m = 0.9, wd = 1e-05, gs = 1
I0511 17:34:43.553920   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:35:28.338800   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:36:10.504659   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:36:50.824117   333 solver.cpp:354] Iteration 700 (0.611512 iter/s, 163.529s/100 iter), 26.4/376.5ep, loss = 2.34746
I0511 17:36:50.824357   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.18696 (* 1 = 2.18696 loss)
I0511 17:36:50.824430   333 sgd_solver.cpp:172] Iteration 700, lr = 0.000748052, m = 0.9, wd = 1e-05, gs = 1
I0511 17:36:53.742157   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:37:35.216517   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:38:20.611024   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:39:04.562110   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:39:31.154881   333 solver.cpp:354] Iteration 800 (0.623712 iter/s, 160.331s/100 iter), 30.1/376.5ep, loss = 2.22285
I0511 17:39:31.155395   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.57144 (* 1 = 1.57144 loss)
I0511 17:39:31.155616   333 sgd_solver.cpp:172] Iteration 800, lr = 0.000716393, m = 0.9, wd = 1e-05, gs = 1
I0511 17:39:43.769470   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:40:27.195207   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:41:11.945358   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:41:50.792615   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:42:08.869518   333 solver.cpp:354] Iteration 900 (0.634058 iter/s, 157.714s/100 iter), 33.9/376.5ep, loss = 2.24554
I0511 17:42:08.869776   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33181 (* 1 = 2.33181 loss)
I0511 17:42:08.869868   333 sgd_solver.cpp:172] Iteration 900, lr = 0.00068575, m = 0.9, wd = 1e-05, gs = 1
I0511 17:42:34.187516   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:43:18.382637   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:02.407367   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:44.571188   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:44:53.026440   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_1000.caffemodel
I0511 17:44:53.122733   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_1000.solverstate
I0511 17:44:53.192353   333 solver.cpp:637] Iteration 1000, Testing net (#0)
I0511 17:45:07.854907   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:45:08.164901   333 solver.cpp:749] class AP 1: 0.900153
I0511 17:45:08.165505   333 solver.cpp:749] class AP 2: 0.895976
I0511 17:45:08.165786   333 solver.cpp:749] class AP 3: 0.902156
I0511 17:45:08.165792   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899428
I0511 17:45:08.165817   333 solver.cpp:284] Tests completed in 179.296s
I0511 17:45:08.773604   333 solver.cpp:354] Iteration 1000 (0.557737 iter/s, 179.296s/100 iter), 37.6/376.5ep, loss = 2.29679
I0511 17:45:08.773689   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33482 (* 1 = 2.33482 loss)
I0511 17:45:08.773717   333 sgd_solver.cpp:172] Iteration 1000, lr = 0.0006561, m = 0.9, wd = 1e-05, gs = 1
I0511 17:45:36.144349   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:46:22.847509   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:04.607033   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:44.797724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:47:46.461377   333 solver.cpp:354] Iteration 1100 (0.634166 iter/s, 157.688s/100 iter), 41.4/376.5ep, loss = 2.19785
I0511 17:47:46.461544   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12194 (* 1 = 2.12194 loss)
I0511 17:47:46.461596   333 sgd_solver.cpp:172] Iteration 1100, lr = 0.000627422, m = 0.9, wd = 1e-05, gs = 1
I0511 17:48:28.288537   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:49:12.091766   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:49:51.407722   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:50:22.809286   333 solver.cpp:354] Iteration 1200 (0.6396 iter/s, 156.348s/100 iter), 45.2/376.5ep, loss = 2.26381
I0511 17:50:22.809757   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.14008 (* 1 = 2.14008 loss)
I0511 17:50:22.809865   333 sgd_solver.cpp:172] Iteration 1200, lr = 0.000599695, m = 0.9, wd = 1e-05, gs = 1
I0511 17:50:32.939734   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:51:20.879318   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:52:01.095721   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:52:40.156750   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:53:03.206368   333 solver.cpp:354] Iteration 1300 (0.623454 iter/s, 160.397s/100 iter), 48.9/376.5ep, loss = 2.35582
I0511 17:53:03.206826   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.73048 (* 1 = 2.73048 loss)
I0511 17:53:03.207052   333 sgd_solver.cpp:172] Iteration 1300, lr = 0.000572898, m = 0.9, wd = 1e-05, gs = 1
I0511 17:53:24.905720   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:54:05.658897   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:54:46.881399   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:55:28.374125   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:55:40.950692   333 solver.cpp:354] Iteration 1400 (0.633938 iter/s, 157.744s/100 iter), 52.7/376.5ep, loss = 2.1636
I0511 17:55:40.951102   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.95651 (* 1 = 1.95651 loss)
I0511 17:55:40.951282   333 sgd_solver.cpp:172] Iteration 1400, lr = 0.000547008, m = 0.9, wd = 1e-05, gs = 1
I0511 17:56:12.735571   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:56:53.579159   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:57:30.539316   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:15.557353   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:16.141642   333 solver.cpp:637] Iteration 1500, Testing net (#0)
I0511 17:58:30.913405   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:58:31.289669   333 solver.cpp:749] class AP 1: 0.901587
I0511 17:58:31.290128   333 solver.cpp:749] class AP 2: 0.885566
I0511 17:58:31.290326   333 solver.cpp:749] class AP 3: 0.903264
I0511 17:58:31.290338   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.896806
I0511 17:58:31.290370   333 solver.cpp:284] Tests completed in 170.339s
I0511 17:58:31.733687   333 solver.cpp:354] Iteration 1500 (0.587063 iter/s, 170.339s/100 iter), 56.5/376.5ep, loss = 2.16692
I0511 17:58:31.733727   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.21362 (* 1 = 2.21362 loss)
I0511 17:58:31.733738   333 sgd_solver.cpp:172] Iteration 1500, lr = 0.000522006, m = 0.9, wd = 1e-05, gs = 1
I0511 17:59:09.987963   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 17:59:49.683116   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:00:31.653239   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:01:07.423645   333 solver.cpp:354] Iteration 1600 (0.642303 iter/s, 155.69s/100 iter), 60.2/376.5ep, loss = 2.27686
I0511 18:01:07.423733   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.01189 (* 1 = 2.01189 loss)
I0511 18:01:07.423758   333 sgd_solver.cpp:172] Iteration 1600, lr = 0.000497871, m = 0.9, wd = 1e-05, gs = 1
I0511 18:01:15.863561   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:01:57.228724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:02:35.436547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:03:18.770547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:03:42.631866   333 solver.cpp:354] Iteration 1700 (0.644297 iter/s, 155.208s/100 iter), 64/376.5ep, loss = 2.20515
I0511 18:03:42.631904   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.08376 (* 1 = 2.08376 loss)
I0511 18:03:42.631916   333 sgd_solver.cpp:172] Iteration 1700, lr = 0.000474583, m = 0.9, wd = 1e-05, gs = 1
I0511 18:04:03.960618   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:04:50.468394   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:05:31.360563   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:06:17.035137   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:06:30.699640   333 solver.cpp:354] Iteration 1800 (0.594999 iter/s, 168.068s/100 iter), 67.8/376.5ep, loss = 2.22852
I0511 18:06:30.699990   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.42132 (* 1 = 2.42132 loss)
I0511 18:06:30.700186   333 sgd_solver.cpp:172] Iteration 1800, lr = 0.000452122, m = 0.9, wd = 1e-05, gs = 1
I0511 18:07:00.961182   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:07:37.246225   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:08:20.879458   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:09:04.433501   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:09:07.426964   333 solver.cpp:354] Iteration 1900 (0.638052 iter/s, 156.727s/100 iter), 71.5/376.5ep, loss = 2.27271
I0511 18:09:07.427042   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.48626 (* 1 = 2.48626 loss)
I0511 18:09:07.427069   333 sgd_solver.cpp:172] Iteration 1900, lr = 0.000430467, m = 0.9, wd = 1e-05, gs = 1
I0511 18:09:42.385061   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:10:20.854490   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:09.156301   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:43.601752   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_2000.caffemodel
I0511 18:11:43.663370   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_2000.solverstate
I0511 18:11:43.728310   333 solver.cpp:637] Iteration 2000, Testing net (#0)
I0511 18:11:55.097368   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:11:55.535323   333 solver.cpp:749] class AP 1: 0.903854
I0511 18:11:55.535676   333 solver.cpp:749] class AP 2: 0.893114
I0511 18:11:55.535877   333 solver.cpp:749] class AP 3: 0.902501
I0511 18:11:55.535888   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899823
I0511 18:11:55.535926   333 solver.cpp:284] Tests completed in 168.109s
I0511 18:11:55.965535   333 solver.cpp:354] Iteration 2000 (0.594853 iter/s, 168.109s/100 iter), 75.3/376.5ep, loss = 2.22376
I0511 18:11:55.965626   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.22239 (* 1 = 2.22239 loss)
I0511 18:11:55.965658   333 sgd_solver.cpp:172] Iteration 2000, lr = 0.0004096, m = 0.9, wd = 1e-05, gs = 1
I0511 18:11:57.897276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:12:39.867288   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:13:24.696393   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:14:10.261602   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:14:34.669576   333 solver.cpp:354] Iteration 2100 (0.630105 iter/s, 158.704s/100 iter), 79.1/376.5ep, loss = 2.2403
I0511 18:14:34.669703   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27341 (* 1 = 2.27341 loss)
I0511 18:14:34.669752   333 sgd_solver.cpp:172] Iteration 2100, lr = 0.000389501, m = 0.9, wd = 1e-05, gs = 1
I0511 18:14:48.245709   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:15:30.749893   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:16:13.501139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:16:55.315295   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:17:10.949285   333 solver.cpp:354] Iteration 2200 (0.63988 iter/s, 156.279s/100 iter), 82.8/376.5ep, loss = 2.27257
I0511 18:17:10.949703   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.39891 (* 1 = 2.39891 loss)
I0511 18:17:10.949903   333 sgd_solver.cpp:172] Iteration 2200, lr = 0.000370151, m = 0.9, wd = 1e-05, gs = 1
I0511 18:17:34.167419   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:18:14.629576   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:02.477895   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:42.704844   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:19:48.350114   333 solver.cpp:354] Iteration 2300 (0.635322 iter/s, 157.4s/100 iter), 86.6/376.5ep, loss = 2.0549
I0511 18:19:48.350543   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82411 (* 1 = 1.82411 loss)
I0511 18:19:48.350757   333 sgd_solver.cpp:172] Iteration 2300, lr = 0.00035153, m = 0.9, wd = 1e-05, gs = 1
I0511 18:20:23.300657   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:21:05.117789   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:21:43.527817   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:22:20.229408   333 solver.cpp:354] Iteration 2400 (0.658419 iter/s, 151.879s/100 iter), 90.4/376.5ep, loss = 2.04018
I0511 18:22:20.229952   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.86707 (* 1 = 1.86707 loss)
I0511 18:22:20.230144   333 sgd_solver.cpp:172] Iteration 2400, lr = 0.000333622, m = 0.9, wd = 1e-05, gs = 1
I0511 18:22:27.298089   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:23:06.215690   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:23:46.155441   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:24:32.111308   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:24:57.386494   333 solver.cpp:637] Iteration 2500, Testing net (#0)
I0511 18:25:13.420470   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:25:13.923166   333 solver.cpp:749] class AP 1: 0.902947
I0511 18:25:13.923579   333 solver.cpp:749] class AP 2: 0.891763
I0511 18:25:13.923779   333 solver.cpp:749] class AP 3: 0.903372
I0511 18:25:13.923785   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89936
I0511 18:25:13.923812   333 solver.cpp:284] Tests completed in 173.694s
I0511 18:25:14.353237   333 solver.cpp:354] Iteration 2500 (0.575725 iter/s, 173.694s/100 iter), 94.1/376.5ep, loss = 2.03225
I0511 18:25:14.353325   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.86228 (* 1 = 1.86228 loss)
I0511 18:25:14.353358   333 sgd_solver.cpp:172] Iteration 2500, lr = 0.000316406, m = 0.9, wd = 1e-05, gs = 1
I0511 18:25:19.602969   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:26:00.657719   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:26:42.633878   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:27:22.068154   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:27:42.064335   333 solver.cpp:354] Iteration 2600 (0.676998 iter/s, 147.711s/100 iter), 97.9/376.5ep, loss = 2.1111
I0511 18:27:42.064678   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.63182 (* 1 = 1.63182 loss)
I0511 18:27:42.064786   333 sgd_solver.cpp:172] Iteration 2600, lr = 0.000299866, m = 0.9, wd = 1e-05, gs = 1
I0511 18:28:03.605492   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:28:49.804783   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:29:30.085525   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:30:08.500242   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:30:19.853060   333 solver.cpp:354] Iteration 2700 (0.63376 iter/s, 157.788s/100 iter), 101.6/376.5ep, loss = 2.31247
I0511 18:30:19.853147   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.33624 (* 1 = 2.33624 loss)
I0511 18:30:19.853176   333 sgd_solver.cpp:172] Iteration 2700, lr = 0.000283982, m = 0.9, wd = 1e-05, gs = 1
I0511 18:30:55.610414   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:31:39.330112   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:32:16.456498   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:32:57.606693   333 solver.cpp:354] Iteration 2800 (0.633901 iter/s, 157.753s/100 iter), 105.4/376.5ep, loss = 2.06867
I0511 18:32:57.607113   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.23668 (* 1 = 2.23668 loss)
I0511 18:32:57.607297   333 sgd_solver.cpp:172] Iteration 2800, lr = 0.000268739, m = 0.9, wd = 1e-05, gs = 1
I0511 18:32:57.950987   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:33:37.820617   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:34:22.074807   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:35:04.936424   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:35:33.652398   333 solver.cpp:354] Iteration 2900 (0.640839 iter/s, 156.045s/100 iter), 109.2/376.5ep, loss = 2.14422
I0511 18:35:33.652766   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.20666 (* 1 = 2.20666 loss)
I0511 18:35:33.652966   333 sgd_solver.cpp:172] Iteration 2900, lr = 0.000254117, m = 0.9, wd = 1e-05, gs = 1
I0511 18:35:48.573292   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:36:29.234525   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:37:13.252988   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:37:56.083349   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:38:16.775661   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_3000.caffemodel
I0511 18:38:16.818892   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_3000.solverstate
I0511 18:38:16.858485   333 solver.cpp:637] Iteration 3000, Testing net (#0)
I0511 18:38:31.198621   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:38:31.766930   333 solver.cpp:749] class AP 1: 0.899426
I0511 18:38:31.767313   333 solver.cpp:749] class AP 2: 0.897494
I0511 18:38:31.767520   333 solver.cpp:749] class AP 3: 0.903234
I0511 18:38:31.767526   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900051
I0511 18:38:31.767554   333 solver.cpp:284] Tests completed in 178.115s
I0511 18:38:32.225045   333 solver.cpp:354] Iteration 3000 (0.561435 iter/s, 178.115s/100 iter), 112.9/376.5ep, loss = 2.20762
I0511 18:38:32.225133   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12977 (* 1 = 2.12977 loss)
I0511 18:38:32.225163   333 sgd_solver.cpp:172] Iteration 3000, lr = 0.0002401, m = 0.9, wd = 1e-05, gs = 1
I0511 18:38:47.630647   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:39:28.963418   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:40:10.237226   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:40:52.660075   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:41:03.604025   333 solver.cpp:354] Iteration 3100 (0.660595 iter/s, 151.379s/100 iter), 116.7/376.5ep, loss = 2.16867
I0511 18:41:03.604110   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82897 (* 1 = 1.82897 loss)
I0511 18:41:03.604135   333 sgd_solver.cpp:172] Iteration 3100, lr = 0.000226671, m = 0.9, wd = 1e-05, gs = 1
I0511 18:41:36.891857   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:42:17.578469   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:42:58.889701   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:43:39.818974   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:43:41.200614   333 solver.cpp:354] Iteration 3200 (0.634533 iter/s, 157.596s/100 iter), 120.5/376.5ep, loss = 2.06938
I0511 18:43:41.200793   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.00634 (* 1 = 2.00634 loss)
I0511 18:43:41.200858   333 sgd_solver.cpp:172] Iteration 3200, lr = 0.000213814, m = 0.9, wd = 1e-05, gs = 1
I0511 18:44:24.222885   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:45:06.174834   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:45:47.801210   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:46:25.766161   333 solver.cpp:354] Iteration 3300 (0.607662 iter/s, 164.565s/100 iter), 124.2/376.5ep, loss = 2.2489
I0511 18:46:25.766561   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74222 (* 1 = 1.74222 loss)
I0511 18:46:25.766767   333 sgd_solver.cpp:172] Iteration 3300, lr = 0.000201511, m = 0.9, wd = 1e-05, gs = 1
I0511 18:46:32.646620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:47:15.757292   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:47:55.306365   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:48:39.434648   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:49:04.382586   333 solver.cpp:354] Iteration 3400 (0.630456 iter/s, 158.615s/100 iter), 128/376.5ep, loss = 2.13052
I0511 18:49:04.382772   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.28424 (* 1 = 2.28424 loss)
I0511 18:49:04.382841   333 sgd_solver.cpp:172] Iteration 3400, lr = 0.000189747, m = 0.9, wd = 1e-05, gs = 1
I0511 18:49:28.654067   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:50:05.578708   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:50:48.657356   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:51:36.528040   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:51:48.473649   333 solver.cpp:637] Iteration 3500, Testing net (#0)
I0511 18:52:01.724843   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:52:02.359068   333 solver.cpp:749] class AP 1: 0.901739
I0511 18:52:02.359395   333 solver.cpp:749] class AP 2: 0.894491
I0511 18:52:02.359593   333 solver.cpp:749] class AP 3: 0.903358
I0511 18:52:02.359602   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899863
I0511 18:52:02.359632   333 solver.cpp:284] Tests completed in 177.976s
I0511 18:52:02.776721   333 solver.cpp:354] Iteration 3500 (0.561873 iter/s, 177.976s/100 iter), 131.8/376.5ep, loss = 2.08863
I0511 18:52:02.776816   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.79966 (* 1 = 1.79966 loss)
I0511 18:52:02.776844   333 sgd_solver.cpp:172] Iteration 3500, lr = 0.000178506, m = 0.9, wd = 1e-05, gs = 1
I0511 18:52:23.673970   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:53:08.379017   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:53:47.937162   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:54:28.750699   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:54:31.163288   333 solver.cpp:354] Iteration 3600 (0.673919 iter/s, 148.386s/100 iter), 135.5/376.5ep, loss = 2.07714
I0511 18:54:31.163471   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.83491 (* 1 = 1.83491 loss)
I0511 18:54:31.163538   333 sgd_solver.cpp:172] Iteration 3600, lr = 0.000167772, m = 0.9, wd = 1e-05, gs = 1
I0511 18:55:11.932245   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:55:49.697877   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:56:35.533568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:57:14.244565   333 solver.cpp:354] Iteration 3700 (0.613194 iter/s, 163.081s/100 iter), 139.3/376.5ep, loss = 2.07412
I0511 18:57:14.244861   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.93443 (* 1 = 1.93443 loss)
I0511 18:57:14.244930   333 sgd_solver.cpp:172] Iteration 3700, lr = 0.00015753, m = 0.9, wd = 1e-05, gs = 1
I0511 18:57:17.763350   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:57:56.776213   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:58:43.296139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:59:22.801322   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 18:59:49.615442   333 solver.cpp:354] Iteration 3800 (0.643624 iter/s, 155.37s/100 iter), 143.1/376.5ep, loss = 2.26262
I0511 18:59:49.615830   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.11317 (* 1 = 2.11317 loss)
I0511 18:59:49.616019   333 sgd_solver.cpp:172] Iteration 3800, lr = 0.000147763, m = 0.9, wd = 1e-05, gs = 1
I0511 19:00:04.168287   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:00:49.451126   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:01:30.498494   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:02:10.791354   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:02:29.300794   333 solver.cpp:354] Iteration 3900 (0.626234 iter/s, 159.685s/100 iter), 146.8/376.5ep, loss = 2.15146
I0511 19:02:29.301172   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.87135 (* 1 = 1.87135 loss)
I0511 19:02:29.301383   333 sgd_solver.cpp:172] Iteration 3900, lr = 0.000138458, m = 0.9, wd = 1e-05, gs = 1
I0511 19:02:54.985561   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:03:35.003332   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:04:16.469045   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:01.983453   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:02.674352   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_4000.caffemodel
I0511 19:05:02.729351   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_4000.solverstate
I0511 19:05:02.817267   333 solver.cpp:637] Iteration 4000, Testing net (#0)
I0511 19:05:17.901135   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:05:18.597537   333 solver.cpp:749] class AP 1: 0.901215
I0511 19:05:18.597880   333 solver.cpp:749] class AP 2: 0.894794
I0511 19:05:18.598089   333 solver.cpp:749] class AP 3: 0.9034
I0511 19:05:18.598098   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899803
I0511 19:05:18.598122   333 solver.cpp:284] Tests completed in 169.297s
I0511 19:05:19.025316   333 solver.cpp:354] Iteration 4000 (0.590679 iter/s, 169.297s/100 iter), 150.6/376.5ep, loss = 2.0302
I0511 19:05:19.025396   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.70781 (* 1 = 1.70781 loss)
I0511 19:05:19.025425   333 sgd_solver.cpp:172] Iteration 4000, lr = 0.0001296, m = 0.9, wd = 1e-05, gs = 1
I0511 19:05:49.007318   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:06:29.712373   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:07:17.207667   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:07:54.277428   333 solver.cpp:354] Iteration 4100 (0.644116 iter/s, 155.252s/100 iter), 154.4/376.5ep, loss = 2.03315
I0511 19:07:54.277530   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74185 (* 1 = 1.74185 loss)
I0511 19:07:54.277559   333 sgd_solver.cpp:172] Iteration 4100, lr = 0.000121174, m = 0.9, wd = 1e-05, gs = 1
I0511 19:07:57.258788   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:08:36.801816   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:09:20.991451   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:10:03.795454   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:10:32.067924   333 solver.cpp:354] Iteration 4200 (0.633754 iter/s, 157.79s/100 iter), 158.1/376.5ep, loss = 2.00974
I0511 19:10:32.067960   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.5257 (* 1 = 1.5257 loss)
I0511 19:10:32.067970   333 sgd_solver.cpp:172] Iteration 4200, lr = 0.000113165, m = 0.9, wd = 1e-05, gs = 1
I0511 19:10:46.294337   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:11:25.306128   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:12:14.035629   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:12:51.068722   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:13:12.803186   333 solver.cpp:354] Iteration 4300 (0.622143 iter/s, 160.735s/100 iter), 161.9/376.5ep, loss = 2.18249
I0511 19:13:12.803400   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.72748 (* 1 = 1.72748 loss)
I0511 19:13:12.803458   333 sgd_solver.cpp:172] Iteration 4300, lr = 0.00010556, m = 0.9, wd = 1e-05, gs = 1
I0511 19:13:38.470691   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:14:17.873144   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:14:58.854243   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:15:45.234310   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:15:50.840242   333 solver.cpp:354] Iteration 4400 (0.632765 iter/s, 158.037s/100 iter), 165.6/376.5ep, loss = 2.08601
I0511 19:15:50.840674   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.68042 (* 1 = 2.68042 loss)
I0511 19:15:50.840831   333 sgd_solver.cpp:172] Iteration 4400, lr = 9.8345e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:16:23.401515   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:17:04.504091   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:17:44.851847   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:24.403488   333 solver.cpp:637] Iteration 4500, Testing net (#0)
I0511 19:18:26.190488   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:38.964437   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:18:39.724462   333 solver.cpp:749] class AP 1: 0.900225
I0511 19:18:39.724790   333 solver.cpp:749] class AP 2: 0.894932
I0511 19:18:39.724978   333 solver.cpp:749] class AP 3: 0.90345
I0511 19:18:39.724985   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899535
I0511 19:18:39.725013   333 solver.cpp:284] Tests completed in 168.884s
I0511 19:18:40.230610   333 solver.cpp:354] Iteration 4500 (0.592121 iter/s, 168.884s/100 iter), 169.4/376.5ep, loss = 2.10947
I0511 19:18:40.230695   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.88875 (* 1 = 1.88875 loss)
I0511 19:18:40.230726   333 sgd_solver.cpp:172] Iteration 4500, lr = 9.15063e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:19:19.545835   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:19:57.820793   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:20:36.446559   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:21:08.981542   333 solver.cpp:354] Iteration 4600 (0.672266 iter/s, 148.751s/100 iter), 173.2/376.5ep, loss = 2.22177
I0511 19:21:08.982142   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.95665 (* 1 = 1.95665 loss)
I0511 19:21:08.982403   333 sgd_solver.cpp:172] Iteration 4600, lr = 8.50305e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:21:19.327275   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:22:05.401099   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:22:42.507550   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:23:23.840378   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:23:45.820061   333 solver.cpp:354] Iteration 4700 (0.637594 iter/s, 156.84s/100 iter), 176.9/376.5ep, loss = 1.84253
I0511 19:23:45.820096   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.85887 (* 1 = 1.85887 loss)
I0511 19:23:45.820106   333 sgd_solver.cpp:172] Iteration 4700, lr = 7.89048e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:24:07.387917   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:24:48.432276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:25:30.404213   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:26:13.927551   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:26:25.360671   333 solver.cpp:354] Iteration 4800 (0.626796 iter/s, 159.541s/100 iter), 180.7/376.5ep, loss = 2.15876
I0511 19:26:25.360754   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.14312 (* 1 = 2.14312 loss)
I0511 19:26:25.360782   333 sgd_solver.cpp:172] Iteration 4800, lr = 7.31161e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:26:59.899405   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:27:38.053601   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:28:19.582867   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:29:04.041452   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:29:05.519140   333 solver.cpp:354] Iteration 4900 (0.624379 iter/s, 160.159s/100 iter), 184.5/376.5ep, loss = 2.14678
I0511 19:29:05.519594   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27382 (* 1 = 2.27382 loss)
I0511 19:29:05.519819   333 sgd_solver.cpp:172] Iteration 4900, lr = 6.7652e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:29:43.849869   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:30:23.743458   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:01.877153   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:35.561861   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_5000.caffemodel
I0511 19:31:35.613117   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_5000.solverstate
I0511 19:31:35.648772   333 solver.cpp:637] Iteration 5000, Testing net (#0)
I0511 19:31:49.057711   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:31:49.885879   333 solver.cpp:749] class AP 1: 0.901071
I0511 19:31:49.886230   333 solver.cpp:749] class AP 2: 0.894496
I0511 19:31:49.886426   333 solver.cpp:749] class AP 3: 0.903653
I0511 19:31:49.886435   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.89974
I0511 19:31:49.886466   333 solver.cpp:284] Tests completed in 164.368s
I0511 19:31:50.387248   333 solver.cpp:354] Iteration 5000 (0.608392 iter/s, 164.368s/100 iter), 188.2/376.5ep, loss = 2.06687
I0511 19:31:50.387336   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.8139 (* 1 = 1.8139 loss)
I0511 19:31:50.387370   333 sgd_solver.cpp:172] Iteration 5000, lr = 6.25e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:31:53.166568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:32:34.955423   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:33:14.951212   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:33:57.944811   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:34:20.971208   333 solver.cpp:354] Iteration 5100 (0.66408 iter/s, 150.584s/100 iter), 192/376.5ep, loss = 2.14621
I0511 19:34:20.971696   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27968 (* 1 = 2.27968 loss)
I0511 19:34:20.971891   333 sgd_solver.cpp:172] Iteration 5100, lr = 5.7648e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:34:41.494321   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:35:20.537277   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:36:03.843325   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:36:46.659878   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:37:01.749354   333 solver.cpp:354] Iteration 5200 (0.621975 iter/s, 160.778s/100 iter), 195.8/376.5ep, loss = 2.20325
I0511 19:37:01.749445   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.17569 (* 1 = 2.17569 loss)
I0511 19:37:01.749478   333 sgd_solver.cpp:172] Iteration 5200, lr = 5.30842e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:37:28.218608   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:38:08.579845   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:38:48.253741   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:39:32.126600   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:39:33.429520   333 solver.cpp:354] Iteration 5300 (0.659282 iter/s, 151.68s/100 iter), 199.5/376.5ep, loss = 2.08597
I0511 19:39:33.429601   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.93086 (* 1 = 1.93086 loss)
I0511 19:39:33.429630   333 sgd_solver.cpp:172] Iteration 5300, lr = 4.87968e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:40:09.510433   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:40:56.328109   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:41:37.030686   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:42:13.194873   333 solver.cpp:354] Iteration 5400 (0.625918 iter/s, 159.765s/100 iter), 203.3/376.5ep, loss = 2.18293
I0511 19:42:13.195312   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12727 (* 1 = 2.12727 loss)
I0511 19:42:13.195466   333 sgd_solver.cpp:172] Iteration 5400, lr = 4.47746e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:42:18.449568   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:43:03.337172   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:43:42.374411   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:44:25.810364   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:44:48.626117   333 solver.cpp:637] Iteration 5500, Testing net (#0)
I0511 19:45:03.002197   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:45:03.899068   333 solver.cpp:749] class AP 1: 0.9017
I0511 19:45:03.899384   333 solver.cpp:749] class AP 2: 0.894776
I0511 19:45:03.899564   333 solver.cpp:749] class AP 3: 0.903797
I0511 19:45:03.899569   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900091
I0511 19:45:03.899595   333 solver.cpp:284] Tests completed in 170.705s
I0511 19:45:04.347746   333 solver.cpp:354] Iteration 5500 (0.585807 iter/s, 170.705s/100 iter), 207.1/376.5ep, loss = 2.01783
I0511 19:45:04.347832   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.36785 (* 1 = 2.36785 loss)
I0511 19:45:04.347865   333 sgd_solver.cpp:172] Iteration 5500, lr = 4.10062e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:45:14.127439   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:45:59.024170   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:46:38.602417   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:47:26.992494   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:47:39.051219   333 solver.cpp:354] Iteration 5600 (0.646398 iter/s, 154.703s/100 iter), 210.8/376.5ep, loss = 2.09941
I0511 19:47:39.051719   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.42673 (* 1 = 2.42673 loss)
I0511 19:47:39.051920   333 sgd_solver.cpp:172] Iteration 5600, lr = 3.7481e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:48:03.793782   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:48:50.710464   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:49:30.663128   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:50:12.062755   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:50:20.091789   333 solver.cpp:354] Iteration 5700 (0.620962 iter/s, 161.04s/100 iter), 214.6/376.5ep, loss = 1.9082
I0511 19:50:20.092170   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.74167 (* 1 = 1.74167 loss)
I0511 19:50:20.092308   333 sgd_solver.cpp:172] Iteration 5700, lr = 3.4188e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:50:57.793723   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:51:40.231519   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:52:22.650105   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:52:57.699731   333 solver.cpp:354] Iteration 5800 (0.634486 iter/s, 157.608s/100 iter), 218.4/376.5ep, loss = 1.93827
I0511 19:52:57.700093   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.91764 (* 1 = 1.91764 loss)
I0511 19:52:57.700191   333 sgd_solver.cpp:172] Iteration 5800, lr = 3.1117e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:53:04.791491   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:53:46.456118   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:54:28.475489   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:55:09.266777   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:55:40.967952   333 solver.cpp:354] Iteration 5900 (0.612489 iter/s, 163.268s/100 iter), 222.1/376.5ep, loss = 1.99071
I0511 19:55:40.968020   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.87105 (* 1 = 1.87105 loss)
I0511 19:55:40.968030   333 sgd_solver.cpp:172] Iteration 5900, lr = 2.82576e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:55:54.954825   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:56:30.833425   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:57:13.155719   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:57:57.059520   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:58:14.870103   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_6000.caffemodel
I0511 19:58:14.893638   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_6000.solverstate
I0511 19:58:14.916656   333 solver.cpp:637] Iteration 6000, Testing net (#0)
I0511 19:58:27.519702   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:58:28.474046   333 solver.cpp:749] class AP 1: 0.903364
I0511 19:58:28.474370   333 solver.cpp:749] class AP 2: 0.894245
I0511 19:58:28.474565   333 solver.cpp:749] class AP 3: 0.903755
I0511 19:58:28.474572   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.900454
I0511 19:58:28.474601   333 solver.cpp:284] Tests completed in 167.507s
I0511 19:58:28.981010   333 solver.cpp:354] Iteration 6000 (0.596992 iter/s, 167.507s/100 iter), 225.9/376.5ep, loss = 2.06508
I0511 19:58:28.981094   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.26495 (* 1 = 2.26495 loss)
I0511 19:58:28.981125   333 sgd_solver.cpp:172] Iteration 6000, lr = 2.56e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 19:58:45.964639   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 19:59:30.016384   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:09.044770   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:50.886214   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:00:59.848747   333 solver.cpp:354] Iteration 6100 (0.662833 iter/s, 150.868s/100 iter), 229.6/376.5ep, loss = 2.11894
I0511 20:00:59.848866   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.36641 (* 1 = 2.36641 loss)
I0511 20:00:59.848908   333 sgd_solver.cpp:172] Iteration 6100, lr = 2.31344e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:01:31.579408   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:02:16.673519   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:02:58.749572   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:03:38.422562   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:03:39.183970   333 solver.cpp:354] Iteration 6200 (0.627608 iter/s, 159.335s/100 iter), 233.4/376.5ep, loss = 2.07418
I0511 20:03:39.184159   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.84768 (* 1 = 1.84768 loss)
I0511 20:03:39.184222   333 sgd_solver.cpp:172] Iteration 6200, lr = 2.08514e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:04:20.552605   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:05:06.445207   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:05:43.039703   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:06:13.098628   333 solver.cpp:354] Iteration 6300 (0.649711 iter/s, 153.915s/100 iter), 237.2/376.5ep, loss = 2.09865
I0511 20:06:13.098876   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.79895 (* 1 = 1.79895 loss)
I0511 20:06:13.098950   333 sgd_solver.cpp:172] Iteration 6300, lr = 1.87416e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:06:24.556910   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:07:08.267098   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:07:47.172108   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:08:27.381362   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:08:52.831589   333 solver.cpp:354] Iteration 6400 (0.626045 iter/s, 159.733s/100 iter), 240.9/376.5ep, loss = 2.05062
I0511 20:08:52.831668   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.30984 (* 1 = 2.30984 loss)
I0511 20:08:52.831697   333 sgd_solver.cpp:172] Iteration 6400, lr = 1.67962e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:09:17.226994   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:09:59.683701   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:10:40.216989   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:18.505347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:29.927526   333 solver.cpp:637] Iteration 6500, Testing net (#0)
I0511 20:11:45.537655   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:11:46.558358   333 solver.cpp:749] class AP 1: 0.901561
I0511 20:11:46.558682   333 solver.cpp:749] class AP 2: 0.892937
I0511 20:11:46.558883   333 solver.cpp:749] class AP 3: 0.903572
I0511 20:11:46.558894   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899357
I0511 20:11:46.558931   333 solver.cpp:284] Tests completed in 173.727s
I0511 20:11:46.999064   333 solver.cpp:354] Iteration 6500 (0.575615 iter/s, 173.727s/100 iter), 244.7/376.5ep, loss = 2.34782
I0511 20:11:46.999156   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.38831 (* 1 = 2.38831 loss)
I0511 20:11:46.999188   333 sgd_solver.cpp:172] Iteration 6500, lr = 1.50063e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:12:12.384449   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:12:54.578227   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:13:32.474273   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:14:18.048465   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:14:19.799342   333 solver.cpp:354] Iteration 6600 (0.65445 iter/s, 152.8s/100 iter), 248.5/376.5ep, loss = 2.18753
I0511 20:14:19.799383   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.54035 (* 1 = 2.54035 loss)
I0511 20:14:19.799392   333 sgd_solver.cpp:172] Iteration 6600, lr = 1.33634e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:14:59.739039   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:15:37.114758   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:16:17.561978   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:16:50.229394   333 solver.cpp:354] Iteration 6700 (0.664762 iter/s, 150.43s/100 iter), 252.2/376.5ep, loss = 2.16264
I0511 20:16:50.229940   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.62793 (* 1 = 1.62793 loss)
I0511 20:16:50.230186   333 sgd_solver.cpp:172] Iteration 6700, lr = 1.18592e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:17:01.219527   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:17:39.833256   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:18:19.635516   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:19:00.760093   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:19:24.686924   333 solver.cpp:354] Iteration 6800 (0.647427 iter/s, 154.458s/100 iter), 256/376.5ep, loss = 1.91357
I0511 20:19:24.687258   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.0049 (* 1 = 2.0049 loss)
I0511 20:19:24.687430   333 sgd_solver.cpp:172] Iteration 6800, lr = 1.04858e-05, m = 0.9, wd = 1e-05, gs = 1
I0511 20:19:45.261571   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:20:24.420707   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:21:03.373297   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:21:46.095031   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:22:00.828568   333 solver.cpp:354] Iteration 6900 (0.640445 iter/s, 156.141s/100 iter), 259.8/376.5ep, loss = 2.08973
I0511 20:22:00.828958   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.82734 (* 1 = 1.82734 loss)
I0511 20:22:00.829071   333 sgd_solver.cpp:172] Iteration 6900, lr = 9.23521e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:22:29.671348   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:23:09.704118   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:23:56.205437   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:37.849931   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:39.806205   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_7000.caffemodel
I0511 20:24:39.828034   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_7000.solverstate
I0511 20:24:39.849496   333 solver.cpp:637] Iteration 7000, Testing net (#0)
I0511 20:24:53.843796   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:24:54.929301   333 solver.cpp:749] class AP 1: 0.901893
I0511 20:24:54.929613   333 solver.cpp:749] class AP 2: 0.892473
I0511 20:24:54.929797   333 solver.cpp:749] class AP 3: 0.903604
I0511 20:24:54.929803   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899323
I0511 20:24:54.929831   333 solver.cpp:284] Tests completed in 174.101s
I0511 20:24:55.447754   333 solver.cpp:354] Iteration 7000 (0.574379 iter/s, 174.101s/100 iter), 263.5/376.5ep, loss = 2.03306
I0511 20:24:55.447873   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27706 (* 1 = 2.27706 loss)
I0511 20:24:55.447921   333 sgd_solver.cpp:172] Iteration 7000, lr = 8.1e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:25:27.499387   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:26:13.159543   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:26:56.920620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:27:30.162551   333 solver.cpp:354] Iteration 7100 (0.646351 iter/s, 154.715s/100 iter), 267.3/376.5ep, loss = 1.99298
I0511 20:27:30.162861   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.98301 (* 1 = 1.98301 loss)
I0511 20:27:30.162935   333 sgd_solver.cpp:172] Iteration 7100, lr = 7.07281e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:27:36.013757   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:28:18.960119   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:29:02.724834   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:29:47.536797   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:30:11.925447   333 solver.cpp:354] Iteration 7200 (0.618189 iter/s, 161.763s/100 iter), 271.1/376.5ep, loss = 2.10133
I0511 20:30:11.925707   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.44128 (* 1 = 2.44128 loss)
I0511 20:30:11.925827   333 sgd_solver.cpp:172] Iteration 7200, lr = 6.14656e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:30:32.539546   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:31:09.955901   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:31:56.346940   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:32:36.103513   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:32:50.682014   333 solver.cpp:354] Iteration 7300 (0.629888 iter/s, 158.758s/100 iter), 274.8/376.5ep, loss = 2.13205
I0511 20:32:50.682054   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.0813 (* 1 = 2.0813 loss)
I0511 20:32:50.682063   333 sgd_solver.cpp:172] Iteration 7300, lr = 5.31441e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:33:18.065133   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:34:04.092648   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:34:47.243006   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:35:27.977479   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:35:36.891813   333 solver.cpp:354] Iteration 7400 (0.601638 iter/s, 166.213s/100 iter), 278.6/376.5ep, loss = 2.04621
I0511 20:35:36.891988   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.53125 (* 1 = 1.53125 loss)
I0511 20:35:36.892036   333 sgd_solver.cpp:172] Iteration 7400, lr = 4.56976e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:36:06.833580   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:36:50.665303   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:37:31.463693   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:09.387392   333 solver.cpp:637] Iteration 7500, Testing net (#0)
I0511 20:38:12.455142   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:22.955489   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:38:24.106088   333 solver.cpp:749] class AP 1: 0.901765
I0511 20:38:24.106434   333 solver.cpp:749] class AP 2: 0.891668
I0511 20:38:24.106623   333 solver.cpp:749] class AP 3: 0.90364
I0511 20:38:24.106631   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899024
I0511 20:38:24.106657   333 solver.cpp:284] Tests completed in 167.217s
I0511 20:38:24.613920   333 solver.cpp:354] Iteration 7500 (0.598025 iter/s, 167.217s/100 iter), 282.4/376.5ep, loss = 1.98951
I0511 20:38:24.614004   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.20505 (* 1 = 2.20505 loss)
I0511 20:38:24.614032   333 sgd_solver.cpp:172] Iteration 7500, lr = 3.90625e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:39:02.539664   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:39:48.095142   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:40:33.083389   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:41:02.386482   333 solver.cpp:354] Iteration 7600 (0.633817 iter/s, 157.774s/100 iter), 286.1/376.5ep, loss = 2.1258
I0511 20:41:02.386865   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.19024 (* 1 = 2.19024 loss)
I0511 20:41:02.387060   333 sgd_solver.cpp:172] Iteration 7600, lr = 3.31776e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:41:14.679160   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:42:01.336994   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:42:45.205490   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:43:24.274483   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:43:46.210186   333 solver.cpp:354] Iteration 7700 (0.610407 iter/s, 163.825s/100 iter), 289.9/376.5ep, loss = 2.14316
I0511 20:43:46.210389   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.67414 (* 1 = 1.67414 loss)
I0511 20:43:46.210458   333 sgd_solver.cpp:172] Iteration 7700, lr = 2.79841e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:44:06.188493   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:44:51.800328   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:45:36.621703   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:46:18.036373   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:46:26.962218   333 solver.cpp:354] Iteration 7800 (0.622072 iter/s, 160.753s/100 iter), 293.6/376.5ep, loss = 1.92477
I0511 20:46:26.962455   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.9255 (* 1 = 1.9255 loss)
I0511 20:46:26.962520   333 sgd_solver.cpp:172] Iteration 7800, lr = 2.34256e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:47:03.351625   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:47:44.307417   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:48:26.429347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:49:08.857781   333 solver.cpp:354] Iteration 7900 (0.617679 iter/s, 161.896s/100 iter), 297.4/376.5ep, loss = 2.01367
I0511 20:49:08.858206   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.78698 (* 1 = 1.78698 loss)
I0511 20:49:08.858331   333 sgd_solver.cpp:172] Iteration 7900, lr = 1.94481e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:49:09.658891   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:49:51.460252   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:50:31.253098   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:14.431957   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:41.935463   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_8000.caffemodel
I0511 20:51:41.955485   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_8000.solverstate
I0511 20:51:41.974681   333 solver.cpp:637] Iteration 8000, Testing net (#0)
I0511 20:51:56.240662   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:51:57.453459   333 solver.cpp:749] class AP 1: 0.90197
I0511 20:51:57.453791   333 solver.cpp:749] class AP 2: 0.891493
I0511 20:51:57.453979   333 solver.cpp:749] class AP 3: 0.903665
I0511 20:51:57.453984   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899043
I0511 20:51:57.454010   333 solver.cpp:284] Tests completed in 168.597s
I0511 20:51:57.874972   333 solver.cpp:354] Iteration 8000 (0.59313 iter/s, 168.597s/100 iter), 301.2/376.5ep, loss = 2.07895
I0511 20:51:57.875056   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.09299 (* 1 = 2.09299 loss)
I0511 20:51:57.875087   333 sgd_solver.cpp:172] Iteration 8000, lr = 1.6e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:52:01.756975   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:52:45.760192   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:53:28.238932   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:54:10.453548   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:54:33.062490   333 solver.cpp:354] Iteration 8100 (0.644379 iter/s, 155.188s/100 iter), 304.9/376.5ep, loss = 1.99671
I0511 20:54:33.062669   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.85317 (* 1 = 1.85317 loss)
I0511 20:54:33.062731   333 sgd_solver.cpp:172] Iteration 8100, lr = 1.30321e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:54:54.600567   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:55:42.416186   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:56:19.824044   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:57:00.943271   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:57:13.574784   333 solver.cpp:354] Iteration 8200 (0.623003 iter/s, 160.513s/100 iter), 308.7/376.5ep, loss = 2.13371
I0511 20:57:13.575054   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.81334 (* 1 = 1.81334 loss)
I0511 20:57:13.575150   333 sgd_solver.cpp:172] Iteration 8200, lr = 1.04976e-06, m = 0.9, wd = 1e-05, gs = 1
I0511 20:57:47.227385   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:58:29.210474   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:07.568279   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:55.976347   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 20:59:56.957222   333 solver.cpp:354] Iteration 8300 (0.612059 iter/s, 163.383s/100 iter), 312.5/376.5ep, loss = 2.18343
I0511 20:59:56.957448   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.32801 (* 1 = 2.32801 loss)
I0511 20:59:56.957516   333 sgd_solver.cpp:172] Iteration 8300, lr = 8.3521e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:00:35.984805   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:01:15.671908   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:01:59.514173   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:02:36.771632   333 solver.cpp:354] Iteration 8400 (0.625724 iter/s, 159.815s/100 iter), 316.2/376.5ep, loss = 2.17791
I0511 21:02:36.771929   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.84049 (* 1 = 1.84049 loss)
I0511 21:02:36.772028   333 sgd_solver.cpp:172] Iteration 8400, lr = 6.5536e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:02:44.454139   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:03:26.891999   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:04:09.673383   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:04:53.429077   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:05:18.184871   333 solver.cpp:637] Iteration 8500, Testing net (#0)
I0511 21:05:30.283288   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:05:31.559095   333 solver.cpp:749] class AP 1: 0.901647
I0511 21:05:31.559430   333 solver.cpp:749] class AP 2: 0.891616
I0511 21:05:31.559624   333 solver.cpp:749] class AP 3: 0.903371
I0511 21:05:31.559633   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.898878
I0511 21:05:31.559662   333 solver.cpp:284] Tests completed in 174.789s
I0511 21:05:31.990664   333 solver.cpp:354] Iteration 8500 (0.57212 iter/s, 174.789s/100 iter), 320/376.5ep, loss = 2.13822
I0511 21:05:31.990744   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.97944 (* 1 = 1.97944 loss)
I0511 21:05:31.990775   333 sgd_solver.cpp:172] Iteration 8500, lr = 5.0625e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:05:46.790441   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:06:27.364746   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:07:04.395090   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:07:48.378629   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:08:03.116268   333 solver.cpp:354] Iteration 8600 (0.661699 iter/s, 151.126s/100 iter), 323.8/376.5ep, loss = 2.27183
I0511 21:08:03.116860   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.91977 (* 1 = 1.91977 loss)
I0511 21:08:03.117131   333 sgd_solver.cpp:172] Iteration 8600, lr = 3.8416e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:08:31.343760   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:09:10.999379   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:09:53.228312   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:10:36.317342   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:10:38.914086   333 solver.cpp:354] Iteration 8700 (0.641856 iter/s, 155.798s/100 iter), 327.5/376.5ep, loss = 1.98083
I0511 21:10:38.914260   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.07024 (* 1 = 2.07024 loss)
I0511 21:10:38.914325   333 sgd_solver.cpp:172] Iteration 8700, lr = 2.8561e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:11:19.195433   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:11:58.426579   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:12:39.896157   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:13:12.742578   333 solver.cpp:354] Iteration 8800 (0.650073 iter/s, 153.829s/100 iter), 331.3/376.5ep, loss = 2.25732
I0511 21:13:12.742779   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.9723 (* 1 = 1.9723 loss)
I0511 21:13:12.742825   333 sgd_solver.cpp:172] Iteration 8800, lr = 2.0736e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:13:21.572659   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:14:03.181809   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:14:47.657747   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:15:36.479804   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:16:00.106556   333 solver.cpp:354] Iteration 8900 (0.597499 iter/s, 167.364s/100 iter), 335.1/376.5ep, loss = 2.04262
I0511 21:16:00.106813   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.00751 (* 1 = 2.00751 loss)
I0511 21:16:00.106899   333 sgd_solver.cpp:172] Iteration 8900, lr = 1.4641e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:16:18.145308   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:16:57.771366   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:17:39.250295   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:26.392151   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:38.295130   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_9000.caffemodel
I0511 21:18:38.365571   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_9000.solverstate
I0511 21:18:38.425760   333 solver.cpp:637] Iteration 9000, Testing net (#0)
I0511 21:18:50.515949   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:18:51.852352   333 solver.cpp:749] class AP 1: 0.90119
I0511 21:18:51.852696   333 solver.cpp:749] class AP 2: 0.892908
I0511 21:18:51.852883   333 solver.cpp:749] class AP 3: 0.902728
I0511 21:18:51.852890   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.898942
I0511 21:18:51.852916   333 solver.cpp:284] Tests completed in 171.747s
I0511 21:18:52.278501   333 solver.cpp:354] Iteration 9000 (0.582252 iter/s, 171.747s/100 iter), 338.8/376.5ep, loss = 2.06684
I0511 21:18:52.278523   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.521 (* 1 = 1.521 loss)
I0511 21:18:52.278529   333 sgd_solver.cpp:172] Iteration 9000, lr = 1e-07, m = 0.9, wd = 1e-05, gs = 1
I0511 21:19:10.352946   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:19:53.415158   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:20:35.455065   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:21:18.951011   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:21:26.108482   333 solver.cpp:354] Iteration 9100 (0.650067 iter/s, 153.83s/100 iter), 342.6/376.5ep, loss = 2.03956
I0511 21:21:26.108692   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.62536 (* 1 = 1.62536 loss)
I0511 21:21:26.108763   333 sgd_solver.cpp:172] Iteration 9100, lr = 6.56099e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:21:59.845499   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:22:47.502254   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:23:28.403669   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:24:09.125432   333 solver.cpp:354] Iteration 9200 (0.613432 iter/s, 163.017s/100 iter), 346.4/376.5ep, loss = 2.09217
I0511 21:24:09.125531   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.73983 (* 1 = 1.73983 loss)
I0511 21:24:09.125557   333 sgd_solver.cpp:172] Iteration 9200, lr = 4.096e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:24:09.617555   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:24:53.837898   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:25:36.629724   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:26:19.207026   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:26:51.372265   333 solver.cpp:354] Iteration 9300 (0.616343 iter/s, 162.247s/100 iter), 350.1/376.5ep, loss = 1.91648
I0511 21:26:51.372471   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.41199 (* 1 = 1.41199 loss)
I0511 21:26:51.372524   333 sgd_solver.cpp:172] Iteration 9300, lr = 2.401e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:27:05.626217   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:27:45.806048   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:28:30.350258   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:29:12.593127   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:29:30.813346   333 solver.cpp:354] Iteration 9400 (0.627189 iter/s, 159.441s/100 iter), 353.9/376.5ep, loss = 2.15166
I0511 21:29:30.813431   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.05091 (* 1 = 2.05091 loss)
I0511 21:29:30.813457   333 sgd_solver.cpp:172] Iteration 9400, lr = 1.296e-08, m = 0.9, wd = 1e-05, gs = 1
I0511 21:29:52.967552   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:30:36.460341   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:31:18.386620   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:31:55.391315   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:32:06.779017   333 solver.cpp:637] Iteration 9500, Testing net (#0)
I0511 21:32:17.900935   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:32:19.306239   333 solver.cpp:749] class AP 1: 0.902223
I0511 21:32:19.306591   333 solver.cpp:749] class AP 2: 0.892146
I0511 21:32:19.306776   333 solver.cpp:749] class AP 3: 0.903544
I0511 21:32:19.306782   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899304
I0511 21:32:19.306813   333 solver.cpp:284] Tests completed in 168.494s
I0511 21:32:19.757396   333 solver.cpp:354] Iteration 9500 (0.593493 iter/s, 168.494s/100 iter), 357.6/376.5ep, loss = 1.98063
I0511 21:32:19.757484   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.99207 (* 1 = 2.99207 loss)
I0511 21:32:19.757529   333 sgd_solver.cpp:172] Iteration 9500, lr = 6.25001e-09, m = 0.9, wd = 1e-05, gs = 1
I0511 21:32:47.832080   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:33:33.259634   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:34:11.186547   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:34:53.445513   333 solver.cpp:354] Iteration 9600 (0.650667 iter/s, 153.689s/100 iter), 361.4/376.5ep, loss = 2.25232
I0511 21:34:53.445617   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.29087 (* 1 = 2.29087 loss)
I0511 21:34:53.445645   333 sgd_solver.cpp:172] Iteration 9600, lr = 2.56001e-09, m = 0.9, wd = 1e-05, gs = 1
I0511 21:34:54.428040   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:35:37.219324   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:36:20.195230   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:37:01.839841   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:37:34.491852   333 solver.cpp:354] Iteration 9700 (0.620938 iter/s, 161.047s/100 iter), 365.2/376.5ep, loss = 2.17996
I0511 21:37:34.492084   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.71302 (* 1 = 1.71302 loss)
I0511 21:37:34.492149   333 sgd_solver.cpp:172] Iteration 9700, lr = 8.09997e-10, m = 0.9, wd = 1e-05, gs = 1
I0511 21:37:44.847134   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:38:25.889276   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:39:05.092862   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:39:53.679926   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:40:13.841048   333 solver.cpp:354] Iteration 9800 (0.627556 iter/s, 159.348s/100 iter), 368.9/376.5ep, loss = 2.27157
I0511 21:40:13.841493   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.27908 (* 1 = 2.27908 loss)
I0511 21:40:13.841683   333 sgd_solver.cpp:172] Iteration 9800, lr = 1.59999e-10, m = 0.9, wd = 1e-05, gs = 1
I0511 21:40:35.639956   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:41:15.715489   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:41:58.198559   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:42:40.277652   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:42:50.433265   333 solver.cpp:354] Iteration 9900 (0.63862 iter/s, 156.588s/100 iter), 372.7/376.5ep, loss = 2.2722
I0511 21:42:50.433460   333 solver.cpp:378]     Train net output #0: mbox_loss = 2.12744 (* 1 = 2.12744 loss)
I0511 21:42:50.433524   333 sgd_solver.cpp:172] Iteration 9900, lr = 9.99996e-12, m = 0.9, wd = 1e-05, gs = 1
I0511 21:43:20.209372   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:44:05.563848   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:44:47.815733   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:25.252507   339 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:26.258118   333 solver.cpp:354] Iteration 9999 (0.635343 iter/s, 155.821s/99 iter), 376.4/376.5ep, loss = 2.03161
I0511 21:45:26.258263   333 solver.cpp:378]     Train net output #0: mbox_loss = 1.71914 (* 1 = 1.71914 loss)
I0511 21:45:26.258314   333 solver.cpp:907] Snapshotting to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_10000.caffemodel
I0511 21:45:26.302434   333 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/EYES/JDetNet/20200511_10-37_ssdSize512x512_dsFac32_dsTypePSP_rWidthHeight768x320_batchNorm0/l1reg/EYES_ssdJacintoNetV2_iter_10000.solverstate
I0511 21:45:26.508044   333 solver.cpp:503] Iteration 10000, loss = 2.07326
I0511 21:45:26.508451   333 solver.cpp:637] Iteration 10000, Testing net (#0)
I0511 21:45:40.300990   376 data_reader.cpp:320] Restarting data pre-fetching
I0511 21:45:41.771572   333 solver.cpp:749] class AP 1: 0.90185
I0511 21:45:41.771916   333 solver.cpp:749] class AP 2: 0.892207
I0511 21:45:41.772109   333 solver.cpp:749] class AP 3: 0.903409
I0511 21:45:41.772116   333 solver.cpp:755] Test net output mAP #0: detection_eval = 0.899155
I0511 21:45:41.772138   333 caffe.cpp:268] Solver performance on device 0: 0.6239 * 16 = 19.96 img/sec (10000 itr in 1.603e+04 sec)
I0511 21:45:41.772145   333 caffe.cpp:271] Optimization Done in 4h 27m 59s
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument
*** Aborted at 1589233541 (unix time) try "date -d @1589233541" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x14d) received by PID 333 (TID 0x7f124bfff700) from PID 333; stack trace: ***
    @     0x7f148a6b3f20 (unknown)
    @     0x7f148a6b3e97 gsignal
    @     0x7f148a6b5801 abort
    @     0x7f148e7bf84a __gnu_cxx::__verbose_terminate_handler()
    @     0x7f148e7bdf47 __cxxabiv1::__terminate()
    @     0x7f148e7bdf7d std::terminate()
    @     0x7f148e7be15a __cxa_throw
    @     0x7f148c2640ac boost::throw_exception<>()
    @     0x7f148c26419d boost::mutex::lock()
    @     0x7f148c265020 boost::unique_lock<>::lock()
    @     0x7f148c6fc89f caffe::BlockingQueue<>::push()
    @     0x7f148c2e349e caffe::AnnotatedDataLayer<>::load_batch()
    @     0x7f148c31d476 caffe::BasePrefetchingDataLayer<>::InternalThreadEntryN()
    @     0x7f148c2917ee caffe::InternalThread::entry()
    @     0x7f148c29354b boost::detail::thread_data<>::run()
    @     0x7f148b8cb7ee thread_proxy
    @     0x7f148a45d6db start_thread
    @     0x7f148a79688f clone
    @                0x0 (unknown)
